--- a/docs/source/hi/_config.py
+++ b//dev/null
@@ -1,9 +0,0 @@
-INSTALL_CONTENT = """
-! pip install smolagents
-"""
-notebook_first_cells = [{"type": "code", "content": INSTALL_CONTENT}]
-black_avoid_patterns = {
-    "{processor_class}": "FakeProcessorClass",
-    "{model_class}": "FakeModelClass",
-    "{object_class}": "FakeObjectClass",
-}

--- a/examples/agent_from_any_llm.py
+++ b//dev/null
@@ -1,32 +0,0 @@
-from typing import Optional
-from smolagents import HfApiModel, LiteLLMModel, TransformersModel, tool
-from smolagents.agents import CodeAgent, ToolCallingAgent
-available_inferences = ["hf_api", "transformers", "ollama", "litellm"]
-chosen_inference = "transformers"
-print(f"Chose model {chosen_inference}")
-if chosen_inference == "hf_api":
-    model = HfApiModel(model_id="meta-llama/Llama-3.3-70B-Instruct")
-elif chosen_inference == "transformers":
-    model = TransformersModel(model_id="HuggingFaceTB/SmolLM2-1.7B-Instruct", device_map="auto", max_new_tokens=1000)
-elif chosen_inference == "ollama":
-    model = LiteLLMModel(
-        model_id="ollama_chat/llama3.2",
-        api_base="http://localhost:11434",  # replace with remote open-ai compatible server if necessary
-        api_key="your-api-key",  # replace with API key if necessary
-    )
-elif chosen_inference == "litellm":
-    model = LiteLLMModel(model_id="gpt-4o")
-@tool
-def get_weather(location: str, celsius: Optional[bool] = False) -> str:
-    """
-    Get weather in the next days at given location.
-    Secretly this tool does not care about the location, it hates the weather everywhere.
-    Args:
-        location: the location
-        celsius: the temperature
-    """
-    return "The weather is UNGODLY with torrential rains and temperatures below -10°C"
-agent = ToolCallingAgent(tools=[get_weather], model=model)
-print("ToolCallingAgent:", agent.run("What's the weather like in Paris?"))
-agent = CodeAgent(tools=[get_weather], model=model)
-print("ToolCallingAgent:", agent.run("What's the weather like in Paris?"))

--- a/examples/e2b_example.py
+++ b/examples/e2b_example.py
@@ -1,26 +1,26 @@
+from smolagents import Tool, CodeAgent, HfApiModel
+from smolagents.default_tools import VisitWebpageTool
 from dotenv import load_dotenv
-from smolagents import CodeAgent, HfApiModel, Tool
-from smolagents.default_tools import VisitWebpageTool
 load_dotenv()
 class GetCatImageTool(Tool):
     name = "get_cat_image"
     description = "Get a cat image"
     inputs = {}
     output_type = "image"
     def __init__(self):
         super().__init__()
         self.url = "https://em-content.zobj.net/source/twitter/53/robot-face_1f916.png"
     def forward(self):
+        from PIL import Image
+        import requests
         from io import BytesIO
-        import requests
-        from PIL import Image
         response = requests.get(self.url)
         return Image.open(BytesIO(response.content))
 get_cat_image = GetCatImageTool()
 agent = CodeAgent(
     tools=[get_cat_image, VisitWebpageTool()],
     model=HfApiModel(),
     additional_authorized_imports=[
         "Pillow",
         "requests",
         "markdownify",

--- a/examples/gradio_upload.py
+++ b/examples/gradio_upload.py
@@ -1,3 +1,3 @@
-from smolagents import CodeAgent, GradioUI, HfApiModel
+from smolagents import CodeAgent, HfApiModel, GradioUI
 agent = CodeAgent(tools=[], model=HfApiModel(), max_steps=4, verbosity_level=1)
 GradioUI(agent, file_upload_folder="./data").launch()

--- a/examples/inspect_runs.py
+++ b/examples/inspect_runs.py
@@ -1,31 +1,35 @@
-from openinference.instrumentation.smolagents import SmolagentsInstrumentor
 from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
 from opentelemetry.sdk.trace import TracerProvider
 from opentelemetry.sdk.trace.export import SimpleSpanProcessor
+from openinference.instrumentation.smolagents import SmolagentsInstrumentor
 from smolagents import (
     CodeAgent,
     DuckDuckGoSearchTool,
-    HfApiModel,
+    VisitWebpageTool,
     ManagedAgent,
     ToolCallingAgent,
-    VisitWebpageTool,
+    HfApiModel,
 )
 trace_provider = TracerProvider()
-trace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter("http://0.0.0.0:6006/v1/traces")))
+trace_provider.add_span_processor(
+    SimpleSpanProcessor(OTLPSpanExporter("http://0.0.0.0:6006/v1/traces"))
+)
 SmolagentsInstrumentor().instrument(tracer_provider=trace_provider, skip_dep_check=True)
 model = HfApiModel()
 agent = ToolCallingAgent(
     tools=[DuckDuckGoSearchTool(), VisitWebpageTool()],
     model=model,
 )
 managed_agent = ManagedAgent(
     agent=agent,
     name="managed_agent",
     description="This is an agent that can do web search.",
 )
 manager_agent = CodeAgent(
     tools=[],
     model=model,
     managed_agents=[managed_agent],
 )
-manager_agent.run("If the US keeps it 2024 growth rate, how many years would it take for the GDP to double?")
+manager_agent.run(
+    "If the US keeps it 2024 growth rate, how many years would it take for the GDP to double?"
+)

--- a/examples/multiple_tools.py
+++ b//dev/null
@@ -1,170 +0,0 @@
-from typing import Optional
-import requests
-from smolagents import CodeAgent, HfApiModel, tool
-model = HfApiModel()
-@tool
-def get_weather(location: str, celsius: Optional[bool] = False) -> str:
-    """
-    Get the current weather at the given location using the WeatherStack API.
-    Args:
-        location: The location (city name).
-        celsius: Whether to return the temperature in Celsius (default is False, which returns Fahrenheit).
-    Returns:
-        A string describing the current weather at the location.
-    """
-    api_key = "your_api_key"  # Replace with your API key from https://weatherstack.com/
-    units = "m" if celsius else "f"  # 'm' for Celsius, 'f' for Fahrenheit
-    url = f"http://api.weatherstack.com/current?access_key={api_key}&query={location}&units={units}"
-    try:
-        response = requests.get(url)
-        response.raise_for_status()  # Raise an exception for HTTP errors
-        data = response.json()
-        if data.get("error"):  # Check if there's an error in the response
-            return f"Error: {data['error'].get('info', 'Unable to fetch weather data.')}"
-        weather = data["current"]["weather_descriptions"][0]
-        temp = data["current"]["temperature"]
-        temp_unit = "°C" if celsius else "°F"
-        return f"The current weather in {location} is {weather} with a temperature of {temp} {temp_unit}."
-    except requests.exceptions.RequestException as e:
-        return f"Error fetching weather data: {str(e)}"
-@tool
-def convert_currency(amount: float, from_currency: str, to_currency: str) -> str:
-    """
-    Converts a specified amount from one currency to another using the ExchangeRate-API.
-    Args:
-        amount: The amount of money to convert.
-        from_currency: The currency code of the currency to convert from (e.g., 'USD').
-        to_currency: The currency code of the currency to convert to (e.g., 'EUR').
-    Returns:
-        str: A string describing the converted amount in the target currency, or an error message if the conversion fails.
-    Raises:
-        requests.exceptions.RequestException: If there is an issue with the HTTP request to the ExchangeRate-API.
-    """
-    api_key = "your_api_key"  # Replace with your actual API key from https://www.exchangerate-api.com/
-    url = f"https://v6.exchangerate-api.com/v6/{api_key}/latest/{from_currency}"
-    try:
-        response = requests.get(url)
-        response.raise_for_status()
-        data = response.json()
-        exchange_rate = data["conversion_rates"].get(to_currency)
-        if not exchange_rate:
-            return f"Error: Unable to find exchange rate for {from_currency} to {to_currency}."
-        converted_amount = amount * exchange_rate
-        return f"{amount} {from_currency} is equal to {converted_amount} {to_currency}."
-    except requests.exceptions.RequestException as e:
-        return f"Error fetching conversion data: {str(e)}"
-@tool
-def get_news_headlines() -> str:
-    """
-    Fetches the top news headlines from the News API for the United States.
-    This function makes a GET request to the News API to retrieve the top news headlines
-    for the United States. It returns the titles and sources of the top 5 articles as a
-    formatted string. If no articles are available, it returns a message indicating that
-    no news is available. In case of a request error, it returns an error message.
-    Returns:
-        str: A string containing the top 5 news headlines and their sources, or an error message.
-    """
-    api_key = "your_api_key"  # Replace with your actual API key from https://newsapi.org/
-    url = f"https://newsapi.org/v2/top-headlines?country=us&apiKey={api_key}"
-    try:
-        response = requests.get(url)
-        response.raise_for_status()
-        data = response.json()
-        articles = data["articles"]
-        if not articles:
-            return "No news available at the moment."
-        headlines = [f"{article['title']} - {article['source']['name']}" for article in articles[:5]]
-        return "\n".join(headlines)
-    except requests.exceptions.RequestException as e:
-        return f"Error fetching news data: {str(e)}"
-@tool
-def get_joke() -> str:
-    """
-    Fetches a random joke from the JokeAPI.
-    This function sends a GET request to the JokeAPI to retrieve a random joke.
-    It handles both single jokes and two-part jokes (setup and delivery).
-    If the request fails or the response does not contain a joke, an error message is returned.
-    Returns:
-        str: The joke as a string, or an error message if the joke could not be fetched.
-    """
-    url = "https://v2.jokeapi.dev/joke/Any?type=single"
-    try:
-        response = requests.get(url)
-        response.raise_for_status()
-        data = response.json()
-        if "joke" in data:
-            return data["joke"]
-        elif "setup" in data and "delivery" in data:
-            return f"{data['setup']} - {data['delivery']}"
-        else:
-            return "Error: Unable to fetch joke."
-    except requests.exceptions.RequestException as e:
-        return f"Error fetching joke: {str(e)}"
-@tool
-def get_time_in_timezone(location: str) -> str:
-    """
-    Fetches the current time for a given location using the World Time API.
-    Args:
-        location: The location for which to fetch the current time, formatted as 'Region/City'.
-    Returns:
-        str: A string indicating the current time in the specified location, or an error message if the request fails.
-    Raises:
-        requests.exceptions.RequestException: If there is an issue with the HTTP request.
-    """
-    url = f"http://worldtimeapi.org/api/timezone/{location}.json"
-    try:
-        response = requests.get(url)
-        response.raise_for_status()
-        data = response.json()
-        current_time = data["datetime"]
-        return f"The current time in {location} is {current_time}."
-    except requests.exceptions.RequestException as e:
-        return f"Error fetching time data: {str(e)}"
-@tool
-def get_random_fact() -> str:
-    """
-    Fetches a random fact from the "uselessfacts.jsph.pl" API.
-    Returns:
-        str: A string containing the random fact or an error message if the request fails.
-    """
-    url = "https://uselessfacts.jsph.pl/random.json?language=en"
-    try:
-        response = requests.get(url)
-        response.raise_for_status()
-        data = response.json()
-        return f"Random Fact: {data['text']}"
-    except requests.exceptions.RequestException as e:
-        return f"Error fetching random fact: {str(e)}"
-@tool
-def search_wikipedia(query: str) -> str:
-    """
-    Fetches a summary of a Wikipedia page for a given query.
-    Args:
-        query: The search term to look up on Wikipedia.
-    Returns:
-        str: A summary of the Wikipedia page if successful, or an error message if the request fails.
-    Raises:
-        requests.exceptions.RequestException: If there is an issue with the HTTP request.
-    """
-    url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{query}"
-    try:
-        response = requests.get(url)
-        response.raise_for_status()
-        data = response.json()
-        title = data["title"]
-        extract = data["extract"]
-        return f"Summary for {title}: {extract}"
-    except requests.exceptions.RequestException as e:
-        return f"Error fetching Wikipedia data: {str(e)}"
-agent = CodeAgent(
-    tools=[
-        convert_currency,
-        get_weather,
-        get_news_headlines,
-        get_joke,
-        get_random_fact,
-        search_wikipedia,
-    ],
-    model=model,
-)
-agent.run("5000 dollars to Euros")

--- a/examples/rag.py
+++ b/examples/rag.py
@@ -1,18 +1,21 @@
 import datasets
 from langchain.docstore.document import Document
 from langchain.text_splitter import RecursiveCharacterTextSplitter
 from langchain_community.retrievers import BM25Retriever
 knowledge_base = datasets.load_dataset("m-ric/huggingface_doc", split="train")
-knowledge_base = knowledge_base.filter(lambda row: row["source"].startswith("huggingface/transformers"))
+knowledge_base = knowledge_base.filter(
+    lambda row: row["source"].startswith("huggingface/transformers")
+)
 source_docs = [
-    Document(page_content=doc["text"], metadata={"source": doc["source"].split("/")[1]}) for doc in knowledge_base
+    Document(page_content=doc["text"], metadata={"source": doc["source"].split("/")[1]})
+    for doc in knowledge_base
 ]
 text_splitter = RecursiveCharacterTextSplitter(
     chunk_size=500,
     chunk_overlap=50,
     add_start_index=True,
     strip_whitespace=True,
     separators=["\n\n", "\n", ".", " ", ""],
 )
 docs_processed = text_splitter.split_documents(source_docs)
 from smolagents import Tool
@@ -28,23 +31,28 @@
     output_type = "string"
     def __init__(self, docs, **kwargs):
         super().__init__(**kwargs)
         self.retriever = BM25Retriever.from_documents(docs, k=10)
     def forward(self, query: str) -> str:
         assert isinstance(query, str), "Your search query must be a string"
         docs = self.retriever.invoke(
             query,
         )
         return "\nRetrieved documents:\n" + "".join(
-            [f"\n\n===== Document {str(i)} =====\n" + doc.page_content for i, doc in enumerate(docs)]
+            [
+                f"\n\n===== Document {str(i)} =====\n" + doc.page_content
+                for i, doc in enumerate(docs)
+            ]
         )
-from smolagents import CodeAgent, HfApiModel
+from smolagents import HfApiModel, CodeAgent
 retriever_tool = RetrieverTool(docs_processed)
 agent = CodeAgent(
     tools=[retriever_tool],
     model=HfApiModel("meta-llama/Llama-3.3-70B-Instruct"),
     max_steps=4,
     verbosity_level=2,
 )
-agent_output = agent.run("For a transformers model training, which is slower, the forward or the backward pass?")
+agent_output = agent.run(
+    "For a transformers model training, which is slower, the forward or the backward pass?"
+)
 print("Final output:")
 print(agent_output)

--- a/examples/rag_using_chromadb.py
+++ b//dev/null
@@ -1,69 +0,0 @@
-import os
-import datasets
-from langchain.docstore.document import Document
-from langchain.text_splitter import RecursiveCharacterTextSplitter
-from langchain_chroma import Chroma
-from langchain_huggingface import HuggingFaceEmbeddings
-from tqdm import tqdm
-from transformers import AutoTokenizer
-from smolagents import LiteLLMModel, Tool
-from smolagents.agents import CodeAgent
-knowledge_base = datasets.load_dataset("m-ric/huggingface_doc", split="train")
-source_docs = [
-    Document(page_content=doc["text"], metadata={"source": doc["source"].split("/")[1]}) for doc in knowledge_base
-]
-text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
-    AutoTokenizer.from_pretrained("thenlper/gte-small"),
-    chunk_size=200,
-    chunk_overlap=20,
-    add_start_index=True,
-    strip_whitespace=True,
-    separators=["\n\n", "\n", ".", " ", ""],
-)
-print("Splitting documents...")
-docs_processed = []
-unique_texts = {}
-for doc in tqdm(source_docs):
-    new_docs = text_splitter.split_documents([doc])
-    for new_doc in new_docs:
-        if new_doc.page_content not in unique_texts:
-            unique_texts[new_doc.page_content] = True
-            docs_processed.append(new_doc)
-print("Embedding documents... This should take a few minutes (5 minutes on MacBook with M1 Pro)")
-embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
-vector_store = Chroma.from_documents(docs_processed, embeddings, persist_directory="./chroma_db")
-class RetrieverTool(Tool):
-    name = "retriever"
-    description = (
-        "Uses semantic search to retrieve the parts of documentation that could be most relevant to answer your query."
-    )
-    inputs = {
-        "query": {
-            "type": "string",
-            "description": "The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.",
-        }
-    }
-    output_type = "string"
-    def __init__(self, vector_store, **kwargs):
-        super().__init__(**kwargs)
-        self.vector_store = vector_store
-    def forward(self, query: str) -> str:
-        assert isinstance(query, str), "Your search query must be a string"
-        docs = self.vector_store.similarity_search(query, k=3)
-        return "\nRetrieved documents:\n" + "".join(
-            [f"\n\n===== Document {str(i)} =====\n" + doc.page_content for i, doc in enumerate(docs)]
-        )
-retriever_tool = RetrieverTool(vector_store)
-model = LiteLLMModel(
-    model_id="groq/llama-3.3-70b-versatile",
-    api_key=os.environ.get("GROQ_API_KEY"),
-)
-agent = CodeAgent(
-    tools=[retriever_tool],
-    model=model,
-    max_steps=4,
-    verbosity_level=2,
-)
-agent_output = agent.run("How can I push a model to the Hub?")
-print("Final output:")
-print(agent_output)

--- a/examples/text_to_sql.py
+++ b/examples/text_to_sql.py
@@ -1,18 +1,18 @@
 from sqlalchemy import (
+    create_engine,
+    MetaData,
+    Table,
     Column,
+    String,
+    Integer,
     Float,
-    Integer,
-    MetaData,
-    String,
-    Table,
-    create_engine,
     insert,
     inspect,
     text,
 )
 engine = create_engine("sqlite:///:memory:")
 metadata_obj = MetaData()
 table_name = "receipts"
 receipts = Table(
     table_name,
     metadata_obj,
@@ -27,21 +27,23 @@
     {"receipt_id": 2, "customer_name": "Alex Mason", "price": 23.86, "tip": 0.24},
     {"receipt_id": 3, "customer_name": "Woodrow Wilson", "price": 53.43, "tip": 5.43},
     {"receipt_id": 4, "customer_name": "Margaret James", "price": 21.11, "tip": 1.00},
 ]
 for row in rows:
     stmt = insert(receipts).values(**row)
     with engine.begin() as connection:
         cursor = connection.execute(stmt)
 inspector = inspect(engine)
 columns_info = [(col["name"], col["type"]) for col in inspector.get_columns("receipts")]
-table_description = "Columns:\n" + "\n".join([f"  - {name}: {col_type}" for name, col_type in columns_info])
+table_description = "Columns:\n" + "\n".join(
+    [f"  - {name}: {col_type}" for name, col_type in columns_info]
+)
 print(table_description)
 from smolagents import tool
 @tool
 def sql_engine(query: str) -> str:
     """
     Allows you to perform SQL queries on the table. Returns a string representation of the result.
     The table is named 'receipts'. Its description is as follows:
         Columns:
         - receipt_id: INTEGER
         - customer_name: VARCHAR(16)

--- a//dev/null
+++ b/examples/tool_calling_agent_from_any_llm.py
@@ -0,0 +1,16 @@
+from smolagents.agents import ToolCallingAgent
+from smolagents import tool, LiteLLMModel
+from typing import Optional
+model = LiteLLMModel(model_id="gpt-4o")
+@tool
+def get_weather(location: str, celsius: Optional[bool] = False) -> str:
+    """
+    Get weather in the next days at given location.
+    Secretly this tool does not care about the location, it hates the weather everywhere.
+    Args:
+        location: the location
+        celsius: the temperature
+    """
+    return "The weather is UNGODLY with torrential rains and temperatures below -10°C"
+agent = ToolCallingAgent(tools=[get_weather], model=model)
+print(agent.run("What's the weather like in Paris?"))

--- a//dev/null
+++ b/examples/tool_calling_agent_mcp.py
@@ -0,0 +1,19 @@
+"""An example of loading a ToolCollection directly from an MCP server.
+Requirements: to run this example, you need to have uv installed and in your path in
+order to run the MCP server with uvx see `mcp_server_params` below.
+Note this is just a demo MCP server that was implemented for the purpose of this example.
+It only provide a single tool to search amongst pubmed papers abstracts.
+Usage:
+>>> uv run examples/tool_calling_agent_mcp.py
+"""
+import os
+from mcp import StdioServerParameters
+from smolagents import CodeAgent, HfApiModel, ToolCollection
+mcp_server_params = StdioServerParameters(
+    command="uvx",
+    args=["--quiet", "pubmedmcp@0.1.3"],
+    env={"UV_PYTHON": "3.12", **os.environ},
+)
+with ToolCollection.from_mcp(mcp_server_params) as tool_collection:
+    agent = CodeAgent(tools=tool_collection.tools, model=HfApiModel(), max_steps=4)
+    agent.run("Find me one risk associated with drinking alcohol regularly on low doses for humans.")

--- a//dev/null
+++ b/examples/tool_calling_agent_ollama.py
@@ -0,0 +1,20 @@
+from smolagents.agents import ToolCallingAgent
+from smolagents import tool, LiteLLMModel
+from typing import Optional
+model = LiteLLMModel(
+    model_id="ollama_chat/llama3.2",
+    api_base="http://localhost:11434",  # replace with remote open-ai compatible server if necessary
+    api_key="your-api-key",  # replace with API key if necessary
+)
+@tool
+def get_weather(location: str, celsius: Optional[bool] = False) -> str:
+    """
+    Get weather in the next days at given location.
+    Secretly this tool does not care about the location, it hates the weather everywhere.
+    Args:
+        location: the location
+        celsius: the temperature
+    """
+    return "The weather is UNGODLY with torrential rains and temperatures below -10°C"
+agent = ToolCallingAgent(tools=[get_weather], model=model)
+print(agent.run("What's the weather like in Paris?"))

--- a/examples/vlm_web_browser.py
+++ b//dev/null
@@ -1,160 +0,0 @@
-from io import BytesIO
-from time import sleep
-import helium
-from dotenv import load_dotenv
-from PIL import Image
-from selenium import webdriver
-from selenium.common.exceptions import ElementNotInteractableException, TimeoutException
-from selenium.webdriver.common.by import By
-from selenium.webdriver.support import expected_conditions as EC
-from selenium.webdriver.support.ui import WebDriverWait
-from smolagents import CodeAgent, LiteLLMModel, OpenAIServerModel, TransformersModel, tool  # noqa: F401
-from smolagents.agents import ActionStep
-load_dotenv()
-import os
-model = OpenAIServerModel(
-    api_key=os.getenv("FIREWORKS_API_KEY"),
-    api_base="https://api.fireworks.ai/inference/v1",
-    model_id="accounts/fireworks/models/qwen2-vl-72b-instruct",
-)
-def save_screenshot(step_log: ActionStep, agent: CodeAgent) -> None:
-    sleep(1.0)  # Let JavaScript animations happen before taking the screenshot
-    driver = helium.get_driver()
-    current_step = step_log.step_number
-    if driver is not None:
-        for step_logs in agent.logs:  # Remove previous screenshots from logs for lean processing
-            if isinstance(step_log, ActionStep) and step_log.step_number <= current_step - 2:
-                step_logs.observations_images = None
-        png_bytes = driver.get_screenshot_as_png()
-        image = Image.open(BytesIO(png_bytes))
-        print(f"Captured a browser screenshot: {image.size} pixels")
-        step_log.observations_images = [image.copy()]  # Create a copy to ensure it persists, important!
-    url_info = f"Current url: {driver.current_url}"
-    step_log.observations = url_info if step_logs.observations is None else step_log.observations + "\n" + url_info
-    return
-chrome_options = webdriver.ChromeOptions()
-chrome_options.add_argument("--force-device-scale-factor=1")
-chrome_options.add_argument("--window-size=1000,1300")
-chrome_options.add_argument("--disable-pdf-viewer")
-driver = helium.start_chrome(headless=False, options=chrome_options)
-@tool
-def search_item_ctrl_f(text: str, nth_result: int = 1) -> str:
-    """
-    Searches for text on the current page via Ctrl + F and jumps to the nth occurrence.
-    Args:
-        text: The text to search for
-        nth_result: Which occurrence to jump to (default: 1)
-    """
-    elements = driver.find_elements(By.XPATH, f"//*[contains(text(), '{text}')]")
-    if nth_result > len(elements):
-        raise Exception(f"Match n°{nth_result} not found (only {len(elements)} matches found)")
-    result = f"Found {len(elements)} matches for '{text}'."
-    elem = elements[nth_result - 1]
-    driver.execute_script("arguments[0].scrollIntoView(true);", elem)
-    result += f"Focused on element {nth_result} of {len(elements)}"
-    return result
-@tool
-def go_back() -> None:
-    """Goes back to previous page."""
-    driver.back()
-@tool
-def close_popups() -> str:
-    """
-    Closes any visible modal or pop-up on the page. Use this to dismiss pop-up windows! This does not work on cookie consent banners.
-    """
-    modal_selectors = [
-        "button[class*='close']",
-        "[class*='modal']",
-        "[class*='modal'] button",
-        "[class*='CloseButton']",
-        "[aria-label*='close']",
-        ".modal-close",
-        ".close-modal",
-        ".modal .close",
-        ".modal-backdrop",
-        ".modal-overlay",
-        "[class*='overlay']",
-    ]
-    wait = WebDriverWait(driver, timeout=0.5)
-    for selector in modal_selectors:
-        try:
-            elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, selector)))
-            for element in elements:
-                if element.is_displayed():
-                    try:
-                        driver.execute_script("arguments[0].click();", element)
-                    except ElementNotInteractableException:
-                        element.click()
-        except TimeoutException:
-            continue
-        except Exception as e:
-            print(f"Error handling selector {selector}: {str(e)}")
-            continue
-    return "Modals closed"
-agent = CodeAgent(
-    tools=[go_back, close_popups, search_item_ctrl_f],
-    model=model,
-    additional_authorized_imports=["helium"],
-    step_callbacks=[save_screenshot],
-    max_steps=20,
-    verbosity_level=2,
-)
-helium_instructions = """
-You can use helium to access websites. Don't bother about the helium driver, it's already managed.
-First you need to import everything from helium, then you can do other actions!
-Code:
-```py
-from helium import *
-go_to('github.com/trending')
-```<end_code>
-You can directly click clickable elements by inputting the text that appears on them.
-Code:
-```py
-click("Top products")
-```<end_code>
-If it's a link:
-Code:
-```py
-click(Link("Top products"))
-```<end_code>
-If you try to interact with an element and it's not found, you'll get a LookupError.
-In general stop your action after each button click to see what happens on your screenshot.
-Never try to login in a page.
-To scroll up or down, use scroll_down or scroll_up with as an argument the number of pixels to scroll from.
-Code:
-```py
-scroll_down(num_pixels=1200) # This will scroll one viewport down
-```<end_code>
-When you have pop-ups with a cross icon to close, don't try to click the close icon by finding its element or targeting an 'X' element (this most often fails).
-Just use your built-in tool `close_popups` to close them:
-Code:
-```py
-close_popups()
-```<end_code>
-You can use .exists() to check for the existence of an element. For example:
-Code:
-```py
-if Text('Accept cookies?').exists():
-    click('I accept')
-```<end_code>
-Proceed in several steps rather than trying to solve the task in one shot.
-And at the end, only when you have your answer, return your final answer.
-Code:
-```py
-final_answer("YOUR_ANSWER_HERE")
-```<end_code>
-If pages seem stuck on loading, you might have to wait, for instance `import time` and run `time.sleep(5.0)`. But don't overuse this!
-To list elements on page, DO NOT try code-based element searches like 'contributors = find_all(S("ol > li"))': just look at the latest screenshot you have and read it visually, or use your tool search_item_ctrl_f.
-Of course, you can act on buttons like a user would do when navigating.
-After each code blob you write, you will be automatically provided with an updated screenshot of the browser and the current browser url.
-But beware that the screenshot will only be taken at the end of the whole action, it won't see intermediate states.
-Don't kill the browser.
-"""
-github_request = """
-I'm trying to find how hard I have to work to get a repo in github.com/trending.
-Can you navigate to the profile for the top author of the top trending repo, and give me their total number of commits over the last year?
-"""  # The agent is able to achieve this request only when powered by GPT-4o or Claude-3.5-sonnet.
-search_request = """
-Please navigate to https://en.wikipedia.org/wiki/Chicago and give me a sentence containing the word "1992" that mentions a construction accident.
-"""
-agent.run(search_request + helium_instructions)

--- a/src/smolagents/__init__.py
+++ b/src/smolagents/__init__.py
@@ -1,12 +1,28 @@
-__version__ = "1.5.0"
-from .agents import *
-from .default_tools import *
-from .e2b_executor import *
-from .gradio_ui import *
-from .local_python_executor import *
-from .models import *
-from .monitoring import *
-from .prompts import *
-from .tools import *
-from .types import *
-from .utils import *
+__version__ = "1.4.1"
+from typing import TYPE_CHECKING
+from transformers.utils import _LazyModule
+from transformers.utils.import_utils import define_import_structure
+if TYPE_CHECKING:
+    from .agents import *
+    from .default_tools import *
+    from .e2b_executor import *
+    from .gradio_ui import *
+    from .local_python_executor import *
+    from .models import *
+    from .monitoring import *
+    from .prompts import *
+    from .tools import *
+    from .types import *
+    from .utils import *
+else:
+    import sys
+    _file = globals()["__file__"]
+    import_structure = define_import_structure(_file)
+    import_structure[""] = {"__version__": __version__}
+    sys.modules[__name__] = _LazyModule(
+        __name__,
+        _file,
+        import_structure,
+        module_spec=__spec__,
+        extra_objects={"__version__": __version__},
+    )

--- a/src/smolagents/_function_type_hints_utils.py
+++ b//dev/null
@@ -1,302 +0,0 @@
-"""This module contains utilities exclusively taken from `transformers` repository.
-Since they are not specific to `transformers` and that `transformers` is an heavy dependencies, those helpers have
-been duplicated.
-TODO: move them to `huggingface_hub` to avoid code duplication.
-"""
-import inspect
-import json
-import os
-import re
-import types
-from copy import copy
-from typing import (
-    Any,
-    Callable,
-    Dict,
-    List,
-    Optional,
-    Tuple,
-    Union,
-    get_args,
-    get_origin,
-    get_type_hints,
-)
-from huggingface_hub.utils import is_torch_available
-from .utils import _is_pillow_available
-def get_imports(filename: Union[str, os.PathLike]) -> List[str]:
-    """
-    Extracts all the libraries (not relative imports this time) that are imported in a file.
-    Args:
-        filename (`str` or `os.PathLike`): The module file to inspect.
-    Returns:
-        `List[str]`: The list of all packages required to use the input module.
-    """
-    with open(filename, "r", encoding="utf-8") as f:
-        content = f.read()
-    content = re.sub(r"\s*try\s*:.*?except.*?:", "", content, flags=re.DOTALL)
-    content = re.sub(
-        r"if is_flash_attn[a-zA-Z0-9_]+available\(\):\s*(from flash_attn\s*.*\s*)+",
-        "",
-        content,
-        flags=re.MULTILINE,
-    )
-    imports = re.findall(r"^\s*import\s+(\S+)\s*$", content, flags=re.MULTILINE)
-    imports += re.findall(r"^\s*from\s+(\S+)\s+import", content, flags=re.MULTILINE)
-    imports = [imp.split(".")[0] for imp in imports if not imp.startswith(".")]
-    return list(set(imports))
-class TypeHintParsingException(Exception):
-    """Exception raised for errors in parsing type hints to generate JSON schemas"""
-class DocstringParsingException(Exception):
-    """Exception raised for errors in parsing docstrings to generate JSON schemas"""
-def get_json_schema(func: Callable) -> Dict:
-    """
-    This function generates a JSON schema for a given function, based on its docstring and type hints. This is
-    mostly used for passing lists of tools to a chat template. The JSON schema contains the name and description of
-    the function, as well as the names, types and descriptions for each of its arguments. `get_json_schema()` requires
-    that the function has a docstring, and that each argument has a description in the docstring, in the standard
-    Google docstring format shown below. It also requires that all the function arguments have a valid Python type hint.
-    Although it is not required, a `Returns` block can also be added, which will be included in the schema. This is
-    optional because most chat templates ignore the return value of the function.
-    Args:
-        func: The function to generate a JSON schema for.
-    Returns:
-        A dictionary containing the JSON schema for the function.
-    Examples:
-    ```python
-    >>> def multiply(x: float, y: float):
-    >>>    '''
-    >>>    A function that multiplies two numbers
-    >>>
-    >>>    Args:
-    >>>        x: The first number to multiply
-    >>>        y: The second number to multiply
-    >>>    '''
-    >>>    return x * y
-    >>>
-    >>> print(get_json_schema(multiply))
-    {
-        "name": "multiply",
-        "description": "A function that multiplies two numbers",
-        "parameters": {
-            "type": "object",
-            "properties": {
-                "x": {"type": "number", "description": "The first number to multiply"},
-                "y": {"type": "number", "description": "The second number to multiply"}
-            },
-            "required": ["x", "y"]
-        }
-    }
-    ```
-    The general use for these schemas is that they are used to generate tool descriptions for chat templates that
-    support them, like so:
-    ```python
-    >>> from transformers import AutoTokenizer
-    >>> from transformers.utils import get_json_schema
-    >>>
-    >>> def multiply(x: float, y: float):
-    >>>    '''
-    >>>    A function that multiplies two numbers
-    >>>
-    >>>    Args:
-    >>>        x: The first number to multiply
-    >>>        y: The second number to multiply
-    >>>    return x * y
-    >>>    '''
-    >>>
-    >>> multiply_schema = get_json_schema(multiply)
-    >>> tokenizer = AutoTokenizer.from_pretrained("CohereForAI/c4ai-command-r-v01")
-    >>> messages = [{"role": "user", "content": "What is 179 x 4571?"}]
-    >>> formatted_chat = tokenizer.apply_chat_template(
-    >>>     messages,
-    >>>     tools=[multiply_schema],
-    >>>     chat_template="tool_use",
-    >>>     return_dict=True,
-    >>>     return_tensors="pt",
-    >>>     add_generation_prompt=True
-    >>> )
-    >>> # The formatted chat can now be passed to model.generate()
-    ```
-    Each argument description can also have an optional `(choices: ...)` block at the end, such as
-    `(choices: ["tea", "coffee"])`, which will be parsed into an `enum` field in the schema. Note that this will
-    only be parsed correctly if it is at the end of the line:
-    ```python
-    >>> def drink_beverage(beverage: str):
-    >>>    '''
-    >>>    A function that drinks a beverage
-    >>>
-    >>>    Args:
-    >>>        beverage: The beverage to drink (choices: ["tea", "coffee"])
-    >>>    '''
-    >>>    pass
-    >>>
-    >>> print(get_json_schema(drink_beverage))
-    ```
-    {
-        'name': 'drink_beverage',
-        'description': 'A function that drinks a beverage',
-        'parameters': {
-            'type': 'object',
-            'properties': {
-                'beverage': {
-                    'type': 'string',
-                    'enum': ['tea', 'coffee'],
-                    'description': 'The beverage to drink'
-                    }
-                },
-            'required': ['beverage']
-        }
-    }
-    """
-    doc = inspect.getdoc(func)
-    if not doc:
-        raise DocstringParsingException(
-            f"Cannot generate JSON schema for {func.__name__} because it has no docstring!"
-        )
-    doc = doc.strip()
-    main_doc, param_descriptions, return_doc = _parse_google_format_docstring(doc)
-    json_schema = _convert_type_hints_to_json_schema(func)
-    if (return_dict := json_schema["properties"].pop("return", None)) is not None:
-        if return_doc is not None:  # We allow a missing return docstring since most templates ignore it
-            return_dict["description"] = return_doc
-    for arg, schema in json_schema["properties"].items():
-        if arg not in param_descriptions:
-            raise DocstringParsingException(
-                f"Cannot generate JSON schema for {func.__name__} because the docstring has no description for the argument '{arg}'"
-            )
-        desc = param_descriptions[arg]
-        enum_choices = re.search(r"\(choices:\s*(.*?)\)\s*$", desc, flags=re.IGNORECASE)
-        if enum_choices:
-            schema["enum"] = [c.strip() for c in json.loads(enum_choices.group(1))]
-            desc = enum_choices.string[: enum_choices.start()].strip()
-        schema["description"] = desc
-    output = {"name": func.__name__, "description": main_doc, "parameters": json_schema}
-    if return_dict is not None:
-        output["return"] = return_dict
-    return {"type": "function", "function": output}
-description_re = re.compile(r"^(.*?)[\n\s]*(Args:|Returns:|Raises:|\Z)", re.DOTALL)
-args_re = re.compile(r"\n\s*Args:\n\s*(.*?)[\n\s]*(Returns:|Raises:|\Z)", re.DOTALL)
-args_split_re = re.compile(
-    r"""
-(?:^|\n)  # Match the start of the args block, or a newline
-\s*(\w+):\s*  # Capture the argument name and strip spacing
-(.*?)\s*  # Capture the argument description, which can span multiple lines, and strip trailing spacing
-(?=\n\s*\w+:|\Z)  # Stop when you hit the next argument or the end of the block
-""",
-    re.DOTALL | re.VERBOSE,
-)
-returns_re = re.compile(r"\n\s*Returns:\n\s*(.*?)[\n\s]*(Raises:|\Z)", re.DOTALL)
-def _parse_google_format_docstring(
-    docstring: str,
-) -> Tuple[Optional[str], Optional[Dict], Optional[str]]:
-    """
-    Parses a Google-style docstring to extract the function description,
-    argument descriptions, and return description.
-    Args:
-        docstring (str): The docstring to parse.
-    Returns:
-        The function description, arguments, and return description.
-    """
-    description_match = description_re.search(docstring)
-    args_match = args_re.search(docstring)
-    returns_match = returns_re.search(docstring)
-    description = description_match.group(1).strip() if description_match else None
-    docstring_args = args_match.group(1).strip() if args_match else None
-    returns = returns_match.group(1).strip() if returns_match else None
-    if docstring_args is not None:
-        docstring_args = "\n".join([line for line in docstring_args.split("\n") if line.strip()])  # Remove blank lines
-        matches = args_split_re.findall(docstring_args)
-        args_dict = {match[0]: re.sub(r"\s*\n+\s*", " ", match[1].strip()) for match in matches}
-    else:
-        args_dict = {}
-    return description, args_dict, returns
-def _convert_type_hints_to_json_schema(func: Callable, error_on_missing_type_hints: bool = True) -> Dict:
-    type_hints = get_type_hints(func)
-    signature = inspect.signature(func)
-    properties = {}
-    for param_name, param_type in type_hints.items():
-        properties[param_name] = _parse_type_hint(param_type)
-    required = []
-    for param_name, param in signature.parameters.items():
-        if param.annotation == inspect.Parameter.empty and error_on_missing_type_hints:
-            raise TypeHintParsingException(f"Argument {param.name} is missing a type hint in function {func.__name__}")
-        if param_name not in properties:
-            properties[param_name] = {}
-        if param.default == inspect.Parameter.empty:
-            required.append(param_name)
-        else:
-            properties[param_name]["nullable"] = True
-    schema = {"type": "object", "properties": properties}
-    if required:
-        schema["required"] = required
-    return schema
-def _parse_type_hint(hint: str) -> Dict:
-    origin = get_origin(hint)
-    args = get_args(hint)
-    if origin is None:
-        try:
-            return _get_json_schema_type(hint)
-        except KeyError:
-            raise TypeHintParsingException(
-                "Couldn't parse this type hint, likely due to a custom class or object: ",
-                hint,
-            )
-    elif origin is Union or (hasattr(types, "UnionType") and origin is types.UnionType):
-        subtypes = [_parse_type_hint(t) for t in args if t is not type(None)]
-        if len(subtypes) == 1:
-            return_dict = subtypes[0]
-        elif all(isinstance(subtype["type"], str) for subtype in subtypes):
-            return_dict = {"type": sorted([subtype["type"] for subtype in subtypes])}
-        else:
-            return_dict = {"anyOf": subtypes}
-        if type(None) in args:
-            return_dict["nullable"] = True
-        return return_dict
-    elif origin is list:
-        if not args:
-            return {"type": "array"}
-        else:
-            return {"type": "array", "items": _parse_type_hint(args[0])}
-    elif origin is tuple:
-        if not args:
-            return {"type": "array"}
-        if len(args) == 1:
-            raise TypeHintParsingException(
-                f"The type hint {str(hint).replace('typing.', '')} is a Tuple with a single element, which "
-                "we do not automatically convert to JSON schema as it is rarely necessary. If this input can contain "
-                "more than one element, we recommend "
-                "using a List[] type instead, or if it really is a single element, remove the Tuple[] wrapper and just "
-                "pass the element directly."
-            )
-        if ... in args:
-            raise TypeHintParsingException(
-                "Conversion of '...' is not supported in Tuple type hints. "
-                "Use List[] types for variable-length"
-                " inputs instead."
-            )
-        return {"type": "array", "prefixItems": [_parse_type_hint(t) for t in args]}
-    elif origin is dict:
-        out = {"type": "object"}
-        if len(args) == 2:
-            out["additionalProperties"] = _parse_type_hint(args[1])
-        return out
-    raise TypeHintParsingException("Couldn't parse this type hint, likely due to a custom class or object: ", hint)
-_BASE_TYPE_MAPPING = {
-    int: {"type": "integer"},
-    float: {"type": "number"},
-    str: {"type": "string"},
-    bool: {"type": "boolean"},
-    Any: {"type": "any"},
-    types.NoneType: {"type": "null"},
-}
-def _get_json_schema_type(param_type: str) -> Dict[str, str]:
-    if param_type in _BASE_TYPE_MAPPING:
-        return copy(_BASE_TYPE_MAPPING[param_type])
-    if str(param_type) == "Image" and _is_pillow_available():
-        from PIL.Image import Image
-        if param_type == Image:
-            return {"type": "image"}
-    if str(param_type) == "Tensor" and is_torch_available():
-        from torch import Tensor
-        if param_type == Tensor:
-            return {"type": "audio"}
-    return {"type": "object"}

--- a/src/smolagents/agents.py
+++ b/src/smolagents/agents.py
@@ -1,32 +1,29 @@
-import inspect
 import time
-from collections import deque
 from dataclasses import dataclass
-from typing import Any, Callable, Dict, Generator, List, Optional, Tuple, Union
+from typing import Any, Callable, Dict, List, Optional, Tuple, Union
+from enum import IntEnum
 from rich import box
 from rich.console import Group
 from rich.panel import Panel
 from rich.rule import Rule
 from rich.syntax import Syntax
 from rich.text import Text
-from .default_tools import TOOL_MAPPING, FinalAnswerTool
+from rich.console import Console
+from .default_tools import FinalAnswerTool, TOOL_MAPPING
 from .e2b_executor import E2BExecutor
 from .local_python_executor import (
     BASE_BUILTIN_MODULES,
     LocalPythonInterpreter,
     fix_final_answer_code,
 )
-from .models import (
-    ChatMessage,
-    MessageRole,
-)
+from .models import MessageRole
 from .monitoring import Monitor
 from .prompts import (
     CODE_SYSTEM_PROMPT,
     MANAGED_AGENT_PROMPT,
     PLAN_UPDATE_FINAL_PLAN_REDACTION,
     SYSTEM_PROMPT_FACTS,
     SYSTEM_PROMPT_FACTS_UPDATE,
     SYSTEM_PROMPT_PLAN,
     SYSTEM_PROMPT_PLAN_UPDATE,
     TOOL_CALLING_SYSTEM_PROMPT,
@@ -37,62 +34,68 @@
 from .tools import (
     DEFAULT_TOOL_DESCRIPTION_TEMPLATE,
     Tool,
     get_tool_description_with_args,
 )
 from .types import AgentAudio, AgentImage, handle_agent_output_types
 from .utils import (
     AgentError,
     AgentExecutionError,
     AgentGenerationError,
-    AgentLogger,
     AgentMaxStepsError,
     AgentParsingError,
-    LogLevel,
+    console,
     parse_code_blobs,
     parse_json_tool_call,
     truncate_content,
 )
 @dataclass
 class ToolCall:
     name: str
     arguments: Any
     id: str
 class AgentStepLog:
     pass
 @dataclass
 class ActionStep(AgentStepLog):
     agent_memory: List[Dict[str, str]] | None = None
     tool_calls: List[ToolCall] | None = None
     start_time: float | None = None
     end_time: float | None = None
-    step_number: int | None = None
+    step: int | None = None
     error: AgentError | None = None
     duration: float | None = None
     llm_output: str | None = None
     observations: str | None = None
-    observations_images: List[str] | None = None
     action_output: Any = None
 @dataclass
 class PlanningStep(AgentStepLog):
     plan: str
     facts: str
 @dataclass
 class TaskStep(AgentStepLog):
     task: str
-    task_images: List[str] | None = None
 @dataclass
 class SystemPromptStep(AgentStepLog):
     system_prompt: str
-def get_tool_descriptions(tools: Dict[str, Tool], tool_description_template: str) -> str:
-    return "\n".join([get_tool_description_with_args(tool, tool_description_template) for tool in tools.values()])
-def format_prompt_with_tools(tools: Dict[str, Tool], prompt_template: str, tool_description_template: str) -> str:
+def get_tool_descriptions(
+    tools: Dict[str, Tool], tool_description_template: str
+) -> str:
+    return "\n".join(
+        [
+            get_tool_description_with_args(tool, tool_description_template)
+            for tool in tools.values()
+        ]
+    )
+def format_prompt_with_tools(
+    tools: Dict[str, Tool], prompt_template: str, tool_description_template: str
+) -> str:
     tool_descriptions = get_tool_descriptions(tools, tool_description_template)
     prompt = prompt_template.replace("{{tool_descriptions}}", tool_descriptions)
     if "{{tool_names}}" in prompt:
         prompt = prompt.replace(
             "{{tool_names}}",
             ", ".join([f"'{tool.name}'" for tool in tools.values()]),
         )
     return prompt
 def show_agents_descriptions(managed_agents: Dict):
     managed_agents_descriptions = """
@@ -108,343 +111,305 @@
     managed_agents,
     agent_descriptions_placeholder: Optional[str] = None,
 ) -> str:
     if agent_descriptions_placeholder is None:
         agent_descriptions_placeholder = "{{managed_agents_descriptions}}"
     if agent_descriptions_placeholder not in prompt_template:
         raise ValueError(
             f"Provided prompt template does not contain the managed agents descriptions placeholder '{agent_descriptions_placeholder}'"
         )
     if len(managed_agents.keys()) > 0:
-        return prompt_template.replace(agent_descriptions_placeholder, show_agents_descriptions(managed_agents))
+        return prompt_template.replace(
+            agent_descriptions_placeholder, show_agents_descriptions(managed_agents)
+        )
     else:
         return prompt_template.replace(agent_descriptions_placeholder, "")
 YELLOW_HEX = "#d4b702"
+class LogLevel(IntEnum):
+    ERROR = 0  # Only errors
+    INFO = 1  # Normal output (default)
+    DEBUG = 2  # Detailed output
+class AgentLogger:
+    def __init__(self, level: LogLevel = LogLevel.INFO):
+        self.level = level
+        self.console = Console()
+    def log(self, *args, level: LogLevel = LogLevel.INFO, **kwargs):
+        if level <= self.level:
+            console.print(*args, **kwargs)
 class MultiStepAgent:
     """
     Agent class that solves the given task step by step, using the ReAct framework:
     While the objective is not reached, the agent will perform a cycle of action (given by the LLM) and observation (obtained from the environment).
-    Args:
-        tools (`list[Tool]`): [`Tool`]s that the agent can use.
-        model (`Callable[[list[dict[str, str]]], ChatMessage]`): Model that will generate the agent's actions.
-        system_prompt (`str`, *optional*): System prompt that will be used to generate the agent's actions.
-        tool_description_template (`str`, *optional*): Template used to describe the tools in the system prompt.
-        max_steps (`int`, default `6`): Maximum number of steps the agent can take to solve the task.
-        tool_parser (`Callable`, *optional*): Function used to parse the tool calls from the LLM output.
-        add_base_tools (`bool`, default `False`): Whether to add the base tools to the agent's tools.
-        verbosity_level (`int`, default `1`): Level of verbosity of the agent's logs.
-        grammar (`dict[str, str]`, *optional*): Grammar used to parse the LLM output.
-        managed_agents (`list`, *optional*): Managed agents that the agent can call.
-        step_callbacks (`list[Callable]`, *optional*): Callbacks that will be called at each step.
-        planning_interval (`int`, *optional*): Interval at which the agent will run a planning step.
     """
     def __init__(
         self,
         tools: List[Tool],
-        model: Callable[[List[Dict[str, str]]], ChatMessage],
+        model: Callable[[List[Dict[str, str]]], str],
         system_prompt: Optional[str] = None,
         tool_description_template: Optional[str] = None,
         max_steps: int = 6,
         tool_parser: Optional[Callable] = None,
         add_base_tools: bool = False,
         verbosity_level: int = 1,
         grammar: Optional[Dict[str, str]] = None,
         managed_agents: Optional[List] = None,
         step_callbacks: Optional[List[Callable]] = None,
         planning_interval: Optional[int] = None,
     ):
         if system_prompt is None:
             system_prompt = CODE_SYSTEM_PROMPT
         if tool_parser is None:
             tool_parser = parse_json_tool_call
         self.agent_name = self.__class__.__name__
         self.model = model
         self.system_prompt_template = system_prompt
         self.tool_description_template = (
-            tool_description_template if tool_description_template else DEFAULT_TOOL_DESCRIPTION_TEMPLATE
+            tool_description_template
+            if tool_description_template
+            else DEFAULT_TOOL_DESCRIPTION_TEMPLATE
         )
         self.max_steps = max_steps
         self.tool_parser = tool_parser
         self.grammar = grammar
         self.planning_interval = planning_interval
         self.state = {}
         self.managed_agents = {}
         if managed_agents is not None:
             self.managed_agents = {agent.name: agent for agent in managed_agents}
-        for tool in tools:
-            assert isinstance(tool, Tool), f"This element is not of class Tool: {str(tool)}"
         self.tools = {tool.name: tool for tool in tools}
         if add_base_tools:
             for tool_name, tool_class in TOOL_MAPPING.items():
-                if tool_name != "python_interpreter" or self.__class__.__name__ == "ToolCallingAgent":
+                if (
+                    tool_name != "python_interpreter"
+                    or self.__class__.__name__ == "ToolCallingAgent"
+                ):
                     self.tools[tool_name] = tool_class()
         self.tools["final_answer"] = FinalAnswerTool()
         self.system_prompt = self.initialize_system_prompt()
         self.input_messages = None
         self.logs = []
         self.task = None
         self.logger = AgentLogger(level=verbosity_level)
         self.monitor = Monitor(self.model, self.logger)
         self.step_callbacks = step_callbacks if step_callbacks is not None else []
         self.step_callbacks.append(self.monitor.update_metrics)
     def initialize_system_prompt(self):
         self.system_prompt = format_prompt_with_tools(
             self.tools,
             self.system_prompt_template,
             self.tool_description_template,
         )
-        self.system_prompt = format_prompt_with_managed_agents_descriptions(self.system_prompt, self.managed_agents)
+        self.system_prompt = format_prompt_with_managed_agents_descriptions(
+            self.system_prompt, self.managed_agents
+        )
         return self.system_prompt
-    def write_inner_memory_from_logs(self, summary_mode: bool = False) -> List[Dict[str, str]]:
+    def write_inner_memory_from_logs(
+        self, summary_mode: Optional[bool] = False
+    ) -> List[Dict[str, str]]:
         """
         Reads past llm_outputs, actions, and observations or errors from the logs into a series of messages
         that can be used as input to the LLM.
-        Args:
-            summary_mode (`bool`): Whether to write a summary of the logs or the full logs.
         """
         memory = []
         for i, step_log in enumerate(self.logs):
             if isinstance(step_log, SystemPromptStep):
                 if not summary_mode:
                     thought_message = {
                         "role": MessageRole.SYSTEM,
-                        "content": [{"type": "text", "text": step_log.system_prompt.strip()}],
+                        "content": step_log.system_prompt.strip(),
                     }
                     memory.append(thought_message)
             elif isinstance(step_log, PlanningStep):
                 thought_message = {
                     "role": MessageRole.ASSISTANT,
                     "content": "[FACTS LIST]:\n" + step_log.facts.strip(),
                 }
                 memory.append(thought_message)
                 if not summary_mode:
                     thought_message = {
                         "role": MessageRole.ASSISTANT,
                         "content": "[PLAN]:\n" + step_log.plan.strip(),
                     }
                     memory.append(thought_message)
             elif isinstance(step_log, TaskStep):
                 task_message = {
                     "role": MessageRole.USER,
-                    "content": [{"type": "text", "text": f"New task:\n{step_log.task}"}],
+                    "content": "New task:\n" + step_log.task,
                 }
-                if step_log.task_images:
-                    for image in step_log.task_images:
-                        task_message["content"].append({"type": "image", "image": image})
                 memory.append(task_message)
             elif isinstance(step_log, ActionStep):
                 if step_log.llm_output is not None and not summary_mode:
                     thought_message = {
                         "role": MessageRole.ASSISTANT,
-                        "content": [{"type": "text", "text": step_log.llm_output.strip()}],
+                        "content": step_log.llm_output.strip(),
                     }
                     memory.append(thought_message)
                 if step_log.tool_calls is not None:
                     tool_call_message = {
                         "role": MessageRole.ASSISTANT,
-                        "content": [
-                            {
-                                "type": "text",
-                                "text": str(
-                                    [
-                                        {
-                                            "id": tool_call.id,
-                                            "type": "function",
-                                            "function": {
-                                                "name": tool_call.name,
-                                                "arguments": tool_call.arguments,
-                                            },
-                                        }
-                                        for tool_call in step_log.tool_calls
-                                    ]
-                                ),
-                            }
-                        ],
+                        "content": str(
+                            [
+                                {
+                                    "id": tool_call.id,
+                                    "type": "function",
+                                    "function": {
+                                        "name": tool_call.name,
+                                        "arguments": tool_call.arguments,
+                                    },
+                                }
+                                for tool_call in step_log.tool_calls
+                            ]
+                        ),
                     }
                     memory.append(tool_call_message)
-                if step_log.error is not None:
-                    error_message = {
+                if step_log.tool_calls is None and step_log.error is not None:
+                    message_content = (
+                        "Error:\n"
+                        + str(step_log.error)
+                        + "\nNow let's retry: take care not to repeat previous errors! If you have retried several times, try a completely different approach.\n"
+                    )
+                    tool_response_message = {
                         "role": MessageRole.ASSISTANT,
-                        "content": [
-                            {
-                                "type": "text",
-                                "text": (
-                                    "Error:\n"
-                                    + str(step_log.error)
-                                    + "\nNow let's retry: take care not to repeat previous errors! If you have retried several times, try a completely different approach.\n"
-                                ),
-                            }
-                        ],
+                        "content": message_content,
                     }
-                    memory.append(error_message)
-                if step_log.observations is not None:
-                    if step_log.tool_calls:
-                        tool_call_reference = f"Call id: {(step_log.tool_calls[0].id if getattr(step_log.tool_calls[0], 'id') else 'call_0')}\n"
-                    else:
-                        tool_call_reference = ""
-                    text_observations = f"Observation:\n{step_log.observations}"
+                if step_log.tool_calls is not None and (
+                    step_log.error is not None or step_log.observations is not None
+                ):
+                    if step_log.error is not None:
+                        message_content = (
+                            "Error:\n"
+                            + str(step_log.error)
+                            + "\nNow let's retry: take care not to repeat previous errors! If you have retried several times, try a completely different approach.\n"
+                        )
+                    elif step_log.observations is not None:
+                        message_content = f"Observation:\n{step_log.observations}"
                     tool_response_message = {
                         "role": MessageRole.TOOL_RESPONSE,
-                        "content": [{"type": "text", "text": tool_call_reference + text_observations}],
+                        "content": f"Call id: {(step_log.tool_calls[0].id if getattr(step_log.tool_calls[0], 'id') else 'call_0')}\n"
+                        + message_content,
                     }
                     memory.append(tool_response_message)
-                if step_log.observations_images:
-                    thought_message_image = {
-                        "role": MessageRole.USER,
-                        "content": [{"type": "text", "text": "Here are the observed images:"}]
-                        + [
-                            {
-                                "type": "image",
-                                "image": image,
-                            }
-                            for image in step_log.observations_images
-                        ],
-                    }
-                    memory.append(thought_message_image)
         return memory
     def get_succinct_logs(self):
-        return [{key: value for key, value in log.items() if key != "agent_memory"} for log in self.logs]
+        return [
+            {key: value for key, value in log.items() if key != "agent_memory"}
+            for log in self.logs
+        ]
     def extract_action(self, llm_output: str, split_token: str) -> Tuple[str, str]:
         """
         Parse action from the LLM output
         Args:
             llm_output (`str`): Output of the LLM
             split_token (`str`): Separator for the action. Should match the example in the system prompt.
         """
         try:
             split = llm_output.split(split_token)
             rationale, action = (
                 split[-2],
                 split[-1],
             )  # NOTE: using indexes starting from the end solves for when you have more than one split_token in the output
         except Exception:
             raise AgentParsingError(
-                f"No '{split_token}' token provided in your output.\nYour output:\n{llm_output}\n. Be sure to include an action, prefaced with '{split_token}'!",
-                self.logger,
+                f"No '{split_token}' token provided in your output.\nYour output:\n{llm_output}\n. Be sure to include an action, prefaced with '{split_token}'!"
             )
         return rationale.strip(), action.strip()
-    def provide_final_answer(self, task: str, images: Optional[list[str]]) -> str:
-        """
-        Provide the final answer to the task, based on the logs of the agent's interactions.
-        Args:
-            task (`str`): Task to perform.
-            images (`list[str]`, *optional*): Paths to image(s).
-        Returns:
-            `str`: Final answer to the task.
-        """
-        if images:
-            self.input_messages[0]["content"] = [
-                {
-                    "type": "text",
-                    "text": "An agent tried to answer a user query but it got stuck and failed to do so. You are tasked with providing an answer instead. Here is the agent's memory:",
-                }
-            ]
-            self.input_messages[0]["content"].append({"type": "image"})
-            self.input_messages += self.write_inner_memory_from_logs()[1:]
-            self.input_messages += [
-                {
-                    "role": MessageRole.USER,
-                    "content": [
-                        {
-                            "type": "text",
-                            "text": f"Based on the above, please provide an answer to the following user request:\n{task}",
-                        }
-                    ],
-                }
-            ]
-        else:
-            self.input_messages[0]["content"] = [
-                {
-                    "type": "text",
-                    "text": "An agent tried to answer a user query but it got stuck and failed to do so. You are tasked with providing an answer instead. Here is the agent's memory:",
-                }
-            ]
-            self.input_messages += self.write_inner_memory_from_logs()[1:]
-            self.input_messages += [
-                {
-                    "role": MessageRole.USER,
-                    "content": [
-                        {
-                            "type": "text",
-                            "text": f"Based on the above, please provide an answer to the following user request:\n{task}",
-                        }
-                    ],
-                }
-            ]
+    def provide_final_answer(self, task) -> str:
+        """
+        This method provides a final answer to the task, based on the logs of the agent's interactions.
+        """
+        self.input_messages = [
+            {
+                "role": MessageRole.SYSTEM,
+                "content": "An agent tried to answer a user query but it got stuck and failed to do so. You are tasked with providing an answer instead. Here is the agent's memory:",
+            }
+        ]
+        self.input_messages += self.write_inner_memory_from_logs()[1:]
+        self.input_messages += [
+            {
+                "role": MessageRole.USER,
+                "content": f"Based on the above, please provide an answer to the following user request:\n{task}",
+            }
+        ]
         try:
             return self.model(self.input_messages).content
         except Exception as e:
             return f"Error in generating final LLM output:\n{e}"
-    def execute_tool_call(self, tool_name: str, arguments: Union[Dict[str, str], str]) -> Any:
+    def execute_tool_call(
+        self, tool_name: str, arguments: Union[Dict[str, str], str]
+    ) -> Any:
         """
         Execute tool with the provided input and returns the result.
         This method replaces arguments with the actual values from the state if they refer to state variables.
         Args:
             tool_name (`str`): Name of the Tool to execute (should be one from self.tools).
             arguments (Dict[str, str]): Arguments passed to the Tool.
         """
         available_tools = {**self.tools, **self.managed_agents}
         if tool_name not in available_tools:
             error_msg = f"Unknown tool {tool_name}, should be instead one of {list(available_tools.keys())}."
-            raise AgentExecutionError(error_msg, self.logger)
+            raise AgentExecutionError(error_msg)
         try:
             if isinstance(arguments, str):
                 if tool_name in self.managed_agents:
                     observation = available_tools[tool_name].__call__(arguments)
                 else:
-                    observation = available_tools[tool_name].__call__(arguments, sanitize_inputs_outputs=True)
+                    observation = available_tools[tool_name].__call__(
+                        arguments, sanitize_inputs_outputs=True
+                    )
             elif isinstance(arguments, dict):
                 for key, value in arguments.items():
                     if isinstance(value, str) and value in self.state:
                         arguments[key] = self.state[value]
                 if tool_name in self.managed_agents:
                     observation = available_tools[tool_name].__call__(**arguments)
                 else:
-                    observation = available_tools[tool_name].__call__(**arguments, sanitize_inputs_outputs=True)
+                    observation = available_tools[tool_name].__call__(
+                        **arguments, sanitize_inputs_outputs=True
+                    )
             else:
                 error_msg = f"Arguments passed to tool should be a dict or string: got a {type(arguments)}."
-                raise AgentExecutionError(error_msg, self.logger)
+                raise AgentExecutionError(error_msg)
             return observation
         except Exception as e:
             if tool_name in self.tools:
-                tool_description = get_tool_description_with_args(available_tools[tool_name])
+                tool_description = get_tool_description_with_args(
+                    available_tools[tool_name]
+                )
                 error_msg = (
                     f"Error in tool call execution: {e}\nYou should only use this tool with a correct input.\n"
                     f"As a reminder, this tool's description is the following:\n{tool_description}"
                 )
-                raise AgentExecutionError(error_msg, self.logger)
+                raise AgentExecutionError(error_msg)
             elif tool_name in self.managed_agents:
                 error_msg = (
                     f"Error in calling team member: {e}\nYou should only ask this team member with a correct request.\n"
                     f"As a reminder, this team member's description is the following:\n{available_tools[tool_name]}"
                 )
-                raise AgentExecutionError(error_msg, self.logger)
+                raise AgentExecutionError(error_msg)
     def step(self, log_entry: ActionStep) -> Union[None, Any]:
         """To be implemented in children classes. Should return either None if the step is not final."""
         pass
     def run(
         self,
         task: str,
         stream: bool = False,
         reset: bool = True,
         single_step: bool = False,
-        images: Optional[List[str]] = None,
         additional_args: Optional[Dict] = None,
     ):
         """
-        Run the agent for the given task.
+        Runs the agent for the given task.
         Args:
-            task (`str`): Task to perform.
+            task (`str`): The task to perform.
             stream (`bool`): Whether to run in a streaming way.
             reset (`bool`): Whether to reset the conversation or keep it going from previous run.
             single_step (`bool`): Whether to run the agent in one-shot fashion.
-            images (`list[str]`, *optional*): Paths to image(s).
             additional_args (`dict`): Any other variables that you want to pass to the agent run, for instance images or dataframes. Give them clear names!
         Example:
         ```py
         from smolagents import CodeAgent
         agent = CodeAgent(tools=[])
         agent.run("What is the result of 2 power 3.7384?")
         ```
         """
         self.task = task
         if additional_args is not None:
@@ -466,49 +431,46 @@
         self.logger.log(
             Panel(
                 f"\n[bold]{self.task.strip()}\n",
                 title="[bold]New run",
                 subtitle=f"{type(self.model).__name__} - {(self.model.model_id if hasattr(self.model, 'model_id') else '')}",
                 border_style=YELLOW_HEX,
                 subtitle_align="left",
             ),
             level=LogLevel.INFO,
         )
-        self.logs.append(TaskStep(task=self.task, task_images=images))
+        self.logs.append(TaskStep(task=self.task))
         if single_step:
             step_start_time = time.time()
-            step_log = ActionStep(start_time=step_start_time, observations_images=images)
+            step_log = ActionStep(start_time=step_start_time)
             step_log.end_time = time.time()
             step_log.duration = step_log.end_time - step_start_time
             result = self.step(step_log)
             return result
         if stream:
-            return self._run(task=self.task, images=images)
-        return deque(self._run(task=self.task, images=images), maxlen=1)[0]
-    def _run(self, task: str, images: List[str] | None = None) -> Generator[str, None, None]:
-        """
-        Run the agent in streaming mode and returns a generator of all the steps.
-        Args:
-            task (`str`): Task to perform.
-            images (`list[str]`): Paths to image(s).
+            return self.stream_run(self.task)
+        else:
+            return self.direct_run(self.task)
+    def stream_run(self, task: str):
+        """
+        Runs the agent in streaming mode, yielding steps as they are executed: should be launched only in the `run` method.
         """
         final_answer = None
         self.step_number = 0
         while final_answer is None and self.step_number < self.max_steps:
             step_start_time = time.time()
-            step_log = ActionStep(
-                step_number=self.step_number,
-                start_time=step_start_time,
-                observations_images=images,
-            )
+            step_log = ActionStep(step=self.step_number, start_time=step_start_time)
             try:
-                if self.planning_interval is not None and self.step_number % self.planning_interval == 0:
+                if (
+                    self.planning_interval is not None
+                    and self.step_number % self.planning_interval == 0
+                ):
                     self.planning_step(
                         task,
                         is_first_step=(self.step_number == 0),
                         step=self.step_number,
                     )
                 self.logger.log(
                     Rule(
                         f"[bold]Step {self.step_number}",
                         characters="━",
                         style=YELLOW_HEX,
@@ -516,153 +478,206 @@
                     level=LogLevel.INFO,
                 )
                 final_answer = self.step(step_log)
             except AgentError as e:
                 step_log.error = e
             finally:
                 step_log.end_time = time.time()
                 step_log.duration = step_log.end_time - step_start_time
                 self.logs.append(step_log)
                 for callback in self.step_callbacks:
-                    if len(inspect.signature(callback).parameters) == 1:
-                        callback(step_log)
-                    else:
-                        callback(step_log=step_log, agent=self)
+                    callback(step_log)
                 self.step_number += 1
                 yield step_log
         if final_answer is None and self.step_number == self.max_steps:
             error_message = "Reached max steps."
-            final_step_log = ActionStep(
-                step_number=self.step_number, error=AgentMaxStepsError(error_message, self.logger)
-            )
+            final_step_log = ActionStep(error=AgentMaxStepsError(error_message))
             self.logs.append(final_step_log)
-            final_answer = self.provide_final_answer(task, images)
+            final_answer = self.provide_final_answer(task)
             self.logger.log(Text(f"Final answer: {final_answer}"), level=LogLevel.INFO)
             final_step_log.action_output = final_answer
             final_step_log.end_time = time.time()
             final_step_log.duration = step_log.end_time - step_start_time
             for callback in self.step_callbacks:
-                if len(inspect.signature(callback).parameters) == 1:
-                    callback(final_step_log)
-                else:
-                    callback(step_log=final_step_log, agent=self)
+                callback(final_step_log)
             yield final_step_log
         yield handle_agent_output_types(final_answer)
-    def planning_step(self, task, is_first_step: bool, step: int) -> None:
+    def direct_run(self, task: str):
+        """
+        Runs the agent in direct mode, returning outputs only at the end: should be launched only in the `run` method.
+        """
+        final_answer = None
+        self.step_number = 0
+        while final_answer is None and self.step_number < self.max_steps:
+            step_start_time = time.time()
+            step_log = ActionStep(step=self.step_number, start_time=step_start_time)
+            try:
+                if (
+                    self.planning_interval is not None
+                    and self.step_number % self.planning_interval == 0
+                ):
+                    self.planning_step(
+                        task,
+                        is_first_step=(self.step_number == 0),
+                        step=self.step_number,
+                    )
+                self.logger.log(
+                    Rule(
+                        f"[bold]Step {self.step_number}",
+                        characters="━",
+                        style=YELLOW_HEX,
+                    ),
+                    level=LogLevel.INFO,
+                )
+                final_answer = self.step(step_log)
+            except AgentError as e:
+                step_log.error = e
+            finally:
+                step_end_time = time.time()
+                step_log.end_time = step_end_time
+                step_log.duration = step_end_time - step_start_time
+                self.logs.append(step_log)
+                for callback in self.step_callbacks:
+                    callback(step_log)
+                self.step_number += 1
+        if final_answer is None and self.step_number == self.max_steps:
+            error_message = "Reached max steps."
+            final_step_log = ActionStep(error=AgentMaxStepsError(error_message))
+            self.logs.append(final_step_log)
+            final_answer = self.provide_final_answer(task)
+            self.logger.log(Text(f"Final answer: {final_answer}"), level=LogLevel.INFO)
+            final_step_log.action_output = final_answer
+            final_step_log.duration = 0
+            for callback in self.step_callbacks:
+                callback(final_step_log)
+        return handle_agent_output_types(final_answer)
+    def planning_step(self, task, is_first_step: bool, step: int):
         """
         Used periodically by the agent to plan the next steps to reach the objective.
         Args:
-            task (`str`): Task to perform.
+            task (`str`): The task to perform
             is_first_step (`bool`): If this step is not the first one, the plan should be an update over a previous plan.
             step (`int`): The number of the current step, used as an indication for the LLM.
         """
         if is_first_step:
             message_prompt_facts = {
                 "role": MessageRole.SYSTEM,
                 "content": SYSTEM_PROMPT_FACTS,
             }
             message_prompt_task = {
                 "role": MessageRole.USER,
                 "content": f"""Here is the task:
 ```
 {task}
 ```
 Now begin!""",
             }
-            answer_facts = self.model([message_prompt_facts, message_prompt_task]).content
+            answer_facts = self.model(
+                [message_prompt_facts, message_prompt_task]
+            ).content
             message_system_prompt_plan = {
                 "role": MessageRole.SYSTEM,
                 "content": SYSTEM_PROMPT_PLAN,
             }
             message_user_prompt_plan = {
                 "role": MessageRole.USER,
                 "content": USER_PROMPT_PLAN.format(
                     task=task,
-                    tool_descriptions=get_tool_descriptions(self.tools, self.tool_description_template),
-                    managed_agents_descriptions=(show_agents_descriptions(self.managed_agents)),
+                    tool_descriptions=get_tool_descriptions(
+                        self.tools, self.tool_description_template
+                    ),
+                    managed_agents_descriptions=(
+                        show_agents_descriptions(self.managed_agents)
+                    ),
                     answer_facts=answer_facts,
                 ),
             }
             answer_plan = self.model(
                 [message_system_prompt_plan, message_user_prompt_plan],
                 stop_sequences=["<end_plan>"],
             ).content
             final_plan_redaction = f"""Here is the plan of action that I will follow to solve the task:
 ```
 {answer_plan}
 ```"""
             final_facts_redaction = f"""Here are the facts that I know so far:
 ```
 {answer_facts}
 ```""".strip()
-            self.logs.append(PlanningStep(plan=final_plan_redaction, facts=final_facts_redaction))
+            self.logs.append(
+                PlanningStep(plan=final_plan_redaction, facts=final_facts_redaction)
+            )
             self.logger.log(
                 Rule("[bold]Initial plan", style="orange"),
                 Text(final_plan_redaction),
                 level=LogLevel.INFO,
             )
         else:  # update plan
             agent_memory = self.write_inner_memory_from_logs(
                 summary_mode=False
             )  # This will not log the plan but will log facts
             facts_update_system_prompt = {
                 "role": MessageRole.SYSTEM,
                 "content": SYSTEM_PROMPT_FACTS_UPDATE,
             }
             facts_update_message = {
                 "role": MessageRole.USER,
                 "content": USER_PROMPT_FACTS_UPDATE,
             }
-            facts_update = self.model([facts_update_system_prompt] + agent_memory + [facts_update_message]).content
+            facts_update = self.model(
+                [facts_update_system_prompt] + agent_memory + [facts_update_message]
+            ).content
             plan_update_message = {
                 "role": MessageRole.SYSTEM,
                 "content": SYSTEM_PROMPT_PLAN_UPDATE.format(task=task),
             }
             plan_update_message_user = {
                 "role": MessageRole.USER,
                 "content": USER_PROMPT_PLAN_UPDATE.format(
                     task=task,
-                    tool_descriptions=get_tool_descriptions(self.tools, self.tool_description_template),
-                    managed_agents_descriptions=(show_agents_descriptions(self.managed_agents)),
+                    tool_descriptions=get_tool_descriptions(
+                        self.tools, self.tool_description_template
+                    ),
+                    managed_agents_descriptions=(
+                        show_agents_descriptions(self.managed_agents)
+                    ),
                     facts_update=facts_update,
                     remaining_steps=(self.max_steps - step),
                 ),
             }
             plan_update = self.model(
                 [plan_update_message] + agent_memory + [plan_update_message_user],
                 stop_sequences=["<end_plan>"],
             ).content
-            final_plan_redaction = PLAN_UPDATE_FINAL_PLAN_REDACTION.format(task=task, plan_update=plan_update)
+            final_plan_redaction = PLAN_UPDATE_FINAL_PLAN_REDACTION.format(
+                task=task, plan_update=plan_update
+            )
             final_facts_redaction = f"""Here is the updated list of the facts that I know:
 ```
 {facts_update}
 ```"""
-            self.logs.append(PlanningStep(plan=final_plan_redaction, facts=final_facts_redaction))
+            self.logs.append(
+                PlanningStep(plan=final_plan_redaction, facts=final_facts_redaction)
+            )
             self.logger.log(
                 Rule("[bold]Updated plan", style="orange"),
                 Text(final_plan_redaction),
                 level=LogLevel.INFO,
             )
 class ToolCallingAgent(MultiStepAgent):
     """
     This agent uses JSON-like tool calls, using method `model.get_tool_call` to leverage the LLM engine's tool calling capabilities.
-    Args:
-        tools (`list[Tool]`): [`Tool`]s that the agent can use.
-        model (`Callable[[list[dict[str, str]]], ChatMessage]`): Model that will generate the agent's actions.
-        system_prompt (`str`, *optional*): System prompt that will be used to generate the agent's actions.
-        planning_interval (`int`, *optional*): Interval at which the agent will run a planning step.
-        **kwargs: Additional keyword arguments.
     """
     def __init__(
         self,
         tools: List[Tool],
-        model: Callable[[List[Dict[str, str]]], ChatMessage],
+        model: Callable,
         system_prompt: Optional[str] = None,
         planning_interval: Optional[int] = None,
         **kwargs,
     ):
         if system_prompt is None:
             system_prompt = TOOL_CALLING_SYSTEM_PROMPT
         super().__init__(
             tools=tools,
             model=model,
             system_prompt=system_prompt,
@@ -676,30 +691,34 @@
         """
         agent_memory = self.write_inner_memory_from_logs()
         self.input_messages = agent_memory
         log_entry.agent_memory = agent_memory.copy()
         try:
             model_message = self.model(
                 self.input_messages,
                 tools_to_call_from=list(self.tools.values()),
                 stop_sequences=["Observation:"],
             )
-            if model_message.tool_calls is None or len(model_message.tool_calls) == 0:
-                raise Exception("Model did not call any tools. Call `final_answer` tool to return a final answer.")
             tool_call = model_message.tool_calls[0]
             tool_name, tool_call_id = tool_call.function.name, tool_call.id
             tool_arguments = tool_call.function.arguments
         except Exception as e:
-            raise AgentGenerationError(f"Error in generating tool call with model:\n{e}", self.logger)
-        log_entry.tool_calls = [ToolCall(name=tool_name, arguments=tool_arguments, id=tool_call_id)]
+            raise AgentGenerationError(
+                f"Error in generating tool call with model:\n{e}"
+            )
+        log_entry.tool_calls = [
+            ToolCall(name=tool_name, arguments=tool_arguments, id=tool_call_id)
+        ]
         self.logger.log(
-            Panel(Text(f"Calling tool: '{tool_name}' with arguments: {tool_arguments}")),
+            Panel(
+                Text(f"Calling tool: '{tool_name}' with arguments: {tool_arguments}")
+            ),
             level=LogLevel.INFO,
         )
         if tool_name == "final_answer":
             if isinstance(tool_arguments, dict):
                 if "answer" in tool_arguments:
                     answer = tool_arguments["answer"]
                 else:
                     answer = tool_arguments
             else:
                 answer = tool_arguments
@@ -735,62 +754,58 @@
                 updated_information = str(observation).strip()
             self.logger.log(
                 f"Observations: {updated_information.replace('[', '|')}",  # escape potential rich-tag-like components
                 level=LogLevel.INFO,
             )
             log_entry.observations = updated_information
             return None
 class CodeAgent(MultiStepAgent):
     """
     In this agent, the tool calls will be formulated by the LLM in code format, then parsed and executed.
-    Args:
-        tools (`list[Tool]`): [`Tool`]s that the agent can use.
-        model (`Callable[[list[dict[str, str]]], ChatMessage]`): Model that will generate the agent's actions.
-        system_prompt (`str`, *optional*): System prompt that will be used to generate the agent's actions.
-        grammar (`dict[str, str]`, *optional*): Grammar used to parse the LLM output.
-        additional_authorized_imports (`list[str]`, *optional*): Additional authorized imports for the agent.
-        planning_interval (`int`, *optional*): Interval at which the agent will run a planning step.
-        use_e2b_executor (`bool`, default `False`): Whether to use the E2B executor for remote code execution.
-        max_print_outputs_length (`int`, *optional*): Maximum length of the print outputs.
-        **kwargs: Additional keyword arguments.
     """
     def __init__(
         self,
         tools: List[Tool],
-        model: Callable[[List[Dict[str, str]]], ChatMessage],
+        model: Callable,
         system_prompt: Optional[str] = None,
         grammar: Optional[Dict[str, str]] = None,
         additional_authorized_imports: Optional[List[str]] = None,
         planning_interval: Optional[int] = None,
         use_e2b_executor: bool = False,
         max_print_outputs_length: Optional[int] = None,
         **kwargs,
     ):
         if system_prompt is None:
             system_prompt = CODE_SYSTEM_PROMPT
-        self.additional_authorized_imports = additional_authorized_imports if additional_authorized_imports else []
-        self.authorized_imports = list(set(BASE_BUILTIN_MODULES) | set(self.additional_authorized_imports))
+        self.additional_authorized_imports = (
+            additional_authorized_imports if additional_authorized_imports else []
+        )
+        self.authorized_imports = list(
+            set(BASE_BUILTIN_MODULES) | set(self.additional_authorized_imports)
+        )
         if "{{authorized_imports}}" not in system_prompt:
-            raise ValueError("Tag '{{authorized_imports}}' should be provided in the prompt.")
+            raise AgentError(
+                "Tag '{{authorized_imports}}' should be provided in the prompt."
+            )
+        if "*" in self.additional_authorized_imports:
+            self.logger.log(
+                "Caution: you set an authorization for all imports, meaning your agent can decide to import any package it deems necessary. This might raise issues if the package is not installed in your environment.",
+                0,
+            )
         super().__init__(
             tools=tools,
             model=model,
             system_prompt=system_prompt,
             grammar=grammar,
             planning_interval=planning_interval,
             **kwargs,
         )
-        if "*" in self.additional_authorized_imports:
-            self.logger.log(
-                "Caution: you set an authorization for all imports, meaning your agent can decide to import any package it deems necessary. This might raise issues if the package is not installed in your environment.",
-                0,
-            )
         if use_e2b_executor and len(self.managed_agents) > 0:
             raise Exception(
                 f"You passed both {use_e2b_executor=} and some managed agents. Managed agents is not yet supported with remote code execution."
             )
         all_tools = {**self.tools, **self.managed_agents}
         if use_e2b_executor:
             self.python_executor = E2BExecutor(
                 self.additional_authorized_imports,
                 list(all_tools.values()),
                 self.logger,
@@ -798,66 +813,68 @@
         else:
             self.python_executor = LocalPythonInterpreter(
                 self.additional_authorized_imports,
                 all_tools,
                 max_print_outputs_length=max_print_outputs_length,
             )
     def initialize_system_prompt(self):
         super().initialize_system_prompt()
         self.system_prompt = self.system_prompt.replace(
             "{{authorized_imports}}",
-            (
-                "You can import from any package you want."
-                if "*" in self.authorized_imports
-                else str(self.authorized_imports)
-            ),
+            "You can import from any package you want."
+            if "*" in self.authorized_imports
+            else str(self.authorized_imports),
         )
         return self.system_prompt
     def step(self, log_entry: ActionStep) -> Union[None, Any]:
         """
         Perform one step in the ReAct framework: the agent thinks, acts, and observes the result.
         Returns None if the step is not final.
         """
         agent_memory = self.write_inner_memory_from_logs()
         self.input_messages = agent_memory.copy()
         log_entry.agent_memory = agent_memory.copy()
         try:
-            additional_args = {"grammar": self.grammar} if self.grammar is not None else {}
+            additional_args = (
+                {"grammar": self.grammar} if self.grammar is not None else {}
+            )
             llm_output = self.model(
                 self.input_messages,
                 stop_sequences=["<end_code>", "Observation:"],
                 **additional_args,
             ).content
             log_entry.llm_output = llm_output
         except Exception as e:
-            raise AgentGenerationError(f"Error in generating model output:\n{e}", self.logger) from e
+            raise AgentGenerationError(f"Error in generating model output:\n{e}")
         self.logger.log(
             Group(
                 Rule(
                     "[italic]Output message of the LLM:",
                     align="left",
                     style="orange",
                 ),
                 Syntax(
                     llm_output,
                     lexer="markdown",
                     theme="github-dark",
                     word_wrap=True,
                 ),
             ),
             level=LogLevel.DEBUG,
         )
         try:
             code_action = fix_final_answer_code(parse_code_blobs(llm_output))
         except Exception as e:
-            error_msg = f"Error in code parsing:\n{e}\nMake sure to provide correct code blobs."
-            raise AgentParsingError(error_msg, self.logger)
+            error_msg = (
+                f"Error in code parsing:\n{e}\nMake sure to provide correct code blobs."
+            )
+            raise AgentParsingError(error_msg)
         log_entry.tool_calls = [
             ToolCall(
                 name="python_interpreter",
                 arguments=code_action,
                 id=f"call_{len(self.logs)}",
             )
         ]
         self.logger.log(
             Panel(
                 Syntax(
@@ -880,78 +897,82 @@
                 self.state,
             )
             execution_outputs_console = []
             if len(execution_logs) > 0:
                 execution_outputs_console += [
                     Text("Execution logs:", style="bold"),
                     Text(execution_logs),
                 ]
             observation += "Execution logs:\n" + execution_logs
         except Exception as e:
-            error_msg = str(e)
-            if "Import of " in error_msg and " is not allowed" in error_msg:
+            if isinstance(e, SyntaxError):
+                error_msg = (
+                    f"Code execution failed on line {e.lineno} due to: {type(e).__name__}\n"
+                    f"{e.text}"
+                    f"{' ' * (e.offset or 0)}^\n"
+                    f"Error: {str(e)}"
+                )
+            else:
+                error_msg = str(e)
+            if "Import of " in str(e) and " is not allowed" in str(e):
                 self.logger.log(
                     "[bold red]Warning to user: Code execution failed due to an unauthorized import - Consider passing said import under `additional_authorized_imports` when initializing your CodeAgent.",
                     level=LogLevel.INFO,
                 )
-            raise AgentExecutionError(error_msg, self.logger)
+            raise AgentExecutionError(error_msg)
         truncated_output = truncate_content(str(output))
         observation += "Last output from code snippet:\n" + truncated_output
         log_entry.observations = observation
         execution_outputs_console += [
             Text(
                 f"{('Out - Final answer' if is_final_answer else 'Out')}: {truncated_output}",
                 style=(f"bold {YELLOW_HEX}" if is_final_answer else ""),
             ),
         ]
         self.logger.log(Group(*execution_outputs_console), level=LogLevel.INFO)
         log_entry.action_output = output
         return output if is_final_answer else None
 class ManagedAgent:
-    """
-    ManagedAgent class that manages an agent and provides additional prompting and run summaries.
-    Args:
-        agent (`object`): The agent to be managed.
-        name (`str`): The name of the managed agent.
-        description (`str`): A description of the managed agent.
-        additional_prompting (`Optional[str]`, *optional*): Additional prompting for the managed agent. Defaults to None.
-        provide_run_summary (`bool`, *optional*): Whether to provide a run summary after the agent completes its task. Defaults to False.
-        managed_agent_prompt (`Optional[str]`, *optional*): Custom prompt for the managed agent. Defaults to None.
-    """
     def __init__(
         self,
         agent,
         name,
         description,
         additional_prompting: Optional[str] = None,
         provide_run_summary: bool = False,
         managed_agent_prompt: Optional[str] = None,
     ):
         self.agent = agent
         self.name = name
         self.description = description
         self.additional_prompting = additional_prompting
         self.provide_run_summary = provide_run_summary
-        self.managed_agent_prompt = managed_agent_prompt if managed_agent_prompt else MANAGED_AGENT_PROMPT
+        self.managed_agent_prompt = (
+            managed_agent_prompt if managed_agent_prompt else MANAGED_AGENT_PROMPT
+        )
     def write_full_task(self, task):
         """Adds additional prompting for the managed agent, like 'add more detail in your answer'."""
         full_task = self.managed_agent_prompt.format(name=self.name, task=task)
         if self.additional_prompting:
-            full_task = full_task.replace("\n{additional_prompting}", self.additional_prompting).strip()
+            full_task = full_task.replace(
+                "\n{{additional_prompting}}", self.additional_prompting
+            ).strip()
         else:
-            full_task = full_task.replace("\n{additional_prompting}", "").strip()
+            full_task = full_task.replace("\n{{additional_prompting}}", "").strip()
         return full_task
     def __call__(self, request, **kwargs):
         full_task = self.write_full_task(request)
         output = self.agent.run(full_task, **kwargs)
         if self.provide_run_summary:
-            answer = f"Here is the final answer from your managed agent '{self.name}':\n"
+            answer = (
+                f"Here is the final answer from your managed agent '{self.name}':\n"
+            )
             answer += str(output)
             answer += f"\n\nFor more detail, find below a summary of this agent's work:\nSUMMARY OF WORK FROM AGENT '{self.name}':\n"
             for message in self.agent.write_inner_memory_from_logs(summary_mode=True):
                 content = message["content"]
                 answer += "\n" + truncate_content(str(content)) + "\n---"
             answer += f"\nEND OF SUMMARY OF WORK FROM AGENT '{self.name}'."
             return answer
         else:
             return output
 __all__ = [

--- a/src/smolagents/default_tools.py
+++ b/src/smolagents/default_tools.py
@@ -1,43 +1,79 @@
+import json
 import re
 from dataclasses import dataclass
-from typing import Any, Dict, Optional
+from typing import Dict, Optional
+from huggingface_hub import hf_hub_download, list_spaces
+from transformers.utils import is_offline_mode, is_torch_available
 from .local_python_executor import (
     BASE_BUILTIN_MODULES,
     BASE_PYTHON_TOOLS,
     evaluate_python_code,
 )
-from .tools import PipelineTool, Tool
+from .tools import TOOL_CONFIG_FILE, PipelineTool, Tool
 from .types import AgentAudio
+if is_torch_available():
+    from transformers.models.whisper import (
+        WhisperForConditionalGeneration,
+        WhisperProcessor,
+    )
+else:
+    WhisperForConditionalGeneration = object
+    WhisperProcessor = object
 @dataclass
 class PreTool:
     name: str
     inputs: Dict[str, str]
     output_type: type
     task: str
     description: str
     repo_id: str
+def get_remote_tools(logger, organization="huggingface-tools"):
+    if is_offline_mode():
+        logger.info("You are in offline mode, so remote tools are not available.")
+        return {}
+    spaces = list_spaces(author=organization)
+    tools = {}
+    for space_info in spaces:
+        repo_id = space_info.id
+        resolved_config_file = hf_hub_download(
+            repo_id, TOOL_CONFIG_FILE, repo_type="space"
+        )
+        with open(resolved_config_file, encoding="utf-8") as reader:
+            config = json.load(reader)
+        task = repo_id.split("/")[-1]
+        tools[config["name"]] = PreTool(
+            task=task,
+            description=config["description"],
+            repo_id=repo_id,
+            name=task,
+            inputs=config["inputs"],
+            output_type=config["output_type"],
+        )
+    return tools
 class PythonInterpreterTool(Tool):
     name = "python_interpreter"
     description = "This is a tool that evaluates python code. It can be used to perform calculations."
     inputs = {
         "code": {
             "type": "string",
             "description": "The python code to run in interpreter",
         }
     }
     output_type = "string"
     def __init__(self, *args, authorized_imports=None, **kwargs):
         if authorized_imports is None:
             self.authorized_imports = list(set(BASE_BUILTIN_MODULES))
         else:
-            self.authorized_imports = list(set(BASE_BUILTIN_MODULES) | set(authorized_imports))
+            self.authorized_imports = list(
+                set(BASE_BUILTIN_MODULES) | set(authorized_imports)
+            )
         self.inputs = {
             "code": {
                 "type": "string",
                 "description": (
                     "The code snippet to evaluate. All variables used in this snippet must be defined in this same snippet, "
                     f"else you will get an error. This code can only import the following python libraries: {authorized_imports}."
                 ),
             }
         }
         self.base_python_tools = BASE_PYTHON_TOOLS
@@ -50,167 +86,172 @@
                 code,
                 state=state,
                 static_tools=self.base_python_tools,
                 authorized_imports=self.authorized_imports,
             )[0]  # The second element is boolean is_final_answer
         )
         return f"Stdout:\n{state['print_outputs']}\nOutput: {output}"
 class FinalAnswerTool(Tool):
     name = "final_answer"
     description = "Provides a final answer to the given problem."
-    inputs = {"answer": {"type": "any", "description": "The final answer to the problem"}}
+    inputs = {
+        "answer": {"type": "any", "description": "The final answer to the problem"}
+    }
     output_type = "any"
-    def forward(self, answer: Any) -> Any:
+    def forward(self, answer):
         return answer
 class UserInputTool(Tool):
     name = "user_input"
     description = "Asks for user's input on a specific question"
-    inputs = {"question": {"type": "string", "description": "The question to ask the user"}}
+    inputs = {
+        "question": {"type": "string", "description": "The question to ask the user"}
+    }
     output_type = "string"
     def forward(self, question):
         user_input = input(f"{question} => Type your answer here:")
         return user_input
 class DuckDuckGoSearchTool(Tool):
     name = "web_search"
     description = """Performs a duckduckgo web search based on your query (think a Google search) then returns the top search results."""
-    inputs = {"query": {"type": "string", "description": "The search query to perform."}}
+    inputs = {
+        "query": {"type": "string", "description": "The search query to perform."}
+    }
     output_type = "string"
     def __init__(self, *args, max_results=10, **kwargs):
         super().__init__(*args, **kwargs)
         self.max_results = max_results
         try:
             from duckduckgo_search import DDGS
-        except ImportError as e:
+        except ImportError:
             raise ImportError(
                 "You must install package `duckduckgo_search` to run this tool: for instance run `pip install duckduckgo-search`."
-            ) from e
+            )
         self.ddgs = DDGS()
     def forward(self, query: str) -> str:
         results = self.ddgs.text(query, max_results=self.max_results)
-        if len(results) == 0:
-            raise Exception("No results found! Try a less restrictive/shorter query.")
-        postprocessed_results = [f"[{result['title']}]({result['href']})\n{result['body']}" for result in results]
+        postprocessed_results = [
+            f"[{result['title']}]({result['href']})\n{result['body']}"
+            for result in results
+        ]
         return "## Search Results\n\n" + "\n\n".join(postprocessed_results)
 class GoogleSearchTool(Tool):
     name = "web_search"
     description = """Performs a google web search for your query then returns a string of the top search results."""
     inputs = {
         "query": {"type": "string", "description": "The search query to perform."},
         "filter_year": {
             "type": "integer",
             "description": "Optionally restrict results to a certain year",
             "nullable": True,
         },
     }
     output_type = "string"
     def __init__(self):
         super().__init__(self)
         import os
         self.serpapi_key = os.getenv("SERPAPI_API_KEY")
     def forward(self, query: str, filter_year: Optional[int] = None) -> str:
         import requests
         if self.serpapi_key is None:
-            raise ValueError("Missing SerpAPI key. Make sure you have 'SERPAPI_API_KEY' in your env variables.")
+            raise ValueError(
+                "Missing SerpAPI key. Make sure you have 'SERPAPI_API_KEY' in your env variables."
+            )
         params = {
             "engine": "google",
             "q": query,
             "api_key": self.serpapi_key,
             "google_domain": "google.com",
         }
         if filter_year is not None:
-            params["tbs"] = f"cdr:1,cd_min:01/01/{filter_year},cd_max:12/31/{filter_year}"
+            params["tbs"] = (
+                f"cdr:1,cd_min:01/01/{filter_year},cd_max:12/31/{filter_year}"
+            )
         response = requests.get("https://serpapi.com/search.json", params=params)
         if response.status_code == 200:
             results = response.json()
         else:
             raise ValueError(response.json())
         if "organic_results" not in results.keys():
             if filter_year is not None:
                 raise Exception(
                     f"'organic_results' key not found for query: '{query}' with filtering on year={filter_year}. Use a less restrictive query or do not filter on year."
                 )
             else:
-                raise Exception(f"'organic_results' key not found for query: '{query}'. Use a less restrictive query.")
+                raise Exception(
+                    f"'organic_results' key not found for query: '{query}'. Use a less restrictive query."
+                )
         if len(results["organic_results"]) == 0:
-            year_filter_message = f" with filter year={filter_year}" if filter_year is not None else ""
+            year_filter_message = (
+                f" with filter year={filter_year}" if filter_year is not None else ""
+            )
             return f"No results found for '{query}'{year_filter_message}. Try with a more general query, or remove the year filter."
         web_snippets = []
         if "organic_results" in results:
             for idx, page in enumerate(results["organic_results"]):
                 date_published = ""
                 if "date" in page:
                     date_published = "\nDate published: " + page["date"]
                 source = ""
                 if "source" in page:
                     source = "\nSource: " + page["source"]
                 snippet = ""
                 if "snippet" in page:
                     snippet = "\n" + page["snippet"]
                 redacted_version = f"{idx}. [{page['title']}]({page['link']}){date_published}{source}\n{snippet}"
-                redacted_version = redacted_version.replace("Your browser can't play this video.", "")
+                redacted_version = redacted_version.replace(
+                    "Your browser can't play this video.", ""
+                )
                 web_snippets.append(redacted_version)
         return "## Search Results\n" + "\n\n".join(web_snippets)
 class VisitWebpageTool(Tool):
     name = "visit_webpage"
-    description = (
-        "Visits a webpage at the given url and reads its content as a markdown string. Use this to browse webpages."
-    )
+    description = "Visits a webpage at the given url and reads its content as a markdown string. Use this to browse webpages."
     inputs = {
         "url": {
             "type": "string",
             "description": "The url of the webpage to visit.",
         }
     }
     output_type = "string"
     def forward(self, url: str) -> str:
         try:
             import requests
             from markdownify import markdownify
             from requests.exceptions import RequestException
             from smolagents.utils import truncate_content
-        except ImportError as e:
+        except ImportError:
             raise ImportError(
                 "You must install packages `markdownify` and `requests` to run this tool: for instance run `pip install markdownify requests`."
-            ) from e
+            )
         try:
-            response = requests.get(url, timeout=20)
+            response = requests.get(url)
             response.raise_for_status()  # Raise an exception for bad status codes
             markdown_content = markdownify(response.text).strip()
             markdown_content = re.sub(r"\n{3,}", "\n\n", markdown_content)
             return truncate_content(markdown_content, 10000)
-        except requests.exceptions.Timeout:
-            return "The request timed out. Please try again later or check the URL."
         except RequestException as e:
             return f"Error fetching the webpage: {str(e)}"
         except Exception as e:
             return f"An unexpected error occurred: {str(e)}"
 class SpeechToTextTool(PipelineTool):
     default_checkpoint = "openai/whisper-large-v3-turbo"
     description = "This is a tool that transcribes an audio into text. It returns the transcribed text."
     name = "transcriber"
+    pre_processor_class = WhisperProcessor
+    model_class = WhisperForConditionalGeneration
     inputs = {
         "audio": {
             "type": "audio",
             "description": "The audio to transcribe. Can be a local path, an url, or a tensor.",
         }
     }
     output_type = "string"
-    def __new__(cls):
-        from transformers.models.whisper import (
-            WhisperForConditionalGeneration,
-            WhisperProcessor,
-        )
-        if not hasattr(cls, "pre_processor_class"):
-            cls.pre_processor_class = WhisperProcessor
-        if not hasattr(cls, "model_class"):
-            cls.model_class = WhisperForConditionalGeneration
-        return super().__new__()
     def encode(self, audio):
         audio = AgentAudio(audio).to_raw()
         return self.pre_processor(audio, return_tensors="pt")
     def forward(self, inputs):
         return self.model.generate(inputs["input_features"])
     def decode(self, outputs):
         return self.pre_processor.batch_decode(outputs, skip_special_tokens=True)[0]
 TOOL_MAPPING = {
     tool_class.name: tool_class
     for tool_class in [

--- a/src/smolagents/e2b_executor.py
+++ b/src/smolagents/e2b_executor.py
@@ -1,50 +1,46 @@
 import base64
 import pickle
 import textwrap
 from io import BytesIO
 from typing import Any, List, Tuple
+from dotenv import load_dotenv
+from e2b_code_interpreter import Sandbox
 from PIL import Image
 from .tool_validation import validate_tool_attributes
 from .tools import Tool
 from .utils import BASE_BUILTIN_MODULES, instance_to_source
-try:
-    from dotenv import load_dotenv
-    load_dotenv()
-except ModuleNotFoundError:
-    pass
+load_dotenv()
 class E2BExecutor:
     def __init__(self, additional_imports: List[str], tools: List[Tool], logger):
-        try:
-            from e2b_code_interpreter import Sandbox
-        except ModuleNotFoundError:
-            raise ModuleNotFoundError(
-                """Please install 'e2b' extra to use E2BExecutor: `pip install "smolagents[e2b]"`"""
-            )
         self.custom_tools = {}
         self.sbx = Sandbox()  # "qywp2ctmu2q7jzprcf4j")
         self.logger = logger
-        additional_imports = additional_imports + ["smolagents"]
+        additional_imports = additional_imports + ["pickle5", "smolagents"]
         if len(additional_imports) > 0:
-            execution = self.sbx.commands.run("pip install " + " ".join(additional_imports))
+            execution = self.sbx.commands.run(
+                "pip install " + " ".join(additional_imports)
+            )
             if execution.error:
                 raise Exception(f"Error installing dependencies: {execution.error}")
             else:
                 logger.log(f"Installation of {additional_imports} succeeded!", 0)
         tool_codes = []
         for tool in tools:
             validate_tool_attributes(tool.__class__, check_imports=False)
             tool_code = instance_to_source(tool, base_cls=Tool)
             tool_code = tool_code.replace("from smolagents.tools import Tool", "")
             tool_code += f"\n{tool.name} = {tool.__class__.__name__}()\n"
             tool_codes.append(tool_code)
-        tool_definition_code = "\n".join([f"import {module}" for module in BASE_BUILTIN_MODULES])
+        tool_definition_code = "\n".join(
+            [f"import {module}" for module in BASE_BUILTIN_MODULES]
+        )
         tool_definition_code += textwrap.dedent("""
         class Tool:
             def __call__(self, *args, **kwargs):
                 return self.forward(*args, **kwargs)
             def forward(self, *args, **kwargs):
                 pass # to be implemented in child class
         """)
         tool_definition_code += "\n\n".join(tool_codes)
         tool_definition_execution = self.run_code_raise_errors(tool_definition_code)
         self.logger.log(tool_definition_execution.logs)
@@ -82,21 +78,23 @@
         execution = self.run_code_raise_errors(code_action)
         execution_logs = "\n".join([str(log) for log in execution.logs.stdout])
         if not execution.results:
             return None, execution_logs
         else:
             for result in execution.results:
                 if result.is_main_result:
                     for attribute_name in ["jpeg", "png"]:
                         if getattr(result, attribute_name) is not None:
                             image_output = getattr(result, attribute_name)
-                            decoded_bytes = base64.b64decode(image_output.encode("utf-8"))
+                            decoded_bytes = base64.b64decode(
+                                image_output.encode("utf-8")
+                            )
                             return Image.open(BytesIO(decoded_bytes)), execution_logs
                     for attribute_name in [
                         "chart",
                         "data",
                         "html",
                         "javascript",
                         "json",
                         "latex",
                         "markdown",
                         "pdf",

--- a/src/smolagents/gradio_ui.py
+++ b/src/smolagents/gradio_ui.py
@@ -1,21 +1,20 @@
+import gradio as gr
+import shutil
+import os
 import mimetypes
-import os
 import re
-import shutil
 from typing import Optional
 from .agents import ActionStep, AgentStepLog, MultiStepAgent
 from .types import AgentAudio, AgentImage, AgentText, handle_agent_output_types
-from .utils import _is_package_available
-def pull_messages_from_step(step_log: AgentStepLog):
+def pull_messages_from_step(step_log: AgentStepLog, test_mode: bool = True):
     """Extract ChatMessage objects from agent steps"""
-    import gradio as gr
     if isinstance(step_log, ActionStep):
         yield gr.ChatMessage(role="assistant", content=step_log.llm_output or "")
         if step_log.tool_calls is not None:
             first_tool_call = step_log.tool_calls[0]
             used_code = first_tool_call.name == "code interpreter"
             content = first_tool_call.arguments
             if used_code:
                 content = f"```py\n{content}\n```"
             yield gr.ChatMessage(
                 role="assistant",
@@ -26,31 +25,29 @@
             yield gr.ChatMessage(role="assistant", content=step_log.observations)
         if step_log.error is not None:
             yield gr.ChatMessage(
                 role="assistant",
                 content=str(step_log.error),
                 metadata={"title": "💥 Error"},
             )
 def stream_to_gradio(
     agent,
     task: str,
+    test_mode: bool = False,
     reset_agent_memory: bool = False,
     additional_args: Optional[dict] = None,
 ):
     """Runs an agent with the given task and streams the messages from the agent as gradio ChatMessages."""
-    if not _is_package_available("gradio"):
-        raise ModuleNotFoundError(
-            "Please install 'gradio' extra to use the GradioUI: `pip install 'smolagents[audio]'`"
-        )
-    import gradio as gr
-    for step_log in agent.run(task, stream=True, reset=reset_agent_memory, additional_args=additional_args):
-        for message in pull_messages_from_step(step_log):
+    for step_log in agent.run(
+        task, stream=True, reset=reset_agent_memory, additional_args=additional_args
+    ):
+        for message in pull_messages_from_step(step_log, test_mode=test_mode):
             yield message
     final_answer = step_log  # Last log is the run's final_answer
     final_answer = handle_agent_output_types(final_answer)
     if isinstance(final_answer, AgentText):
         yield gr.ChatMessage(
             role="assistant",
             content=f"**Final answer:**\n{final_answer.to_string()}\n",
         )
     elif isinstance(final_answer, AgentImage):
         yield gr.ChatMessage(
@@ -60,100 +57,99 @@
     elif isinstance(final_answer, AgentAudio):
         yield gr.ChatMessage(
             role="assistant",
             content={"path": final_answer.to_string(), "mime_type": "audio/wav"},
         )
     else:
         yield gr.ChatMessage(role="assistant", content=str(final_answer))
 class GradioUI:
     """A one-line interface to launch your agent in Gradio"""
     def __init__(self, agent: MultiStepAgent, file_upload_folder: str | None = None):
-        if not _is_package_available("gradio"):
-            raise ModuleNotFoundError(
-                "Please install 'gradio' extra to use the GradioUI: `pip install 'smolagents[audio]'`"
-            )
         self.agent = agent
         self.file_upload_folder = file_upload_folder
         if self.file_upload_folder is not None:
             if not os.path.exists(file_upload_folder):
                 os.mkdir(file_upload_folder)
     def interact_with_agent(self, prompt, messages):
-        import gradio as gr
         messages.append(gr.ChatMessage(role="user", content=prompt))
         yield messages
         for msg in stream_to_gradio(self.agent, task=prompt, reset_agent_memory=False):
             messages.append(msg)
             yield messages
         yield messages
     def upload_file(
         self,
         file,
         file_uploads_log,
         allowed_file_types=[
             "application/pdf",
             "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
             "text/plain",
         ],
     ):
         """
         Handle file uploads, default allowed types are .pdf, .docx, and .txt
         """
-        import gradio as gr
         if file is None:
             return gr.Textbox("No file uploaded", visible=True), file_uploads_log
         try:
             mime_type, _ = mimetypes.guess_type(file.name)
         except Exception as e:
             return gr.Textbox(f"Error: {e}", visible=True), file_uploads_log
         if mime_type not in allowed_file_types:
             return gr.Textbox("File type disallowed", visible=True), file_uploads_log
         original_name = os.path.basename(file.name)
         sanitized_name = re.sub(
             r"[^\w\-.]", "_", original_name
         )  # Replace any non-alphanumeric, non-dash, or non-dot characters with underscores
         type_to_ext = {}
         for ext, t in mimetypes.types_map.items():
             if t not in type_to_ext:
                 type_to_ext[t] = ext
         sanitized_name = sanitized_name.split(".")[:-1]
         sanitized_name.append("" + type_to_ext[mime_type])
         sanitized_name = "".join(sanitized_name)
-        file_path = os.path.join(self.file_upload_folder, os.path.basename(sanitized_name))
+        file_path = os.path.join(
+            self.file_upload_folder, os.path.basename(sanitized_name)
+        )
         shutil.copy(file.name, file_path)
-        return gr.Textbox(f"File uploaded: {file_path}", visible=True), file_uploads_log + [file_path]
+        return gr.Textbox(
+            f"File uploaded: {file_path}", visible=True
+        ), file_uploads_log + [file_path]
     def log_user_message(self, text_input, file_uploads_log):
         return (
             text_input
             + (
                 f"\nYou have been provided with these files, which might be helpful or not: {file_uploads_log}"
                 if len(file_uploads_log) > 0
                 else ""
             ),
             "",
         )
     def launch(self):
-        import gradio as gr
         with gr.Blocks() as demo:
             stored_messages = gr.State([])
             file_uploads_log = gr.State([])
             chatbot = gr.Chatbot(
                 label="Agent",
                 type="messages",
                 avatar_images=(
                     None,
                     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/mascot_smol.png",
                 ),
                 resizeable=True,
             )
             if self.file_upload_folder is not None:
                 upload_file = gr.File(label="Upload a file")
-                upload_status = gr.Textbox(label="Upload Status", interactive=False, visible=False)
+                upload_status = gr.Textbox(
+                    label="Upload Status", interactive=False, visible=False
+                )
                 upload_file.change(
                     self.upload_file,
                     [upload_file, file_uploads_log],
                     [upload_status, file_uploads_log],
                 )
             text_input = gr.Textbox(lines=1, label="Chat Message")
             text_input.submit(
                 self.log_user_message,
                 [text_input, file_uploads_log],
                 [stored_messages, text_input],

--- a/src/smolagents/local_python_executor.py
+++ b/src/smolagents/local_python_executor.py
@@ -1,33 +1,33 @@
 import ast
 import builtins
 import difflib
-import inspect
 import math
 import re
 from collections.abc import Mapping
 from importlib import import_module
 from types import ModuleType
 from typing import Any, Callable, Dict, List, Optional, Tuple
 import numpy as np
 import pandas as pd
 from .utils import BASE_BUILTIN_MODULES, truncate_content
 class InterpreterError(ValueError):
     """
     An error raised when the interpreter cannot evaluate a Python expression, due to syntax error or unsupported
     operations.
     """
     pass
 ERRORS = {
     name: getattr(builtins, name)
     for name in dir(builtins)
-    if isinstance(getattr(builtins, name), type) and issubclass(getattr(builtins, name), BaseException)
+    if isinstance(getattr(builtins, name), type)
+    and issubclass(getattr(builtins, name), BaseException)
 }
 PRINT_OUTPUTS, DEFAULT_MAX_LEN_OUTPUT = "", 50000
 OPERATIONS_COUNT, MAX_OPERATIONS = 0, 10000000
 def custom_print(*args):
     return None
 BASE_PYTHON_TOOLS = {
     "print": custom_print,
     "isinstance": isinstance,
     "range": range,
     "float": float,
@@ -108,31 +108,35 @@
     variable_regex = r"(?<!\.)(?<!\w)(\bfinal_answer\b)(?!\s*\()"
     code = re.sub(variable_regex, "final_answer_variable", code)
     return code
 def evaluate_unaryop(
     expression: ast.UnaryOp,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> Any:
-    operand = evaluate_ast(expression.operand, state, static_tools, custom_tools, authorized_imports)
+    operand = evaluate_ast(
+        expression.operand, state, static_tools, custom_tools, authorized_imports
+    )
     if isinstance(expression.op, ast.USub):
         return -operand
     elif isinstance(expression.op, ast.UAdd):
         return operand
     elif isinstance(expression.op, ast.Not):
         return not operand
     elif isinstance(expression.op, ast.Invert):
         return ~operand
     else:
-        raise InterpreterError(f"Unary operation {expression.op.__class__.__name__} is not supported.")
+        raise InterpreterError(
+            f"Unary operation {expression.op.__class__.__name__} is not supported."
+        )
 def evaluate_lambda(
     lambda_expression: ast.Lambda,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> Callable:
     args = [arg.arg for arg in lambda_expression.args.args]
     def lambda_func(*values: Any) -> Any:
         new_state = state.copy()
@@ -148,44 +152,51 @@
     return lambda_func
 def evaluate_while(
     while_loop: ast.While,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> None:
     max_iterations = 1000
     iterations = 0
-    while evaluate_ast(while_loop.test, state, static_tools, custom_tools, authorized_imports):
+    while evaluate_ast(
+        while_loop.test, state, static_tools, custom_tools, authorized_imports
+    ):
         for node in while_loop.body:
             try:
-                evaluate_ast(node, state, static_tools, custom_tools, authorized_imports)
+                evaluate_ast(
+                    node, state, static_tools, custom_tools, authorized_imports
+                )
             except BreakException:
                 return None
             except ContinueException:
                 break
         iterations += 1
         if iterations > max_iterations:
-            raise InterpreterError(f"Maximum number of {max_iterations} iterations in While loop exceeded")
+            raise InterpreterError(
+                f"Maximum number of {max_iterations} iterations in While loop exceeded"
+            )
     return None
 def create_function(
     func_def: ast.FunctionDef,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> Callable:
     def new_func(*args: Any, **kwargs: Any) -> Any:
         func_state = state.copy()
         arg_names = [arg.arg for arg in func_def.args.args]
         default_values = [
-            evaluate_ast(d, state, static_tools, custom_tools, authorized_imports) for d in func_def.args.defaults
+            evaluate_ast(d, state, static_tools, custom_tools, authorized_imports)
+            for d in func_def.args.defaults
         ]
         defaults = dict(zip(arg_names[-len(default_values) :], default_values))
         for name, value in zip(arg_names, args):
             func_state[name] = value
         for name, value in kwargs.items():
             func_state[name] = value
         if func_def.args.vararg:
             vararg_name = func_def.args.vararg.arg
             func_state[vararg_name] = args
         if func_def.args.kwarg:
@@ -194,163 +205,196 @@
         for name, value in defaults.items():
             if name not in func_state:
                 func_state[name] = value
         if func_def.args.args and func_def.args.args[0].arg == "self":
             if args:
                 func_state["self"] = args[0]
                 func_state["__class__"] = args[0].__class__
         result = None
         try:
             for stmt in func_def.body:
-                result = evaluate_ast(stmt, func_state, static_tools, custom_tools, authorized_imports)
+                result = evaluate_ast(
+                    stmt, func_state, static_tools, custom_tools, authorized_imports
+                )
         except ReturnException as e:
             result = e.value
         if func_def.name == "__init__":
             return None
         return result
     return new_func
 def evaluate_function_def(
     func_def: ast.FunctionDef,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> Callable:
-    custom_tools[func_def.name] = create_function(func_def, state, static_tools, custom_tools, authorized_imports)
+    custom_tools[func_def.name] = create_function(
+        func_def, state, static_tools, custom_tools, authorized_imports
+    )
     return custom_tools[func_def.name]
 def evaluate_class_def(
     class_def: ast.ClassDef,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> type:
     class_name = class_def.name
-    bases = [evaluate_ast(base, state, static_tools, custom_tools, authorized_imports) for base in class_def.bases]
+    bases = [
+        evaluate_ast(base, state, static_tools, custom_tools, authorized_imports)
+        for base in class_def.bases
+    ]
     class_dict = {}
     for stmt in class_def.body:
         if isinstance(stmt, ast.FunctionDef):
-            class_dict[stmt.name] = evaluate_function_def(stmt, state, static_tools, custom_tools, authorized_imports)
+            class_dict[stmt.name] = evaluate_function_def(
+                stmt, state, static_tools, custom_tools, authorized_imports
+            )
         elif isinstance(stmt, ast.Assign):
             for target in stmt.targets:
                 if isinstance(target, ast.Name):
                     class_dict[target.id] = evaluate_ast(
                         stmt.value,
                         state,
                         static_tools,
                         custom_tools,
                         authorized_imports,
                     )
                 elif isinstance(target, ast.Attribute):
                     class_dict[target.attr] = evaluate_ast(
                         stmt.value,
                         state,
                         static_tools,
                         custom_tools,
                         authorized_imports,
                     )
         else:
-            raise InterpreterError(f"Unsupported statement in class body: {stmt.__class__.__name__}")
+            raise InterpreterError(
+                f"Unsupported statement in class body: {stmt.__class__.__name__}"
+            )
     new_class = type(class_name, tuple(bases), class_dict)
     state[class_name] = new_class
     return new_class
 def evaluate_augassign(
     expression: ast.AugAssign,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> Any:
     def get_current_value(target: ast.AST) -> Any:
         if isinstance(target, ast.Name):
             return state.get(target.id, 0)
         elif isinstance(target, ast.Subscript):
-            obj = evaluate_ast(target.value, state, static_tools, custom_tools, authorized_imports)
-            key = evaluate_ast(target.slice, state, static_tools, custom_tools, authorized_imports)
+            obj = evaluate_ast(
+                target.value, state, static_tools, custom_tools, authorized_imports
+            )
+            key = evaluate_ast(
+                target.slice, state, static_tools, custom_tools, authorized_imports
+            )
             return obj[key]
         elif isinstance(target, ast.Attribute):
-            obj = evaluate_ast(target.value, state, static_tools, custom_tools, authorized_imports)
+            obj = evaluate_ast(
+                target.value, state, static_tools, custom_tools, authorized_imports
+            )
             return getattr(obj, target.attr)
         elif isinstance(target, ast.Tuple):
             return tuple(get_current_value(elt) for elt in target.elts)
         elif isinstance(target, ast.List):
             return [get_current_value(elt) for elt in target.elts]
         else:
-            raise InterpreterError("AugAssign not supported for {type(target)} targets.")
+            raise InterpreterError(
+                "AugAssign not supported for {type(target)} targets."
+            )
     current_value = get_current_value(expression.target)
-    value_to_add = evaluate_ast(expression.value, state, static_tools, custom_tools, authorized_imports)
+    value_to_add = evaluate_ast(
+        expression.value, state, static_tools, custom_tools, authorized_imports
+    )
     if isinstance(expression.op, ast.Add):
         if isinstance(current_value, list):
             if not isinstance(value_to_add, list):
-                raise InterpreterError(f"Cannot add non-list value {value_to_add} to a list.")
-            current_value += value_to_add
-        else:
-            current_value += value_to_add
+                raise InterpreterError(
+                    f"Cannot add non-list value {value_to_add} to a list."
+                )
+            updated_value = current_value + value_to_add
+        else:
+            updated_value = current_value + value_to_add
     elif isinstance(expression.op, ast.Sub):
-        current_value -= value_to_add
+        updated_value = current_value - value_to_add
     elif isinstance(expression.op, ast.Mult):
-        current_value *= value_to_add
+        updated_value = current_value * value_to_add
     elif isinstance(expression.op, ast.Div):
-        current_value /= value_to_add
+        updated_value = current_value / value_to_add
     elif isinstance(expression.op, ast.Mod):
-        current_value %= value_to_add
+        updated_value = current_value % value_to_add
     elif isinstance(expression.op, ast.Pow):
-        current_value **= value_to_add
+        updated_value = current_value**value_to_add
     elif isinstance(expression.op, ast.FloorDiv):
-        current_value //= value_to_add
+        updated_value = current_value // value_to_add
     elif isinstance(expression.op, ast.BitAnd):
-        current_value &= value_to_add
+        updated_value = current_value & value_to_add
     elif isinstance(expression.op, ast.BitOr):
-        current_value |= value_to_add
+        updated_value = current_value | value_to_add
     elif isinstance(expression.op, ast.BitXor):
-        current_value ^= value_to_add
+        updated_value = current_value ^ value_to_add
     elif isinstance(expression.op, ast.LShift):
-        current_value <<= value_to_add
+        updated_value = current_value << value_to_add
     elif isinstance(expression.op, ast.RShift):
-        current_value >>= value_to_add
+        updated_value = current_value >> value_to_add
     else:
-        raise InterpreterError(f"Operation {type(expression.op).__name__} is not supported.")
+        raise InterpreterError(
+            f"Operation {type(expression.op).__name__} is not supported."
+        )
     set_value(
         expression.target,
-        current_value,
+        updated_value,
         state,
         static_tools,
         custom_tools,
         authorized_imports,
     )
-    return current_value
+    return updated_value
 def evaluate_boolop(
     node: ast.BoolOp,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> bool:
     if isinstance(node.op, ast.And):
         for value in node.values:
-            if not evaluate_ast(value, state, static_tools, custom_tools, authorized_imports):
+            if not evaluate_ast(
+                value, state, static_tools, custom_tools, authorized_imports
+            ):
                 return False
         return True
     elif isinstance(node.op, ast.Or):
         for value in node.values:
-            if evaluate_ast(value, state, static_tools, custom_tools, authorized_imports):
+            if evaluate_ast(
+                value, state, static_tools, custom_tools, authorized_imports
+            ):
                 return True
         return False
 def evaluate_binop(
     binop: ast.BinOp,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> Any:
-    left_val = evaluate_ast(binop.left, state, static_tools, custom_tools, authorized_imports)
-    right_val = evaluate_ast(binop.right, state, static_tools, custom_tools, authorized_imports)
+    left_val = evaluate_ast(
+        binop.left, state, static_tools, custom_tools, authorized_imports
+    )
+    right_val = evaluate_ast(
+        binop.right, state, static_tools, custom_tools, authorized_imports
+    )
     if isinstance(binop.op, ast.Add):
         return left_val + right_val
     elif isinstance(binop.op, ast.Sub):
         return left_val - right_val
     elif isinstance(binop.op, ast.Mult):
         return left_val * right_val
     elif isinstance(binop.op, ast.Div):
         return left_val / right_val
     elif isinstance(binop.op, ast.Mod):
         return left_val % right_val
@@ -362,122 +406,156 @@
         return left_val & right_val
     elif isinstance(binop.op, ast.BitOr):
         return left_val | right_val
     elif isinstance(binop.op, ast.BitXor):
         return left_val ^ right_val
     elif isinstance(binop.op, ast.LShift):
         return left_val << right_val
     elif isinstance(binop.op, ast.RShift):
         return left_val >> right_val
     else:
-        raise NotImplementedError(f"Binary operation {type(binop.op).__name__} is not implemented.")
+        raise NotImplementedError(
+            f"Binary operation {type(binop.op).__name__} is not implemented."
+        )
 def evaluate_assign(
     assign: ast.Assign,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> Any:
-    result = evaluate_ast(assign.value, state, static_tools, custom_tools, authorized_imports)
+    result = evaluate_ast(
+        assign.value, state, static_tools, custom_tools, authorized_imports
+    )
     if len(assign.targets) == 1:
         target = assign.targets[0]
         set_value(target, result, state, static_tools, custom_tools, authorized_imports)
     else:
         if len(assign.targets) != len(result):
-            raise InterpreterError(f"Assign failed: expected {len(result)} values but got {len(assign.targets)}.")
+            raise InterpreterError(
+                f"Assign failed: expected {len(result)} values but got {len(assign.targets)}."
+            )
         expanded_values = []
         for tgt in assign.targets:
             if isinstance(tgt, ast.Starred):
                 expanded_values.extend(result)
             else:
                 expanded_values.append(result)
         for tgt, val in zip(assign.targets, expanded_values):
             set_value(tgt, val, state, static_tools, custom_tools, authorized_imports)
     return result
 def set_value(
     target: ast.AST,
     value: Any,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> None:
     if isinstance(target, ast.Name):
         if target.id in static_tools:
-            raise InterpreterError(f"Cannot assign to name '{target.id}': doing this would erase the existing tool!")
+            raise InterpreterError(
+                f"Cannot assign to name '{target.id}': doing this would erase the existing tool!"
+            )
         state[target.id] = value
     elif isinstance(target, ast.Tuple):
         if not isinstance(value, tuple):
             if hasattr(value, "__iter__") and not isinstance(value, (str, bytes)):
                 value = tuple(value)
             else:
                 raise InterpreterError("Cannot unpack non-tuple value")
         if len(target.elts) != len(value):
             raise InterpreterError("Cannot unpack tuple of wrong size")
         for i, elem in enumerate(target.elts):
-            set_value(elem, value[i], state, static_tools, custom_tools, authorized_imports)
+            set_value(
+                elem, value[i], state, static_tools, custom_tools, authorized_imports
+            )
     elif isinstance(target, ast.Subscript):
-        obj = evaluate_ast(target.value, state, static_tools, custom_tools, authorized_imports)
-        key = evaluate_ast(target.slice, state, static_tools, custom_tools, authorized_imports)
+        obj = evaluate_ast(
+            target.value, state, static_tools, custom_tools, authorized_imports
+        )
+        key = evaluate_ast(
+            target.slice, state, static_tools, custom_tools, authorized_imports
+        )
         obj[key] = value
     elif isinstance(target, ast.Attribute):
-        obj = evaluate_ast(target.value, state, static_tools, custom_tools, authorized_imports)
+        obj = evaluate_ast(
+            target.value, state, static_tools, custom_tools, authorized_imports
+        )
         setattr(obj, target.attr, value)
 def evaluate_call(
     call: ast.Call,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> Any:
     if not (
-        isinstance(call.func, ast.Attribute) or isinstance(call.func, ast.Name) or isinstance(call.func, ast.Subscript)
+        isinstance(call.func, ast.Attribute)
+        or isinstance(call.func, ast.Name)
+        or isinstance(call.func, ast.Subscript)
     ):
         raise InterpreterError(f"This is not a correct function: {call.func}).")
     if isinstance(call.func, ast.Attribute):
-        obj = evaluate_ast(call.func.value, state, static_tools, custom_tools, authorized_imports)
+        obj = evaluate_ast(
+            call.func.value, state, static_tools, custom_tools, authorized_imports
+        )
         func_name = call.func.attr
         if not hasattr(obj, func_name):
             raise InterpreterError(f"Object {obj} has no attribute {func_name}")
         func = getattr(obj, func_name)
     elif isinstance(call.func, ast.Name):
         func_name = call.func.id
         if func_name in state:
             func = state[func_name]
         elif func_name in static_tools:
             func = static_tools[func_name]
         elif func_name in custom_tools:
             func = custom_tools[func_name]
         elif func_name in ERRORS:
             func = ERRORS[func_name]
         else:
             raise InterpreterError(
-                f"It is not permitted to evaluate other functions than the provided tools or functions defined/imported in previous code (tried to execute {call.func.id})."
+                f"It is not permitted to evaluate other functions than the provided tools or functions defined in previous code (tried to execute {call.func.id})."
             )
     elif isinstance(call.func, ast.Subscript):
-        value = evaluate_ast(call.func.value, state, static_tools, custom_tools, authorized_imports)
-        index = evaluate_ast(call.func.slice, state, static_tools, custom_tools, authorized_imports)
+        value = evaluate_ast(
+            call.func.value, state, static_tools, custom_tools, authorized_imports
+        )
+        index = evaluate_ast(
+            call.func.slice, state, static_tools, custom_tools, authorized_imports
+        )
         if isinstance(value, (list, tuple)):
             func = value[index]
         else:
-            raise InterpreterError(f"Cannot subscript object of type {type(value).__name__}")
+            raise InterpreterError(
+                f"Cannot subscript object of type {type(value).__name__}"
+            )
         if not callable(func):
             raise InterpreterError(f"This is not a correct function: {call.func}).")
         func_name = None
     args = []
     for arg in call.args:
         if isinstance(arg, ast.Starred):
-            args.extend(evaluate_ast(arg.value, state, static_tools, custom_tools, authorized_imports))
-        else:
-            args.append(evaluate_ast(arg, state, static_tools, custom_tools, authorized_imports))
+            args.extend(
+                evaluate_ast(
+                    arg.value, state, static_tools, custom_tools, authorized_imports
+                )
+            )
+        else:
+            args.append(
+                evaluate_ast(arg, state, static_tools, custom_tools, authorized_imports)
+            )
     kwargs = {
-        keyword.arg: evaluate_ast(keyword.value, state, static_tools, custom_tools, authorized_imports)
+        keyword.arg: evaluate_ast(
+            keyword.value, state, static_tools, custom_tools, authorized_imports
+        )
         for keyword in call.keywords
     }
     if func_name == "super":
         if not args:
             if "__class__" in state and "self" in state:
                 return super(state["__class__"], state["self"])
             else:
                 raise InterpreterError("super() needs at least one argument")
         cls = args[0]
         if not isinstance(cls, type):
@@ -489,69 +567,69 @@
             return super(cls, instance)
         else:
             raise InterpreterError("super() takes at most 2 arguments")
     else:
         if func_name == "print":
             output = " ".join(map(str, args))
             global PRINT_OUTPUTS
             PRINT_OUTPUTS += output + "\n"
             return None
         else:  # Assume it's a callable object
-            if (
-                (inspect.getmodule(func) == builtins)
-                and inspect.isbuiltin(func)
-                and (func not in static_tools.values())
-            ):
-                raise InterpreterError(
-                    f"Invoking a builtin function that has not been explicitly added as a tool is not allowed ({func_name})."
-                )
             return func(*args, **kwargs)
 def evaluate_subscript(
     subscript: ast.Subscript,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> Any:
-    index = evaluate_ast(subscript.slice, state, static_tools, custom_tools, authorized_imports)
-    value = evaluate_ast(subscript.value, state, static_tools, custom_tools, authorized_imports)
+    index = evaluate_ast(
+        subscript.slice, state, static_tools, custom_tools, authorized_imports
+    )
+    value = evaluate_ast(
+        subscript.value, state, static_tools, custom_tools, authorized_imports
+    )
     if isinstance(value, str) and isinstance(index, str):
-        raise InterpreterError("You're trying to subscript a string with a string index, which is impossible")
+        raise InterpreterError(
+            "You're trying to subscript a string with a string index, which is impossible"
+        )
     if isinstance(value, pd.core.indexing._LocIndexer):
         parent_object = value.obj
         return parent_object.loc[index]
     if isinstance(value, pd.core.indexing._iLocIndexer):
         parent_object = value.obj
         return parent_object.iloc[index]
     if isinstance(value, (pd.DataFrame, pd.Series, np.ndarray)):
         return value[index]
     elif isinstance(value, pd.core.groupby.generic.DataFrameGroupBy):
         return value[index]
     elif isinstance(index, slice):
         return value[index]
     elif isinstance(value, (list, tuple)):
         if not (-len(value) <= index < len(value)):
-            raise InterpreterError(f"Index {index} out of bounds for list of length {len(value)}")
+            raise InterpreterError(
+                f"Index {index} out of bounds for list of length {len(value)}"
+            )
         return value[int(index)]
     elif isinstance(value, str):
         if not (-len(value) <= index < len(value)):
-            raise InterpreterError(f"Index {index} out of bounds for string of length {len(value)}")
+            raise InterpreterError(
+                f"Index {index} out of bounds for string of length {len(value)}"
+            )
         return value[index]
     elif index in value:
         return value[index]
-    else:
-        error_message = f"Could not index {value} with '{index}'."
-        if isinstance(index, str) and isinstance(value, Mapping):
-            close_matches = difflib.get_close_matches(index, list(value.keys()))
-            if len(close_matches) > 0:
-                error_message += f" Maybe you meant one of these indexes instead: {str(close_matches)}"
-        raise InterpreterError(error_message)
+    elif isinstance(index, str) and isinstance(value, Mapping):
+        close_matches = difflib.get_close_matches(index, list(value.keys()))
+        if len(close_matches) > 0:
+            return value[close_matches[0]]
+    raise InterpreterError(f"Could not index {value} with '{index}'.")
 def evaluate_name(
     name: ast.Name,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> Any:
     if name.id in state:
         return state[name.id]
     elif name.id in static_tools:
@@ -564,23 +642,26 @@
     if len(close_matches) > 0:
         return state[close_matches[0]]
     raise InterpreterError(f"The variable `{name.id}` is not defined.")
 def evaluate_condition(
     condition: ast.Compare,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> bool:
-    left = evaluate_ast(condition.left, state, static_tools, custom_tools, authorized_imports)
+    left = evaluate_ast(
+        condition.left, state, static_tools, custom_tools, authorized_imports
+    )
     comparators = [
-        evaluate_ast(c, state, static_tools, custom_tools, authorized_imports) for c in condition.comparators
+        evaluate_ast(c, state, static_tools, custom_tools, authorized_imports)
+        for c in condition.comparators
     ]
     ops = [type(op) for op in condition.ops]
     result = True
     current_left = left
     for op, comparator in zip(ops, comparators):
         if op == ast.Eq:
             current_result = current_left == comparator
         elif op == ast.NotEq:
             current_result = current_left != comparator
         elif op == ast.Lt:
@@ -607,71 +688,83 @@
             break
     return result if isinstance(result, (bool, pd.Series)) else result.all()
 def evaluate_if(
     if_statement: ast.If,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> Any:
     result = None
-    test_result = evaluate_ast(if_statement.test, state, static_tools, custom_tools, authorized_imports)
+    test_result = evaluate_ast(
+        if_statement.test, state, static_tools, custom_tools, authorized_imports
+    )
     if test_result:
         for line in if_statement.body:
-            line_result = evaluate_ast(line, state, static_tools, custom_tools, authorized_imports)
+            line_result = evaluate_ast(
+                line, state, static_tools, custom_tools, authorized_imports
+            )
             if line_result is not None:
                 result = line_result
     else:
         for line in if_statement.orelse:
-            line_result = evaluate_ast(line, state, static_tools, custom_tools, authorized_imports)
+            line_result = evaluate_ast(
+                line, state, static_tools, custom_tools, authorized_imports
+            )
             if line_result is not None:
                 result = line_result
     return result
 def evaluate_for(
     for_loop: ast.For,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> Any:
     result = None
-    iterator = evaluate_ast(for_loop.iter, state, static_tools, custom_tools, authorized_imports)
+    iterator = evaluate_ast(
+        for_loop.iter, state, static_tools, custom_tools, authorized_imports
+    )
     for counter in iterator:
         set_value(
             for_loop.target,
             counter,
             state,
             static_tools,
             custom_tools,
             authorized_imports,
         )
         for node in for_loop.body:
             try:
-                line_result = evaluate_ast(node, state, static_tools, custom_tools, authorized_imports)
+                line_result = evaluate_ast(
+                    node, state, static_tools, custom_tools, authorized_imports
+                )
                 if line_result is not None:
                     result = line_result
             except BreakException:
                 break
             except ContinueException:
                 continue
         else:
             continue
         break
     return result
 def evaluate_listcomp(
     listcomp: ast.ListComp,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> List[Any]:
-    def inner_evaluate(generators: List[ast.comprehension], index: int, current_state: Dict[str, Any]) -> List[Any]:
+    def inner_evaluate(
+        generators: List[ast.comprehension], index: int, current_state: Dict[str, Any]
+    ) -> List[Any]:
         if index >= len(generators):
             return [
                 evaluate_ast(
                     listcomp.elt,
                     current_state,
                     static_tools,
                     custom_tools,
                     authorized_imports,
                 )
             ]
@@ -685,106 +778,126 @@
         )
         result = []
         for value in iter_value:
             new_state = current_state.copy()
             if isinstance(generator.target, ast.Tuple):
                 for idx, elem in enumerate(generator.target.elts):
                     new_state[elem.id] = value[idx]
             else:
                 new_state[generator.target.id] = value
             if all(
-                evaluate_ast(if_clause, new_state, static_tools, custom_tools, authorized_imports)
+                evaluate_ast(
+                    if_clause, new_state, static_tools, custom_tools, authorized_imports
+                )
                 for if_clause in generator.ifs
             ):
                 result.extend(inner_evaluate(generators, index + 1, new_state))
         return result
     return inner_evaluate(listcomp.generators, 0, state)
 def evaluate_try(
     try_node: ast.Try,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> None:
     try:
         for stmt in try_node.body:
             evaluate_ast(stmt, state, static_tools, custom_tools, authorized_imports)
     except Exception as e:
         matched = False
         for handler in try_node.handlers:
             if handler.type is None or isinstance(
                 e,
-                evaluate_ast(handler.type, state, static_tools, custom_tools, authorized_imports),
+                evaluate_ast(
+                    handler.type, state, static_tools, custom_tools, authorized_imports
+                ),
             ):
                 matched = True
                 if handler.name:
                     state[handler.name] = e
                 for stmt in handler.body:
-                    evaluate_ast(stmt, state, static_tools, custom_tools, authorized_imports)
+                    evaluate_ast(
+                        stmt, state, static_tools, custom_tools, authorized_imports
+                    )
                 break
         if not matched:
             raise e
     else:
         if try_node.orelse:
             for stmt in try_node.orelse:
-                evaluate_ast(stmt, state, static_tools, custom_tools, authorized_imports)
+                evaluate_ast(
+                    stmt, state, static_tools, custom_tools, authorized_imports
+                )
     finally:
         if try_node.finalbody:
             for stmt in try_node.finalbody:
-                evaluate_ast(stmt, state, static_tools, custom_tools, authorized_imports)
+                evaluate_ast(
+                    stmt, state, static_tools, custom_tools, authorized_imports
+                )
 def evaluate_raise(
     raise_node: ast.Raise,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> None:
     if raise_node.exc is not None:
-        exc = evaluate_ast(raise_node.exc, state, static_tools, custom_tools, authorized_imports)
+        exc = evaluate_ast(
+            raise_node.exc, state, static_tools, custom_tools, authorized_imports
+        )
     else:
         exc = None
     if raise_node.cause is not None:
-        cause = evaluate_ast(raise_node.cause, state, static_tools, custom_tools, authorized_imports)
+        cause = evaluate_ast(
+            raise_node.cause, state, static_tools, custom_tools, authorized_imports
+        )
     else:
         cause = None
     if exc is not None:
         if cause is not None:
             raise exc from cause
         else:
             raise exc
     else:
         raise InterpreterError("Re-raise is not supported without an active exception")
 def evaluate_assert(
     assert_node: ast.Assert,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> None:
-    test_result = evaluate_ast(assert_node.test, state, static_tools, custom_tools, authorized_imports)
+    test_result = evaluate_ast(
+        assert_node.test, state, static_tools, custom_tools, authorized_imports
+    )
     if not test_result:
         if assert_node.msg:
-            msg = evaluate_ast(assert_node.msg, state, static_tools, custom_tools, authorized_imports)
+            msg = evaluate_ast(
+                assert_node.msg, state, static_tools, custom_tools, authorized_imports
+            )
             raise AssertionError(msg)
         else:
             test_code = ast.unparse(assert_node.test)
             raise AssertionError(f"Assertion failed: {test_code}")
 def evaluate_with(
     with_node: ast.With,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> None:
     contexts = []
     for item in with_node.items:
-        context_expr = evaluate_ast(item.context_expr, state, static_tools, custom_tools, authorized_imports)
+        context_expr = evaluate_ast(
+            item.context_expr, state, static_tools, custom_tools, authorized_imports
+        )
         if item.optional_vars:
             state[item.optional_vars.id] = context_expr.__enter__()
             contexts.append(state[item.optional_vars.id])
         else:
             context_var = context_expr.__enter__()
             contexts.append(context_var)
     try:
         for stmt in with_node.body:
             evaluate_ast(stmt, state, static_tools, custom_tools, authorized_imports)
     except Exception as e:
@@ -799,25 +912,30 @@
     if not isinstance(unsafe_module, ModuleType):
         return unsafe_module
     if visited is None:
         visited = set()
     module_id = id(unsafe_module)
     if module_id in visited:
         return unsafe_module  # Return original for circular refs
     visited.add(module_id)
     safe_module = ModuleType(unsafe_module.__name__)
     for attr_name in dir(unsafe_module):
-        if any(pattern in f"{unsafe_module.__name__}.{attr_name}" for pattern in dangerous_patterns):
+        if any(
+            pattern in f"{unsafe_module.__name__}.{attr_name}"
+            for pattern in dangerous_patterns
+        ):
             continue
         attr_value = getattr(unsafe_module, attr_name)
         if isinstance(attr_value, ModuleType):
-            attr_value = get_safe_module(attr_value, dangerous_patterns, visited=visited)
+            attr_value = get_safe_module(
+                attr_value, dangerous_patterns, visited=visited
+            )
         setattr(safe_module, attr_name, attr_value)
     return safe_module
 def import_modules(expression, state, authorized_imports):
     dangerous_patterns = (
         "_os",
         "os",
         "subprocess",
         "_subprocess",
         "pty",
         "system",
@@ -833,74 +951,74 @@
         "exec",
         "multiprocessing",
     )
     def check_module_authorized(module_name):
         if "*" in authorized_imports:
             return True
         else:
             module_path = module_name.split(".")
             if any([module in dangerous_patterns for module in module_path]):
                 return False
-            module_subpaths = [".".join(module_path[:i]) for i in range(1, len(module_path) + 1)]
+            module_subpaths = [
+                ".".join(module_path[:i]) for i in range(1, len(module_path) + 1)
+            ]
             return any(subpath in authorized_imports for subpath in module_subpaths)
     if isinstance(expression, ast.Import):
         for alias in expression.names:
             if check_module_authorized(alias.name):
                 raw_module = import_module(alias.name)
-                state[alias.asname or alias.name] = get_safe_module(raw_module, dangerous_patterns)
+                state[alias.asname or alias.name] = get_safe_module(
+                    raw_module, dangerous_patterns
+                )
             else:
                 raise InterpreterError(
                     f"Import of {alias.name} is not allowed. Authorized imports are: {str(authorized_imports)}"
                 )
         return None
     elif isinstance(expression, ast.ImportFrom):
         if check_module_authorized(expression.module):
-            module = __import__(expression.module, fromlist=[alias.name for alias in expression.names])
-            if expression.names[0].name == "*":  # Handle "from module import *"
-                if hasattr(module, "__all__"):  # If module has __all__, import only those names
-                    for name in module.__all__:
-                        state[name] = getattr(module, name)
-                else:  # If no __all__, import all public names (those not starting with '_')
-                    for name in dir(module):
-                        if not name.startswith("_"):
-                            state[name] = getattr(module, name)
-            else:  # regular from imports
-                for alias in expression.names:
-                    if hasattr(module, alias.name):
-                        state[alias.asname or alias.name] = getattr(module, alias.name)
-                    else:
-                        raise InterpreterError(f"Module {expression.module} has no attribute {alias.name}")
+            raw_module = __import__(
+                expression.module, fromlist=[alias.name for alias in expression.names]
+            )
+            for alias in expression.names:
+                state[alias.asname or alias.name] = get_safe_module(
+                    getattr(raw_module, alias.name), dangerous_patterns
+                )
         else:
             raise InterpreterError(f"Import from {expression.module} is not allowed.")
         return None
 def evaluate_dictcomp(
     dictcomp: ast.DictComp,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str],
 ) -> Dict[Any, Any]:
     result = {}
     for gen in dictcomp.generators:
-        iter_value = evaluate_ast(gen.iter, state, static_tools, custom_tools, authorized_imports)
+        iter_value = evaluate_ast(
+            gen.iter, state, static_tools, custom_tools, authorized_imports
+        )
         for value in iter_value:
             new_state = state.copy()
             set_value(
                 gen.target,
                 value,
                 new_state,
                 static_tools,
                 custom_tools,
                 authorized_imports,
             )
             if all(
-                evaluate_ast(if_clause, new_state, static_tools, custom_tools, authorized_imports)
+                evaluate_ast(
+                    if_clause, new_state, static_tools, custom_tools, authorized_imports
+                )
                 for if_clause in gen.ifs
             ):
                 key = evaluate_ast(
                     dictcomp.key,
                     new_state,
                     static_tools,
                     custom_tools,
                     authorized_imports,
                 )
                 val = evaluate_ast(
@@ -915,21 +1033,21 @@
 def evaluate_ast(
     expression: ast.AST,
     state: Dict[str, Any],
     static_tools: Dict[str, Callable],
     custom_tools: Dict[str, Callable],
     authorized_imports: List[str] = BASE_BUILTIN_MODULES,
 ):
     """
     Evaluate an abstract syntax tree using the content of the variables stored in a state and only evaluating a given
     set of functions.
-    This function will recurse through the nodes of the tree provided.
+    This function will recurse trough the nodes of the tree provided.
     Args:
         expression (`ast.AST`):
             The code to evaluate, as an abstract syntax tree.
         state (`Dict[str, Any]`):
             A dictionary mapping variable names to values. The `state` is updated if need be when the evaluation
             encounters assignments.
         static_tools (`Dict[str, Callable]`):
             Functions that may be called during the evaluation. Trying to change one of these static_tools will raise an error.
         custom_tools (`Dict[str, Callable]`):
             Functions that may be called during the evaluation. These static_tools can be overwritten.
@@ -937,117 +1055,203 @@
             The list of modules that can be imported by the code. By default, only a few safe modules are allowed.
             If it contains "*", it will authorize any import. Use this at your own risk!
     """
     global OPERATIONS_COUNT
     if OPERATIONS_COUNT >= MAX_OPERATIONS:
         raise InterpreterError(
             f"Reached the max number of operations of {MAX_OPERATIONS}. Maybe there is an infinite loop somewhere in the code, or you're just asking too many calculations."
         )
     OPERATIONS_COUNT += 1
     if isinstance(expression, ast.Assign):
-        return evaluate_assign(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_assign(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.AugAssign):
-        return evaluate_augassign(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_augassign(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.Call):
-        return evaluate_call(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_call(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.Constant):
         return expression.value
     elif isinstance(expression, ast.Tuple):
         return tuple(
-            evaluate_ast(elt, state, static_tools, custom_tools, authorized_imports) for elt in expression.elts
+            evaluate_ast(elt, state, static_tools, custom_tools, authorized_imports)
+            for elt in expression.elts
         )
     elif isinstance(expression, (ast.ListComp, ast.GeneratorExp)):
-        return evaluate_listcomp(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_listcomp(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.UnaryOp):
-        return evaluate_unaryop(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_unaryop(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.Starred):
-        return evaluate_ast(expression.value, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_ast(
+            expression.value, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.BoolOp):
-        return evaluate_boolop(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_boolop(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.Break):
         raise BreakException()
     elif isinstance(expression, ast.Continue):
         raise ContinueException()
     elif isinstance(expression, ast.BinOp):
-        return evaluate_binop(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_binop(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.Compare):
-        return evaluate_condition(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_condition(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.Lambda):
-        return evaluate_lambda(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_lambda(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.FunctionDef):
-        return evaluate_function_def(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_function_def(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.Dict):
-        keys = [evaluate_ast(k, state, static_tools, custom_tools, authorized_imports) for k in expression.keys]
-        values = [evaluate_ast(v, state, static_tools, custom_tools, authorized_imports) for v in expression.values]
+        keys = [
+            evaluate_ast(k, state, static_tools, custom_tools, authorized_imports)
+            for k in expression.keys
+        ]
+        values = [
+            evaluate_ast(v, state, static_tools, custom_tools, authorized_imports)
+            for v in expression.values
+        ]
         return dict(zip(keys, values))
     elif isinstance(expression, ast.Expr):
-        return evaluate_ast(expression.value, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_ast(
+            expression.value, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.For):
-        return evaluate_for(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_for(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.FormattedValue):
-        return evaluate_ast(expression.value, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_ast(
+            expression.value, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.If):
-        return evaluate_if(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_if(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif hasattr(ast, "Index") and isinstance(expression, ast.Index):
-        return evaluate_ast(expression.value, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_ast(
+            expression.value, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.JoinedStr):
         return "".join(
-            [str(evaluate_ast(v, state, static_tools, custom_tools, authorized_imports)) for v in expression.values]
+            [
+                str(
+                    evaluate_ast(
+                        v, state, static_tools, custom_tools, authorized_imports
+                    )
+                )
+                for v in expression.values
+            ]
         )
     elif isinstance(expression, ast.List):
-        return [evaluate_ast(elt, state, static_tools, custom_tools, authorized_imports) for elt in expression.elts]
+        return [
+            evaluate_ast(elt, state, static_tools, custom_tools, authorized_imports)
+            for elt in expression.elts
+        ]
     elif isinstance(expression, ast.Name):
-        return evaluate_name(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_name(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.Subscript):
-        return evaluate_subscript(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_subscript(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.IfExp):
-        test_val = evaluate_ast(expression.test, state, static_tools, custom_tools, authorized_imports)
+        test_val = evaluate_ast(
+            expression.test, state, static_tools, custom_tools, authorized_imports
+        )
         if test_val:
-            return evaluate_ast(expression.body, state, static_tools, custom_tools, authorized_imports)
-        else:
-            return evaluate_ast(expression.orelse, state, static_tools, custom_tools, authorized_imports)
+            return evaluate_ast(
+                expression.body, state, static_tools, custom_tools, authorized_imports
+            )
+        else:
+            return evaluate_ast(
+                expression.orelse, state, static_tools, custom_tools, authorized_imports
+            )
     elif isinstance(expression, ast.Attribute):
-        value = evaluate_ast(expression.value, state, static_tools, custom_tools, authorized_imports)
+        value = evaluate_ast(
+            expression.value, state, static_tools, custom_tools, authorized_imports
+        )
         return getattr(value, expression.attr)
     elif isinstance(expression, ast.Slice):
         return slice(
-            evaluate_ast(expression.lower, state, static_tools, custom_tools, authorized_imports)
+            evaluate_ast(
+                expression.lower, state, static_tools, custom_tools, authorized_imports
+            )
             if expression.lower is not None
             else None,
-            evaluate_ast(expression.upper, state, static_tools, custom_tools, authorized_imports)
+            evaluate_ast(
+                expression.upper, state, static_tools, custom_tools, authorized_imports
+            )
             if expression.upper is not None
             else None,
-            evaluate_ast(expression.step, state, static_tools, custom_tools, authorized_imports)
+            evaluate_ast(
+                expression.step, state, static_tools, custom_tools, authorized_imports
+            )
             if expression.step is not None
             else None,
         )
     elif isinstance(expression, ast.DictComp):
-        return evaluate_dictcomp(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_dictcomp(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.While):
-        return evaluate_while(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_while(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, (ast.Import, ast.ImportFrom)):
         return import_modules(expression, state, authorized_imports)
     elif isinstance(expression, ast.ClassDef):
-        return evaluate_class_def(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_class_def(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.Try):
-        return evaluate_try(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_try(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.Raise):
-        return evaluate_raise(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_raise(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.Assert):
-        return evaluate_assert(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_assert(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.With):
-        return evaluate_with(expression, state, static_tools, custom_tools, authorized_imports)
+        return evaluate_with(
+            expression, state, static_tools, custom_tools, authorized_imports
+        )
     elif isinstance(expression, ast.Set):
-        return {evaluate_ast(elt, state, static_tools, custom_tools, authorized_imports) for elt in expression.elts}
+        return {
+            evaluate_ast(elt, state, static_tools, custom_tools, authorized_imports)
+            for elt in expression.elts
+        }
     elif isinstance(expression, ast.Return):
         raise ReturnException(
-            evaluate_ast(expression.value, state, static_tools, custom_tools, authorized_imports)
+            evaluate_ast(
+                expression.value, state, static_tools, custom_tools, authorized_imports
+            )
             if expression.value
             else None
         )
     elif isinstance(expression, ast.Pass):
         return None
     else:
         raise InterpreterError(f"{expression.__class__.__name__} is not supported.")
 class FinalAnswerException(Exception):
     def __init__(self, value):
         self.value = value
@@ -1070,77 +1274,78 @@
             The functions that may be called during the evaluation. These can also be agents in a multiagent setting.
             These tools cannot be overwritten in the code: any assignment to their name will raise an error.
         custom_tools (`Dict[str, Callable]`):
             The functions that may be called during the evaluation.
             These tools can be overwritten in the code: any assignment to their name will overwrite them.
         state (`Dict[str, Any]`):
             A dictionary mapping variable names to values. The `state` should contain the initial inputs but will be
             updated by this function to contain all variables as they are evaluated.
             The print outputs will be stored in the state under the key 'print_outputs'.
     """
-    try:
-        expression = ast.parse(code)
-    except SyntaxError as e:
-        raise InterpreterError(
-            f"Code execution failed on line {e.lineno} due to: {type(e).__name__}\n"
-            f"{e.text}"
-            f"{' ' * (e.offset or 0)}^\n"
-            f"Error: {str(e)}"
-        )
+    expression = ast.parse(code)
     if state is None:
         state = {}
-    static_tools = static_tools.copy() if static_tools is not None else {}
-    custom_tools = custom_tools if custom_tools is not None else {}
+    if static_tools is None:
+        static_tools = {}
+    if custom_tools is None:
+        custom_tools = {}
     result = None
     global PRINT_OUTPUTS
     PRINT_OUTPUTS = ""
     global OPERATIONS_COUNT
     OPERATIONS_COUNT = 0
     def final_answer(value):
         raise FinalAnswerException(value)
     static_tools["final_answer"] = final_answer
     try:
         for node in expression.body:
-            result = evaluate_ast(node, state, static_tools, custom_tools, authorized_imports)
-        state["print_outputs"] = truncate_content(PRINT_OUTPUTS, max_length=max_print_outputs_length)
+            result = evaluate_ast(
+                node, state, static_tools, custom_tools, authorized_imports
+            )
+        state["print_outputs"] = truncate_content(
+            PRINT_OUTPUTS, max_length=max_print_outputs_length
+        )
         is_final_answer = False
         return result, is_final_answer
     except FinalAnswerException as e:
-        state["print_outputs"] = truncate_content(PRINT_OUTPUTS, max_length=max_print_outputs_length)
+        state["print_outputs"] = truncate_content(
+            PRINT_OUTPUTS, max_length=max_print_outputs_length
+        )
         is_final_answer = True
         return e.value, is_final_answer
-    except Exception as e:
-        exception_type = type(e).__name__
-        error_msg = truncate_content(PRINT_OUTPUTS, max_length=max_print_outputs_length)
-        error_msg = (
-            f"Code execution failed at line '{ast.get_source_segment(code, node)}' due to: {exception_type}:{str(e)}"
-        )
-        raise InterpreterError(error_msg)
+    except InterpreterError as e:
+        msg = truncate_content(PRINT_OUTPUTS, max_length=max_print_outputs_length)
+        msg += f"Code execution failed at line '{ast.get_source_segment(code, node)}' because of the following error:\n{e}"
+        raise InterpreterError(msg)
 class LocalPythonInterpreter:
     def __init__(
         self,
         additional_authorized_imports: List[str],
         tools: Dict,
         max_print_outputs_length: Optional[int] = None,
     ):
         self.custom_tools = {}
         self.state = {}
         self.max_print_outputs_length = max_print_outputs_length
         if max_print_outputs_length is None:
             self.max_print_outputs_length = DEFAULT_MAX_LEN_OUTPUT
         self.additional_authorized_imports = additional_authorized_imports
-        self.authorized_imports = list(set(BASE_BUILTIN_MODULES) | set(self.additional_authorized_imports))
+        self.authorized_imports = list(
+            set(BASE_BUILTIN_MODULES) | set(self.additional_authorized_imports)
+        )
         self.static_tools = {
             **tools,
             **BASE_PYTHON_TOOLS.copy(),
         }
-    def __call__(self, code_action: str, additional_variables: Dict) -> Tuple[Any, str, bool]:
+    def __call__(
+        self, code_action: str, additional_variables: Dict
+    ) -> Tuple[Any, str, bool]:
         self.state.update(additional_variables)
         output, is_final_answer = evaluate_python_code(
             code_action,
             static_tools=self.static_tools,
             custom_tools=self.custom_tools,
             state=self.state,
             authorized_imports=self.authorized_imports,
             max_print_outputs_length=self.max_print_outputs_length,
         )
         logs = self.state["print_outputs"]

--- a/src/smolagents/models.py
+++ b/src/smolagents/models.py
@@ -1,30 +1,27 @@
+from dataclasses import dataclass, asdict
 import json
 import logging
 import os
 import random
 from copy import deepcopy
-from dataclasses import asdict, dataclass
 from enum import Enum
-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union
+from typing import Dict, List, Optional, Union, Any
 from huggingface_hub import InferenceClient
-from PIL import Image
 from transformers import (
-    AutoModelForImageTextToText,
-    AutoProcessor,
+    AutoModelForCausalLM,
+    AutoTokenizer,
+    StoppingCriteria,
     StoppingCriteriaList,
     is_torch_available,
 )
 from .tools import Tool
-from .utils import _is_package_available, encode_image_base64, make_image_url
-if TYPE_CHECKING:
-    from transformers import StoppingCriteriaList
 logger = logging.getLogger(__name__)
 DEFAULT_JSONAGENT_REGEX_GRAMMAR = {
     "type": "regex",
     "value": 'Thought: .+?\\nAction:\\n\\{\\n\\s{4}"action":\\s"[^"\\n]+",\\n\\s{4}"action_input":\\s"[^"\\n]+"\\n\\}\\n<end_code>',
 }
 DEFAULT_CODEAGENT_REGEX_GRAMMAR = {
     "type": "regex",
     "value": "Thought: .+?\\nCode:\\n```(?:py|python)?\\n(?:.|\\s)+?\\n```<end_code>",
 }
 def get_dict_from_nested_dataclasses(obj):
@@ -61,59 +58,39 @@
 class ChatMessage:
     role: str
     content: Optional[str] = None
     tool_calls: Optional[List[ChatMessageToolCall]] = None
     def model_dump_json(self):
         return json.dumps(get_dict_from_nested_dataclasses(self))
     @classmethod
     def from_hf_api(cls, message) -> "ChatMessage":
         tool_calls = None
         if getattr(message, "tool_calls", None) is not None:
-            tool_calls = [ChatMessageToolCall.from_hf_api(tool_call) for tool_call in message.tool_calls]
+            tool_calls = [
+                ChatMessageToolCall.from_hf_api(tool_call)
+                for tool_call in message.tool_calls
+            ]
         return cls(role=message.role, content=message.content, tool_calls=tool_calls)
-    @classmethod
-    def from_dict(cls, data: dict) -> "ChatMessage":
-        if data.get("tool_calls"):
-            tool_calls = [
-                ChatMessageToolCall(
-                    function=ChatMessageToolCallDefinition(**tc["function"]), id=tc["id"], type=tc["type"]
-                )
-                for tc in data["tool_calls"]
-            ]
-            data["tool_calls"] = tool_calls
-        return cls(**data)
-def parse_json_if_needed(arguments: Union[str, dict]) -> Union[str, dict]:
-    if isinstance(arguments, dict):
-        return arguments
-    else:
-        try:
-            return json.loads(arguments)
-        except Exception:
-            return arguments
-def parse_tool_args_if_needed(message: ChatMessage) -> ChatMessage:
-    for tool_call in message.tool_calls:
-        tool_call.function.arguments = parse_json_if_needed(tool_call.function.arguments)
-    return message
 class MessageRole(str, Enum):
     USER = "user"
     ASSISTANT = "assistant"
     SYSTEM = "system"
     TOOL_CALL = "tool-call"
     TOOL_RESPONSE = "tool-response"
     @classmethod
     def roles(cls):
         return [r.value for r in cls]
 tool_role_conversions = {
     MessageRole.TOOL_CALL: MessageRole.ASSISTANT,
     MessageRole.TOOL_RESPONSE: MessageRole.USER,
 }
-def get_tool_json_schema(tool: Tool) -> Dict:
+def get_json_schema(tool: Tool) -> Dict:
     properties = deepcopy(tool.inputs)
     required = []
     for key, value in properties.items():
         if value["type"] == "any":
             value["type"] = "string"
         if not ("nullable" in value and value["nullable"]):
             required.append(key)
     return {
         "type": "function",
         "function": {
@@ -127,154 +104,92 @@
         },
     }
 def remove_stop_sequences(content: str, stop_sequences: List[str]) -> str:
     for stop_seq in stop_sequences:
         if content[-len(stop_seq) :] == stop_seq:
             content = content[: -len(stop_seq)]
     return content
 def get_clean_message_list(
     message_list: List[Dict[str, str]],
     role_conversions: Dict[MessageRole, MessageRole] = {},
-    convert_images_to_image_urls: bool = False,
-    flatten_messages_as_text: bool = False,
 ) -> List[Dict[str, str]]:
     """
     Subsequent messages with the same role will be concatenated to a single message.
-    output_message_list is a list of messages that will be used to generate the final message that is chat template compatible with transformers LLM chat template.
     Args:
-        message_list (`list[dict[str, str]]`): List of chat messages.
-        role_conversions (`dict[MessageRole, MessageRole]`, *optional* ): Mapping to convert roles.
-        convert_images_to_image_urls (`bool`, default `False`): Whether to convert images to image URLs.
-        flatten_messages_as_text (`bool`, default `False`): Whether to flatten messages as text.
+        message_list (`List[Dict[str, str]]`): List of chat messages.
     """
-    output_message_list = []
+    final_message_list = []
     message_list = deepcopy(message_list)  # Avoid modifying the original list
     for message in message_list:
         role = message["role"]
         if role not in MessageRole.roles():
-            raise ValueError(f"Incorrect role {role}, only {MessageRole.roles()} are supported for now.")
+            raise ValueError(
+                f"Incorrect role {role}, only {MessageRole.roles()} are supported for now."
+            )
         if role in role_conversions:
             message["role"] = role_conversions[role]
-        if isinstance(message["content"], list):
-            for i, element in enumerate(message["content"]):
-                if element["type"] == "image":
-                    assert not flatten_messages_as_text, f"Cannot use images with {flatten_messages_as_text=}"
-                    if convert_images_to_image_urls:
-                        message["content"][i] = {
-                            "type": "image_url",
-                            "image_url": {"url": make_image_url(encode_image_base64(element["image"]))},
-                        }
-                    else:
-                        message["content"][i]["image"] = encode_image_base64(element["image"])
-        if len(output_message_list) > 0 and message["role"] == output_message_list[-1]["role"]:
-            assert isinstance(message["content"], list), "Error: wrong content:" + str(message["content"])
-            if flatten_messages_as_text:
-                output_message_list[-1]["content"] += message["content"][0]["text"]
-            else:
-                output_message_list[-1]["content"] += message["content"]
+        if (
+            len(final_message_list) > 0
+            and message["role"] == final_message_list[-1]["role"]
+        ):
+            final_message_list[-1]["content"] += "\n=======\n" + message["content"]
         else:
-            if flatten_messages_as_text:
-                content = message["content"][0]["text"]
-            else:
-                content = message["content"]
-            output_message_list.append({"role": message["role"], "content": content})
-    return output_message_list
+            final_message_list.append(message)
+    return final_message_list
+def parse_dictionary(possible_dictionary: str) -> Union[Dict, str]:
+    try:
+        start, end = (
+            possible_dictionary.find("{"),
+            possible_dictionary.rfind("}") + 1,
+        )
+        return json.loads(possible_dictionary[start:end])
+    except Exception:
+        return possible_dictionary
 class Model:
-    def __init__(self, **kwargs):
+    def __init__(self):
         self.last_input_token_count = None
         self.last_output_token_count = None
-        kwargs.setdefault("max_tokens", 4096)
-        self.kwargs = kwargs
-    def _prepare_completion_kwargs(
-        self,
-        messages: List[Dict[str, str]],
-        stop_sequences: Optional[List[str]] = None,
-        grammar: Optional[str] = None,
-        tools_to_call_from: Optional[List[Tool]] = None,
-        custom_role_conversions: Optional[Dict[str, str]] = None,
-        convert_images_to_image_urls: bool = False,
-        flatten_messages_as_text: bool = False,
-        **kwargs,
-    ) -> Dict:
-        """
-        Prepare parameters required for model invocation, handling parameter priorities.
-        Parameter priority from high to low:
-        1. Explicitly passed kwargs
-        2. Specific parameters (stop_sequences, grammar, etc.)
-        3. Default values in self.kwargs
-        """
-        messages = get_clean_message_list(
-            messages,
-            role_conversions=custom_role_conversions or tool_role_conversions,
-            convert_images_to_image_urls=convert_images_to_image_urls,
-            flatten_messages_as_text=flatten_messages_as_text,
-        )
-        completion_kwargs = {
-            **self.kwargs,
-            "messages": messages,
-        }
-        if stop_sequences is not None:
-            completion_kwargs["stop"] = stop_sequences
-        if grammar is not None:
-            completion_kwargs["grammar"] = grammar
-        if tools_to_call_from:
-            completion_kwargs.update(
-                {
-                    "tools": [get_tool_json_schema(tool) for tool in tools_to_call_from],
-                    "tool_choice": "required",
-                }
-            )
-        completion_kwargs.update(kwargs)
-        return completion_kwargs
     def get_token_counts(self) -> Dict[str, int]:
         return {
             "input_token_count": self.last_input_token_count,
             "output_token_count": self.last_output_token_count,
         }
     def __call__(
         self,
         messages: List[Dict[str, str]],
         stop_sequences: Optional[List[str]] = None,
         grammar: Optional[str] = None,
-        tools_to_call_from: Optional[List[Tool]] = None,
-        **kwargs,
     ) -> ChatMessage:
         """Process the input messages and return the model's response.
         Parameters:
             messages (`List[Dict[str, str]]`):
                 A list of message dictionaries to be processed. Each dictionary should have the structure `{"role": "user/system", "content": "message content"}`.
             stop_sequences (`List[str]`, *optional*):
                 A list of strings that will stop the generation if encountered in the model's output.
             grammar (`str`, *optional*):
                 The grammar or formatting structure to use in the model's response.
-            tools_to_call_from (`List[Tool]`, *optional*):
-                A list of tools that the model can use to generate responses.
-            **kwargs:
-                Additional keyword arguments to be passed to the underlying model.
         Returns:
-            `ChatMessage`: A chat message object containing the model's response.
+            `str`: The text content of the model's response.
         """
         pass  # To be implemented in child classes!
 class HfApiModel(Model):
     """A class to interact with Hugging Face's Inference API for language model interaction.
     This model allows you to communicate with Hugging Face's models using the Inference API. It can be used in both serverless mode or with a dedicated endpoint, supporting features like stop sequences and grammar customization.
     Parameters:
         model_id (`str`, *optional*, defaults to `"Qwen/Qwen2.5-Coder-32B-Instruct"`):
             The Hugging Face model ID to be used for inference. This can be a path or model identifier from the Hugging Face model hub.
         token (`str`, *optional*):
             Token used by the Hugging Face API for authentication. This token need to be authorized 'Make calls to the serverless Inference API'.
             If the model is gated (like Llama-3 models), the token also needs 'Read access to contents of all public gated repos you can access'.
             If not provided, the class will try to use environment variable 'HF_TOKEN', else use the token stored in the Hugging Face CLI configuration.
         timeout (`int`, *optional*, defaults to 120):
             Timeout for the API request, in seconds.
-        **kwargs:
-            Additional keyword arguments to pass to the Hugging Face API.
     Raises:
         ValueError:
             If the model name is not provided.
     Example:
     ```python
     >>> engine = HfApiModel(
     ...     model_id="Qwen/Qwen2.5-Coder-32B-Instruct",
     ...     token="your_hf_token_here",
     ...     max_tokens=5000,
     ... )
@@ -282,71 +197,78 @@
     >>> response = engine(messages, stop_sequences=["END"])
     >>> print(response)
     "Quantum mechanics is the branch of physics that studies..."
     ```
     """
     def __init__(
         self,
         model_id: str = "Qwen/Qwen2.5-Coder-32B-Instruct",
         token: Optional[str] = None,
         timeout: Optional[int] = 120,
+        temperature: float = 0.5,
         **kwargs,
     ):
-        super().__init__(**kwargs)
+        super().__init__()
         self.model_id = model_id
         if token is None:
             token = os.getenv("HF_TOKEN")
         self.client = InferenceClient(self.model_id, token=token, timeout=timeout)
+        self.temperature = temperature
+        self.kwargs = kwargs
     def __call__(
         self,
         messages: List[Dict[str, str]],
         stop_sequences: Optional[List[str]] = None,
         grammar: Optional[str] = None,
         tools_to_call_from: Optional[List[Tool]] = None,
-        **kwargs,
     ) -> ChatMessage:
-        completion_kwargs = self._prepare_completion_kwargs(
-            messages=messages,
-            stop_sequences=stop_sequences,
-            grammar=grammar,
-            tools_to_call_from=tools_to_call_from,
-            convert_images_to_image_urls=True,
-            **kwargs,
-        )
-        response = self.client.chat_completion(**completion_kwargs)
+        """
+        Gets an LLM output message for the given list of input messages.
+        If argument `tools_to_call_from` is passed, the model's tool calling options will be used to return a tool call.
+        """
+        messages = get_clean_message_list(
+            messages, role_conversions=tool_role_conversions
+        )
+        if tools_to_call_from:
+            response = self.client.chat.completions.create(
+                messages=messages,
+                tools=[get_json_schema(tool) for tool in tools_to_call_from],
+                tool_choice="auto",
+                stop=stop_sequences,
+                temperature=self.temperature,
+                **self.kwargs,
+            )
+        else:
+            response = self.client.chat.completions.create(
+                model=self.model_id,
+                messages=messages,
+                stop=stop_sequences,
+                temperature=self.temperature,
+                **self.kwargs,
+            )
         self.last_input_token_count = response.usage.prompt_tokens
         self.last_output_token_count = response.usage.completion_tokens
-        message = ChatMessage.from_hf_api(response.choices[0].message)
-        if tools_to_call_from is not None:
-            return parse_tool_args_if_needed(message)
-        return message
+        return ChatMessage.from_hf_api(response.choices[0].message)
 class TransformersModel(Model):
     """A class to interact with Hugging Face's Inference API for language model interaction.
     This model allows you to communicate with Hugging Face's models using the Inference API. It can be used in both serverless mode or with a dedicated endpoint, supporting features like stop sequences and grammar customization.
-    > [!TIP]
-    > You must have `transformers` and `torch` installed on your machine. Please run `pip install smolagents[transformers]` if it's not the case.
     Parameters:
         model_id (`str`, *optional*, defaults to `"Qwen/Qwen2.5-Coder-32B-Instruct"`):
             The Hugging Face model ID to be used for inference. This can be a path or model identifier from the Hugging Face model hub.
         device_map (`str`, *optional*):
             The device_map to initialize your model with.
         torch_dtype (`str`, *optional*):
             The torch_dtype to initialize your model with.
-        trust_remote_code (bool, default `False`):
+        trust_remote_code (bool):
             Some models on the Hub require running remote code: for this model, you would have to set this flag to True.
-        flatten_messages_as_text (`bool`, default `True`):
-            Whether to flatten messages as text: this must be sent to False to use VLMs (as opposed to LLMs for which this flag can be ignored).
-            Caution: this parameter is experimental and will be removed in an upcoming PR as we auto-detect VLMs.
         kwargs (dict, *optional*):
             Any additional keyword arguments that you want to use in model.generate(), for instance `max_new_tokens` or `device`.
-        **kwargs:
-            Additional keyword arguments to pass to `model.generate()`, for instance `max_new_tokens` or `device`.
     Raises:
         ValueError:
             If the model name is not provided.
     Example:
     ```python
     >>> engine = TransformersModel(
     ...     model_id="Qwen/Qwen2.5-Coder-32B-Instruct",
     ...     device="cuda",
     ...     max_new_tokens=5000,
     ... )
@@ -355,319 +277,273 @@
     >>> print(response)
     "Quantum mechanics is the branch of physics that studies..."
     ```
     """
     def __init__(
         self,
         model_id: Optional[str] = None,
         device_map: Optional[str] = None,
         torch_dtype: Optional[str] = None,
         trust_remote_code: bool = False,
-        flatten_messages_as_text: bool = True,
         **kwargs,
     ):
-        super().__init__(**kwargs)
-        if not is_torch_available() or not _is_package_available("transformers"):
-            raise ModuleNotFoundError(
-                "Please install 'transformers' extra to use 'TransformersModel': `pip install 'smolagents[transformers]'`"
-            )
+        super().__init__()
+        if not is_torch_available():
+            raise ImportError("Please install torch in order to use TransformersModel.")
         import torch
-        from transformers import AutoModelForCausalLM, AutoTokenizer
         default_model_id = "HuggingFaceTB/SmolLM2-1.7B-Instruct"
         if model_id is None:
             model_id = default_model_id
-            logger.warning(f"`model_id`not provided, using this default tokenizer for token counts: '{model_id}'")
+            logger.warning(
+                f"`model_id`not provided, using this default tokenizer for token counts: '{model_id}'"
+            )
         self.model_id = model_id
         self.kwargs = kwargs
         if device_map is None:
             device_map = "cuda" if torch.cuda.is_available() else "cpu"
         logger.info(f"Using device: {device_map}")
         try:
+            self.tokenizer = AutoTokenizer.from_pretrained(model_id)
             self.model = AutoModelForCausalLM.from_pretrained(
                 model_id,
                 device_map=device_map,
                 torch_dtype=torch_dtype,
                 trust_remote_code=trust_remote_code,
             )
-            self.tokenizer = AutoTokenizer.from_pretrained(model_id)
-        except ValueError as e:
-            if "Unrecognized configuration class" in str(e):
-                self.model = AutoModelForImageTextToText.from_pretrained(model_id, device_map=device_map)
-                self.processor = AutoProcessor.from_pretrained(model_id)
-            else:
-                raise e
         except Exception as e:
             logger.warning(
                 f"Failed to load tokenizer and model for {model_id=}: {e}. Loading default tokenizer and model instead from {default_model_id=}."
             )
             self.model_id = default_model_id
             self.tokenizer = AutoTokenizer.from_pretrained(default_model_id)
-            self.model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device_map, torch_dtype=torch_dtype)
-        self.flatten_messages_as_text = flatten_messages_as_text
-    def make_stopping_criteria(self, stop_sequences: List[str], tokenizer) -> "StoppingCriteriaList":
-        from transformers import StoppingCriteria, StoppingCriteriaList
+            self.model = AutoModelForCausalLM.from_pretrained(
+                model_id, device_map=device_map, torch_dtype=torch_dtype
+            )
+    def make_stopping_criteria(self, stop_sequences: List[str]) -> StoppingCriteriaList:
         class StopOnStrings(StoppingCriteria):
             def __init__(self, stop_strings: List[str], tokenizer):
                 self.stop_strings = stop_strings
                 self.tokenizer = tokenizer
                 self.stream = ""
             def reset(self):
                 self.stream = ""
             def __call__(self, input_ids, scores, **kwargs):
-                generated = self.tokenizer.decode(input_ids[0][-1], skip_special_tokens=True)
+                generated = self.tokenizer.decode(
+                    input_ids[0][-1], skip_special_tokens=True
+                )
                 self.stream += generated
-                if any([self.stream.endswith(stop_string) for stop_string in self.stop_strings]):
+                if any(
+                    [
+                        self.stream.endswith(stop_string)
+                        for stop_string in self.stop_strings
+                    ]
+                ):
                     return True
                 return False
-        return StoppingCriteriaList([StopOnStrings(stop_sequences, tokenizer)])
+        return StoppingCriteriaList([StopOnStrings(stop_sequences, self.tokenizer)])
     def __call__(
         self,
         messages: List[Dict[str, str]],
         stop_sequences: Optional[List[str]] = None,
         grammar: Optional[str] = None,
         tools_to_call_from: Optional[List[Tool]] = None,
-        images: Optional[List[Image.Image]] = None,
-        **kwargs,
     ) -> ChatMessage:
-        completion_kwargs = self._prepare_completion_kwargs(
-            messages=messages,
-            stop_sequences=stop_sequences,
-            grammar=grammar,
-            tools_to_call_from=tools_to_call_from,
-            flatten_messages_as_text=self.flatten_messages_as_text,
-            **kwargs,
-        )
-        messages = completion_kwargs.pop("messages")
-        stop_sequences = completion_kwargs.pop("stop", None)
-        max_new_tokens = (
-            kwargs.get("max_new_tokens")
-            or kwargs.get("max_tokens")
-            or self.kwargs.get("max_new_tokens")
-            or self.kwargs.get("max_tokens")
-        )
-        if max_new_tokens:
-            completion_kwargs["max_new_tokens"] = max_new_tokens
-        if hasattr(self, "processor"):
-            images = [Image.open(image) for image in images] if images else None
-            prompt_tensor = self.processor.apply_chat_template(
+        messages = get_clean_message_list(
+            messages, role_conversions=tool_role_conversions
+        )
+        if tools_to_call_from is not None:
+            prompt_tensor = self.tokenizer.apply_chat_template(
                 messages,
-                tools=[get_tool_json_schema(tool) for tool in tools_to_call_from] if tools_to_call_from else None,
+                tools=[get_json_schema(tool) for tool in tools_to_call_from],
                 return_tensors="pt",
-                tokenize=True,
                 return_dict=True,
-                images=images,
-                add_generation_prompt=True if tools_to_call_from else False,
+                add_generation_prompt=True,
             )
         else:
             prompt_tensor = self.tokenizer.apply_chat_template(
                 messages,
-                tools=[get_tool_json_schema(tool) for tool in tools_to_call_from] if tools_to_call_from else None,
                 return_tensors="pt",
                 return_dict=True,
-                add_generation_prompt=True if tools_to_call_from else False,
             )
         prompt_tensor = prompt_tensor.to(self.model.device)
         count_prompt_tokens = prompt_tensor["input_ids"].shape[1]
-        if stop_sequences:
-            stopping_criteria = self.make_stopping_criteria(
-                stop_sequences, tokenizer=self.processor if hasattr(self, "processor") else self.tokenizer
-            )
-        else:
-            stopping_criteria = None
         out = self.model.generate(
             **prompt_tensor,
-            stopping_criteria=stopping_criteria,
-            **completion_kwargs,
+            stopping_criteria=(
+                self.make_stopping_criteria(stop_sequences) if stop_sequences else None
+            ),
+            **self.kwargs,
         )
         generated_tokens = out[0, count_prompt_tokens:]
-        if hasattr(self, "processor"):
-            output = self.processor.decode(generated_tokens, skip_special_tokens=True)
-        else:
-            output = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
+        output = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
         self.last_input_token_count = count_prompt_tokens
         self.last_output_token_count = len(generated_tokens)
         if stop_sequences is not None:
             output = remove_stop_sequences(output, stop_sequences)
         if tools_to_call_from is None:
             return ChatMessage(role="assistant", content=output)
         else:
             if "Action:" in output:
                 output = output.split("Action:", 1)[1].strip()
             parsed_output = json.loads(output)
             tool_name = parsed_output.get("tool_name")
             tool_arguments = parsed_output.get("tool_arguments")
             return ChatMessage(
                 role="assistant",
                 content="",
                 tool_calls=[
                     ChatMessageToolCall(
                         id="".join(random.choices("0123456789", k=5)),
                         type="function",
-                        function=ChatMessageToolCallDefinition(name=tool_name, arguments=tool_arguments),
+                        function=ChatMessageToolCallDefinition(
+                            name=tool_name, arguments=tool_arguments
+                        ),
                     )
                 ],
             )
 class LiteLLMModel(Model):
     """This model connects to [LiteLLM](https://www.litellm.ai/) as a gateway to hundreds of LLMs.
     Parameters:
         model_id (`str`):
             The model identifier to use on the server (e.g. "gpt-3.5-turbo").
-        api_base (`str`, *optional*):
+        api_base (`str`):
             The base URL of the OpenAI-compatible API server.
-        api_key (`str`, *optional*):
+        api_key (`str`):
             The API key to use for authentication.
         **kwargs:
             Additional keyword arguments to pass to the OpenAI API.
     """
     def __init__(
         self,
         model_id="anthropic/claude-3-5-sonnet-20240620",
         api_base=None,
         api_key=None,
         **kwargs,
     ):
         try:
             import litellm
         except ModuleNotFoundError:
             raise ModuleNotFoundError(
                 "Please install 'litellm' extra to use LiteLLMModel: `pip install 'smolagents[litellm]'`"
             )
-        super().__init__(**kwargs)
+        super().__init__()
         self.model_id = model_id
         litellm.add_function_to_prompt = True
         self.api_base = api_base
         self.api_key = api_key
+        self.kwargs = kwargs
     def __call__(
         self,
         messages: List[Dict[str, str]],
         stop_sequences: Optional[List[str]] = None,
         grammar: Optional[str] = None,
         tools_to_call_from: Optional[List[Tool]] = None,
-        **kwargs,
     ) -> ChatMessage:
+        messages = get_clean_message_list(
+            messages, role_conversions=tool_role_conversions
+        )
         import litellm
-        completion_kwargs = self._prepare_completion_kwargs(
-            messages=messages,
-            stop_sequences=stop_sequences,
-            grammar=grammar,
-            tools_to_call_from=tools_to_call_from,
-            model=self.model_id,
-            api_base=self.api_base,
-            api_key=self.api_key,
-            convert_images_to_image_urls=True,
-            **kwargs,
-        )
-        response = litellm.completion(**completion_kwargs)
+        if tools_to_call_from:
+            response = litellm.completion(
+                model=self.model_id,
+                messages=messages,
+                tools=[get_json_schema(tool) for tool in tools_to_call_from],
+                tool_choice="required",
+                stop=stop_sequences,
+                api_base=self.api_base,
+                api_key=self.api_key,
+                **self.kwargs,
+            )
+        else:
+            response = litellm.completion(
+                model=self.model_id,
+                messages=messages,
+                stop=stop_sequences,
+                api_base=self.api_base,
+                api_key=self.api_key,
+                **self.kwargs,
+            )
         self.last_input_token_count = response.usage.prompt_tokens
         self.last_output_token_count = response.usage.completion_tokens
-        message = ChatMessage.from_dict(
-            response.choices[0].message.model_dump(include={"role", "content", "tool_calls"})
-        )
-        if tools_to_call_from is not None:
-            return parse_tool_args_if_needed(message)
-        return message
+        return response.choices[0].message
 class OpenAIServerModel(Model):
     """This model connects to an OpenAI-compatible API server.
     Parameters:
         model_id (`str`):
             The model identifier to use on the server (e.g. "gpt-3.5-turbo").
         api_base (`str`, *optional*):
             The base URL of the OpenAI-compatible API server.
         api_key (`str`, *optional*):
             The API key to use for authentication.
-        custom_role_conversions (`dict[str, str]`, *optional*):
+        custom_role_conversions (`Dict{str, str]`, *optional*):
             Custom role conversion mapping to convert message roles in others.
             Useful for specific models that do not support specific message roles like "system".
         **kwargs:
             Additional keyword arguments to pass to the OpenAI API.
     """
     def __init__(
         self,
         model_id: str,
         api_base: Optional[str] = None,
         api_key: Optional[str] = None,
         custom_role_conversions: Optional[Dict[str, str]] = None,
         **kwargs,
     ):
         try:
             import openai
         except ModuleNotFoundError:
             raise ModuleNotFoundError(
                 "Please install 'openai' extra to use OpenAIServerModel: `pip install 'smolagents[openai]'`"
             ) from None
-        super().__init__(**kwargs)
+        super().__init__()
         self.model_id = model_id
         self.client = openai.OpenAI(
             base_url=api_base,
             api_key=api_key,
         )
+        self.kwargs = kwargs
         self.custom_role_conversions = custom_role_conversions
     def __call__(
         self,
         messages: List[Dict[str, str]],
         stop_sequences: Optional[List[str]] = None,
         grammar: Optional[str] = None,
         tools_to_call_from: Optional[List[Tool]] = None,
-        **kwargs,
     ) -> ChatMessage:
-        completion_kwargs = self._prepare_completion_kwargs(
-            messages=messages,
-            stop_sequences=stop_sequences,
-            grammar=grammar,
-            tools_to_call_from=tools_to_call_from,
-            model=self.model_id,
-            custom_role_conversions=self.custom_role_conversions,
-            convert_images_to_image_urls=True,
-            **kwargs,
-        )
-        response = self.client.chat.completions.create(**completion_kwargs)
+        messages = get_clean_message_list(
+            messages,
+            role_conversions=(
+                self.custom_role_conversions
+                if self.custom_role_conversions
+                else tool_role_conversions
+            ),
+        )
+        if tools_to_call_from:
+            response = self.client.chat.completions.create(
+                model=self.model_id,
+                messages=messages,
+                tools=[get_json_schema(tool) for tool in tools_to_call_from],
+                tool_choice="auto",
+                stop=stop_sequences,
+                **self.kwargs,
+            )
+        else:
+            response = self.client.chat.completions.create(
+                model=self.model_id,
+                messages=messages,
+                stop=stop_sequences,
+                **self.kwargs,
+            )
         self.last_input_token_count = response.usage.prompt_tokens
         self.last_output_token_count = response.usage.completion_tokens
-        message = ChatMessage.from_dict(
-            response.choices[0].message.model_dump(include={"role", "content", "tool_calls"})
-        )
-        if tools_to_call_from is not None:
-            return parse_tool_args_if_needed(message)
-        return message
-class AzureOpenAIServerModel(OpenAIServerModel):
-    """This model connects to an Azure OpenAI deployment.
-    Parameters:
-        model_id (`str`):
-            The model deployment name to use when connecting (e.g. "gpt-4o-mini").
-        azure_endpoint (`str`, *optional*):
-            The Azure endpoint, including the resource, e.g. `https://example-resource.azure.openai.com/`. If not provided, it will be inferred from the `AZURE_OPENAI_ENDPOINT` environment variable.
-        api_key (`str`, *optional*):
-            The API key to use for authentication. If not provided, it will be inferred from the `AZURE_OPENAI_API_KEY` environment variable.
-        api_version (`str`, *optional*):
-            The API version to use. If not provided, it will be inferred from the `OPENAI_API_VERSION` environment variable.
-        custom_role_conversions (`dict[str, str]`, *optional*):
-            Custom role conversion mapping to convert message roles in others.
-            Useful for specific models that do not support specific message roles like "system".
-        **kwargs:
-            Additional keyword arguments to pass to the Azure OpenAI API.
-    """
-    def __init__(
-        self,
-        model_id: str,
-        azure_endpoint: Optional[str] = None,
-        api_key: Optional[str] = None,
-        api_version: Optional[str] = None,
-        custom_role_conversions: Optional[Dict[str, str]] = None,
-        **kwargs,
-    ):
-        if api_key is None:
-            api_key = os.environ.get("AZURE_OPENAI_API_KEY")
-        super().__init__(model_id=model_id, api_key=api_key, custom_role_conversions=custom_role_conversions, **kwargs)
-        import openai
-        self.client = openai.AzureOpenAI(api_key=api_key, api_version=api_version, azure_endpoint=azure_endpoint)
+        return response.choices[0].message
 __all__ = [
     "MessageRole",
     "tool_role_conversions",
     "get_clean_message_list",
     "Model",
     "TransformersModel",
     "HfApiModel",
     "LiteLLMModel",
     "OpenAIServerModel",
-    "AzureOpenAIServerModel",
     "ChatMessage",
 ]

--- a/src/smolagents/monitoring.py
+++ b/src/smolagents/monitoring.py
@@ -1,35 +1,32 @@
 from rich.text import Text
 class Monitor:
     def __init__(self, tracked_model, logger):
         self.step_durations = []
         self.tracked_model = tracked_model
         self.logger = logger
-        if getattr(self.tracked_model, "last_input_token_count", "Not found") != "Not found":
+        if (
+            getattr(self.tracked_model, "last_input_token_count", "Not found")
+            != "Not found"
+        ):
             self.total_input_token_count = 0
             self.total_output_token_count = 0
     def get_total_token_counts(self):
         return {
             "input": self.total_input_token_count,
             "output": self.total_output_token_count,
         }
     def reset(self):
         self.step_durations = []
         self.total_input_token_count = 0
         self.total_output_token_count = 0
     def update_metrics(self, step_log):
-        """Update the metrics of the monitor.
-        Args:
-            step_log ([`AgentStepLog`]): Step log to update the monitor with.
-        """
         step_duration = step_log.duration
         self.step_durations.append(step_duration)
         console_outputs = f"[Step {len(self.step_durations) - 1}: Duration {step_duration:.2f} seconds"
         if getattr(self.tracked_model, "last_input_token_count", None) is not None:
             self.total_input_token_count += self.tracked_model.last_input_token_count
             self.total_output_token_count += self.tracked_model.last_output_token_count
-            console_outputs += (
-                f"| Input tokens: {self.total_input_token_count:,} | Output tokens: {self.total_output_token_count:,}"
-            )
+            console_outputs += f"| Input tokens: {self.total_input_token_count:,} | Output tokens: {self.total_output_token_count:,}"
         console_outputs += "]"
         self.logger.log(Text(console_outputs, style="dim"), level=1)
 __all__ = ["Monitor"]

--- a/src/smolagents/prompts.py
+++ b/src/smolagents/prompts.py
@@ -79,80 +79,80 @@
 TOOL_CALLING_SYSTEM_PROMPT = """You are an expert assistant who can solve any task using  tool calls. You will be given a task to solve as best you can.
 To do so, you have been given access to the following tools: {{tool_names}}
 The tool call you write is an action: after the tool is executed, you will get the result of the tool call as an "observation".
 This Action/Observation can repeat N times, you should take several steps when needed.
 You can use the result of the previous action as input for the next action.
 The observation will always be a string: it can represent a file, like "image_1.jpg".
 Then you can use it as input for the next action. You can do it for instance as follows:
 Observation: "image_1.jpg"
 Action:
 {
-  "name": "image_transformer",
-  "arguments": {"image": "image_1.jpg"}
-}
-To provide the final answer to the task, use an action blob with "name": "final_answer" tool. It is the only way to complete the task, else you will be stuck on a loop. So your final output should look like this:
-Action:
-{
-  "name": "final_answer",
-  "arguments": {"answer": "insert your final answer here"}
+  "tool_name": "image_transformer",
+  "tool_arguments": {"image": "image_1.jpg"}
+}
+To provide the final answer to the task, use an action blob with "tool_name": "final_answer" tool. It is the only way to complete the task, else you will be stuck on a loop. So your final output should look like this:
+Action:
+{
+  "tool_name": "final_answer",
+  "tool_arguments": {"answer": "insert your final answer here"}
 }
 Here are a few examples using notional tools:
 ---
 Task: "Generate an image of the oldest person in this document."
 Action:
 {
-  "name": "document_qa",
-  "arguments": {"document": "document.pdf", "question": "Who is the oldest person mentioned?"}
+  "tool_name": "document_qa",
+  "tool_arguments": {"document": "document.pdf", "question": "Who is the oldest person mentioned?"}
 }
 Observation: "The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland."
 Action:
 {
-  "name": "image_generator",
-  "arguments": {"prompt": "A portrait of John Doe, a 55-year-old man living in Canada."}
+  "tool_name": "image_generator",
+  "tool_arguments": {"prompt": "A portrait of John Doe, a 55-year-old man living in Canada."}
 }
 Observation: "image.png"
 Action:
 {
-  "name": "final_answer",
-  "arguments": "image.png"
+  "tool_name": "final_answer",
+  "tool_arguments": "image.png"
 }
 ---
 Task: "What is the result of the following operation: 5 + 3 + 1294.678?"
 Action:
 {
-    "name": "python_interpreter",
-    "arguments": {"code": "5 + 3 + 1294.678"}
+    "tool_name": "python_interpreter",
+    "tool_arguments": {"code": "5 + 3 + 1294.678"}
 }
 Observation: 1302.678
 Action:
 {
-  "name": "final_answer",
-  "arguments": "1302.678"
+  "tool_name": "final_answer",
+  "tool_arguments": "1302.678"
 }
 ---
 Task: "Which city has the highest population , Guangzhou or Shanghai?"
 Action:
 {
-    "name": "search",
-    "arguments": "Population Guangzhou"
+    "tool_name": "search",
+    "tool_arguments": "Population Guangzhou"
 }
 Observation: ['Guangzhou has a population of 15 million inhabitants as of 2021.']
 Action:
 {
-    "name": "search",
-    "arguments": "Population Shanghai"
+    "tool_name": "search",
+    "tool_arguments": "Population Shanghai"
 }
 Observation: '26 million (2019)'
 Action:
 {
-  "name": "final_answer",
-  "arguments": "Shanghai"
+  "tool_name": "final_answer",
+  "tool_arguments": "Shanghai"
 }
 Above example were using notional tools that might not exist for you. You only have access to these tools:
 {{tool_descriptions}}
 {{managed_agents_descriptions}}
 Here are the rules you should always follow to solve your task:
 1. ALWAYS provide a tool call, else you will fail.
 2. Always use the right arguments for the tools. Never use variable names as the action arguments, use the value instead.
 3. Call a tool only when needed: do not call the search agent if you do not need information, try to solve the task yourself.
 If no tool call is needed, use final_answer tool to return your answer.
 4. Never re-do a tool call that you previously did with the exact same parameters.

--- a/src/smolagents/tool_validation.py
+++ b/src/smolagents/tool_validation.py
@@ -1,15 +1,16 @@
 import ast
 import builtins
 import inspect
+import textwrap
 from typing import Set
-from .utils import BASE_BUILTIN_MODULES, get_source
+from .utils import BASE_BUILTIN_MODULES
 _BUILTIN_NAMES = set(vars(builtins))
 class MethodChecker(ast.NodeVisitor):
     """
     Checks that a method
     - only uses defined names
     - contains no local imports (e.g. numpy is ok but local_script is not)
     """
     def __init__(self, class_attributes: Set[str], check_imports: bool = True):
         self.undefined_names = set()
         self.imports = {}
@@ -103,27 +104,29 @@
     0. __init__ takes no argument (args chosen at init are not traceable so we cannot rebuild the source code for them, make them class attributes!).
     1. About the class:
         - Class attributes should only be strings or dicts
         - Class attributes cannot be complex attributes
     2. About all class methods:
         - Imports must be from packages, not local files
         - All methods must be self-contained
     Raises all errors encountered, if no error returns None.
     """
     errors = []
-    source = get_source(cls)
+    source = textwrap.dedent(inspect.getsource(cls))
     tree = ast.parse(source)
     if not isinstance(tree.body[0], ast.ClassDef):
         raise ValueError("Source code must define a class")
     if not cls.__init__.__qualname__ == "Tool.__init__":
         sig = inspect.signature(cls.__init__)
-        non_self_params = list([arg_name for arg_name in sig.parameters.keys() if arg_name != "self"])
+        non_self_params = list(
+            [arg_name for arg_name in sig.parameters.keys() if arg_name != "self"]
+        )
         if len(non_self_params) > 0:
             errors.append(
                 f"This tool has additional args specified in __init__(self): {non_self_params}. Make sure it does not, all values should be hardcoded!"
             )
     class_node = tree.body[0]
     class ClassLevelChecker(ast.NodeVisitor):
         def __init__(self):
             self.imported_names = set()
             self.complex_attributes = set()
             self.class_attributes = set()
@@ -133,31 +136,35 @@
             self.in_method = True
             self.generic_visit(node)
             self.in_method = old_context
         def visit_Assign(self, node):
             if self.in_method:
                 return
             for target in node.targets:
                 if isinstance(target, ast.Name):
                     self.class_attributes.add(target.id)
             if not all(
-                isinstance(val, (ast.Str, ast.Num, ast.Constant, ast.Dict, ast.List, ast.Set))
+                isinstance(
+                    val, (ast.Str, ast.Num, ast.Constant, ast.Dict, ast.List, ast.Set)
+                )
                 for val in ast.walk(node.value)
             ):
                 for target in node.targets:
                     if isinstance(target, ast.Name):
                         self.complex_attributes.add(target.id)
     class_level_checker = ClassLevelChecker()
     class_level_checker.visit(class_node)
     if class_level_checker.complex_attributes:
         errors.append(
             f"Complex attributes should be defined in __init__, not as class attributes: "
             f"{', '.join(class_level_checker.complex_attributes)}"
         )
     for node in class_node.body:
         if isinstance(node, ast.FunctionDef):
-            method_checker = MethodChecker(class_level_checker.class_attributes, check_imports=check_imports)
+            method_checker = MethodChecker(
+                class_level_checker.class_attributes, check_imports=check_imports
+            )
             method_checker.visit(node)
             errors += [f"- {node.name}: {error}" for error in method_checker.errors]
     if errors:
         raise ValueError("Tool validation failed:\n" + "\n".join(errors))
     return

--- a/src/smolagents/tools.py
+++ b/src/smolagents/tools.py
@@ -3,59 +3,102 @@
 import inspect
 import json
 import logging
 import os
 import sys
 import tempfile
 import textwrap
 from contextlib import contextmanager
 from functools import lru_cache, wraps
 from pathlib import Path
-from typing import Callable, Dict, List, Optional, Union
+from typing import Callable, Dict, List, Optional, Union, get_type_hints
 from huggingface_hub import (
     create_repo,
     get_collection,
     hf_hub_download,
     metadata_update,
     upload_folder,
 )
-from huggingface_hub.utils import is_torch_available
+from huggingface_hub.utils import RepositoryNotFoundError
 from packaging import version
-from ._function_type_hints_utils import (
+from transformers.dynamic_module_utils import get_imports
+from transformers.utils import (
     TypeHintParsingException,
-    _convert_type_hints_to_json_schema,
-    get_imports,
+    cached_file,
     get_json_schema,
+    is_accelerate_available,
+    is_torch_available,
 )
+from transformers.utils.chat_template_utils import _parse_type_hint
 from .tool_validation import MethodChecker, validate_tool_attributes
-from .types import handle_agent_input_types, handle_agent_output_types
-from .utils import _is_package_available, _is_pillow_available, get_source, instance_to_source
+from .types import ImageType, handle_agent_input_types, handle_agent_output_types
+from .utils import instance_to_source
 logger = logging.getLogger(__name__)
+if is_accelerate_available():
+    from accelerate import PartialState
+    from accelerate.utils import send_to_device
+if is_torch_available():
+    from transformers import AutoProcessor
+else:
+    AutoProcessor = object
+TOOL_CONFIG_FILE = "tool_config.json"
+def get_repo_type(repo_id, repo_type=None, **hub_kwargs):
+    if repo_type is not None:
+        return repo_type
+    try:
+        hf_hub_download(repo_id, TOOL_CONFIG_FILE, repo_type="space", **hub_kwargs)
+        return "space"
+    except RepositoryNotFoundError:
+        try:
+            hf_hub_download(repo_id, TOOL_CONFIG_FILE, repo_type="model", **hub_kwargs)
+            return "model"
+        except RepositoryNotFoundError:
+            raise EnvironmentError(
+                f"`{repo_id}` does not seem to be a valid repo identifier on the Hub."
+            )
+        except Exception:
+            return "model"
+    except Exception:
+        return "space"
 def validate_after_init(cls):
     original_init = cls.__init__
     @wraps(original_init)
     def new_init(self, *args, **kwargs):
         original_init(self, *args, **kwargs)
         self.validate_arguments()
     cls.__init__ = new_init
     return cls
+def _convert_type_hints_to_json_schema(func: Callable) -> Dict:
+    type_hints = get_type_hints(func)
+    signature = inspect.signature(func)
+    properties = {}
+    for param_name, param_type in type_hints.items():
+        if param_name != "return":
+            properties[param_name] = _parse_type_hint(param_type)
+            if signature.parameters[param_name].default != inspect.Parameter.empty:
+                properties[param_name]["nullable"] = True
+    for param_name in signature.parameters.keys():
+        if signature.parameters[param_name].default != inspect.Parameter.empty:
+            if (
+                param_name not in properties
+            ):  # this can happen if the param has no type hint but a default value
+                properties[param_name] = {"nullable": True}
+    return properties
 AUTHORIZED_TYPES = [
     "string",
     "boolean",
     "integer",
     "number",
     "image",
     "audio",
-    "array",
+    "any",
     "object",
-    "any",
-    "null",
 ]
 CONVERSION_DICT = {"str": "string", "int": "integer", "float": "number"}
 class Tool:
     """
     A base class for the functions used by the agent. Subclass this and implement the `forward` method as well as the
     following class attributes:
     - **description** (`str`) -- A short description of what your tool does, the inputs it expects and the output(s) it
       will return. For instance 'This is a tool that downloads a file from a `url`. It takes the `url` as input, and
       returns the text contained in the file'.
     - **name** (`str`) -- A performative name that will be used for your tool in the prompt to the agent. For instance
@@ -88,47 +131,44 @@
         }
         for attr, expected_type in required_attributes.items():
             attr_value = getattr(self, attr, None)
             if attr_value is None:
                 raise TypeError(f"You must set an attribute {attr}.")
             if not isinstance(attr_value, expected_type):
                 raise TypeError(
                     f"Attribute {attr} should have type {expected_type.__name__}, got {type(attr_value)} instead."
                 )
         for input_name, input_content in self.inputs.items():
-            assert isinstance(input_content, dict), f"Input '{input_name}' should be a dictionary."
+            assert isinstance(input_content, dict), (
+                f"Input '{input_name}' should be a dictionary."
+            )
             assert "type" in input_content and "description" in input_content, (
                 f"Input '{input_name}' should have keys 'type' and 'description', has only {list(input_content.keys())}."
             )
             if input_content["type"] not in AUTHORIZED_TYPES:
                 raise Exception(
                     f"Input '{input_name}': type '{input_content['type']}' is not an authorized value, should be one of {AUTHORIZED_TYPES}."
                 )
         assert getattr(self, "output_type", None) in AUTHORIZED_TYPES
         if not (
             hasattr(self, "skip_forward_signature_validation")
             and getattr(self, "skip_forward_signature_validation") is True
         ):
             signature = inspect.signature(self.forward)
             if not set(signature.parameters.keys()) == set(self.inputs.keys()):
                 raise Exception(
                     "Tool's 'forward' method should take 'self' as its first argument, then its next arguments should match the keys of tool attribute 'inputs'."
                 )
-            json_schema = _convert_type_hints_to_json_schema(self.forward, error_on_missing_type_hints=False)[
-                "properties"
-            ]  # This function will not raise an error on missing docstrings, contrary to get_json_schema
+            json_schema = _convert_type_hints_to_json_schema(self.forward)
             for key, value in self.inputs.items():
-                assert key in json_schema, (
-                    f"Input '{key}' should be present in function signature, found only {json_schema.keys()}"
-                )
                 if "nullable" in value:
-                    assert "nullable" in json_schema[key], (
+                    assert key in json_schema and "nullable" in json_schema[key], (
                         f"Nullable argument '{key}' in inputs should have key 'nullable' set to True in function signature."
                     )
                 if key in json_schema and "nullable" in json_schema[key]:
                     assert "nullable" in value, (
                         f"Nullable argument '{key}' in function signature should have key 'nullable' set to True in inputs."
                     )
     def forward(self, *args, **kwargs):
         return NotImplementedError("Write this method in your subclass of `Tool`.")
     def __call__(self, *args, sanitize_inputs_outputs: bool = False, **kwargs):
         if not self.is_initialized:
@@ -158,27 +198,27 @@
         - an `app.py` file providing an UI for your tool when it is exported to a Space with `tool.push_to_hub()`
         - a `requirements.txt` containing the names of the module used by your tool (as detected when inspecting its
           code)
         Args:
             output_dir (`str`): The folder in which you want to save your tool.
         """
         os.makedirs(output_dir, exist_ok=True)
         class_name = self.__class__.__name__
         tool_file = os.path.join(output_dir, "tool.py")
         if type(self).__name__ == "SimpleTool":
-            source_code = get_source(self.forward).replace("@tool", "")
-            forward_node = ast.parse(source_code)
+            source_code = inspect.getsource(self.forward).replace("@tool", "")
+            forward_node = ast.parse(textwrap.dedent(source_code))
             method_checker = MethodChecker(set())
             method_checker.visit(forward_node)
             if len(method_checker.errors) > 0:
                 raise (ValueError("\n".join(method_checker.errors)))
-            forward_source_code = get_source(self.forward)
+            forward_source_code = inspect.getsource(self.forward)
             tool_code = textwrap.dedent(
                 f"""
             from smolagents import Tool
             from typing import Optional
             class {class_name}(Tool):
                 name = "{self.name}"
                 description = "{self.description}"
                 inputs = {json.dumps(self.inputs, separators=(",", ":"))}
                 output_type = "{self.output_type}"
             """
@@ -216,22 +256,33 @@
                 textwrap.dedent(
                     f"""
             from smolagents import launch_gradio_demo
             from typing import Optional
             from tool import {class_name}
             tool = {class_name}()
             launch_gradio_demo(tool)
             """
                 ).lstrip()
             )
-        imports = {el for el in get_imports(tool_file) if el not in sys.stdlib_module_names} | {"smolagents"}
         requirements_file = os.path.join(output_dir, "requirements.txt")
+        imports = []
+        for module in [tool_file]:
+            imports.extend(get_imports(module))
+        imports = list(
+            set(
+                [
+                    el
+                    for el in imports + ["smolagents"]
+                    if el not in sys.stdlib_module_names
+                ]
+            )
+        )
         with open(requirements_file, "w", encoding="utf-8") as f:
             f.write("\n".join(imports) + "\n")
     def push_to_hub(
         self,
         repo_id: str,
         commit_message: str = "Upload tool",
         private: Optional[bool] = None,
         token: Optional[Union[bool, str]] = None,
         create_pr: bool = False,
     ) -> str:
@@ -263,23 +314,26 @@
             token=token,
             private=private,
             exist_ok=True,
             repo_type="space",
             space_sdk="gradio",
         )
         repo_id = repo_url.repo_id
         metadata_update(repo_id, {"tags": ["tool"]}, repo_type="space")
         with tempfile.TemporaryDirectory() as work_dir:
             self.save(work_dir)
+            print(work_dir)
             with open(work_dir + "/tool.py", "r") as f:
                 print("\n".join(f.readlines()))
-            logger.info(f"Uploading the following files to {repo_id}: {','.join(os.listdir(work_dir))}")
+            logger.info(
+                f"Uploading the following files to {repo_id}: {','.join(os.listdir(work_dir))}"
+            )
             return upload_folder(
                 repo_id=repo_id,
                 commit_message=commit_message,
                 folder_path=work_dir,
                 token=token,
                 create_pr=create_pr,
                 repo_type="space",
             )
     @classmethod
     def from_hub(
@@ -303,38 +357,62 @@
                 The token to identify you on hf.co. If unset, will use the token generated when running
                 `huggingface-cli login` (stored in `~/.huggingface`).
             trust_remote_code(`str`, *optional*, defaults to False):
                 This flags marks that you understand the risk of running remote code and that you trust this tool.
                 If not setting this to True, loading the tool from Hub will fail.
             kwargs (additional keyword arguments, *optional*):
                 Additional keyword arguments that will be split in two: all arguments relevant to the Hub (such as
                 `cache_dir`, `revision`, `subfolder`) will be used when downloading the files for your tool, and the
                 others will be passed along to its init.
         """
-        if not trust_remote_code:
-            raise ValueError(
-                "Loading a tool from Hub requires to trust remote code. Make sure you've inspected the repo and pass `trust_remote_code=True` to load the tool."
-            )
-        tool_file = hf_hub_download(
+        assert trust_remote_code, (
+            "Loading a tool from Hub requires to trust remote code. Make sure you've inspected the repo and pass `trust_remote_code=True` to load the tool."
+        )
+        hub_kwargs_names = [
+            "cache_dir",
+            "force_download",
+            "resume_download",
+            "proxies",
+            "revision",
+            "repo_type",
+            "subfolder",
+            "local_files_only",
+        ]
+        hub_kwargs = {k: v for k, v in kwargs.items() if k in hub_kwargs_names}
+        tool_file = "tool.py"
+        hub_kwargs["repo_type"] = get_repo_type(repo_id, **hub_kwargs)
+        resolved_tool_file = cached_file(
             repo_id,
-            "tool.py",
+            tool_file,
             token=token,
-            repo_type="space",
-            cache_dir=kwargs.get("cache_dir"),
-            force_download=kwargs.get("force_download"),
-            resume_download=kwargs.get("resume_download"),
-            proxies=kwargs.get("proxies"),
-            revision=kwargs.get("revision"),
-            subfolder=kwargs.get("subfolder"),
-            local_files_only=kwargs.get("local_files_only"),
+            **hub_kwargs,
+            _raise_exceptions_for_gated_repo=False,
+            _raise_exceptions_for_missing_entries=False,
+            _raise_exceptions_for_connection_errors=False,
         )
-        tool_code = Path(tool_file).read_text()
+        tool_code = resolved_tool_file is not None
+        if resolved_tool_file is None:
+            resolved_tool_file = cached_file(
+                repo_id,
+                tool_file,
+                token=token,
+                **hub_kwargs,
+                _raise_exceptions_for_gated_repo=False,
+                _raise_exceptions_for_missing_entries=False,
+                _raise_exceptions_for_connection_errors=False,
+            )
+        if resolved_tool_file is None:
+            raise EnvironmentError(
+                f"{repo_id} does not appear to provide a valid configuration in `tool_config.json` or `config.json`."
+            )
+        with open(resolved_tool_file, encoding="utf-8") as reader:
+            tool_code = "".join(reader.readlines())
         with tempfile.TemporaryDirectory() as temp_dir:
             module_path = os.path.join(temp_dir, "tool.py")
             with open(module_path, "w") as f:
                 f.write(tool_code)
             print("TOOL CODE:\n", tool_code)
             spec = importlib.util.spec_from_file_location("tool", module_path)
             module = importlib.util.module_from_spec(spec)
             spec.loader.exec_module(module)
             for item_name in dir(module):
                 item = getattr(module, item_name)
@@ -395,54 +473,56 @@
                 self,
                 space_id: str,
                 name: str,
                 description: str,
                 api_name: Optional[str] = None,
                 token: Optional[str] = None,
             ):
                 self.name = name
                 self.description = description
                 self.client = Client(space_id, hf_token=token)
-                space_description = self.client.view_api(return_format="dict", print_info=False)["named_endpoints"]
+                space_description = self.client.view_api(
+                    return_format="dict", print_info=False
+                )["named_endpoints"]
                 if api_name is None:
                     api_name = list(space_description.keys())[0]
                     logger.warning(
                         f"Since `api_name` was not defined, it was automatically set to the first available API: `{api_name}`."
                     )
                 self.api_name = api_name
                 try:
                     space_description_api = space_description[api_name]
                 except KeyError:
-                    raise KeyError(f"Could not find specified {api_name=} among available api names.")
+                    raise KeyError(
+                        f"Could not find specified {api_name=} among available api names."
+                    )
                 self.inputs = {}
                 for parameter in space_description_api["parameters"]:
                     if not parameter["parameter_has_default"]:
                         parameter_type = parameter["type"]["type"]
                         if parameter_type == "object":
                             parameter_type = "any"
                         self.inputs[parameter["parameter_name"]] = {
                             "type": parameter_type,
                             "description": parameter["python_type"]["description"],
                         }
                 output_component = space_description_api["returns"][0]["component"]
                 if output_component == "Image":
                     self.output_type = "image"
                 elif output_component == "Audio":
                     self.output_type = "audio"
                 else:
                     self.output_type = "any"
                 self.is_initialized = True
             def sanitize_argument_for_prediction(self, arg):
                 from gradio_client.utils import is_http_url_like
-                if _is_pillow_available():
-                    from PIL.Image import Image
-                if _is_pillow_available() and isinstance(arg, Image):
+                if isinstance(arg, ImageType):
                     temp_file = tempfile.NamedTemporaryFile(suffix=".png", delete=False)
                     arg.save(temp_file.name)
                     arg = temp_file.name
                 if (
                     (isinstance(arg, str) and os.path.isfile(arg))
                     or (isinstance(arg, Path) and arg.exists() and arg.is_file())
                     or is_http_url_like(arg)
                 ):
                     arg = handle_file(arg)
                 return arg
@@ -472,102 +552,110 @@
         """
         import inspect
         class GradioToolWrapper(Tool):
             def __init__(self, _gradio_tool):
                 self.name = _gradio_tool.name
                 self.description = _gradio_tool.description
                 self.output_type = "string"
                 self._gradio_tool = _gradio_tool
                 func_args = list(inspect.signature(_gradio_tool.run).parameters.items())
                 self.inputs = {
-                    key: {"type": CONVERSION_DICT[value.annotation], "description": ""} for key, value in func_args
+                    key: {"type": CONVERSION_DICT[value.annotation], "description": ""}
+                    for key, value in func_args
                 }
                 self.forward = self._gradio_tool.run
         return GradioToolWrapper(gradio_tool)
     @staticmethod
     def from_langchain(langchain_tool):
         """
         Creates a [`Tool`] from a langchain tool.
         """
         class LangChainToolWrapper(Tool):
-            skip_forward_signature_validation = True
             def __init__(self, _langchain_tool):
                 self.name = _langchain_tool.name.lower()
                 self.description = _langchain_tool.description
                 self.inputs = _langchain_tool.args.copy()
                 for input_content in self.inputs.values():
                     if "title" in input_content:
                         input_content.pop("title")
                     input_content["description"] = ""
                 self.output_type = "string"
                 self.langchain_tool = _langchain_tool
-                self.is_initialized = True
             def forward(self, *args, **kwargs):
                 tool_input = kwargs.copy()
                 for index, argument in enumerate(args):
                     if index < len(self.inputs):
                         input_key = next(iter(self.inputs))
                         tool_input[input_key] = argument
                 return self.langchain_tool.run(tool_input)
         return LangChainToolWrapper(langchain_tool)
 DEFAULT_TOOL_DESCRIPTION_TEMPLATE = """
 - {{ tool.name }}: {{ tool.description }}
     Takes inputs: {{tool.inputs}}
     Returns an output of type: {{tool.output_type}}
 """
-def get_tool_description_with_args(tool: Tool, description_template: Optional[str] = None) -> str:
+def get_tool_description_with_args(
+    tool: Tool, description_template: Optional[str] = None
+) -> str:
     if description_template is None:
         description_template = DEFAULT_TOOL_DESCRIPTION_TEMPLATE
     compiled_template = compile_jinja_template(description_template)
     tool_description = compiled_template.render(
         tool=tool,
     )
     return tool_description
 @lru_cache
 def compile_jinja_template(template):
     try:
         import jinja2
         from jinja2.exceptions import TemplateError
         from jinja2.sandbox import ImmutableSandboxedEnvironment
     except ImportError:
         raise ImportError("template requires jinja2 to be installed.")
     if version.parse(jinja2.__version__) < version.parse("3.1.0"):
-        raise ImportError(f"template requires jinja2>=3.1.0 to be installed. Your version is {jinja2.__version__}.")
+        raise ImportError(
+            "template requires jinja2>=3.1.0 to be installed. Your version is "
+            f"{jinja2.__version__}."
+        )
     def raise_exception(message):
         raise TemplateError(message)
     jinja_env = ImmutableSandboxedEnvironment(trim_blocks=True, lstrip_blocks=True)
     jinja_env.globals["raise_exception"] = raise_exception
     return jinja_env.from_string(template)
 def launch_gradio_demo(tool: Tool):
     """
     Launches a gradio demo for a tool. The corresponding tool class needs to properly implement the class attributes
     `inputs` and `output_type`.
     Args:
         tool (`type`): The tool for which to launch the demo.
     """
     try:
         import gradio as gr
     except ImportError:
-        raise ImportError("Gradio should be installed in order to launch a gradio demo.")
+        raise ImportError(
+            "Gradio should be installed in order to launch a gradio demo."
+        )
     TYPE_TO_COMPONENT_CLASS_MAPPING = {
         "image": gr.Image,
         "audio": gr.Audio,
         "string": gr.Textbox,
         "integer": gr.Textbox,
         "number": gr.Textbox,
     }
     def tool_forward(*args, **kwargs):
         return tool(*args, sanitize_inputs_outputs=True, **kwargs)
     tool_forward.__signature__ = inspect.signature(tool.forward)
     gradio_inputs = []
     for input_name, input_details in tool.inputs.items():
-        input_gradio_component_class = TYPE_TO_COMPONENT_CLASS_MAPPING[input_details["type"]]
+        input_gradio_component_class = TYPE_TO_COMPONENT_CLASS_MAPPING[
+            input_details["type"]
+        ]
         new_component = input_gradio_component_class(label=input_name)
         gradio_inputs.append(new_component)
     output_gradio_componentclass = TYPE_TO_COMPONENT_CLASS_MAPPING[tool.output_type]
     gradio_output = output_gradio_componentclass(label="Output")
     gr.Interface(
         fn=tool_forward,
         inputs=gradio_inputs,
         outputs=gradio_output,
         title=tool.name,
         article=tool.description,
@@ -655,22 +743,27 @@
             ToolCollection: A tool collection instance loaded with the tools.
         Example:
         ```py
         >>> from smolagents import ToolCollection, CodeAgent
         >>> image_tool_collection = ToolCollection.from_hub("huggingface-tools/diffusion-tools-6630bb19a942c2306a2cdb6f")
         >>> agent = CodeAgent(tools=[*image_tool_collection.tools], add_base_tools=True)
         >>> agent.run("Please draw me a picture of rivers and lakes.")
         ```
         """
         _collection = get_collection(collection_slug, token=token)
-        _hub_repo_ids = {item.item_id for item in _collection.items if item.item_type == "space"}
-        tools = {Tool.from_hub(repo_id, token, trust_remote_code) for repo_id in _hub_repo_ids}
+        _hub_repo_ids = {
+            item.item_id for item in _collection.items if item.item_type == "space"
+        }
+        tools = {
+            Tool.from_hub(repo_id, token, trust_remote_code)
+            for repo_id in _hub_repo_ids
+        }
         return cls(tools)
     @classmethod
     @contextmanager
     def from_mcp(cls, server_parameters) -> "ToolCollection":
         """Automatically load a tool collection from an MCP server.
         Note: a separate thread will be spawned to run an asyncio event loop handling
         the MCP server.
         Args:
             server_parameters (mcp.StdioServerParameters): The server parameters to use to
             connect to the MCP server.
@@ -699,64 +792,59 @@
             )
         with MCPAdapt(server_parameters, SmolAgentsAdapter()) as tools:
             yield cls(tools)
 def tool(tool_function: Callable) -> Tool:
     """
     Converts a function into an instance of a Tool subclass.
     Args:
         tool_function: Your function. Should have type hints for each input and a type hint for the output.
         Should also have a docstring description including an 'Args:' part where each argument is described.
     """
-    tool_json_schema = get_json_schema(tool_function)["function"]
-    if "return" not in tool_json_schema:
-        raise TypeHintParsingException("Tool return type not found: make sure your function has a return type hint!")
+    parameters = get_json_schema(tool_function)["function"]
+    if "return" not in parameters:
+        raise TypeHintParsingException(
+            "Tool return type not found: make sure your function has a return type hint!"
+        )
     class SimpleTool(Tool):
-        def __init__(
-            self,
-            name: str,
-            description: str,
-            inputs: Dict[str, Dict[str, str]],
-            output_type: str,
-            function: Callable,
-        ):
+        def __init__(self, name, description, inputs, output_type, function):
             self.name = name
             self.description = description
             self.inputs = inputs
             self.output_type = output_type
             self.forward = function
             self.is_initialized = True
     simple_tool = SimpleTool(
-        name=tool_json_schema["name"],
-        description=tool_json_schema["description"],
-        inputs=tool_json_schema["parameters"]["properties"],
-        output_type=tool_json_schema["return"]["type"],
+        parameters["name"],
+        parameters["description"],
+        parameters["parameters"]["properties"],
+        parameters["return"]["type"],
         function=tool_function,
     )
     original_signature = inspect.signature(tool_function)
-    new_parameters = [inspect.Parameter("self", inspect.Parameter.POSITIONAL_ONLY)] + list(
-        original_signature.parameters.values()
-    )
+    new_parameters = [
+        inspect.Parameter("self", inspect.Parameter.POSITIONAL_ONLY)
+    ] + list(original_signature.parameters.values())
     new_signature = original_signature.replace(parameters=new_parameters)
     simple_tool.forward.__signature__ = new_signature
     return simple_tool
 class PipelineTool(Tool):
     """
     A [`Tool`] tailored towards Transformer models. On top of the class attributes of the base class [`Tool`], you will
     need to specify:
     - **model_class** (`type`) -- The class to use to load the model in this tool.
     - **default_checkpoint** (`str`) -- The default checkpoint that should be used when the user doesn't specify one.
-    - **pre_processor_class** (`type`, *optional*, defaults to [`transformers.AutoProcessor`]) -- The class to use to load the
+    - **pre_processor_class** (`type`, *optional*, defaults to [`AutoProcessor`]) -- The class to use to load the
       pre-processor
-    - **post_processor_class** (`type`, *optional*, defaults to [`transformers.AutoProcessor`]) -- The class to use to load the
+    - **post_processor_class** (`type`, *optional*, defaults to [`AutoProcessor`]) -- The class to use to load the
       post-processor (when different from the pre-processor).
     Args:
-        model (`str` or [`transformers.PreTrainedModel`], *optional*):
+        model (`str` or [`PreTrainedModel`], *optional*):
             The name of the checkpoint to use for the model, or the instantiated model. If unset, will default to the
             value of the class attribute `default_checkpoint`.
         pre_processor (`str` or `Any`, *optional*):
             The name of the checkpoint to use for the pre-processor, or the instantiated pre-processor (can be a
             tokenizer, an image processor, a feature extractor or a processor). Will default to the value of `model` if
             unset.
         post_processor (`str` or `Any`, *optional*):
             The name of the checkpoint to use for the post-processor, or the instantiated pre-processor (can be a
             tokenizer, an image processor, a feature extractor or a processor). Will default to the `pre_processor` if
             unset.
@@ -766,84 +854,85 @@
         device_map (`str` or `dict`, *optional*):
             If passed along, will be used to instantiate the model.
         model_kwargs (`dict`, *optional*):
             Any keyword argument to send to the model instantiation.
         token (`str`, *optional*):
             The token to use as HTTP bearer authorization for remote files. If unset, will use the token generated when
             running `huggingface-cli login` (stored in `~/.huggingface`).
         hub_kwargs (additional keyword arguments, *optional*):
             Any additional keyword argument to send to the methods that will load the data from the Hub.
     """
-    pre_processor_class = None
+    pre_processor_class = AutoProcessor
     model_class = None
-    post_processor_class = None
+    post_processor_class = AutoProcessor
     default_checkpoint = None
     description = "This is a pipeline tool"
     name = "pipeline"
     inputs = {"prompt": str}
     output_type = str
     skip_forward_signature_validation = True
     def __init__(
         self,
         model=None,
         pre_processor=None,
         post_processor=None,
         device=None,
         device_map=None,
         model_kwargs=None,
         token=None,
         **hub_kwargs,
     ):
-        if not is_torch_available() or not _is_package_available("accelerate"):
-            raise ModuleNotFoundError(
-                "Please install 'transformers' extra to use a PipelineTool: `pip install 'smolagents[transformers]'`"
-            )
+        if not is_torch_available():
+            raise ImportError("Please install torch in order to use this tool.")
+        if not is_accelerate_available():
+            raise ImportError("Please install accelerate in order to use this tool.")
         if model is None:
             if self.default_checkpoint is None:
-                raise ValueError("This tool does not implement a default checkpoint, you need to pass one.")
+                raise ValueError(
+                    "This tool does not implement a default checkpoint, you need to pass one."
+                )
             model = self.default_checkpoint
         if pre_processor is None:
             pre_processor = model
         self.model = model
         self.pre_processor = pre_processor
         self.post_processor = post_processor
         self.device = device
         self.device_map = device_map
         self.model_kwargs = {} if model_kwargs is None else model_kwargs
         if device_map is not None:
             self.model_kwargs["device_map"] = device_map
         self.hub_kwargs = hub_kwargs
         self.hub_kwargs["token"] = token
         super().__init__()
     def setup(self):
         """
         Instantiates the `pre_processor`, `model` and `post_processor` if necessary.
         """
         if isinstance(self.pre_processor, str):
-            if self.pre_processor_class is None:
-                from transformers import AutoProcessor
-                self.pre_processor_class = AutoProcessor
-            self.pre_processor = self.pre_processor_class.from_pretrained(self.pre_processor, **self.hub_kwargs)
+            self.pre_processor = self.pre_processor_class.from_pretrained(
+                self.pre_processor, **self.hub_kwargs
+            )
         if isinstance(self.model, str):
-            self.model = self.model_class.from_pretrained(self.model, **self.model_kwargs, **self.hub_kwargs)
+            self.model = self.model_class.from_pretrained(
+                self.model, **self.model_kwargs, **self.hub_kwargs
+            )
         if self.post_processor is None:
             self.post_processor = self.pre_processor
         elif isinstance(self.post_processor, str):
-            if self.post_processor_class is None:
-                from transformers import AutoProcessor
-                self.post_processor_class = AutoProcessor
-            self.post_processor = self.post_processor_class.from_pretrained(self.post_processor, **self.hub_kwargs)
+            self.post_processor = self.post_processor_class.from_pretrained(
+                self.post_processor, **self.hub_kwargs
+            )
         if self.device is None:
             if self.device_map is not None:
                 self.device = list(self.model.hf_device_map.values())[0]
             else:
-                from accelerate import PartialState
                 self.device = PartialState().default_device
         if self.device_map is None:
             self.model.to(self.device)
         super().setup()
     def encode(self, raw_inputs):
         """
         Uses the `pre_processor` to prepare the inputs for the `model`.
         """
         return self.pre_processor(raw_inputs)
     def forward(self, inputs):
@@ -853,27 +942,30 @@
         import torch
         with torch.no_grad():
             return self.model(**inputs)
     def decode(self, outputs):
         """
         Uses the `post_processor` to decode the model output.
         """
         return self.post_processor(outputs)
     def __call__(self, *args, **kwargs):
         import torch
-        from accelerate.utils import send_to_device
         args, kwargs = handle_agent_input_types(*args, **kwargs)
         if not self.is_initialized:
             self.setup()
         encoded_inputs = self.encode(*args, **kwargs)
-        tensor_inputs = {k: v for k, v in encoded_inputs.items() if isinstance(v, torch.Tensor)}
-        non_tensor_inputs = {k: v for k, v in encoded_inputs.items() if not isinstance(v, torch.Tensor)}
+        tensor_inputs = {
+            k: v for k, v in encoded_inputs.items() if isinstance(v, torch.Tensor)
+        }
+        non_tensor_inputs = {
+            k: v for k, v in encoded_inputs.items() if not isinstance(v, torch.Tensor)
+        }
         encoded_inputs = send_to_device(tensor_inputs, self.device)
         outputs = self.forward({**encoded_inputs, **non_tensor_inputs})
         outputs = send_to_device(outputs, "cpu")
         decoded_outputs = self.decode(outputs)
         return handle_agent_output_types(decoded_outputs, self.output_type)
 __all__ = [
     "AUTHORIZED_TYPES",
     "Tool",
     "tool",
     "load_tool",

--- a/src/smolagents/types.py
+++ b/src/smolagents/types.py
@@ -1,23 +1,34 @@
+import importlib.util
 import logging
 import os
 import pathlib
 import tempfile
 import uuid
 from io import BytesIO
 import numpy as np
 import requests
-from huggingface_hub.utils import is_torch_available
-from PIL import Image
-from PIL.Image import Image as ImageType
-from .utils import _is_package_available
+from transformers.utils import (
+    is_torch_available,
+    is_vision_available,
+)
 logger = logging.getLogger(__name__)
+if is_vision_available():
+    from PIL import Image
+    from PIL.Image import Image as ImageType
+else:
+    ImageType = object
+if is_torch_available():
+    import torch
+    from torch import Tensor
+else:
+    Tensor = object
 class AgentType:
     """
     Abstract class to be reimplemented to define types that can be returned by agents.
     These objects serve three purposes:
     - They behave as they were the type they're meant to be, e.g., a string for text, a PIL.Image for images
     - They can be stringified: str(object) in order to return a string defining the object
     - They should be displayed correctly in ipython notebooks/colab/jupyter
     """
     def __init__(self, value):
         self._value = value
@@ -41,39 +52,41 @@
         return self._value
     def to_string(self):
         return str(self._value)
 class AgentImage(AgentType, ImageType):
     """
     Image type returned by the agent. Behaves as a PIL.Image.
     """
     def __init__(self, value):
         AgentType.__init__(self, value)
         ImageType.__init__(self)
+        if not is_vision_available():
+            raise ImportError("PIL must be installed in order to handle images.")
         self._path = None
         self._raw = None
         self._tensor = None
         if isinstance(value, AgentImage):
             self._raw, self._path, self._tensor = value._raw, value._path, value._tensor
         elif isinstance(value, ImageType):
             self._raw = value
         elif isinstance(value, bytes):
             self._raw = Image.open(BytesIO(value))
         elif isinstance(value, (str, pathlib.Path)):
             self._path = value
-        elif is_torch_available():
-            import torch
-            if isinstance(value, torch.Tensor):
-                self._tensor = value
-            if isinstance(value, np.ndarray):
-                self._tensor = torch.from_numpy(value)
-        if self._path is None and self._raw is None and self._tensor is None:
-            raise TypeError(f"Unsupported type for {self.__class__.__name__}: {type(value)}")
+        elif isinstance(value, torch.Tensor):
+            self._tensor = value
+        elif isinstance(value, np.ndarray):
+            self._tensor = torch.from_numpy(value)
+        else:
+            raise TypeError(
+                f"Unsupported type for {self.__class__.__name__}: {type(value)}"
+            )
     def _ipython_display_(self, include=None, exclude=None):
         """
         Displays correctly this type in an ipython notebook (ipython, colab, jupyter, ...)
         """
         from IPython.display import Image, display
         display(Image(self.to_string()))
     def to_raw(self):
         """
         Returns the "raw" version of that object. In the case of an AgentImage, it is a PIL.Image.
         """
@@ -112,25 +125,24 @@
             format (str): The format to use for the output image. The format is the same as in PIL.Image.save.
             **params: Additional parameters to pass to PIL.Image.save.
         """
         img = self.to_raw()
         img.save(output_bytes, format=format, **params)
 class AgentAudio(AgentType, str):
     """
     Audio type returned by the agent.
     """
     def __init__(self, value, samplerate=16_000):
-        if not _is_package_available("soundfile") or not is_torch_available:
+        if importlib.util.find_spec("soundfile") is None:
             raise ModuleNotFoundError(
                 "Please install 'audio' extra to use AgentAudio: `pip install 'smolagents[audio]'`"
             )
-        import torch
         super().__init__(value)
         self._path = None
         self._tensor = None
         self.samplerate = samplerate
         if isinstance(value, (str, pathlib.Path)):
             self._path = value
         elif is_torch_available() and isinstance(value, torch.Tensor):
             self._tensor = value
         elif isinstance(value, tuple):
             self.samplerate = value[0]
@@ -146,21 +158,20 @@
         """
         from IPython.display import Audio, display
         display(Audio(self.to_string(), rate=self.samplerate))
     def to_raw(self):
         """
         Returns the "raw" version of that object. It is a `torch.Tensor` object.
         """
         import soundfile as sf
         if self._tensor is not None:
             return self._tensor
-        import torch
         if self._path is not None:
             if "://" in str(self._path):
                 response = requests.get(self._path)
                 response.raise_for_status()
                 tensor, self.samplerate = sf.read(BytesIO(response.content))
             else:
                 tensor, self.samplerate = sf.read(self._path)
             self._tensor = torch.tensor(tensor)
             return self._tensor
     def to_string(self):
@@ -169,29 +180,37 @@
         version of the audio.
         """
         import soundfile as sf
         if self._path is not None:
             return self._path
         if self._tensor is not None:
             directory = tempfile.mkdtemp()
             self._path = os.path.join(directory, str(uuid.uuid4()) + ".wav")
             sf.write(self._path, self._tensor, samplerate=self.samplerate)
             return self._path
-_AGENT_TYPE_MAPPING = {"string": AgentText, "image": AgentImage, "audio": AgentAudio}
+AGENT_TYPE_MAPPING = {"string": AgentText, "image": AgentImage, "audio": AgentAudio}
+INSTANCE_TYPE_MAPPING = {
+    str: AgentText,
+    ImageType: AgentImage,
+    Tensor: AgentAudio,
+}
+if is_torch_available():
+    INSTANCE_TYPE_MAPPING[Tensor] = AgentAudio
 def handle_agent_input_types(*args, **kwargs):
     args = [(arg.to_raw() if isinstance(arg, AgentType) else arg) for arg in args]
-    kwargs = {k: (v.to_raw() if isinstance(v, AgentType) else v) for k, v in kwargs.items()}
+    kwargs = {
+        k: (v.to_raw() if isinstance(v, AgentType) else v) for k, v in kwargs.items()
+    }
     return args, kwargs
 def handle_agent_output_types(output, output_type=None):
-    if output_type in _AGENT_TYPE_MAPPING:
-        decoded_outputs = _AGENT_TYPE_MAPPING[output_type](output)
+    if output_type in AGENT_TYPE_MAPPING:
+        decoded_outputs = AGENT_TYPE_MAPPING[output_type](output)
         return decoded_outputs
-    if isinstance(output, str):
-        return AgentText(output)
-    if isinstance(output, ImageType):
-        return AgentImage(output)
-    if is_torch_available():
-        import torch
-        if isinstance(output, torch.Tensor):
-            return AgentAudio(output)
-    return output
+    else:
+        for _k, _v in INSTANCE_TYPE_MAPPING.items():
+            if isinstance(output, _k):
+                if (
+                    _k is not object
+                ):  # avoid converting to audio if torch is not installed
+                    return _v(output)
+        return output
 __all__ = ["AgentType", "AgentImage", "AgentText", "AgentAudio"]

--- a/src/smolagents/utils.py
+++ b/src/smolagents/utils.py
@@ -1,83 +1,59 @@
 import ast
-import base64
-import importlib.metadata
 import importlib.util
 import inspect
 import json
 import re
-import textwrap
 import types
-from enum import IntEnum
-from functools import lru_cache
-from io import BytesIO
 from typing import Dict, Tuple, Union
 from rich.console import Console
-__all__ = ["AgentError"]
-@lru_cache
-def _is_package_available(package_name: str) -> bool:
-    try:
-        importlib.metadata.version(package_name)
-        return True
-    except importlib.metadata.PackageNotFoundError:
-        return False
-@lru_cache
-def _is_pillow_available():
-    return importlib.util.find_spec("PIL") is not None
+def is_pygments_available():
+    return importlib.util.find_spec("soundfile") is not None
 console = Console()
 BASE_BUILTIN_MODULES = [
     "collections",
     "datetime",
     "itertools",
     "math",
     "queue",
     "random",
     "re",
     "stat",
     "statistics",
     "time",
     "unicodedata",
 ]
-class LogLevel(IntEnum):
-    ERROR = 0  # Only errors
-    INFO = 1  # Normal output (default)
-    DEBUG = 2  # Detailed output
-class AgentLogger:
-    def __init__(self, level: LogLevel = LogLevel.INFO):
-        self.level = level
-        self.console = Console()
-    def log(self, *args, level: LogLevel = LogLevel.INFO, **kwargs):
-        if level <= self.level:
-            self.console.print(*args, **kwargs)
 class AgentError(Exception):
     """Base class for other agent-related exceptions"""
-    def __init__(self, message, logger: AgentLogger):
+    def __init__(self, message):
         super().__init__(message)
         self.message = message
-        logger.log(f"[bold red]{message}[/bold red]", level=LogLevel.ERROR)
+        console.print(f"[bold red]{message}[/bold red]")
 class AgentParsingError(AgentError):
     """Exception raised for errors in parsing in the agent"""
     pass
 class AgentExecutionError(AgentError):
     """Exception raised for errors in execution in the agent"""
     pass
 class AgentMaxStepsError(AgentError):
     """Exception raised for errors in execution in the agent"""
     pass
 class AgentGenerationError(AgentError):
     """Exception raised for errors in generation in the agent"""
     pass
 def parse_json_blob(json_blob: str) -> Dict[str, str]:
     try:
         first_accolade_index = json_blob.find("{")
         last_accolade_index = [a.start() for a in list(re.finditer("}", json_blob))][-1]
-        json_blob = json_blob[first_accolade_index : last_accolade_index + 1].replace('\\"', "'")
+        json_blob = json_blob[first_accolade_index : last_accolade_index + 1].replace(
+            '\\"', "'"
+        )
         json_data = json.loads(json_blob, strict=False)
         return json_data
     except json.JSONDecodeError as e:
         place = e.pos
         if json_blob[place - 1 : place + 2] == "},\n":
             raise ValueError(
                 "JSON is invalid: you probably tried to provide multiple tool calls in one action. PROVIDE ONLY ONE TOOL CALL."
             )
         raise ValueError(
             f"The JSON blob you used is invalid due to the following error: {e}.\n"
@@ -130,21 +106,23 @@
         if possible_tool_arguments_key in tool_call:
             tool_arguments_key = possible_tool_arguments_key
     if tool_name_key is not None:
         if tool_arguments_key is not None:
             return tool_call[tool_name_key], tool_call[tool_arguments_key]
         else:
             return tool_call[tool_name_key], None
     error_msg = "No tool name key found in tool call!" + f" Tool call: {json_blob}"
     raise AgentParsingError(error_msg)
 MAX_LENGTH_TRUNCATE_CONTENT = 20000
-def truncate_content(content: str, max_length: int = MAX_LENGTH_TRUNCATE_CONTENT) -> str:
+def truncate_content(
+    content: str, max_length: int = MAX_LENGTH_TRUNCATE_CONTENT
+) -> str:
     if len(content) <= max_length:
         return content
     else:
         return (
             content[: max_length // 2]
             + f"\n..._This content has been truncated to stay below {max_length} characters_...\n"
             + content[-max_length // 2 :]
         )
 class ImportFinder(ast.NodeVisitor):
     def __init__(self):
@@ -154,28 +132,32 @@
             base_package = alias.name.split(".")[0]
             self.packages.add(base_package)
     def visit_ImportFrom(self, node):
         if node.module:  # for "from x import y" statements
             base_package = node.module.split(".")[0]
             self.packages.add(base_package)
 def get_method_source(method):
     """Get source code for a method, including bound methods."""
     if isinstance(method, types.MethodType):
         method = method.__func__
-    return get_source(method)
+    return inspect.getsource(method).strip()
 def is_same_method(method1, method2):
     """Compare two methods by their source code."""
     try:
         source1 = get_method_source(method1)
         source2 = get_method_source(method2)
-        source1 = "\n".join(line for line in source1.split("\n") if not line.strip().startswith("@"))
-        source2 = "\n".join(line for line in source2.split("\n") if not line.strip().startswith("@"))
+        source1 = "\n".join(
+            line for line in source1.split("\n") if not line.strip().startswith("@")
+        )
+        source2 = "\n".join(
+            line for line in source2.split("\n") if not line.strip().startswith("@")
+        )
         return source1 == source2
     except (TypeError, OSError):
         return False
 def is_same_item(item1, item2):
     """Compare two class items (methods or attributes) for equality."""
     if callable(item1) and callable(item2):
         return is_same_method(item1, item2)
     else:
         return item1 == item2
 def instance_to_source(instance, base_cls=None):
@@ -187,97 +169,58 @@
         class_lines.append(f"class {class_name}({base_cls.__name__}):")
     else:
         class_lines.append(f"class {class_name}:")
     if cls.__doc__ and (not base_cls or cls.__doc__ != base_cls.__doc__):
         class_lines.append(f'    """{cls.__doc__}"""')
     class_attrs = {
         name: value
         for name, value in cls.__dict__.items()
         if not name.startswith("__")
         and not callable(value)
-        and not (base_cls and hasattr(base_cls, name) and getattr(base_cls, name) == value)
+        and not (
+            base_cls and hasattr(base_cls, name) and getattr(base_cls, name) == value
+        )
     }
     for name, value in class_attrs.items():
         if isinstance(value, str):
             if "\n" in value:
                 class_lines.append(f'    {name} = """{value}"""')
             else:
                 class_lines.append(f'    {name} = "{value}"')
         else:
             class_lines.append(f"    {name} = {repr(value)}")
     if class_attrs:
         class_lines.append("")
     methods = {
         name: func
         for name, func in cls.__dict__.items()
         if callable(func)
         and not (
-            base_cls and hasattr(base_cls, name) and getattr(base_cls, name).__code__.co_code == func.__code__.co_code
+            base_cls
+            and hasattr(base_cls, name)
+            and getattr(base_cls, name).__code__.co_code == func.__code__.co_code
         )
     }
     for name, method in methods.items():
-        method_source = get_source(method)
+        method_source = inspect.getsource(method)
         method_lines = method_source.split("\n")
         first_line = method_lines[0]
         indent = len(first_line) - len(first_line.lstrip())
         method_lines = [line[indent:] for line in method_lines]
-        method_source = "\n".join(["    " + line if line.strip() else line for line in method_lines])
+        method_source = "\n".join(
+            ["    " + line if line.strip() else line for line in method_lines]
+        )
         class_lines.append(method_source)
         class_lines.append("")
     import_finder = ImportFinder()
     import_finder.visit(ast.parse("\n".join(class_lines)))
     required_imports = import_finder.packages
     final_lines = []
     if base_cls:
         final_lines.append(f"from {base_cls.__module__} import {base_cls.__name__}")
     for package in required_imports:
         final_lines.append(f"import {package}")
     if final_lines:  # Add empty line after imports
         final_lines.append("")
     final_lines.extend(class_lines)
     return "\n".join(final_lines)
-def get_source(obj) -> str:
-    """Get the source code of a class or callable object (e.g.: function, method).
-    First attempts to get the source code using `inspect.getsource`.
-    In a dynamic environment (e.g.: Jupyter, IPython), if this fails,
-    falls back to retrieving the source code from the current interactive shell session.
-    Args:
-        obj: A class or callable object (e.g.: function, method)
-    Returns:
-        str: The source code of the object, dedented and stripped
-    Raises:
-        TypeError: If object is not a class or callable
-        OSError: If source code cannot be retrieved from any source
-        ValueError: If source cannot be found in IPython history
-    Note:
-        TODO: handle Python standard REPL
-    """
-    if not (isinstance(obj, type) or callable(obj)):
-        raise TypeError(f"Expected class or callable, got {type(obj)}")
-    inspect_error = None
-    try:
-        return textwrap.dedent(inspect.getsource(obj)).strip()
-    except OSError as e:
-        inspect_error = e
-    try:
-        import IPython
-        shell = IPython.get_ipython()
-        if not shell:
-            raise ImportError("No active IPython shell found")
-        all_cells = "\n".join(shell.user_ns.get("In", [])).strip()
-        if not all_cells:
-            raise ValueError("No code cells found in IPython session")
-        tree = ast.parse(all_cells)
-        for node in ast.walk(tree):
-            if isinstance(node, (ast.ClassDef, ast.FunctionDef)) and node.name == obj.__name__:
-                return textwrap.dedent("\n".join(all_cells.split("\n")[node.lineno - 1 : node.end_lineno])).strip()
-        raise ValueError(f"Could not find source code for {obj.__name__} in IPython history")
-    except ImportError:
-        raise inspect_error
-    except ValueError as e:
-        raise e from inspect_error
-def encode_image_base64(image):
-    buffered = BytesIO()
-    image.save(buffered, format="PNG")
-    return base64.b64encode(buffered.getvalue()).decode("utf-8")
-def make_image_url(base64_image):
-    return f"data:image/png;base64,{base64_image}"
+__all__ = ["AgentError"]
