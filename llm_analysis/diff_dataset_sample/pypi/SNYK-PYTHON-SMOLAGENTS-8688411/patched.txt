# ====================================================================
# FILE: docs/source/hi/_config.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-9 ---
     1| INSTALL_CONTENT = """
     2| ! pip install smolagents
     3| """
     4| notebook_first_cells = [{"type": "code", "content": INSTALL_CONTENT}]
     5| black_avoid_patterns = {
     6|     "{processor_class}": "FakeProcessorClass",
     7|     "{model_class}": "FakeModelClass",
     8|     "{object_class}": "FakeObjectClass",
     9| }


# ====================================================================
# FILE: examples/agent_from_any_llm.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-32 ---
     1| from typing import Optional
     2| from smolagents import HfApiModel, LiteLLMModel, TransformersModel, tool
     3| from smolagents.agents import CodeAgent, ToolCallingAgent
     4| available_inferences = ["hf_api", "transformers", "ollama", "litellm"]
     5| chosen_inference = "transformers"
     6| print(f"Chose model {chosen_inference}")
     7| if chosen_inference == "hf_api":
     8|     model = HfApiModel(model_id="meta-llama/Llama-3.3-70B-Instruct")
     9| elif chosen_inference == "transformers":
    10|     model = TransformersModel(model_id="HuggingFaceTB/SmolLM2-1.7B-Instruct", device_map="auto", max_new_tokens=1000)
    11| elif chosen_inference == "ollama":
    12|     model = LiteLLMModel(
    13|         model_id="ollama_chat/llama3.2",
    14|         api_base="http://localhost:11434",  # replace with remote open-ai compatible server if necessary
    15|         api_key="your-api-key",  # replace with API key if necessary
    16|     )
    17| elif chosen_inference == "litellm":
    18|     model = LiteLLMModel(model_id="gpt-4o")
    19| @tool
    20| def get_weather(location: str, celsius: Optional[bool] = False) -> str:
    21|     """
    22|     Get weather in the next days at given location.
    23|     Secretly this tool does not care about the location, it hates the weather everywhere.
    24|     Args:
    25|         location: the location
    26|         celsius: the temperature
    27|     """
    28|     return "The weather is UNGODLY with torrential rains and temperatures below -10째C"
    29| agent = ToolCallingAgent(tools=[get_weather], model=model)
    30| print("ToolCallingAgent:", agent.run("What's the weather like in Paris?"))
    31| agent = CodeAgent(tools=[get_weather], model=model)
    32| print("ToolCallingAgent:", agent.run("What's the weather like in Paris?"))


# ====================================================================
# FILE: examples/e2b_example.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-35 ---
     1| from dotenv import load_dotenv
     2| from smolagents import CodeAgent, HfApiModel, Tool
     3| from smolagents.default_tools import VisitWebpageTool
     4| load_dotenv()
     5| class GetCatImageTool(Tool):
     6|     name = "get_cat_image"
     7|     description = "Get a cat image"
     8|     inputs = {}
     9|     output_type = "image"
    10|     def __init__(self):
    11|         super().__init__()
    12|         self.url = "https://em-content.zobj.net/source/twitter/53/robot-face_1f916.png"
    13|     def forward(self):
    14|         from io import BytesIO
    15|         import requests
    16|         from PIL import Image
    17|         response = requests.get(self.url)
    18|         return Image.open(BytesIO(response.content))
    19| get_cat_image = GetCatImageTool()
    20| agent = CodeAgent(
    21|     tools=[get_cat_image, VisitWebpageTool()],
    22|     model=HfApiModel(),
    23|     additional_authorized_imports=[
    24|         "Pillow",
    25|         "requests",
    26|         "markdownify",
    27|     ],  # "duckduckgo-search",
    28|     use_e2b_executor=True,
    29| )
    30| agent.run(
    31|     "Return me an image of a cat. Directly use the image provided in your state.",
    32|     additional_args={"cat_image": get_cat_image()},
    33| )  # Asking to directly return the image from state tests that additional_args are properly sent to server.
    34| from smolagents import GradioUI
    35| GradioUI(agent).launch()


# ====================================================================
# FILE: examples/gradio_upload.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-3 ---
     1| from smolagents import CodeAgent, GradioUI, HfApiModel
     2| agent = CodeAgent(tools=[], model=HfApiModel(), max_steps=4, verbosity_level=1)
     3| GradioUI(agent, file_upload_folder="./data").launch()


# ====================================================================
# FILE: examples/inspect_runs.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| from openinference.instrumentation.smolagents import SmolagentsInstrumentor
     2| from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
     3| from opentelemetry.sdk.trace import TracerProvider
     4| from opentelemetry.sdk.trace.export import SimpleSpanProcessor
     5| from smolagents import (
     6|     CodeAgent,
     7|     DuckDuckGoSearchTool,
     8|     HfApiModel,
     9|     ManagedAgent,
    10|     ToolCallingAgent,
    11|     VisitWebpageTool,
    12| )
    13| trace_provider = TracerProvider()
    14| trace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter("http://0.0.0.0:6006/v1/traces")))
    15| SmolagentsInstrumentor().instrument(tracer_provider=trace_provider, skip_dep_check=True)
    16| model = HfApiModel()
    17| agent = ToolCallingAgent(
    18|     tools=[DuckDuckGoSearchTool(), VisitWebpageTool()],
    19|     model=model,
    20| )
    21| managed_agent = ManagedAgent(
    22|     agent=agent,
    23|     name="managed_agent",
    24|     description="This is an agent that can do web search.",
    25| )
    26| manager_agent = CodeAgent(
    27|     tools=[],
    28|     model=model,
    29|     managed_agents=[managed_agent],
    30| )
    31| manager_agent.run("If the US keeps it 2024 growth rate, how many years would it take for the GDP to double?")


# ====================================================================
# FILE: examples/multiple_tools.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-170 ---
     1| from typing import Optional
     2| import requests
     3| from smolagents import CodeAgent, HfApiModel, tool
     4| model = HfApiModel()
     5| @tool
     6| def get_weather(location: str, celsius: Optional[bool] = False) -> str:
     7|     """
     8|     Get the current weather at the given location using the WeatherStack API.
     9|     Args:
    10|         location: The location (city name).
    11|         celsius: Whether to return the temperature in Celsius (default is False, which returns Fahrenheit).
    12|     Returns:
    13|         A string describing the current weather at the location.
    14|     """
    15|     api_key = "your_api_key"  # Replace with your API key from https://weatherstack.com/
    16|     units = "m" if celsius else "f"  # 'm' for Celsius, 'f' for Fahrenheit
    17|     url = f"http://api.weatherstack.com/current?access_key={api_key}&query={location}&units={units}"
    18|     try:
    19|         response = requests.get(url)
    20|         response.raise_for_status()  # Raise an exception for HTTP errors
    21|         data = response.json()
    22|         if data.get("error"):  # Check if there's an error in the response
    23|             return f"Error: {data['error'].get('info', 'Unable to fetch weather data.')}"
    24|         weather = data["current"]["weather_descriptions"][0]
    25|         temp = data["current"]["temperature"]
    26|         temp_unit = "째C" if celsius else "째F"
    27|         return f"The current weather in {location} is {weather} with a temperature of {temp} {temp_unit}."
    28|     except requests.exceptions.RequestException as e:
    29|         return f"Error fetching weather data: {str(e)}"
    30| @tool
    31| def convert_currency(amount: float, from_currency: str, to_currency: str) -> str:
    32|     """
    33|     Converts a specified amount from one currency to another using the ExchangeRate-API.
    34|     Args:
    35|         amount: The amount of money to convert.
    36|         from_currency: The currency code of the currency to convert from (e.g., 'USD').
    37|         to_currency: The currency code of the currency to convert to (e.g., 'EUR').
    38|     Returns:
    39|         str: A string describing the converted amount in the target currency, or an error message if the conversion fails.
    40|     Raises:
    41|         requests.exceptions.RequestException: If there is an issue with the HTTP request to the ExchangeRate-API.
    42|     """
    43|     api_key = "your_api_key"  # Replace with your actual API key from https://www.exchangerate-api.com/
    44|     url = f"https://v6.exchangerate-api.com/v6/{api_key}/latest/{from_currency}"
    45|     try:
    46|         response = requests.get(url)
    47|         response.raise_for_status()
    48|         data = response.json()
    49|         exchange_rate = data["conversion_rates"].get(to_currency)
    50|         if not exchange_rate:
    51|             return f"Error: Unable to find exchange rate for {from_currency} to {to_currency}."
    52|         converted_amount = amount * exchange_rate
    53|         return f"{amount} {from_currency} is equal to {converted_amount} {to_currency}."
    54|     except requests.exceptions.RequestException as e:
    55|         return f"Error fetching conversion data: {str(e)}"
    56| @tool
    57| def get_news_headlines() -> str:
    58|     """
    59|     Fetches the top news headlines from the News API for the United States.
    60|     This function makes a GET request to the News API to retrieve the top news headlines
    61|     for the United States. It returns the titles and sources of the top 5 articles as a
    62|     formatted string. If no articles are available, it returns a message indicating that
    63|     no news is available. In case of a request error, it returns an error message.
    64|     Returns:
    65|         str: A string containing the top 5 news headlines and their sources, or an error message.
    66|     """
    67|     api_key = "your_api_key"  # Replace with your actual API key from https://newsapi.org/
    68|     url = f"https://newsapi.org/v2/top-headlines?country=us&apiKey={api_key}"
    69|     try:
    70|         response = requests.get(url)
    71|         response.raise_for_status()
    72|         data = response.json()
    73|         articles = data["articles"]
    74|         if not articles:
    75|             return "No news available at the moment."
    76|         headlines = [f"{article['title']} - {article['source']['name']}" for article in articles[:5]]
    77|         return "\n".join(headlines)
    78|     except requests.exceptions.RequestException as e:
    79|         return f"Error fetching news data: {str(e)}"
    80| @tool
    81| def get_joke() -> str:
    82|     """
    83|     Fetches a random joke from the JokeAPI.
    84|     This function sends a GET request to the JokeAPI to retrieve a random joke.
    85|     It handles both single jokes and two-part jokes (setup and delivery).
    86|     If the request fails or the response does not contain a joke, an error message is returned.
    87|     Returns:
    88|         str: The joke as a string, or an error message if the joke could not be fetched.
    89|     """
    90|     url = "https://v2.jokeapi.dev/joke/Any?type=single"
    91|     try:
    92|         response = requests.get(url)
    93|         response.raise_for_status()
    94|         data = response.json()
    95|         if "joke" in data:
    96|             return data["joke"]
    97|         elif "setup" in data and "delivery" in data:
    98|             return f"{data['setup']} - {data['delivery']}"
    99|         else:
   100|             return "Error: Unable to fetch joke."
   101|     except requests.exceptions.RequestException as e:
   102|         return f"Error fetching joke: {str(e)}"
   103| @tool
   104| def get_time_in_timezone(location: str) -> str:
   105|     """
   106|     Fetches the current time for a given location using the World Time API.
   107|     Args:
   108|         location: The location for which to fetch the current time, formatted as 'Region/City'.
   109|     Returns:
   110|         str: A string indicating the current time in the specified location, or an error message if the request fails.
   111|     Raises:
   112|         requests.exceptions.RequestException: If there is an issue with the HTTP request.
   113|     """
   114|     url = f"http://worldtimeapi.org/api/timezone/{location}.json"
   115|     try:
   116|         response = requests.get(url)
   117|         response.raise_for_status()
   118|         data = response.json()
   119|         current_time = data["datetime"]
   120|         return f"The current time in {location} is {current_time}."
   121|     except requests.exceptions.RequestException as e:
   122|         return f"Error fetching time data: {str(e)}"
   123| @tool
   124| def get_random_fact() -> str:
   125|     """
   126|     Fetches a random fact from the "uselessfacts.jsph.pl" API.
   127|     Returns:
   128|         str: A string containing the random fact or an error message if the request fails.
   129|     """
   130|     url = "https://uselessfacts.jsph.pl/random.json?language=en"
   131|     try:
   132|         response = requests.get(url)
   133|         response.raise_for_status()
   134|         data = response.json()
   135|         return f"Random Fact: {data['text']}"
   136|     except requests.exceptions.RequestException as e:
   137|         return f"Error fetching random fact: {str(e)}"
   138| @tool
   139| def search_wikipedia(query: str) -> str:
   140|     """
   141|     Fetches a summary of a Wikipedia page for a given query.
   142|     Args:
   143|         query: The search term to look up on Wikipedia.
   144|     Returns:
   145|         str: A summary of the Wikipedia page if successful, or an error message if the request fails.
   146|     Raises:
   147|         requests.exceptions.RequestException: If there is an issue with the HTTP request.
   148|     """
   149|     url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{query}"
   150|     try:
   151|         response = requests.get(url)
   152|         response.raise_for_status()
   153|         data = response.json()
   154|         title = data["title"]
   155|         extract = data["extract"]
   156|         return f"Summary for {title}: {extract}"
   157|     except requests.exceptions.RequestException as e:
   158|         return f"Error fetching Wikipedia data: {str(e)}"
   159| agent = CodeAgent(
   160|     tools=[
   161|         convert_currency,
   162|         get_weather,
   163|         get_news_headlines,
   164|         get_joke,
   165|         get_random_fact,
   166|         search_wikipedia,
   167|     ],
   168|     model=model,
   169| )
   170| agent.run("5000 dollars to Euros")


# ====================================================================
# FILE: examples/rag.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-50 ---
     1| import datasets
     2| from langchain.docstore.document import Document
     3| from langchain.text_splitter import RecursiveCharacterTextSplitter
     4| from langchain_community.retrievers import BM25Retriever
     5| knowledge_base = datasets.load_dataset("m-ric/huggingface_doc", split="train")
     6| knowledge_base = knowledge_base.filter(lambda row: row["source"].startswith("huggingface/transformers"))
     7| source_docs = [
     8|     Document(page_content=doc["text"], metadata={"source": doc["source"].split("/")[1]}) for doc in knowledge_base
     9| ]
    10| text_splitter = RecursiveCharacterTextSplitter(
    11|     chunk_size=500,
    12|     chunk_overlap=50,
    13|     add_start_index=True,
    14|     strip_whitespace=True,
    15|     separators=["\n\n", "\n", ".", " ", ""],
    16| )
    17| docs_processed = text_splitter.split_documents(source_docs)
    18| from smolagents import Tool
    19| class RetrieverTool(Tool):
    20|     name = "retriever"
    21|     description = "Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query."
    22|     inputs = {
    23|         "query": {
    24|             "type": "string",
    25|             "description": "The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.",
    26|         }
    27|     }
    28|     output_type = "string"
    29|     def __init__(self, docs, **kwargs):
    30|         super().__init__(**kwargs)
    31|         self.retriever = BM25Retriever.from_documents(docs, k=10)
    32|     def forward(self, query: str) -> str:
    33|         assert isinstance(query, str), "Your search query must be a string"
    34|         docs = self.retriever.invoke(
    35|             query,
    36|         )
    37|         return "\nRetrieved documents:\n" + "".join(
    38|             [f"\n\n===== Document {str(i)} =====\n" + doc.page_content for i, doc in enumerate(docs)]
    39|         )
    40| from smolagents import CodeAgent, HfApiModel
    41| retriever_tool = RetrieverTool(docs_processed)
    42| agent = CodeAgent(
    43|     tools=[retriever_tool],
    44|     model=HfApiModel("meta-llama/Llama-3.3-70B-Instruct"),
    45|     max_steps=4,
    46|     verbosity_level=2,
    47| )
    48| agent_output = agent.run("For a transformers model training, which is slower, the forward or the backward pass?")
    49| print("Final output:")
    50| print(agent_output)


# ====================================================================
# FILE: examples/rag_using_chromadb.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-69 ---
     1| import os
     2| import datasets
     3| from langchain.docstore.document import Document
     4| from langchain.text_splitter import RecursiveCharacterTextSplitter
     5| from langchain_chroma import Chroma
     6| from langchain_huggingface import HuggingFaceEmbeddings
     7| from tqdm import tqdm
     8| from transformers import AutoTokenizer
     9| from smolagents import LiteLLMModel, Tool
    10| from smolagents.agents import CodeAgent
    11| knowledge_base = datasets.load_dataset("m-ric/huggingface_doc", split="train")
    12| source_docs = [
    13|     Document(page_content=doc["text"], metadata={"source": doc["source"].split("/")[1]}) for doc in knowledge_base
    14| ]
    15| text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
    16|     AutoTokenizer.from_pretrained("thenlper/gte-small"),
    17|     chunk_size=200,
    18|     chunk_overlap=20,
    19|     add_start_index=True,
    20|     strip_whitespace=True,
    21|     separators=["\n\n", "\n", ".", " ", ""],
    22| )
    23| print("Splitting documents...")
    24| docs_processed = []
    25| unique_texts = {}
    26| for doc in tqdm(source_docs):
    27|     new_docs = text_splitter.split_documents([doc])
    28|     for new_doc in new_docs:
    29|         if new_doc.page_content not in unique_texts:
    30|             unique_texts[new_doc.page_content] = True
    31|             docs_processed.append(new_doc)
    32| print("Embedding documents... This should take a few minutes (5 minutes on MacBook with M1 Pro)")
    33| embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    34| vector_store = Chroma.from_documents(docs_processed, embeddings, persist_directory="./chroma_db")
    35| class RetrieverTool(Tool):
    36|     name = "retriever"
    37|     description = (
    38|         "Uses semantic search to retrieve the parts of documentation that could be most relevant to answer your query."
    39|     )
    40|     inputs = {
    41|         "query": {
    42|             "type": "string",
    43|             "description": "The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.",
    44|         }
    45|     }
    46|     output_type = "string"
    47|     def __init__(self, vector_store, **kwargs):
    48|         super().__init__(**kwargs)
    49|         self.vector_store = vector_store
    50|     def forward(self, query: str) -> str:
    51|         assert isinstance(query, str), "Your search query must be a string"
    52|         docs = self.vector_store.similarity_search(query, k=3)
    53|         return "\nRetrieved documents:\n" + "".join(
    54|             [f"\n\n===== Document {str(i)} =====\n" + doc.page_content for i, doc in enumerate(docs)]
    55|         )
    56| retriever_tool = RetrieverTool(vector_store)
    57| model = LiteLLMModel(
    58|     model_id="groq/llama-3.3-70b-versatile",
    59|     api_key=os.environ.get("GROQ_API_KEY"),
    60| )
    61| agent = CodeAgent(
    62|     tools=[retriever_tool],
    63|     model=model,
    64|     max_steps=4,
    65|     verbosity_level=2,
    66| )
    67| agent_output = agent.run("How can I push a model to the Hub?")
    68| print("Final output:")
    69| print(agent_output)


# ====================================================================
# FILE: examples/text_to_sql.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-57 ---
     1| from sqlalchemy import (
     2|     Column,
     3|     Float,
     4|     Integer,
     5|     MetaData,
     6|     String,
     7|     Table,
     8|     create_engine,
     9|     insert,
    10|     inspect,
    11|     text,
    12| )
    13| engine = create_engine("sqlite:///:memory:")
    14| metadata_obj = MetaData()
    15| table_name = "receipts"
    16| receipts = Table(
    17|     table_name,
    18|     metadata_obj,
    19|     Column("receipt_id", Integer, primary_key=True),
    20|     Column("customer_name", String(16), primary_key=True),
    21|     Column("price", Float),
    22|     Column("tip", Float),
    23| )
    24| metadata_obj.create_all(engine)
    25| rows = [
    26|     {"receipt_id": 1, "customer_name": "Alan Payne", "price": 12.06, "tip": 1.20},
    27|     {"receipt_id": 2, "customer_name": "Alex Mason", "price": 23.86, "tip": 0.24},
    28|     {"receipt_id": 3, "customer_name": "Woodrow Wilson", "price": 53.43, "tip": 5.43},
    29|     {"receipt_id": 4, "customer_name": "Margaret James", "price": 21.11, "tip": 1.00},
    30| ]
    31| for row in rows:
    32|     stmt = insert(receipts).values(**row)
    33|     with engine.begin() as connection:
    34|         cursor = connection.execute(stmt)
    35| inspector = inspect(engine)
    36| columns_info = [(col["name"], col["type"]) for col in inspector.get_columns("receipts")]
    37| table_description = "Columns:\n" + "\n".join([f"  - {name}: {col_type}" for name, col_type in columns_info])
    38| print(table_description)
    39| from smolagents import tool
    40| @tool
    41| def sql_engine(query: str) -> str:
    42|     """
    43|     Allows you to perform SQL queries on the table. Returns a string representation of the result.
    44|     The table is named 'receipts'. Its description is as follows:
    45|         Columns:
    46|         - receipt_id: INTEGER
    47|         - customer_name: VARCHAR(16)
    48|         - price: FLOAT
    49|         - tip: FLOAT
    50|     Args:
    51|         query: The query to perform. This should be correct SQL.
    52|     """
    53|     output = ""
    54|     with engine.connect() as con:
    55|         rows = con.execute(text(query))
    56|         for row in rows:
    57|             output += "\n" + str(row)


# ====================================================================
# FILE: examples/vlm_web_browser.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-160 ---
     1| from io import BytesIO
     2| from time import sleep
     3| import helium
     4| from dotenv import load_dotenv
     5| from PIL import Image
     6| from selenium import webdriver
     7| from selenium.common.exceptions import ElementNotInteractableException, TimeoutException
     8| from selenium.webdriver.common.by import By
     9| from selenium.webdriver.support import expected_conditions as EC
    10| from selenium.webdriver.support.ui import WebDriverWait
    11| from smolagents import CodeAgent, LiteLLMModel, OpenAIServerModel, TransformersModel, tool  # noqa: F401
    12| from smolagents.agents import ActionStep
    13| load_dotenv()
    14| import os
    15| model = OpenAIServerModel(
    16|     api_key=os.getenv("FIREWORKS_API_KEY"),
    17|     api_base="https://api.fireworks.ai/inference/v1",
    18|     model_id="accounts/fireworks/models/qwen2-vl-72b-instruct",
    19| )
    20| def save_screenshot(step_log: ActionStep, agent: CodeAgent) -> None:
    21|     sleep(1.0)  # Let JavaScript animations happen before taking the screenshot
    22|     driver = helium.get_driver()
    23|     current_step = step_log.step_number
    24|     if driver is not None:
    25|         for step_logs in agent.logs:  # Remove previous screenshots from logs for lean processing
    26|             if isinstance(step_log, ActionStep) and step_log.step_number <= current_step - 2:
    27|                 step_logs.observations_images = None
    28|         png_bytes = driver.get_screenshot_as_png()
    29|         image = Image.open(BytesIO(png_bytes))
    30|         print(f"Captured a browser screenshot: {image.size} pixels")
    31|         step_log.observations_images = [image.copy()]  # Create a copy to ensure it persists, important!
    32|     url_info = f"Current url: {driver.current_url}"
    33|     step_log.observations = url_info if step_logs.observations is None else step_log.observations + "\n" + url_info
    34|     return
    35| chrome_options = webdriver.ChromeOptions()
    36| chrome_options.add_argument("--force-device-scale-factor=1")
    37| chrome_options.add_argument("--window-size=1000,1300")
    38| chrome_options.add_argument("--disable-pdf-viewer")
    39| driver = helium.start_chrome(headless=False, options=chrome_options)
    40| @tool
    41| def search_item_ctrl_f(text: str, nth_result: int = 1) -> str:
    42|     """
    43|     Searches for text on the current page via Ctrl + F and jumps to the nth occurrence.
    44|     Args:
    45|         text: The text to search for
    46|         nth_result: Which occurrence to jump to (default: 1)
    47|     """
    48|     elements = driver.find_elements(By.XPATH, f"//*[contains(text(), '{text}')]")
    49|     if nth_result > len(elements):
    50|         raise Exception(f"Match n째{nth_result} not found (only {len(elements)} matches found)")
    51|     result = f"Found {len(elements)} matches for '{text}'."
    52|     elem = elements[nth_result - 1]
    53|     driver.execute_script("arguments[0].scrollIntoView(true);", elem)
    54|     result += f"Focused on element {nth_result} of {len(elements)}"
    55|     return result
    56| @tool
    57| def go_back() -> None:
    58|     """Goes back to previous page."""
    59|     driver.back()
    60| @tool
    61| def close_popups() -> str:
    62|     """
    63|     Closes any visible modal or pop-up on the page. Use this to dismiss pop-up windows! This does not work on cookie consent banners.
    64|     """
    65|     modal_selectors = [
    66|         "button[class*='close']",
    67|         "[class*='modal']",
    68|         "[class*='modal'] button",
    69|         "[class*='CloseButton']",
    70|         "[aria-label*='close']",
    71|         ".modal-close",
    72|         ".close-modal",
    73|         ".modal .close",
    74|         ".modal-backdrop",
    75|         ".modal-overlay",
    76|         "[class*='overlay']",
    77|     ]
    78|     wait = WebDriverWait(driver, timeout=0.5)
    79|     for selector in modal_selectors:
    80|         try:
    81|             elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, selector)))
    82|             for element in elements:
    83|                 if element.is_displayed():
    84|                     try:
    85|                         driver.execute_script("arguments[0].click();", element)
    86|                     except ElementNotInteractableException:
    87|                         element.click()
    88|         except TimeoutException:
    89|             continue
    90|         except Exception as e:
    91|             print(f"Error handling selector {selector}: {str(e)}")
    92|             continue
    93|     return "Modals closed"
    94| agent = CodeAgent(
    95|     tools=[go_back, close_popups, search_item_ctrl_f],
    96|     model=model,
    97|     additional_authorized_imports=["helium"],
    98|     step_callbacks=[save_screenshot],
    99|     max_steps=20,
   100|     verbosity_level=2,
   101| )
   102| helium_instructions = """
   103| You can use helium to access websites. Don't bother about the helium driver, it's already managed.
   104| First you need to import everything from helium, then you can do other actions!
   105| Code:
   106| ```py
   107| from helium import *
   108| go_to('github.com/trending')
   109| ```<end_code>
   110| You can directly click clickable elements by inputting the text that appears on them.
   111| Code:
   112| ```py
   113| click("Top products")
   114| ```<end_code>
   115| If it's a link:
   116| Code:
   117| ```py
   118| click(Link("Top products"))
   119| ```<end_code>
   120| If you try to interact with an element and it's not found, you'll get a LookupError.
   121| In general stop your action after each button click to see what happens on your screenshot.
   122| Never try to login in a page.
   123| To scroll up or down, use scroll_down or scroll_up with as an argument the number of pixels to scroll from.
   124| Code:
   125| ```py
   126| scroll_down(num_pixels=1200) # This will scroll one viewport down
   127| ```<end_code>
   128| When you have pop-ups with a cross icon to close, don't try to click the close icon by finding its element or targeting an 'X' element (this most often fails).
   129| Just use your built-in tool `close_popups` to close them:
   130| Code:
   131| ```py
   132| close_popups()
   133| ```<end_code>
   134| You can use .exists() to check for the existence of an element. For example:
   135| Code:
   136| ```py
   137| if Text('Accept cookies?').exists():
   138|     click('I accept')
   139| ```<end_code>
   140| Proceed in several steps rather than trying to solve the task in one shot.
   141| And at the end, only when you have your answer, return your final answer.
   142| Code:
   143| ```py
   144| final_answer("YOUR_ANSWER_HERE")
   145| ```<end_code>
   146| If pages seem stuck on loading, you might have to wait, for instance `import time` and run `time.sleep(5.0)`. But don't overuse this!
   147| To list elements on page, DO NOT try code-based element searches like 'contributors = find_all(S("ol > li"))': just look at the latest screenshot you have and read it visually, or use your tool search_item_ctrl_f.
   148| Of course, you can act on buttons like a user would do when navigating.
   149| After each code blob you write, you will be automatically provided with an updated screenshot of the browser and the current browser url.
   150| But beware that the screenshot will only be taken at the end of the whole action, it won't see intermediate states.
   151| Don't kill the browser.
   152| """
   153| github_request = """
   154| I'm trying to find how hard I have to work to get a repo in github.com/trending.
   155| Can you navigate to the profile for the top author of the top trending repo, and give me their total number of commits over the last year?
   156| """  # The agent is able to achieve this request only when powered by GPT-4o or Claude-3.5-sonnet.
   157| search_request = """
   158| Please navigate to https://en.wikipedia.org/wiki/Chicago and give me a sentence containing the word "1992" that mentions a construction accident.
   159| """
   160| agent.run(search_request + helium_instructions)


# ====================================================================
# FILE: src/smolagents/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-12 ---
     1| __version__ = "1.5.0"
     2| from .agents import *
     3| from .default_tools import *
     4| from .e2b_executor import *
     5| from .gradio_ui import *
     6| from .local_python_executor import *
     7| from .models import *
     8| from .monitoring import *
     9| from .prompts import *
    10| from .tools import *
    11| from .types import *
    12| from .utils import *


# ====================================================================
# FILE: src/smolagents/_function_type_hints_utils.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-302 ---
     1| """This module contains utilities exclusively taken from `transformers` repository.
     2| Since they are not specific to `transformers` and that `transformers` is an heavy dependencies, those helpers have
     3| been duplicated.
     4| TODO: move them to `huggingface_hub` to avoid code duplication.
     5| """
     6| import inspect
     7| import json
     8| import os
     9| import re
    10| import types
    11| from copy import copy
    12| from typing import (
    13|     Any,
    14|     Callable,
    15|     Dict,
    16|     List,
    17|     Optional,
    18|     Tuple,
    19|     Union,
    20|     get_args,
    21|     get_origin,
    22|     get_type_hints,
    23| )
    24| from huggingface_hub.utils import is_torch_available
    25| from .utils import _is_pillow_available
    26| def get_imports(filename: Union[str, os.PathLike]) -> List[str]:
    27|     """
    28|     Extracts all the libraries (not relative imports this time) that are imported in a file.
    29|     Args:
    30|         filename (`str` or `os.PathLike`): The module file to inspect.
    31|     Returns:
    32|         `List[str]`: The list of all packages required to use the input module.
    33|     """
    34|     with open(filename, "r", encoding="utf-8") as f:
    35|         content = f.read()
    36|     content = re.sub(r"\s*try\s*:.*?except.*?:", "", content, flags=re.DOTALL)
    37|     content = re.sub(
    38|         r"if is_flash_attn[a-zA-Z0-9_]+available\(\):\s*(from flash_attn\s*.*\s*)+",
    39|         "",
    40|         content,
    41|         flags=re.MULTILINE,
    42|     )
    43|     imports = re.findall(r"^\s*import\s+(\S+)\s*$", content, flags=re.MULTILINE)
    44|     imports += re.findall(r"^\s*from\s+(\S+)\s+import", content, flags=re.MULTILINE)
    45|     imports = [imp.split(".")[0] for imp in imports if not imp.startswith(".")]
    46|     return list(set(imports))
    47| class TypeHintParsingException(Exception):
    48|     """Exception raised for errors in parsing type hints to generate JSON schemas"""
    49| class DocstringParsingException(Exception):
    50|     """Exception raised for errors in parsing docstrings to generate JSON schemas"""
    51| def get_json_schema(func: Callable) -> Dict:
    52|     """
    53|     This function generates a JSON schema for a given function, based on its docstring and type hints. This is
    54|     mostly used for passing lists of tools to a chat template. The JSON schema contains the name and description of
    55|     the function, as well as the names, types and descriptions for each of its arguments. `get_json_schema()` requires
    56|     that the function has a docstring, and that each argument has a description in the docstring, in the standard
    57|     Google docstring format shown below. It also requires that all the function arguments have a valid Python type hint.
    58|     Although it is not required, a `Returns` block can also be added, which will be included in the schema. This is
    59|     optional because most chat templates ignore the return value of the function.
    60|     Args:
    61|         func: The function to generate a JSON schema for.
    62|     Returns:
    63|         A dictionary containing the JSON schema for the function.
    64|     Examples:
    65|     ```python
    66|     >>> def multiply(x: float, y: float):
    67|     >>>    '''
    68|     >>>    A function that multiplies two numbers
    69|     >>>
    70|     >>>    Args:
    71|     >>>        x: The first number to multiply
    72|     >>>        y: The second number to multiply
    73|     >>>    '''
    74|     >>>    return x * y
    75|     >>>
    76|     >>> print(get_json_schema(multiply))
    77|     {
    78|         "name": "multiply",
    79|         "description": "A function that multiplies two numbers",
    80|         "parameters": {
    81|             "type": "object",
    82|             "properties": {
    83|                 "x": {"type": "number", "description": "The first number to multiply"},
    84|                 "y": {"type": "number", "description": "The second number to multiply"}
    85|             },
    86|             "required": ["x", "y"]
    87|         }
    88|     }
    89|     ```
    90|     The general use for these schemas is that they are used to generate tool descriptions for chat templates that
    91|     support them, like so:
    92|     ```python
    93|     >>> from transformers import AutoTokenizer
    94|     >>> from transformers.utils import get_json_schema
    95|     >>>
    96|     >>> def multiply(x: float, y: float):
    97|     >>>    '''
    98|     >>>    A function that multiplies two numbers
    99|     >>>
   100|     >>>    Args:
   101|     >>>        x: The first number to multiply
   102|     >>>        y: The second number to multiply
   103|     >>>    return x * y
   104|     >>>    '''
   105|     >>>
   106|     >>> multiply_schema = get_json_schema(multiply)
   107|     >>> tokenizer = AutoTokenizer.from_pretrained("CohereForAI/c4ai-command-r-v01")
   108|     >>> messages = [{"role": "user", "content": "What is 179 x 4571?"}]
   109|     >>> formatted_chat = tokenizer.apply_chat_template(
   110|     >>>     messages,
   111|     >>>     tools=[multiply_schema],
   112|     >>>     chat_template="tool_use",
   113|     >>>     return_dict=True,
   114|     >>>     return_tensors="pt",
   115|     >>>     add_generation_prompt=True
   116|     >>> )
   117|     >>> # The formatted chat can now be passed to model.generate()
   118|     ```
   119|     Each argument description can also have an optional `(choices: ...)` block at the end, such as
   120|     `(choices: ["tea", "coffee"])`, which will be parsed into an `enum` field in the schema. Note that this will
   121|     only be parsed correctly if it is at the end of the line:
   122|     ```python
   123|     >>> def drink_beverage(beverage: str):
   124|     >>>    '''
   125|     >>>    A function that drinks a beverage
   126|     >>>
   127|     >>>    Args:
   128|     >>>        beverage: The beverage to drink (choices: ["tea", "coffee"])
   129|     >>>    '''
   130|     >>>    pass
   131|     >>>
   132|     >>> print(get_json_schema(drink_beverage))
   133|     ```
   134|     {
   135|         'name': 'drink_beverage',
   136|         'description': 'A function that drinks a beverage',
   137|         'parameters': {
   138|             'type': 'object',
   139|             'properties': {
   140|                 'beverage': {
   141|                     'type': 'string',
   142|                     'enum': ['tea', 'coffee'],
   143|                     'description': 'The beverage to drink'
   144|                     }
   145|                 },
   146|             'required': ['beverage']
   147|         }
   148|     }
   149|     """
   150|     doc = inspect.getdoc(func)
   151|     if not doc:
   152|         raise DocstringParsingException(
   153|             f"Cannot generate JSON schema for {func.__name__} because it has no docstring!"
   154|         )
   155|     doc = doc.strip()
   156|     main_doc, param_descriptions, return_doc = _parse_google_format_docstring(doc)
   157|     json_schema = _convert_type_hints_to_json_schema(func)
   158|     if (return_dict := json_schema["properties"].pop("return", None)) is not None:
   159|         if return_doc is not None:  # We allow a missing return docstring since most templates ignore it
   160|             return_dict["description"] = return_doc
   161|     for arg, schema in json_schema["properties"].items():
   162|         if arg not in param_descriptions:
   163|             raise DocstringParsingException(
   164|                 f"Cannot generate JSON schema for {func.__name__} because the docstring has no description for the argument '{arg}'"
   165|             )
   166|         desc = param_descriptions[arg]
   167|         enum_choices = re.search(r"\(choices:\s*(.*?)\)\s*$", desc, flags=re.IGNORECASE)
   168|         if enum_choices:
   169|             schema["enum"] = [c.strip() for c in json.loads(enum_choices.group(1))]
   170|             desc = enum_choices.string[: enum_choices.start()].strip()
   171|         schema["description"] = desc
   172|     output = {"name": func.__name__, "description": main_doc, "parameters": json_schema}
   173|     if return_dict is not None:
   174|         output["return"] = return_dict
   175|     return {"type": "function", "function": output}
   176| description_re = re.compile(r"^(.*?)[\n\s]*(Args:|Returns:|Raises:|\Z)", re.DOTALL)
   177| args_re = re.compile(r"\n\s*Args:\n\s*(.*?)[\n\s]*(Returns:|Raises:|\Z)", re.DOTALL)
   178| args_split_re = re.compile(
   179|     r"""
   180| (?:^|\n)  # Match the start of the args block, or a newline
   181| \s*(\w+):\s*  # Capture the argument name and strip spacing
   182| (.*?)\s*  # Capture the argument description, which can span multiple lines, and strip trailing spacing
   183| (?=\n\s*\w+:|\Z)  # Stop when you hit the next argument or the end of the block
   184| """,
   185|     re.DOTALL | re.VERBOSE,
   186| )
   187| returns_re = re.compile(r"\n\s*Returns:\n\s*(.*?)[\n\s]*(Raises:|\Z)", re.DOTALL)
   188| def _parse_google_format_docstring(
   189|     docstring: str,
   190| ) -> Tuple[Optional[str], Optional[Dict], Optional[str]]:
   191|     """
   192|     Parses a Google-style docstring to extract the function description,
   193|     argument descriptions, and return description.
   194|     Args:
   195|         docstring (str): The docstring to parse.
   196|     Returns:
   197|         The function description, arguments, and return description.
   198|     """
   199|     description_match = description_re.search(docstring)
   200|     args_match = args_re.search(docstring)
   201|     returns_match = returns_re.search(docstring)
   202|     description = description_match.group(1).strip() if description_match else None
   203|     docstring_args = args_match.group(1).strip() if args_match else None
   204|     returns = returns_match.group(1).strip() if returns_match else None
   205|     if docstring_args is not None:
   206|         docstring_args = "\n".join([line for line in docstring_args.split("\n") if line.strip()])  # Remove blank lines
   207|         matches = args_split_re.findall(docstring_args)
   208|         args_dict = {match[0]: re.sub(r"\s*\n+\s*", " ", match[1].strip()) for match in matches}
   209|     else:
   210|         args_dict = {}
   211|     return description, args_dict, returns
   212| def _convert_type_hints_to_json_schema(func: Callable, error_on_missing_type_hints: bool = True) -> Dict:
   213|     type_hints = get_type_hints(func)
   214|     signature = inspect.signature(func)
   215|     properties = {}
   216|     for param_name, param_type in type_hints.items():
   217|         properties[param_name] = _parse_type_hint(param_type)
   218|     required = []
   219|     for param_name, param in signature.parameters.items():
   220|         if param.annotation == inspect.Parameter.empty and error_on_missing_type_hints:
   221|             raise TypeHintParsingException(f"Argument {param.name} is missing a type hint in function {func.__name__}")
   222|         if param_name not in properties:
   223|             properties[param_name] = {}
   224|         if param.default == inspect.Parameter.empty:
   225|             required.append(param_name)
   226|         else:
   227|             properties[param_name]["nullable"] = True
   228|     schema = {"type": "object", "properties": properties}
   229|     if required:
   230|         schema["required"] = required
   231|     return schema
   232| def _parse_type_hint(hint: str) -> Dict:
   233|     origin = get_origin(hint)
   234|     args = get_args(hint)
   235|     if origin is None:
   236|         try:
   237|             return _get_json_schema_type(hint)
   238|         except KeyError:
   239|             raise TypeHintParsingException(
   240|                 "Couldn't parse this type hint, likely due to a custom class or object: ",
   241|                 hint,
   242|             )
   243|     elif origin is Union or (hasattr(types, "UnionType") and origin is types.UnionType):
   244|         subtypes = [_parse_type_hint(t) for t in args if t is not type(None)]
   245|         if len(subtypes) == 1:
   246|             return_dict = subtypes[0]
   247|         elif all(isinstance(subtype["type"], str) for subtype in subtypes):
   248|             return_dict = {"type": sorted([subtype["type"] for subtype in subtypes])}
   249|         else:
   250|             return_dict = {"anyOf": subtypes}
   251|         if type(None) in args:
   252|             return_dict["nullable"] = True
   253|         return return_dict
   254|     elif origin is list:
   255|         if not args:
   256|             return {"type": "array"}
   257|         else:
   258|             return {"type": "array", "items": _parse_type_hint(args[0])}
   259|     elif origin is tuple:
   260|         if not args:
   261|             return {"type": "array"}
   262|         if len(args) == 1:
   263|             raise TypeHintParsingException(
   264|                 f"The type hint {str(hint).replace('typing.', '')} is a Tuple with a single element, which "
   265|                 "we do not automatically convert to JSON schema as it is rarely necessary. If this input can contain "
   266|                 "more than one element, we recommend "
   267|                 "using a List[] type instead, or if it really is a single element, remove the Tuple[] wrapper and just "
   268|                 "pass the element directly."
   269|             )
   270|         if ... in args:
   271|             raise TypeHintParsingException(
   272|                 "Conversion of '...' is not supported in Tuple type hints. "
   273|                 "Use List[] types for variable-length"
   274|                 " inputs instead."
   275|             )
   276|         return {"type": "array", "prefixItems": [_parse_type_hint(t) for t in args]}
   277|     elif origin is dict:
   278|         out = {"type": "object"}
   279|         if len(args) == 2:
   280|             out["additionalProperties"] = _parse_type_hint(args[1])
   281|         return out
   282|     raise TypeHintParsingException("Couldn't parse this type hint, likely due to a custom class or object: ", hint)
   283| _BASE_TYPE_MAPPING = {
   284|     int: {"type": "integer"},
   285|     float: {"type": "number"},
   286|     str: {"type": "string"},
   287|     bool: {"type": "boolean"},
   288|     Any: {"type": "any"},
   289|     types.NoneType: {"type": "null"},
   290| }
   291| def _get_json_schema_type(param_type: str) -> Dict[str, str]:
   292|     if param_type in _BASE_TYPE_MAPPING:
   293|         return copy(_BASE_TYPE_MAPPING[param_type])
   294|     if str(param_type) == "Image" and _is_pillow_available():
   295|         from PIL.Image import Image
   296|         if param_type == Image:
   297|             return {"type": "image"}
   298|     if str(param_type) == "Tensor" and is_torch_available():
   299|         from torch import Tensor
   300|         if param_type == Tensor:
   301|             return {"type": "audio"}
   302|     return {"type": "object"}


# ====================================================================
# FILE: src/smolagents/agents.py
# Total hunks: 9
# ====================================================================
# --- HUNK 1: Lines 1-715 ---
     1| import inspect
     2| import time
     3| from collections import deque
     4| from dataclasses import dataclass
     5| from typing import Any, Callable, Dict, Generator, List, Optional, Tuple, Union
     6| from rich import box
     7| from rich.console import Group
     8| from rich.panel import Panel
     9| from rich.rule import Rule
    10| from rich.syntax import Syntax
    11| from rich.text import Text
    12| from .default_tools import TOOL_MAPPING, FinalAnswerTool
    13| from .e2b_executor import E2BExecutor
    14| from .local_python_executor import (
    15|     BASE_BUILTIN_MODULES,
    16|     LocalPythonInterpreter,
    17|     fix_final_answer_code,
    18| )
    19| from .models import (
    20|     ChatMessage,
    21|     MessageRole,
    22| )
    23| from .monitoring import Monitor
    24| from .prompts import (
    25|     CODE_SYSTEM_PROMPT,
    26|     MANAGED_AGENT_PROMPT,
    27|     PLAN_UPDATE_FINAL_PLAN_REDACTION,
    28|     SYSTEM_PROMPT_FACTS,
    29|     SYSTEM_PROMPT_FACTS_UPDATE,
    30|     SYSTEM_PROMPT_PLAN,
    31|     SYSTEM_PROMPT_PLAN_UPDATE,
    32|     TOOL_CALLING_SYSTEM_PROMPT,
    33|     USER_PROMPT_FACTS_UPDATE,
    34|     USER_PROMPT_PLAN,
    35|     USER_PROMPT_PLAN_UPDATE,
    36| )
    37| from .tools import (
    38|     DEFAULT_TOOL_DESCRIPTION_TEMPLATE,
    39|     Tool,
    40|     get_tool_description_with_args,
    41| )
    42| from .types import AgentAudio, AgentImage, handle_agent_output_types
    43| from .utils import (
    44|     AgentError,
    45|     AgentExecutionError,
    46|     AgentGenerationError,
    47|     AgentLogger,
    48|     AgentMaxStepsError,
    49|     AgentParsingError,
    50|     LogLevel,
    51|     parse_code_blobs,
    52|     parse_json_tool_call,
    53|     truncate_content,
    54| )
    55| @dataclass
    56| class ToolCall:
    57|     name: str
    58|     arguments: Any
    59|     id: str
    60| class AgentStepLog:
    61|     pass
    62| @dataclass
    63| class ActionStep(AgentStepLog):
    64|     agent_memory: List[Dict[str, str]] | None = None
    65|     tool_calls: List[ToolCall] | None = None
    66|     start_time: float | None = None
    67|     end_time: float | None = None
    68|     step_number: int | None = None
    69|     error: AgentError | None = None
    70|     duration: float | None = None
    71|     llm_output: str | None = None
    72|     observations: str | None = None
    73|     observations_images: List[str] | None = None
    74|     action_output: Any = None
    75| @dataclass
    76| class PlanningStep(AgentStepLog):
    77|     plan: str
    78|     facts: str
    79| @dataclass
    80| class TaskStep(AgentStepLog):
    81|     task: str
    82|     task_images: List[str] | None = None
    83| @dataclass
    84| class SystemPromptStep(AgentStepLog):
    85|     system_prompt: str
    86| def get_tool_descriptions(tools: Dict[str, Tool], tool_description_template: str) -> str:
    87|     return "\n".join([get_tool_description_with_args(tool, tool_description_template) for tool in tools.values()])
    88| def format_prompt_with_tools(tools: Dict[str, Tool], prompt_template: str, tool_description_template: str) -> str:
    89|     tool_descriptions = get_tool_descriptions(tools, tool_description_template)
    90|     prompt = prompt_template.replace("{{tool_descriptions}}", tool_descriptions)
    91|     if "{{tool_names}}" in prompt:
    92|         prompt = prompt.replace(
    93|             "{{tool_names}}",
    94|             ", ".join([f"'{tool.name}'" for tool in tools.values()]),
    95|         )
    96|     return prompt
    97| def show_agents_descriptions(managed_agents: Dict):
    98|     managed_agents_descriptions = """
    99| You can also give requests to team members.
   100| Calling a team member works the same as for calling a tool: simply, the only argument you can give in the call is 'request', a long string explaining your request.
   101| Given that this team member is a real human, you should be very verbose in your request.
   102| Here is a list of the team members that you can call:"""
   103|     for agent in managed_agents.values():
   104|         managed_agents_descriptions += f"\n- {agent.name}: {agent.description}"
   105|     return managed_agents_descriptions
   106| def format_prompt_with_managed_agents_descriptions(
   107|     prompt_template,
   108|     managed_agents,
   109|     agent_descriptions_placeholder: Optional[str] = None,
   110| ) -> str:
   111|     if agent_descriptions_placeholder is None:
   112|         agent_descriptions_placeholder = "{{managed_agents_descriptions}}"
   113|     if agent_descriptions_placeholder not in prompt_template:
   114|         raise ValueError(
   115|             f"Provided prompt template does not contain the managed agents descriptions placeholder '{agent_descriptions_placeholder}'"
   116|         )
   117|     if len(managed_agents.keys()) > 0:
   118|         return prompt_template.replace(agent_descriptions_placeholder, show_agents_descriptions(managed_agents))
   119|     else:
   120|         return prompt_template.replace(agent_descriptions_placeholder, "")
   121| YELLOW_HEX = "#d4b702"
   122| class MultiStepAgent:
   123|     """
   124|     Agent class that solves the given task step by step, using the ReAct framework:
   125|     While the objective is not reached, the agent will perform a cycle of action (given by the LLM) and observation (obtained from the environment).
   126|     Args:
   127|         tools (`list[Tool]`): [`Tool`]s that the agent can use.
   128|         model (`Callable[[list[dict[str, str]]], ChatMessage]`): Model that will generate the agent's actions.
   129|         system_prompt (`str`, *optional*): System prompt that will be used to generate the agent's actions.
   130|         tool_description_template (`str`, *optional*): Template used to describe the tools in the system prompt.
   131|         max_steps (`int`, default `6`): Maximum number of steps the agent can take to solve the task.
   132|         tool_parser (`Callable`, *optional*): Function used to parse the tool calls from the LLM output.
   133|         add_base_tools (`bool`, default `False`): Whether to add the base tools to the agent's tools.
   134|         verbosity_level (`int`, default `1`): Level of verbosity of the agent's logs.
   135|         grammar (`dict[str, str]`, *optional*): Grammar used to parse the LLM output.
   136|         managed_agents (`list`, *optional*): Managed agents that the agent can call.
   137|         step_callbacks (`list[Callable]`, *optional*): Callbacks that will be called at each step.
   138|         planning_interval (`int`, *optional*): Interval at which the agent will run a planning step.
   139|     """
   140|     def __init__(
   141|         self,
   142|         tools: List[Tool],
   143|         model: Callable[[List[Dict[str, str]]], ChatMessage],
   144|         system_prompt: Optional[str] = None,
   145|         tool_description_template: Optional[str] = None,
   146|         max_steps: int = 6,
   147|         tool_parser: Optional[Callable] = None,
   148|         add_base_tools: bool = False,
   149|         verbosity_level: int = 1,
   150|         grammar: Optional[Dict[str, str]] = None,
   151|         managed_agents: Optional[List] = None,
   152|         step_callbacks: Optional[List[Callable]] = None,
   153|         planning_interval: Optional[int] = None,
   154|     ):
   155|         if system_prompt is None:
   156|             system_prompt = CODE_SYSTEM_PROMPT
   157|         if tool_parser is None:
   158|             tool_parser = parse_json_tool_call
   159|         self.agent_name = self.__class__.__name__
   160|         self.model = model
   161|         self.system_prompt_template = system_prompt
   162|         self.tool_description_template = (
   163|             tool_description_template if tool_description_template else DEFAULT_TOOL_DESCRIPTION_TEMPLATE
   164|         )
   165|         self.max_steps = max_steps
   166|         self.tool_parser = tool_parser
   167|         self.grammar = grammar
   168|         self.planning_interval = planning_interval
   169|         self.state = {}
   170|         self.managed_agents = {}
   171|         if managed_agents is not None:
   172|             self.managed_agents = {agent.name: agent for agent in managed_agents}
   173|         for tool in tools:
   174|             assert isinstance(tool, Tool), f"This element is not of class Tool: {str(tool)}"
   175|         self.tools = {tool.name: tool for tool in tools}
   176|         if add_base_tools:
   177|             for tool_name, tool_class in TOOL_MAPPING.items():
   178|                 if tool_name != "python_interpreter" or self.__class__.__name__ == "ToolCallingAgent":
   179|                     self.tools[tool_name] = tool_class()
   180|         self.tools["final_answer"] = FinalAnswerTool()
   181|         self.system_prompt = self.initialize_system_prompt()
   182|         self.input_messages = None
   183|         self.logs = []
   184|         self.task = None
   185|         self.logger = AgentLogger(level=verbosity_level)
   186|         self.monitor = Monitor(self.model, self.logger)
   187|         self.step_callbacks = step_callbacks if step_callbacks is not None else []
   188|         self.step_callbacks.append(self.monitor.update_metrics)
   189|     def initialize_system_prompt(self):
   190|         self.system_prompt = format_prompt_with_tools(
   191|             self.tools,
   192|             self.system_prompt_template,
   193|             self.tool_description_template,
   194|         )
   195|         self.system_prompt = format_prompt_with_managed_agents_descriptions(self.system_prompt, self.managed_agents)
   196|         return self.system_prompt
   197|     def write_inner_memory_from_logs(self, summary_mode: bool = False) -> List[Dict[str, str]]:
   198|         """
   199|         Reads past llm_outputs, actions, and observations or errors from the logs into a series of messages
   200|         that can be used as input to the LLM.
   201|         Args:
   202|             summary_mode (`bool`): Whether to write a summary of the logs or the full logs.
   203|         """
   204|         memory = []
   205|         for i, step_log in enumerate(self.logs):
   206|             if isinstance(step_log, SystemPromptStep):
   207|                 if not summary_mode:
   208|                     thought_message = {
   209|                         "role": MessageRole.SYSTEM,
   210|                         "content": [{"type": "text", "text": step_log.system_prompt.strip()}],
   211|                     }
   212|                     memory.append(thought_message)
   213|             elif isinstance(step_log, PlanningStep):
   214|                 thought_message = {
   215|                     "role": MessageRole.ASSISTANT,
   216|                     "content": "[FACTS LIST]:\n" + step_log.facts.strip(),
   217|                 }
   218|                 memory.append(thought_message)
   219|                 if not summary_mode:
   220|                     thought_message = {
   221|                         "role": MessageRole.ASSISTANT,
   222|                         "content": "[PLAN]:\n" + step_log.plan.strip(),
   223|                     }
   224|                     memory.append(thought_message)
   225|             elif isinstance(step_log, TaskStep):
   226|                 task_message = {
   227|                     "role": MessageRole.USER,
   228|                     "content": [{"type": "text", "text": f"New task:\n{step_log.task}"}],
   229|                 }
   230|                 if step_log.task_images:
   231|                     for image in step_log.task_images:
   232|                         task_message["content"].append({"type": "image", "image": image})
   233|                 memory.append(task_message)
   234|             elif isinstance(step_log, ActionStep):
   235|                 if step_log.llm_output is not None and not summary_mode:
   236|                     thought_message = {
   237|                         "role": MessageRole.ASSISTANT,
   238|                         "content": [{"type": "text", "text": step_log.llm_output.strip()}],
   239|                     }
   240|                     memory.append(thought_message)
   241|                 if step_log.tool_calls is not None:
   242|                     tool_call_message = {
   243|                         "role": MessageRole.ASSISTANT,
   244|                         "content": [
   245|                             {
   246|                                 "type": "text",
   247|                                 "text": str(
   248|                                     [
   249|                                         {
   250|                                             "id": tool_call.id,
   251|                                             "type": "function",
   252|                                             "function": {
   253|                                                 "name": tool_call.name,
   254|                                                 "arguments": tool_call.arguments,
   255|                                             },
   256|                                         }
   257|                                         for tool_call in step_log.tool_calls
   258|                                     ]
   259|                                 ),
   260|                             }
   261|                         ],
   262|                     }
   263|                     memory.append(tool_call_message)
   264|                 if step_log.error is not None:
   265|                     error_message = {
   266|                         "role": MessageRole.ASSISTANT,
   267|                         "content": [
   268|                             {
   269|                                 "type": "text",
   270|                                 "text": (
   271|                                     "Error:\n"
   272|                                     + str(step_log.error)
   273|                                     + "\nNow let's retry: take care not to repeat previous errors! If you have retried several times, try a completely different approach.\n"
   274|                                 ),
   275|                             }
   276|                         ],
   277|                     }
   278|                     memory.append(error_message)
   279|                 if step_log.observations is not None:
   280|                     if step_log.tool_calls:
   281|                         tool_call_reference = f"Call id: {(step_log.tool_calls[0].id if getattr(step_log.tool_calls[0], 'id') else 'call_0')}\n"
   282|                     else:
   283|                         tool_call_reference = ""
   284|                     text_observations = f"Observation:\n{step_log.observations}"
   285|                     tool_response_message = {
   286|                         "role": MessageRole.TOOL_RESPONSE,
   287|                         "content": [{"type": "text", "text": tool_call_reference + text_observations}],
   288|                     }
   289|                     memory.append(tool_response_message)
   290|                 if step_log.observations_images:
   291|                     thought_message_image = {
   292|                         "role": MessageRole.USER,
   293|                         "content": [{"type": "text", "text": "Here are the observed images:"}]
   294|                         + [
   295|                             {
   296|                                 "type": "image",
   297|                                 "image": image,
   298|                             }
   299|                             for image in step_log.observations_images
   300|                         ],
   301|                     }
   302|                     memory.append(thought_message_image)
   303|         return memory
   304|     def get_succinct_logs(self):
   305|         return [{key: value for key, value in log.items() if key != "agent_memory"} for log in self.logs]
   306|     def extract_action(self, llm_output: str, split_token: str) -> Tuple[str, str]:
   307|         """
   308|         Parse action from the LLM output
   309|         Args:
   310|             llm_output (`str`): Output of the LLM
   311|             split_token (`str`): Separator for the action. Should match the example in the system prompt.
   312|         """
   313|         try:
   314|             split = llm_output.split(split_token)
   315|             rationale, action = (
   316|                 split[-2],
   317|                 split[-1],
   318|             )  # NOTE: using indexes starting from the end solves for when you have more than one split_token in the output
   319|         except Exception:
   320|             raise AgentParsingError(
   321|                 f"No '{split_token}' token provided in your output.\nYour output:\n{llm_output}\n. Be sure to include an action, prefaced with '{split_token}'!",
   322|                 self.logger,
   323|             )
   324|         return rationale.strip(), action.strip()
   325|     def provide_final_answer(self, task: str, images: Optional[list[str]]) -> str:
   326|         """
   327|         Provide the final answer to the task, based on the logs of the agent's interactions.
   328|         Args:
   329|             task (`str`): Task to perform.
   330|             images (`list[str]`, *optional*): Paths to image(s).
   331|         Returns:
   332|             `str`: Final answer to the task.
   333|         """
   334|         if images:
   335|             self.input_messages[0]["content"] = [
   336|                 {
   337|                     "type": "text",
   338|                     "text": "An agent tried to answer a user query but it got stuck and failed to do so. You are tasked with providing an answer instead. Here is the agent's memory:",
   339|                 }
   340|             ]
   341|             self.input_messages[0]["content"].append({"type": "image"})
   342|             self.input_messages += self.write_inner_memory_from_logs()[1:]
   343|             self.input_messages += [
   344|                 {
   345|                     "role": MessageRole.USER,
   346|                     "content": [
   347|                         {
   348|                             "type": "text",
   349|                             "text": f"Based on the above, please provide an answer to the following user request:\n{task}",
   350|                         }
   351|                     ],
   352|                 }
   353|             ]
   354|         else:
   355|             self.input_messages[0]["content"] = [
   356|                 {
   357|                     "type": "text",
   358|                     "text": "An agent tried to answer a user query but it got stuck and failed to do so. You are tasked with providing an answer instead. Here is the agent's memory:",
   359|                 }
   360|             ]
   361|             self.input_messages += self.write_inner_memory_from_logs()[1:]
   362|             self.input_messages += [
   363|                 {
   364|                     "role": MessageRole.USER,
   365|                     "content": [
   366|                         {
   367|                             "type": "text",
   368|                             "text": f"Based on the above, please provide an answer to the following user request:\n{task}",
   369|                         }
   370|                     ],
   371|                 }
   372|             ]
   373|         try:
   374|             return self.model(self.input_messages).content
   375|         except Exception as e:
   376|             return f"Error in generating final LLM output:\n{e}"
   377|     def execute_tool_call(self, tool_name: str, arguments: Union[Dict[str, str], str]) -> Any:
   378|         """
   379|         Execute tool with the provided input and returns the result.
   380|         This method replaces arguments with the actual values from the state if they refer to state variables.
   381|         Args:
   382|             tool_name (`str`): Name of the Tool to execute (should be one from self.tools).
   383|             arguments (Dict[str, str]): Arguments passed to the Tool.
   384|         """
   385|         available_tools = {**self.tools, **self.managed_agents}
   386|         if tool_name not in available_tools:
   387|             error_msg = f"Unknown tool {tool_name}, should be instead one of {list(available_tools.keys())}."
   388|             raise AgentExecutionError(error_msg, self.logger)
   389|         try:
   390|             if isinstance(arguments, str):
   391|                 if tool_name in self.managed_agents:
   392|                     observation = available_tools[tool_name].__call__(arguments)
   393|                 else:
   394|                     observation = available_tools[tool_name].__call__(arguments, sanitize_inputs_outputs=True)
   395|             elif isinstance(arguments, dict):
   396|                 for key, value in arguments.items():
   397|                     if isinstance(value, str) and value in self.state:
   398|                         arguments[key] = self.state[value]
   399|                 if tool_name in self.managed_agents:
   400|                     observation = available_tools[tool_name].__call__(**arguments)
   401|                 else:
   402|                     observation = available_tools[tool_name].__call__(**arguments, sanitize_inputs_outputs=True)
   403|             else:
   404|                 error_msg = f"Arguments passed to tool should be a dict or string: got a {type(arguments)}."
   405|                 raise AgentExecutionError(error_msg, self.logger)
   406|             return observation
   407|         except Exception as e:
   408|             if tool_name in self.tools:
   409|                 tool_description = get_tool_description_with_args(available_tools[tool_name])
   410|                 error_msg = (
   411|                     f"Error in tool call execution: {e}\nYou should only use this tool with a correct input.\n"
   412|                     f"As a reminder, this tool's description is the following:\n{tool_description}"
   413|                 )
   414|                 raise AgentExecutionError(error_msg, self.logger)
   415|             elif tool_name in self.managed_agents:
   416|                 error_msg = (
   417|                     f"Error in calling team member: {e}\nYou should only ask this team member with a correct request.\n"
   418|                     f"As a reminder, this team member's description is the following:\n{available_tools[tool_name]}"
   419|                 )
   420|                 raise AgentExecutionError(error_msg, self.logger)
   421|     def step(self, log_entry: ActionStep) -> Union[None, Any]:
   422|         """To be implemented in children classes. Should return either None if the step is not final."""
   423|         pass
   424|     def run(
   425|         self,
   426|         task: str,
   427|         stream: bool = False,
   428|         reset: bool = True,
   429|         single_step: bool = False,
   430|         images: Optional[List[str]] = None,
   431|         additional_args: Optional[Dict] = None,
   432|     ):
   433|         """
   434|         Run the agent for the given task.
   435|         Args:
   436|             task (`str`): Task to perform.
   437|             stream (`bool`): Whether to run in a streaming way.
   438|             reset (`bool`): Whether to reset the conversation or keep it going from previous run.
   439|             single_step (`bool`): Whether to run the agent in one-shot fashion.
   440|             images (`list[str]`, *optional*): Paths to image(s).
   441|             additional_args (`dict`): Any other variables that you want to pass to the agent run, for instance images or dataframes. Give them clear names!
   442|         Example:
   443|         ```py
   444|         from smolagents import CodeAgent
   445|         agent = CodeAgent(tools=[])
   446|         agent.run("What is the result of 2 power 3.7384?")
   447|         ```
   448|         """
   449|         self.task = task
   450|         if additional_args is not None:
   451|             self.state.update(additional_args)
   452|             self.task += f"""
   453| You have been provided with these additional arguments, that you can access using the keys as variables in your python code:
   454| {str(additional_args)}."""
   455|         self.initialize_system_prompt()
   456|         system_prompt_step = SystemPromptStep(system_prompt=self.system_prompt)
   457|         if reset:
   458|             self.logs = []
   459|             self.logs.append(system_prompt_step)
   460|             self.monitor.reset()
   461|         else:
   462|             if len(self.logs) > 0:
   463|                 self.logs[0] = system_prompt_step
   464|             else:
   465|                 self.logs.append(system_prompt_step)
   466|         self.logger.log(
   467|             Panel(
   468|                 f"\n[bold]{self.task.strip()}\n",
   469|                 title="[bold]New run",
   470|                 subtitle=f"{type(self.model).__name__} - {(self.model.model_id if hasattr(self.model, 'model_id') else '')}",
   471|                 border_style=YELLOW_HEX,
   472|                 subtitle_align="left",
   473|             ),
   474|             level=LogLevel.INFO,
   475|         )
   476|         self.logs.append(TaskStep(task=self.task, task_images=images))
   477|         if single_step:
   478|             step_start_time = time.time()
   479|             step_log = ActionStep(start_time=step_start_time, observations_images=images)
   480|             step_log.end_time = time.time()
   481|             step_log.duration = step_log.end_time - step_start_time
   482|             result = self.step(step_log)
   483|             return result
   484|         if stream:
   485|             return self._run(task=self.task, images=images)
   486|         return deque(self._run(task=self.task, images=images), maxlen=1)[0]
   487|     def _run(self, task: str, images: List[str] | None = None) -> Generator[str, None, None]:
   488|         """
   489|         Run the agent in streaming mode and returns a generator of all the steps.
   490|         Args:
   491|             task (`str`): Task to perform.
   492|             images (`list[str]`): Paths to image(s).
   493|         """
   494|         final_answer = None
   495|         self.step_number = 0
   496|         while final_answer is None and self.step_number < self.max_steps:
   497|             step_start_time = time.time()
   498|             step_log = ActionStep(
   499|                 step_number=self.step_number,
   500|                 start_time=step_start_time,
   501|                 observations_images=images,
   502|             )
   503|             try:
   504|                 if self.planning_interval is not None and self.step_number % self.planning_interval == 0:
   505|                     self.planning_step(
   506|                         task,
   507|                         is_first_step=(self.step_number == 0),
   508|                         step=self.step_number,
   509|                     )
   510|                 self.logger.log(
   511|                     Rule(
   512|                         f"[bold]Step {self.step_number}",
   513|                         characters="",
   514|                         style=YELLOW_HEX,
   515|                     ),
   516|                     level=LogLevel.INFO,
   517|                 )
   518|                 final_answer = self.step(step_log)
   519|             except AgentError as e:
   520|                 step_log.error = e
   521|             finally:
   522|                 step_log.end_time = time.time()
   523|                 step_log.duration = step_log.end_time - step_start_time
   524|                 self.logs.append(step_log)
   525|                 for callback in self.step_callbacks:
   526|                     if len(inspect.signature(callback).parameters) == 1:
   527|                         callback(step_log)
   528|                     else:
   529|                         callback(step_log=step_log, agent=self)
   530|                 self.step_number += 1
   531|                 yield step_log
   532|         if final_answer is None and self.step_number == self.max_steps:
   533|             error_message = "Reached max steps."
   534|             final_step_log = ActionStep(
   535|                 step_number=self.step_number, error=AgentMaxStepsError(error_message, self.logger)
   536|             )
   537|             self.logs.append(final_step_log)
   538|             final_answer = self.provide_final_answer(task, images)
   539|             self.logger.log(Text(f"Final answer: {final_answer}"), level=LogLevel.INFO)
   540|             final_step_log.action_output = final_answer
   541|             final_step_log.end_time = time.time()
   542|             final_step_log.duration = step_log.end_time - step_start_time
   543|             for callback in self.step_callbacks:
   544|                 if len(inspect.signature(callback).parameters) == 1:
   545|                     callback(final_step_log)
   546|                 else:
   547|                     callback(step_log=final_step_log, agent=self)
   548|             yield final_step_log
   549|         yield handle_agent_output_types(final_answer)
   550|     def planning_step(self, task, is_first_step: bool, step: int) -> None:
   551|         """
   552|         Used periodically by the agent to plan the next steps to reach the objective.
   553|         Args:
   554|             task (`str`): Task to perform.
   555|             is_first_step (`bool`): If this step is not the first one, the plan should be an update over a previous plan.
   556|             step (`int`): The number of the current step, used as an indication for the LLM.
   557|         """
   558|         if is_first_step:
   559|             message_prompt_facts = {
   560|                 "role": MessageRole.SYSTEM,
   561|                 "content": SYSTEM_PROMPT_FACTS,
   562|             }
   563|             message_prompt_task = {
   564|                 "role": MessageRole.USER,
   565|                 "content": f"""Here is the task:
   566| ```
   567| {task}
   568| ```
   569| Now begin!""",
   570|             }
   571|             answer_facts = self.model([message_prompt_facts, message_prompt_task]).content
   572|             message_system_prompt_plan = {
   573|                 "role": MessageRole.SYSTEM,
   574|                 "content": SYSTEM_PROMPT_PLAN,
   575|             }
   576|             message_user_prompt_plan = {
   577|                 "role": MessageRole.USER,
   578|                 "content": USER_PROMPT_PLAN.format(
   579|                     task=task,
   580|                     tool_descriptions=get_tool_descriptions(self.tools, self.tool_description_template),
   581|                     managed_agents_descriptions=(show_agents_descriptions(self.managed_agents)),
   582|                     answer_facts=answer_facts,
   583|                 ),
   584|             }
   585|             answer_plan = self.model(
   586|                 [message_system_prompt_plan, message_user_prompt_plan],
   587|                 stop_sequences=["<end_plan>"],
   588|             ).content
   589|             final_plan_redaction = f"""Here is the plan of action that I will follow to solve the task:
   590| ```
   591| {answer_plan}
   592| ```"""
   593|             final_facts_redaction = f"""Here are the facts that I know so far:
   594| ```
   595| {answer_facts}
   596| ```""".strip()
   597|             self.logs.append(PlanningStep(plan=final_plan_redaction, facts=final_facts_redaction))
   598|             self.logger.log(
   599|                 Rule("[bold]Initial plan", style="orange"),
   600|                 Text(final_plan_redaction),
   601|                 level=LogLevel.INFO,
   602|             )
   603|         else:  # update plan
   604|             agent_memory = self.write_inner_memory_from_logs(
   605|                 summary_mode=False
   606|             )  # This will not log the plan but will log facts
   607|             facts_update_system_prompt = {
   608|                 "role": MessageRole.SYSTEM,
   609|                 "content": SYSTEM_PROMPT_FACTS_UPDATE,
   610|             }
   611|             facts_update_message = {
   612|                 "role": MessageRole.USER,
   613|                 "content": USER_PROMPT_FACTS_UPDATE,
   614|             }
   615|             facts_update = self.model([facts_update_system_prompt] + agent_memory + [facts_update_message]).content
   616|             plan_update_message = {
   617|                 "role": MessageRole.SYSTEM,
   618|                 "content": SYSTEM_PROMPT_PLAN_UPDATE.format(task=task),
   619|             }
   620|             plan_update_message_user = {
   621|                 "role": MessageRole.USER,
   622|                 "content": USER_PROMPT_PLAN_UPDATE.format(
   623|                     task=task,
   624|                     tool_descriptions=get_tool_descriptions(self.tools, self.tool_description_template),
   625|                     managed_agents_descriptions=(show_agents_descriptions(self.managed_agents)),
   626|                     facts_update=facts_update,
   627|                     remaining_steps=(self.max_steps - step),
   628|                 ),
   629|             }
   630|             plan_update = self.model(
   631|                 [plan_update_message] + agent_memory + [plan_update_message_user],
   632|                 stop_sequences=["<end_plan>"],
   633|             ).content
   634|             final_plan_redaction = PLAN_UPDATE_FINAL_PLAN_REDACTION.format(task=task, plan_update=plan_update)
   635|             final_facts_redaction = f"""Here is the updated list of the facts that I know:
   636| ```
   637| {facts_update}
   638| ```"""
   639|             self.logs.append(PlanningStep(plan=final_plan_redaction, facts=final_facts_redaction))
   640|             self.logger.log(
   641|                 Rule("[bold]Updated plan", style="orange"),
   642|                 Text(final_plan_redaction),
   643|                 level=LogLevel.INFO,
   644|             )
   645| class ToolCallingAgent(MultiStepAgent):
   646|     """
   647|     This agent uses JSON-like tool calls, using method `model.get_tool_call` to leverage the LLM engine's tool calling capabilities.
   648|     Args:
   649|         tools (`list[Tool]`): [`Tool`]s that the agent can use.
   650|         model (`Callable[[list[dict[str, str]]], ChatMessage]`): Model that will generate the agent's actions.
   651|         system_prompt (`str`, *optional*): System prompt that will be used to generate the agent's actions.
   652|         planning_interval (`int`, *optional*): Interval at which the agent will run a planning step.
   653|         **kwargs: Additional keyword arguments.
   654|     """
   655|     def __init__(
   656|         self,
   657|         tools: List[Tool],
   658|         model: Callable[[List[Dict[str, str]]], ChatMessage],
   659|         system_prompt: Optional[str] = None,
   660|         planning_interval: Optional[int] = None,
   661|         **kwargs,
   662|     ):
   663|         if system_prompt is None:
   664|             system_prompt = TOOL_CALLING_SYSTEM_PROMPT
   665|         super().__init__(
   666|             tools=tools,
   667|             model=model,
   668|             system_prompt=system_prompt,
   669|             planning_interval=planning_interval,
   670|             **kwargs,
   671|         )
   672|     def step(self, log_entry: ActionStep) -> Union[None, Any]:
   673|         """
   674|         Perform one step in the ReAct framework: the agent thinks, acts, and observes the result.
   675|         Returns None if the step is not final.
   676|         """
   677|         agent_memory = self.write_inner_memory_from_logs()
   678|         self.input_messages = agent_memory
   679|         log_entry.agent_memory = agent_memory.copy()
   680|         try:
   681|             model_message = self.model(
   682|                 self.input_messages,
   683|                 tools_to_call_from=list(self.tools.values()),
   684|                 stop_sequences=["Observation:"],
   685|             )
   686|             if model_message.tool_calls is None or len(model_message.tool_calls) == 0:
   687|                 raise Exception("Model did not call any tools. Call `final_answer` tool to return a final answer.")
   688|             tool_call = model_message.tool_calls[0]
   689|             tool_name, tool_call_id = tool_call.function.name, tool_call.id
   690|             tool_arguments = tool_call.function.arguments
   691|         except Exception as e:
   692|             raise AgentGenerationError(f"Error in generating tool call with model:\n{e}", self.logger)
   693|         log_entry.tool_calls = [ToolCall(name=tool_name, arguments=tool_arguments, id=tool_call_id)]
   694|         self.logger.log(
   695|             Panel(Text(f"Calling tool: '{tool_name}' with arguments: {tool_arguments}")),
   696|             level=LogLevel.INFO,
   697|         )
   698|         if tool_name == "final_answer":
   699|             if isinstance(tool_arguments, dict):
   700|                 if "answer" in tool_arguments:
   701|                     answer = tool_arguments["answer"]
   702|                 else:
   703|                     answer = tool_arguments
   704|             else:
   705|                 answer = tool_arguments
   706|             if (
   707|                 isinstance(answer, str) and answer in self.state.keys()
   708|             ):  # if the answer is a state variable, return the value
   709|                 final_answer = self.state[answer]
   710|                 self.logger.log(
   711|                     f"[bold {YELLOW_HEX}]Final answer:[/bold {YELLOW_HEX}] Extracting key '{answer}' from state to return value '{final_answer}'.",
   712|                     level=LogLevel.INFO,
   713|                 )
   714|             else:
   715|                 final_answer = answer

# --- HUNK 2: Lines 725-962 ---
   725|             observation = self.execute_tool_call(tool_name, tool_arguments)
   726|             observation_type = type(observation)
   727|             if observation_type in [AgentImage, AgentAudio]:
   728|                 if observation_type == AgentImage:
   729|                     observation_name = "image.png"
   730|                 elif observation_type == AgentAudio:
   731|                     observation_name = "audio.mp3"
   732|                 self.state[observation_name] = observation
   733|                 updated_information = f"Stored '{observation_name}' in memory."
   734|             else:
   735|                 updated_information = str(observation).strip()
   736|             self.logger.log(
   737|                 f"Observations: {updated_information.replace('[', '|')}",  # escape potential rich-tag-like components
   738|                 level=LogLevel.INFO,
   739|             )
   740|             log_entry.observations = updated_information
   741|             return None
   742| class CodeAgent(MultiStepAgent):
   743|     """
   744|     In this agent, the tool calls will be formulated by the LLM in code format, then parsed and executed.
   745|     Args:
   746|         tools (`list[Tool]`): [`Tool`]s that the agent can use.
   747|         model (`Callable[[list[dict[str, str]]], ChatMessage]`): Model that will generate the agent's actions.
   748|         system_prompt (`str`, *optional*): System prompt that will be used to generate the agent's actions.
   749|         grammar (`dict[str, str]`, *optional*): Grammar used to parse the LLM output.
   750|         additional_authorized_imports (`list[str]`, *optional*): Additional authorized imports for the agent.
   751|         planning_interval (`int`, *optional*): Interval at which the agent will run a planning step.
   752|         use_e2b_executor (`bool`, default `False`): Whether to use the E2B executor for remote code execution.
   753|         max_print_outputs_length (`int`, *optional*): Maximum length of the print outputs.
   754|         **kwargs: Additional keyword arguments.
   755|     """
   756|     def __init__(
   757|         self,
   758|         tools: List[Tool],
   759|         model: Callable[[List[Dict[str, str]]], ChatMessage],
   760|         system_prompt: Optional[str] = None,
   761|         grammar: Optional[Dict[str, str]] = None,
   762|         additional_authorized_imports: Optional[List[str]] = None,
   763|         planning_interval: Optional[int] = None,
   764|         use_e2b_executor: bool = False,
   765|         max_print_outputs_length: Optional[int] = None,
   766|         **kwargs,
   767|     ):
   768|         if system_prompt is None:
   769|             system_prompt = CODE_SYSTEM_PROMPT
   770|         self.additional_authorized_imports = additional_authorized_imports if additional_authorized_imports else []
   771|         self.authorized_imports = list(set(BASE_BUILTIN_MODULES) | set(self.additional_authorized_imports))
   772|         if "{{authorized_imports}}" not in system_prompt:
   773|             raise ValueError("Tag '{{authorized_imports}}' should be provided in the prompt.")
   774|         super().__init__(
   775|             tools=tools,
   776|             model=model,
   777|             system_prompt=system_prompt,
   778|             grammar=grammar,
   779|             planning_interval=planning_interval,
   780|             **kwargs,
   781|         )
   782|         if "*" in self.additional_authorized_imports:
   783|             self.logger.log(
   784|                 "Caution: you set an authorization for all imports, meaning your agent can decide to import any package it deems necessary. This might raise issues if the package is not installed in your environment.",
   785|                 0,
   786|             )
   787|         if use_e2b_executor and len(self.managed_agents) > 0:
   788|             raise Exception(
   789|                 f"You passed both {use_e2b_executor=} and some managed agents. Managed agents is not yet supported with remote code execution."
   790|             )
   791|         all_tools = {**self.tools, **self.managed_agents}
   792|         if use_e2b_executor:
   793|             self.python_executor = E2BExecutor(
   794|                 self.additional_authorized_imports,
   795|                 list(all_tools.values()),
   796|                 self.logger,
   797|             )
   798|         else:
   799|             self.python_executor = LocalPythonInterpreter(
   800|                 self.additional_authorized_imports,
   801|                 all_tools,
   802|                 max_print_outputs_length=max_print_outputs_length,
   803|             )
   804|     def initialize_system_prompt(self):
   805|         super().initialize_system_prompt()
   806|         self.system_prompt = self.system_prompt.replace(
   807|             "{{authorized_imports}}",
   808|             (
   809|                 "You can import from any package you want."
   810|                 if "*" in self.authorized_imports
   811|                 else str(self.authorized_imports)
   812|             ),
   813|         )
   814|         return self.system_prompt
   815|     def step(self, log_entry: ActionStep) -> Union[None, Any]:
   816|         """
   817|         Perform one step in the ReAct framework: the agent thinks, acts, and observes the result.
   818|         Returns None if the step is not final.
   819|         """
   820|         agent_memory = self.write_inner_memory_from_logs()
   821|         self.input_messages = agent_memory.copy()
   822|         log_entry.agent_memory = agent_memory.copy()
   823|         try:
   824|             additional_args = {"grammar": self.grammar} if self.grammar is not None else {}
   825|             llm_output = self.model(
   826|                 self.input_messages,
   827|                 stop_sequences=["<end_code>", "Observation:"],
   828|                 **additional_args,
   829|             ).content
   830|             log_entry.llm_output = llm_output
   831|         except Exception as e:
   832|             raise AgentGenerationError(f"Error in generating model output:\n{e}", self.logger) from e
   833|         self.logger.log(
   834|             Group(
   835|                 Rule(
   836|                     "[italic]Output message of the LLM:",
   837|                     align="left",
   838|                     style="orange",
   839|                 ),
   840|                 Syntax(
   841|                     llm_output,
   842|                     lexer="markdown",
   843|                     theme="github-dark",
   844|                     word_wrap=True,
   845|                 ),
   846|             ),
   847|             level=LogLevel.DEBUG,
   848|         )
   849|         try:
   850|             code_action = fix_final_answer_code(parse_code_blobs(llm_output))
   851|         except Exception as e:
   852|             error_msg = f"Error in code parsing:\n{e}\nMake sure to provide correct code blobs."
   853|             raise AgentParsingError(error_msg, self.logger)
   854|         log_entry.tool_calls = [
   855|             ToolCall(
   856|                 name="python_interpreter",
   857|                 arguments=code_action,
   858|                 id=f"call_{len(self.logs)}",
   859|             )
   860|         ]
   861|         self.logger.log(
   862|             Panel(
   863|                 Syntax(
   864|                     code_action,
   865|                     lexer="python",
   866|                     theme="monokai",
   867|                     word_wrap=True,
   868|                 ),
   869|                 title="[bold]Executing this code:",
   870|                 title_align="left",
   871|                 box=box.HORIZONTALS,
   872|             ),
   873|             level=LogLevel.INFO,
   874|         )
   875|         observation = ""
   876|         is_final_answer = False
   877|         try:
   878|             output, execution_logs, is_final_answer = self.python_executor(
   879|                 code_action,
   880|                 self.state,
   881|             )
   882|             execution_outputs_console = []
   883|             if len(execution_logs) > 0:
   884|                 execution_outputs_console += [
   885|                     Text("Execution logs:", style="bold"),
   886|                     Text(execution_logs),
   887|                 ]
   888|             observation += "Execution logs:\n" + execution_logs
   889|         except Exception as e:
   890|             error_msg = str(e)
   891|             if "Import of " in error_msg and " is not allowed" in error_msg:
   892|                 self.logger.log(
   893|                     "[bold red]Warning to user: Code execution failed due to an unauthorized import - Consider passing said import under `additional_authorized_imports` when initializing your CodeAgent.",
   894|                     level=LogLevel.INFO,
   895|                 )
   896|             raise AgentExecutionError(error_msg, self.logger)
   897|         truncated_output = truncate_content(str(output))
   898|         observation += "Last output from code snippet:\n" + truncated_output
   899|         log_entry.observations = observation
   900|         execution_outputs_console += [
   901|             Text(
   902|                 f"{('Out - Final answer' if is_final_answer else 'Out')}: {truncated_output}",
   903|                 style=(f"bold {YELLOW_HEX}" if is_final_answer else ""),
   904|             ),
   905|         ]
   906|         self.logger.log(Group(*execution_outputs_console), level=LogLevel.INFO)
   907|         log_entry.action_output = output
   908|         return output if is_final_answer else None
   909| class ManagedAgent:
   910|     """
   911|     ManagedAgent class that manages an agent and provides additional prompting and run summaries.
   912|     Args:
   913|         agent (`object`): The agent to be managed.
   914|         name (`str`): The name of the managed agent.
   915|         description (`str`): A description of the managed agent.
   916|         additional_prompting (`Optional[str]`, *optional*): Additional prompting for the managed agent. Defaults to None.
   917|         provide_run_summary (`bool`, *optional*): Whether to provide a run summary after the agent completes its task. Defaults to False.
   918|         managed_agent_prompt (`Optional[str]`, *optional*): Custom prompt for the managed agent. Defaults to None.
   919|     """
   920|     def __init__(
   921|         self,
   922|         agent,
   923|         name,
   924|         description,
   925|         additional_prompting: Optional[str] = None,
   926|         provide_run_summary: bool = False,
   927|         managed_agent_prompt: Optional[str] = None,
   928|     ):
   929|         self.agent = agent
   930|         self.name = name
   931|         self.description = description
   932|         self.additional_prompting = additional_prompting
   933|         self.provide_run_summary = provide_run_summary
   934|         self.managed_agent_prompt = managed_agent_prompt if managed_agent_prompt else MANAGED_AGENT_PROMPT
   935|     def write_full_task(self, task):
   936|         """Adds additional prompting for the managed agent, like 'add more detail in your answer'."""
   937|         full_task = self.managed_agent_prompt.format(name=self.name, task=task)
   938|         if self.additional_prompting:
   939|             full_task = full_task.replace("\n{additional_prompting}", self.additional_prompting).strip()
   940|         else:
   941|             full_task = full_task.replace("\n{additional_prompting}", "").strip()
   942|         return full_task
   943|     def __call__(self, request, **kwargs):
   944|         full_task = self.write_full_task(request)
   945|         output = self.agent.run(full_task, **kwargs)
   946|         if self.provide_run_summary:
   947|             answer = f"Here is the final answer from your managed agent '{self.name}':\n"
   948|             answer += str(output)
   949|             answer += f"\n\nFor more detail, find below a summary of this agent's work:\nSUMMARY OF WORK FROM AGENT '{self.name}':\n"
   950|             for message in self.agent.write_inner_memory_from_logs(summary_mode=True):
   951|                 content = message["content"]
   952|                 answer += "\n" + truncate_content(str(content)) + "\n---"
   953|             answer += f"\nEND OF SUMMARY OF WORK FROM AGENT '{self.name}'."
   954|             return answer
   955|         else:
   956|             return output
   957| __all__ = [
   958|     "ManagedAgent",
   959|     "MultiStepAgent",
   960|     "CodeAgent",
   961|     "ToolCallingAgent",
   962| ]


# ====================================================================
# FILE: src/smolagents/default_tools.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-226 ---
     1| import re
     2| from dataclasses import dataclass
     3| from typing import Any, Dict, Optional
     4| from .local_python_executor import (
     5|     BASE_BUILTIN_MODULES,
     6|     BASE_PYTHON_TOOLS,
     7|     evaluate_python_code,
     8| )
     9| from .tools import PipelineTool, Tool
    10| from .types import AgentAudio
    11| @dataclass
    12| class PreTool:
    13|     name: str
    14|     inputs: Dict[str, str]
    15|     output_type: type
    16|     task: str
    17|     description: str
    18|     repo_id: str
    19| class PythonInterpreterTool(Tool):
    20|     name = "python_interpreter"
    21|     description = "This is a tool that evaluates python code. It can be used to perform calculations."
    22|     inputs = {
    23|         "code": {
    24|             "type": "string",
    25|             "description": "The python code to run in interpreter",
    26|         }
    27|     }
    28|     output_type = "string"
    29|     def __init__(self, *args, authorized_imports=None, **kwargs):
    30|         if authorized_imports is None:
    31|             self.authorized_imports = list(set(BASE_BUILTIN_MODULES))
    32|         else:
    33|             self.authorized_imports = list(set(BASE_BUILTIN_MODULES) | set(authorized_imports))
    34|         self.inputs = {
    35|             "code": {
    36|                 "type": "string",
    37|                 "description": (
    38|                     "The code snippet to evaluate. All variables used in this snippet must be defined in this same snippet, "
    39|                     f"else you will get an error. This code can only import the following python libraries: {authorized_imports}."
    40|                 ),
    41|             }
    42|         }
    43|         self.base_python_tools = BASE_PYTHON_TOOLS
    44|         self.python_evaluator = evaluate_python_code
    45|         super().__init__(*args, **kwargs)
    46|     def forward(self, code: str) -> str:
    47|         state = {}
    48|         output = str(
    49|             self.python_evaluator(
    50|                 code,
    51|                 state=state,
    52|                 static_tools=self.base_python_tools,
    53|                 authorized_imports=self.authorized_imports,
    54|             )[0]  # The second element is boolean is_final_answer
    55|         )
    56|         return f"Stdout:\n{state['print_outputs']}\nOutput: {output}"
    57| class FinalAnswerTool(Tool):
    58|     name = "final_answer"
    59|     description = "Provides a final answer to the given problem."
    60|     inputs = {"answer": {"type": "any", "description": "The final answer to the problem"}}
    61|     output_type = "any"
    62|     def forward(self, answer: Any) -> Any:
    63|         return answer
    64| class UserInputTool(Tool):
    65|     name = "user_input"
    66|     description = "Asks for user's input on a specific question"
    67|     inputs = {"question": {"type": "string", "description": "The question to ask the user"}}
    68|     output_type = "string"
    69|     def forward(self, question):
    70|         user_input = input(f"{question} => Type your answer here:")
    71|         return user_input
    72| class DuckDuckGoSearchTool(Tool):
    73|     name = "web_search"
    74|     description = """Performs a duckduckgo web search based on your query (think a Google search) then returns the top search results."""
    75|     inputs = {"query": {"type": "string", "description": "The search query to perform."}}
    76|     output_type = "string"
    77|     def __init__(self, *args, max_results=10, **kwargs):
    78|         super().__init__(*args, **kwargs)
    79|         self.max_results = max_results
    80|         try:
    81|             from duckduckgo_search import DDGS
    82|         except ImportError as e:
    83|             raise ImportError(
    84|                 "You must install package `duckduckgo_search` to run this tool: for instance run `pip install duckduckgo-search`."
    85|             ) from e
    86|         self.ddgs = DDGS()
    87|     def forward(self, query: str) -> str:
    88|         results = self.ddgs.text(query, max_results=self.max_results)
    89|         if len(results) == 0:
    90|             raise Exception("No results found! Try a less restrictive/shorter query.")
    91|         postprocessed_results = [f"[{result['title']}]({result['href']})\n{result['body']}" for result in results]
    92|         return "## Search Results\n\n" + "\n\n".join(postprocessed_results)
    93| class GoogleSearchTool(Tool):
    94|     name = "web_search"
    95|     description = """Performs a google web search for your query then returns a string of the top search results."""
    96|     inputs = {
    97|         "query": {"type": "string", "description": "The search query to perform."},
    98|         "filter_year": {
    99|             "type": "integer",
   100|             "description": "Optionally restrict results to a certain year",
   101|             "nullable": True,
   102|         },
   103|     }
   104|     output_type = "string"
   105|     def __init__(self):
   106|         super().__init__(self)
   107|         import os
   108|         self.serpapi_key = os.getenv("SERPAPI_API_KEY")
   109|     def forward(self, query: str, filter_year: Optional[int] = None) -> str:
   110|         import requests
   111|         if self.serpapi_key is None:
   112|             raise ValueError("Missing SerpAPI key. Make sure you have 'SERPAPI_API_KEY' in your env variables.")
   113|         params = {
   114|             "engine": "google",
   115|             "q": query,
   116|             "api_key": self.serpapi_key,
   117|             "google_domain": "google.com",
   118|         }
   119|         if filter_year is not None:
   120|             params["tbs"] = f"cdr:1,cd_min:01/01/{filter_year},cd_max:12/31/{filter_year}"
   121|         response = requests.get("https://serpapi.com/search.json", params=params)
   122|         if response.status_code == 200:
   123|             results = response.json()
   124|         else:
   125|             raise ValueError(response.json())
   126|         if "organic_results" not in results.keys():
   127|             if filter_year is not None:
   128|                 raise Exception(
   129|                     f"'organic_results' key not found for query: '{query}' with filtering on year={filter_year}. Use a less restrictive query or do not filter on year."
   130|                 )
   131|             else:
   132|                 raise Exception(f"'organic_results' key not found for query: '{query}'. Use a less restrictive query.")
   133|         if len(results["organic_results"]) == 0:
   134|             year_filter_message = f" with filter year={filter_year}" if filter_year is not None else ""
   135|             return f"No results found for '{query}'{year_filter_message}. Try with a more general query, or remove the year filter."
   136|         web_snippets = []
   137|         if "organic_results" in results:
   138|             for idx, page in enumerate(results["organic_results"]):
   139|                 date_published = ""
   140|                 if "date" in page:
   141|                     date_published = "\nDate published: " + page["date"]
   142|                 source = ""
   143|                 if "source" in page:
   144|                     source = "\nSource: " + page["source"]
   145|                 snippet = ""
   146|                 if "snippet" in page:
   147|                     snippet = "\n" + page["snippet"]
   148|                 redacted_version = f"{idx}. [{page['title']}]({page['link']}){date_published}{source}\n{snippet}"
   149|                 redacted_version = redacted_version.replace("Your browser can't play this video.", "")
   150|                 web_snippets.append(redacted_version)
   151|         return "## Search Results\n" + "\n\n".join(web_snippets)
   152| class VisitWebpageTool(Tool):
   153|     name = "visit_webpage"
   154|     description = (
   155|         "Visits a webpage at the given url and reads its content as a markdown string. Use this to browse webpages."
   156|     )
   157|     inputs = {
   158|         "url": {
   159|             "type": "string",
   160|             "description": "The url of the webpage to visit.",
   161|         }
   162|     }
   163|     output_type = "string"
   164|     def forward(self, url: str) -> str:
   165|         try:
   166|             import requests
   167|             from markdownify import markdownify
   168|             from requests.exceptions import RequestException
   169|             from smolagents.utils import truncate_content
   170|         except ImportError as e:
   171|             raise ImportError(
   172|                 "You must install packages `markdownify` and `requests` to run this tool: for instance run `pip install markdownify requests`."
   173|             ) from e
   174|         try:
   175|             response = requests.get(url, timeout=20)
   176|             response.raise_for_status()  # Raise an exception for bad status codes
   177|             markdown_content = markdownify(response.text).strip()
   178|             markdown_content = re.sub(r"\n{3,}", "\n\n", markdown_content)
   179|             return truncate_content(markdown_content, 10000)
   180|         except requests.exceptions.Timeout:
   181|             return "The request timed out. Please try again later or check the URL."
   182|         except RequestException as e:
   183|             return f"Error fetching the webpage: {str(e)}"
   184|         except Exception as e:
   185|             return f"An unexpected error occurred: {str(e)}"
   186| class SpeechToTextTool(PipelineTool):
   187|     default_checkpoint = "openai/whisper-large-v3-turbo"
   188|     description = "This is a tool that transcribes an audio into text. It returns the transcribed text."
   189|     name = "transcriber"
   190|     inputs = {
   191|         "audio": {
   192|             "type": "audio",
   193|             "description": "The audio to transcribe. Can be a local path, an url, or a tensor.",
   194|         }
   195|     }
   196|     output_type = "string"
   197|     def __new__(cls):
   198|         from transformers.models.whisper import (
   199|             WhisperForConditionalGeneration,
   200|             WhisperProcessor,
   201|         )
   202|         if not hasattr(cls, "pre_processor_class"):
   203|             cls.pre_processor_class = WhisperProcessor
   204|         if not hasattr(cls, "model_class"):
   205|             cls.model_class = WhisperForConditionalGeneration
   206|         return super().__new__()
   207|     def encode(self, audio):
   208|         audio = AgentAudio(audio).to_raw()
   209|         return self.pre_processor(audio, return_tensors="pt")
   210|     def forward(self, inputs):
   211|         return self.model.generate(inputs["input_features"])
   212|     def decode(self, outputs):
   213|         return self.pre_processor.batch_decode(outputs, skip_special_tokens=True)[0]
   214| TOOL_MAPPING = {
   215|     tool_class.name: tool_class
   216|     for tool_class in [
   217|         PythonInterpreterTool,
   218|         DuckDuckGoSearchTool,
   219|         VisitWebpageTool,
   220|     ]
   221| }
   222| __all__ = [
   223|     "PythonInterpreterTool",
   224|     "FinalAnswerTool",
   225|     "UserInputTool",
   226|     "DuckDuckGoSearchTool",


# ====================================================================
# FILE: src/smolagents/e2b_executor.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-60 ---
     1| import base64
     2| import pickle
     3| import textwrap
     4| from io import BytesIO
     5| from typing import Any, List, Tuple
     6| from PIL import Image
     7| from .tool_validation import validate_tool_attributes
     8| from .tools import Tool
     9| from .utils import BASE_BUILTIN_MODULES, instance_to_source
    10| try:
    11|     from dotenv import load_dotenv
    12|     load_dotenv()
    13| except ModuleNotFoundError:
    14|     pass
    15| class E2BExecutor:
    16|     def __init__(self, additional_imports: List[str], tools: List[Tool], logger):
    17|         try:
    18|             from e2b_code_interpreter import Sandbox
    19|         except ModuleNotFoundError:
    20|             raise ModuleNotFoundError(
    21|                 """Please install 'e2b' extra to use E2BExecutor: `pip install "smolagents[e2b]"`"""
    22|             )
    23|         self.custom_tools = {}
    24|         self.sbx = Sandbox()  # "qywp2ctmu2q7jzprcf4j")
    25|         self.logger = logger
    26|         additional_imports = additional_imports + ["smolagents"]
    27|         if len(additional_imports) > 0:
    28|             execution = self.sbx.commands.run("pip install " + " ".join(additional_imports))
    29|             if execution.error:
    30|                 raise Exception(f"Error installing dependencies: {execution.error}")
    31|             else:
    32|                 logger.log(f"Installation of {additional_imports} succeeded!", 0)
    33|         tool_codes = []
    34|         for tool in tools:
    35|             validate_tool_attributes(tool.__class__, check_imports=False)
    36|             tool_code = instance_to_source(tool, base_cls=Tool)
    37|             tool_code = tool_code.replace("from smolagents.tools import Tool", "")
    38|             tool_code += f"\n{tool.name} = {tool.__class__.__name__}()\n"
    39|             tool_codes.append(tool_code)
    40|         tool_definition_code = "\n".join([f"import {module}" for module in BASE_BUILTIN_MODULES])
    41|         tool_definition_code += textwrap.dedent("""
    42|         class Tool:
    43|             def __call__(self, *args, **kwargs):
    44|                 return self.forward(*args, **kwargs)
    45|             def forward(self, *args, **kwargs):
    46|                 pass # to be implemented in child class
    47|         """)
    48|         tool_definition_code += "\n\n".join(tool_codes)
    49|         tool_definition_execution = self.run_code_raise_errors(tool_definition_code)
    50|         self.logger.log(tool_definition_execution.logs)
    51|     def run_code_raise_errors(self, code: str):
    52|         execution = self.sbx.run_code(
    53|             code,
    54|         )
    55|         if execution.error:
    56|             execution_logs = "\n".join([str(log) for log in execution.logs.stdout])
    57|             logs = execution_logs
    58|             logs += "Executing code yielded an error:"
    59|             logs += execution.error.name
    60|             logs += execution.error.value

# --- HUNK 2: Lines 72-109 ---
    72|             remote_unloading_code = """import pickle
    73| import os
    74| print("File path", os.path.getsize('/home/state.pkl'))
    75| with open('/home/state.pkl', 'rb') as f:
    76|     pickle_dict = pickle.load(f)
    77| locals().update({key: value for key, value in pickle_dict.items()})
    78| """
    79|             execution = self.run_code_raise_errors(remote_unloading_code)
    80|             execution_logs = "\n".join([str(log) for log in execution.logs.stdout])
    81|             self.logger.log(execution_logs, 1)
    82|         execution = self.run_code_raise_errors(code_action)
    83|         execution_logs = "\n".join([str(log) for log in execution.logs.stdout])
    84|         if not execution.results:
    85|             return None, execution_logs
    86|         else:
    87|             for result in execution.results:
    88|                 if result.is_main_result:
    89|                     for attribute_name in ["jpeg", "png"]:
    90|                         if getattr(result, attribute_name) is not None:
    91|                             image_output = getattr(result, attribute_name)
    92|                             decoded_bytes = base64.b64decode(image_output.encode("utf-8"))
    93|                             return Image.open(BytesIO(decoded_bytes)), execution_logs
    94|                     for attribute_name in [
    95|                         "chart",
    96|                         "data",
    97|                         "html",
    98|                         "javascript",
    99|                         "json",
   100|                         "latex",
   101|                         "markdown",
   102|                         "pdf",
   103|                         "svg",
   104|                         "text",
   105|                     ]:
   106|                         if getattr(result, attribute_name) is not None:
   107|                             return getattr(result, attribute_name), execution_logs
   108|             raise ValueError("No main result returned by executor!")
   109| __all__ = ["E2BExecutor"]


# ====================================================================
# FILE: src/smolagents/gradio_ui.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-162 ---
     1| import mimetypes
     2| import os
     3| import re
     4| import shutil
     5| from typing import Optional
     6| from .agents import ActionStep, AgentStepLog, MultiStepAgent
     7| from .types import AgentAudio, AgentImage, AgentText, handle_agent_output_types
     8| from .utils import _is_package_available
     9| def pull_messages_from_step(step_log: AgentStepLog):
    10|     """Extract ChatMessage objects from agent steps"""
    11|     import gradio as gr
    12|     if isinstance(step_log, ActionStep):
    13|         yield gr.ChatMessage(role="assistant", content=step_log.llm_output or "")
    14|         if step_log.tool_calls is not None:
    15|             first_tool_call = step_log.tool_calls[0]
    16|             used_code = first_tool_call.name == "code interpreter"
    17|             content = first_tool_call.arguments
    18|             if used_code:
    19|                 content = f"```py\n{content}\n```"
    20|             yield gr.ChatMessage(
    21|                 role="assistant",
    22|                 metadata={"title": f"截 Used tool {first_tool_call.name}"},
    23|                 content=str(content),
    24|             )
    25|         if step_log.observations is not None:
    26|             yield gr.ChatMessage(role="assistant", content=step_log.observations)
    27|         if step_log.error is not None:
    28|             yield gr.ChatMessage(
    29|                 role="assistant",
    30|                 content=str(step_log.error),
    31|                 metadata={"title": " Error"},
    32|             )
    33| def stream_to_gradio(
    34|     agent,
    35|     task: str,
    36|     reset_agent_memory: bool = False,
    37|     additional_args: Optional[dict] = None,
    38| ):
    39|     """Runs an agent with the given task and streams the messages from the agent as gradio ChatMessages."""
    40|     if not _is_package_available("gradio"):
    41|         raise ModuleNotFoundError(
    42|             "Please install 'gradio' extra to use the GradioUI: `pip install 'smolagents[audio]'`"
    43|         )
    44|     import gradio as gr
    45|     for step_log in agent.run(task, stream=True, reset=reset_agent_memory, additional_args=additional_args):
    46|         for message in pull_messages_from_step(step_log):
    47|             yield message
    48|     final_answer = step_log  # Last log is the run's final_answer
    49|     final_answer = handle_agent_output_types(final_answer)
    50|     if isinstance(final_answer, AgentText):
    51|         yield gr.ChatMessage(
    52|             role="assistant",
    53|             content=f"**Final answer:**\n{final_answer.to_string()}\n",
    54|         )
    55|     elif isinstance(final_answer, AgentImage):
    56|         yield gr.ChatMessage(
    57|             role="assistant",
    58|             content={"path": final_answer.to_string(), "mime_type": "image/png"},
    59|         )
    60|     elif isinstance(final_answer, AgentAudio):
    61|         yield gr.ChatMessage(
    62|             role="assistant",
    63|             content={"path": final_answer.to_string(), "mime_type": "audio/wav"},
    64|         )
    65|     else:
    66|         yield gr.ChatMessage(role="assistant", content=str(final_answer))
    67| class GradioUI:
    68|     """A one-line interface to launch your agent in Gradio"""
    69|     def __init__(self, agent: MultiStepAgent, file_upload_folder: str | None = None):
    70|         if not _is_package_available("gradio"):
    71|             raise ModuleNotFoundError(
    72|                 "Please install 'gradio' extra to use the GradioUI: `pip install 'smolagents[audio]'`"
    73|             )
    74|         self.agent = agent
    75|         self.file_upload_folder = file_upload_folder
    76|         if self.file_upload_folder is not None:
    77|             if not os.path.exists(file_upload_folder):
    78|                 os.mkdir(file_upload_folder)
    79|     def interact_with_agent(self, prompt, messages):
    80|         import gradio as gr
    81|         messages.append(gr.ChatMessage(role="user", content=prompt))
    82|         yield messages
    83|         for msg in stream_to_gradio(self.agent, task=prompt, reset_agent_memory=False):
    84|             messages.append(msg)
    85|             yield messages
    86|         yield messages
    87|     def upload_file(
    88|         self,
    89|         file,
    90|         file_uploads_log,
    91|         allowed_file_types=[
    92|             "application/pdf",
    93|             "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    94|             "text/plain",
    95|         ],
    96|     ):
    97|         """
    98|         Handle file uploads, default allowed types are .pdf, .docx, and .txt
    99|         """
   100|         import gradio as gr
   101|         if file is None:
   102|             return gr.Textbox("No file uploaded", visible=True), file_uploads_log
   103|         try:
   104|             mime_type, _ = mimetypes.guess_type(file.name)
   105|         except Exception as e:
   106|             return gr.Textbox(f"Error: {e}", visible=True), file_uploads_log
   107|         if mime_type not in allowed_file_types:
   108|             return gr.Textbox("File type disallowed", visible=True), file_uploads_log
   109|         original_name = os.path.basename(file.name)
   110|         sanitized_name = re.sub(
   111|             r"[^\w\-.]", "_", original_name
   112|         )  # Replace any non-alphanumeric, non-dash, or non-dot characters with underscores
   113|         type_to_ext = {}
   114|         for ext, t in mimetypes.types_map.items():
   115|             if t not in type_to_ext:
   116|                 type_to_ext[t] = ext
   117|         sanitized_name = sanitized_name.split(".")[:-1]
   118|         sanitized_name.append("" + type_to_ext[mime_type])
   119|         sanitized_name = "".join(sanitized_name)
   120|         file_path = os.path.join(self.file_upload_folder, os.path.basename(sanitized_name))
   121|         shutil.copy(file.name, file_path)
   122|         return gr.Textbox(f"File uploaded: {file_path}", visible=True), file_uploads_log + [file_path]
   123|     def log_user_message(self, text_input, file_uploads_log):
   124|         return (
   125|             text_input
   126|             + (
   127|                 f"\nYou have been provided with these files, which might be helpful or not: {file_uploads_log}"
   128|                 if len(file_uploads_log) > 0
   129|                 else ""
   130|             ),
   131|             "",
   132|         )
   133|     def launch(self):
   134|         import gradio as gr
   135|         with gr.Blocks() as demo:
   136|             stored_messages = gr.State([])
   137|             file_uploads_log = gr.State([])
   138|             chatbot = gr.Chatbot(
   139|                 label="Agent",
   140|                 type="messages",
   141|                 avatar_images=(
   142|                     None,
   143|                     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/mascot_smol.png",
   144|                 ),
   145|                 resizeable=True,
   146|             )
   147|             if self.file_upload_folder is not None:
   148|                 upload_file = gr.File(label="Upload a file")
   149|                 upload_status = gr.Textbox(label="Upload Status", interactive=False, visible=False)
   150|                 upload_file.change(
   151|                     self.upload_file,
   152|                     [upload_file, file_uploads_log],
   153|                     [upload_status, file_uploads_log],
   154|                 )
   155|             text_input = gr.Textbox(lines=1, label="Chat Message")
   156|             text_input.submit(
   157|                 self.log_user_message,
   158|                 [text_input, file_uploads_log],
   159|                 [stored_messages, text_input],
   160|             ).then(self.interact_with_agent, [stored_messages, chatbot], [chatbot])
   161|         demo.launch()
   162| __all__ = ["stream_to_gradio", "GradioUI"]


# ====================================================================
# FILE: src/smolagents/local_python_executor.py
# Total hunks: 14
# ====================================================================
# --- HUNK 1: Lines 1-43 ---
     1| import ast
     2| import builtins
     3| import difflib
     4| import inspect
     5| import math
     6| import re
     7| from collections.abc import Mapping
     8| from importlib import import_module
     9| from types import ModuleType
    10| from typing import Any, Callable, Dict, List, Optional, Tuple
    11| import numpy as np
    12| import pandas as pd
    13| from .utils import BASE_BUILTIN_MODULES, truncate_content
    14| class InterpreterError(ValueError):
    15|     """
    16|     An error raised when the interpreter cannot evaluate a Python expression, due to syntax error or unsupported
    17|     operations.
    18|     """
    19|     pass
    20| ERRORS = {
    21|     name: getattr(builtins, name)
    22|     for name in dir(builtins)
    23|     if isinstance(getattr(builtins, name), type) and issubclass(getattr(builtins, name), BaseException)
    24| }
    25| PRINT_OUTPUTS, DEFAULT_MAX_LEN_OUTPUT = "", 50000
    26| OPERATIONS_COUNT, MAX_OPERATIONS = 0, 10000000
    27| def custom_print(*args):
    28|     return None
    29| BASE_PYTHON_TOOLS = {
    30|     "print": custom_print,
    31|     "isinstance": isinstance,
    32|     "range": range,
    33|     "float": float,
    34|     "int": int,
    35|     "bool": bool,
    36|     "str": str,
    37|     "set": set,
    38|     "list": list,
    39|     "dict": dict,
    40|     "tuple": tuple,
    41|     "round": round,
    42|     "ceil": math.ceil,
    43|     "floor": math.floor,

# --- HUNK 2: Lines 98-1148 ---
    98|     """
    99|     Sometimes an LLM can try to assign a variable to final_answer, which would break the final_answer() tool.
   100|     This function fixes this behaviour by replacing variable assignments to final_answer with final_answer_variable,
   101|     while preserving function calls to final_answer().
   102|     """
   103|     assignment_pattern = r"(?<!\.)(?<!\w)\bfinal_answer\s*="
   104|     if "final_answer(" not in code or not re.search(assignment_pattern, code):
   105|         return code
   106|     assignment_regex = r"(?<!\.)(?<!\w)(\bfinal_answer)(\s*=)"
   107|     code = re.sub(assignment_regex, r"final_answer_variable\2", code)
   108|     variable_regex = r"(?<!\.)(?<!\w)(\bfinal_answer\b)(?!\s*\()"
   109|     code = re.sub(variable_regex, "final_answer_variable", code)
   110|     return code
   111| def evaluate_unaryop(
   112|     expression: ast.UnaryOp,
   113|     state: Dict[str, Any],
   114|     static_tools: Dict[str, Callable],
   115|     custom_tools: Dict[str, Callable],
   116|     authorized_imports: List[str],
   117| ) -> Any:
   118|     operand = evaluate_ast(expression.operand, state, static_tools, custom_tools, authorized_imports)
   119|     if isinstance(expression.op, ast.USub):
   120|         return -operand
   121|     elif isinstance(expression.op, ast.UAdd):
   122|         return operand
   123|     elif isinstance(expression.op, ast.Not):
   124|         return not operand
   125|     elif isinstance(expression.op, ast.Invert):
   126|         return ~operand
   127|     else:
   128|         raise InterpreterError(f"Unary operation {expression.op.__class__.__name__} is not supported.")
   129| def evaluate_lambda(
   130|     lambda_expression: ast.Lambda,
   131|     state: Dict[str, Any],
   132|     static_tools: Dict[str, Callable],
   133|     custom_tools: Dict[str, Callable],
   134|     authorized_imports: List[str],
   135| ) -> Callable:
   136|     args = [arg.arg for arg in lambda_expression.args.args]
   137|     def lambda_func(*values: Any) -> Any:
   138|         new_state = state.copy()
   139|         for arg, value in zip(args, values):
   140|             new_state[arg] = value
   141|         return evaluate_ast(
   142|             lambda_expression.body,
   143|             new_state,
   144|             static_tools,
   145|             custom_tools,
   146|             authorized_imports,
   147|         )
   148|     return lambda_func
   149| def evaluate_while(
   150|     while_loop: ast.While,
   151|     state: Dict[str, Any],
   152|     static_tools: Dict[str, Callable],
   153|     custom_tools: Dict[str, Callable],
   154|     authorized_imports: List[str],
   155| ) -> None:
   156|     max_iterations = 1000
   157|     iterations = 0
   158|     while evaluate_ast(while_loop.test, state, static_tools, custom_tools, authorized_imports):
   159|         for node in while_loop.body:
   160|             try:
   161|                 evaluate_ast(node, state, static_tools, custom_tools, authorized_imports)
   162|             except BreakException:
   163|                 return None
   164|             except ContinueException:
   165|                 break
   166|         iterations += 1
   167|         if iterations > max_iterations:
   168|             raise InterpreterError(f"Maximum number of {max_iterations} iterations in While loop exceeded")
   169|     return None
   170| def create_function(
   171|     func_def: ast.FunctionDef,
   172|     state: Dict[str, Any],
   173|     static_tools: Dict[str, Callable],
   174|     custom_tools: Dict[str, Callable],
   175|     authorized_imports: List[str],
   176| ) -> Callable:
   177|     def new_func(*args: Any, **kwargs: Any) -> Any:
   178|         func_state = state.copy()
   179|         arg_names = [arg.arg for arg in func_def.args.args]
   180|         default_values = [
   181|             evaluate_ast(d, state, static_tools, custom_tools, authorized_imports) for d in func_def.args.defaults
   182|         ]
   183|         defaults = dict(zip(arg_names[-len(default_values) :], default_values))
   184|         for name, value in zip(arg_names, args):
   185|             func_state[name] = value
   186|         for name, value in kwargs.items():
   187|             func_state[name] = value
   188|         if func_def.args.vararg:
   189|             vararg_name = func_def.args.vararg.arg
   190|             func_state[vararg_name] = args
   191|         if func_def.args.kwarg:
   192|             kwarg_name = func_def.args.kwarg.arg
   193|             func_state[kwarg_name] = kwargs
   194|         for name, value in defaults.items():
   195|             if name not in func_state:
   196|                 func_state[name] = value
   197|         if func_def.args.args and func_def.args.args[0].arg == "self":
   198|             if args:
   199|                 func_state["self"] = args[0]
   200|                 func_state["__class__"] = args[0].__class__
   201|         result = None
   202|         try:
   203|             for stmt in func_def.body:
   204|                 result = evaluate_ast(stmt, func_state, static_tools, custom_tools, authorized_imports)
   205|         except ReturnException as e:
   206|             result = e.value
   207|         if func_def.name == "__init__":
   208|             return None
   209|         return result
   210|     return new_func
   211| def evaluate_function_def(
   212|     func_def: ast.FunctionDef,
   213|     state: Dict[str, Any],
   214|     static_tools: Dict[str, Callable],
   215|     custom_tools: Dict[str, Callable],
   216|     authorized_imports: List[str],
   217| ) -> Callable:
   218|     custom_tools[func_def.name] = create_function(func_def, state, static_tools, custom_tools, authorized_imports)
   219|     return custom_tools[func_def.name]
   220| def evaluate_class_def(
   221|     class_def: ast.ClassDef,
   222|     state: Dict[str, Any],
   223|     static_tools: Dict[str, Callable],
   224|     custom_tools: Dict[str, Callable],
   225|     authorized_imports: List[str],
   226| ) -> type:
   227|     class_name = class_def.name
   228|     bases = [evaluate_ast(base, state, static_tools, custom_tools, authorized_imports) for base in class_def.bases]
   229|     class_dict = {}
   230|     for stmt in class_def.body:
   231|         if isinstance(stmt, ast.FunctionDef):
   232|             class_dict[stmt.name] = evaluate_function_def(stmt, state, static_tools, custom_tools, authorized_imports)
   233|         elif isinstance(stmt, ast.Assign):
   234|             for target in stmt.targets:
   235|                 if isinstance(target, ast.Name):
   236|                     class_dict[target.id] = evaluate_ast(
   237|                         stmt.value,
   238|                         state,
   239|                         static_tools,
   240|                         custom_tools,
   241|                         authorized_imports,
   242|                     )
   243|                 elif isinstance(target, ast.Attribute):
   244|                     class_dict[target.attr] = evaluate_ast(
   245|                         stmt.value,
   246|                         state,
   247|                         static_tools,
   248|                         custom_tools,
   249|                         authorized_imports,
   250|                     )
   251|         else:
   252|             raise InterpreterError(f"Unsupported statement in class body: {stmt.__class__.__name__}")
   253|     new_class = type(class_name, tuple(bases), class_dict)
   254|     state[class_name] = new_class
   255|     return new_class
   256| def evaluate_augassign(
   257|     expression: ast.AugAssign,
   258|     state: Dict[str, Any],
   259|     static_tools: Dict[str, Callable],
   260|     custom_tools: Dict[str, Callable],
   261|     authorized_imports: List[str],
   262| ) -> Any:
   263|     def get_current_value(target: ast.AST) -> Any:
   264|         if isinstance(target, ast.Name):
   265|             return state.get(target.id, 0)
   266|         elif isinstance(target, ast.Subscript):
   267|             obj = evaluate_ast(target.value, state, static_tools, custom_tools, authorized_imports)
   268|             key = evaluate_ast(target.slice, state, static_tools, custom_tools, authorized_imports)
   269|             return obj[key]
   270|         elif isinstance(target, ast.Attribute):
   271|             obj = evaluate_ast(target.value, state, static_tools, custom_tools, authorized_imports)
   272|             return getattr(obj, target.attr)
   273|         elif isinstance(target, ast.Tuple):
   274|             return tuple(get_current_value(elt) for elt in target.elts)
   275|         elif isinstance(target, ast.List):
   276|             return [get_current_value(elt) for elt in target.elts]
   277|         else:
   278|             raise InterpreterError("AugAssign not supported for {type(target)} targets.")
   279|     current_value = get_current_value(expression.target)
   280|     value_to_add = evaluate_ast(expression.value, state, static_tools, custom_tools, authorized_imports)
   281|     if isinstance(expression.op, ast.Add):
   282|         if isinstance(current_value, list):
   283|             if not isinstance(value_to_add, list):
   284|                 raise InterpreterError(f"Cannot add non-list value {value_to_add} to a list.")
   285|             current_value += value_to_add
   286|         else:
   287|             current_value += value_to_add
   288|     elif isinstance(expression.op, ast.Sub):
   289|         current_value -= value_to_add
   290|     elif isinstance(expression.op, ast.Mult):
   291|         current_value *= value_to_add
   292|     elif isinstance(expression.op, ast.Div):
   293|         current_value /= value_to_add
   294|     elif isinstance(expression.op, ast.Mod):
   295|         current_value %= value_to_add
   296|     elif isinstance(expression.op, ast.Pow):
   297|         current_value **= value_to_add
   298|     elif isinstance(expression.op, ast.FloorDiv):
   299|         current_value //= value_to_add
   300|     elif isinstance(expression.op, ast.BitAnd):
   301|         current_value &= value_to_add
   302|     elif isinstance(expression.op, ast.BitOr):
   303|         current_value |= value_to_add
   304|     elif isinstance(expression.op, ast.BitXor):
   305|         current_value ^= value_to_add
   306|     elif isinstance(expression.op, ast.LShift):
   307|         current_value <<= value_to_add
   308|     elif isinstance(expression.op, ast.RShift):
   309|         current_value >>= value_to_add
   310|     else:
   311|         raise InterpreterError(f"Operation {type(expression.op).__name__} is not supported.")
   312|     set_value(
   313|         expression.target,
   314|         current_value,
   315|         state,
   316|         static_tools,
   317|         custom_tools,
   318|         authorized_imports,
   319|     )
   320|     return current_value
   321| def evaluate_boolop(
   322|     node: ast.BoolOp,
   323|     state: Dict[str, Any],
   324|     static_tools: Dict[str, Callable],
   325|     custom_tools: Dict[str, Callable],
   326|     authorized_imports: List[str],
   327| ) -> bool:
   328|     if isinstance(node.op, ast.And):
   329|         for value in node.values:
   330|             if not evaluate_ast(value, state, static_tools, custom_tools, authorized_imports):
   331|                 return False
   332|         return True
   333|     elif isinstance(node.op, ast.Or):
   334|         for value in node.values:
   335|             if evaluate_ast(value, state, static_tools, custom_tools, authorized_imports):
   336|                 return True
   337|         return False
   338| def evaluate_binop(
   339|     binop: ast.BinOp,
   340|     state: Dict[str, Any],
   341|     static_tools: Dict[str, Callable],
   342|     custom_tools: Dict[str, Callable],
   343|     authorized_imports: List[str],
   344| ) -> Any:
   345|     left_val = evaluate_ast(binop.left, state, static_tools, custom_tools, authorized_imports)
   346|     right_val = evaluate_ast(binop.right, state, static_tools, custom_tools, authorized_imports)
   347|     if isinstance(binop.op, ast.Add):
   348|         return left_val + right_val
   349|     elif isinstance(binop.op, ast.Sub):
   350|         return left_val - right_val
   351|     elif isinstance(binop.op, ast.Mult):
   352|         return left_val * right_val
   353|     elif isinstance(binop.op, ast.Div):
   354|         return left_val / right_val
   355|     elif isinstance(binop.op, ast.Mod):
   356|         return left_val % right_val
   357|     elif isinstance(binop.op, ast.Pow):
   358|         return left_val**right_val
   359|     elif isinstance(binop.op, ast.FloorDiv):
   360|         return left_val // right_val
   361|     elif isinstance(binop.op, ast.BitAnd):
   362|         return left_val & right_val
   363|     elif isinstance(binop.op, ast.BitOr):
   364|         return left_val | right_val
   365|     elif isinstance(binop.op, ast.BitXor):
   366|         return left_val ^ right_val
   367|     elif isinstance(binop.op, ast.LShift):
   368|         return left_val << right_val
   369|     elif isinstance(binop.op, ast.RShift):
   370|         return left_val >> right_val
   371|     else:
   372|         raise NotImplementedError(f"Binary operation {type(binop.op).__name__} is not implemented.")
   373| def evaluate_assign(
   374|     assign: ast.Assign,
   375|     state: Dict[str, Any],
   376|     static_tools: Dict[str, Callable],
   377|     custom_tools: Dict[str, Callable],
   378|     authorized_imports: List[str],
   379| ) -> Any:
   380|     result = evaluate_ast(assign.value, state, static_tools, custom_tools, authorized_imports)
   381|     if len(assign.targets) == 1:
   382|         target = assign.targets[0]
   383|         set_value(target, result, state, static_tools, custom_tools, authorized_imports)
   384|     else:
   385|         if len(assign.targets) != len(result):
   386|             raise InterpreterError(f"Assign failed: expected {len(result)} values but got {len(assign.targets)}.")
   387|         expanded_values = []
   388|         for tgt in assign.targets:
   389|             if isinstance(tgt, ast.Starred):
   390|                 expanded_values.extend(result)
   391|             else:
   392|                 expanded_values.append(result)
   393|         for tgt, val in zip(assign.targets, expanded_values):
   394|             set_value(tgt, val, state, static_tools, custom_tools, authorized_imports)
   395|     return result
   396| def set_value(
   397|     target: ast.AST,
   398|     value: Any,
   399|     state: Dict[str, Any],
   400|     static_tools: Dict[str, Callable],
   401|     custom_tools: Dict[str, Callable],
   402|     authorized_imports: List[str],
   403| ) -> None:
   404|     if isinstance(target, ast.Name):
   405|         if target.id in static_tools:
   406|             raise InterpreterError(f"Cannot assign to name '{target.id}': doing this would erase the existing tool!")
   407|         state[target.id] = value
   408|     elif isinstance(target, ast.Tuple):
   409|         if not isinstance(value, tuple):
   410|             if hasattr(value, "__iter__") and not isinstance(value, (str, bytes)):
   411|                 value = tuple(value)
   412|             else:
   413|                 raise InterpreterError("Cannot unpack non-tuple value")
   414|         if len(target.elts) != len(value):
   415|             raise InterpreterError("Cannot unpack tuple of wrong size")
   416|         for i, elem in enumerate(target.elts):
   417|             set_value(elem, value[i], state, static_tools, custom_tools, authorized_imports)
   418|     elif isinstance(target, ast.Subscript):
   419|         obj = evaluate_ast(target.value, state, static_tools, custom_tools, authorized_imports)
   420|         key = evaluate_ast(target.slice, state, static_tools, custom_tools, authorized_imports)
   421|         obj[key] = value
   422|     elif isinstance(target, ast.Attribute):
   423|         obj = evaluate_ast(target.value, state, static_tools, custom_tools, authorized_imports)
   424|         setattr(obj, target.attr, value)
   425| def evaluate_call(
   426|     call: ast.Call,
   427|     state: Dict[str, Any],
   428|     static_tools: Dict[str, Callable],
   429|     custom_tools: Dict[str, Callable],
   430|     authorized_imports: List[str],
   431| ) -> Any:
   432|     if not (
   433|         isinstance(call.func, ast.Attribute) or isinstance(call.func, ast.Name) or isinstance(call.func, ast.Subscript)
   434|     ):
   435|         raise InterpreterError(f"This is not a correct function: {call.func}).")
   436|     if isinstance(call.func, ast.Attribute):
   437|         obj = evaluate_ast(call.func.value, state, static_tools, custom_tools, authorized_imports)
   438|         func_name = call.func.attr
   439|         if not hasattr(obj, func_name):
   440|             raise InterpreterError(f"Object {obj} has no attribute {func_name}")
   441|         func = getattr(obj, func_name)
   442|     elif isinstance(call.func, ast.Name):
   443|         func_name = call.func.id
   444|         if func_name in state:
   445|             func = state[func_name]
   446|         elif func_name in static_tools:
   447|             func = static_tools[func_name]
   448|         elif func_name in custom_tools:
   449|             func = custom_tools[func_name]
   450|         elif func_name in ERRORS:
   451|             func = ERRORS[func_name]
   452|         else:
   453|             raise InterpreterError(
   454|                 f"It is not permitted to evaluate other functions than the provided tools or functions defined/imported in previous code (tried to execute {call.func.id})."
   455|             )
   456|     elif isinstance(call.func, ast.Subscript):
   457|         value = evaluate_ast(call.func.value, state, static_tools, custom_tools, authorized_imports)
   458|         index = evaluate_ast(call.func.slice, state, static_tools, custom_tools, authorized_imports)
   459|         if isinstance(value, (list, tuple)):
   460|             func = value[index]
   461|         else:
   462|             raise InterpreterError(f"Cannot subscript object of type {type(value).__name__}")
   463|         if not callable(func):
   464|             raise InterpreterError(f"This is not a correct function: {call.func}).")
   465|         func_name = None
   466|     args = []
   467|     for arg in call.args:
   468|         if isinstance(arg, ast.Starred):
   469|             args.extend(evaluate_ast(arg.value, state, static_tools, custom_tools, authorized_imports))
   470|         else:
   471|             args.append(evaluate_ast(arg, state, static_tools, custom_tools, authorized_imports))
   472|     kwargs = {
   473|         keyword.arg: evaluate_ast(keyword.value, state, static_tools, custom_tools, authorized_imports)
   474|         for keyword in call.keywords
   475|     }
   476|     if func_name == "super":
   477|         if not args:
   478|             if "__class__" in state and "self" in state:
   479|                 return super(state["__class__"], state["self"])
   480|             else:
   481|                 raise InterpreterError("super() needs at least one argument")
   482|         cls = args[0]
   483|         if not isinstance(cls, type):
   484|             raise InterpreterError("super() argument 1 must be type")
   485|         if len(args) == 1:
   486|             return super(cls)
   487|         elif len(args) == 2:
   488|             instance = args[1]
   489|             return super(cls, instance)
   490|         else:
   491|             raise InterpreterError("super() takes at most 2 arguments")
   492|     else:
   493|         if func_name == "print":
   494|             output = " ".join(map(str, args))
   495|             global PRINT_OUTPUTS
   496|             PRINT_OUTPUTS += output + "\n"
   497|             return None
   498|         else:  # Assume it's a callable object
   499|             if (
   500|                 (inspect.getmodule(func) == builtins)
   501|                 and inspect.isbuiltin(func)
   502|                 and (func not in static_tools.values())
   503|             ):
   504|                 raise InterpreterError(
   505|                     f"Invoking a builtin function that has not been explicitly added as a tool is not allowed ({func_name})."
   506|                 )
   507|             return func(*args, **kwargs)
   508| def evaluate_subscript(
   509|     subscript: ast.Subscript,
   510|     state: Dict[str, Any],
   511|     static_tools: Dict[str, Callable],
   512|     custom_tools: Dict[str, Callable],
   513|     authorized_imports: List[str],
   514| ) -> Any:
   515|     index = evaluate_ast(subscript.slice, state, static_tools, custom_tools, authorized_imports)
   516|     value = evaluate_ast(subscript.value, state, static_tools, custom_tools, authorized_imports)
   517|     if isinstance(value, str) and isinstance(index, str):
   518|         raise InterpreterError("You're trying to subscript a string with a string index, which is impossible")
   519|     if isinstance(value, pd.core.indexing._LocIndexer):
   520|         parent_object = value.obj
   521|         return parent_object.loc[index]
   522|     if isinstance(value, pd.core.indexing._iLocIndexer):
   523|         parent_object = value.obj
   524|         return parent_object.iloc[index]
   525|     if isinstance(value, (pd.DataFrame, pd.Series, np.ndarray)):
   526|         return value[index]
   527|     elif isinstance(value, pd.core.groupby.generic.DataFrameGroupBy):
   528|         return value[index]
   529|     elif isinstance(index, slice):
   530|         return value[index]
   531|     elif isinstance(value, (list, tuple)):
   532|         if not (-len(value) <= index < len(value)):
   533|             raise InterpreterError(f"Index {index} out of bounds for list of length {len(value)}")
   534|         return value[int(index)]
   535|     elif isinstance(value, str):
   536|         if not (-len(value) <= index < len(value)):
   537|             raise InterpreterError(f"Index {index} out of bounds for string of length {len(value)}")
   538|         return value[index]
   539|     elif index in value:
   540|         return value[index]
   541|     else:
   542|         error_message = f"Could not index {value} with '{index}'."
   543|         if isinstance(index, str) and isinstance(value, Mapping):
   544|             close_matches = difflib.get_close_matches(index, list(value.keys()))
   545|             if len(close_matches) > 0:
   546|                 error_message += f" Maybe you meant one of these indexes instead: {str(close_matches)}"
   547|         raise InterpreterError(error_message)
   548| def evaluate_name(
   549|     name: ast.Name,
   550|     state: Dict[str, Any],
   551|     static_tools: Dict[str, Callable],
   552|     custom_tools: Dict[str, Callable],
   553|     authorized_imports: List[str],
   554| ) -> Any:
   555|     if name.id in state:
   556|         return state[name.id]
   557|     elif name.id in static_tools:
   558|         return static_tools[name.id]
   559|     elif name.id in custom_tools:
   560|         return custom_tools[name.id]
   561|     elif name.id in ERRORS:
   562|         return ERRORS[name.id]
   563|     close_matches = difflib.get_close_matches(name.id, list(state.keys()))
   564|     if len(close_matches) > 0:
   565|         return state[close_matches[0]]
   566|     raise InterpreterError(f"The variable `{name.id}` is not defined.")
   567| def evaluate_condition(
   568|     condition: ast.Compare,
   569|     state: Dict[str, Any],
   570|     static_tools: Dict[str, Callable],
   571|     custom_tools: Dict[str, Callable],
   572|     authorized_imports: List[str],
   573| ) -> bool:
   574|     left = evaluate_ast(condition.left, state, static_tools, custom_tools, authorized_imports)
   575|     comparators = [
   576|         evaluate_ast(c, state, static_tools, custom_tools, authorized_imports) for c in condition.comparators
   577|     ]
   578|     ops = [type(op) for op in condition.ops]
   579|     result = True
   580|     current_left = left
   581|     for op, comparator in zip(ops, comparators):
   582|         if op == ast.Eq:
   583|             current_result = current_left == comparator
   584|         elif op == ast.NotEq:
   585|             current_result = current_left != comparator
   586|         elif op == ast.Lt:
   587|             current_result = current_left < comparator
   588|         elif op == ast.LtE:
   589|             current_result = current_left <= comparator
   590|         elif op == ast.Gt:
   591|             current_result = current_left > comparator
   592|         elif op == ast.GtE:
   593|             current_result = current_left >= comparator
   594|         elif op == ast.Is:
   595|             current_result = current_left is comparator
   596|         elif op == ast.IsNot:
   597|             current_result = current_left is not comparator
   598|         elif op == ast.In:
   599|             current_result = current_left in comparator
   600|         elif op == ast.NotIn:
   601|             current_result = current_left not in comparator
   602|         else:
   603|             raise InterpreterError(f"Operator not supported: {op}")
   604|         result = result & current_result
   605|         current_left = comparator
   606|         if isinstance(result, bool) and not result:
   607|             break
   608|     return result if isinstance(result, (bool, pd.Series)) else result.all()
   609| def evaluate_if(
   610|     if_statement: ast.If,
   611|     state: Dict[str, Any],
   612|     static_tools: Dict[str, Callable],
   613|     custom_tools: Dict[str, Callable],
   614|     authorized_imports: List[str],
   615| ) -> Any:
   616|     result = None
   617|     test_result = evaluate_ast(if_statement.test, state, static_tools, custom_tools, authorized_imports)
   618|     if test_result:
   619|         for line in if_statement.body:
   620|             line_result = evaluate_ast(line, state, static_tools, custom_tools, authorized_imports)
   621|             if line_result is not None:
   622|                 result = line_result
   623|     else:
   624|         for line in if_statement.orelse:
   625|             line_result = evaluate_ast(line, state, static_tools, custom_tools, authorized_imports)
   626|             if line_result is not None:
   627|                 result = line_result
   628|     return result
   629| def evaluate_for(
   630|     for_loop: ast.For,
   631|     state: Dict[str, Any],
   632|     static_tools: Dict[str, Callable],
   633|     custom_tools: Dict[str, Callable],
   634|     authorized_imports: List[str],
   635| ) -> Any:
   636|     result = None
   637|     iterator = evaluate_ast(for_loop.iter, state, static_tools, custom_tools, authorized_imports)
   638|     for counter in iterator:
   639|         set_value(
   640|             for_loop.target,
   641|             counter,
   642|             state,
   643|             static_tools,
   644|             custom_tools,
   645|             authorized_imports,
   646|         )
   647|         for node in for_loop.body:
   648|             try:
   649|                 line_result = evaluate_ast(node, state, static_tools, custom_tools, authorized_imports)
   650|                 if line_result is not None:
   651|                     result = line_result
   652|             except BreakException:
   653|                 break
   654|             except ContinueException:
   655|                 continue
   656|         else:
   657|             continue
   658|         break
   659|     return result
   660| def evaluate_listcomp(
   661|     listcomp: ast.ListComp,
   662|     state: Dict[str, Any],
   663|     static_tools: Dict[str, Callable],
   664|     custom_tools: Dict[str, Callable],
   665|     authorized_imports: List[str],
   666| ) -> List[Any]:
   667|     def inner_evaluate(generators: List[ast.comprehension], index: int, current_state: Dict[str, Any]) -> List[Any]:
   668|         if index >= len(generators):
   669|             return [
   670|                 evaluate_ast(
   671|                     listcomp.elt,
   672|                     current_state,
   673|                     static_tools,
   674|                     custom_tools,
   675|                     authorized_imports,
   676|                 )
   677|             ]
   678|         generator = generators[index]
   679|         iter_value = evaluate_ast(
   680|             generator.iter,
   681|             current_state,
   682|             static_tools,
   683|             custom_tools,
   684|             authorized_imports,
   685|         )
   686|         result = []
   687|         for value in iter_value:
   688|             new_state = current_state.copy()
   689|             if isinstance(generator.target, ast.Tuple):
   690|                 for idx, elem in enumerate(generator.target.elts):
   691|                     new_state[elem.id] = value[idx]
   692|             else:
   693|                 new_state[generator.target.id] = value
   694|             if all(
   695|                 evaluate_ast(if_clause, new_state, static_tools, custom_tools, authorized_imports)
   696|                 for if_clause in generator.ifs
   697|             ):
   698|                 result.extend(inner_evaluate(generators, index + 1, new_state))
   699|         return result
   700|     return inner_evaluate(listcomp.generators, 0, state)
   701| def evaluate_try(
   702|     try_node: ast.Try,
   703|     state: Dict[str, Any],
   704|     static_tools: Dict[str, Callable],
   705|     custom_tools: Dict[str, Callable],
   706|     authorized_imports: List[str],
   707| ) -> None:
   708|     try:
   709|         for stmt in try_node.body:
   710|             evaluate_ast(stmt, state, static_tools, custom_tools, authorized_imports)
   711|     except Exception as e:
   712|         matched = False
   713|         for handler in try_node.handlers:
   714|             if handler.type is None or isinstance(
   715|                 e,
   716|                 evaluate_ast(handler.type, state, static_tools, custom_tools, authorized_imports),
   717|             ):
   718|                 matched = True
   719|                 if handler.name:
   720|                     state[handler.name] = e
   721|                 for stmt in handler.body:
   722|                     evaluate_ast(stmt, state, static_tools, custom_tools, authorized_imports)
   723|                 break
   724|         if not matched:
   725|             raise e
   726|     else:
   727|         if try_node.orelse:
   728|             for stmt in try_node.orelse:
   729|                 evaluate_ast(stmt, state, static_tools, custom_tools, authorized_imports)
   730|     finally:
   731|         if try_node.finalbody:
   732|             for stmt in try_node.finalbody:
   733|                 evaluate_ast(stmt, state, static_tools, custom_tools, authorized_imports)
   734| def evaluate_raise(
   735|     raise_node: ast.Raise,
   736|     state: Dict[str, Any],
   737|     static_tools: Dict[str, Callable],
   738|     custom_tools: Dict[str, Callable],
   739|     authorized_imports: List[str],
   740| ) -> None:
   741|     if raise_node.exc is not None:
   742|         exc = evaluate_ast(raise_node.exc, state, static_tools, custom_tools, authorized_imports)
   743|     else:
   744|         exc = None
   745|     if raise_node.cause is not None:
   746|         cause = evaluate_ast(raise_node.cause, state, static_tools, custom_tools, authorized_imports)
   747|     else:
   748|         cause = None
   749|     if exc is not None:
   750|         if cause is not None:
   751|             raise exc from cause
   752|         else:
   753|             raise exc
   754|     else:
   755|         raise InterpreterError("Re-raise is not supported without an active exception")
   756| def evaluate_assert(
   757|     assert_node: ast.Assert,
   758|     state: Dict[str, Any],
   759|     static_tools: Dict[str, Callable],
   760|     custom_tools: Dict[str, Callable],
   761|     authorized_imports: List[str],
   762| ) -> None:
   763|     test_result = evaluate_ast(assert_node.test, state, static_tools, custom_tools, authorized_imports)
   764|     if not test_result:
   765|         if assert_node.msg:
   766|             msg = evaluate_ast(assert_node.msg, state, static_tools, custom_tools, authorized_imports)
   767|             raise AssertionError(msg)
   768|         else:
   769|             test_code = ast.unparse(assert_node.test)
   770|             raise AssertionError(f"Assertion failed: {test_code}")
   771| def evaluate_with(
   772|     with_node: ast.With,
   773|     state: Dict[str, Any],
   774|     static_tools: Dict[str, Callable],
   775|     custom_tools: Dict[str, Callable],
   776|     authorized_imports: List[str],
   777| ) -> None:
   778|     contexts = []
   779|     for item in with_node.items:
   780|         context_expr = evaluate_ast(item.context_expr, state, static_tools, custom_tools, authorized_imports)
   781|         if item.optional_vars:
   782|             state[item.optional_vars.id] = context_expr.__enter__()
   783|             contexts.append(state[item.optional_vars.id])
   784|         else:
   785|             context_var = context_expr.__enter__()
   786|             contexts.append(context_var)
   787|     try:
   788|         for stmt in with_node.body:
   789|             evaluate_ast(stmt, state, static_tools, custom_tools, authorized_imports)
   790|     except Exception as e:
   791|         for context in reversed(contexts):
   792|             context.__exit__(type(e), e, e.__traceback__)
   793|         raise
   794|     else:
   795|         for context in reversed(contexts):
   796|             context.__exit__(None, None, None)
   797| def get_safe_module(unsafe_module, dangerous_patterns, visited=None):
   798|     """Creates a safe copy of a module or returns the original if it's a function"""
   799|     if not isinstance(unsafe_module, ModuleType):
   800|         return unsafe_module
   801|     if visited is None:
   802|         visited = set()
   803|     module_id = id(unsafe_module)
   804|     if module_id in visited:
   805|         return unsafe_module  # Return original for circular refs
   806|     visited.add(module_id)
   807|     safe_module = ModuleType(unsafe_module.__name__)
   808|     for attr_name in dir(unsafe_module):
   809|         if any(pattern in f"{unsafe_module.__name__}.{attr_name}" for pattern in dangerous_patterns):
   810|             continue
   811|         attr_value = getattr(unsafe_module, attr_name)
   812|         if isinstance(attr_value, ModuleType):
   813|             attr_value = get_safe_module(attr_value, dangerous_patterns, visited=visited)
   814|         setattr(safe_module, attr_name, attr_value)
   815|     return safe_module
   816| def import_modules(expression, state, authorized_imports):
   817|     dangerous_patterns = (
   818|         "_os",
   819|         "os",
   820|         "subprocess",
   821|         "_subprocess",
   822|         "pty",
   823|         "system",
   824|         "popen",
   825|         "spawn",
   826|         "shutil",
   827|         "sys",
   828|         "pathlib",
   829|         "io",
   830|         "socket",
   831|         "compile",
   832|         "eval",
   833|         "exec",
   834|         "multiprocessing",
   835|     )
   836|     def check_module_authorized(module_name):
   837|         if "*" in authorized_imports:
   838|             return True
   839|         else:
   840|             module_path = module_name.split(".")
   841|             if any([module in dangerous_patterns for module in module_path]):
   842|                 return False
   843|             module_subpaths = [".".join(module_path[:i]) for i in range(1, len(module_path) + 1)]
   844|             return any(subpath in authorized_imports for subpath in module_subpaths)
   845|     if isinstance(expression, ast.Import):
   846|         for alias in expression.names:
   847|             if check_module_authorized(alias.name):
   848|                 raw_module = import_module(alias.name)
   849|                 state[alias.asname or alias.name] = get_safe_module(raw_module, dangerous_patterns)
   850|             else:
   851|                 raise InterpreterError(
   852|                     f"Import of {alias.name} is not allowed. Authorized imports are: {str(authorized_imports)}"
   853|                 )
   854|         return None
   855|     elif isinstance(expression, ast.ImportFrom):
   856|         if check_module_authorized(expression.module):
   857|             module = __import__(expression.module, fromlist=[alias.name for alias in expression.names])
   858|             if expression.names[0].name == "*":  # Handle "from module import *"
   859|                 if hasattr(module, "__all__"):  # If module has __all__, import only those names
   860|                     for name in module.__all__:
   861|                         state[name] = getattr(module, name)
   862|                 else:  # If no __all__, import all public names (those not starting with '_')
   863|                     for name in dir(module):
   864|                         if not name.startswith("_"):
   865|                             state[name] = getattr(module, name)
   866|             else:  # regular from imports
   867|                 for alias in expression.names:
   868|                     if hasattr(module, alias.name):
   869|                         state[alias.asname or alias.name] = getattr(module, alias.name)
   870|                     else:
   871|                         raise InterpreterError(f"Module {expression.module} has no attribute {alias.name}")
   872|         else:
   873|             raise InterpreterError(f"Import from {expression.module} is not allowed.")
   874|         return None
   875| def evaluate_dictcomp(
   876|     dictcomp: ast.DictComp,
   877|     state: Dict[str, Any],
   878|     static_tools: Dict[str, Callable],
   879|     custom_tools: Dict[str, Callable],
   880|     authorized_imports: List[str],
   881| ) -> Dict[Any, Any]:
   882|     result = {}
   883|     for gen in dictcomp.generators:
   884|         iter_value = evaluate_ast(gen.iter, state, static_tools, custom_tools, authorized_imports)
   885|         for value in iter_value:
   886|             new_state = state.copy()
   887|             set_value(
   888|                 gen.target,
   889|                 value,
   890|                 new_state,
   891|                 static_tools,
   892|                 custom_tools,
   893|                 authorized_imports,
   894|             )
   895|             if all(
   896|                 evaluate_ast(if_clause, new_state, static_tools, custom_tools, authorized_imports)
   897|                 for if_clause in gen.ifs
   898|             ):
   899|                 key = evaluate_ast(
   900|                     dictcomp.key,
   901|                     new_state,
   902|                     static_tools,
   903|                     custom_tools,
   904|                     authorized_imports,
   905|                 )
   906|                 val = evaluate_ast(
   907|                     dictcomp.value,
   908|                     new_state,
   909|                     static_tools,
   910|                     custom_tools,
   911|                     authorized_imports,
   912|                 )
   913|                 result[key] = val
   914|     return result
   915| def evaluate_ast(
   916|     expression: ast.AST,
   917|     state: Dict[str, Any],
   918|     static_tools: Dict[str, Callable],
   919|     custom_tools: Dict[str, Callable],
   920|     authorized_imports: List[str] = BASE_BUILTIN_MODULES,
   921| ):
   922|     """
   923|     Evaluate an abstract syntax tree using the content of the variables stored in a state and only evaluating a given
   924|     set of functions.
   925|     This function will recurse through the nodes of the tree provided.
   926|     Args:
   927|         expression (`ast.AST`):
   928|             The code to evaluate, as an abstract syntax tree.
   929|         state (`Dict[str, Any]`):
   930|             A dictionary mapping variable names to values. The `state` is updated if need be when the evaluation
   931|             encounters assignments.
   932|         static_tools (`Dict[str, Callable]`):
   933|             Functions that may be called during the evaluation. Trying to change one of these static_tools will raise an error.
   934|         custom_tools (`Dict[str, Callable]`):
   935|             Functions that may be called during the evaluation. These static_tools can be overwritten.
   936|         authorized_imports (`List[str]`):
   937|             The list of modules that can be imported by the code. By default, only a few safe modules are allowed.
   938|             If it contains "*", it will authorize any import. Use this at your own risk!
   939|     """
   940|     global OPERATIONS_COUNT
   941|     if OPERATIONS_COUNT >= MAX_OPERATIONS:
   942|         raise InterpreterError(
   943|             f"Reached the max number of operations of {MAX_OPERATIONS}. Maybe there is an infinite loop somewhere in the code, or you're just asking too many calculations."
   944|         )
   945|     OPERATIONS_COUNT += 1
   946|     if isinstance(expression, ast.Assign):
   947|         return evaluate_assign(expression, state, static_tools, custom_tools, authorized_imports)
   948|     elif isinstance(expression, ast.AugAssign):
   949|         return evaluate_augassign(expression, state, static_tools, custom_tools, authorized_imports)
   950|     elif isinstance(expression, ast.Call):
   951|         return evaluate_call(expression, state, static_tools, custom_tools, authorized_imports)
   952|     elif isinstance(expression, ast.Constant):
   953|         return expression.value
   954|     elif isinstance(expression, ast.Tuple):
   955|         return tuple(
   956|             evaluate_ast(elt, state, static_tools, custom_tools, authorized_imports) for elt in expression.elts
   957|         )
   958|     elif isinstance(expression, (ast.ListComp, ast.GeneratorExp)):
   959|         return evaluate_listcomp(expression, state, static_tools, custom_tools, authorized_imports)
   960|     elif isinstance(expression, ast.UnaryOp):
   961|         return evaluate_unaryop(expression, state, static_tools, custom_tools, authorized_imports)
   962|     elif isinstance(expression, ast.Starred):
   963|         return evaluate_ast(expression.value, state, static_tools, custom_tools, authorized_imports)
   964|     elif isinstance(expression, ast.BoolOp):
   965|         return evaluate_boolop(expression, state, static_tools, custom_tools, authorized_imports)
   966|     elif isinstance(expression, ast.Break):
   967|         raise BreakException()
   968|     elif isinstance(expression, ast.Continue):
   969|         raise ContinueException()
   970|     elif isinstance(expression, ast.BinOp):
   971|         return evaluate_binop(expression, state, static_tools, custom_tools, authorized_imports)
   972|     elif isinstance(expression, ast.Compare):
   973|         return evaluate_condition(expression, state, static_tools, custom_tools, authorized_imports)
   974|     elif isinstance(expression, ast.Lambda):
   975|         return evaluate_lambda(expression, state, static_tools, custom_tools, authorized_imports)
   976|     elif isinstance(expression, ast.FunctionDef):
   977|         return evaluate_function_def(expression, state, static_tools, custom_tools, authorized_imports)
   978|     elif isinstance(expression, ast.Dict):
   979|         keys = [evaluate_ast(k, state, static_tools, custom_tools, authorized_imports) for k in expression.keys]
   980|         values = [evaluate_ast(v, state, static_tools, custom_tools, authorized_imports) for v in expression.values]
   981|         return dict(zip(keys, values))
   982|     elif isinstance(expression, ast.Expr):
   983|         return evaluate_ast(expression.value, state, static_tools, custom_tools, authorized_imports)
   984|     elif isinstance(expression, ast.For):
   985|         return evaluate_for(expression, state, static_tools, custom_tools, authorized_imports)
   986|     elif isinstance(expression, ast.FormattedValue):
   987|         return evaluate_ast(expression.value, state, static_tools, custom_tools, authorized_imports)
   988|     elif isinstance(expression, ast.If):
   989|         return evaluate_if(expression, state, static_tools, custom_tools, authorized_imports)
   990|     elif hasattr(ast, "Index") and isinstance(expression, ast.Index):
   991|         return evaluate_ast(expression.value, state, static_tools, custom_tools, authorized_imports)
   992|     elif isinstance(expression, ast.JoinedStr):
   993|         return "".join(
   994|             [str(evaluate_ast(v, state, static_tools, custom_tools, authorized_imports)) for v in expression.values]
   995|         )
   996|     elif isinstance(expression, ast.List):
   997|         return [evaluate_ast(elt, state, static_tools, custom_tools, authorized_imports) for elt in expression.elts]
   998|     elif isinstance(expression, ast.Name):
   999|         return evaluate_name(expression, state, static_tools, custom_tools, authorized_imports)
  1000|     elif isinstance(expression, ast.Subscript):
  1001|         return evaluate_subscript(expression, state, static_tools, custom_tools, authorized_imports)
  1002|     elif isinstance(expression, ast.IfExp):
  1003|         test_val = evaluate_ast(expression.test, state, static_tools, custom_tools, authorized_imports)
  1004|         if test_val:
  1005|             return evaluate_ast(expression.body, state, static_tools, custom_tools, authorized_imports)
  1006|         else:
  1007|             return evaluate_ast(expression.orelse, state, static_tools, custom_tools, authorized_imports)
  1008|     elif isinstance(expression, ast.Attribute):
  1009|         value = evaluate_ast(expression.value, state, static_tools, custom_tools, authorized_imports)
  1010|         return getattr(value, expression.attr)
  1011|     elif isinstance(expression, ast.Slice):
  1012|         return slice(
  1013|             evaluate_ast(expression.lower, state, static_tools, custom_tools, authorized_imports)
  1014|             if expression.lower is not None
  1015|             else None,
  1016|             evaluate_ast(expression.upper, state, static_tools, custom_tools, authorized_imports)
  1017|             if expression.upper is not None
  1018|             else None,
  1019|             evaluate_ast(expression.step, state, static_tools, custom_tools, authorized_imports)
  1020|             if expression.step is not None
  1021|             else None,
  1022|         )
  1023|     elif isinstance(expression, ast.DictComp):
  1024|         return evaluate_dictcomp(expression, state, static_tools, custom_tools, authorized_imports)
  1025|     elif isinstance(expression, ast.While):
  1026|         return evaluate_while(expression, state, static_tools, custom_tools, authorized_imports)
  1027|     elif isinstance(expression, (ast.Import, ast.ImportFrom)):
  1028|         return import_modules(expression, state, authorized_imports)
  1029|     elif isinstance(expression, ast.ClassDef):
  1030|         return evaluate_class_def(expression, state, static_tools, custom_tools, authorized_imports)
  1031|     elif isinstance(expression, ast.Try):
  1032|         return evaluate_try(expression, state, static_tools, custom_tools, authorized_imports)
  1033|     elif isinstance(expression, ast.Raise):
  1034|         return evaluate_raise(expression, state, static_tools, custom_tools, authorized_imports)
  1035|     elif isinstance(expression, ast.Assert):
  1036|         return evaluate_assert(expression, state, static_tools, custom_tools, authorized_imports)
  1037|     elif isinstance(expression, ast.With):
  1038|         return evaluate_with(expression, state, static_tools, custom_tools, authorized_imports)
  1039|     elif isinstance(expression, ast.Set):
  1040|         return {evaluate_ast(elt, state, static_tools, custom_tools, authorized_imports) for elt in expression.elts}
  1041|     elif isinstance(expression, ast.Return):
  1042|         raise ReturnException(
  1043|             evaluate_ast(expression.value, state, static_tools, custom_tools, authorized_imports)
  1044|             if expression.value
  1045|             else None
  1046|         )
  1047|     elif isinstance(expression, ast.Pass):
  1048|         return None
  1049|     else:
  1050|         raise InterpreterError(f"{expression.__class__.__name__} is not supported.")
  1051| class FinalAnswerException(Exception):
  1052|     def __init__(self, value):
  1053|         self.value = value
  1054| def evaluate_python_code(
  1055|     code: str,
  1056|     static_tools: Optional[Dict[str, Callable]] = None,
  1057|     custom_tools: Optional[Dict[str, Callable]] = None,
  1058|     state: Optional[Dict[str, Any]] = None,
  1059|     authorized_imports: List[str] = BASE_BUILTIN_MODULES,
  1060|     max_print_outputs_length: int = DEFAULT_MAX_LEN_OUTPUT,
  1061| ):
  1062|     """
  1063|     Evaluate a python expression using the content of the variables stored in a state and only evaluating a given set
  1064|     of functions.
  1065|     This function will recurse through the nodes of the tree provided.
  1066|     Args:
  1067|         code (`str`):
  1068|             The code to evaluate.
  1069|         static_tools (`Dict[str, Callable]`):
  1070|             The functions that may be called during the evaluation. These can also be agents in a multiagent setting.
  1071|             These tools cannot be overwritten in the code: any assignment to their name will raise an error.
  1072|         custom_tools (`Dict[str, Callable]`):
  1073|             The functions that may be called during the evaluation.
  1074|             These tools can be overwritten in the code: any assignment to their name will overwrite them.
  1075|         state (`Dict[str, Any]`):
  1076|             A dictionary mapping variable names to values. The `state` should contain the initial inputs but will be
  1077|             updated by this function to contain all variables as they are evaluated.
  1078|             The print outputs will be stored in the state under the key 'print_outputs'.
  1079|     """
  1080|     try:
  1081|         expression = ast.parse(code)
  1082|     except SyntaxError as e:
  1083|         raise InterpreterError(
  1084|             f"Code execution failed on line {e.lineno} due to: {type(e).__name__}\n"
  1085|             f"{e.text}"
  1086|             f"{' ' * (e.offset or 0)}^\n"
  1087|             f"Error: {str(e)}"
  1088|         )
  1089|     if state is None:
  1090|         state = {}
  1091|     static_tools = static_tools.copy() if static_tools is not None else {}
  1092|     custom_tools = custom_tools if custom_tools is not None else {}
  1093|     result = None
  1094|     global PRINT_OUTPUTS
  1095|     PRINT_OUTPUTS = ""
  1096|     global OPERATIONS_COUNT
  1097|     OPERATIONS_COUNT = 0
  1098|     def final_answer(value):
  1099|         raise FinalAnswerException(value)
  1100|     static_tools["final_answer"] = final_answer
  1101|     try:
  1102|         for node in expression.body:
  1103|             result = evaluate_ast(node, state, static_tools, custom_tools, authorized_imports)
  1104|         state["print_outputs"] = truncate_content(PRINT_OUTPUTS, max_length=max_print_outputs_length)
  1105|         is_final_answer = False
  1106|         return result, is_final_answer
  1107|     except FinalAnswerException as e:
  1108|         state["print_outputs"] = truncate_content(PRINT_OUTPUTS, max_length=max_print_outputs_length)
  1109|         is_final_answer = True
  1110|         return e.value, is_final_answer
  1111|     except Exception as e:
  1112|         exception_type = type(e).__name__
  1113|         error_msg = truncate_content(PRINT_OUTPUTS, max_length=max_print_outputs_length)
  1114|         error_msg = (
  1115|             f"Code execution failed at line '{ast.get_source_segment(code, node)}' due to: {exception_type}:{str(e)}"
  1116|         )
  1117|         raise InterpreterError(error_msg)
  1118| class LocalPythonInterpreter:
  1119|     def __init__(
  1120|         self,
  1121|         additional_authorized_imports: List[str],
  1122|         tools: Dict,
  1123|         max_print_outputs_length: Optional[int] = None,
  1124|     ):
  1125|         self.custom_tools = {}
  1126|         self.state = {}
  1127|         self.max_print_outputs_length = max_print_outputs_length
  1128|         if max_print_outputs_length is None:
  1129|             self.max_print_outputs_length = DEFAULT_MAX_LEN_OUTPUT
  1130|         self.additional_authorized_imports = additional_authorized_imports
  1131|         self.authorized_imports = list(set(BASE_BUILTIN_MODULES) | set(self.additional_authorized_imports))
  1132|         self.static_tools = {
  1133|             **tools,
  1134|             **BASE_PYTHON_TOOLS.copy(),
  1135|         }
  1136|     def __call__(self, code_action: str, additional_variables: Dict) -> Tuple[Any, str, bool]:
  1137|         self.state.update(additional_variables)
  1138|         output, is_final_answer = evaluate_python_code(
  1139|             code_action,
  1140|             static_tools=self.static_tools,
  1141|             custom_tools=self.custom_tools,
  1142|             state=self.state,
  1143|             authorized_imports=self.authorized_imports,
  1144|             max_print_outputs_length=self.max_print_outputs_length,
  1145|         )
  1146|         logs = self.state["print_outputs"]
  1147|         return output, logs, is_final_answer
  1148| __all__ = ["evaluate_python_code", "LocalPythonInterpreter"]


# ====================================================================
# FILE: src/smolagents/models.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 1-40 ---
     1| import json
     2| import logging
     3| import os
     4| import random
     5| from copy import deepcopy
     6| from dataclasses import asdict, dataclass
     7| from enum import Enum
     8| from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union
     9| from huggingface_hub import InferenceClient
    10| from PIL import Image
    11| from transformers import (
    12|     AutoModelForImageTextToText,
    13|     AutoProcessor,
    14|     StoppingCriteriaList,
    15|     is_torch_available,
    16| )
    17| from .tools import Tool
    18| from .utils import _is_package_available, encode_image_base64, make_image_url
    19| if TYPE_CHECKING:
    20|     from transformers import StoppingCriteriaList
    21| logger = logging.getLogger(__name__)
    22| DEFAULT_JSONAGENT_REGEX_GRAMMAR = {
    23|     "type": "regex",
    24|     "value": 'Thought: .+?\\nAction:\\n\\{\\n\\s{4}"action":\\s"[^"\\n]+",\\n\\s{4}"action_input":\\s"[^"\\n]+"\\n\\}\\n<end_code>',
    25| }
    26| DEFAULT_CODEAGENT_REGEX_GRAMMAR = {
    27|     "type": "regex",
    28|     "value": "Thought: .+?\\nCode:\\n```(?:py|python)?\\n(?:.|\\s)+?\\n```<end_code>",
    29| }
    30| def get_dict_from_nested_dataclasses(obj):
    31|     def convert(obj):
    32|         if hasattr(obj, "__dataclass_fields__"):
    33|             return {k: convert(v) for k, v in asdict(obj).items()}
    34|         return obj
    35|     return convert(obj)
    36| @dataclass
    37| class ChatMessageToolCallDefinition:
    38|     arguments: Any
    39|     name: str
    40|     description: Optional[str] = None

# --- HUNK 2: Lines 51-673 ---
    51|     id: str
    52|     type: str
    53|     @classmethod
    54|     def from_hf_api(cls, tool_call) -> "ChatMessageToolCall":
    55|         return cls(
    56|             function=ChatMessageToolCallDefinition.from_hf_api(tool_call.function),
    57|             id=tool_call.id,
    58|             type=tool_call.type,
    59|         )
    60| @dataclass
    61| class ChatMessage:
    62|     role: str
    63|     content: Optional[str] = None
    64|     tool_calls: Optional[List[ChatMessageToolCall]] = None
    65|     def model_dump_json(self):
    66|         return json.dumps(get_dict_from_nested_dataclasses(self))
    67|     @classmethod
    68|     def from_hf_api(cls, message) -> "ChatMessage":
    69|         tool_calls = None
    70|         if getattr(message, "tool_calls", None) is not None:
    71|             tool_calls = [ChatMessageToolCall.from_hf_api(tool_call) for tool_call in message.tool_calls]
    72|         return cls(role=message.role, content=message.content, tool_calls=tool_calls)
    73|     @classmethod
    74|     def from_dict(cls, data: dict) -> "ChatMessage":
    75|         if data.get("tool_calls"):
    76|             tool_calls = [
    77|                 ChatMessageToolCall(
    78|                     function=ChatMessageToolCallDefinition(**tc["function"]), id=tc["id"], type=tc["type"]
    79|                 )
    80|                 for tc in data["tool_calls"]
    81|             ]
    82|             data["tool_calls"] = tool_calls
    83|         return cls(**data)
    84| def parse_json_if_needed(arguments: Union[str, dict]) -> Union[str, dict]:
    85|     if isinstance(arguments, dict):
    86|         return arguments
    87|     else:
    88|         try:
    89|             return json.loads(arguments)
    90|         except Exception:
    91|             return arguments
    92| def parse_tool_args_if_needed(message: ChatMessage) -> ChatMessage:
    93|     for tool_call in message.tool_calls:
    94|         tool_call.function.arguments = parse_json_if_needed(tool_call.function.arguments)
    95|     return message
    96| class MessageRole(str, Enum):
    97|     USER = "user"
    98|     ASSISTANT = "assistant"
    99|     SYSTEM = "system"
   100|     TOOL_CALL = "tool-call"
   101|     TOOL_RESPONSE = "tool-response"
   102|     @classmethod
   103|     def roles(cls):
   104|         return [r.value for r in cls]
   105| tool_role_conversions = {
   106|     MessageRole.TOOL_CALL: MessageRole.ASSISTANT,
   107|     MessageRole.TOOL_RESPONSE: MessageRole.USER,
   108| }
   109| def get_tool_json_schema(tool: Tool) -> Dict:
   110|     properties = deepcopy(tool.inputs)
   111|     required = []
   112|     for key, value in properties.items():
   113|         if value["type"] == "any":
   114|             value["type"] = "string"
   115|         if not ("nullable" in value and value["nullable"]):
   116|             required.append(key)
   117|     return {
   118|         "type": "function",
   119|         "function": {
   120|             "name": tool.name,
   121|             "description": tool.description,
   122|             "parameters": {
   123|                 "type": "object",
   124|                 "properties": properties,
   125|                 "required": required,
   126|             },
   127|         },
   128|     }
   129| def remove_stop_sequences(content: str, stop_sequences: List[str]) -> str:
   130|     for stop_seq in stop_sequences:
   131|         if content[-len(stop_seq) :] == stop_seq:
   132|             content = content[: -len(stop_seq)]
   133|     return content
   134| def get_clean_message_list(
   135|     message_list: List[Dict[str, str]],
   136|     role_conversions: Dict[MessageRole, MessageRole] = {},
   137|     convert_images_to_image_urls: bool = False,
   138|     flatten_messages_as_text: bool = False,
   139| ) -> List[Dict[str, str]]:
   140|     """
   141|     Subsequent messages with the same role will be concatenated to a single message.
   142|     output_message_list is a list of messages that will be used to generate the final message that is chat template compatible with transformers LLM chat template.
   143|     Args:
   144|         message_list (`list[dict[str, str]]`): List of chat messages.
   145|         role_conversions (`dict[MessageRole, MessageRole]`, *optional* ): Mapping to convert roles.
   146|         convert_images_to_image_urls (`bool`, default `False`): Whether to convert images to image URLs.
   147|         flatten_messages_as_text (`bool`, default `False`): Whether to flatten messages as text.
   148|     """
   149|     output_message_list = []
   150|     message_list = deepcopy(message_list)  # Avoid modifying the original list
   151|     for message in message_list:
   152|         role = message["role"]
   153|         if role not in MessageRole.roles():
   154|             raise ValueError(f"Incorrect role {role}, only {MessageRole.roles()} are supported for now.")
   155|         if role in role_conversions:
   156|             message["role"] = role_conversions[role]
   157|         if isinstance(message["content"], list):
   158|             for i, element in enumerate(message["content"]):
   159|                 if element["type"] == "image":
   160|                     assert not flatten_messages_as_text, f"Cannot use images with {flatten_messages_as_text=}"
   161|                     if convert_images_to_image_urls:
   162|                         message["content"][i] = {
   163|                             "type": "image_url",
   164|                             "image_url": {"url": make_image_url(encode_image_base64(element["image"]))},
   165|                         }
   166|                     else:
   167|                         message["content"][i]["image"] = encode_image_base64(element["image"])
   168|         if len(output_message_list) > 0 and message["role"] == output_message_list[-1]["role"]:
   169|             assert isinstance(message["content"], list), "Error: wrong content:" + str(message["content"])
   170|             if flatten_messages_as_text:
   171|                 output_message_list[-1]["content"] += message["content"][0]["text"]
   172|             else:
   173|                 output_message_list[-1]["content"] += message["content"]
   174|         else:
   175|             if flatten_messages_as_text:
   176|                 content = message["content"][0]["text"]
   177|             else:
   178|                 content = message["content"]
   179|             output_message_list.append({"role": message["role"], "content": content})
   180|     return output_message_list
   181| class Model:
   182|     def __init__(self, **kwargs):
   183|         self.last_input_token_count = None
   184|         self.last_output_token_count = None
   185|         kwargs.setdefault("max_tokens", 4096)
   186|         self.kwargs = kwargs
   187|     def _prepare_completion_kwargs(
   188|         self,
   189|         messages: List[Dict[str, str]],
   190|         stop_sequences: Optional[List[str]] = None,
   191|         grammar: Optional[str] = None,
   192|         tools_to_call_from: Optional[List[Tool]] = None,
   193|         custom_role_conversions: Optional[Dict[str, str]] = None,
   194|         convert_images_to_image_urls: bool = False,
   195|         flatten_messages_as_text: bool = False,
   196|         **kwargs,
   197|     ) -> Dict:
   198|         """
   199|         Prepare parameters required for model invocation, handling parameter priorities.
   200|         Parameter priority from high to low:
   201|         1. Explicitly passed kwargs
   202|         2. Specific parameters (stop_sequences, grammar, etc.)
   203|         3. Default values in self.kwargs
   204|         """
   205|         messages = get_clean_message_list(
   206|             messages,
   207|             role_conversions=custom_role_conversions or tool_role_conversions,
   208|             convert_images_to_image_urls=convert_images_to_image_urls,
   209|             flatten_messages_as_text=flatten_messages_as_text,
   210|         )
   211|         completion_kwargs = {
   212|             **self.kwargs,
   213|             "messages": messages,
   214|         }
   215|         if stop_sequences is not None:
   216|             completion_kwargs["stop"] = stop_sequences
   217|         if grammar is not None:
   218|             completion_kwargs["grammar"] = grammar
   219|         if tools_to_call_from:
   220|             completion_kwargs.update(
   221|                 {
   222|                     "tools": [get_tool_json_schema(tool) for tool in tools_to_call_from],
   223|                     "tool_choice": "required",
   224|                 }
   225|             )
   226|         completion_kwargs.update(kwargs)
   227|         return completion_kwargs
   228|     def get_token_counts(self) -> Dict[str, int]:
   229|         return {
   230|             "input_token_count": self.last_input_token_count,
   231|             "output_token_count": self.last_output_token_count,
   232|         }
   233|     def __call__(
   234|         self,
   235|         messages: List[Dict[str, str]],
   236|         stop_sequences: Optional[List[str]] = None,
   237|         grammar: Optional[str] = None,
   238|         tools_to_call_from: Optional[List[Tool]] = None,
   239|         **kwargs,
   240|     ) -> ChatMessage:
   241|         """Process the input messages and return the model's response.
   242|         Parameters:
   243|             messages (`List[Dict[str, str]]`):
   244|                 A list of message dictionaries to be processed. Each dictionary should have the structure `{"role": "user/system", "content": "message content"}`.
   245|             stop_sequences (`List[str]`, *optional*):
   246|                 A list of strings that will stop the generation if encountered in the model's output.
   247|             grammar (`str`, *optional*):
   248|                 The grammar or formatting structure to use in the model's response.
   249|             tools_to_call_from (`List[Tool]`, *optional*):
   250|                 A list of tools that the model can use to generate responses.
   251|             **kwargs:
   252|                 Additional keyword arguments to be passed to the underlying model.
   253|         Returns:
   254|             `ChatMessage`: A chat message object containing the model's response.
   255|         """
   256|         pass  # To be implemented in child classes!
   257| class HfApiModel(Model):
   258|     """A class to interact with Hugging Face's Inference API for language model interaction.
   259|     This model allows you to communicate with Hugging Face's models using the Inference API. It can be used in both serverless mode or with a dedicated endpoint, supporting features like stop sequences and grammar customization.
   260|     Parameters:
   261|         model_id (`str`, *optional*, defaults to `"Qwen/Qwen2.5-Coder-32B-Instruct"`):
   262|             The Hugging Face model ID to be used for inference. This can be a path or model identifier from the Hugging Face model hub.
   263|         token (`str`, *optional*):
   264|             Token used by the Hugging Face API for authentication. This token need to be authorized 'Make calls to the serverless Inference API'.
   265|             If the model is gated (like Llama-3 models), the token also needs 'Read access to contents of all public gated repos you can access'.
   266|             If not provided, the class will try to use environment variable 'HF_TOKEN', else use the token stored in the Hugging Face CLI configuration.
   267|         timeout (`int`, *optional*, defaults to 120):
   268|             Timeout for the API request, in seconds.
   269|         **kwargs:
   270|             Additional keyword arguments to pass to the Hugging Face API.
   271|     Raises:
   272|         ValueError:
   273|             If the model name is not provided.
   274|     Example:
   275|     ```python
   276|     >>> engine = HfApiModel(
   277|     ...     model_id="Qwen/Qwen2.5-Coder-32B-Instruct",
   278|     ...     token="your_hf_token_here",
   279|     ...     max_tokens=5000,
   280|     ... )
   281|     >>> messages = [{"role": "user", "content": "Explain quantum mechanics in simple terms."}]
   282|     >>> response = engine(messages, stop_sequences=["END"])
   283|     >>> print(response)
   284|     "Quantum mechanics is the branch of physics that studies..."
   285|     ```
   286|     """
   287|     def __init__(
   288|         self,
   289|         model_id: str = "Qwen/Qwen2.5-Coder-32B-Instruct",
   290|         token: Optional[str] = None,
   291|         timeout: Optional[int] = 120,
   292|         **kwargs,
   293|     ):
   294|         super().__init__(**kwargs)
   295|         self.model_id = model_id
   296|         if token is None:
   297|             token = os.getenv("HF_TOKEN")
   298|         self.client = InferenceClient(self.model_id, token=token, timeout=timeout)
   299|     def __call__(
   300|         self,
   301|         messages: List[Dict[str, str]],
   302|         stop_sequences: Optional[List[str]] = None,
   303|         grammar: Optional[str] = None,
   304|         tools_to_call_from: Optional[List[Tool]] = None,
   305|         **kwargs,
   306|     ) -> ChatMessage:
   307|         completion_kwargs = self._prepare_completion_kwargs(
   308|             messages=messages,
   309|             stop_sequences=stop_sequences,
   310|             grammar=grammar,
   311|             tools_to_call_from=tools_to_call_from,
   312|             convert_images_to_image_urls=True,
   313|             **kwargs,
   314|         )
   315|         response = self.client.chat_completion(**completion_kwargs)
   316|         self.last_input_token_count = response.usage.prompt_tokens
   317|         self.last_output_token_count = response.usage.completion_tokens
   318|         message = ChatMessage.from_hf_api(response.choices[0].message)
   319|         if tools_to_call_from is not None:
   320|             return parse_tool_args_if_needed(message)
   321|         return message
   322| class TransformersModel(Model):
   323|     """A class to interact with Hugging Face's Inference API for language model interaction.
   324|     This model allows you to communicate with Hugging Face's models using the Inference API. It can be used in both serverless mode or with a dedicated endpoint, supporting features like stop sequences and grammar customization.
   325|     > [!TIP]
   326|     > You must have `transformers` and `torch` installed on your machine. Please run `pip install smolagents[transformers]` if it's not the case.
   327|     Parameters:
   328|         model_id (`str`, *optional*, defaults to `"Qwen/Qwen2.5-Coder-32B-Instruct"`):
   329|             The Hugging Face model ID to be used for inference. This can be a path or model identifier from the Hugging Face model hub.
   330|         device_map (`str`, *optional*):
   331|             The device_map to initialize your model with.
   332|         torch_dtype (`str`, *optional*):
   333|             The torch_dtype to initialize your model with.
   334|         trust_remote_code (bool, default `False`):
   335|             Some models on the Hub require running remote code: for this model, you would have to set this flag to True.
   336|         flatten_messages_as_text (`bool`, default `True`):
   337|             Whether to flatten messages as text: this must be sent to False to use VLMs (as opposed to LLMs for which this flag can be ignored).
   338|             Caution: this parameter is experimental and will be removed in an upcoming PR as we auto-detect VLMs.
   339|         kwargs (dict, *optional*):
   340|             Any additional keyword arguments that you want to use in model.generate(), for instance `max_new_tokens` or `device`.
   341|         **kwargs:
   342|             Additional keyword arguments to pass to `model.generate()`, for instance `max_new_tokens` or `device`.
   343|     Raises:
   344|         ValueError:
   345|             If the model name is not provided.
   346|     Example:
   347|     ```python
   348|     >>> engine = TransformersModel(
   349|     ...     model_id="Qwen/Qwen2.5-Coder-32B-Instruct",
   350|     ...     device="cuda",
   351|     ...     max_new_tokens=5000,
   352|     ... )
   353|     >>> messages = [{"role": "user", "content": "Explain quantum mechanics in simple terms."}]
   354|     >>> response = engine(messages, stop_sequences=["END"])
   355|     >>> print(response)
   356|     "Quantum mechanics is the branch of physics that studies..."
   357|     ```
   358|     """
   359|     def __init__(
   360|         self,
   361|         model_id: Optional[str] = None,
   362|         device_map: Optional[str] = None,
   363|         torch_dtype: Optional[str] = None,
   364|         trust_remote_code: bool = False,
   365|         flatten_messages_as_text: bool = True,
   366|         **kwargs,
   367|     ):
   368|         super().__init__(**kwargs)
   369|         if not is_torch_available() or not _is_package_available("transformers"):
   370|             raise ModuleNotFoundError(
   371|                 "Please install 'transformers' extra to use 'TransformersModel': `pip install 'smolagents[transformers]'`"
   372|             )
   373|         import torch
   374|         from transformers import AutoModelForCausalLM, AutoTokenizer
   375|         default_model_id = "HuggingFaceTB/SmolLM2-1.7B-Instruct"
   376|         if model_id is None:
   377|             model_id = default_model_id
   378|             logger.warning(f"`model_id`not provided, using this default tokenizer for token counts: '{model_id}'")
   379|         self.model_id = model_id
   380|         self.kwargs = kwargs
   381|         if device_map is None:
   382|             device_map = "cuda" if torch.cuda.is_available() else "cpu"
   383|         logger.info(f"Using device: {device_map}")
   384|         try:
   385|             self.model = AutoModelForCausalLM.from_pretrained(
   386|                 model_id,
   387|                 device_map=device_map,
   388|                 torch_dtype=torch_dtype,
   389|                 trust_remote_code=trust_remote_code,
   390|             )
   391|             self.tokenizer = AutoTokenizer.from_pretrained(model_id)
   392|         except ValueError as e:
   393|             if "Unrecognized configuration class" in str(e):
   394|                 self.model = AutoModelForImageTextToText.from_pretrained(model_id, device_map=device_map)
   395|                 self.processor = AutoProcessor.from_pretrained(model_id)
   396|             else:
   397|                 raise e
   398|         except Exception as e:
   399|             logger.warning(
   400|                 f"Failed to load tokenizer and model for {model_id=}: {e}. Loading default tokenizer and model instead from {default_model_id=}."
   401|             )
   402|             self.model_id = default_model_id
   403|             self.tokenizer = AutoTokenizer.from_pretrained(default_model_id)
   404|             self.model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device_map, torch_dtype=torch_dtype)
   405|         self.flatten_messages_as_text = flatten_messages_as_text
   406|     def make_stopping_criteria(self, stop_sequences: List[str], tokenizer) -> "StoppingCriteriaList":
   407|         from transformers import StoppingCriteria, StoppingCriteriaList
   408|         class StopOnStrings(StoppingCriteria):
   409|             def __init__(self, stop_strings: List[str], tokenizer):
   410|                 self.stop_strings = stop_strings
   411|                 self.tokenizer = tokenizer
   412|                 self.stream = ""
   413|             def reset(self):
   414|                 self.stream = ""
   415|             def __call__(self, input_ids, scores, **kwargs):
   416|                 generated = self.tokenizer.decode(input_ids[0][-1], skip_special_tokens=True)
   417|                 self.stream += generated
   418|                 if any([self.stream.endswith(stop_string) for stop_string in self.stop_strings]):
   419|                     return True
   420|                 return False
   421|         return StoppingCriteriaList([StopOnStrings(stop_sequences, tokenizer)])
   422|     def __call__(
   423|         self,
   424|         messages: List[Dict[str, str]],
   425|         stop_sequences: Optional[List[str]] = None,
   426|         grammar: Optional[str] = None,
   427|         tools_to_call_from: Optional[List[Tool]] = None,
   428|         images: Optional[List[Image.Image]] = None,
   429|         **kwargs,
   430|     ) -> ChatMessage:
   431|         completion_kwargs = self._prepare_completion_kwargs(
   432|             messages=messages,
   433|             stop_sequences=stop_sequences,
   434|             grammar=grammar,
   435|             tools_to_call_from=tools_to_call_from,
   436|             flatten_messages_as_text=self.flatten_messages_as_text,
   437|             **kwargs,
   438|         )
   439|         messages = completion_kwargs.pop("messages")
   440|         stop_sequences = completion_kwargs.pop("stop", None)
   441|         max_new_tokens = (
   442|             kwargs.get("max_new_tokens")
   443|             or kwargs.get("max_tokens")
   444|             or self.kwargs.get("max_new_tokens")
   445|             or self.kwargs.get("max_tokens")
   446|         )
   447|         if max_new_tokens:
   448|             completion_kwargs["max_new_tokens"] = max_new_tokens
   449|         if hasattr(self, "processor"):
   450|             images = [Image.open(image) for image in images] if images else None
   451|             prompt_tensor = self.processor.apply_chat_template(
   452|                 messages,
   453|                 tools=[get_tool_json_schema(tool) for tool in tools_to_call_from] if tools_to_call_from else None,
   454|                 return_tensors="pt",
   455|                 tokenize=True,
   456|                 return_dict=True,
   457|                 images=images,
   458|                 add_generation_prompt=True if tools_to_call_from else False,
   459|             )
   460|         else:
   461|             prompt_tensor = self.tokenizer.apply_chat_template(
   462|                 messages,
   463|                 tools=[get_tool_json_schema(tool) for tool in tools_to_call_from] if tools_to_call_from else None,
   464|                 return_tensors="pt",
   465|                 return_dict=True,
   466|                 add_generation_prompt=True if tools_to_call_from else False,
   467|             )
   468|         prompt_tensor = prompt_tensor.to(self.model.device)
   469|         count_prompt_tokens = prompt_tensor["input_ids"].shape[1]
   470|         if stop_sequences:
   471|             stopping_criteria = self.make_stopping_criteria(
   472|                 stop_sequences, tokenizer=self.processor if hasattr(self, "processor") else self.tokenizer
   473|             )
   474|         else:
   475|             stopping_criteria = None
   476|         out = self.model.generate(
   477|             **prompt_tensor,
   478|             stopping_criteria=stopping_criteria,
   479|             **completion_kwargs,
   480|         )
   481|         generated_tokens = out[0, count_prompt_tokens:]
   482|         if hasattr(self, "processor"):
   483|             output = self.processor.decode(generated_tokens, skip_special_tokens=True)
   484|         else:
   485|             output = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
   486|         self.last_input_token_count = count_prompt_tokens
   487|         self.last_output_token_count = len(generated_tokens)
   488|         if stop_sequences is not None:
   489|             output = remove_stop_sequences(output, stop_sequences)
   490|         if tools_to_call_from is None:
   491|             return ChatMessage(role="assistant", content=output)
   492|         else:
   493|             if "Action:" in output:
   494|                 output = output.split("Action:", 1)[1].strip()
   495|             parsed_output = json.loads(output)
   496|             tool_name = parsed_output.get("tool_name")
   497|             tool_arguments = parsed_output.get("tool_arguments")
   498|             return ChatMessage(
   499|                 role="assistant",
   500|                 content="",
   501|                 tool_calls=[
   502|                     ChatMessageToolCall(
   503|                         id="".join(random.choices("0123456789", k=5)),
   504|                         type="function",
   505|                         function=ChatMessageToolCallDefinition(name=tool_name, arguments=tool_arguments),
   506|                     )
   507|                 ],
   508|             )
   509| class LiteLLMModel(Model):
   510|     """This model connects to [LiteLLM](https://www.litellm.ai/) as a gateway to hundreds of LLMs.
   511|     Parameters:
   512|         model_id (`str`):
   513|             The model identifier to use on the server (e.g. "gpt-3.5-turbo").
   514|         api_base (`str`, *optional*):
   515|             The base URL of the OpenAI-compatible API server.
   516|         api_key (`str`, *optional*):
   517|             The API key to use for authentication.
   518|         **kwargs:
   519|             Additional keyword arguments to pass to the OpenAI API.
   520|     """
   521|     def __init__(
   522|         self,
   523|         model_id="anthropic/claude-3-5-sonnet-20240620",
   524|         api_base=None,
   525|         api_key=None,
   526|         **kwargs,
   527|     ):
   528|         try:
   529|             import litellm
   530|         except ModuleNotFoundError:
   531|             raise ModuleNotFoundError(
   532|                 "Please install 'litellm' extra to use LiteLLMModel: `pip install 'smolagents[litellm]'`"
   533|             )
   534|         super().__init__(**kwargs)
   535|         self.model_id = model_id
   536|         litellm.add_function_to_prompt = True
   537|         self.api_base = api_base
   538|         self.api_key = api_key
   539|     def __call__(
   540|         self,
   541|         messages: List[Dict[str, str]],
   542|         stop_sequences: Optional[List[str]] = None,
   543|         grammar: Optional[str] = None,
   544|         tools_to_call_from: Optional[List[Tool]] = None,
   545|         **kwargs,
   546|     ) -> ChatMessage:
   547|         import litellm
   548|         completion_kwargs = self._prepare_completion_kwargs(
   549|             messages=messages,
   550|             stop_sequences=stop_sequences,
   551|             grammar=grammar,
   552|             tools_to_call_from=tools_to_call_from,
   553|             model=self.model_id,
   554|             api_base=self.api_base,
   555|             api_key=self.api_key,
   556|             convert_images_to_image_urls=True,
   557|             **kwargs,
   558|         )
   559|         response = litellm.completion(**completion_kwargs)
   560|         self.last_input_token_count = response.usage.prompt_tokens
   561|         self.last_output_token_count = response.usage.completion_tokens
   562|         message = ChatMessage.from_dict(
   563|             response.choices[0].message.model_dump(include={"role", "content", "tool_calls"})
   564|         )
   565|         if tools_to_call_from is not None:
   566|             return parse_tool_args_if_needed(message)
   567|         return message
   568| class OpenAIServerModel(Model):
   569|     """This model connects to an OpenAI-compatible API server.
   570|     Parameters:
   571|         model_id (`str`):
   572|             The model identifier to use on the server (e.g. "gpt-3.5-turbo").
   573|         api_base (`str`, *optional*):
   574|             The base URL of the OpenAI-compatible API server.
   575|         api_key (`str`, *optional*):
   576|             The API key to use for authentication.
   577|         custom_role_conversions (`dict[str, str]`, *optional*):
   578|             Custom role conversion mapping to convert message roles in others.
   579|             Useful for specific models that do not support specific message roles like "system".
   580|         **kwargs:
   581|             Additional keyword arguments to pass to the OpenAI API.
   582|     """
   583|     def __init__(
   584|         self,
   585|         model_id: str,
   586|         api_base: Optional[str] = None,
   587|         api_key: Optional[str] = None,
   588|         custom_role_conversions: Optional[Dict[str, str]] = None,
   589|         **kwargs,
   590|     ):
   591|         try:
   592|             import openai
   593|         except ModuleNotFoundError:
   594|             raise ModuleNotFoundError(
   595|                 "Please install 'openai' extra to use OpenAIServerModel: `pip install 'smolagents[openai]'`"
   596|             ) from None
   597|         super().__init__(**kwargs)
   598|         self.model_id = model_id
   599|         self.client = openai.OpenAI(
   600|             base_url=api_base,
   601|             api_key=api_key,
   602|         )
   603|         self.custom_role_conversions = custom_role_conversions
   604|     def __call__(
   605|         self,
   606|         messages: List[Dict[str, str]],
   607|         stop_sequences: Optional[List[str]] = None,
   608|         grammar: Optional[str] = None,
   609|         tools_to_call_from: Optional[List[Tool]] = None,
   610|         **kwargs,
   611|     ) -> ChatMessage:
   612|         completion_kwargs = self._prepare_completion_kwargs(
   613|             messages=messages,
   614|             stop_sequences=stop_sequences,
   615|             grammar=grammar,
   616|             tools_to_call_from=tools_to_call_from,
   617|             model=self.model_id,
   618|             custom_role_conversions=self.custom_role_conversions,
   619|             convert_images_to_image_urls=True,
   620|             **kwargs,
   621|         )
   622|         response = self.client.chat.completions.create(**completion_kwargs)
   623|         self.last_input_token_count = response.usage.prompt_tokens
   624|         self.last_output_token_count = response.usage.completion_tokens
   625|         message = ChatMessage.from_dict(
   626|             response.choices[0].message.model_dump(include={"role", "content", "tool_calls"})
   627|         )
   628|         if tools_to_call_from is not None:
   629|             return parse_tool_args_if_needed(message)
   630|         return message
   631| class AzureOpenAIServerModel(OpenAIServerModel):
   632|     """This model connects to an Azure OpenAI deployment.
   633|     Parameters:
   634|         model_id (`str`):
   635|             The model deployment name to use when connecting (e.g. "gpt-4o-mini").
   636|         azure_endpoint (`str`, *optional*):
   637|             The Azure endpoint, including the resource, e.g. `https://example-resource.azure.openai.com/`. If not provided, it will be inferred from the `AZURE_OPENAI_ENDPOINT` environment variable.
   638|         api_key (`str`, *optional*):
   639|             The API key to use for authentication. If not provided, it will be inferred from the `AZURE_OPENAI_API_KEY` environment variable.
   640|         api_version (`str`, *optional*):
   641|             The API version to use. If not provided, it will be inferred from the `OPENAI_API_VERSION` environment variable.
   642|         custom_role_conversions (`dict[str, str]`, *optional*):
   643|             Custom role conversion mapping to convert message roles in others.
   644|             Useful for specific models that do not support specific message roles like "system".
   645|         **kwargs:
   646|             Additional keyword arguments to pass to the Azure OpenAI API.
   647|     """
   648|     def __init__(
   649|         self,
   650|         model_id: str,
   651|         azure_endpoint: Optional[str] = None,
   652|         api_key: Optional[str] = None,
   653|         api_version: Optional[str] = None,
   654|         custom_role_conversions: Optional[Dict[str, str]] = None,
   655|         **kwargs,
   656|     ):
   657|         if api_key is None:
   658|             api_key = os.environ.get("AZURE_OPENAI_API_KEY")
   659|         super().__init__(model_id=model_id, api_key=api_key, custom_role_conversions=custom_role_conversions, **kwargs)
   660|         import openai
   661|         self.client = openai.AzureOpenAI(api_key=api_key, api_version=api_version, azure_endpoint=azure_endpoint)
   662| __all__ = [
   663|     "MessageRole",
   664|     "tool_role_conversions",
   665|     "get_clean_message_list",
   666|     "Model",
   667|     "TransformersModel",
   668|     "HfApiModel",
   669|     "LiteLLMModel",
   670|     "OpenAIServerModel",
   671|     "AzureOpenAIServerModel",
   672|     "ChatMessage",
   673| ]


# ====================================================================
# FILE: src/smolagents/monitoring.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-35 ---
     1| from rich.text import Text
     2| class Monitor:
     3|     def __init__(self, tracked_model, logger):
     4|         self.step_durations = []
     5|         self.tracked_model = tracked_model
     6|         self.logger = logger
     7|         if getattr(self.tracked_model, "last_input_token_count", "Not found") != "Not found":
     8|             self.total_input_token_count = 0
     9|             self.total_output_token_count = 0
    10|     def get_total_token_counts(self):
    11|         return {
    12|             "input": self.total_input_token_count,
    13|             "output": self.total_output_token_count,
    14|         }
    15|     def reset(self):
    16|         self.step_durations = []
    17|         self.total_input_token_count = 0
    18|         self.total_output_token_count = 0
    19|     def update_metrics(self, step_log):
    20|         """Update the metrics of the monitor.
    21|         Args:
    22|             step_log ([`AgentStepLog`]): Step log to update the monitor with.
    23|         """
    24|         step_duration = step_log.duration
    25|         self.step_durations.append(step_duration)
    26|         console_outputs = f"[Step {len(self.step_durations) - 1}: Duration {step_duration:.2f} seconds"
    27|         if getattr(self.tracked_model, "last_input_token_count", None) is not None:
    28|             self.total_input_token_count += self.tracked_model.last_input_token_count
    29|             self.total_output_token_count += self.tracked_model.last_output_token_count
    30|             console_outputs += (
    31|                 f"| Input tokens: {self.total_input_token_count:,} | Output tokens: {self.total_output_token_count:,}"
    32|             )
    33|         console_outputs += "]"
    34|         self.logger.log(Text(console_outputs, style="dim"), level=1)
    35| __all__ = ["Monitor"]


# ====================================================================
# FILE: src/smolagents/prompts.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 69-168 ---
    69| ```<end_code>
    70| ---
    71| Above example were using tools that might not exist for you. You only have access to these tools:
    72| {{tool_names}}
    73| {{managed_agents_descriptions}}
    74| Remember to make sure that variables you use are all defined. In particular don't import packages!
    75| Be sure to provide a 'Code:\n```' sequence before the code and '```<end_code>' after, else you will get an error.
    76| DO NOT pass the arguments as a dict as in 'answer = ask_search_agent({'query': "What is the place where James Bond lives?"})', but use the arguments directly as in 'answer = ask_search_agent(query="What is the place where James Bond lives?")'.
    77| Now Begin! If you solve the task correctly, you will receive a reward of $1,000,000.
    78| """
    79| TOOL_CALLING_SYSTEM_PROMPT = """You are an expert assistant who can solve any task using  tool calls. You will be given a task to solve as best you can.
    80| To do so, you have been given access to the following tools: {{tool_names}}
    81| The tool call you write is an action: after the tool is executed, you will get the result of the tool call as an "observation".
    82| This Action/Observation can repeat N times, you should take several steps when needed.
    83| You can use the result of the previous action as input for the next action.
    84| The observation will always be a string: it can represent a file, like "image_1.jpg".
    85| Then you can use it as input for the next action. You can do it for instance as follows:
    86| Observation: "image_1.jpg"
    87| Action:
    88| {
    89|   "name": "image_transformer",
    90|   "arguments": {"image": "image_1.jpg"}
    91| }
    92| To provide the final answer to the task, use an action blob with "name": "final_answer" tool. It is the only way to complete the task, else you will be stuck on a loop. So your final output should look like this:
    93| Action:
    94| {
    95|   "name": "final_answer",
    96|   "arguments": {"answer": "insert your final answer here"}
    97| }
    98| Here are a few examples using notional tools:
    99| ---
   100| Task: "Generate an image of the oldest person in this document."
   101| Action:
   102| {
   103|   "name": "document_qa",
   104|   "arguments": {"document": "document.pdf", "question": "Who is the oldest person mentioned?"}
   105| }
   106| Observation: "The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland."
   107| Action:
   108| {
   109|   "name": "image_generator",
   110|   "arguments": {"prompt": "A portrait of John Doe, a 55-year-old man living in Canada."}
   111| }
   112| Observation: "image.png"
   113| Action:
   114| {
   115|   "name": "final_answer",
   116|   "arguments": "image.png"
   117| }
   118| ---
   119| Task: "What is the result of the following operation: 5 + 3 + 1294.678?"
   120| Action:
   121| {
   122|     "name": "python_interpreter",
   123|     "arguments": {"code": "5 + 3 + 1294.678"}
   124| }
   125| Observation: 1302.678
   126| Action:
   127| {
   128|   "name": "final_answer",
   129|   "arguments": "1302.678"
   130| }
   131| ---
   132| Task: "Which city has the highest population , Guangzhou or Shanghai?"
   133| Action:
   134| {
   135|     "name": "search",
   136|     "arguments": "Population Guangzhou"
   137| }
   138| Observation: ['Guangzhou has a population of 15 million inhabitants as of 2021.']
   139| Action:
   140| {
   141|     "name": "search",
   142|     "arguments": "Population Shanghai"
   143| }
   144| Observation: '26 million (2019)'
   145| Action:
   146| {
   147|   "name": "final_answer",
   148|   "arguments": "Shanghai"
   149| }
   150| Above example were using notional tools that might not exist for you. You only have access to these tools:
   151| {{tool_descriptions}}
   152| {{managed_agents_descriptions}}
   153| Here are the rules you should always follow to solve your task:
   154| 1. ALWAYS provide a tool call, else you will fail.
   155| 2. Always use the right arguments for the tools. Never use variable names as the action arguments, use the value instead.
   156| 3. Call a tool only when needed: do not call the search agent if you do not need information, try to solve the task yourself.
   157| If no tool call is needed, use final_answer tool to return your answer.
   158| 4. Never re-do a tool call that you previously did with the exact same parameters.
   159| Now Begin! If you solve the task correctly, you will receive a reward of $1,000,000.
   160| """
   161| CODE_SYSTEM_PROMPT = """You are an expert assistant who can solve any task using code blobs. You will be given a task to solve as best you can.
   162| To do so, you have been given access to a list of tools: these tools are basically Python functions which you can call with code.
   163| To solve the task, you must plan forward to proceed in a series of steps, in a cycle of 'Thought:', 'Code:', and 'Observation:' sequences.
   164| At each step, in the 'Thought:' sequence, you should first explain your reasoning towards solving the task and the tools that you want to use.
   165| Then in the 'Code:' sequence, you should write the code in simple Python. The code sequence must end with '<end_code>' sequence.
   166| During each intermediate step, you can use 'print()' to save whatever important information you will then need.
   167| These print outputs will then appear in the 'Observation:' field, which will be available as input for the next step.
   168| In the end you have to return a final answer using the `final_answer` tool.


# ====================================================================
# FILE: src/smolagents/tool_validation.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| import ast
     2| import builtins
     3| import inspect
     4| from typing import Set
     5| from .utils import BASE_BUILTIN_MODULES, get_source
     6| _BUILTIN_NAMES = set(vars(builtins))
     7| class MethodChecker(ast.NodeVisitor):
     8|     """
     9|     Checks that a method
    10|     - only uses defined names
    11|     - contains no local imports (e.g. numpy is ok but local_script is not)
    12|     """
    13|     def __init__(self, class_attributes: Set[str], check_imports: bool = True):
    14|         self.undefined_names = set()
    15|         self.imports = {}
    16|         self.from_imports = {}
    17|         self.assigned_names = set()
    18|         self.arg_names = set()
    19|         self.class_attributes = class_attributes
    20|         self.errors = []
    21|         self.check_imports = check_imports
    22|     def visit_arguments(self, node):
    23|         """Collect function arguments"""
    24|         self.arg_names = {arg.arg for arg in node.args}
    25|         if node.kwarg:

# --- HUNK 2: Lines 93-163 ---
    93|                 or node.func.id in self.class_attributes
    94|                 or node.func.id in self.imports
    95|                 or node.func.id in self.from_imports
    96|                 or node.func.id in self.assigned_names
    97|             ):
    98|                 self.errors.append(f"Name '{node.func.id}' is undefined.")
    99|         self.generic_visit(node)
   100| def validate_tool_attributes(cls, check_imports: bool = True) -> None:
   101|     """
   102|     Validates that a Tool class follows the proper patterns:
   103|     0. __init__ takes no argument (args chosen at init are not traceable so we cannot rebuild the source code for them, make them class attributes!).
   104|     1. About the class:
   105|         - Class attributes should only be strings or dicts
   106|         - Class attributes cannot be complex attributes
   107|     2. About all class methods:
   108|         - Imports must be from packages, not local files
   109|         - All methods must be self-contained
   110|     Raises all errors encountered, if no error returns None.
   111|     """
   112|     errors = []
   113|     source = get_source(cls)
   114|     tree = ast.parse(source)
   115|     if not isinstance(tree.body[0], ast.ClassDef):
   116|         raise ValueError("Source code must define a class")
   117|     if not cls.__init__.__qualname__ == "Tool.__init__":
   118|         sig = inspect.signature(cls.__init__)
   119|         non_self_params = list([arg_name for arg_name in sig.parameters.keys() if arg_name != "self"])
   120|         if len(non_self_params) > 0:
   121|             errors.append(
   122|                 f"This tool has additional args specified in __init__(self): {non_self_params}. Make sure it does not, all values should be hardcoded!"
   123|             )
   124|     class_node = tree.body[0]
   125|     class ClassLevelChecker(ast.NodeVisitor):
   126|         def __init__(self):
   127|             self.imported_names = set()
   128|             self.complex_attributes = set()
   129|             self.class_attributes = set()
   130|             self.in_method = False
   131|         def visit_FunctionDef(self, node):
   132|             old_context = self.in_method
   133|             self.in_method = True
   134|             self.generic_visit(node)
   135|             self.in_method = old_context
   136|         def visit_Assign(self, node):
   137|             if self.in_method:
   138|                 return
   139|             for target in node.targets:
   140|                 if isinstance(target, ast.Name):
   141|                     self.class_attributes.add(target.id)
   142|             if not all(
   143|                 isinstance(val, (ast.Str, ast.Num, ast.Constant, ast.Dict, ast.List, ast.Set))
   144|                 for val in ast.walk(node.value)
   145|             ):
   146|                 for target in node.targets:
   147|                     if isinstance(target, ast.Name):
   148|                         self.complex_attributes.add(target.id)
   149|     class_level_checker = ClassLevelChecker()
   150|     class_level_checker.visit(class_node)
   151|     if class_level_checker.complex_attributes:
   152|         errors.append(
   153|             f"Complex attributes should be defined in __init__, not as class attributes: "
   154|             f"{', '.join(class_level_checker.complex_attributes)}"
   155|         )
   156|     for node in class_node.body:
   157|         if isinstance(node, ast.FunctionDef):
   158|             method_checker = MethodChecker(class_level_checker.class_attributes, check_imports=check_imports)
   159|             method_checker.visit(node)
   160|             errors += [f"- {node.name}: {error}" for error in method_checker.errors]
   161|     if errors:
   162|         raise ValueError("Tool validation failed:\n" + "\n".join(errors))
   163|     return


# ====================================================================
# FILE: src/smolagents/tools.py
# Total hunks: 12
# ====================================================================
# --- HUNK 1: Lines 1-71 ---
     1| import ast
     2| import importlib
     3| import inspect
     4| import json
     5| import logging
     6| import os
     7| import sys
     8| import tempfile
     9| import textwrap
    10| from contextlib import contextmanager
    11| from functools import lru_cache, wraps
    12| from pathlib import Path
    13| from typing import Callable, Dict, List, Optional, Union
    14| from huggingface_hub import (
    15|     create_repo,
    16|     get_collection,
    17|     hf_hub_download,
    18|     metadata_update,
    19|     upload_folder,
    20| )
    21| from huggingface_hub.utils import is_torch_available
    22| from packaging import version
    23| from ._function_type_hints_utils import (
    24|     TypeHintParsingException,
    25|     _convert_type_hints_to_json_schema,
    26|     get_imports,
    27|     get_json_schema,
    28| )
    29| from .tool_validation import MethodChecker, validate_tool_attributes
    30| from .types import handle_agent_input_types, handle_agent_output_types
    31| from .utils import _is_package_available, _is_pillow_available, get_source, instance_to_source
    32| logger = logging.getLogger(__name__)
    33| def validate_after_init(cls):
    34|     original_init = cls.__init__
    35|     @wraps(original_init)
    36|     def new_init(self, *args, **kwargs):
    37|         original_init(self, *args, **kwargs)
    38|         self.validate_arguments()
    39|     cls.__init__ = new_init
    40|     return cls
    41| AUTHORIZED_TYPES = [
    42|     "string",
    43|     "boolean",
    44|     "integer",
    45|     "number",
    46|     "image",
    47|     "audio",
    48|     "array",
    49|     "object",
    50|     "any",
    51|     "null",
    52| ]
    53| CONVERSION_DICT = {"str": "string", "int": "integer", "float": "number"}
    54| class Tool:
    55|     """
    56|     A base class for the functions used by the agent. Subclass this and implement the `forward` method as well as the
    57|     following class attributes:
    58|     - **description** (`str`) -- A short description of what your tool does, the inputs it expects and the output(s) it
    59|       will return. For instance 'This is a tool that downloads a file from a `url`. It takes the `url` as input, and
    60|       returns the text contained in the file'.
    61|     - **name** (`str`) -- A performative name that will be used for your tool in the prompt to the agent. For instance
    62|       `"text-classifier"` or `"image_generator"`.
    63|     - **inputs** (`Dict[str, Dict[str, Union[str, type]]]`) -- The dict of modalities expected for the inputs.
    64|       It has one `type`key and a `description`key.
    65|       This is used by `launch_gradio_demo` or to make a nice space from your tool, and also can be used in the generated
    66|       description for your tool.
    67|     - **output_type** (`type`) -- The type of the tool output. This is used by `launch_gradio_demo`
    68|       or to make a nice space from your tool, and also can be used in the generated description for your tool.
    69|     You can also override the method [`~Tool.setup`] if your tool has an expensive operation to perform before being
    70|     usable (such as loading a model). [`~Tool.setup`] will be called the first time you use your tool, but not at
    71|     instantiation.

# --- HUNK 2: Lines 78-144 ---
    78|         self.is_initialized = False
    79|     def __init_subclass__(cls, **kwargs):
    80|         super().__init_subclass__(**kwargs)
    81|         validate_after_init(cls)
    82|     def validate_arguments(self):
    83|         required_attributes = {
    84|             "description": str,
    85|             "name": str,
    86|             "inputs": dict,
    87|             "output_type": str,
    88|         }
    89|         for attr, expected_type in required_attributes.items():
    90|             attr_value = getattr(self, attr, None)
    91|             if attr_value is None:
    92|                 raise TypeError(f"You must set an attribute {attr}.")
    93|             if not isinstance(attr_value, expected_type):
    94|                 raise TypeError(
    95|                     f"Attribute {attr} should have type {expected_type.__name__}, got {type(attr_value)} instead."
    96|                 )
    97|         for input_name, input_content in self.inputs.items():
    98|             assert isinstance(input_content, dict), f"Input '{input_name}' should be a dictionary."
    99|             assert "type" in input_content and "description" in input_content, (
   100|                 f"Input '{input_name}' should have keys 'type' and 'description', has only {list(input_content.keys())}."
   101|             )
   102|             if input_content["type"] not in AUTHORIZED_TYPES:
   103|                 raise Exception(
   104|                     f"Input '{input_name}': type '{input_content['type']}' is not an authorized value, should be one of {AUTHORIZED_TYPES}."
   105|                 )
   106|         assert getattr(self, "output_type", None) in AUTHORIZED_TYPES
   107|         if not (
   108|             hasattr(self, "skip_forward_signature_validation")
   109|             and getattr(self, "skip_forward_signature_validation") is True
   110|         ):
   111|             signature = inspect.signature(self.forward)
   112|             if not set(signature.parameters.keys()) == set(self.inputs.keys()):
   113|                 raise Exception(
   114|                     "Tool's 'forward' method should take 'self' as its first argument, then its next arguments should match the keys of tool attribute 'inputs'."
   115|                 )
   116|             json_schema = _convert_type_hints_to_json_schema(self.forward, error_on_missing_type_hints=False)[
   117|                 "properties"
   118|             ]  # This function will not raise an error on missing docstrings, contrary to get_json_schema
   119|             for key, value in self.inputs.items():
   120|                 assert key in json_schema, (
   121|                     f"Input '{key}' should be present in function signature, found only {json_schema.keys()}"
   122|                 )
   123|                 if "nullable" in value:
   124|                     assert "nullable" in json_schema[key], (
   125|                         f"Nullable argument '{key}' in inputs should have key 'nullable' set to True in function signature."
   126|                     )
   127|                 if key in json_schema and "nullable" in json_schema[key]:
   128|                     assert "nullable" in value, (
   129|                         f"Nullable argument '{key}' in function signature should have key 'nullable' set to True in inputs."
   130|                     )
   131|     def forward(self, *args, **kwargs):
   132|         return NotImplementedError("Write this method in your subclass of `Tool`.")
   133|     def __call__(self, *args, sanitize_inputs_outputs: bool = False, **kwargs):
   134|         if not self.is_initialized:
   135|             self.setup()
   136|         if len(args) == 1 and len(kwargs) == 0 and isinstance(args[0], dict):
   137|             potential_kwargs = args[0]
   138|             if all(key in self.inputs for key in potential_kwargs):
   139|                 args = ()
   140|                 kwargs = potential_kwargs
   141|         if sanitize_inputs_outputs:
   142|             args, kwargs = handle_agent_input_types(*args, **kwargs)
   143|         outputs = self.forward(*args, **kwargs)
   144|         if sanitize_inputs_outputs:

# --- HUNK 3: Lines 148-194 ---
   148|         """
   149|         Overwrite this method here for any operation that is expensive and needs to be executed before you start using
   150|         your tool. Such as loading a big model.
   151|         """
   152|         self.is_initialized = True
   153|     def save(self, output_dir):
   154|         """
   155|         Saves the relevant code files for your tool so it can be pushed to the Hub. This will copy the code of your
   156|         tool in `output_dir` as well as autogenerate:
   157|         - a `tool.py` file containing the logic for your tool.
   158|         - an `app.py` file providing an UI for your tool when it is exported to a Space with `tool.push_to_hub()`
   159|         - a `requirements.txt` containing the names of the module used by your tool (as detected when inspecting its
   160|           code)
   161|         Args:
   162|             output_dir (`str`): The folder in which you want to save your tool.
   163|         """
   164|         os.makedirs(output_dir, exist_ok=True)
   165|         class_name = self.__class__.__name__
   166|         tool_file = os.path.join(output_dir, "tool.py")
   167|         if type(self).__name__ == "SimpleTool":
   168|             source_code = get_source(self.forward).replace("@tool", "")
   169|             forward_node = ast.parse(source_code)
   170|             method_checker = MethodChecker(set())
   171|             method_checker.visit(forward_node)
   172|             if len(method_checker.errors) > 0:
   173|                 raise (ValueError("\n".join(method_checker.errors)))
   174|             forward_source_code = get_source(self.forward)
   175|             tool_code = textwrap.dedent(
   176|                 f"""
   177|             from smolagents import Tool
   178|             from typing import Optional
   179|             class {class_name}(Tool):
   180|                 name = "{self.name}"
   181|                 description = "{self.description}"
   182|                 inputs = {json.dumps(self.inputs, separators=(",", ":"))}
   183|                 output_type = "{self.output_type}"
   184|             """
   185|             ).strip()
   186|             import re
   187|             def add_self_argument(source_code: str) -> str:
   188|                 """Add 'self' as first argument to a function definition if not present."""
   189|                 pattern = r"def forward\(((?!self)[^)]*)\)"
   190|                 def replacement(match):
   191|                     args = match.group(1).strip()
   192|                     if args:  # If there are other arguments
   193|                         return f"def forward(self, {args})"
   194|                     return "def forward(self)"

# --- HUNK 4: Lines 206-247 ---
   206|                 raise ValueError(
   207|                     "Cannot save objects created with from_space, from_langchain or from_gradio, as this would create errors."
   208|                 )
   209|             validate_tool_attributes(self.__class__)
   210|             tool_code = instance_to_source(self, base_cls=Tool)
   211|         with open(tool_file, "w", encoding="utf-8") as f:
   212|             f.write(tool_code.replace(":true,", ":True,").replace(":true}", ":True}"))
   213|         app_file = os.path.join(output_dir, "app.py")
   214|         with open(app_file, "w", encoding="utf-8") as f:
   215|             f.write(
   216|                 textwrap.dedent(
   217|                     f"""
   218|             from smolagents import launch_gradio_demo
   219|             from typing import Optional
   220|             from tool import {class_name}
   221|             tool = {class_name}()
   222|             launch_gradio_demo(tool)
   223|             """
   224|                 ).lstrip()
   225|             )
   226|         imports = {el for el in get_imports(tool_file) if el not in sys.stdlib_module_names} | {"smolagents"}
   227|         requirements_file = os.path.join(output_dir, "requirements.txt")
   228|         with open(requirements_file, "w", encoding="utf-8") as f:
   229|             f.write("\n".join(imports) + "\n")
   230|     def push_to_hub(
   231|         self,
   232|         repo_id: str,
   233|         commit_message: str = "Upload tool",
   234|         private: Optional[bool] = None,
   235|         token: Optional[Union[bool, str]] = None,
   236|         create_pr: bool = False,
   237|     ) -> str:
   238|         """
   239|         Upload the tool to the Hub.
   240|         For this method to work properly, your tool must have been defined in a separate module (not `__main__`).
   241|         For instance:
   242|         ```
   243|         from my_tool_module import MyTool
   244|         my_tool = MyTool()
   245|         my_tool.push_to_hub("my-username/my-space")
   246|         ```
   247|         Parameters:

# --- HUNK 5: Lines 253-350 ---
   253|             private (`bool`, *optional*):
   254|                 Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.
   255|             token (`bool` or `str`, *optional*):
   256|                 The token to use as HTTP bearer authorization for remote files. If unset, will use the token generated
   257|                 when running `huggingface-cli login` (stored in `~/.huggingface`).
   258|             create_pr (`bool`, *optional*, defaults to `False`):
   259|                 Whether or not to create a PR with the uploaded files or directly commit.
   260|         """
   261|         repo_url = create_repo(
   262|             repo_id=repo_id,
   263|             token=token,
   264|             private=private,
   265|             exist_ok=True,
   266|             repo_type="space",
   267|             space_sdk="gradio",
   268|         )
   269|         repo_id = repo_url.repo_id
   270|         metadata_update(repo_id, {"tags": ["tool"]}, repo_type="space")
   271|         with tempfile.TemporaryDirectory() as work_dir:
   272|             self.save(work_dir)
   273|             with open(work_dir + "/tool.py", "r") as f:
   274|                 print("\n".join(f.readlines()))
   275|             logger.info(f"Uploading the following files to {repo_id}: {','.join(os.listdir(work_dir))}")
   276|             return upload_folder(
   277|                 repo_id=repo_id,
   278|                 commit_message=commit_message,
   279|                 folder_path=work_dir,
   280|                 token=token,
   281|                 create_pr=create_pr,
   282|                 repo_type="space",
   283|             )
   284|     @classmethod
   285|     def from_hub(
   286|         cls,
   287|         repo_id: str,
   288|         token: Optional[str] = None,
   289|         trust_remote_code: bool = False,
   290|         **kwargs,
   291|     ):
   292|         """
   293|         Loads a tool defined on the Hub.
   294|         <Tip warning={true}>
   295|         Loading a tool from the Hub means that you'll download the tool and execute it locally.
   296|         ALWAYS inspect the tool you're downloading before loading it within your runtime, as you would do when
   297|         installing a package using pip/npm/apt.
   298|         </Tip>
   299|         Args:
   300|             repo_id (`str`):
   301|                 The name of the repo on the Hub where your tool is defined.
   302|             token (`str`, *optional*):
   303|                 The token to identify you on hf.co. If unset, will use the token generated when running
   304|                 `huggingface-cli login` (stored in `~/.huggingface`).
   305|             trust_remote_code(`str`, *optional*, defaults to False):
   306|                 This flags marks that you understand the risk of running remote code and that you trust this tool.
   307|                 If not setting this to True, loading the tool from Hub will fail.
   308|             kwargs (additional keyword arguments, *optional*):
   309|                 Additional keyword arguments that will be split in two: all arguments relevant to the Hub (such as
   310|                 `cache_dir`, `revision`, `subfolder`) will be used when downloading the files for your tool, and the
   311|                 others will be passed along to its init.
   312|         """
   313|         if not trust_remote_code:
   314|             raise ValueError(
   315|                 "Loading a tool from Hub requires to trust remote code. Make sure you've inspected the repo and pass `trust_remote_code=True` to load the tool."
   316|             )
   317|         tool_file = hf_hub_download(
   318|             repo_id,
   319|             "tool.py",
   320|             token=token,
   321|             repo_type="space",
   322|             cache_dir=kwargs.get("cache_dir"),
   323|             force_download=kwargs.get("force_download"),
   324|             resume_download=kwargs.get("resume_download"),
   325|             proxies=kwargs.get("proxies"),
   326|             revision=kwargs.get("revision"),
   327|             subfolder=kwargs.get("subfolder"),
   328|             local_files_only=kwargs.get("local_files_only"),
   329|         )
   330|         tool_code = Path(tool_file).read_text()
   331|         with tempfile.TemporaryDirectory() as temp_dir:
   332|             module_path = os.path.join(temp_dir, "tool.py")
   333|             with open(module_path, "w") as f:
   334|                 f.write(tool_code)
   335|             print("TOOL CODE:\n", tool_code)
   336|             spec = importlib.util.spec_from_file_location("tool", module_path)
   337|             module = importlib.util.module_from_spec(spec)
   338|             spec.loader.exec_module(module)
   339|             for item_name in dir(module):
   340|                 item = getattr(module, item_name)
   341|                 if isinstance(item, type) and issubclass(item, Tool) and item != Tool:
   342|                     tool_class = item
   343|                     break
   344|             if tool_class is None:
   345|                 raise ValueError("No Tool subclass found in the code.")
   346|         if not isinstance(tool_class.inputs, dict):
   347|             tool_class.inputs = ast.literal_eval(tool_class.inputs)
   348|         return tool_class(**kwargs)
   349|     @staticmethod
   350|     def from_space(

# --- HUNK 6: Lines 385-458 ---
   385|         ...     "face_swapper",
   386|         ...     "Tool that puts the face shown on the first image on the second image. You can give it paths to images.",
   387|         ... )
   388|         >>> image = face_swapper('./aymeric.jpeg', './ruth.jpg')
   389|         ```
   390|         """
   391|         from gradio_client import Client, handle_file
   392|         class SpaceToolWrapper(Tool):
   393|             skip_forward_signature_validation = True
   394|             def __init__(
   395|                 self,
   396|                 space_id: str,
   397|                 name: str,
   398|                 description: str,
   399|                 api_name: Optional[str] = None,
   400|                 token: Optional[str] = None,
   401|             ):
   402|                 self.name = name
   403|                 self.description = description
   404|                 self.client = Client(space_id, hf_token=token)
   405|                 space_description = self.client.view_api(return_format="dict", print_info=False)["named_endpoints"]
   406|                 if api_name is None:
   407|                     api_name = list(space_description.keys())[0]
   408|                     logger.warning(
   409|                         f"Since `api_name` was not defined, it was automatically set to the first available API: `{api_name}`."
   410|                     )
   411|                 self.api_name = api_name
   412|                 try:
   413|                     space_description_api = space_description[api_name]
   414|                 except KeyError:
   415|                     raise KeyError(f"Could not find specified {api_name=} among available api names.")
   416|                 self.inputs = {}
   417|                 for parameter in space_description_api["parameters"]:
   418|                     if not parameter["parameter_has_default"]:
   419|                         parameter_type = parameter["type"]["type"]
   420|                         if parameter_type == "object":
   421|                             parameter_type = "any"
   422|                         self.inputs[parameter["parameter_name"]] = {
   423|                             "type": parameter_type,
   424|                             "description": parameter["python_type"]["description"],
   425|                         }
   426|                 output_component = space_description_api["returns"][0]["component"]
   427|                 if output_component == "Image":
   428|                     self.output_type = "image"
   429|                 elif output_component == "Audio":
   430|                     self.output_type = "audio"
   431|                 else:
   432|                     self.output_type = "any"
   433|                 self.is_initialized = True
   434|             def sanitize_argument_for_prediction(self, arg):
   435|                 from gradio_client.utils import is_http_url_like
   436|                 if _is_pillow_available():
   437|                     from PIL.Image import Image
   438|                 if _is_pillow_available() and isinstance(arg, Image):
   439|                     temp_file = tempfile.NamedTemporaryFile(suffix=".png", delete=False)
   440|                     arg.save(temp_file.name)
   441|                     arg = temp_file.name
   442|                 if (
   443|                     (isinstance(arg, str) and os.path.isfile(arg))
   444|                     or (isinstance(arg, Path) and arg.exists() and arg.is_file())
   445|                     or is_http_url_like(arg)
   446|                 ):
   447|                     arg = handle_file(arg)
   448|                 return arg
   449|             def forward(self, *args, **kwargs):
   450|                 args = list(args)
   451|                 for i, arg in enumerate(args):
   452|                     args[i] = self.sanitize_argument_for_prediction(arg)
   453|                 for arg_name, arg in kwargs.items():
   454|                     kwargs[arg_name] = self.sanitize_argument_for_prediction(arg)
   455|                 output = self.client.predict(*args, api_name=self.api_name, **kwargs)
   456|                 if isinstance(output, tuple) or isinstance(output, list):
   457|                     return output[
   458|                         0

# --- HUNK 7: Lines 462-583 ---
   462|             space_id=space_id,
   463|             name=name,
   464|             description=description,
   465|             api_name=api_name,
   466|             token=token,
   467|         )
   468|     @staticmethod
   469|     def from_gradio(gradio_tool):
   470|         """
   471|         Creates a [`Tool`] from a gradio tool.
   472|         """
   473|         import inspect
   474|         class GradioToolWrapper(Tool):
   475|             def __init__(self, _gradio_tool):
   476|                 self.name = _gradio_tool.name
   477|                 self.description = _gradio_tool.description
   478|                 self.output_type = "string"
   479|                 self._gradio_tool = _gradio_tool
   480|                 func_args = list(inspect.signature(_gradio_tool.run).parameters.items())
   481|                 self.inputs = {
   482|                     key: {"type": CONVERSION_DICT[value.annotation], "description": ""} for key, value in func_args
   483|                 }
   484|                 self.forward = self._gradio_tool.run
   485|         return GradioToolWrapper(gradio_tool)
   486|     @staticmethod
   487|     def from_langchain(langchain_tool):
   488|         """
   489|         Creates a [`Tool`] from a langchain tool.
   490|         """
   491|         class LangChainToolWrapper(Tool):
   492|             skip_forward_signature_validation = True
   493|             def __init__(self, _langchain_tool):
   494|                 self.name = _langchain_tool.name.lower()
   495|                 self.description = _langchain_tool.description
   496|                 self.inputs = _langchain_tool.args.copy()
   497|                 for input_content in self.inputs.values():
   498|                     if "title" in input_content:
   499|                         input_content.pop("title")
   500|                     input_content["description"] = ""
   501|                 self.output_type = "string"
   502|                 self.langchain_tool = _langchain_tool
   503|                 self.is_initialized = True
   504|             def forward(self, *args, **kwargs):
   505|                 tool_input = kwargs.copy()
   506|                 for index, argument in enumerate(args):
   507|                     if index < len(self.inputs):
   508|                         input_key = next(iter(self.inputs))
   509|                         tool_input[input_key] = argument
   510|                 return self.langchain_tool.run(tool_input)
   511|         return LangChainToolWrapper(langchain_tool)
   512| DEFAULT_TOOL_DESCRIPTION_TEMPLATE = """
   513| - {{ tool.name }}: {{ tool.description }}
   514|     Takes inputs: {{tool.inputs}}
   515|     Returns an output of type: {{tool.output_type}}
   516| """
   517| def get_tool_description_with_args(tool: Tool, description_template: Optional[str] = None) -> str:
   518|     if description_template is None:
   519|         description_template = DEFAULT_TOOL_DESCRIPTION_TEMPLATE
   520|     compiled_template = compile_jinja_template(description_template)
   521|     tool_description = compiled_template.render(
   522|         tool=tool,
   523|     )
   524|     return tool_description
   525| @lru_cache
   526| def compile_jinja_template(template):
   527|     try:
   528|         import jinja2
   529|         from jinja2.exceptions import TemplateError
   530|         from jinja2.sandbox import ImmutableSandboxedEnvironment
   531|     except ImportError:
   532|         raise ImportError("template requires jinja2 to be installed.")
   533|     if version.parse(jinja2.__version__) < version.parse("3.1.0"):
   534|         raise ImportError(f"template requires jinja2>=3.1.0 to be installed. Your version is {jinja2.__version__}.")
   535|     def raise_exception(message):
   536|         raise TemplateError(message)
   537|     jinja_env = ImmutableSandboxedEnvironment(trim_blocks=True, lstrip_blocks=True)
   538|     jinja_env.globals["raise_exception"] = raise_exception
   539|     return jinja_env.from_string(template)
   540| def launch_gradio_demo(tool: Tool):
   541|     """
   542|     Launches a gradio demo for a tool. The corresponding tool class needs to properly implement the class attributes
   543|     `inputs` and `output_type`.
   544|     Args:
   545|         tool (`type`): The tool for which to launch the demo.
   546|     """
   547|     try:
   548|         import gradio as gr
   549|     except ImportError:
   550|         raise ImportError("Gradio should be installed in order to launch a gradio demo.")
   551|     TYPE_TO_COMPONENT_CLASS_MAPPING = {
   552|         "image": gr.Image,
   553|         "audio": gr.Audio,
   554|         "string": gr.Textbox,
   555|         "integer": gr.Textbox,
   556|         "number": gr.Textbox,
   557|     }
   558|     def tool_forward(*args, **kwargs):
   559|         return tool(*args, sanitize_inputs_outputs=True, **kwargs)
   560|     tool_forward.__signature__ = inspect.signature(tool.forward)
   561|     gradio_inputs = []
   562|     for input_name, input_details in tool.inputs.items():
   563|         input_gradio_component_class = TYPE_TO_COMPONENT_CLASS_MAPPING[input_details["type"]]
   564|         new_component = input_gradio_component_class(label=input_name)
   565|         gradio_inputs.append(new_component)
   566|     output_gradio_componentclass = TYPE_TO_COMPONENT_CLASS_MAPPING[tool.output_type]
   567|     gradio_output = output_gradio_componentclass(label="Output")
   568|     gr.Interface(
   569|         fn=tool_forward,
   570|         inputs=gradio_inputs,
   571|         outputs=gradio_output,
   572|         title=tool.name,
   573|         article=tool.description,
   574|         description=tool.description,
   575|         api_name=tool.name,
   576|     ).launch()
   577| def load_tool(
   578|     task_or_repo_id,
   579|     model_repo_id: Optional[str] = None,
   580|     token: Optional[str] = None,
   581|     trust_remote_code: bool = False,
   582|     **kwargs,
   583| ):

# --- HUNK 8: Lines 645-686 ---
   645|         """Loads a tool collection from the Hub.
   646|         it adds a collection of tools from all Spaces in the collection to the agent's toolbox
   647|         > [!NOTE]
   648|         > Only Spaces will be fetched, so you can feel free to add models and datasets to your collection if you'd
   649|         > like for this collection to showcase them.
   650|         Args:
   651|             collection_slug (str): The collection slug referencing the collection.
   652|             token (str, *optional*): The authentication token if the collection is private.
   653|             trust_remote_code (bool, *optional*, defaults to False): Whether to trust the remote code.
   654|         Returns:
   655|             ToolCollection: A tool collection instance loaded with the tools.
   656|         Example:
   657|         ```py
   658|         >>> from smolagents import ToolCollection, CodeAgent
   659|         >>> image_tool_collection = ToolCollection.from_hub("huggingface-tools/diffusion-tools-6630bb19a942c2306a2cdb6f")
   660|         >>> agent = CodeAgent(tools=[*image_tool_collection.tools], add_base_tools=True)
   661|         >>> agent.run("Please draw me a picture of rivers and lakes.")
   662|         ```
   663|         """
   664|         _collection = get_collection(collection_slug, token=token)
   665|         _hub_repo_ids = {item.item_id for item in _collection.items if item.item_type == "space"}
   666|         tools = {Tool.from_hub(repo_id, token, trust_remote_code) for repo_id in _hub_repo_ids}
   667|         return cls(tools)
   668|     @classmethod
   669|     @contextmanager
   670|     def from_mcp(cls, server_parameters) -> "ToolCollection":
   671|         """Automatically load a tool collection from an MCP server.
   672|         Note: a separate thread will be spawned to run an asyncio event loop handling
   673|         the MCP server.
   674|         Args:
   675|             server_parameters (mcp.StdioServerParameters): The server parameters to use to
   676|             connect to the MCP server.
   677|         Returns:
   678|             ToolCollection: A tool collection instance.
   679|         Example:
   680|         ```py
   681|         >>> from smolagents import ToolCollection, CodeAgent
   682|         >>> from mcp import StdioServerParameters
   683|         >>> server_parameters = StdioServerParameters(
   684|         >>>     command="uv",
   685|         >>>     args=["--quiet", "pubmedmcp@0.1.3"],
   686|         >>>     env={"UV_PYTHON": "3.12", **os.environ},

# --- HUNK 9: Lines 689-882 ---
   689|         >>>     agent = CodeAgent(tools=[*tool_collection.tools], add_base_tools=True)
   690|         >>>     agent.run("Please find a remedy for hangover.")
   691|         ```
   692|         """
   693|         try:
   694|             from mcpadapt.core import MCPAdapt
   695|             from mcpadapt.smolagents_adapter import SmolAgentsAdapter
   696|         except ImportError:
   697|             raise ImportError(
   698|                 """Please install 'mcp' extra to use ToolCollection.from_mcp: `pip install "smolagents[mcp]"`."""
   699|             )
   700|         with MCPAdapt(server_parameters, SmolAgentsAdapter()) as tools:
   701|             yield cls(tools)
   702| def tool(tool_function: Callable) -> Tool:
   703|     """
   704|     Converts a function into an instance of a Tool subclass.
   705|     Args:
   706|         tool_function: Your function. Should have type hints for each input and a type hint for the output.
   707|         Should also have a docstring description including an 'Args:' part where each argument is described.
   708|     """
   709|     tool_json_schema = get_json_schema(tool_function)["function"]
   710|     if "return" not in tool_json_schema:
   711|         raise TypeHintParsingException("Tool return type not found: make sure your function has a return type hint!")
   712|     class SimpleTool(Tool):
   713|         def __init__(
   714|             self,
   715|             name: str,
   716|             description: str,
   717|             inputs: Dict[str, Dict[str, str]],
   718|             output_type: str,
   719|             function: Callable,
   720|         ):
   721|             self.name = name
   722|             self.description = description
   723|             self.inputs = inputs
   724|             self.output_type = output_type
   725|             self.forward = function
   726|             self.is_initialized = True
   727|     simple_tool = SimpleTool(
   728|         name=tool_json_schema["name"],
   729|         description=tool_json_schema["description"],
   730|         inputs=tool_json_schema["parameters"]["properties"],
   731|         output_type=tool_json_schema["return"]["type"],
   732|         function=tool_function,
   733|     )
   734|     original_signature = inspect.signature(tool_function)
   735|     new_parameters = [inspect.Parameter("self", inspect.Parameter.POSITIONAL_ONLY)] + list(
   736|         original_signature.parameters.values()
   737|     )
   738|     new_signature = original_signature.replace(parameters=new_parameters)
   739|     simple_tool.forward.__signature__ = new_signature
   740|     return simple_tool
   741| class PipelineTool(Tool):
   742|     """
   743|     A [`Tool`] tailored towards Transformer models. On top of the class attributes of the base class [`Tool`], you will
   744|     need to specify:
   745|     - **model_class** (`type`) -- The class to use to load the model in this tool.
   746|     - **default_checkpoint** (`str`) -- The default checkpoint that should be used when the user doesn't specify one.
   747|     - **pre_processor_class** (`type`, *optional*, defaults to [`transformers.AutoProcessor`]) -- The class to use to load the
   748|       pre-processor
   749|     - **post_processor_class** (`type`, *optional*, defaults to [`transformers.AutoProcessor`]) -- The class to use to load the
   750|       post-processor (when different from the pre-processor).
   751|     Args:
   752|         model (`str` or [`transformers.PreTrainedModel`], *optional*):
   753|             The name of the checkpoint to use for the model, or the instantiated model. If unset, will default to the
   754|             value of the class attribute `default_checkpoint`.
   755|         pre_processor (`str` or `Any`, *optional*):
   756|             The name of the checkpoint to use for the pre-processor, or the instantiated pre-processor (can be a
   757|             tokenizer, an image processor, a feature extractor or a processor). Will default to the value of `model` if
   758|             unset.
   759|         post_processor (`str` or `Any`, *optional*):
   760|             The name of the checkpoint to use for the post-processor, or the instantiated pre-processor (can be a
   761|             tokenizer, an image processor, a feature extractor or a processor). Will default to the `pre_processor` if
   762|             unset.
   763|         device (`int`, `str` or `torch.device`, *optional*):
   764|             The device on which to execute the model. Will default to any accelerator available (GPU, MPS etc...), the
   765|             CPU otherwise.
   766|         device_map (`str` or `dict`, *optional*):
   767|             If passed along, will be used to instantiate the model.
   768|         model_kwargs (`dict`, *optional*):
   769|             Any keyword argument to send to the model instantiation.
   770|         token (`str`, *optional*):
   771|             The token to use as HTTP bearer authorization for remote files. If unset, will use the token generated when
   772|             running `huggingface-cli login` (stored in `~/.huggingface`).
   773|         hub_kwargs (additional keyword arguments, *optional*):
   774|             Any additional keyword argument to send to the methods that will load the data from the Hub.
   775|     """
   776|     pre_processor_class = None
   777|     model_class = None
   778|     post_processor_class = None
   779|     default_checkpoint = None
   780|     description = "This is a pipeline tool"
   781|     name = "pipeline"
   782|     inputs = {"prompt": str}
   783|     output_type = str
   784|     skip_forward_signature_validation = True
   785|     def __init__(
   786|         self,
   787|         model=None,
   788|         pre_processor=None,
   789|         post_processor=None,
   790|         device=None,
   791|         device_map=None,
   792|         model_kwargs=None,
   793|         token=None,
   794|         **hub_kwargs,
   795|     ):
   796|         if not is_torch_available() or not _is_package_available("accelerate"):
   797|             raise ModuleNotFoundError(
   798|                 "Please install 'transformers' extra to use a PipelineTool: `pip install 'smolagents[transformers]'`"
   799|             )
   800|         if model is None:
   801|             if self.default_checkpoint is None:
   802|                 raise ValueError("This tool does not implement a default checkpoint, you need to pass one.")
   803|             model = self.default_checkpoint
   804|         if pre_processor is None:
   805|             pre_processor = model
   806|         self.model = model
   807|         self.pre_processor = pre_processor
   808|         self.post_processor = post_processor
   809|         self.device = device
   810|         self.device_map = device_map
   811|         self.model_kwargs = {} if model_kwargs is None else model_kwargs
   812|         if device_map is not None:
   813|             self.model_kwargs["device_map"] = device_map
   814|         self.hub_kwargs = hub_kwargs
   815|         self.hub_kwargs["token"] = token
   816|         super().__init__()
   817|     def setup(self):
   818|         """
   819|         Instantiates the `pre_processor`, `model` and `post_processor` if necessary.
   820|         """
   821|         if isinstance(self.pre_processor, str):
   822|             if self.pre_processor_class is None:
   823|                 from transformers import AutoProcessor
   824|                 self.pre_processor_class = AutoProcessor
   825|             self.pre_processor = self.pre_processor_class.from_pretrained(self.pre_processor, **self.hub_kwargs)
   826|         if isinstance(self.model, str):
   827|             self.model = self.model_class.from_pretrained(self.model, **self.model_kwargs, **self.hub_kwargs)
   828|         if self.post_processor is None:
   829|             self.post_processor = self.pre_processor
   830|         elif isinstance(self.post_processor, str):
   831|             if self.post_processor_class is None:
   832|                 from transformers import AutoProcessor
   833|                 self.post_processor_class = AutoProcessor
   834|             self.post_processor = self.post_processor_class.from_pretrained(self.post_processor, **self.hub_kwargs)
   835|         if self.device is None:
   836|             if self.device_map is not None:
   837|                 self.device = list(self.model.hf_device_map.values())[0]
   838|             else:
   839|                 from accelerate import PartialState
   840|                 self.device = PartialState().default_device
   841|         if self.device_map is None:
   842|             self.model.to(self.device)
   843|         super().setup()
   844|     def encode(self, raw_inputs):
   845|         """
   846|         Uses the `pre_processor` to prepare the inputs for the `model`.
   847|         """
   848|         return self.pre_processor(raw_inputs)
   849|     def forward(self, inputs):
   850|         """
   851|         Sends the inputs through the `model`.
   852|         """
   853|         import torch
   854|         with torch.no_grad():
   855|             return self.model(**inputs)
   856|     def decode(self, outputs):
   857|         """
   858|         Uses the `post_processor` to decode the model output.
   859|         """
   860|         return self.post_processor(outputs)
   861|     def __call__(self, *args, **kwargs):
   862|         import torch
   863|         from accelerate.utils import send_to_device
   864|         args, kwargs = handle_agent_input_types(*args, **kwargs)
   865|         if not self.is_initialized:
   866|             self.setup()
   867|         encoded_inputs = self.encode(*args, **kwargs)
   868|         tensor_inputs = {k: v for k, v in encoded_inputs.items() if isinstance(v, torch.Tensor)}
   869|         non_tensor_inputs = {k: v for k, v in encoded_inputs.items() if not isinstance(v, torch.Tensor)}
   870|         encoded_inputs = send_to_device(tensor_inputs, self.device)
   871|         outputs = self.forward({**encoded_inputs, **non_tensor_inputs})
   872|         outputs = send_to_device(outputs, "cpu")
   873|         decoded_outputs = self.decode(outputs)
   874|         return handle_agent_output_types(decoded_outputs, self.output_type)
   875| __all__ = [
   876|     "AUTHORIZED_TYPES",
   877|     "Tool",
   878|     "tool",
   879|     "load_tool",
   880|     "launch_gradio_demo",
   881|     "ToolCollection",
   882| ]


# ====================================================================
# FILE: src/smolagents/types.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 1-89 ---
     1| import logging
     2| import os
     3| import pathlib
     4| import tempfile
     5| import uuid
     6| from io import BytesIO
     7| import numpy as np
     8| import requests
     9| from huggingface_hub.utils import is_torch_available
    10| from PIL import Image
    11| from PIL.Image import Image as ImageType
    12| from .utils import _is_package_available
    13| logger = logging.getLogger(__name__)
    14| class AgentType:
    15|     """
    16|     Abstract class to be reimplemented to define types that can be returned by agents.
    17|     These objects serve three purposes:
    18|     - They behave as they were the type they're meant to be, e.g., a string for text, a PIL.Image for images
    19|     - They can be stringified: str(object) in order to return a string defining the object
    20|     - They should be displayed correctly in ipython notebooks/colab/jupyter
    21|     """
    22|     def __init__(self, value):
    23|         self._value = value
    24|     def __str__(self):
    25|         return self.to_string()
    26|     def to_raw(self):
    27|         logger.error(
    28|             "This is a raw AgentType of unknown type. Display in notebooks and string conversion will be unreliable"
    29|         )
    30|         return self._value
    31|     def to_string(self) -> str:
    32|         logger.error(
    33|             "This is a raw AgentType of unknown type. Display in notebooks and string conversion will be unreliable"
    34|         )
    35|         return str(self._value)
    36| class AgentText(AgentType, str):
    37|     """
    38|     Text type returned by the agent. Behaves as a string.
    39|     """
    40|     def to_raw(self):
    41|         return self._value
    42|     def to_string(self):
    43|         return str(self._value)
    44| class AgentImage(AgentType, ImageType):
    45|     """
    46|     Image type returned by the agent. Behaves as a PIL.Image.
    47|     """
    48|     def __init__(self, value):
    49|         AgentType.__init__(self, value)
    50|         ImageType.__init__(self)
    51|         self._path = None
    52|         self._raw = None
    53|         self._tensor = None
    54|         if isinstance(value, AgentImage):
    55|             self._raw, self._path, self._tensor = value._raw, value._path, value._tensor
    56|         elif isinstance(value, ImageType):
    57|             self._raw = value
    58|         elif isinstance(value, bytes):
    59|             self._raw = Image.open(BytesIO(value))
    60|         elif isinstance(value, (str, pathlib.Path)):
    61|             self._path = value
    62|         elif is_torch_available():
    63|             import torch
    64|             if isinstance(value, torch.Tensor):
    65|                 self._tensor = value
    66|             if isinstance(value, np.ndarray):
    67|                 self._tensor = torch.from_numpy(value)
    68|         if self._path is None and self._raw is None and self._tensor is None:
    69|             raise TypeError(f"Unsupported type for {self.__class__.__name__}: {type(value)}")
    70|     def _ipython_display_(self, include=None, exclude=None):
    71|         """
    72|         Displays correctly this type in an ipython notebook (ipython, colab, jupyter, ...)
    73|         """
    74|         from IPython.display import Image, display
    75|         display(Image(self.to_string()))
    76|     def to_raw(self):
    77|         """
    78|         Returns the "raw" version of that object. In the case of an AgentImage, it is a PIL.Image.
    79|         """
    80|         if self._raw is not None:
    81|             return self._raw
    82|         if self._path is not None:
    83|             self._raw = Image.open(self._path)
    84|             return self._raw
    85|         if self._tensor is not None:
    86|             array = self._tensor.cpu().detach().numpy()
    87|             return Image.fromarray((255 - array * 255).astype(np.uint8))
    88|     def to_string(self):
    89|         """

# --- HUNK 2: Lines 102-197 ---
   102|             img = Image.fromarray((255 - array * 255).astype(np.uint8))
   103|             directory = tempfile.mkdtemp()
   104|             self._path = os.path.join(directory, str(uuid.uuid4()) + ".png")
   105|             img.save(self._path, format="png")
   106|             return self._path
   107|     def save(self, output_bytes, format: str = None, **params):
   108|         """
   109|         Saves the image to a file.
   110|         Args:
   111|             output_bytes (bytes): The output bytes to save the image to.
   112|             format (str): The format to use for the output image. The format is the same as in PIL.Image.save.
   113|             **params: Additional parameters to pass to PIL.Image.save.
   114|         """
   115|         img = self.to_raw()
   116|         img.save(output_bytes, format=format, **params)
   117| class AgentAudio(AgentType, str):
   118|     """
   119|     Audio type returned by the agent.
   120|     """
   121|     def __init__(self, value, samplerate=16_000):
   122|         if not _is_package_available("soundfile") or not is_torch_available:
   123|             raise ModuleNotFoundError(
   124|                 "Please install 'audio' extra to use AgentAudio: `pip install 'smolagents[audio]'`"
   125|             )
   126|         import torch
   127|         super().__init__(value)
   128|         self._path = None
   129|         self._tensor = None
   130|         self.samplerate = samplerate
   131|         if isinstance(value, (str, pathlib.Path)):
   132|             self._path = value
   133|         elif is_torch_available() and isinstance(value, torch.Tensor):
   134|             self._tensor = value
   135|         elif isinstance(value, tuple):
   136|             self.samplerate = value[0]
   137|             if isinstance(value[1], np.ndarray):
   138|                 self._tensor = torch.from_numpy(value[1])
   139|             else:
   140|                 self._tensor = torch.tensor(value[1])
   141|         else:
   142|             raise ValueError(f"Unsupported audio type: {type(value)}")
   143|     def _ipython_display_(self, include=None, exclude=None):
   144|         """
   145|         Displays correctly this type in an ipython notebook (ipython, colab, jupyter, ...)
   146|         """
   147|         from IPython.display import Audio, display
   148|         display(Audio(self.to_string(), rate=self.samplerate))
   149|     def to_raw(self):
   150|         """
   151|         Returns the "raw" version of that object. It is a `torch.Tensor` object.
   152|         """
   153|         import soundfile as sf
   154|         if self._tensor is not None:
   155|             return self._tensor
   156|         import torch
   157|         if self._path is not None:
   158|             if "://" in str(self._path):
   159|                 response = requests.get(self._path)
   160|                 response.raise_for_status()
   161|                 tensor, self.samplerate = sf.read(BytesIO(response.content))
   162|             else:
   163|                 tensor, self.samplerate = sf.read(self._path)
   164|             self._tensor = torch.tensor(tensor)
   165|             return self._tensor
   166|     def to_string(self):
   167|         """
   168|         Returns the stringified version of that object. In the case of an AgentAudio, it is a path to the serialized
   169|         version of the audio.
   170|         """
   171|         import soundfile as sf
   172|         if self._path is not None:
   173|             return self._path
   174|         if self._tensor is not None:
   175|             directory = tempfile.mkdtemp()
   176|             self._path = os.path.join(directory, str(uuid.uuid4()) + ".wav")
   177|             sf.write(self._path, self._tensor, samplerate=self.samplerate)
   178|             return self._path
   179| _AGENT_TYPE_MAPPING = {"string": AgentText, "image": AgentImage, "audio": AgentAudio}
   180| def handle_agent_input_types(*args, **kwargs):
   181|     args = [(arg.to_raw() if isinstance(arg, AgentType) else arg) for arg in args]
   182|     kwargs = {k: (v.to_raw() if isinstance(v, AgentType) else v) for k, v in kwargs.items()}
   183|     return args, kwargs
   184| def handle_agent_output_types(output, output_type=None):
   185|     if output_type in _AGENT_TYPE_MAPPING:
   186|         decoded_outputs = _AGENT_TYPE_MAPPING[output_type](output)
   187|         return decoded_outputs
   188|     if isinstance(output, str):
   189|         return AgentText(output)
   190|     if isinstance(output, ImageType):
   191|         return AgentImage(output)
   192|     if is_torch_available():
   193|         import torch
   194|         if isinstance(output, torch.Tensor):
   195|             return AgentAudio(output)
   196|     return output
   197| __all__ = ["AgentType", "AgentImage", "AgentText", "AgentAudio"]


# ====================================================================
# FILE: src/smolagents/utils.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-93 ---
     1| import ast
     2| import base64
     3| import importlib.metadata
     4| import importlib.util
     5| import inspect
     6| import json
     7| import re
     8| import textwrap
     9| import types
    10| from enum import IntEnum
    11| from functools import lru_cache
    12| from io import BytesIO
    13| from typing import Dict, Tuple, Union
    14| from rich.console import Console
    15| __all__ = ["AgentError"]
    16| @lru_cache
    17| def _is_package_available(package_name: str) -> bool:
    18|     try:
    19|         importlib.metadata.version(package_name)
    20|         return True
    21|     except importlib.metadata.PackageNotFoundError:
    22|         return False
    23| @lru_cache
    24| def _is_pillow_available():
    25|     return importlib.util.find_spec("PIL") is not None
    26| console = Console()
    27| BASE_BUILTIN_MODULES = [
    28|     "collections",
    29|     "datetime",
    30|     "itertools",
    31|     "math",
    32|     "queue",
    33|     "random",
    34|     "re",
    35|     "stat",
    36|     "statistics",
    37|     "time",
    38|     "unicodedata",
    39| ]
    40| class LogLevel(IntEnum):
    41|     ERROR = 0  # Only errors
    42|     INFO = 1  # Normal output (default)
    43|     DEBUG = 2  # Detailed output
    44| class AgentLogger:
    45|     def __init__(self, level: LogLevel = LogLevel.INFO):
    46|         self.level = level
    47|         self.console = Console()
    48|     def log(self, *args, level: LogLevel = LogLevel.INFO, **kwargs):
    49|         if level <= self.level:
    50|             self.console.print(*args, **kwargs)
    51| class AgentError(Exception):
    52|     """Base class for other agent-related exceptions"""
    53|     def __init__(self, message, logger: AgentLogger):
    54|         super().__init__(message)
    55|         self.message = message
    56|         logger.log(f"[bold red]{message}[/bold red]", level=LogLevel.ERROR)
    57| class AgentParsingError(AgentError):
    58|     """Exception raised for errors in parsing in the agent"""
    59|     pass
    60| class AgentExecutionError(AgentError):
    61|     """Exception raised for errors in execution in the agent"""
    62|     pass
    63| class AgentMaxStepsError(AgentError):
    64|     """Exception raised for errors in execution in the agent"""
    65|     pass
    66| class AgentGenerationError(AgentError):
    67|     """Exception raised for errors in generation in the agent"""
    68|     pass
    69| def parse_json_blob(json_blob: str) -> Dict[str, str]:
    70|     try:
    71|         first_accolade_index = json_blob.find("{")
    72|         last_accolade_index = [a.start() for a in list(re.finditer("}", json_blob))][-1]
    73|         json_blob = json_blob[first_accolade_index : last_accolade_index + 1].replace('\\"', "'")
    74|         json_data = json.loads(json_blob, strict=False)
    75|         return json_data
    76|     except json.JSONDecodeError as e:
    77|         place = e.pos
    78|         if json_blob[place - 1 : place + 2] == "},\n":
    79|             raise ValueError(
    80|                 "JSON is invalid: you probably tried to provide multiple tool calls in one action. PROVIDE ONLY ONE TOOL CALL."
    81|             )
    82|         raise ValueError(
    83|             f"The JSON blob you used is invalid due to the following error: {e}.\n"
    84|             f"JSON blob was: {json_blob}, decoding failed on that specific part of the blob:\n"
    85|             f"'{json_blob[place - 4 : place + 5]}'."
    86|         )
    87|     except Exception as e:
    88|         raise ValueError(f"Error in parsing the JSON blob: {e}")
    89| def parse_code_blobs(code_blob: str) -> str:
    90|     """Parses the LLM's output to get any code blob inside. Will return the code directly if it's code."""
    91|     pattern = r"```(?:py|python)?\n(.*?)\n```"
    92|     matches = re.findall(pattern, code_blob, re.DOTALL)
    93|     if len(matches) == 0:

# --- HUNK 2: Lines 120-283 ---
   120|     tool_name_key, tool_arguments_key = None, None
   121|     for possible_tool_name_key in ["action", "tool_name", "tool", "name", "function"]:
   122|         if possible_tool_name_key in tool_call:
   123|             tool_name_key = possible_tool_name_key
   124|     for possible_tool_arguments_key in [
   125|         "action_input",
   126|         "tool_arguments",
   127|         "tool_args",
   128|         "parameters",
   129|     ]:
   130|         if possible_tool_arguments_key in tool_call:
   131|             tool_arguments_key = possible_tool_arguments_key
   132|     if tool_name_key is not None:
   133|         if tool_arguments_key is not None:
   134|             return tool_call[tool_name_key], tool_call[tool_arguments_key]
   135|         else:
   136|             return tool_call[tool_name_key], None
   137|     error_msg = "No tool name key found in tool call!" + f" Tool call: {json_blob}"
   138|     raise AgentParsingError(error_msg)
   139| MAX_LENGTH_TRUNCATE_CONTENT = 20000
   140| def truncate_content(content: str, max_length: int = MAX_LENGTH_TRUNCATE_CONTENT) -> str:
   141|     if len(content) <= max_length:
   142|         return content
   143|     else:
   144|         return (
   145|             content[: max_length // 2]
   146|             + f"\n..._This content has been truncated to stay below {max_length} characters_...\n"
   147|             + content[-max_length // 2 :]
   148|         )
   149| class ImportFinder(ast.NodeVisitor):
   150|     def __init__(self):
   151|         self.packages = set()
   152|     def visit_Import(self, node):
   153|         for alias in node.names:
   154|             base_package = alias.name.split(".")[0]
   155|             self.packages.add(base_package)
   156|     def visit_ImportFrom(self, node):
   157|         if node.module:  # for "from x import y" statements
   158|             base_package = node.module.split(".")[0]
   159|             self.packages.add(base_package)
   160| def get_method_source(method):
   161|     """Get source code for a method, including bound methods."""
   162|     if isinstance(method, types.MethodType):
   163|         method = method.__func__
   164|     return get_source(method)
   165| def is_same_method(method1, method2):
   166|     """Compare two methods by their source code."""
   167|     try:
   168|         source1 = get_method_source(method1)
   169|         source2 = get_method_source(method2)
   170|         source1 = "\n".join(line for line in source1.split("\n") if not line.strip().startswith("@"))
   171|         source2 = "\n".join(line for line in source2.split("\n") if not line.strip().startswith("@"))
   172|         return source1 == source2
   173|     except (TypeError, OSError):
   174|         return False
   175| def is_same_item(item1, item2):
   176|     """Compare two class items (methods or attributes) for equality."""
   177|     if callable(item1) and callable(item2):
   178|         return is_same_method(item1, item2)
   179|     else:
   180|         return item1 == item2
   181| def instance_to_source(instance, base_cls=None):
   182|     """Convert an instance to its class source code representation."""
   183|     cls = instance.__class__
   184|     class_name = cls.__name__
   185|     class_lines = []
   186|     if base_cls:
   187|         class_lines.append(f"class {class_name}({base_cls.__name__}):")
   188|     else:
   189|         class_lines.append(f"class {class_name}:")
   190|     if cls.__doc__ and (not base_cls or cls.__doc__ != base_cls.__doc__):
   191|         class_lines.append(f'    """{cls.__doc__}"""')
   192|     class_attrs = {
   193|         name: value
   194|         for name, value in cls.__dict__.items()
   195|         if not name.startswith("__")
   196|         and not callable(value)
   197|         and not (base_cls and hasattr(base_cls, name) and getattr(base_cls, name) == value)
   198|     }
   199|     for name, value in class_attrs.items():
   200|         if isinstance(value, str):
   201|             if "\n" in value:
   202|                 class_lines.append(f'    {name} = """{value}"""')
   203|             else:
   204|                 class_lines.append(f'    {name} = "{value}"')
   205|         else:
   206|             class_lines.append(f"    {name} = {repr(value)}")
   207|     if class_attrs:
   208|         class_lines.append("")
   209|     methods = {
   210|         name: func
   211|         for name, func in cls.__dict__.items()
   212|         if callable(func)
   213|         and not (
   214|             base_cls and hasattr(base_cls, name) and getattr(base_cls, name).__code__.co_code == func.__code__.co_code
   215|         )
   216|     }
   217|     for name, method in methods.items():
   218|         method_source = get_source(method)
   219|         method_lines = method_source.split("\n")
   220|         first_line = method_lines[0]
   221|         indent = len(first_line) - len(first_line.lstrip())
   222|         method_lines = [line[indent:] for line in method_lines]
   223|         method_source = "\n".join(["    " + line if line.strip() else line for line in method_lines])
   224|         class_lines.append(method_source)
   225|         class_lines.append("")
   226|     import_finder = ImportFinder()
   227|     import_finder.visit(ast.parse("\n".join(class_lines)))
   228|     required_imports = import_finder.packages
   229|     final_lines = []
   230|     if base_cls:
   231|         final_lines.append(f"from {base_cls.__module__} import {base_cls.__name__}")
   232|     for package in required_imports:
   233|         final_lines.append(f"import {package}")
   234|     if final_lines:  # Add empty line after imports
   235|         final_lines.append("")
   236|     final_lines.extend(class_lines)
   237|     return "\n".join(final_lines)
   238| def get_source(obj) -> str:
   239|     """Get the source code of a class or callable object (e.g.: function, method).
   240|     First attempts to get the source code using `inspect.getsource`.
   241|     In a dynamic environment (e.g.: Jupyter, IPython), if this fails,
   242|     falls back to retrieving the source code from the current interactive shell session.
   243|     Args:
   244|         obj: A class or callable object (e.g.: function, method)
   245|     Returns:
   246|         str: The source code of the object, dedented and stripped
   247|     Raises:
   248|         TypeError: If object is not a class or callable
   249|         OSError: If source code cannot be retrieved from any source
   250|         ValueError: If source cannot be found in IPython history
   251|     Note:
   252|         TODO: handle Python standard REPL
   253|     """
   254|     if not (isinstance(obj, type) or callable(obj)):
   255|         raise TypeError(f"Expected class or callable, got {type(obj)}")
   256|     inspect_error = None
   257|     try:
   258|         return textwrap.dedent(inspect.getsource(obj)).strip()
   259|     except OSError as e:
   260|         inspect_error = e
   261|     try:
   262|         import IPython
   263|         shell = IPython.get_ipython()
   264|         if not shell:
   265|             raise ImportError("No active IPython shell found")
   266|         all_cells = "\n".join(shell.user_ns.get("In", [])).strip()
   267|         if not all_cells:
   268|             raise ValueError("No code cells found in IPython session")
   269|         tree = ast.parse(all_cells)
   270|         for node in ast.walk(tree):
   271|             if isinstance(node, (ast.ClassDef, ast.FunctionDef)) and node.name == obj.__name__:
   272|                 return textwrap.dedent("\n".join(all_cells.split("\n")[node.lineno - 1 : node.end_lineno])).strip()
   273|         raise ValueError(f"Could not find source code for {obj.__name__} in IPython history")
   274|     except ImportError:
   275|         raise inspect_error
   276|     except ValueError as e:
   277|         raise e from inspect_error
   278| def encode_image_base64(image):
   279|     buffered = BytesIO()
   280|     image.save(buffered, format="PNG")
   281|     return base64.b64encode(buffered.getvalue()).decode("utf-8")
   282| def make_image_url(base64_image):
   283|     return f"data:image/png;base64,{base64_image}"

