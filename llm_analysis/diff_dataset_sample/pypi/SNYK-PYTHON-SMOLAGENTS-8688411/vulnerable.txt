# ====================================================================
# FILE: examples/e2b_example.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-35 ---
     1| from smolagents import Tool, CodeAgent, HfApiModel
     2| from smolagents.default_tools import VisitWebpageTool
     3| from dotenv import load_dotenv
     4| load_dotenv()
     5| class GetCatImageTool(Tool):
     6|     name = "get_cat_image"
     7|     description = "Get a cat image"
     8|     inputs = {}
     9|     output_type = "image"
    10|     def __init__(self):
    11|         super().__init__()
    12|         self.url = "https://em-content.zobj.net/source/twitter/53/robot-face_1f916.png"
    13|     def forward(self):
    14|         from PIL import Image
    15|         import requests
    16|         from io import BytesIO
    17|         response = requests.get(self.url)
    18|         return Image.open(BytesIO(response.content))
    19| get_cat_image = GetCatImageTool()
    20| agent = CodeAgent(
    21|     tools=[get_cat_image, VisitWebpageTool()],
    22|     model=HfApiModel(),
    23|     additional_authorized_imports=[
    24|         "Pillow",
    25|         "requests",
    26|         "markdownify",
    27|     ],  # "duckduckgo-search",
    28|     use_e2b_executor=True,
    29| )
    30| agent.run(
    31|     "Return me an image of a cat. Directly use the image provided in your state.",
    32|     additional_args={"cat_image": get_cat_image()},
    33| )  # Asking to directly return the image from state tests that additional_args are properly sent to server.
    34| from smolagents import GradioUI
    35| GradioUI(agent).launch()


# ====================================================================
# FILE: examples/gradio_upload.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-3 ---
     1| from smolagents import CodeAgent, HfApiModel, GradioUI
     2| agent = CodeAgent(tools=[], model=HfApiModel(), max_steps=4, verbosity_level=1)
     3| GradioUI(agent, file_upload_folder="./data").launch()


# ====================================================================
# FILE: examples/inspect_runs.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-35 ---
     1| from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
     2| from opentelemetry.sdk.trace import TracerProvider
     3| from opentelemetry.sdk.trace.export import SimpleSpanProcessor
     4| from openinference.instrumentation.smolagents import SmolagentsInstrumentor
     5| from smolagents import (
     6|     CodeAgent,
     7|     DuckDuckGoSearchTool,
     8|     VisitWebpageTool,
     9|     ManagedAgent,
    10|     ToolCallingAgent,
    11|     HfApiModel,
    12| )
    13| trace_provider = TracerProvider()
    14| trace_provider.add_span_processor(
    15|     SimpleSpanProcessor(OTLPSpanExporter("http://0.0.0.0:6006/v1/traces"))
    16| )
    17| SmolagentsInstrumentor().instrument(tracer_provider=trace_provider, skip_dep_check=True)
    18| model = HfApiModel()
    19| agent = ToolCallingAgent(
    20|     tools=[DuckDuckGoSearchTool(), VisitWebpageTool()],
    21|     model=model,
    22| )
    23| managed_agent = ManagedAgent(
    24|     agent=agent,
    25|     name="managed_agent",
    26|     description="This is an agent that can do web search.",
    27| )
    28| manager_agent = CodeAgent(
    29|     tools=[],
    30|     model=model,
    31|     managed_agents=[managed_agent],
    32| )
    33| manager_agent.run(
    34|     "If the US keeps it 2024 growth rate, how many years would it take for the GDP to double?"
    35| )


# ====================================================================
# FILE: examples/rag.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-58 ---
     1| import datasets
     2| from langchain.docstore.document import Document
     3| from langchain.text_splitter import RecursiveCharacterTextSplitter
     4| from langchain_community.retrievers import BM25Retriever
     5| knowledge_base = datasets.load_dataset("m-ric/huggingface_doc", split="train")
     6| knowledge_base = knowledge_base.filter(
     7|     lambda row: row["source"].startswith("huggingface/transformers")
     8| )
     9| source_docs = [
    10|     Document(page_content=doc["text"], metadata={"source": doc["source"].split("/")[1]})
    11|     for doc in knowledge_base
    12| ]
    13| text_splitter = RecursiveCharacterTextSplitter(
    14|     chunk_size=500,
    15|     chunk_overlap=50,
    16|     add_start_index=True,
    17|     strip_whitespace=True,
    18|     separators=["\n\n", "\n", ".", " ", ""],
    19| )
    20| docs_processed = text_splitter.split_documents(source_docs)
    21| from smolagents import Tool
    22| class RetrieverTool(Tool):
    23|     name = "retriever"
    24|     description = "Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query."
    25|     inputs = {
    26|         "query": {
    27|             "type": "string",
    28|             "description": "The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.",
    29|         }
    30|     }
    31|     output_type = "string"
    32|     def __init__(self, docs, **kwargs):
    33|         super().__init__(**kwargs)
    34|         self.retriever = BM25Retriever.from_documents(docs, k=10)
    35|     def forward(self, query: str) -> str:
    36|         assert isinstance(query, str), "Your search query must be a string"
    37|         docs = self.retriever.invoke(
    38|             query,
    39|         )
    40|         return "\nRetrieved documents:\n" + "".join(
    41|             [
    42|                 f"\n\n===== Document {str(i)} =====\n" + doc.page_content
    43|                 for i, doc in enumerate(docs)
    44|             ]
    45|         )
    46| from smolagents import HfApiModel, CodeAgent
    47| retriever_tool = RetrieverTool(docs_processed)
    48| agent = CodeAgent(
    49|     tools=[retriever_tool],
    50|     model=HfApiModel("meta-llama/Llama-3.3-70B-Instruct"),
    51|     max_steps=4,
    52|     verbosity_level=2,
    53| )
    54| agent_output = agent.run(
    55|     "For a transformers model training, which is slower, the forward or the backward pass?"
    56| )
    57| print("Final output:")
    58| print(agent_output)


# ====================================================================
# FILE: examples/text_to_sql.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-59 ---
     1| from sqlalchemy import (
     2|     create_engine,
     3|     MetaData,
     4|     Table,
     5|     Column,
     6|     String,
     7|     Integer,
     8|     Float,
     9|     insert,
    10|     inspect,
    11|     text,
    12| )
    13| engine = create_engine("sqlite:///:memory:")
    14| metadata_obj = MetaData()
    15| table_name = "receipts"
    16| receipts = Table(
    17|     table_name,
    18|     metadata_obj,
    19|     Column("receipt_id", Integer, primary_key=True),
    20|     Column("customer_name", String(16), primary_key=True),
    21|     Column("price", Float),
    22|     Column("tip", Float),
    23| )
    24| metadata_obj.create_all(engine)
    25| rows = [
    26|     {"receipt_id": 1, "customer_name": "Alan Payne", "price": 12.06, "tip": 1.20},
    27|     {"receipt_id": 2, "customer_name": "Alex Mason", "price": 23.86, "tip": 0.24},
    28|     {"receipt_id": 3, "customer_name": "Woodrow Wilson", "price": 53.43, "tip": 5.43},
    29|     {"receipt_id": 4, "customer_name": "Margaret James", "price": 21.11, "tip": 1.00},
    30| ]
    31| for row in rows:
    32|     stmt = insert(receipts).values(**row)
    33|     with engine.begin() as connection:
    34|         cursor = connection.execute(stmt)
    35| inspector = inspect(engine)
    36| columns_info = [(col["name"], col["type"]) for col in inspector.get_columns("receipts")]
    37| table_description = "Columns:\n" + "\n".join(
    38|     [f"  - {name}: {col_type}" for name, col_type in columns_info]
    39| )
    40| print(table_description)
    41| from smolagents import tool
    42| @tool
    43| def sql_engine(query: str) -> str:
    44|     """
    45|     Allows you to perform SQL queries on the table. Returns a string representation of the result.
    46|     The table is named 'receipts'. Its description is as follows:
    47|         Columns:
    48|         - receipt_id: INTEGER
    49|         - customer_name: VARCHAR(16)
    50|         - price: FLOAT
    51|         - tip: FLOAT
    52|     Args:
    53|         query: The query to perform. This should be correct SQL.
    54|     """
    55|     output = ""
    56|     with engine.connect() as con:
    57|         rows = con.execute(text(query))
    58|         for row in rows:
    59|             output += "\n" + str(row)


# ====================================================================
# FILE: examples/tool_calling_agent_from_any_llm.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-16 ---
     1| from smolagents.agents import ToolCallingAgent
     2| from smolagents import tool, LiteLLMModel
     3| from typing import Optional
     4| model = LiteLLMModel(model_id="gpt-4o")
     5| @tool
     6| def get_weather(location: str, celsius: Optional[bool] = False) -> str:
     7|     """
     8|     Get weather in the next days at given location.
     9|     Secretly this tool does not care about the location, it hates the weather everywhere.
    10|     Args:
    11|         location: the location
    12|         celsius: the temperature
    13|     """
    14|     return "The weather is UNGODLY with torrential rains and temperatures below -10°C"
    15| agent = ToolCallingAgent(tools=[get_weather], model=model)
    16| print(agent.run("What's the weather like in Paris?"))


# ====================================================================
# FILE: examples/tool_calling_agent_mcp.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-19 ---
     1| """An example of loading a ToolCollection directly from an MCP server.
     2| Requirements: to run this example, you need to have uv installed and in your path in
     3| order to run the MCP server with uvx see `mcp_server_params` below.
     4| Note this is just a demo MCP server that was implemented for the purpose of this example.
     5| It only provide a single tool to search amongst pubmed papers abstracts.
     6| Usage:
     7| >>> uv run examples/tool_calling_agent_mcp.py
     8| """
     9| import os
    10| from mcp import StdioServerParameters
    11| from smolagents import CodeAgent, HfApiModel, ToolCollection
    12| mcp_server_params = StdioServerParameters(
    13|     command="uvx",
    14|     args=["--quiet", "pubmedmcp@0.1.3"],
    15|     env={"UV_PYTHON": "3.12", **os.environ},
    16| )
    17| with ToolCollection.from_mcp(mcp_server_params) as tool_collection:
    18|     agent = CodeAgent(tools=tool_collection.tools, model=HfApiModel(), max_steps=4)
    19|     agent.run("Find me one risk associated with drinking alcohol regularly on low doses for humans.")


# ====================================================================
# FILE: examples/tool_calling_agent_ollama.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-20 ---
     1| from smolagents.agents import ToolCallingAgent
     2| from smolagents import tool, LiteLLMModel
     3| from typing import Optional
     4| model = LiteLLMModel(
     5|     model_id="ollama_chat/llama3.2",
     6|     api_base="http://localhost:11434",  # replace with remote open-ai compatible server if necessary
     7|     api_key="your-api-key",  # replace with API key if necessary
     8| )
     9| @tool
    10| def get_weather(location: str, celsius: Optional[bool] = False) -> str:
    11|     """
    12|     Get weather in the next days at given location.
    13|     Secretly this tool does not care about the location, it hates the weather everywhere.
    14|     Args:
    15|         location: the location
    16|         celsius: the temperature
    17|     """
    18|     return "The weather is UNGODLY with torrential rains and temperatures below -10°C"
    19| agent = ToolCallingAgent(tools=[get_weather], model=model)
    20| print(agent.run("What's the weather like in Paris?"))


# ====================================================================
# FILE: src/smolagents/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| __version__ = "1.4.1"
     2| from typing import TYPE_CHECKING
     3| from transformers.utils import _LazyModule
     4| from transformers.utils.import_utils import define_import_structure
     5| if TYPE_CHECKING:
     6|     from .agents import *
     7|     from .default_tools import *
     8|     from .e2b_executor import *
     9|     from .gradio_ui import *
    10|     from .local_python_executor import *
    11|     from .models import *
    12|     from .monitoring import *
    13|     from .prompts import *
    14|     from .tools import *
    15|     from .types import *
    16|     from .utils import *
    17| else:
    18|     import sys
    19|     _file = globals()["__file__"]
    20|     import_structure = define_import_structure(_file)
    21|     import_structure[""] = {"__version__": __version__}
    22|     sys.modules[__name__] = _LazyModule(
    23|         __name__,
    24|         _file,
    25|         import_structure,
    26|         module_spec=__spec__,
    27|         extra_objects={"__version__": __version__},
    28|     )


# ====================================================================
# FILE: src/smolagents/agents.py
# Total hunks: 9
# ====================================================================
# --- HUNK 1: Lines 1-734 ---
     1| import time
     2| from dataclasses import dataclass
     3| from typing import Any, Callable, Dict, List, Optional, Tuple, Union
     4| from enum import IntEnum
     5| from rich import box
     6| from rich.console import Group
     7| from rich.panel import Panel
     8| from rich.rule import Rule
     9| from rich.syntax import Syntax
    10| from rich.text import Text
    11| from rich.console import Console
    12| from .default_tools import FinalAnswerTool, TOOL_MAPPING
    13| from .e2b_executor import E2BExecutor
    14| from .local_python_executor import (
    15|     BASE_BUILTIN_MODULES,
    16|     LocalPythonInterpreter,
    17|     fix_final_answer_code,
    18| )
    19| from .models import MessageRole
    20| from .monitoring import Monitor
    21| from .prompts import (
    22|     CODE_SYSTEM_PROMPT,
    23|     MANAGED_AGENT_PROMPT,
    24|     PLAN_UPDATE_FINAL_PLAN_REDACTION,
    25|     SYSTEM_PROMPT_FACTS,
    26|     SYSTEM_PROMPT_FACTS_UPDATE,
    27|     SYSTEM_PROMPT_PLAN,
    28|     SYSTEM_PROMPT_PLAN_UPDATE,
    29|     TOOL_CALLING_SYSTEM_PROMPT,
    30|     USER_PROMPT_FACTS_UPDATE,
    31|     USER_PROMPT_PLAN,
    32|     USER_PROMPT_PLAN_UPDATE,
    33| )
    34| from .tools import (
    35|     DEFAULT_TOOL_DESCRIPTION_TEMPLATE,
    36|     Tool,
    37|     get_tool_description_with_args,
    38| )
    39| from .types import AgentAudio, AgentImage, handle_agent_output_types
    40| from .utils import (
    41|     AgentError,
    42|     AgentExecutionError,
    43|     AgentGenerationError,
    44|     AgentMaxStepsError,
    45|     AgentParsingError,
    46|     console,
    47|     parse_code_blobs,
    48|     parse_json_tool_call,
    49|     truncate_content,
    50| )
    51| @dataclass
    52| class ToolCall:
    53|     name: str
    54|     arguments: Any
    55|     id: str
    56| class AgentStepLog:
    57|     pass
    58| @dataclass
    59| class ActionStep(AgentStepLog):
    60|     agent_memory: List[Dict[str, str]] | None = None
    61|     tool_calls: List[ToolCall] | None = None
    62|     start_time: float | None = None
    63|     end_time: float | None = None
    64|     step: int | None = None
    65|     error: AgentError | None = None
    66|     duration: float | None = None
    67|     llm_output: str | None = None
    68|     observations: str | None = None
    69|     action_output: Any = None
    70| @dataclass
    71| class PlanningStep(AgentStepLog):
    72|     plan: str
    73|     facts: str
    74| @dataclass
    75| class TaskStep(AgentStepLog):
    76|     task: str
    77| @dataclass
    78| class SystemPromptStep(AgentStepLog):
    79|     system_prompt: str
    80| def get_tool_descriptions(
    81|     tools: Dict[str, Tool], tool_description_template: str
    82| ) -> str:
    83|     return "\n".join(
    84|         [
    85|             get_tool_description_with_args(tool, tool_description_template)
    86|             for tool in tools.values()
    87|         ]
    88|     )
    89| def format_prompt_with_tools(
    90|     tools: Dict[str, Tool], prompt_template: str, tool_description_template: str
    91| ) -> str:
    92|     tool_descriptions = get_tool_descriptions(tools, tool_description_template)
    93|     prompt = prompt_template.replace("{{tool_descriptions}}", tool_descriptions)
    94|     if "{{tool_names}}" in prompt:
    95|         prompt = prompt.replace(
    96|             "{{tool_names}}",
    97|             ", ".join([f"'{tool.name}'" for tool in tools.values()]),
    98|         )
    99|     return prompt
   100| def show_agents_descriptions(managed_agents: Dict):
   101|     managed_agents_descriptions = """
   102| You can also give requests to team members.
   103| Calling a team member works the same as for calling a tool: simply, the only argument you can give in the call is 'request', a long string explaining your request.
   104| Given that this team member is a real human, you should be very verbose in your request.
   105| Here is a list of the team members that you can call:"""
   106|     for agent in managed_agents.values():
   107|         managed_agents_descriptions += f"\n- {agent.name}: {agent.description}"
   108|     return managed_agents_descriptions
   109| def format_prompt_with_managed_agents_descriptions(
   110|     prompt_template,
   111|     managed_agents,
   112|     agent_descriptions_placeholder: Optional[str] = None,
   113| ) -> str:
   114|     if agent_descriptions_placeholder is None:
   115|         agent_descriptions_placeholder = "{{managed_agents_descriptions}}"
   116|     if agent_descriptions_placeholder not in prompt_template:
   117|         raise ValueError(
   118|             f"Provided prompt template does not contain the managed agents descriptions placeholder '{agent_descriptions_placeholder}'"
   119|         )
   120|     if len(managed_agents.keys()) > 0:
   121|         return prompt_template.replace(
   122|             agent_descriptions_placeholder, show_agents_descriptions(managed_agents)
   123|         )
   124|     else:
   125|         return prompt_template.replace(agent_descriptions_placeholder, "")
   126| YELLOW_HEX = "#d4b702"
   127| class LogLevel(IntEnum):
   128|     ERROR = 0  # Only errors
   129|     INFO = 1  # Normal output (default)
   130|     DEBUG = 2  # Detailed output
   131| class AgentLogger:
   132|     def __init__(self, level: LogLevel = LogLevel.INFO):
   133|         self.level = level
   134|         self.console = Console()
   135|     def log(self, *args, level: LogLevel = LogLevel.INFO, **kwargs):
   136|         if level <= self.level:
   137|             console.print(*args, **kwargs)
   138| class MultiStepAgent:
   139|     """
   140|     Agent class that solves the given task step by step, using the ReAct framework:
   141|     While the objective is not reached, the agent will perform a cycle of action (given by the LLM) and observation (obtained from the environment).
   142|     """
   143|     def __init__(
   144|         self,
   145|         tools: List[Tool],
   146|         model: Callable[[List[Dict[str, str]]], str],
   147|         system_prompt: Optional[str] = None,
   148|         tool_description_template: Optional[str] = None,
   149|         max_steps: int = 6,
   150|         tool_parser: Optional[Callable] = None,
   151|         add_base_tools: bool = False,
   152|         verbosity_level: int = 1,
   153|         grammar: Optional[Dict[str, str]] = None,
   154|         managed_agents: Optional[List] = None,
   155|         step_callbacks: Optional[List[Callable]] = None,
   156|         planning_interval: Optional[int] = None,
   157|     ):
   158|         if system_prompt is None:
   159|             system_prompt = CODE_SYSTEM_PROMPT
   160|         if tool_parser is None:
   161|             tool_parser = parse_json_tool_call
   162|         self.agent_name = self.__class__.__name__
   163|         self.model = model
   164|         self.system_prompt_template = system_prompt
   165|         self.tool_description_template = (
   166|             tool_description_template
   167|             if tool_description_template
   168|             else DEFAULT_TOOL_DESCRIPTION_TEMPLATE
   169|         )
   170|         self.max_steps = max_steps
   171|         self.tool_parser = tool_parser
   172|         self.grammar = grammar
   173|         self.planning_interval = planning_interval
   174|         self.state = {}
   175|         self.managed_agents = {}
   176|         if managed_agents is not None:
   177|             self.managed_agents = {agent.name: agent for agent in managed_agents}
   178|         self.tools = {tool.name: tool for tool in tools}
   179|         if add_base_tools:
   180|             for tool_name, tool_class in TOOL_MAPPING.items():
   181|                 if (
   182|                     tool_name != "python_interpreter"
   183|                     or self.__class__.__name__ == "ToolCallingAgent"
   184|                 ):
   185|                     self.tools[tool_name] = tool_class()
   186|         self.tools["final_answer"] = FinalAnswerTool()
   187|         self.system_prompt = self.initialize_system_prompt()
   188|         self.input_messages = None
   189|         self.logs = []
   190|         self.task = None
   191|         self.logger = AgentLogger(level=verbosity_level)
   192|         self.monitor = Monitor(self.model, self.logger)
   193|         self.step_callbacks = step_callbacks if step_callbacks is not None else []
   194|         self.step_callbacks.append(self.monitor.update_metrics)
   195|     def initialize_system_prompt(self):
   196|         self.system_prompt = format_prompt_with_tools(
   197|             self.tools,
   198|             self.system_prompt_template,
   199|             self.tool_description_template,
   200|         )
   201|         self.system_prompt = format_prompt_with_managed_agents_descriptions(
   202|             self.system_prompt, self.managed_agents
   203|         )
   204|         return self.system_prompt
   205|     def write_inner_memory_from_logs(
   206|         self, summary_mode: Optional[bool] = False
   207|     ) -> List[Dict[str, str]]:
   208|         """
   209|         Reads past llm_outputs, actions, and observations or errors from the logs into a series of messages
   210|         that can be used as input to the LLM.
   211|         """
   212|         memory = []
   213|         for i, step_log in enumerate(self.logs):
   214|             if isinstance(step_log, SystemPromptStep):
   215|                 if not summary_mode:
   216|                     thought_message = {
   217|                         "role": MessageRole.SYSTEM,
   218|                         "content": step_log.system_prompt.strip(),
   219|                     }
   220|                     memory.append(thought_message)
   221|             elif isinstance(step_log, PlanningStep):
   222|                 thought_message = {
   223|                     "role": MessageRole.ASSISTANT,
   224|                     "content": "[FACTS LIST]:\n" + step_log.facts.strip(),
   225|                 }
   226|                 memory.append(thought_message)
   227|                 if not summary_mode:
   228|                     thought_message = {
   229|                         "role": MessageRole.ASSISTANT,
   230|                         "content": "[PLAN]:\n" + step_log.plan.strip(),
   231|                     }
   232|                     memory.append(thought_message)
   233|             elif isinstance(step_log, TaskStep):
   234|                 task_message = {
   235|                     "role": MessageRole.USER,
   236|                     "content": "New task:\n" + step_log.task,
   237|                 }
   238|                 memory.append(task_message)
   239|             elif isinstance(step_log, ActionStep):
   240|                 if step_log.llm_output is not None and not summary_mode:
   241|                     thought_message = {
   242|                         "role": MessageRole.ASSISTANT,
   243|                         "content": step_log.llm_output.strip(),
   244|                     }
   245|                     memory.append(thought_message)
   246|                 if step_log.tool_calls is not None:
   247|                     tool_call_message = {
   248|                         "role": MessageRole.ASSISTANT,
   249|                         "content": str(
   250|                             [
   251|                                 {
   252|                                     "id": tool_call.id,
   253|                                     "type": "function",
   254|                                     "function": {
   255|                                         "name": tool_call.name,
   256|                                         "arguments": tool_call.arguments,
   257|                                     },
   258|                                 }
   259|                                 for tool_call in step_log.tool_calls
   260|                             ]
   261|                         ),
   262|                     }
   263|                     memory.append(tool_call_message)
   264|                 if step_log.tool_calls is None and step_log.error is not None:
   265|                     message_content = (
   266|                         "Error:\n"
   267|                         + str(step_log.error)
   268|                         + "\nNow let's retry: take care not to repeat previous errors! If you have retried several times, try a completely different approach.\n"
   269|                     )
   270|                     tool_response_message = {
   271|                         "role": MessageRole.ASSISTANT,
   272|                         "content": message_content,
   273|                     }
   274|                 if step_log.tool_calls is not None and (
   275|                     step_log.error is not None or step_log.observations is not None
   276|                 ):
   277|                     if step_log.error is not None:
   278|                         message_content = (
   279|                             "Error:\n"
   280|                             + str(step_log.error)
   281|                             + "\nNow let's retry: take care not to repeat previous errors! If you have retried several times, try a completely different approach.\n"
   282|                         )
   283|                     elif step_log.observations is not None:
   284|                         message_content = f"Observation:\n{step_log.observations}"
   285|                     tool_response_message = {
   286|                         "role": MessageRole.TOOL_RESPONSE,
   287|                         "content": f"Call id: {(step_log.tool_calls[0].id if getattr(step_log.tool_calls[0], 'id') else 'call_0')}\n"
   288|                         + message_content,
   289|                     }
   290|                     memory.append(tool_response_message)
   291|         return memory
   292|     def get_succinct_logs(self):
   293|         return [
   294|             {key: value for key, value in log.items() if key != "agent_memory"}
   295|             for log in self.logs
   296|         ]
   297|     def extract_action(self, llm_output: str, split_token: str) -> Tuple[str, str]:
   298|         """
   299|         Parse action from the LLM output
   300|         Args:
   301|             llm_output (`str`): Output of the LLM
   302|             split_token (`str`): Separator for the action. Should match the example in the system prompt.
   303|         """
   304|         try:
   305|             split = llm_output.split(split_token)
   306|             rationale, action = (
   307|                 split[-2],
   308|                 split[-1],
   309|             )  # NOTE: using indexes starting from the end solves for when you have more than one split_token in the output
   310|         except Exception:
   311|             raise AgentParsingError(
   312|                 f"No '{split_token}' token provided in your output.\nYour output:\n{llm_output}\n. Be sure to include an action, prefaced with '{split_token}'!"
   313|             )
   314|         return rationale.strip(), action.strip()
   315|     def provide_final_answer(self, task) -> str:
   316|         """
   317|         This method provides a final answer to the task, based on the logs of the agent's interactions.
   318|         """
   319|         self.input_messages = [
   320|             {
   321|                 "role": MessageRole.SYSTEM,
   322|                 "content": "An agent tried to answer a user query but it got stuck and failed to do so. You are tasked with providing an answer instead. Here is the agent's memory:",
   323|             }
   324|         ]
   325|         self.input_messages += self.write_inner_memory_from_logs()[1:]
   326|         self.input_messages += [
   327|             {
   328|                 "role": MessageRole.USER,
   329|                 "content": f"Based on the above, please provide an answer to the following user request:\n{task}",
   330|             }
   331|         ]
   332|         try:
   333|             return self.model(self.input_messages).content
   334|         except Exception as e:
   335|             return f"Error in generating final LLM output:\n{e}"
   336|     def execute_tool_call(
   337|         self, tool_name: str, arguments: Union[Dict[str, str], str]
   338|     ) -> Any:
   339|         """
   340|         Execute tool with the provided input and returns the result.
   341|         This method replaces arguments with the actual values from the state if they refer to state variables.
   342|         Args:
   343|             tool_name (`str`): Name of the Tool to execute (should be one from self.tools).
   344|             arguments (Dict[str, str]): Arguments passed to the Tool.
   345|         """
   346|         available_tools = {**self.tools, **self.managed_agents}
   347|         if tool_name not in available_tools:
   348|             error_msg = f"Unknown tool {tool_name}, should be instead one of {list(available_tools.keys())}."
   349|             raise AgentExecutionError(error_msg)
   350|         try:
   351|             if isinstance(arguments, str):
   352|                 if tool_name in self.managed_agents:
   353|                     observation = available_tools[tool_name].__call__(arguments)
   354|                 else:
   355|                     observation = available_tools[tool_name].__call__(
   356|                         arguments, sanitize_inputs_outputs=True
   357|                     )
   358|             elif isinstance(arguments, dict):
   359|                 for key, value in arguments.items():
   360|                     if isinstance(value, str) and value in self.state:
   361|                         arguments[key] = self.state[value]
   362|                 if tool_name in self.managed_agents:
   363|                     observation = available_tools[tool_name].__call__(**arguments)
   364|                 else:
   365|                     observation = available_tools[tool_name].__call__(
   366|                         **arguments, sanitize_inputs_outputs=True
   367|                     )
   368|             else:
   369|                 error_msg = f"Arguments passed to tool should be a dict or string: got a {type(arguments)}."
   370|                 raise AgentExecutionError(error_msg)
   371|             return observation
   372|         except Exception as e:
   373|             if tool_name in self.tools:
   374|                 tool_description = get_tool_description_with_args(
   375|                     available_tools[tool_name]
   376|                 )
   377|                 error_msg = (
   378|                     f"Error in tool call execution: {e}\nYou should only use this tool with a correct input.\n"
   379|                     f"As a reminder, this tool's description is the following:\n{tool_description}"
   380|                 )
   381|                 raise AgentExecutionError(error_msg)
   382|             elif tool_name in self.managed_agents:
   383|                 error_msg = (
   384|                     f"Error in calling team member: {e}\nYou should only ask this team member with a correct request.\n"
   385|                     f"As a reminder, this team member's description is the following:\n{available_tools[tool_name]}"
   386|                 )
   387|                 raise AgentExecutionError(error_msg)
   388|     def step(self, log_entry: ActionStep) -> Union[None, Any]:
   389|         """To be implemented in children classes. Should return either None if the step is not final."""
   390|         pass
   391|     def run(
   392|         self,
   393|         task: str,
   394|         stream: bool = False,
   395|         reset: bool = True,
   396|         single_step: bool = False,
   397|         additional_args: Optional[Dict] = None,
   398|     ):
   399|         """
   400|         Runs the agent for the given task.
   401|         Args:
   402|             task (`str`): The task to perform.
   403|             stream (`bool`): Whether to run in a streaming way.
   404|             reset (`bool`): Whether to reset the conversation or keep it going from previous run.
   405|             single_step (`bool`): Whether to run the agent in one-shot fashion.
   406|             additional_args (`dict`): Any other variables that you want to pass to the agent run, for instance images or dataframes. Give them clear names!
   407|         Example:
   408|         ```py
   409|         from smolagents import CodeAgent
   410|         agent = CodeAgent(tools=[])
   411|         agent.run("What is the result of 2 power 3.7384?")
   412|         ```
   413|         """
   414|         self.task = task
   415|         if additional_args is not None:
   416|             self.state.update(additional_args)
   417|             self.task += f"""
   418| You have been provided with these additional arguments, that you can access using the keys as variables in your python code:
   419| {str(additional_args)}."""
   420|         self.initialize_system_prompt()
   421|         system_prompt_step = SystemPromptStep(system_prompt=self.system_prompt)
   422|         if reset:
   423|             self.logs = []
   424|             self.logs.append(system_prompt_step)
   425|             self.monitor.reset()
   426|         else:
   427|             if len(self.logs) > 0:
   428|                 self.logs[0] = system_prompt_step
   429|             else:
   430|                 self.logs.append(system_prompt_step)
   431|         self.logger.log(
   432|             Panel(
   433|                 f"\n[bold]{self.task.strip()}\n",
   434|                 title="[bold]New run",
   435|                 subtitle=f"{type(self.model).__name__} - {(self.model.model_id if hasattr(self.model, 'model_id') else '')}",
   436|                 border_style=YELLOW_HEX,
   437|                 subtitle_align="left",
   438|             ),
   439|             level=LogLevel.INFO,
   440|         )
   441|         self.logs.append(TaskStep(task=self.task))
   442|         if single_step:
   443|             step_start_time = time.time()
   444|             step_log = ActionStep(start_time=step_start_time)
   445|             step_log.end_time = time.time()
   446|             step_log.duration = step_log.end_time - step_start_time
   447|             result = self.step(step_log)
   448|             return result
   449|         if stream:
   450|             return self.stream_run(self.task)
   451|         else:
   452|             return self.direct_run(self.task)
   453|     def stream_run(self, task: str):
   454|         """
   455|         Runs the agent in streaming mode, yielding steps as they are executed: should be launched only in the `run` method.
   456|         """
   457|         final_answer = None
   458|         self.step_number = 0
   459|         while final_answer is None and self.step_number < self.max_steps:
   460|             step_start_time = time.time()
   461|             step_log = ActionStep(step=self.step_number, start_time=step_start_time)
   462|             try:
   463|                 if (
   464|                     self.planning_interval is not None
   465|                     and self.step_number % self.planning_interval == 0
   466|                 ):
   467|                     self.planning_step(
   468|                         task,
   469|                         is_first_step=(self.step_number == 0),
   470|                         step=self.step_number,
   471|                     )
   472|                 self.logger.log(
   473|                     Rule(
   474|                         f"[bold]Step {self.step_number}",
   475|                         characters="━",
   476|                         style=YELLOW_HEX,
   477|                     ),
   478|                     level=LogLevel.INFO,
   479|                 )
   480|                 final_answer = self.step(step_log)
   481|             except AgentError as e:
   482|                 step_log.error = e
   483|             finally:
   484|                 step_log.end_time = time.time()
   485|                 step_log.duration = step_log.end_time - step_start_time
   486|                 self.logs.append(step_log)
   487|                 for callback in self.step_callbacks:
   488|                     callback(step_log)
   489|                 self.step_number += 1
   490|                 yield step_log
   491|         if final_answer is None and self.step_number == self.max_steps:
   492|             error_message = "Reached max steps."
   493|             final_step_log = ActionStep(error=AgentMaxStepsError(error_message))
   494|             self.logs.append(final_step_log)
   495|             final_answer = self.provide_final_answer(task)
   496|             self.logger.log(Text(f"Final answer: {final_answer}"), level=LogLevel.INFO)
   497|             final_step_log.action_output = final_answer
   498|             final_step_log.end_time = time.time()
   499|             final_step_log.duration = step_log.end_time - step_start_time
   500|             for callback in self.step_callbacks:
   501|                 callback(final_step_log)
   502|             yield final_step_log
   503|         yield handle_agent_output_types(final_answer)
   504|     def direct_run(self, task: str):
   505|         """
   506|         Runs the agent in direct mode, returning outputs only at the end: should be launched only in the `run` method.
   507|         """
   508|         final_answer = None
   509|         self.step_number = 0
   510|         while final_answer is None and self.step_number < self.max_steps:
   511|             step_start_time = time.time()
   512|             step_log = ActionStep(step=self.step_number, start_time=step_start_time)
   513|             try:
   514|                 if (
   515|                     self.planning_interval is not None
   516|                     and self.step_number % self.planning_interval == 0
   517|                 ):
   518|                     self.planning_step(
   519|                         task,
   520|                         is_first_step=(self.step_number == 0),
   521|                         step=self.step_number,
   522|                     )
   523|                 self.logger.log(
   524|                     Rule(
   525|                         f"[bold]Step {self.step_number}",
   526|                         characters="━",
   527|                         style=YELLOW_HEX,
   528|                     ),
   529|                     level=LogLevel.INFO,
   530|                 )
   531|                 final_answer = self.step(step_log)
   532|             except AgentError as e:
   533|                 step_log.error = e
   534|             finally:
   535|                 step_end_time = time.time()
   536|                 step_log.end_time = step_end_time
   537|                 step_log.duration = step_end_time - step_start_time
   538|                 self.logs.append(step_log)
   539|                 for callback in self.step_callbacks:
   540|                     callback(step_log)
   541|                 self.step_number += 1
   542|         if final_answer is None and self.step_number == self.max_steps:
   543|             error_message = "Reached max steps."
   544|             final_step_log = ActionStep(error=AgentMaxStepsError(error_message))
   545|             self.logs.append(final_step_log)
   546|             final_answer = self.provide_final_answer(task)
   547|             self.logger.log(Text(f"Final answer: {final_answer}"), level=LogLevel.INFO)
   548|             final_step_log.action_output = final_answer
   549|             final_step_log.duration = 0
   550|             for callback in self.step_callbacks:
   551|                 callback(final_step_log)
   552|         return handle_agent_output_types(final_answer)
   553|     def planning_step(self, task, is_first_step: bool, step: int):
   554|         """
   555|         Used periodically by the agent to plan the next steps to reach the objective.
   556|         Args:
   557|             task (`str`): The task to perform
   558|             is_first_step (`bool`): If this step is not the first one, the plan should be an update over a previous plan.
   559|             step (`int`): The number of the current step, used as an indication for the LLM.
   560|         """
   561|         if is_first_step:
   562|             message_prompt_facts = {
   563|                 "role": MessageRole.SYSTEM,
   564|                 "content": SYSTEM_PROMPT_FACTS,
   565|             }
   566|             message_prompt_task = {
   567|                 "role": MessageRole.USER,
   568|                 "content": f"""Here is the task:
   569| ```
   570| {task}
   571| ```
   572| Now begin!""",
   573|             }
   574|             answer_facts = self.model(
   575|                 [message_prompt_facts, message_prompt_task]
   576|             ).content
   577|             message_system_prompt_plan = {
   578|                 "role": MessageRole.SYSTEM,
   579|                 "content": SYSTEM_PROMPT_PLAN,
   580|             }
   581|             message_user_prompt_plan = {
   582|                 "role": MessageRole.USER,
   583|                 "content": USER_PROMPT_PLAN.format(
   584|                     task=task,
   585|                     tool_descriptions=get_tool_descriptions(
   586|                         self.tools, self.tool_description_template
   587|                     ),
   588|                     managed_agents_descriptions=(
   589|                         show_agents_descriptions(self.managed_agents)
   590|                     ),
   591|                     answer_facts=answer_facts,
   592|                 ),
   593|             }
   594|             answer_plan = self.model(
   595|                 [message_system_prompt_plan, message_user_prompt_plan],
   596|                 stop_sequences=["<end_plan>"],
   597|             ).content
   598|             final_plan_redaction = f"""Here is the plan of action that I will follow to solve the task:
   599| ```
   600| {answer_plan}
   601| ```"""
   602|             final_facts_redaction = f"""Here are the facts that I know so far:
   603| ```
   604| {answer_facts}
   605| ```""".strip()
   606|             self.logs.append(
   607|                 PlanningStep(plan=final_plan_redaction, facts=final_facts_redaction)
   608|             )
   609|             self.logger.log(
   610|                 Rule("[bold]Initial plan", style="orange"),
   611|                 Text(final_plan_redaction),
   612|                 level=LogLevel.INFO,
   613|             )
   614|         else:  # update plan
   615|             agent_memory = self.write_inner_memory_from_logs(
   616|                 summary_mode=False
   617|             )  # This will not log the plan but will log facts
   618|             facts_update_system_prompt = {
   619|                 "role": MessageRole.SYSTEM,
   620|                 "content": SYSTEM_PROMPT_FACTS_UPDATE,
   621|             }
   622|             facts_update_message = {
   623|                 "role": MessageRole.USER,
   624|                 "content": USER_PROMPT_FACTS_UPDATE,
   625|             }
   626|             facts_update = self.model(
   627|                 [facts_update_system_prompt] + agent_memory + [facts_update_message]
   628|             ).content
   629|             plan_update_message = {
   630|                 "role": MessageRole.SYSTEM,
   631|                 "content": SYSTEM_PROMPT_PLAN_UPDATE.format(task=task),
   632|             }
   633|             plan_update_message_user = {
   634|                 "role": MessageRole.USER,
   635|                 "content": USER_PROMPT_PLAN_UPDATE.format(
   636|                     task=task,
   637|                     tool_descriptions=get_tool_descriptions(
   638|                         self.tools, self.tool_description_template
   639|                     ),
   640|                     managed_agents_descriptions=(
   641|                         show_agents_descriptions(self.managed_agents)
   642|                     ),
   643|                     facts_update=facts_update,
   644|                     remaining_steps=(self.max_steps - step),
   645|                 ),
   646|             }
   647|             plan_update = self.model(
   648|                 [plan_update_message] + agent_memory + [plan_update_message_user],
   649|                 stop_sequences=["<end_plan>"],
   650|             ).content
   651|             final_plan_redaction = PLAN_UPDATE_FINAL_PLAN_REDACTION.format(
   652|                 task=task, plan_update=plan_update
   653|             )
   654|             final_facts_redaction = f"""Here is the updated list of the facts that I know:
   655| ```
   656| {facts_update}
   657| ```"""
   658|             self.logs.append(
   659|                 PlanningStep(plan=final_plan_redaction, facts=final_facts_redaction)
   660|             )
   661|             self.logger.log(
   662|                 Rule("[bold]Updated plan", style="orange"),
   663|                 Text(final_plan_redaction),
   664|                 level=LogLevel.INFO,
   665|             )
   666| class ToolCallingAgent(MultiStepAgent):
   667|     """
   668|     This agent uses JSON-like tool calls, using method `model.get_tool_call` to leverage the LLM engine's tool calling capabilities.
   669|     """
   670|     def __init__(
   671|         self,
   672|         tools: List[Tool],
   673|         model: Callable,
   674|         system_prompt: Optional[str] = None,
   675|         planning_interval: Optional[int] = None,
   676|         **kwargs,
   677|     ):
   678|         if system_prompt is None:
   679|             system_prompt = TOOL_CALLING_SYSTEM_PROMPT
   680|         super().__init__(
   681|             tools=tools,
   682|             model=model,
   683|             system_prompt=system_prompt,
   684|             planning_interval=planning_interval,
   685|             **kwargs,
   686|         )
   687|     def step(self, log_entry: ActionStep) -> Union[None, Any]:
   688|         """
   689|         Perform one step in the ReAct framework: the agent thinks, acts, and observes the result.
   690|         Returns None if the step is not final.
   691|         """
   692|         agent_memory = self.write_inner_memory_from_logs()
   693|         self.input_messages = agent_memory
   694|         log_entry.agent_memory = agent_memory.copy()
   695|         try:
   696|             model_message = self.model(
   697|                 self.input_messages,
   698|                 tools_to_call_from=list(self.tools.values()),
   699|                 stop_sequences=["Observation:"],
   700|             )
   701|             tool_call = model_message.tool_calls[0]
   702|             tool_name, tool_call_id = tool_call.function.name, tool_call.id
   703|             tool_arguments = tool_call.function.arguments
   704|         except Exception as e:
   705|             raise AgentGenerationError(
   706|                 f"Error in generating tool call with model:\n{e}"
   707|             )
   708|         log_entry.tool_calls = [
   709|             ToolCall(name=tool_name, arguments=tool_arguments, id=tool_call_id)
   710|         ]
   711|         self.logger.log(
   712|             Panel(
   713|                 Text(f"Calling tool: '{tool_name}' with arguments: {tool_arguments}")
   714|             ),
   715|             level=LogLevel.INFO,
   716|         )
   717|         if tool_name == "final_answer":
   718|             if isinstance(tool_arguments, dict):
   719|                 if "answer" in tool_arguments:
   720|                     answer = tool_arguments["answer"]
   721|                 else:
   722|                     answer = tool_arguments
   723|             else:
   724|                 answer = tool_arguments
   725|             if (
   726|                 isinstance(answer, str) and answer in self.state.keys()
   727|             ):  # if the answer is a state variable, return the value
   728|                 final_answer = self.state[answer]
   729|                 self.logger.log(
   730|                     f"[bold {YELLOW_HEX}]Final answer:[/bold {YELLOW_HEX}] Extracting key '{answer}' from state to return value '{final_answer}'.",
   731|                     level=LogLevel.INFO,
   732|                 )
   733|             else:
   734|                 final_answer = answer

# --- HUNK 2: Lines 744-983 ---
   744|             observation = self.execute_tool_call(tool_name, tool_arguments)
   745|             observation_type = type(observation)
   746|             if observation_type in [AgentImage, AgentAudio]:
   747|                 if observation_type == AgentImage:
   748|                     observation_name = "image.png"
   749|                 elif observation_type == AgentAudio:
   750|                     observation_name = "audio.mp3"
   751|                 self.state[observation_name] = observation
   752|                 updated_information = f"Stored '{observation_name}' in memory."
   753|             else:
   754|                 updated_information = str(observation).strip()
   755|             self.logger.log(
   756|                 f"Observations: {updated_information.replace('[', '|')}",  # escape potential rich-tag-like components
   757|                 level=LogLevel.INFO,
   758|             )
   759|             log_entry.observations = updated_information
   760|             return None
   761| class CodeAgent(MultiStepAgent):
   762|     """
   763|     In this agent, the tool calls will be formulated by the LLM in code format, then parsed and executed.
   764|     """
   765|     def __init__(
   766|         self,
   767|         tools: List[Tool],
   768|         model: Callable,
   769|         system_prompt: Optional[str] = None,
   770|         grammar: Optional[Dict[str, str]] = None,
   771|         additional_authorized_imports: Optional[List[str]] = None,
   772|         planning_interval: Optional[int] = None,
   773|         use_e2b_executor: bool = False,
   774|         max_print_outputs_length: Optional[int] = None,
   775|         **kwargs,
   776|     ):
   777|         if system_prompt is None:
   778|             system_prompt = CODE_SYSTEM_PROMPT
   779|         self.additional_authorized_imports = (
   780|             additional_authorized_imports if additional_authorized_imports else []
   781|         )
   782|         self.authorized_imports = list(
   783|             set(BASE_BUILTIN_MODULES) | set(self.additional_authorized_imports)
   784|         )
   785|         if "{{authorized_imports}}" not in system_prompt:
   786|             raise AgentError(
   787|                 "Tag '{{authorized_imports}}' should be provided in the prompt."
   788|             )
   789|         if "*" in self.additional_authorized_imports:
   790|             self.logger.log(
   791|                 "Caution: you set an authorization for all imports, meaning your agent can decide to import any package it deems necessary. This might raise issues if the package is not installed in your environment.",
   792|                 0,
   793|             )
   794|         super().__init__(
   795|             tools=tools,
   796|             model=model,
   797|             system_prompt=system_prompt,
   798|             grammar=grammar,
   799|             planning_interval=planning_interval,
   800|             **kwargs,
   801|         )
   802|         if use_e2b_executor and len(self.managed_agents) > 0:
   803|             raise Exception(
   804|                 f"You passed both {use_e2b_executor=} and some managed agents. Managed agents is not yet supported with remote code execution."
   805|             )
   806|         all_tools = {**self.tools, **self.managed_agents}
   807|         if use_e2b_executor:
   808|             self.python_executor = E2BExecutor(
   809|                 self.additional_authorized_imports,
   810|                 list(all_tools.values()),
   811|                 self.logger,
   812|             )
   813|         else:
   814|             self.python_executor = LocalPythonInterpreter(
   815|                 self.additional_authorized_imports,
   816|                 all_tools,
   817|                 max_print_outputs_length=max_print_outputs_length,
   818|             )
   819|     def initialize_system_prompt(self):
   820|         super().initialize_system_prompt()
   821|         self.system_prompt = self.system_prompt.replace(
   822|             "{{authorized_imports}}",
   823|             "You can import from any package you want."
   824|             if "*" in self.authorized_imports
   825|             else str(self.authorized_imports),
   826|         )
   827|         return self.system_prompt
   828|     def step(self, log_entry: ActionStep) -> Union[None, Any]:
   829|         """
   830|         Perform one step in the ReAct framework: the agent thinks, acts, and observes the result.
   831|         Returns None if the step is not final.
   832|         """
   833|         agent_memory = self.write_inner_memory_from_logs()
   834|         self.input_messages = agent_memory.copy()
   835|         log_entry.agent_memory = agent_memory.copy()
   836|         try:
   837|             additional_args = (
   838|                 {"grammar": self.grammar} if self.grammar is not None else {}
   839|             )
   840|             llm_output = self.model(
   841|                 self.input_messages,
   842|                 stop_sequences=["<end_code>", "Observation:"],
   843|                 **additional_args,
   844|             ).content
   845|             log_entry.llm_output = llm_output
   846|         except Exception as e:
   847|             raise AgentGenerationError(f"Error in generating model output:\n{e}")
   848|         self.logger.log(
   849|             Group(
   850|                 Rule(
   851|                     "[italic]Output message of the LLM:",
   852|                     align="left",
   853|                     style="orange",
   854|                 ),
   855|                 Syntax(
   856|                     llm_output,
   857|                     lexer="markdown",
   858|                     theme="github-dark",
   859|                     word_wrap=True,
   860|                 ),
   861|             ),
   862|             level=LogLevel.DEBUG,
   863|         )
   864|         try:
   865|             code_action = fix_final_answer_code(parse_code_blobs(llm_output))
   866|         except Exception as e:
   867|             error_msg = (
   868|                 f"Error in code parsing:\n{e}\nMake sure to provide correct code blobs."
   869|             )
   870|             raise AgentParsingError(error_msg)
   871|         log_entry.tool_calls = [
   872|             ToolCall(
   873|                 name="python_interpreter",
   874|                 arguments=code_action,
   875|                 id=f"call_{len(self.logs)}",
   876|             )
   877|         ]
   878|         self.logger.log(
   879|             Panel(
   880|                 Syntax(
   881|                     code_action,
   882|                     lexer="python",
   883|                     theme="monokai",
   884|                     word_wrap=True,
   885|                 ),
   886|                 title="[bold]Executing this code:",
   887|                 title_align="left",
   888|                 box=box.HORIZONTALS,
   889|             ),
   890|             level=LogLevel.INFO,
   891|         )
   892|         observation = ""
   893|         is_final_answer = False
   894|         try:
   895|             output, execution_logs, is_final_answer = self.python_executor(
   896|                 code_action,
   897|                 self.state,
   898|             )
   899|             execution_outputs_console = []
   900|             if len(execution_logs) > 0:
   901|                 execution_outputs_console += [
   902|                     Text("Execution logs:", style="bold"),
   903|                     Text(execution_logs),
   904|                 ]
   905|             observation += "Execution logs:\n" + execution_logs
   906|         except Exception as e:
   907|             if isinstance(e, SyntaxError):
   908|                 error_msg = (
   909|                     f"Code execution failed on line {e.lineno} due to: {type(e).__name__}\n"
   910|                     f"{e.text}"
   911|                     f"{' ' * (e.offset or 0)}^\n"
   912|                     f"Error: {str(e)}"
   913|                 )
   914|             else:
   915|                 error_msg = str(e)
   916|             if "Import of " in str(e) and " is not allowed" in str(e):
   917|                 self.logger.log(
   918|                     "[bold red]Warning to user: Code execution failed due to an unauthorized import - Consider passing said import under `additional_authorized_imports` when initializing your CodeAgent.",
   919|                     level=LogLevel.INFO,
   920|                 )
   921|             raise AgentExecutionError(error_msg)
   922|         truncated_output = truncate_content(str(output))
   923|         observation += "Last output from code snippet:\n" + truncated_output
   924|         log_entry.observations = observation
   925|         execution_outputs_console += [
   926|             Text(
   927|                 f"{('Out - Final answer' if is_final_answer else 'Out')}: {truncated_output}",
   928|                 style=(f"bold {YELLOW_HEX}" if is_final_answer else ""),
   929|             ),
   930|         ]
   931|         self.logger.log(Group(*execution_outputs_console), level=LogLevel.INFO)
   932|         log_entry.action_output = output
   933|         return output if is_final_answer else None
   934| class ManagedAgent:
   935|     def __init__(
   936|         self,
   937|         agent,
   938|         name,
   939|         description,
   940|         additional_prompting: Optional[str] = None,
   941|         provide_run_summary: bool = False,
   942|         managed_agent_prompt: Optional[str] = None,
   943|     ):
   944|         self.agent = agent
   945|         self.name = name
   946|         self.description = description
   947|         self.additional_prompting = additional_prompting
   948|         self.provide_run_summary = provide_run_summary
   949|         self.managed_agent_prompt = (
   950|             managed_agent_prompt if managed_agent_prompt else MANAGED_AGENT_PROMPT
   951|         )
   952|     def write_full_task(self, task):
   953|         """Adds additional prompting for the managed agent, like 'add more detail in your answer'."""
   954|         full_task = self.managed_agent_prompt.format(name=self.name, task=task)
   955|         if self.additional_prompting:
   956|             full_task = full_task.replace(
   957|                 "\n{{additional_prompting}}", self.additional_prompting
   958|             ).strip()
   959|         else:
   960|             full_task = full_task.replace("\n{{additional_prompting}}", "").strip()
   961|         return full_task
   962|     def __call__(self, request, **kwargs):
   963|         full_task = self.write_full_task(request)
   964|         output = self.agent.run(full_task, **kwargs)
   965|         if self.provide_run_summary:
   966|             answer = (
   967|                 f"Here is the final answer from your managed agent '{self.name}':\n"
   968|             )
   969|             answer += str(output)
   970|             answer += f"\n\nFor more detail, find below a summary of this agent's work:\nSUMMARY OF WORK FROM AGENT '{self.name}':\n"
   971|             for message in self.agent.write_inner_memory_from_logs(summary_mode=True):
   972|                 content = message["content"]
   973|                 answer += "\n" + truncate_content(str(content)) + "\n---"
   974|             answer += f"\nEND OF SUMMARY OF WORK FROM AGENT '{self.name}'."
   975|             return answer
   976|         else:
   977|             return output
   978| __all__ = [
   979|     "ManagedAgent",
   980|     "MultiStepAgent",
   981|     "CodeAgent",
   982|     "ToolCallingAgent",
   983| ]


# ====================================================================
# FILE: src/smolagents/default_tools.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-267 ---
     1| import json
     2| import re
     3| from dataclasses import dataclass
     4| from typing import Dict, Optional
     5| from huggingface_hub import hf_hub_download, list_spaces
     6| from transformers.utils import is_offline_mode, is_torch_available
     7| from .local_python_executor import (
     8|     BASE_BUILTIN_MODULES,
     9|     BASE_PYTHON_TOOLS,
    10|     evaluate_python_code,
    11| )
    12| from .tools import TOOL_CONFIG_FILE, PipelineTool, Tool
    13| from .types import AgentAudio
    14| if is_torch_available():
    15|     from transformers.models.whisper import (
    16|         WhisperForConditionalGeneration,
    17|         WhisperProcessor,
    18|     )
    19| else:
    20|     WhisperForConditionalGeneration = object
    21|     WhisperProcessor = object
    22| @dataclass
    23| class PreTool:
    24|     name: str
    25|     inputs: Dict[str, str]
    26|     output_type: type
    27|     task: str
    28|     description: str
    29|     repo_id: str
    30| def get_remote_tools(logger, organization="huggingface-tools"):
    31|     if is_offline_mode():
    32|         logger.info("You are in offline mode, so remote tools are not available.")
    33|         return {}
    34|     spaces = list_spaces(author=organization)
    35|     tools = {}
    36|     for space_info in spaces:
    37|         repo_id = space_info.id
    38|         resolved_config_file = hf_hub_download(
    39|             repo_id, TOOL_CONFIG_FILE, repo_type="space"
    40|         )
    41|         with open(resolved_config_file, encoding="utf-8") as reader:
    42|             config = json.load(reader)
    43|         task = repo_id.split("/")[-1]
    44|         tools[config["name"]] = PreTool(
    45|             task=task,
    46|             description=config["description"],
    47|             repo_id=repo_id,
    48|             name=task,
    49|             inputs=config["inputs"],
    50|             output_type=config["output_type"],
    51|         )
    52|     return tools
    53| class PythonInterpreterTool(Tool):
    54|     name = "python_interpreter"
    55|     description = "This is a tool that evaluates python code. It can be used to perform calculations."
    56|     inputs = {
    57|         "code": {
    58|             "type": "string",
    59|             "description": "The python code to run in interpreter",
    60|         }
    61|     }
    62|     output_type = "string"
    63|     def __init__(self, *args, authorized_imports=None, **kwargs):
    64|         if authorized_imports is None:
    65|             self.authorized_imports = list(set(BASE_BUILTIN_MODULES))
    66|         else:
    67|             self.authorized_imports = list(
    68|                 set(BASE_BUILTIN_MODULES) | set(authorized_imports)
    69|             )
    70|         self.inputs = {
    71|             "code": {
    72|                 "type": "string",
    73|                 "description": (
    74|                     "The code snippet to evaluate. All variables used in this snippet must be defined in this same snippet, "
    75|                     f"else you will get an error. This code can only import the following python libraries: {authorized_imports}."
    76|                 ),
    77|             }
    78|         }
    79|         self.base_python_tools = BASE_PYTHON_TOOLS
    80|         self.python_evaluator = evaluate_python_code
    81|         super().__init__(*args, **kwargs)
    82|     def forward(self, code: str) -> str:
    83|         state = {}
    84|         output = str(
    85|             self.python_evaluator(
    86|                 code,
    87|                 state=state,
    88|                 static_tools=self.base_python_tools,
    89|                 authorized_imports=self.authorized_imports,
    90|             )[0]  # The second element is boolean is_final_answer
    91|         )
    92|         return f"Stdout:\n{state['print_outputs']}\nOutput: {output}"
    93| class FinalAnswerTool(Tool):
    94|     name = "final_answer"
    95|     description = "Provides a final answer to the given problem."
    96|     inputs = {
    97|         "answer": {"type": "any", "description": "The final answer to the problem"}
    98|     }
    99|     output_type = "any"
   100|     def forward(self, answer):
   101|         return answer
   102| class UserInputTool(Tool):
   103|     name = "user_input"
   104|     description = "Asks for user's input on a specific question"
   105|     inputs = {
   106|         "question": {"type": "string", "description": "The question to ask the user"}
   107|     }
   108|     output_type = "string"
   109|     def forward(self, question):
   110|         user_input = input(f"{question} => Type your answer here:")
   111|         return user_input
   112| class DuckDuckGoSearchTool(Tool):
   113|     name = "web_search"
   114|     description = """Performs a duckduckgo web search based on your query (think a Google search) then returns the top search results."""
   115|     inputs = {
   116|         "query": {"type": "string", "description": "The search query to perform."}
   117|     }
   118|     output_type = "string"
   119|     def __init__(self, *args, max_results=10, **kwargs):
   120|         super().__init__(*args, **kwargs)
   121|         self.max_results = max_results
   122|         try:
   123|             from duckduckgo_search import DDGS
   124|         except ImportError:
   125|             raise ImportError(
   126|                 "You must install package `duckduckgo_search` to run this tool: for instance run `pip install duckduckgo-search`."
   127|             )
   128|         self.ddgs = DDGS()
   129|     def forward(self, query: str) -> str:
   130|         results = self.ddgs.text(query, max_results=self.max_results)
   131|         postprocessed_results = [
   132|             f"[{result['title']}]({result['href']})\n{result['body']}"
   133|             for result in results
   134|         ]
   135|         return "## Search Results\n\n" + "\n\n".join(postprocessed_results)
   136| class GoogleSearchTool(Tool):
   137|     name = "web_search"
   138|     description = """Performs a google web search for your query then returns a string of the top search results."""
   139|     inputs = {
   140|         "query": {"type": "string", "description": "The search query to perform."},
   141|         "filter_year": {
   142|             "type": "integer",
   143|             "description": "Optionally restrict results to a certain year",
   144|             "nullable": True,
   145|         },
   146|     }
   147|     output_type = "string"
   148|     def __init__(self):
   149|         super().__init__(self)
   150|         import os
   151|         self.serpapi_key = os.getenv("SERPAPI_API_KEY")
   152|     def forward(self, query: str, filter_year: Optional[int] = None) -> str:
   153|         import requests
   154|         if self.serpapi_key is None:
   155|             raise ValueError(
   156|                 "Missing SerpAPI key. Make sure you have 'SERPAPI_API_KEY' in your env variables."
   157|             )
   158|         params = {
   159|             "engine": "google",
   160|             "q": query,
   161|             "api_key": self.serpapi_key,
   162|             "google_domain": "google.com",
   163|         }
   164|         if filter_year is not None:
   165|             params["tbs"] = (
   166|                 f"cdr:1,cd_min:01/01/{filter_year},cd_max:12/31/{filter_year}"
   167|             )
   168|         response = requests.get("https://serpapi.com/search.json", params=params)
   169|         if response.status_code == 200:
   170|             results = response.json()
   171|         else:
   172|             raise ValueError(response.json())
   173|         if "organic_results" not in results.keys():
   174|             if filter_year is not None:
   175|                 raise Exception(
   176|                     f"'organic_results' key not found for query: '{query}' with filtering on year={filter_year}. Use a less restrictive query or do not filter on year."
   177|                 )
   178|             else:
   179|                 raise Exception(
   180|                     f"'organic_results' key not found for query: '{query}'. Use a less restrictive query."
   181|                 )
   182|         if len(results["organic_results"]) == 0:
   183|             year_filter_message = (
   184|                 f" with filter year={filter_year}" if filter_year is not None else ""
   185|             )
   186|             return f"No results found for '{query}'{year_filter_message}. Try with a more general query, or remove the year filter."
   187|         web_snippets = []
   188|         if "organic_results" in results:
   189|             for idx, page in enumerate(results["organic_results"]):
   190|                 date_published = ""
   191|                 if "date" in page:
   192|                     date_published = "\nDate published: " + page["date"]
   193|                 source = ""
   194|                 if "source" in page:
   195|                     source = "\nSource: " + page["source"]
   196|                 snippet = ""
   197|                 if "snippet" in page:
   198|                     snippet = "\n" + page["snippet"]
   199|                 redacted_version = f"{idx}. [{page['title']}]({page['link']}){date_published}{source}\n{snippet}"
   200|                 redacted_version = redacted_version.replace(
   201|                     "Your browser can't play this video.", ""
   202|                 )
   203|                 web_snippets.append(redacted_version)
   204|         return "## Search Results\n" + "\n\n".join(web_snippets)
   205| class VisitWebpageTool(Tool):
   206|     name = "visit_webpage"
   207|     description = "Visits a webpage at the given url and reads its content as a markdown string. Use this to browse webpages."
   208|     inputs = {
   209|         "url": {
   210|             "type": "string",
   211|             "description": "The url of the webpage to visit.",
   212|         }
   213|     }
   214|     output_type = "string"
   215|     def forward(self, url: str) -> str:
   216|         try:
   217|             import requests
   218|             from markdownify import markdownify
   219|             from requests.exceptions import RequestException
   220|             from smolagents.utils import truncate_content
   221|         except ImportError:
   222|             raise ImportError(
   223|                 "You must install packages `markdownify` and `requests` to run this tool: for instance run `pip install markdownify requests`."
   224|             )
   225|         try:
   226|             response = requests.get(url)
   227|             response.raise_for_status()  # Raise an exception for bad status codes
   228|             markdown_content = markdownify(response.text).strip()
   229|             markdown_content = re.sub(r"\n{3,}", "\n\n", markdown_content)
   230|             return truncate_content(markdown_content, 10000)
   231|         except RequestException as e:
   232|             return f"Error fetching the webpage: {str(e)}"
   233|         except Exception as e:
   234|             return f"An unexpected error occurred: {str(e)}"
   235| class SpeechToTextTool(PipelineTool):
   236|     default_checkpoint = "openai/whisper-large-v3-turbo"
   237|     description = "This is a tool that transcribes an audio into text. It returns the transcribed text."
   238|     name = "transcriber"
   239|     pre_processor_class = WhisperProcessor
   240|     model_class = WhisperForConditionalGeneration
   241|     inputs = {
   242|         "audio": {
   243|             "type": "audio",
   244|             "description": "The audio to transcribe. Can be a local path, an url, or a tensor.",
   245|         }
   246|     }
   247|     output_type = "string"
   248|     def encode(self, audio):
   249|         audio = AgentAudio(audio).to_raw()
   250|         return self.pre_processor(audio, return_tensors="pt")
   251|     def forward(self, inputs):
   252|         return self.model.generate(inputs["input_features"])
   253|     def decode(self, outputs):
   254|         return self.pre_processor.batch_decode(outputs, skip_special_tokens=True)[0]
   255| TOOL_MAPPING = {
   256|     tool_class.name: tool_class
   257|     for tool_class in [
   258|         PythonInterpreterTool,
   259|         DuckDuckGoSearchTool,
   260|         VisitWebpageTool,
   261|     ]
   262| }
   263| __all__ = [
   264|     "PythonInterpreterTool",
   265|     "FinalAnswerTool",
   266|     "UserInputTool",
   267|     "DuckDuckGoSearchTool",


# ====================================================================
# FILE: src/smolagents/e2b_executor.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-56 ---
     1| import base64
     2| import pickle
     3| import textwrap
     4| from io import BytesIO
     5| from typing import Any, List, Tuple
     6| from dotenv import load_dotenv
     7| from e2b_code_interpreter import Sandbox
     8| from PIL import Image
     9| from .tool_validation import validate_tool_attributes
    10| from .tools import Tool
    11| from .utils import BASE_BUILTIN_MODULES, instance_to_source
    12| load_dotenv()
    13| class E2BExecutor:
    14|     def __init__(self, additional_imports: List[str], tools: List[Tool], logger):
    15|         self.custom_tools = {}
    16|         self.sbx = Sandbox()  # "qywp2ctmu2q7jzprcf4j")
    17|         self.logger = logger
    18|         additional_imports = additional_imports + ["pickle5", "smolagents"]
    19|         if len(additional_imports) > 0:
    20|             execution = self.sbx.commands.run(
    21|                 "pip install " + " ".join(additional_imports)
    22|             )
    23|             if execution.error:
    24|                 raise Exception(f"Error installing dependencies: {execution.error}")
    25|             else:
    26|                 logger.log(f"Installation of {additional_imports} succeeded!", 0)
    27|         tool_codes = []
    28|         for tool in tools:
    29|             validate_tool_attributes(tool.__class__, check_imports=False)
    30|             tool_code = instance_to_source(tool, base_cls=Tool)
    31|             tool_code = tool_code.replace("from smolagents.tools import Tool", "")
    32|             tool_code += f"\n{tool.name} = {tool.__class__.__name__}()\n"
    33|             tool_codes.append(tool_code)
    34|         tool_definition_code = "\n".join(
    35|             [f"import {module}" for module in BASE_BUILTIN_MODULES]
    36|         )
    37|         tool_definition_code += textwrap.dedent("""
    38|         class Tool:
    39|             def __call__(self, *args, **kwargs):
    40|                 return self.forward(*args, **kwargs)
    41|             def forward(self, *args, **kwargs):
    42|                 pass # to be implemented in child class
    43|         """)
    44|         tool_definition_code += "\n\n".join(tool_codes)
    45|         tool_definition_execution = self.run_code_raise_errors(tool_definition_code)
    46|         self.logger.log(tool_definition_execution.logs)
    47|     def run_code_raise_errors(self, code: str):
    48|         execution = self.sbx.run_code(
    49|             code,
    50|         )
    51|         if execution.error:
    52|             execution_logs = "\n".join([str(log) for log in execution.logs.stdout])
    53|             logs = execution_logs
    54|             logs += "Executing code yielded an error:"
    55|             logs += execution.error.name
    56|             logs += execution.error.value

# --- HUNK 2: Lines 68-107 ---
    68|             remote_unloading_code = """import pickle
    69| import os
    70| print("File path", os.path.getsize('/home/state.pkl'))
    71| with open('/home/state.pkl', 'rb') as f:
    72|     pickle_dict = pickle.load(f)
    73| locals().update({key: value for key, value in pickle_dict.items()})
    74| """
    75|             execution = self.run_code_raise_errors(remote_unloading_code)
    76|             execution_logs = "\n".join([str(log) for log in execution.logs.stdout])
    77|             self.logger.log(execution_logs, 1)
    78|         execution = self.run_code_raise_errors(code_action)
    79|         execution_logs = "\n".join([str(log) for log in execution.logs.stdout])
    80|         if not execution.results:
    81|             return None, execution_logs
    82|         else:
    83|             for result in execution.results:
    84|                 if result.is_main_result:
    85|                     for attribute_name in ["jpeg", "png"]:
    86|                         if getattr(result, attribute_name) is not None:
    87|                             image_output = getattr(result, attribute_name)
    88|                             decoded_bytes = base64.b64decode(
    89|                                 image_output.encode("utf-8")
    90|                             )
    91|                             return Image.open(BytesIO(decoded_bytes)), execution_logs
    92|                     for attribute_name in [
    93|                         "chart",
    94|                         "data",
    95|                         "html",
    96|                         "javascript",
    97|                         "json",
    98|                         "latex",
    99|                         "markdown",
   100|                         "pdf",
   101|                         "svg",
   102|                         "text",
   103|                     ]:
   104|                         if getattr(result, attribute_name) is not None:
   105|                             return getattr(result, attribute_name), execution_logs
   106|             raise ValueError("No main result returned by executor!")
   107| __all__ = ["E2BExecutor"]


# ====================================================================
# FILE: src/smolagents/gradio_ui.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-158 ---
     1| import gradio as gr
     2| import shutil
     3| import os
     4| import mimetypes
     5| import re
     6| from typing import Optional
     7| from .agents import ActionStep, AgentStepLog, MultiStepAgent
     8| from .types import AgentAudio, AgentImage, AgentText, handle_agent_output_types
     9| def pull_messages_from_step(step_log: AgentStepLog, test_mode: bool = True):
    10|     """Extract ChatMessage objects from agent steps"""
    11|     if isinstance(step_log, ActionStep):
    12|         yield gr.ChatMessage(role="assistant", content=step_log.llm_output or "")
    13|         if step_log.tool_calls is not None:
    14|             first_tool_call = step_log.tool_calls[0]
    15|             used_code = first_tool_call.name == "code interpreter"
    16|             content = first_tool_call.arguments
    17|             if used_code:
    18|                 content = f"```py\n{content}\n```"
    19|             yield gr.ChatMessage(
    20|                 role="assistant",
    21|                 metadata={"title": f"🛠️ Used tool {first_tool_call.name}"},
    22|                 content=str(content),
    23|             )
    24|         if step_log.observations is not None:
    25|             yield gr.ChatMessage(role="assistant", content=step_log.observations)
    26|         if step_log.error is not None:
    27|             yield gr.ChatMessage(
    28|                 role="assistant",
    29|                 content=str(step_log.error),
    30|                 metadata={"title": "💥 Error"},
    31|             )
    32| def stream_to_gradio(
    33|     agent,
    34|     task: str,
    35|     test_mode: bool = False,
    36|     reset_agent_memory: bool = False,
    37|     additional_args: Optional[dict] = None,
    38| ):
    39|     """Runs an agent with the given task and streams the messages from the agent as gradio ChatMessages."""
    40|     for step_log in agent.run(
    41|         task, stream=True, reset=reset_agent_memory, additional_args=additional_args
    42|     ):
    43|         for message in pull_messages_from_step(step_log, test_mode=test_mode):
    44|             yield message
    45|     final_answer = step_log  # Last log is the run's final_answer
    46|     final_answer = handle_agent_output_types(final_answer)
    47|     if isinstance(final_answer, AgentText):
    48|         yield gr.ChatMessage(
    49|             role="assistant",
    50|             content=f"**Final answer:**\n{final_answer.to_string()}\n",
    51|         )
    52|     elif isinstance(final_answer, AgentImage):
    53|         yield gr.ChatMessage(
    54|             role="assistant",
    55|             content={"path": final_answer.to_string(), "mime_type": "image/png"},
    56|         )
    57|     elif isinstance(final_answer, AgentAudio):
    58|         yield gr.ChatMessage(
    59|             role="assistant",
    60|             content={"path": final_answer.to_string(), "mime_type": "audio/wav"},
    61|         )
    62|     else:
    63|         yield gr.ChatMessage(role="assistant", content=str(final_answer))
    64| class GradioUI:
    65|     """A one-line interface to launch your agent in Gradio"""
    66|     def __init__(self, agent: MultiStepAgent, file_upload_folder: str | None = None):
    67|         self.agent = agent
    68|         self.file_upload_folder = file_upload_folder
    69|         if self.file_upload_folder is not None:
    70|             if not os.path.exists(file_upload_folder):
    71|                 os.mkdir(file_upload_folder)
    72|     def interact_with_agent(self, prompt, messages):
    73|         messages.append(gr.ChatMessage(role="user", content=prompt))
    74|         yield messages
    75|         for msg in stream_to_gradio(self.agent, task=prompt, reset_agent_memory=False):
    76|             messages.append(msg)
    77|             yield messages
    78|         yield messages
    79|     def upload_file(
    80|         self,
    81|         file,
    82|         file_uploads_log,
    83|         allowed_file_types=[
    84|             "application/pdf",
    85|             "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
    86|             "text/plain",
    87|         ],
    88|     ):
    89|         """
    90|         Handle file uploads, default allowed types are .pdf, .docx, and .txt
    91|         """
    92|         if file is None:
    93|             return gr.Textbox("No file uploaded", visible=True), file_uploads_log
    94|         try:
    95|             mime_type, _ = mimetypes.guess_type(file.name)
    96|         except Exception as e:
    97|             return gr.Textbox(f"Error: {e}", visible=True), file_uploads_log
    98|         if mime_type not in allowed_file_types:
    99|             return gr.Textbox("File type disallowed", visible=True), file_uploads_log
   100|         original_name = os.path.basename(file.name)
   101|         sanitized_name = re.sub(
   102|             r"[^\w\-.]", "_", original_name
   103|         )  # Replace any non-alphanumeric, non-dash, or non-dot characters with underscores
   104|         type_to_ext = {}
   105|         for ext, t in mimetypes.types_map.items():
   106|             if t not in type_to_ext:
   107|                 type_to_ext[t] = ext
   108|         sanitized_name = sanitized_name.split(".")[:-1]
   109|         sanitized_name.append("" + type_to_ext[mime_type])
   110|         sanitized_name = "".join(sanitized_name)
   111|         file_path = os.path.join(
   112|             self.file_upload_folder, os.path.basename(sanitized_name)
   113|         )
   114|         shutil.copy(file.name, file_path)
   115|         return gr.Textbox(
   116|             f"File uploaded: {file_path}", visible=True
   117|         ), file_uploads_log + [file_path]
   118|     def log_user_message(self, text_input, file_uploads_log):
   119|         return (
   120|             text_input
   121|             + (
   122|                 f"\nYou have been provided with these files, which might be helpful or not: {file_uploads_log}"
   123|                 if len(file_uploads_log) > 0
   124|                 else ""
   125|             ),
   126|             "",
   127|         )
   128|     def launch(self):
   129|         with gr.Blocks() as demo:
   130|             stored_messages = gr.State([])
   131|             file_uploads_log = gr.State([])
   132|             chatbot = gr.Chatbot(
   133|                 label="Agent",
   134|                 type="messages",
   135|                 avatar_images=(
   136|                     None,
   137|                     "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/mascot_smol.png",
   138|                 ),
   139|                 resizeable=True,
   140|             )
   141|             if self.file_upload_folder is not None:
   142|                 upload_file = gr.File(label="Upload a file")
   143|                 upload_status = gr.Textbox(
   144|                     label="Upload Status", interactive=False, visible=False
   145|                 )
   146|                 upload_file.change(
   147|                     self.upload_file,
   148|                     [upload_file, file_uploads_log],
   149|                     [upload_status, file_uploads_log],
   150|                 )
   151|             text_input = gr.Textbox(lines=1, label="Chat Message")
   152|             text_input.submit(
   153|                 self.log_user_message,
   154|                 [text_input, file_uploads_log],
   155|                 [stored_messages, text_input],
   156|             ).then(self.interact_with_agent, [stored_messages, chatbot], [chatbot])
   157|         demo.launch()
   158| __all__ = ["stream_to_gradio", "GradioUI"]


# ====================================================================
# FILE: src/smolagents/local_python_executor.py
# Total hunks: 14
# ====================================================================
# --- HUNK 1: Lines 1-43 ---
     1| import ast
     2| import builtins
     3| import difflib
     4| import math
     5| import re
     6| from collections.abc import Mapping
     7| from importlib import import_module
     8| from types import ModuleType
     9| from typing import Any, Callable, Dict, List, Optional, Tuple
    10| import numpy as np
    11| import pandas as pd
    12| from .utils import BASE_BUILTIN_MODULES, truncate_content
    13| class InterpreterError(ValueError):
    14|     """
    15|     An error raised when the interpreter cannot evaluate a Python expression, due to syntax error or unsupported
    16|     operations.
    17|     """
    18|     pass
    19| ERRORS = {
    20|     name: getattr(builtins, name)
    21|     for name in dir(builtins)
    22|     if isinstance(getattr(builtins, name), type)
    23|     and issubclass(getattr(builtins, name), BaseException)
    24| }
    25| PRINT_OUTPUTS, DEFAULT_MAX_LEN_OUTPUT = "", 50000
    26| OPERATIONS_COUNT, MAX_OPERATIONS = 0, 10000000
    27| def custom_print(*args):
    28|     return None
    29| BASE_PYTHON_TOOLS = {
    30|     "print": custom_print,
    31|     "isinstance": isinstance,
    32|     "range": range,
    33|     "float": float,
    34|     "int": int,
    35|     "bool": bool,
    36|     "str": str,
    37|     "set": set,
    38|     "list": list,
    39|     "dict": dict,
    40|     "tuple": tuple,
    41|     "round": round,
    42|     "ceil": math.ceil,
    43|     "floor": math.floor,

# --- HUNK 2: Lines 98-1353 ---
    98|     """
    99|     Sometimes an LLM can try to assign a variable to final_answer, which would break the final_answer() tool.
   100|     This function fixes this behaviour by replacing variable assignments to final_answer with final_answer_variable,
   101|     while preserving function calls to final_answer().
   102|     """
   103|     assignment_pattern = r"(?<!\.)(?<!\w)\bfinal_answer\s*="
   104|     if "final_answer(" not in code or not re.search(assignment_pattern, code):
   105|         return code
   106|     assignment_regex = r"(?<!\.)(?<!\w)(\bfinal_answer)(\s*=)"
   107|     code = re.sub(assignment_regex, r"final_answer_variable\2", code)
   108|     variable_regex = r"(?<!\.)(?<!\w)(\bfinal_answer\b)(?!\s*\()"
   109|     code = re.sub(variable_regex, "final_answer_variable", code)
   110|     return code
   111| def evaluate_unaryop(
   112|     expression: ast.UnaryOp,
   113|     state: Dict[str, Any],
   114|     static_tools: Dict[str, Callable],
   115|     custom_tools: Dict[str, Callable],
   116|     authorized_imports: List[str],
   117| ) -> Any:
   118|     operand = evaluate_ast(
   119|         expression.operand, state, static_tools, custom_tools, authorized_imports
   120|     )
   121|     if isinstance(expression.op, ast.USub):
   122|         return -operand
   123|     elif isinstance(expression.op, ast.UAdd):
   124|         return operand
   125|     elif isinstance(expression.op, ast.Not):
   126|         return not operand
   127|     elif isinstance(expression.op, ast.Invert):
   128|         return ~operand
   129|     else:
   130|         raise InterpreterError(
   131|             f"Unary operation {expression.op.__class__.__name__} is not supported."
   132|         )
   133| def evaluate_lambda(
   134|     lambda_expression: ast.Lambda,
   135|     state: Dict[str, Any],
   136|     static_tools: Dict[str, Callable],
   137|     custom_tools: Dict[str, Callable],
   138|     authorized_imports: List[str],
   139| ) -> Callable:
   140|     args = [arg.arg for arg in lambda_expression.args.args]
   141|     def lambda_func(*values: Any) -> Any:
   142|         new_state = state.copy()
   143|         for arg, value in zip(args, values):
   144|             new_state[arg] = value
   145|         return evaluate_ast(
   146|             lambda_expression.body,
   147|             new_state,
   148|             static_tools,
   149|             custom_tools,
   150|             authorized_imports,
   151|         )
   152|     return lambda_func
   153| def evaluate_while(
   154|     while_loop: ast.While,
   155|     state: Dict[str, Any],
   156|     static_tools: Dict[str, Callable],
   157|     custom_tools: Dict[str, Callable],
   158|     authorized_imports: List[str],
   159| ) -> None:
   160|     max_iterations = 1000
   161|     iterations = 0
   162|     while evaluate_ast(
   163|         while_loop.test, state, static_tools, custom_tools, authorized_imports
   164|     ):
   165|         for node in while_loop.body:
   166|             try:
   167|                 evaluate_ast(
   168|                     node, state, static_tools, custom_tools, authorized_imports
   169|                 )
   170|             except BreakException:
   171|                 return None
   172|             except ContinueException:
   173|                 break
   174|         iterations += 1
   175|         if iterations > max_iterations:
   176|             raise InterpreterError(
   177|                 f"Maximum number of {max_iterations} iterations in While loop exceeded"
   178|             )
   179|     return None
   180| def create_function(
   181|     func_def: ast.FunctionDef,
   182|     state: Dict[str, Any],
   183|     static_tools: Dict[str, Callable],
   184|     custom_tools: Dict[str, Callable],
   185|     authorized_imports: List[str],
   186| ) -> Callable:
   187|     def new_func(*args: Any, **kwargs: Any) -> Any:
   188|         func_state = state.copy()
   189|         arg_names = [arg.arg for arg in func_def.args.args]
   190|         default_values = [
   191|             evaluate_ast(d, state, static_tools, custom_tools, authorized_imports)
   192|             for d in func_def.args.defaults
   193|         ]
   194|         defaults = dict(zip(arg_names[-len(default_values) :], default_values))
   195|         for name, value in zip(arg_names, args):
   196|             func_state[name] = value
   197|         for name, value in kwargs.items():
   198|             func_state[name] = value
   199|         if func_def.args.vararg:
   200|             vararg_name = func_def.args.vararg.arg
   201|             func_state[vararg_name] = args
   202|         if func_def.args.kwarg:
   203|             kwarg_name = func_def.args.kwarg.arg
   204|             func_state[kwarg_name] = kwargs
   205|         for name, value in defaults.items():
   206|             if name not in func_state:
   207|                 func_state[name] = value
   208|         if func_def.args.args and func_def.args.args[0].arg == "self":
   209|             if args:
   210|                 func_state["self"] = args[0]
   211|                 func_state["__class__"] = args[0].__class__
   212|         result = None
   213|         try:
   214|             for stmt in func_def.body:
   215|                 result = evaluate_ast(
   216|                     stmt, func_state, static_tools, custom_tools, authorized_imports
   217|                 )
   218|         except ReturnException as e:
   219|             result = e.value
   220|         if func_def.name == "__init__":
   221|             return None
   222|         return result
   223|     return new_func
   224| def evaluate_function_def(
   225|     func_def: ast.FunctionDef,
   226|     state: Dict[str, Any],
   227|     static_tools: Dict[str, Callable],
   228|     custom_tools: Dict[str, Callable],
   229|     authorized_imports: List[str],
   230| ) -> Callable:
   231|     custom_tools[func_def.name] = create_function(
   232|         func_def, state, static_tools, custom_tools, authorized_imports
   233|     )
   234|     return custom_tools[func_def.name]
   235| def evaluate_class_def(
   236|     class_def: ast.ClassDef,
   237|     state: Dict[str, Any],
   238|     static_tools: Dict[str, Callable],
   239|     custom_tools: Dict[str, Callable],
   240|     authorized_imports: List[str],
   241| ) -> type:
   242|     class_name = class_def.name
   243|     bases = [
   244|         evaluate_ast(base, state, static_tools, custom_tools, authorized_imports)
   245|         for base in class_def.bases
   246|     ]
   247|     class_dict = {}
   248|     for stmt in class_def.body:
   249|         if isinstance(stmt, ast.FunctionDef):
   250|             class_dict[stmt.name] = evaluate_function_def(
   251|                 stmt, state, static_tools, custom_tools, authorized_imports
   252|             )
   253|         elif isinstance(stmt, ast.Assign):
   254|             for target in stmt.targets:
   255|                 if isinstance(target, ast.Name):
   256|                     class_dict[target.id] = evaluate_ast(
   257|                         stmt.value,
   258|                         state,
   259|                         static_tools,
   260|                         custom_tools,
   261|                         authorized_imports,
   262|                     )
   263|                 elif isinstance(target, ast.Attribute):
   264|                     class_dict[target.attr] = evaluate_ast(
   265|                         stmt.value,
   266|                         state,
   267|                         static_tools,
   268|                         custom_tools,
   269|                         authorized_imports,
   270|                     )
   271|         else:
   272|             raise InterpreterError(
   273|                 f"Unsupported statement in class body: {stmt.__class__.__name__}"
   274|             )
   275|     new_class = type(class_name, tuple(bases), class_dict)
   276|     state[class_name] = new_class
   277|     return new_class
   278| def evaluate_augassign(
   279|     expression: ast.AugAssign,
   280|     state: Dict[str, Any],
   281|     static_tools: Dict[str, Callable],
   282|     custom_tools: Dict[str, Callable],
   283|     authorized_imports: List[str],
   284| ) -> Any:
   285|     def get_current_value(target: ast.AST) -> Any:
   286|         if isinstance(target, ast.Name):
   287|             return state.get(target.id, 0)
   288|         elif isinstance(target, ast.Subscript):
   289|             obj = evaluate_ast(
   290|                 target.value, state, static_tools, custom_tools, authorized_imports
   291|             )
   292|             key = evaluate_ast(
   293|                 target.slice, state, static_tools, custom_tools, authorized_imports
   294|             )
   295|             return obj[key]
   296|         elif isinstance(target, ast.Attribute):
   297|             obj = evaluate_ast(
   298|                 target.value, state, static_tools, custom_tools, authorized_imports
   299|             )
   300|             return getattr(obj, target.attr)
   301|         elif isinstance(target, ast.Tuple):
   302|             return tuple(get_current_value(elt) for elt in target.elts)
   303|         elif isinstance(target, ast.List):
   304|             return [get_current_value(elt) for elt in target.elts]
   305|         else:
   306|             raise InterpreterError(
   307|                 "AugAssign not supported for {type(target)} targets."
   308|             )
   309|     current_value = get_current_value(expression.target)
   310|     value_to_add = evaluate_ast(
   311|         expression.value, state, static_tools, custom_tools, authorized_imports
   312|     )
   313|     if isinstance(expression.op, ast.Add):
   314|         if isinstance(current_value, list):
   315|             if not isinstance(value_to_add, list):
   316|                 raise InterpreterError(
   317|                     f"Cannot add non-list value {value_to_add} to a list."
   318|                 )
   319|             updated_value = current_value + value_to_add
   320|         else:
   321|             updated_value = current_value + value_to_add
   322|     elif isinstance(expression.op, ast.Sub):
   323|         updated_value = current_value - value_to_add
   324|     elif isinstance(expression.op, ast.Mult):
   325|         updated_value = current_value * value_to_add
   326|     elif isinstance(expression.op, ast.Div):
   327|         updated_value = current_value / value_to_add
   328|     elif isinstance(expression.op, ast.Mod):
   329|         updated_value = current_value % value_to_add
   330|     elif isinstance(expression.op, ast.Pow):
   331|         updated_value = current_value**value_to_add
   332|     elif isinstance(expression.op, ast.FloorDiv):
   333|         updated_value = current_value // value_to_add
   334|     elif isinstance(expression.op, ast.BitAnd):
   335|         updated_value = current_value & value_to_add
   336|     elif isinstance(expression.op, ast.BitOr):
   337|         updated_value = current_value | value_to_add
   338|     elif isinstance(expression.op, ast.BitXor):
   339|         updated_value = current_value ^ value_to_add
   340|     elif isinstance(expression.op, ast.LShift):
   341|         updated_value = current_value << value_to_add
   342|     elif isinstance(expression.op, ast.RShift):
   343|         updated_value = current_value >> value_to_add
   344|     else:
   345|         raise InterpreterError(
   346|             f"Operation {type(expression.op).__name__} is not supported."
   347|         )
   348|     set_value(
   349|         expression.target,
   350|         updated_value,
   351|         state,
   352|         static_tools,
   353|         custom_tools,
   354|         authorized_imports,
   355|     )
   356|     return updated_value
   357| def evaluate_boolop(
   358|     node: ast.BoolOp,
   359|     state: Dict[str, Any],
   360|     static_tools: Dict[str, Callable],
   361|     custom_tools: Dict[str, Callable],
   362|     authorized_imports: List[str],
   363| ) -> bool:
   364|     if isinstance(node.op, ast.And):
   365|         for value in node.values:
   366|             if not evaluate_ast(
   367|                 value, state, static_tools, custom_tools, authorized_imports
   368|             ):
   369|                 return False
   370|         return True
   371|     elif isinstance(node.op, ast.Or):
   372|         for value in node.values:
   373|             if evaluate_ast(
   374|                 value, state, static_tools, custom_tools, authorized_imports
   375|             ):
   376|                 return True
   377|         return False
   378| def evaluate_binop(
   379|     binop: ast.BinOp,
   380|     state: Dict[str, Any],
   381|     static_tools: Dict[str, Callable],
   382|     custom_tools: Dict[str, Callable],
   383|     authorized_imports: List[str],
   384| ) -> Any:
   385|     left_val = evaluate_ast(
   386|         binop.left, state, static_tools, custom_tools, authorized_imports
   387|     )
   388|     right_val = evaluate_ast(
   389|         binop.right, state, static_tools, custom_tools, authorized_imports
   390|     )
   391|     if isinstance(binop.op, ast.Add):
   392|         return left_val + right_val
   393|     elif isinstance(binop.op, ast.Sub):
   394|         return left_val - right_val
   395|     elif isinstance(binop.op, ast.Mult):
   396|         return left_val * right_val
   397|     elif isinstance(binop.op, ast.Div):
   398|         return left_val / right_val
   399|     elif isinstance(binop.op, ast.Mod):
   400|         return left_val % right_val
   401|     elif isinstance(binop.op, ast.Pow):
   402|         return left_val**right_val
   403|     elif isinstance(binop.op, ast.FloorDiv):
   404|         return left_val // right_val
   405|     elif isinstance(binop.op, ast.BitAnd):
   406|         return left_val & right_val
   407|     elif isinstance(binop.op, ast.BitOr):
   408|         return left_val | right_val
   409|     elif isinstance(binop.op, ast.BitXor):
   410|         return left_val ^ right_val
   411|     elif isinstance(binop.op, ast.LShift):
   412|         return left_val << right_val
   413|     elif isinstance(binop.op, ast.RShift):
   414|         return left_val >> right_val
   415|     else:
   416|         raise NotImplementedError(
   417|             f"Binary operation {type(binop.op).__name__} is not implemented."
   418|         )
   419| def evaluate_assign(
   420|     assign: ast.Assign,
   421|     state: Dict[str, Any],
   422|     static_tools: Dict[str, Callable],
   423|     custom_tools: Dict[str, Callable],
   424|     authorized_imports: List[str],
   425| ) -> Any:
   426|     result = evaluate_ast(
   427|         assign.value, state, static_tools, custom_tools, authorized_imports
   428|     )
   429|     if len(assign.targets) == 1:
   430|         target = assign.targets[0]
   431|         set_value(target, result, state, static_tools, custom_tools, authorized_imports)
   432|     else:
   433|         if len(assign.targets) != len(result):
   434|             raise InterpreterError(
   435|                 f"Assign failed: expected {len(result)} values but got {len(assign.targets)}."
   436|             )
   437|         expanded_values = []
   438|         for tgt in assign.targets:
   439|             if isinstance(tgt, ast.Starred):
   440|                 expanded_values.extend(result)
   441|             else:
   442|                 expanded_values.append(result)
   443|         for tgt, val in zip(assign.targets, expanded_values):
   444|             set_value(tgt, val, state, static_tools, custom_tools, authorized_imports)
   445|     return result
   446| def set_value(
   447|     target: ast.AST,
   448|     value: Any,
   449|     state: Dict[str, Any],
   450|     static_tools: Dict[str, Callable],
   451|     custom_tools: Dict[str, Callable],
   452|     authorized_imports: List[str],
   453| ) -> None:
   454|     if isinstance(target, ast.Name):
   455|         if target.id in static_tools:
   456|             raise InterpreterError(
   457|                 f"Cannot assign to name '{target.id}': doing this would erase the existing tool!"
   458|             )
   459|         state[target.id] = value
   460|     elif isinstance(target, ast.Tuple):
   461|         if not isinstance(value, tuple):
   462|             if hasattr(value, "__iter__") and not isinstance(value, (str, bytes)):
   463|                 value = tuple(value)
   464|             else:
   465|                 raise InterpreterError("Cannot unpack non-tuple value")
   466|         if len(target.elts) != len(value):
   467|             raise InterpreterError("Cannot unpack tuple of wrong size")
   468|         for i, elem in enumerate(target.elts):
   469|             set_value(
   470|                 elem, value[i], state, static_tools, custom_tools, authorized_imports
   471|             )
   472|     elif isinstance(target, ast.Subscript):
   473|         obj = evaluate_ast(
   474|             target.value, state, static_tools, custom_tools, authorized_imports
   475|         )
   476|         key = evaluate_ast(
   477|             target.slice, state, static_tools, custom_tools, authorized_imports
   478|         )
   479|         obj[key] = value
   480|     elif isinstance(target, ast.Attribute):
   481|         obj = evaluate_ast(
   482|             target.value, state, static_tools, custom_tools, authorized_imports
   483|         )
   484|         setattr(obj, target.attr, value)
   485| def evaluate_call(
   486|     call: ast.Call,
   487|     state: Dict[str, Any],
   488|     static_tools: Dict[str, Callable],
   489|     custom_tools: Dict[str, Callable],
   490|     authorized_imports: List[str],
   491| ) -> Any:
   492|     if not (
   493|         isinstance(call.func, ast.Attribute)
   494|         or isinstance(call.func, ast.Name)
   495|         or isinstance(call.func, ast.Subscript)
   496|     ):
   497|         raise InterpreterError(f"This is not a correct function: {call.func}).")
   498|     if isinstance(call.func, ast.Attribute):
   499|         obj = evaluate_ast(
   500|             call.func.value, state, static_tools, custom_tools, authorized_imports
   501|         )
   502|         func_name = call.func.attr
   503|         if not hasattr(obj, func_name):
   504|             raise InterpreterError(f"Object {obj} has no attribute {func_name}")
   505|         func = getattr(obj, func_name)
   506|     elif isinstance(call.func, ast.Name):
   507|         func_name = call.func.id
   508|         if func_name in state:
   509|             func = state[func_name]
   510|         elif func_name in static_tools:
   511|             func = static_tools[func_name]
   512|         elif func_name in custom_tools:
   513|             func = custom_tools[func_name]
   514|         elif func_name in ERRORS:
   515|             func = ERRORS[func_name]
   516|         else:
   517|             raise InterpreterError(
   518|                 f"It is not permitted to evaluate other functions than the provided tools or functions defined in previous code (tried to execute {call.func.id})."
   519|             )
   520|     elif isinstance(call.func, ast.Subscript):
   521|         value = evaluate_ast(
   522|             call.func.value, state, static_tools, custom_tools, authorized_imports
   523|         )
   524|         index = evaluate_ast(
   525|             call.func.slice, state, static_tools, custom_tools, authorized_imports
   526|         )
   527|         if isinstance(value, (list, tuple)):
   528|             func = value[index]
   529|         else:
   530|             raise InterpreterError(
   531|                 f"Cannot subscript object of type {type(value).__name__}"
   532|             )
   533|         if not callable(func):
   534|             raise InterpreterError(f"This is not a correct function: {call.func}).")
   535|         func_name = None
   536|     args = []
   537|     for arg in call.args:
   538|         if isinstance(arg, ast.Starred):
   539|             args.extend(
   540|                 evaluate_ast(
   541|                     arg.value, state, static_tools, custom_tools, authorized_imports
   542|                 )
   543|             )
   544|         else:
   545|             args.append(
   546|                 evaluate_ast(arg, state, static_tools, custom_tools, authorized_imports)
   547|             )
   548|     kwargs = {
   549|         keyword.arg: evaluate_ast(
   550|             keyword.value, state, static_tools, custom_tools, authorized_imports
   551|         )
   552|         for keyword in call.keywords
   553|     }
   554|     if func_name == "super":
   555|         if not args:
   556|             if "__class__" in state and "self" in state:
   557|                 return super(state["__class__"], state["self"])
   558|             else:
   559|                 raise InterpreterError("super() needs at least one argument")
   560|         cls = args[0]
   561|         if not isinstance(cls, type):
   562|             raise InterpreterError("super() argument 1 must be type")
   563|         if len(args) == 1:
   564|             return super(cls)
   565|         elif len(args) == 2:
   566|             instance = args[1]
   567|             return super(cls, instance)
   568|         else:
   569|             raise InterpreterError("super() takes at most 2 arguments")
   570|     else:
   571|         if func_name == "print":
   572|             output = " ".join(map(str, args))
   573|             global PRINT_OUTPUTS
   574|             PRINT_OUTPUTS += output + "\n"
   575|             return None
   576|         else:  # Assume it's a callable object
   577|             return func(*args, **kwargs)
   578| def evaluate_subscript(
   579|     subscript: ast.Subscript,
   580|     state: Dict[str, Any],
   581|     static_tools: Dict[str, Callable],
   582|     custom_tools: Dict[str, Callable],
   583|     authorized_imports: List[str],
   584| ) -> Any:
   585|     index = evaluate_ast(
   586|         subscript.slice, state, static_tools, custom_tools, authorized_imports
   587|     )
   588|     value = evaluate_ast(
   589|         subscript.value, state, static_tools, custom_tools, authorized_imports
   590|     )
   591|     if isinstance(value, str) and isinstance(index, str):
   592|         raise InterpreterError(
   593|             "You're trying to subscript a string with a string index, which is impossible"
   594|         )
   595|     if isinstance(value, pd.core.indexing._LocIndexer):
   596|         parent_object = value.obj
   597|         return parent_object.loc[index]
   598|     if isinstance(value, pd.core.indexing._iLocIndexer):
   599|         parent_object = value.obj
   600|         return parent_object.iloc[index]
   601|     if isinstance(value, (pd.DataFrame, pd.Series, np.ndarray)):
   602|         return value[index]
   603|     elif isinstance(value, pd.core.groupby.generic.DataFrameGroupBy):
   604|         return value[index]
   605|     elif isinstance(index, slice):
   606|         return value[index]
   607|     elif isinstance(value, (list, tuple)):
   608|         if not (-len(value) <= index < len(value)):
   609|             raise InterpreterError(
   610|                 f"Index {index} out of bounds for list of length {len(value)}"
   611|             )
   612|         return value[int(index)]
   613|     elif isinstance(value, str):
   614|         if not (-len(value) <= index < len(value)):
   615|             raise InterpreterError(
   616|                 f"Index {index} out of bounds for string of length {len(value)}"
   617|             )
   618|         return value[index]
   619|     elif index in value:
   620|         return value[index]
   621|     elif isinstance(index, str) and isinstance(value, Mapping):
   622|         close_matches = difflib.get_close_matches(index, list(value.keys()))
   623|         if len(close_matches) > 0:
   624|             return value[close_matches[0]]
   625|     raise InterpreterError(f"Could not index {value} with '{index}'.")
   626| def evaluate_name(
   627|     name: ast.Name,
   628|     state: Dict[str, Any],
   629|     static_tools: Dict[str, Callable],
   630|     custom_tools: Dict[str, Callable],
   631|     authorized_imports: List[str],
   632| ) -> Any:
   633|     if name.id in state:
   634|         return state[name.id]
   635|     elif name.id in static_tools:
   636|         return static_tools[name.id]
   637|     elif name.id in custom_tools:
   638|         return custom_tools[name.id]
   639|     elif name.id in ERRORS:
   640|         return ERRORS[name.id]
   641|     close_matches = difflib.get_close_matches(name.id, list(state.keys()))
   642|     if len(close_matches) > 0:
   643|         return state[close_matches[0]]
   644|     raise InterpreterError(f"The variable `{name.id}` is not defined.")
   645| def evaluate_condition(
   646|     condition: ast.Compare,
   647|     state: Dict[str, Any],
   648|     static_tools: Dict[str, Callable],
   649|     custom_tools: Dict[str, Callable],
   650|     authorized_imports: List[str],
   651| ) -> bool:
   652|     left = evaluate_ast(
   653|         condition.left, state, static_tools, custom_tools, authorized_imports
   654|     )
   655|     comparators = [
   656|         evaluate_ast(c, state, static_tools, custom_tools, authorized_imports)
   657|         for c in condition.comparators
   658|     ]
   659|     ops = [type(op) for op in condition.ops]
   660|     result = True
   661|     current_left = left
   662|     for op, comparator in zip(ops, comparators):
   663|         if op == ast.Eq:
   664|             current_result = current_left == comparator
   665|         elif op == ast.NotEq:
   666|             current_result = current_left != comparator
   667|         elif op == ast.Lt:
   668|             current_result = current_left < comparator
   669|         elif op == ast.LtE:
   670|             current_result = current_left <= comparator
   671|         elif op == ast.Gt:
   672|             current_result = current_left > comparator
   673|         elif op == ast.GtE:
   674|             current_result = current_left >= comparator
   675|         elif op == ast.Is:
   676|             current_result = current_left is comparator
   677|         elif op == ast.IsNot:
   678|             current_result = current_left is not comparator
   679|         elif op == ast.In:
   680|             current_result = current_left in comparator
   681|         elif op == ast.NotIn:
   682|             current_result = current_left not in comparator
   683|         else:
   684|             raise InterpreterError(f"Operator not supported: {op}")
   685|         result = result & current_result
   686|         current_left = comparator
   687|         if isinstance(result, bool) and not result:
   688|             break
   689|     return result if isinstance(result, (bool, pd.Series)) else result.all()
   690| def evaluate_if(
   691|     if_statement: ast.If,
   692|     state: Dict[str, Any],
   693|     static_tools: Dict[str, Callable],
   694|     custom_tools: Dict[str, Callable],
   695|     authorized_imports: List[str],
   696| ) -> Any:
   697|     result = None
   698|     test_result = evaluate_ast(
   699|         if_statement.test, state, static_tools, custom_tools, authorized_imports
   700|     )
   701|     if test_result:
   702|         for line in if_statement.body:
   703|             line_result = evaluate_ast(
   704|                 line, state, static_tools, custom_tools, authorized_imports
   705|             )
   706|             if line_result is not None:
   707|                 result = line_result
   708|     else:
   709|         for line in if_statement.orelse:
   710|             line_result = evaluate_ast(
   711|                 line, state, static_tools, custom_tools, authorized_imports
   712|             )
   713|             if line_result is not None:
   714|                 result = line_result
   715|     return result
   716| def evaluate_for(
   717|     for_loop: ast.For,
   718|     state: Dict[str, Any],
   719|     static_tools: Dict[str, Callable],
   720|     custom_tools: Dict[str, Callable],
   721|     authorized_imports: List[str],
   722| ) -> Any:
   723|     result = None
   724|     iterator = evaluate_ast(
   725|         for_loop.iter, state, static_tools, custom_tools, authorized_imports
   726|     )
   727|     for counter in iterator:
   728|         set_value(
   729|             for_loop.target,
   730|             counter,
   731|             state,
   732|             static_tools,
   733|             custom_tools,
   734|             authorized_imports,
   735|         )
   736|         for node in for_loop.body:
   737|             try:
   738|                 line_result = evaluate_ast(
   739|                     node, state, static_tools, custom_tools, authorized_imports
   740|                 )
   741|                 if line_result is not None:
   742|                     result = line_result
   743|             except BreakException:
   744|                 break
   745|             except ContinueException:
   746|                 continue
   747|         else:
   748|             continue
   749|         break
   750|     return result
   751| def evaluate_listcomp(
   752|     listcomp: ast.ListComp,
   753|     state: Dict[str, Any],
   754|     static_tools: Dict[str, Callable],
   755|     custom_tools: Dict[str, Callable],
   756|     authorized_imports: List[str],
   757| ) -> List[Any]:
   758|     def inner_evaluate(
   759|         generators: List[ast.comprehension], index: int, current_state: Dict[str, Any]
   760|     ) -> List[Any]:
   761|         if index >= len(generators):
   762|             return [
   763|                 evaluate_ast(
   764|                     listcomp.elt,
   765|                     current_state,
   766|                     static_tools,
   767|                     custom_tools,
   768|                     authorized_imports,
   769|                 )
   770|             ]
   771|         generator = generators[index]
   772|         iter_value = evaluate_ast(
   773|             generator.iter,
   774|             current_state,
   775|             static_tools,
   776|             custom_tools,
   777|             authorized_imports,
   778|         )
   779|         result = []
   780|         for value in iter_value:
   781|             new_state = current_state.copy()
   782|             if isinstance(generator.target, ast.Tuple):
   783|                 for idx, elem in enumerate(generator.target.elts):
   784|                     new_state[elem.id] = value[idx]
   785|             else:
   786|                 new_state[generator.target.id] = value
   787|             if all(
   788|                 evaluate_ast(
   789|                     if_clause, new_state, static_tools, custom_tools, authorized_imports
   790|                 )
   791|                 for if_clause in generator.ifs
   792|             ):
   793|                 result.extend(inner_evaluate(generators, index + 1, new_state))
   794|         return result
   795|     return inner_evaluate(listcomp.generators, 0, state)
   796| def evaluate_try(
   797|     try_node: ast.Try,
   798|     state: Dict[str, Any],
   799|     static_tools: Dict[str, Callable],
   800|     custom_tools: Dict[str, Callable],
   801|     authorized_imports: List[str],
   802| ) -> None:
   803|     try:
   804|         for stmt in try_node.body:
   805|             evaluate_ast(stmt, state, static_tools, custom_tools, authorized_imports)
   806|     except Exception as e:
   807|         matched = False
   808|         for handler in try_node.handlers:
   809|             if handler.type is None or isinstance(
   810|                 e,
   811|                 evaluate_ast(
   812|                     handler.type, state, static_tools, custom_tools, authorized_imports
   813|                 ),
   814|             ):
   815|                 matched = True
   816|                 if handler.name:
   817|                     state[handler.name] = e
   818|                 for stmt in handler.body:
   819|                     evaluate_ast(
   820|                         stmt, state, static_tools, custom_tools, authorized_imports
   821|                     )
   822|                 break
   823|         if not matched:
   824|             raise e
   825|     else:
   826|         if try_node.orelse:
   827|             for stmt in try_node.orelse:
   828|                 evaluate_ast(
   829|                     stmt, state, static_tools, custom_tools, authorized_imports
   830|                 )
   831|     finally:
   832|         if try_node.finalbody:
   833|             for stmt in try_node.finalbody:
   834|                 evaluate_ast(
   835|                     stmt, state, static_tools, custom_tools, authorized_imports
   836|                 )
   837| def evaluate_raise(
   838|     raise_node: ast.Raise,
   839|     state: Dict[str, Any],
   840|     static_tools: Dict[str, Callable],
   841|     custom_tools: Dict[str, Callable],
   842|     authorized_imports: List[str],
   843| ) -> None:
   844|     if raise_node.exc is not None:
   845|         exc = evaluate_ast(
   846|             raise_node.exc, state, static_tools, custom_tools, authorized_imports
   847|         )
   848|     else:
   849|         exc = None
   850|     if raise_node.cause is not None:
   851|         cause = evaluate_ast(
   852|             raise_node.cause, state, static_tools, custom_tools, authorized_imports
   853|         )
   854|     else:
   855|         cause = None
   856|     if exc is not None:
   857|         if cause is not None:
   858|             raise exc from cause
   859|         else:
   860|             raise exc
   861|     else:
   862|         raise InterpreterError("Re-raise is not supported without an active exception")
   863| def evaluate_assert(
   864|     assert_node: ast.Assert,
   865|     state: Dict[str, Any],
   866|     static_tools: Dict[str, Callable],
   867|     custom_tools: Dict[str, Callable],
   868|     authorized_imports: List[str],
   869| ) -> None:
   870|     test_result = evaluate_ast(
   871|         assert_node.test, state, static_tools, custom_tools, authorized_imports
   872|     )
   873|     if not test_result:
   874|         if assert_node.msg:
   875|             msg = evaluate_ast(
   876|                 assert_node.msg, state, static_tools, custom_tools, authorized_imports
   877|             )
   878|             raise AssertionError(msg)
   879|         else:
   880|             test_code = ast.unparse(assert_node.test)
   881|             raise AssertionError(f"Assertion failed: {test_code}")
   882| def evaluate_with(
   883|     with_node: ast.With,
   884|     state: Dict[str, Any],
   885|     static_tools: Dict[str, Callable],
   886|     custom_tools: Dict[str, Callable],
   887|     authorized_imports: List[str],
   888| ) -> None:
   889|     contexts = []
   890|     for item in with_node.items:
   891|         context_expr = evaluate_ast(
   892|             item.context_expr, state, static_tools, custom_tools, authorized_imports
   893|         )
   894|         if item.optional_vars:
   895|             state[item.optional_vars.id] = context_expr.__enter__()
   896|             contexts.append(state[item.optional_vars.id])
   897|         else:
   898|             context_var = context_expr.__enter__()
   899|             contexts.append(context_var)
   900|     try:
   901|         for stmt in with_node.body:
   902|             evaluate_ast(stmt, state, static_tools, custom_tools, authorized_imports)
   903|     except Exception as e:
   904|         for context in reversed(contexts):
   905|             context.__exit__(type(e), e, e.__traceback__)
   906|         raise
   907|     else:
   908|         for context in reversed(contexts):
   909|             context.__exit__(None, None, None)
   910| def get_safe_module(unsafe_module, dangerous_patterns, visited=None):
   911|     """Creates a safe copy of a module or returns the original if it's a function"""
   912|     if not isinstance(unsafe_module, ModuleType):
   913|         return unsafe_module
   914|     if visited is None:
   915|         visited = set()
   916|     module_id = id(unsafe_module)
   917|     if module_id in visited:
   918|         return unsafe_module  # Return original for circular refs
   919|     visited.add(module_id)
   920|     safe_module = ModuleType(unsafe_module.__name__)
   921|     for attr_name in dir(unsafe_module):
   922|         if any(
   923|             pattern in f"{unsafe_module.__name__}.{attr_name}"
   924|             for pattern in dangerous_patterns
   925|         ):
   926|             continue
   927|         attr_value = getattr(unsafe_module, attr_name)
   928|         if isinstance(attr_value, ModuleType):
   929|             attr_value = get_safe_module(
   930|                 attr_value, dangerous_patterns, visited=visited
   931|             )
   932|         setattr(safe_module, attr_name, attr_value)
   933|     return safe_module
   934| def import_modules(expression, state, authorized_imports):
   935|     dangerous_patterns = (
   936|         "_os",
   937|         "os",
   938|         "subprocess",
   939|         "_subprocess",
   940|         "pty",
   941|         "system",
   942|         "popen",
   943|         "spawn",
   944|         "shutil",
   945|         "sys",
   946|         "pathlib",
   947|         "io",
   948|         "socket",
   949|         "compile",
   950|         "eval",
   951|         "exec",
   952|         "multiprocessing",
   953|     )
   954|     def check_module_authorized(module_name):
   955|         if "*" in authorized_imports:
   956|             return True
   957|         else:
   958|             module_path = module_name.split(".")
   959|             if any([module in dangerous_patterns for module in module_path]):
   960|                 return False
   961|             module_subpaths = [
   962|                 ".".join(module_path[:i]) for i in range(1, len(module_path) + 1)
   963|             ]
   964|             return any(subpath in authorized_imports for subpath in module_subpaths)
   965|     if isinstance(expression, ast.Import):
   966|         for alias in expression.names:
   967|             if check_module_authorized(alias.name):
   968|                 raw_module = import_module(alias.name)
   969|                 state[alias.asname or alias.name] = get_safe_module(
   970|                     raw_module, dangerous_patterns
   971|                 )
   972|             else:
   973|                 raise InterpreterError(
   974|                     f"Import of {alias.name} is not allowed. Authorized imports are: {str(authorized_imports)}"
   975|                 )
   976|         return None
   977|     elif isinstance(expression, ast.ImportFrom):
   978|         if check_module_authorized(expression.module):
   979|             raw_module = __import__(
   980|                 expression.module, fromlist=[alias.name for alias in expression.names]
   981|             )
   982|             for alias in expression.names:
   983|                 state[alias.asname or alias.name] = get_safe_module(
   984|                     getattr(raw_module, alias.name), dangerous_patterns
   985|                 )
   986|         else:
   987|             raise InterpreterError(f"Import from {expression.module} is not allowed.")
   988|         return None
   989| def evaluate_dictcomp(
   990|     dictcomp: ast.DictComp,
   991|     state: Dict[str, Any],
   992|     static_tools: Dict[str, Callable],
   993|     custom_tools: Dict[str, Callable],
   994|     authorized_imports: List[str],
   995| ) -> Dict[Any, Any]:
   996|     result = {}
   997|     for gen in dictcomp.generators:
   998|         iter_value = evaluate_ast(
   999|             gen.iter, state, static_tools, custom_tools, authorized_imports
  1000|         )
  1001|         for value in iter_value:
  1002|             new_state = state.copy()
  1003|             set_value(
  1004|                 gen.target,
  1005|                 value,
  1006|                 new_state,
  1007|                 static_tools,
  1008|                 custom_tools,
  1009|                 authorized_imports,
  1010|             )
  1011|             if all(
  1012|                 evaluate_ast(
  1013|                     if_clause, new_state, static_tools, custom_tools, authorized_imports
  1014|                 )
  1015|                 for if_clause in gen.ifs
  1016|             ):
  1017|                 key = evaluate_ast(
  1018|                     dictcomp.key,
  1019|                     new_state,
  1020|                     static_tools,
  1021|                     custom_tools,
  1022|                     authorized_imports,
  1023|                 )
  1024|                 val = evaluate_ast(
  1025|                     dictcomp.value,
  1026|                     new_state,
  1027|                     static_tools,
  1028|                     custom_tools,
  1029|                     authorized_imports,
  1030|                 )
  1031|                 result[key] = val
  1032|     return result
  1033| def evaluate_ast(
  1034|     expression: ast.AST,
  1035|     state: Dict[str, Any],
  1036|     static_tools: Dict[str, Callable],
  1037|     custom_tools: Dict[str, Callable],
  1038|     authorized_imports: List[str] = BASE_BUILTIN_MODULES,
  1039| ):
  1040|     """
  1041|     Evaluate an abstract syntax tree using the content of the variables stored in a state and only evaluating a given
  1042|     set of functions.
  1043|     This function will recurse trough the nodes of the tree provided.
  1044|     Args:
  1045|         expression (`ast.AST`):
  1046|             The code to evaluate, as an abstract syntax tree.
  1047|         state (`Dict[str, Any]`):
  1048|             A dictionary mapping variable names to values. The `state` is updated if need be when the evaluation
  1049|             encounters assignments.
  1050|         static_tools (`Dict[str, Callable]`):
  1051|             Functions that may be called during the evaluation. Trying to change one of these static_tools will raise an error.
  1052|         custom_tools (`Dict[str, Callable]`):
  1053|             Functions that may be called during the evaluation. These static_tools can be overwritten.
  1054|         authorized_imports (`List[str]`):
  1055|             The list of modules that can be imported by the code. By default, only a few safe modules are allowed.
  1056|             If it contains "*", it will authorize any import. Use this at your own risk!
  1057|     """
  1058|     global OPERATIONS_COUNT
  1059|     if OPERATIONS_COUNT >= MAX_OPERATIONS:
  1060|         raise InterpreterError(
  1061|             f"Reached the max number of operations of {MAX_OPERATIONS}. Maybe there is an infinite loop somewhere in the code, or you're just asking too many calculations."
  1062|         )
  1063|     OPERATIONS_COUNT += 1
  1064|     if isinstance(expression, ast.Assign):
  1065|         return evaluate_assign(
  1066|             expression, state, static_tools, custom_tools, authorized_imports
  1067|         )
  1068|     elif isinstance(expression, ast.AugAssign):
  1069|         return evaluate_augassign(
  1070|             expression, state, static_tools, custom_tools, authorized_imports
  1071|         )
  1072|     elif isinstance(expression, ast.Call):
  1073|         return evaluate_call(
  1074|             expression, state, static_tools, custom_tools, authorized_imports
  1075|         )
  1076|     elif isinstance(expression, ast.Constant):
  1077|         return expression.value
  1078|     elif isinstance(expression, ast.Tuple):
  1079|         return tuple(
  1080|             evaluate_ast(elt, state, static_tools, custom_tools, authorized_imports)
  1081|             for elt in expression.elts
  1082|         )
  1083|     elif isinstance(expression, (ast.ListComp, ast.GeneratorExp)):
  1084|         return evaluate_listcomp(
  1085|             expression, state, static_tools, custom_tools, authorized_imports
  1086|         )
  1087|     elif isinstance(expression, ast.UnaryOp):
  1088|         return evaluate_unaryop(
  1089|             expression, state, static_tools, custom_tools, authorized_imports
  1090|         )
  1091|     elif isinstance(expression, ast.Starred):
  1092|         return evaluate_ast(
  1093|             expression.value, state, static_tools, custom_tools, authorized_imports
  1094|         )
  1095|     elif isinstance(expression, ast.BoolOp):
  1096|         return evaluate_boolop(
  1097|             expression, state, static_tools, custom_tools, authorized_imports
  1098|         )
  1099|     elif isinstance(expression, ast.Break):
  1100|         raise BreakException()
  1101|     elif isinstance(expression, ast.Continue):
  1102|         raise ContinueException()
  1103|     elif isinstance(expression, ast.BinOp):
  1104|         return evaluate_binop(
  1105|             expression, state, static_tools, custom_tools, authorized_imports
  1106|         )
  1107|     elif isinstance(expression, ast.Compare):
  1108|         return evaluate_condition(
  1109|             expression, state, static_tools, custom_tools, authorized_imports
  1110|         )
  1111|     elif isinstance(expression, ast.Lambda):
  1112|         return evaluate_lambda(
  1113|             expression, state, static_tools, custom_tools, authorized_imports
  1114|         )
  1115|     elif isinstance(expression, ast.FunctionDef):
  1116|         return evaluate_function_def(
  1117|             expression, state, static_tools, custom_tools, authorized_imports
  1118|         )
  1119|     elif isinstance(expression, ast.Dict):
  1120|         keys = [
  1121|             evaluate_ast(k, state, static_tools, custom_tools, authorized_imports)
  1122|             for k in expression.keys
  1123|         ]
  1124|         values = [
  1125|             evaluate_ast(v, state, static_tools, custom_tools, authorized_imports)
  1126|             for v in expression.values
  1127|         ]
  1128|         return dict(zip(keys, values))
  1129|     elif isinstance(expression, ast.Expr):
  1130|         return evaluate_ast(
  1131|             expression.value, state, static_tools, custom_tools, authorized_imports
  1132|         )
  1133|     elif isinstance(expression, ast.For):
  1134|         return evaluate_for(
  1135|             expression, state, static_tools, custom_tools, authorized_imports
  1136|         )
  1137|     elif isinstance(expression, ast.FormattedValue):
  1138|         return evaluate_ast(
  1139|             expression.value, state, static_tools, custom_tools, authorized_imports
  1140|         )
  1141|     elif isinstance(expression, ast.If):
  1142|         return evaluate_if(
  1143|             expression, state, static_tools, custom_tools, authorized_imports
  1144|         )
  1145|     elif hasattr(ast, "Index") and isinstance(expression, ast.Index):
  1146|         return evaluate_ast(
  1147|             expression.value, state, static_tools, custom_tools, authorized_imports
  1148|         )
  1149|     elif isinstance(expression, ast.JoinedStr):
  1150|         return "".join(
  1151|             [
  1152|                 str(
  1153|                     evaluate_ast(
  1154|                         v, state, static_tools, custom_tools, authorized_imports
  1155|                     )
  1156|                 )
  1157|                 for v in expression.values
  1158|             ]
  1159|         )
  1160|     elif isinstance(expression, ast.List):
  1161|         return [
  1162|             evaluate_ast(elt, state, static_tools, custom_tools, authorized_imports)
  1163|             for elt in expression.elts
  1164|         ]
  1165|     elif isinstance(expression, ast.Name):
  1166|         return evaluate_name(
  1167|             expression, state, static_tools, custom_tools, authorized_imports
  1168|         )
  1169|     elif isinstance(expression, ast.Subscript):
  1170|         return evaluate_subscript(
  1171|             expression, state, static_tools, custom_tools, authorized_imports
  1172|         )
  1173|     elif isinstance(expression, ast.IfExp):
  1174|         test_val = evaluate_ast(
  1175|             expression.test, state, static_tools, custom_tools, authorized_imports
  1176|         )
  1177|         if test_val:
  1178|             return evaluate_ast(
  1179|                 expression.body, state, static_tools, custom_tools, authorized_imports
  1180|             )
  1181|         else:
  1182|             return evaluate_ast(
  1183|                 expression.orelse, state, static_tools, custom_tools, authorized_imports
  1184|             )
  1185|     elif isinstance(expression, ast.Attribute):
  1186|         value = evaluate_ast(
  1187|             expression.value, state, static_tools, custom_tools, authorized_imports
  1188|         )
  1189|         return getattr(value, expression.attr)
  1190|     elif isinstance(expression, ast.Slice):
  1191|         return slice(
  1192|             evaluate_ast(
  1193|                 expression.lower, state, static_tools, custom_tools, authorized_imports
  1194|             )
  1195|             if expression.lower is not None
  1196|             else None,
  1197|             evaluate_ast(
  1198|                 expression.upper, state, static_tools, custom_tools, authorized_imports
  1199|             )
  1200|             if expression.upper is not None
  1201|             else None,
  1202|             evaluate_ast(
  1203|                 expression.step, state, static_tools, custom_tools, authorized_imports
  1204|             )
  1205|             if expression.step is not None
  1206|             else None,
  1207|         )
  1208|     elif isinstance(expression, ast.DictComp):
  1209|         return evaluate_dictcomp(
  1210|             expression, state, static_tools, custom_tools, authorized_imports
  1211|         )
  1212|     elif isinstance(expression, ast.While):
  1213|         return evaluate_while(
  1214|             expression, state, static_tools, custom_tools, authorized_imports
  1215|         )
  1216|     elif isinstance(expression, (ast.Import, ast.ImportFrom)):
  1217|         return import_modules(expression, state, authorized_imports)
  1218|     elif isinstance(expression, ast.ClassDef):
  1219|         return evaluate_class_def(
  1220|             expression, state, static_tools, custom_tools, authorized_imports
  1221|         )
  1222|     elif isinstance(expression, ast.Try):
  1223|         return evaluate_try(
  1224|             expression, state, static_tools, custom_tools, authorized_imports
  1225|         )
  1226|     elif isinstance(expression, ast.Raise):
  1227|         return evaluate_raise(
  1228|             expression, state, static_tools, custom_tools, authorized_imports
  1229|         )
  1230|     elif isinstance(expression, ast.Assert):
  1231|         return evaluate_assert(
  1232|             expression, state, static_tools, custom_tools, authorized_imports
  1233|         )
  1234|     elif isinstance(expression, ast.With):
  1235|         return evaluate_with(
  1236|             expression, state, static_tools, custom_tools, authorized_imports
  1237|         )
  1238|     elif isinstance(expression, ast.Set):
  1239|         return {
  1240|             evaluate_ast(elt, state, static_tools, custom_tools, authorized_imports)
  1241|             for elt in expression.elts
  1242|         }
  1243|     elif isinstance(expression, ast.Return):
  1244|         raise ReturnException(
  1245|             evaluate_ast(
  1246|                 expression.value, state, static_tools, custom_tools, authorized_imports
  1247|             )
  1248|             if expression.value
  1249|             else None
  1250|         )
  1251|     elif isinstance(expression, ast.Pass):
  1252|         return None
  1253|     else:
  1254|         raise InterpreterError(f"{expression.__class__.__name__} is not supported.")
  1255| class FinalAnswerException(Exception):
  1256|     def __init__(self, value):
  1257|         self.value = value
  1258| def evaluate_python_code(
  1259|     code: str,
  1260|     static_tools: Optional[Dict[str, Callable]] = None,
  1261|     custom_tools: Optional[Dict[str, Callable]] = None,
  1262|     state: Optional[Dict[str, Any]] = None,
  1263|     authorized_imports: List[str] = BASE_BUILTIN_MODULES,
  1264|     max_print_outputs_length: int = DEFAULT_MAX_LEN_OUTPUT,
  1265| ):
  1266|     """
  1267|     Evaluate a python expression using the content of the variables stored in a state and only evaluating a given set
  1268|     of functions.
  1269|     This function will recurse through the nodes of the tree provided.
  1270|     Args:
  1271|         code (`str`):
  1272|             The code to evaluate.
  1273|         static_tools (`Dict[str, Callable]`):
  1274|             The functions that may be called during the evaluation. These can also be agents in a multiagent setting.
  1275|             These tools cannot be overwritten in the code: any assignment to their name will raise an error.
  1276|         custom_tools (`Dict[str, Callable]`):
  1277|             The functions that may be called during the evaluation.
  1278|             These tools can be overwritten in the code: any assignment to their name will overwrite them.
  1279|         state (`Dict[str, Any]`):
  1280|             A dictionary mapping variable names to values. The `state` should contain the initial inputs but will be
  1281|             updated by this function to contain all variables as they are evaluated.
  1282|             The print outputs will be stored in the state under the key 'print_outputs'.
  1283|     """
  1284|     expression = ast.parse(code)
  1285|     if state is None:
  1286|         state = {}
  1287|     if static_tools is None:
  1288|         static_tools = {}
  1289|     if custom_tools is None:
  1290|         custom_tools = {}
  1291|     result = None
  1292|     global PRINT_OUTPUTS
  1293|     PRINT_OUTPUTS = ""
  1294|     global OPERATIONS_COUNT
  1295|     OPERATIONS_COUNT = 0
  1296|     def final_answer(value):
  1297|         raise FinalAnswerException(value)
  1298|     static_tools["final_answer"] = final_answer
  1299|     try:
  1300|         for node in expression.body:
  1301|             result = evaluate_ast(
  1302|                 node, state, static_tools, custom_tools, authorized_imports
  1303|             )
  1304|         state["print_outputs"] = truncate_content(
  1305|             PRINT_OUTPUTS, max_length=max_print_outputs_length
  1306|         )
  1307|         is_final_answer = False
  1308|         return result, is_final_answer
  1309|     except FinalAnswerException as e:
  1310|         state["print_outputs"] = truncate_content(
  1311|             PRINT_OUTPUTS, max_length=max_print_outputs_length
  1312|         )
  1313|         is_final_answer = True
  1314|         return e.value, is_final_answer
  1315|     except InterpreterError as e:
  1316|         msg = truncate_content(PRINT_OUTPUTS, max_length=max_print_outputs_length)
  1317|         msg += f"Code execution failed at line '{ast.get_source_segment(code, node)}' because of the following error:\n{e}"
  1318|         raise InterpreterError(msg)
  1319| class LocalPythonInterpreter:
  1320|     def __init__(
  1321|         self,
  1322|         additional_authorized_imports: List[str],
  1323|         tools: Dict,
  1324|         max_print_outputs_length: Optional[int] = None,
  1325|     ):
  1326|         self.custom_tools = {}
  1327|         self.state = {}
  1328|         self.max_print_outputs_length = max_print_outputs_length
  1329|         if max_print_outputs_length is None:
  1330|             self.max_print_outputs_length = DEFAULT_MAX_LEN_OUTPUT
  1331|         self.additional_authorized_imports = additional_authorized_imports
  1332|         self.authorized_imports = list(
  1333|             set(BASE_BUILTIN_MODULES) | set(self.additional_authorized_imports)
  1334|         )
  1335|         self.static_tools = {
  1336|             **tools,
  1337|             **BASE_PYTHON_TOOLS.copy(),
  1338|         }
  1339|     def __call__(
  1340|         self, code_action: str, additional_variables: Dict
  1341|     ) -> Tuple[Any, str, bool]:
  1342|         self.state.update(additional_variables)
  1343|         output, is_final_answer = evaluate_python_code(
  1344|             code_action,
  1345|             static_tools=self.static_tools,
  1346|             custom_tools=self.custom_tools,
  1347|             state=self.state,
  1348|             authorized_imports=self.authorized_imports,
  1349|             max_print_outputs_length=self.max_print_outputs_length,
  1350|         )
  1351|         logs = self.state["print_outputs"]
  1352|         return output, logs, is_final_answer
  1353| __all__ = ["evaluate_python_code", "LocalPythonInterpreter"]


# ====================================================================
# FILE: src/smolagents/models.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| from dataclasses import dataclass, asdict
     2| import json
     3| import logging
     4| import os
     5| import random
     6| from copy import deepcopy
     7| from enum import Enum
     8| from typing import Dict, List, Optional, Union, Any
     9| from huggingface_hub import InferenceClient
    10| from transformers import (
    11|     AutoModelForCausalLM,
    12|     AutoTokenizer,
    13|     StoppingCriteria,
    14|     StoppingCriteriaList,
    15|     is_torch_available,
    16| )
    17| from .tools import Tool
    18| logger = logging.getLogger(__name__)
    19| DEFAULT_JSONAGENT_REGEX_GRAMMAR = {
    20|     "type": "regex",
    21|     "value": 'Thought: .+?\\nAction:\\n\\{\\n\\s{4}"action":\\s"[^"\\n]+",\\n\\s{4}"action_input":\\s"[^"\\n]+"\\n\\}\\n<end_code>',
    22| }
    23| DEFAULT_CODEAGENT_REGEX_GRAMMAR = {
    24|     "type": "regex",
    25|     "value": "Thought: .+?\\nCode:\\n```(?:py|python)?\\n(?:.|\\s)+?\\n```<end_code>",
    26| }
    27| def get_dict_from_nested_dataclasses(obj):
    28|     def convert(obj):
    29|         if hasattr(obj, "__dataclass_fields__"):
    30|             return {k: convert(v) for k, v in asdict(obj).items()}
    31|         return obj
    32|     return convert(obj)
    33| @dataclass
    34| class ChatMessageToolCallDefinition:
    35|     arguments: Any
    36|     name: str
    37|     description: Optional[str] = None

# --- HUNK 2: Lines 48-549 ---
    48|     id: str
    49|     type: str
    50|     @classmethod
    51|     def from_hf_api(cls, tool_call) -> "ChatMessageToolCall":
    52|         return cls(
    53|             function=ChatMessageToolCallDefinition.from_hf_api(tool_call.function),
    54|             id=tool_call.id,
    55|             type=tool_call.type,
    56|         )
    57| @dataclass
    58| class ChatMessage:
    59|     role: str
    60|     content: Optional[str] = None
    61|     tool_calls: Optional[List[ChatMessageToolCall]] = None
    62|     def model_dump_json(self):
    63|         return json.dumps(get_dict_from_nested_dataclasses(self))
    64|     @classmethod
    65|     def from_hf_api(cls, message) -> "ChatMessage":
    66|         tool_calls = None
    67|         if getattr(message, "tool_calls", None) is not None:
    68|             tool_calls = [
    69|                 ChatMessageToolCall.from_hf_api(tool_call)
    70|                 for tool_call in message.tool_calls
    71|             ]
    72|         return cls(role=message.role, content=message.content, tool_calls=tool_calls)
    73| class MessageRole(str, Enum):
    74|     USER = "user"
    75|     ASSISTANT = "assistant"
    76|     SYSTEM = "system"
    77|     TOOL_CALL = "tool-call"
    78|     TOOL_RESPONSE = "tool-response"
    79|     @classmethod
    80|     def roles(cls):
    81|         return [r.value for r in cls]
    82| tool_role_conversions = {
    83|     MessageRole.TOOL_CALL: MessageRole.ASSISTANT,
    84|     MessageRole.TOOL_RESPONSE: MessageRole.USER,
    85| }
    86| def get_json_schema(tool: Tool) -> Dict:
    87|     properties = deepcopy(tool.inputs)
    88|     required = []
    89|     for key, value in properties.items():
    90|         if value["type"] == "any":
    91|             value["type"] = "string"
    92|         if not ("nullable" in value and value["nullable"]):
    93|             required.append(key)
    94|     return {
    95|         "type": "function",
    96|         "function": {
    97|             "name": tool.name,
    98|             "description": tool.description,
    99|             "parameters": {
   100|                 "type": "object",
   101|                 "properties": properties,
   102|                 "required": required,
   103|             },
   104|         },
   105|     }
   106| def remove_stop_sequences(content: str, stop_sequences: List[str]) -> str:
   107|     for stop_seq in stop_sequences:
   108|         if content[-len(stop_seq) :] == stop_seq:
   109|             content = content[: -len(stop_seq)]
   110|     return content
   111| def get_clean_message_list(
   112|     message_list: List[Dict[str, str]],
   113|     role_conversions: Dict[MessageRole, MessageRole] = {},
   114| ) -> List[Dict[str, str]]:
   115|     """
   116|     Subsequent messages with the same role will be concatenated to a single message.
   117|     Args:
   118|         message_list (`List[Dict[str, str]]`): List of chat messages.
   119|     """
   120|     final_message_list = []
   121|     message_list = deepcopy(message_list)  # Avoid modifying the original list
   122|     for message in message_list:
   123|         role = message["role"]
   124|         if role not in MessageRole.roles():
   125|             raise ValueError(
   126|                 f"Incorrect role {role}, only {MessageRole.roles()} are supported for now."
   127|             )
   128|         if role in role_conversions:
   129|             message["role"] = role_conversions[role]
   130|         if (
   131|             len(final_message_list) > 0
   132|             and message["role"] == final_message_list[-1]["role"]
   133|         ):
   134|             final_message_list[-1]["content"] += "\n=======\n" + message["content"]
   135|         else:
   136|             final_message_list.append(message)
   137|     return final_message_list
   138| def parse_dictionary(possible_dictionary: str) -> Union[Dict, str]:
   139|     try:
   140|         start, end = (
   141|             possible_dictionary.find("{"),
   142|             possible_dictionary.rfind("}") + 1,
   143|         )
   144|         return json.loads(possible_dictionary[start:end])
   145|     except Exception:
   146|         return possible_dictionary
   147| class Model:
   148|     def __init__(self):
   149|         self.last_input_token_count = None
   150|         self.last_output_token_count = None
   151|     def get_token_counts(self) -> Dict[str, int]:
   152|         return {
   153|             "input_token_count": self.last_input_token_count,
   154|             "output_token_count": self.last_output_token_count,
   155|         }
   156|     def __call__(
   157|         self,
   158|         messages: List[Dict[str, str]],
   159|         stop_sequences: Optional[List[str]] = None,
   160|         grammar: Optional[str] = None,
   161|     ) -> ChatMessage:
   162|         """Process the input messages and return the model's response.
   163|         Parameters:
   164|             messages (`List[Dict[str, str]]`):
   165|                 A list of message dictionaries to be processed. Each dictionary should have the structure `{"role": "user/system", "content": "message content"}`.
   166|             stop_sequences (`List[str]`, *optional*):
   167|                 A list of strings that will stop the generation if encountered in the model's output.
   168|             grammar (`str`, *optional*):
   169|                 The grammar or formatting structure to use in the model's response.
   170|         Returns:
   171|             `str`: The text content of the model's response.
   172|         """
   173|         pass  # To be implemented in child classes!
   174| class HfApiModel(Model):
   175|     """A class to interact with Hugging Face's Inference API for language model interaction.
   176|     This model allows you to communicate with Hugging Face's models using the Inference API. It can be used in both serverless mode or with a dedicated endpoint, supporting features like stop sequences and grammar customization.
   177|     Parameters:
   178|         model_id (`str`, *optional*, defaults to `"Qwen/Qwen2.5-Coder-32B-Instruct"`):
   179|             The Hugging Face model ID to be used for inference. This can be a path or model identifier from the Hugging Face model hub.
   180|         token (`str`, *optional*):
   181|             Token used by the Hugging Face API for authentication. This token need to be authorized 'Make calls to the serverless Inference API'.
   182|             If the model is gated (like Llama-3 models), the token also needs 'Read access to contents of all public gated repos you can access'.
   183|             If not provided, the class will try to use environment variable 'HF_TOKEN', else use the token stored in the Hugging Face CLI configuration.
   184|         timeout (`int`, *optional*, defaults to 120):
   185|             Timeout for the API request, in seconds.
   186|     Raises:
   187|         ValueError:
   188|             If the model name is not provided.
   189|     Example:
   190|     ```python
   191|     >>> engine = HfApiModel(
   192|     ...     model_id="Qwen/Qwen2.5-Coder-32B-Instruct",
   193|     ...     token="your_hf_token_here",
   194|     ...     max_tokens=5000,
   195|     ... )
   196|     >>> messages = [{"role": "user", "content": "Explain quantum mechanics in simple terms."}]
   197|     >>> response = engine(messages, stop_sequences=["END"])
   198|     >>> print(response)
   199|     "Quantum mechanics is the branch of physics that studies..."
   200|     ```
   201|     """
   202|     def __init__(
   203|         self,
   204|         model_id: str = "Qwen/Qwen2.5-Coder-32B-Instruct",
   205|         token: Optional[str] = None,
   206|         timeout: Optional[int] = 120,
   207|         temperature: float = 0.5,
   208|         **kwargs,
   209|     ):
   210|         super().__init__()
   211|         self.model_id = model_id
   212|         if token is None:
   213|             token = os.getenv("HF_TOKEN")
   214|         self.client = InferenceClient(self.model_id, token=token, timeout=timeout)
   215|         self.temperature = temperature
   216|         self.kwargs = kwargs
   217|     def __call__(
   218|         self,
   219|         messages: List[Dict[str, str]],
   220|         stop_sequences: Optional[List[str]] = None,
   221|         grammar: Optional[str] = None,
   222|         tools_to_call_from: Optional[List[Tool]] = None,
   223|     ) -> ChatMessage:
   224|         """
   225|         Gets an LLM output message for the given list of input messages.
   226|         If argument `tools_to_call_from` is passed, the model's tool calling options will be used to return a tool call.
   227|         """
   228|         messages = get_clean_message_list(
   229|             messages, role_conversions=tool_role_conversions
   230|         )
   231|         if tools_to_call_from:
   232|             response = self.client.chat.completions.create(
   233|                 messages=messages,
   234|                 tools=[get_json_schema(tool) for tool in tools_to_call_from],
   235|                 tool_choice="auto",
   236|                 stop=stop_sequences,
   237|                 temperature=self.temperature,
   238|                 **self.kwargs,
   239|             )
   240|         else:
   241|             response = self.client.chat.completions.create(
   242|                 model=self.model_id,
   243|                 messages=messages,
   244|                 stop=stop_sequences,
   245|                 temperature=self.temperature,
   246|                 **self.kwargs,
   247|             )
   248|         self.last_input_token_count = response.usage.prompt_tokens
   249|         self.last_output_token_count = response.usage.completion_tokens
   250|         return ChatMessage.from_hf_api(response.choices[0].message)
   251| class TransformersModel(Model):
   252|     """A class to interact with Hugging Face's Inference API for language model interaction.
   253|     This model allows you to communicate with Hugging Face's models using the Inference API. It can be used in both serverless mode or with a dedicated endpoint, supporting features like stop sequences and grammar customization.
   254|     Parameters:
   255|         model_id (`str`, *optional*, defaults to `"Qwen/Qwen2.5-Coder-32B-Instruct"`):
   256|             The Hugging Face model ID to be used for inference. This can be a path or model identifier from the Hugging Face model hub.
   257|         device_map (`str`, *optional*):
   258|             The device_map to initialize your model with.
   259|         torch_dtype (`str`, *optional*):
   260|             The torch_dtype to initialize your model with.
   261|         trust_remote_code (bool):
   262|             Some models on the Hub require running remote code: for this model, you would have to set this flag to True.
   263|         kwargs (dict, *optional*):
   264|             Any additional keyword arguments that you want to use in model.generate(), for instance `max_new_tokens` or `device`.
   265|     Raises:
   266|         ValueError:
   267|             If the model name is not provided.
   268|     Example:
   269|     ```python
   270|     >>> engine = TransformersModel(
   271|     ...     model_id="Qwen/Qwen2.5-Coder-32B-Instruct",
   272|     ...     device="cuda",
   273|     ...     max_new_tokens=5000,
   274|     ... )
   275|     >>> messages = [{"role": "user", "content": "Explain quantum mechanics in simple terms."}]
   276|     >>> response = engine(messages, stop_sequences=["END"])
   277|     >>> print(response)
   278|     "Quantum mechanics is the branch of physics that studies..."
   279|     ```
   280|     """
   281|     def __init__(
   282|         self,
   283|         model_id: Optional[str] = None,
   284|         device_map: Optional[str] = None,
   285|         torch_dtype: Optional[str] = None,
   286|         trust_remote_code: bool = False,
   287|         **kwargs,
   288|     ):
   289|         super().__init__()
   290|         if not is_torch_available():
   291|             raise ImportError("Please install torch in order to use TransformersModel.")
   292|         import torch
   293|         default_model_id = "HuggingFaceTB/SmolLM2-1.7B-Instruct"
   294|         if model_id is None:
   295|             model_id = default_model_id
   296|             logger.warning(
   297|                 f"`model_id`not provided, using this default tokenizer for token counts: '{model_id}'"
   298|             )
   299|         self.model_id = model_id
   300|         self.kwargs = kwargs
   301|         if device_map is None:
   302|             device_map = "cuda" if torch.cuda.is_available() else "cpu"
   303|         logger.info(f"Using device: {device_map}")
   304|         try:
   305|             self.tokenizer = AutoTokenizer.from_pretrained(model_id)
   306|             self.model = AutoModelForCausalLM.from_pretrained(
   307|                 model_id,
   308|                 device_map=device_map,
   309|                 torch_dtype=torch_dtype,
   310|                 trust_remote_code=trust_remote_code,
   311|             )
   312|         except Exception as e:
   313|             logger.warning(
   314|                 f"Failed to load tokenizer and model for {model_id=}: {e}. Loading default tokenizer and model instead from {default_model_id=}."
   315|             )
   316|             self.model_id = default_model_id
   317|             self.tokenizer = AutoTokenizer.from_pretrained(default_model_id)
   318|             self.model = AutoModelForCausalLM.from_pretrained(
   319|                 model_id, device_map=device_map, torch_dtype=torch_dtype
   320|             )
   321|     def make_stopping_criteria(self, stop_sequences: List[str]) -> StoppingCriteriaList:
   322|         class StopOnStrings(StoppingCriteria):
   323|             def __init__(self, stop_strings: List[str], tokenizer):
   324|                 self.stop_strings = stop_strings
   325|                 self.tokenizer = tokenizer
   326|                 self.stream = ""
   327|             def reset(self):
   328|                 self.stream = ""
   329|             def __call__(self, input_ids, scores, **kwargs):
   330|                 generated = self.tokenizer.decode(
   331|                     input_ids[0][-1], skip_special_tokens=True
   332|                 )
   333|                 self.stream += generated
   334|                 if any(
   335|                     [
   336|                         self.stream.endswith(stop_string)
   337|                         for stop_string in self.stop_strings
   338|                     ]
   339|                 ):
   340|                     return True
   341|                 return False
   342|         return StoppingCriteriaList([StopOnStrings(stop_sequences, self.tokenizer)])
   343|     def __call__(
   344|         self,
   345|         messages: List[Dict[str, str]],
   346|         stop_sequences: Optional[List[str]] = None,
   347|         grammar: Optional[str] = None,
   348|         tools_to_call_from: Optional[List[Tool]] = None,
   349|     ) -> ChatMessage:
   350|         messages = get_clean_message_list(
   351|             messages, role_conversions=tool_role_conversions
   352|         )
   353|         if tools_to_call_from is not None:
   354|             prompt_tensor = self.tokenizer.apply_chat_template(
   355|                 messages,
   356|                 tools=[get_json_schema(tool) for tool in tools_to_call_from],
   357|                 return_tensors="pt",
   358|                 return_dict=True,
   359|                 add_generation_prompt=True,
   360|             )
   361|         else:
   362|             prompt_tensor = self.tokenizer.apply_chat_template(
   363|                 messages,
   364|                 return_tensors="pt",
   365|                 return_dict=True,
   366|             )
   367|         prompt_tensor = prompt_tensor.to(self.model.device)
   368|         count_prompt_tokens = prompt_tensor["input_ids"].shape[1]
   369|         out = self.model.generate(
   370|             **prompt_tensor,
   371|             stopping_criteria=(
   372|                 self.make_stopping_criteria(stop_sequences) if stop_sequences else None
   373|             ),
   374|             **self.kwargs,
   375|         )
   376|         generated_tokens = out[0, count_prompt_tokens:]
   377|         output = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
   378|         self.last_input_token_count = count_prompt_tokens
   379|         self.last_output_token_count = len(generated_tokens)
   380|         if stop_sequences is not None:
   381|             output = remove_stop_sequences(output, stop_sequences)
   382|         if tools_to_call_from is None:
   383|             return ChatMessage(role="assistant", content=output)
   384|         else:
   385|             if "Action:" in output:
   386|                 output = output.split("Action:", 1)[1].strip()
   387|             parsed_output = json.loads(output)
   388|             tool_name = parsed_output.get("tool_name")
   389|             tool_arguments = parsed_output.get("tool_arguments")
   390|             return ChatMessage(
   391|                 role="assistant",
   392|                 content="",
   393|                 tool_calls=[
   394|                     ChatMessageToolCall(
   395|                         id="".join(random.choices("0123456789", k=5)),
   396|                         type="function",
   397|                         function=ChatMessageToolCallDefinition(
   398|                             name=tool_name, arguments=tool_arguments
   399|                         ),
   400|                     )
   401|                 ],
   402|             )
   403| class LiteLLMModel(Model):
   404|     """This model connects to [LiteLLM](https://www.litellm.ai/) as a gateway to hundreds of LLMs.
   405|     Parameters:
   406|         model_id (`str`):
   407|             The model identifier to use on the server (e.g. "gpt-3.5-turbo").
   408|         api_base (`str`):
   409|             The base URL of the OpenAI-compatible API server.
   410|         api_key (`str`):
   411|             The API key to use for authentication.
   412|         **kwargs:
   413|             Additional keyword arguments to pass to the OpenAI API.
   414|     """
   415|     def __init__(
   416|         self,
   417|         model_id="anthropic/claude-3-5-sonnet-20240620",
   418|         api_base=None,
   419|         api_key=None,
   420|         **kwargs,
   421|     ):
   422|         try:
   423|             import litellm
   424|         except ModuleNotFoundError:
   425|             raise ModuleNotFoundError(
   426|                 "Please install 'litellm' extra to use LiteLLMModel: `pip install 'smolagents[litellm]'`"
   427|             )
   428|         super().__init__()
   429|         self.model_id = model_id
   430|         litellm.add_function_to_prompt = True
   431|         self.api_base = api_base
   432|         self.api_key = api_key
   433|         self.kwargs = kwargs
   434|     def __call__(
   435|         self,
   436|         messages: List[Dict[str, str]],
   437|         stop_sequences: Optional[List[str]] = None,
   438|         grammar: Optional[str] = None,
   439|         tools_to_call_from: Optional[List[Tool]] = None,
   440|     ) -> ChatMessage:
   441|         messages = get_clean_message_list(
   442|             messages, role_conversions=tool_role_conversions
   443|         )
   444|         import litellm
   445|         if tools_to_call_from:
   446|             response = litellm.completion(
   447|                 model=self.model_id,
   448|                 messages=messages,
   449|                 tools=[get_json_schema(tool) for tool in tools_to_call_from],
   450|                 tool_choice="required",
   451|                 stop=stop_sequences,
   452|                 api_base=self.api_base,
   453|                 api_key=self.api_key,
   454|                 **self.kwargs,
   455|             )
   456|         else:
   457|             response = litellm.completion(
   458|                 model=self.model_id,
   459|                 messages=messages,
   460|                 stop=stop_sequences,
   461|                 api_base=self.api_base,
   462|                 api_key=self.api_key,
   463|                 **self.kwargs,
   464|             )
   465|         self.last_input_token_count = response.usage.prompt_tokens
   466|         self.last_output_token_count = response.usage.completion_tokens
   467|         return response.choices[0].message
   468| class OpenAIServerModel(Model):
   469|     """This model connects to an OpenAI-compatible API server.
   470|     Parameters:
   471|         model_id (`str`):
   472|             The model identifier to use on the server (e.g. "gpt-3.5-turbo").
   473|         api_base (`str`, *optional*):
   474|             The base URL of the OpenAI-compatible API server.
   475|         api_key (`str`, *optional*):
   476|             The API key to use for authentication.
   477|         custom_role_conversions (`Dict{str, str]`, *optional*):
   478|             Custom role conversion mapping to convert message roles in others.
   479|             Useful for specific models that do not support specific message roles like "system".
   480|         **kwargs:
   481|             Additional keyword arguments to pass to the OpenAI API.
   482|     """
   483|     def __init__(
   484|         self,
   485|         model_id: str,
   486|         api_base: Optional[str] = None,
   487|         api_key: Optional[str] = None,
   488|         custom_role_conversions: Optional[Dict[str, str]] = None,
   489|         **kwargs,
   490|     ):
   491|         try:
   492|             import openai
   493|         except ModuleNotFoundError:
   494|             raise ModuleNotFoundError(
   495|                 "Please install 'openai' extra to use OpenAIServerModel: `pip install 'smolagents[openai]'`"
   496|             ) from None
   497|         super().__init__()
   498|         self.model_id = model_id
   499|         self.client = openai.OpenAI(
   500|             base_url=api_base,
   501|             api_key=api_key,
   502|         )
   503|         self.kwargs = kwargs
   504|         self.custom_role_conversions = custom_role_conversions
   505|     def __call__(
   506|         self,
   507|         messages: List[Dict[str, str]],
   508|         stop_sequences: Optional[List[str]] = None,
   509|         grammar: Optional[str] = None,
   510|         tools_to_call_from: Optional[List[Tool]] = None,
   511|     ) -> ChatMessage:
   512|         messages = get_clean_message_list(
   513|             messages,
   514|             role_conversions=(
   515|                 self.custom_role_conversions
   516|                 if self.custom_role_conversions
   517|                 else tool_role_conversions
   518|             ),
   519|         )
   520|         if tools_to_call_from:
   521|             response = self.client.chat.completions.create(
   522|                 model=self.model_id,
   523|                 messages=messages,
   524|                 tools=[get_json_schema(tool) for tool in tools_to_call_from],
   525|                 tool_choice="auto",
   526|                 stop=stop_sequences,
   527|                 **self.kwargs,
   528|             )
   529|         else:
   530|             response = self.client.chat.completions.create(
   531|                 model=self.model_id,
   532|                 messages=messages,
   533|                 stop=stop_sequences,
   534|                 **self.kwargs,
   535|             )
   536|         self.last_input_token_count = response.usage.prompt_tokens
   537|         self.last_output_token_count = response.usage.completion_tokens
   538|         return response.choices[0].message
   539| __all__ = [
   540|     "MessageRole",
   541|     "tool_role_conversions",
   542|     "get_clean_message_list",
   543|     "Model",
   544|     "TransformersModel",
   545|     "HfApiModel",
   546|     "LiteLLMModel",
   547|     "OpenAIServerModel",
   548|     "ChatMessage",
   549| ]


# ====================================================================
# FILE: src/smolagents/monitoring.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-32 ---
     1| from rich.text import Text
     2| class Monitor:
     3|     def __init__(self, tracked_model, logger):
     4|         self.step_durations = []
     5|         self.tracked_model = tracked_model
     6|         self.logger = logger
     7|         if (
     8|             getattr(self.tracked_model, "last_input_token_count", "Not found")
     9|             != "Not found"
    10|         ):
    11|             self.total_input_token_count = 0
    12|             self.total_output_token_count = 0
    13|     def get_total_token_counts(self):
    14|         return {
    15|             "input": self.total_input_token_count,
    16|             "output": self.total_output_token_count,
    17|         }
    18|     def reset(self):
    19|         self.step_durations = []
    20|         self.total_input_token_count = 0
    21|         self.total_output_token_count = 0
    22|     def update_metrics(self, step_log):
    23|         step_duration = step_log.duration
    24|         self.step_durations.append(step_duration)
    25|         console_outputs = f"[Step {len(self.step_durations) - 1}: Duration {step_duration:.2f} seconds"
    26|         if getattr(self.tracked_model, "last_input_token_count", None) is not None:
    27|             self.total_input_token_count += self.tracked_model.last_input_token_count
    28|             self.total_output_token_count += self.tracked_model.last_output_token_count
    29|             console_outputs += f"| Input tokens: {self.total_input_token_count:,} | Output tokens: {self.total_output_token_count:,}"
    30|         console_outputs += "]"
    31|         self.logger.log(Text(console_outputs, style="dim"), level=1)
    32| __all__ = ["Monitor"]


# ====================================================================
# FILE: src/smolagents/prompts.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 69-168 ---
    69| ```<end_code>
    70| ---
    71| Above example were using tools that might not exist for you. You only have access to these tools:
    72| {{tool_names}}
    73| {{managed_agents_descriptions}}
    74| Remember to make sure that variables you use are all defined. In particular don't import packages!
    75| Be sure to provide a 'Code:\n```' sequence before the code and '```<end_code>' after, else you will get an error.
    76| DO NOT pass the arguments as a dict as in 'answer = ask_search_agent({'query': "What is the place where James Bond lives?"})', but use the arguments directly as in 'answer = ask_search_agent(query="What is the place where James Bond lives?")'.
    77| Now Begin! If you solve the task correctly, you will receive a reward of $1,000,000.
    78| """
    79| TOOL_CALLING_SYSTEM_PROMPT = """You are an expert assistant who can solve any task using  tool calls. You will be given a task to solve as best you can.
    80| To do so, you have been given access to the following tools: {{tool_names}}
    81| The tool call you write is an action: after the tool is executed, you will get the result of the tool call as an "observation".
    82| This Action/Observation can repeat N times, you should take several steps when needed.
    83| You can use the result of the previous action as input for the next action.
    84| The observation will always be a string: it can represent a file, like "image_1.jpg".
    85| Then you can use it as input for the next action. You can do it for instance as follows:
    86| Observation: "image_1.jpg"
    87| Action:
    88| {
    89|   "tool_name": "image_transformer",
    90|   "tool_arguments": {"image": "image_1.jpg"}
    91| }
    92| To provide the final answer to the task, use an action blob with "tool_name": "final_answer" tool. It is the only way to complete the task, else you will be stuck on a loop. So your final output should look like this:
    93| Action:
    94| {
    95|   "tool_name": "final_answer",
    96|   "tool_arguments": {"answer": "insert your final answer here"}
    97| }
    98| Here are a few examples using notional tools:
    99| ---
   100| Task: "Generate an image of the oldest person in this document."
   101| Action:
   102| {
   103|   "tool_name": "document_qa",
   104|   "tool_arguments": {"document": "document.pdf", "question": "Who is the oldest person mentioned?"}
   105| }
   106| Observation: "The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland."
   107| Action:
   108| {
   109|   "tool_name": "image_generator",
   110|   "tool_arguments": {"prompt": "A portrait of John Doe, a 55-year-old man living in Canada."}
   111| }
   112| Observation: "image.png"
   113| Action:
   114| {
   115|   "tool_name": "final_answer",
   116|   "tool_arguments": "image.png"
   117| }
   118| ---
   119| Task: "What is the result of the following operation: 5 + 3 + 1294.678?"
   120| Action:
   121| {
   122|     "tool_name": "python_interpreter",
   123|     "tool_arguments": {"code": "5 + 3 + 1294.678"}
   124| }
   125| Observation: 1302.678
   126| Action:
   127| {
   128|   "tool_name": "final_answer",
   129|   "tool_arguments": "1302.678"
   130| }
   131| ---
   132| Task: "Which city has the highest population , Guangzhou or Shanghai?"
   133| Action:
   134| {
   135|     "tool_name": "search",
   136|     "tool_arguments": "Population Guangzhou"
   137| }
   138| Observation: ['Guangzhou has a population of 15 million inhabitants as of 2021.']
   139| Action:
   140| {
   141|     "tool_name": "search",
   142|     "tool_arguments": "Population Shanghai"
   143| }
   144| Observation: '26 million (2019)'
   145| Action:
   146| {
   147|   "tool_name": "final_answer",
   148|   "tool_arguments": "Shanghai"
   149| }
   150| Above example were using notional tools that might not exist for you. You only have access to these tools:
   151| {{tool_descriptions}}
   152| {{managed_agents_descriptions}}
   153| Here are the rules you should always follow to solve your task:
   154| 1. ALWAYS provide a tool call, else you will fail.
   155| 2. Always use the right arguments for the tools. Never use variable names as the action arguments, use the value instead.
   156| 3. Call a tool only when needed: do not call the search agent if you do not need information, try to solve the task yourself.
   157| If no tool call is needed, use final_answer tool to return your answer.
   158| 4. Never re-do a tool call that you previously did with the exact same parameters.
   159| Now Begin! If you solve the task correctly, you will receive a reward of $1,000,000.
   160| """
   161| CODE_SYSTEM_PROMPT = """You are an expert assistant who can solve any task using code blobs. You will be given a task to solve as best you can.
   162| To do so, you have been given access to a list of tools: these tools are basically Python functions which you can call with code.
   163| To solve the task, you must plan forward to proceed in a series of steps, in a cycle of 'Thought:', 'Code:', and 'Observation:' sequences.
   164| At each step, in the 'Thought:' sequence, you should first explain your reasoning towards solving the task and the tools that you want to use.
   165| Then in the 'Code:' sequence, you should write the code in simple Python. The code sequence must end with '<end_code>' sequence.
   166| During each intermediate step, you can use 'print()' to save whatever important information you will then need.
   167| These print outputs will then appear in the 'Observation:' field, which will be available as input for the next step.
   168| In the end you have to return a final answer using the `final_answer` tool.


# ====================================================================
# FILE: src/smolagents/tool_validation.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-26 ---
     1| import ast
     2| import builtins
     3| import inspect
     4| import textwrap
     5| from typing import Set
     6| from .utils import BASE_BUILTIN_MODULES
     7| _BUILTIN_NAMES = set(vars(builtins))
     8| class MethodChecker(ast.NodeVisitor):
     9|     """
    10|     Checks that a method
    11|     - only uses defined names
    12|     - contains no local imports (e.g. numpy is ok but local_script is not)
    13|     """
    14|     def __init__(self, class_attributes: Set[str], check_imports: bool = True):
    15|         self.undefined_names = set()
    16|         self.imports = {}
    17|         self.from_imports = {}
    18|         self.assigned_names = set()
    19|         self.arg_names = set()
    20|         self.class_attributes = class_attributes
    21|         self.errors = []
    22|         self.check_imports = check_imports
    23|     def visit_arguments(self, node):
    24|         """Collect function arguments"""
    25|         self.arg_names = {arg.arg for arg in node.args}
    26|         if node.kwarg:

# --- HUNK 2: Lines 94-170 ---
    94|                 or node.func.id in self.class_attributes
    95|                 or node.func.id in self.imports
    96|                 or node.func.id in self.from_imports
    97|                 or node.func.id in self.assigned_names
    98|             ):
    99|                 self.errors.append(f"Name '{node.func.id}' is undefined.")
   100|         self.generic_visit(node)
   101| def validate_tool_attributes(cls, check_imports: bool = True) -> None:
   102|     """
   103|     Validates that a Tool class follows the proper patterns:
   104|     0. __init__ takes no argument (args chosen at init are not traceable so we cannot rebuild the source code for them, make them class attributes!).
   105|     1. About the class:
   106|         - Class attributes should only be strings or dicts
   107|         - Class attributes cannot be complex attributes
   108|     2. About all class methods:
   109|         - Imports must be from packages, not local files
   110|         - All methods must be self-contained
   111|     Raises all errors encountered, if no error returns None.
   112|     """
   113|     errors = []
   114|     source = textwrap.dedent(inspect.getsource(cls))
   115|     tree = ast.parse(source)
   116|     if not isinstance(tree.body[0], ast.ClassDef):
   117|         raise ValueError("Source code must define a class")
   118|     if not cls.__init__.__qualname__ == "Tool.__init__":
   119|         sig = inspect.signature(cls.__init__)
   120|         non_self_params = list(
   121|             [arg_name for arg_name in sig.parameters.keys() if arg_name != "self"]
   122|         )
   123|         if len(non_self_params) > 0:
   124|             errors.append(
   125|                 f"This tool has additional args specified in __init__(self): {non_self_params}. Make sure it does not, all values should be hardcoded!"
   126|             )
   127|     class_node = tree.body[0]
   128|     class ClassLevelChecker(ast.NodeVisitor):
   129|         def __init__(self):
   130|             self.imported_names = set()
   131|             self.complex_attributes = set()
   132|             self.class_attributes = set()
   133|             self.in_method = False
   134|         def visit_FunctionDef(self, node):
   135|             old_context = self.in_method
   136|             self.in_method = True
   137|             self.generic_visit(node)
   138|             self.in_method = old_context
   139|         def visit_Assign(self, node):
   140|             if self.in_method:
   141|                 return
   142|             for target in node.targets:
   143|                 if isinstance(target, ast.Name):
   144|                     self.class_attributes.add(target.id)
   145|             if not all(
   146|                 isinstance(
   147|                     val, (ast.Str, ast.Num, ast.Constant, ast.Dict, ast.List, ast.Set)
   148|                 )
   149|                 for val in ast.walk(node.value)
   150|             ):
   151|                 for target in node.targets:
   152|                     if isinstance(target, ast.Name):
   153|                         self.complex_attributes.add(target.id)
   154|     class_level_checker = ClassLevelChecker()
   155|     class_level_checker.visit(class_node)
   156|     if class_level_checker.complex_attributes:
   157|         errors.append(
   158|             f"Complex attributes should be defined in __init__, not as class attributes: "
   159|             f"{', '.join(class_level_checker.complex_attributes)}"
   160|         )
   161|     for node in class_node.body:
   162|         if isinstance(node, ast.FunctionDef):
   163|             method_checker = MethodChecker(
   164|                 class_level_checker.class_attributes, check_imports=check_imports
   165|             )
   166|             method_checker.visit(node)
   167|             errors += [f"- {node.name}: {error}" for error in method_checker.errors]
   168|     if errors:
   169|         raise ValueError("Tool validation failed:\n" + "\n".join(errors))
   170|     return


# ====================================================================
# FILE: src/smolagents/tools.py
# Total hunks: 12
# ====================================================================
# --- HUNK 1: Lines 1-114 ---
     1| import ast
     2| import importlib
     3| import inspect
     4| import json
     5| import logging
     6| import os
     7| import sys
     8| import tempfile
     9| import textwrap
    10| from contextlib import contextmanager
    11| from functools import lru_cache, wraps
    12| from pathlib import Path
    13| from typing import Callable, Dict, List, Optional, Union, get_type_hints
    14| from huggingface_hub import (
    15|     create_repo,
    16|     get_collection,
    17|     hf_hub_download,
    18|     metadata_update,
    19|     upload_folder,
    20| )
    21| from huggingface_hub.utils import RepositoryNotFoundError
    22| from packaging import version
    23| from transformers.dynamic_module_utils import get_imports
    24| from transformers.utils import (
    25|     TypeHintParsingException,
    26|     cached_file,
    27|     get_json_schema,
    28|     is_accelerate_available,
    29|     is_torch_available,
    30| )
    31| from transformers.utils.chat_template_utils import _parse_type_hint
    32| from .tool_validation import MethodChecker, validate_tool_attributes
    33| from .types import ImageType, handle_agent_input_types, handle_agent_output_types
    34| from .utils import instance_to_source
    35| logger = logging.getLogger(__name__)
    36| if is_accelerate_available():
    37|     from accelerate import PartialState
    38|     from accelerate.utils import send_to_device
    39| if is_torch_available():
    40|     from transformers import AutoProcessor
    41| else:
    42|     AutoProcessor = object
    43| TOOL_CONFIG_FILE = "tool_config.json"
    44| def get_repo_type(repo_id, repo_type=None, **hub_kwargs):
    45|     if repo_type is not None:
    46|         return repo_type
    47|     try:
    48|         hf_hub_download(repo_id, TOOL_CONFIG_FILE, repo_type="space", **hub_kwargs)
    49|         return "space"
    50|     except RepositoryNotFoundError:
    51|         try:
    52|             hf_hub_download(repo_id, TOOL_CONFIG_FILE, repo_type="model", **hub_kwargs)
    53|             return "model"
    54|         except RepositoryNotFoundError:
    55|             raise EnvironmentError(
    56|                 f"`{repo_id}` does not seem to be a valid repo identifier on the Hub."
    57|             )
    58|         except Exception:
    59|             return "model"
    60|     except Exception:
    61|         return "space"
    62| def validate_after_init(cls):
    63|     original_init = cls.__init__
    64|     @wraps(original_init)
    65|     def new_init(self, *args, **kwargs):
    66|         original_init(self, *args, **kwargs)
    67|         self.validate_arguments()
    68|     cls.__init__ = new_init
    69|     return cls
    70| def _convert_type_hints_to_json_schema(func: Callable) -> Dict:
    71|     type_hints = get_type_hints(func)
    72|     signature = inspect.signature(func)
    73|     properties = {}
    74|     for param_name, param_type in type_hints.items():
    75|         if param_name != "return":
    76|             properties[param_name] = _parse_type_hint(param_type)
    77|             if signature.parameters[param_name].default != inspect.Parameter.empty:
    78|                 properties[param_name]["nullable"] = True
    79|     for param_name in signature.parameters.keys():
    80|         if signature.parameters[param_name].default != inspect.Parameter.empty:
    81|             if (
    82|                 param_name not in properties
    83|             ):  # this can happen if the param has no type hint but a default value
    84|                 properties[param_name] = {"nullable": True}
    85|     return properties
    86| AUTHORIZED_TYPES = [
    87|     "string",
    88|     "boolean",
    89|     "integer",
    90|     "number",
    91|     "image",
    92|     "audio",
    93|     "any",
    94|     "object",
    95| ]
    96| CONVERSION_DICT = {"str": "string", "int": "integer", "float": "number"}
    97| class Tool:
    98|     """
    99|     A base class for the functions used by the agent. Subclass this and implement the `forward` method as well as the
   100|     following class attributes:
   101|     - **description** (`str`) -- A short description of what your tool does, the inputs it expects and the output(s) it
   102|       will return. For instance 'This is a tool that downloads a file from a `url`. It takes the `url` as input, and
   103|       returns the text contained in the file'.
   104|     - **name** (`str`) -- A performative name that will be used for your tool in the prompt to the agent. For instance
   105|       `"text-classifier"` or `"image_generator"`.
   106|     - **inputs** (`Dict[str, Dict[str, Union[str, type]]]`) -- The dict of modalities expected for the inputs.
   107|       It has one `type`key and a `description`key.
   108|       This is used by `launch_gradio_demo` or to make a nice space from your tool, and also can be used in the generated
   109|       description for your tool.
   110|     - **output_type** (`type`) -- The type of the tool output. This is used by `launch_gradio_demo`
   111|       or to make a nice space from your tool, and also can be used in the generated description for your tool.
   112|     You can also override the method [`~Tool.setup`] if your tool has an expensive operation to perform before being
   113|     usable (such as loading a model). [`~Tool.setup`] will be called the first time you use your tool, but not at
   114|     instantiation.

# --- HUNK 2: Lines 121-184 ---
   121|         self.is_initialized = False
   122|     def __init_subclass__(cls, **kwargs):
   123|         super().__init_subclass__(**kwargs)
   124|         validate_after_init(cls)
   125|     def validate_arguments(self):
   126|         required_attributes = {
   127|             "description": str,
   128|             "name": str,
   129|             "inputs": dict,
   130|             "output_type": str,
   131|         }
   132|         for attr, expected_type in required_attributes.items():
   133|             attr_value = getattr(self, attr, None)
   134|             if attr_value is None:
   135|                 raise TypeError(f"You must set an attribute {attr}.")
   136|             if not isinstance(attr_value, expected_type):
   137|                 raise TypeError(
   138|                     f"Attribute {attr} should have type {expected_type.__name__}, got {type(attr_value)} instead."
   139|                 )
   140|         for input_name, input_content in self.inputs.items():
   141|             assert isinstance(input_content, dict), (
   142|                 f"Input '{input_name}' should be a dictionary."
   143|             )
   144|             assert "type" in input_content and "description" in input_content, (
   145|                 f"Input '{input_name}' should have keys 'type' and 'description', has only {list(input_content.keys())}."
   146|             )
   147|             if input_content["type"] not in AUTHORIZED_TYPES:
   148|                 raise Exception(
   149|                     f"Input '{input_name}': type '{input_content['type']}' is not an authorized value, should be one of {AUTHORIZED_TYPES}."
   150|                 )
   151|         assert getattr(self, "output_type", None) in AUTHORIZED_TYPES
   152|         if not (
   153|             hasattr(self, "skip_forward_signature_validation")
   154|             and getattr(self, "skip_forward_signature_validation") is True
   155|         ):
   156|             signature = inspect.signature(self.forward)
   157|             if not set(signature.parameters.keys()) == set(self.inputs.keys()):
   158|                 raise Exception(
   159|                     "Tool's 'forward' method should take 'self' as its first argument, then its next arguments should match the keys of tool attribute 'inputs'."
   160|                 )
   161|             json_schema = _convert_type_hints_to_json_schema(self.forward)
   162|             for key, value in self.inputs.items():
   163|                 if "nullable" in value:
   164|                     assert key in json_schema and "nullable" in json_schema[key], (
   165|                         f"Nullable argument '{key}' in inputs should have key 'nullable' set to True in function signature."
   166|                     )
   167|                 if key in json_schema and "nullable" in json_schema[key]:
   168|                     assert "nullable" in value, (
   169|                         f"Nullable argument '{key}' in function signature should have key 'nullable' set to True in inputs."
   170|                     )
   171|     def forward(self, *args, **kwargs):
   172|         return NotImplementedError("Write this method in your subclass of `Tool`.")
   173|     def __call__(self, *args, sanitize_inputs_outputs: bool = False, **kwargs):
   174|         if not self.is_initialized:
   175|             self.setup()
   176|         if len(args) == 1 and len(kwargs) == 0 and isinstance(args[0], dict):
   177|             potential_kwargs = args[0]
   178|             if all(key in self.inputs for key in potential_kwargs):
   179|                 args = ()
   180|                 kwargs = potential_kwargs
   181|         if sanitize_inputs_outputs:
   182|             args, kwargs = handle_agent_input_types(*args, **kwargs)
   183|         outputs = self.forward(*args, **kwargs)
   184|         if sanitize_inputs_outputs:

# --- HUNK 3: Lines 188-234 ---
   188|         """
   189|         Overwrite this method here for any operation that is expensive and needs to be executed before you start using
   190|         your tool. Such as loading a big model.
   191|         """
   192|         self.is_initialized = True
   193|     def save(self, output_dir):
   194|         """
   195|         Saves the relevant code files for your tool so it can be pushed to the Hub. This will copy the code of your
   196|         tool in `output_dir` as well as autogenerate:
   197|         - a `tool.py` file containing the logic for your tool.
   198|         - an `app.py` file providing an UI for your tool when it is exported to a Space with `tool.push_to_hub()`
   199|         - a `requirements.txt` containing the names of the module used by your tool (as detected when inspecting its
   200|           code)
   201|         Args:
   202|             output_dir (`str`): The folder in which you want to save your tool.
   203|         """
   204|         os.makedirs(output_dir, exist_ok=True)
   205|         class_name = self.__class__.__name__
   206|         tool_file = os.path.join(output_dir, "tool.py")
   207|         if type(self).__name__ == "SimpleTool":
   208|             source_code = inspect.getsource(self.forward).replace("@tool", "")
   209|             forward_node = ast.parse(textwrap.dedent(source_code))
   210|             method_checker = MethodChecker(set())
   211|             method_checker.visit(forward_node)
   212|             if len(method_checker.errors) > 0:
   213|                 raise (ValueError("\n".join(method_checker.errors)))
   214|             forward_source_code = inspect.getsource(self.forward)
   215|             tool_code = textwrap.dedent(
   216|                 f"""
   217|             from smolagents import Tool
   218|             from typing import Optional
   219|             class {class_name}(Tool):
   220|                 name = "{self.name}"
   221|                 description = "{self.description}"
   222|                 inputs = {json.dumps(self.inputs, separators=(",", ":"))}
   223|                 output_type = "{self.output_type}"
   224|             """
   225|             ).strip()
   226|             import re
   227|             def add_self_argument(source_code: str) -> str:
   228|                 """Add 'self' as first argument to a function definition if not present."""
   229|                 pattern = r"def forward\(((?!self)[^)]*)\)"
   230|                 def replacement(match):
   231|                     args = match.group(1).strip()
   232|                     if args:  # If there are other arguments
   233|                         return f"def forward(self, {args})"
   234|                     return "def forward(self)"

# --- HUNK 4: Lines 246-298 ---
   246|                 raise ValueError(
   247|                     "Cannot save objects created with from_space, from_langchain or from_gradio, as this would create errors."
   248|                 )
   249|             validate_tool_attributes(self.__class__)
   250|             tool_code = instance_to_source(self, base_cls=Tool)
   251|         with open(tool_file, "w", encoding="utf-8") as f:
   252|             f.write(tool_code.replace(":true,", ":True,").replace(":true}", ":True}"))
   253|         app_file = os.path.join(output_dir, "app.py")
   254|         with open(app_file, "w", encoding="utf-8") as f:
   255|             f.write(
   256|                 textwrap.dedent(
   257|                     f"""
   258|             from smolagents import launch_gradio_demo
   259|             from typing import Optional
   260|             from tool import {class_name}
   261|             tool = {class_name}()
   262|             launch_gradio_demo(tool)
   263|             """
   264|                 ).lstrip()
   265|             )
   266|         requirements_file = os.path.join(output_dir, "requirements.txt")
   267|         imports = []
   268|         for module in [tool_file]:
   269|             imports.extend(get_imports(module))
   270|         imports = list(
   271|             set(
   272|                 [
   273|                     el
   274|                     for el in imports + ["smolagents"]
   275|                     if el not in sys.stdlib_module_names
   276|                 ]
   277|             )
   278|         )
   279|         with open(requirements_file, "w", encoding="utf-8") as f:
   280|             f.write("\n".join(imports) + "\n")
   281|     def push_to_hub(
   282|         self,
   283|         repo_id: str,
   284|         commit_message: str = "Upload tool",
   285|         private: Optional[bool] = None,
   286|         token: Optional[Union[bool, str]] = None,
   287|         create_pr: bool = False,
   288|     ) -> str:
   289|         """
   290|         Upload the tool to the Hub.
   291|         For this method to work properly, your tool must have been defined in a separate module (not `__main__`).
   292|         For instance:
   293|         ```
   294|         from my_tool_module import MyTool
   295|         my_tool = MyTool()
   296|         my_tool.push_to_hub("my-username/my-space")
   297|         ```
   298|         Parameters:

# --- HUNK 5: Lines 304-428 ---
   304|             private (`bool`, *optional*):
   305|                 Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.
   306|             token (`bool` or `str`, *optional*):
   307|                 The token to use as HTTP bearer authorization for remote files. If unset, will use the token generated
   308|                 when running `huggingface-cli login` (stored in `~/.huggingface`).
   309|             create_pr (`bool`, *optional*, defaults to `False`):
   310|                 Whether or not to create a PR with the uploaded files or directly commit.
   311|         """
   312|         repo_url = create_repo(
   313|             repo_id=repo_id,
   314|             token=token,
   315|             private=private,
   316|             exist_ok=True,
   317|             repo_type="space",
   318|             space_sdk="gradio",
   319|         )
   320|         repo_id = repo_url.repo_id
   321|         metadata_update(repo_id, {"tags": ["tool"]}, repo_type="space")
   322|         with tempfile.TemporaryDirectory() as work_dir:
   323|             self.save(work_dir)
   324|             print(work_dir)
   325|             with open(work_dir + "/tool.py", "r") as f:
   326|                 print("\n".join(f.readlines()))
   327|             logger.info(
   328|                 f"Uploading the following files to {repo_id}: {','.join(os.listdir(work_dir))}"
   329|             )
   330|             return upload_folder(
   331|                 repo_id=repo_id,
   332|                 commit_message=commit_message,
   333|                 folder_path=work_dir,
   334|                 token=token,
   335|                 create_pr=create_pr,
   336|                 repo_type="space",
   337|             )
   338|     @classmethod
   339|     def from_hub(
   340|         cls,
   341|         repo_id: str,
   342|         token: Optional[str] = None,
   343|         trust_remote_code: bool = False,
   344|         **kwargs,
   345|     ):
   346|         """
   347|         Loads a tool defined on the Hub.
   348|         <Tip warning={true}>
   349|         Loading a tool from the Hub means that you'll download the tool and execute it locally.
   350|         ALWAYS inspect the tool you're downloading before loading it within your runtime, as you would do when
   351|         installing a package using pip/npm/apt.
   352|         </Tip>
   353|         Args:
   354|             repo_id (`str`):
   355|                 The name of the repo on the Hub where your tool is defined.
   356|             token (`str`, *optional*):
   357|                 The token to identify you on hf.co. If unset, will use the token generated when running
   358|                 `huggingface-cli login` (stored in `~/.huggingface`).
   359|             trust_remote_code(`str`, *optional*, defaults to False):
   360|                 This flags marks that you understand the risk of running remote code and that you trust this tool.
   361|                 If not setting this to True, loading the tool from Hub will fail.
   362|             kwargs (additional keyword arguments, *optional*):
   363|                 Additional keyword arguments that will be split in two: all arguments relevant to the Hub (such as
   364|                 `cache_dir`, `revision`, `subfolder`) will be used when downloading the files for your tool, and the
   365|                 others will be passed along to its init.
   366|         """
   367|         assert trust_remote_code, (
   368|             "Loading a tool from Hub requires to trust remote code. Make sure you've inspected the repo and pass `trust_remote_code=True` to load the tool."
   369|         )
   370|         hub_kwargs_names = [
   371|             "cache_dir",
   372|             "force_download",
   373|             "resume_download",
   374|             "proxies",
   375|             "revision",
   376|             "repo_type",
   377|             "subfolder",
   378|             "local_files_only",
   379|         ]
   380|         hub_kwargs = {k: v for k, v in kwargs.items() if k in hub_kwargs_names}
   381|         tool_file = "tool.py"
   382|         hub_kwargs["repo_type"] = get_repo_type(repo_id, **hub_kwargs)
   383|         resolved_tool_file = cached_file(
   384|             repo_id,
   385|             tool_file,
   386|             token=token,
   387|             **hub_kwargs,
   388|             _raise_exceptions_for_gated_repo=False,
   389|             _raise_exceptions_for_missing_entries=False,
   390|             _raise_exceptions_for_connection_errors=False,
   391|         )
   392|         tool_code = resolved_tool_file is not None
   393|         if resolved_tool_file is None:
   394|             resolved_tool_file = cached_file(
   395|                 repo_id,
   396|                 tool_file,
   397|                 token=token,
   398|                 **hub_kwargs,
   399|                 _raise_exceptions_for_gated_repo=False,
   400|                 _raise_exceptions_for_missing_entries=False,
   401|                 _raise_exceptions_for_connection_errors=False,
   402|             )
   403|         if resolved_tool_file is None:
   404|             raise EnvironmentError(
   405|                 f"{repo_id} does not appear to provide a valid configuration in `tool_config.json` or `config.json`."
   406|             )
   407|         with open(resolved_tool_file, encoding="utf-8") as reader:
   408|             tool_code = "".join(reader.readlines())
   409|         with tempfile.TemporaryDirectory() as temp_dir:
   410|             module_path = os.path.join(temp_dir, "tool.py")
   411|             with open(module_path, "w") as f:
   412|                 f.write(tool_code)
   413|             print("TOOL CODE:\n", tool_code)
   414|             spec = importlib.util.spec_from_file_location("tool", module_path)
   415|             module = importlib.util.module_from_spec(spec)
   416|             spec.loader.exec_module(module)
   417|             for item_name in dir(module):
   418|                 item = getattr(module, item_name)
   419|                 if isinstance(item, type) and issubclass(item, Tool) and item != Tool:
   420|                     tool_class = item
   421|                     break
   422|             if tool_class is None:
   423|                 raise ValueError("No Tool subclass found in the code.")
   424|         if not isinstance(tool_class.inputs, dict):
   425|             tool_class.inputs = ast.literal_eval(tool_class.inputs)
   426|         return tool_class(**kwargs)
   427|     @staticmethod
   428|     def from_space(

# --- HUNK 6: Lines 463-538 ---
   463|         ...     "face_swapper",
   464|         ...     "Tool that puts the face shown on the first image on the second image. You can give it paths to images.",
   465|         ... )
   466|         >>> image = face_swapper('./aymeric.jpeg', './ruth.jpg')
   467|         ```
   468|         """
   469|         from gradio_client import Client, handle_file
   470|         class SpaceToolWrapper(Tool):
   471|             skip_forward_signature_validation = True
   472|             def __init__(
   473|                 self,
   474|                 space_id: str,
   475|                 name: str,
   476|                 description: str,
   477|                 api_name: Optional[str] = None,
   478|                 token: Optional[str] = None,
   479|             ):
   480|                 self.name = name
   481|                 self.description = description
   482|                 self.client = Client(space_id, hf_token=token)
   483|                 space_description = self.client.view_api(
   484|                     return_format="dict", print_info=False
   485|                 )["named_endpoints"]
   486|                 if api_name is None:
   487|                     api_name = list(space_description.keys())[0]
   488|                     logger.warning(
   489|                         f"Since `api_name` was not defined, it was automatically set to the first available API: `{api_name}`."
   490|                     )
   491|                 self.api_name = api_name
   492|                 try:
   493|                     space_description_api = space_description[api_name]
   494|                 except KeyError:
   495|                     raise KeyError(
   496|                         f"Could not find specified {api_name=} among available api names."
   497|                     )
   498|                 self.inputs = {}
   499|                 for parameter in space_description_api["parameters"]:
   500|                     if not parameter["parameter_has_default"]:
   501|                         parameter_type = parameter["type"]["type"]
   502|                         if parameter_type == "object":
   503|                             parameter_type = "any"
   504|                         self.inputs[parameter["parameter_name"]] = {
   505|                             "type": parameter_type,
   506|                             "description": parameter["python_type"]["description"],
   507|                         }
   508|                 output_component = space_description_api["returns"][0]["component"]
   509|                 if output_component == "Image":
   510|                     self.output_type = "image"
   511|                 elif output_component == "Audio":
   512|                     self.output_type = "audio"
   513|                 else:
   514|                     self.output_type = "any"
   515|                 self.is_initialized = True
   516|             def sanitize_argument_for_prediction(self, arg):
   517|                 from gradio_client.utils import is_http_url_like
   518|                 if isinstance(arg, ImageType):
   519|                     temp_file = tempfile.NamedTemporaryFile(suffix=".png", delete=False)
   520|                     arg.save(temp_file.name)
   521|                     arg = temp_file.name
   522|                 if (
   523|                     (isinstance(arg, str) and os.path.isfile(arg))
   524|                     or (isinstance(arg, Path) and arg.exists() and arg.is_file())
   525|                     or is_http_url_like(arg)
   526|                 ):
   527|                     arg = handle_file(arg)
   528|                 return arg
   529|             def forward(self, *args, **kwargs):
   530|                 args = list(args)
   531|                 for i, arg in enumerate(args):
   532|                     args[i] = self.sanitize_argument_for_prediction(arg)
   533|                 for arg_name, arg in kwargs.items():
   534|                     kwargs[arg_name] = self.sanitize_argument_for_prediction(arg)
   535|                 output = self.client.predict(*args, api_name=self.api_name, **kwargs)
   536|                 if isinstance(output, tuple) or isinstance(output, list):
   537|                     return output[
   538|                         0

# --- HUNK 7: Lines 542-671 ---
   542|             space_id=space_id,
   543|             name=name,
   544|             description=description,
   545|             api_name=api_name,
   546|             token=token,
   547|         )
   548|     @staticmethod
   549|     def from_gradio(gradio_tool):
   550|         """
   551|         Creates a [`Tool`] from a gradio tool.
   552|         """
   553|         import inspect
   554|         class GradioToolWrapper(Tool):
   555|             def __init__(self, _gradio_tool):
   556|                 self.name = _gradio_tool.name
   557|                 self.description = _gradio_tool.description
   558|                 self.output_type = "string"
   559|                 self._gradio_tool = _gradio_tool
   560|                 func_args = list(inspect.signature(_gradio_tool.run).parameters.items())
   561|                 self.inputs = {
   562|                     key: {"type": CONVERSION_DICT[value.annotation], "description": ""}
   563|                     for key, value in func_args
   564|                 }
   565|                 self.forward = self._gradio_tool.run
   566|         return GradioToolWrapper(gradio_tool)
   567|     @staticmethod
   568|     def from_langchain(langchain_tool):
   569|         """
   570|         Creates a [`Tool`] from a langchain tool.
   571|         """
   572|         class LangChainToolWrapper(Tool):
   573|             def __init__(self, _langchain_tool):
   574|                 self.name = _langchain_tool.name.lower()
   575|                 self.description = _langchain_tool.description
   576|                 self.inputs = _langchain_tool.args.copy()
   577|                 for input_content in self.inputs.values():
   578|                     if "title" in input_content:
   579|                         input_content.pop("title")
   580|                     input_content["description"] = ""
   581|                 self.output_type = "string"
   582|                 self.langchain_tool = _langchain_tool
   583|             def forward(self, *args, **kwargs):
   584|                 tool_input = kwargs.copy()
   585|                 for index, argument in enumerate(args):
   586|                     if index < len(self.inputs):
   587|                         input_key = next(iter(self.inputs))
   588|                         tool_input[input_key] = argument
   589|                 return self.langchain_tool.run(tool_input)
   590|         return LangChainToolWrapper(langchain_tool)
   591| DEFAULT_TOOL_DESCRIPTION_TEMPLATE = """
   592| - {{ tool.name }}: {{ tool.description }}
   593|     Takes inputs: {{tool.inputs}}
   594|     Returns an output of type: {{tool.output_type}}
   595| """
   596| def get_tool_description_with_args(
   597|     tool: Tool, description_template: Optional[str] = None
   598| ) -> str:
   599|     if description_template is None:
   600|         description_template = DEFAULT_TOOL_DESCRIPTION_TEMPLATE
   601|     compiled_template = compile_jinja_template(description_template)
   602|     tool_description = compiled_template.render(
   603|         tool=tool,
   604|     )
   605|     return tool_description
   606| @lru_cache
   607| def compile_jinja_template(template):
   608|     try:
   609|         import jinja2
   610|         from jinja2.exceptions import TemplateError
   611|         from jinja2.sandbox import ImmutableSandboxedEnvironment
   612|     except ImportError:
   613|         raise ImportError("template requires jinja2 to be installed.")
   614|     if version.parse(jinja2.__version__) < version.parse("3.1.0"):
   615|         raise ImportError(
   616|             "template requires jinja2>=3.1.0 to be installed. Your version is "
   617|             f"{jinja2.__version__}."
   618|         )
   619|     def raise_exception(message):
   620|         raise TemplateError(message)
   621|     jinja_env = ImmutableSandboxedEnvironment(trim_blocks=True, lstrip_blocks=True)
   622|     jinja_env.globals["raise_exception"] = raise_exception
   623|     return jinja_env.from_string(template)
   624| def launch_gradio_demo(tool: Tool):
   625|     """
   626|     Launches a gradio demo for a tool. The corresponding tool class needs to properly implement the class attributes
   627|     `inputs` and `output_type`.
   628|     Args:
   629|         tool (`type`): The tool for which to launch the demo.
   630|     """
   631|     try:
   632|         import gradio as gr
   633|     except ImportError:
   634|         raise ImportError(
   635|             "Gradio should be installed in order to launch a gradio demo."
   636|         )
   637|     TYPE_TO_COMPONENT_CLASS_MAPPING = {
   638|         "image": gr.Image,
   639|         "audio": gr.Audio,
   640|         "string": gr.Textbox,
   641|         "integer": gr.Textbox,
   642|         "number": gr.Textbox,
   643|     }
   644|     def tool_forward(*args, **kwargs):
   645|         return tool(*args, sanitize_inputs_outputs=True, **kwargs)
   646|     tool_forward.__signature__ = inspect.signature(tool.forward)
   647|     gradio_inputs = []
   648|     for input_name, input_details in tool.inputs.items():
   649|         input_gradio_component_class = TYPE_TO_COMPONENT_CLASS_MAPPING[
   650|             input_details["type"]
   651|         ]
   652|         new_component = input_gradio_component_class(label=input_name)
   653|         gradio_inputs.append(new_component)
   654|     output_gradio_componentclass = TYPE_TO_COMPONENT_CLASS_MAPPING[tool.output_type]
   655|     gradio_output = output_gradio_componentclass(label="Output")
   656|     gr.Interface(
   657|         fn=tool_forward,
   658|         inputs=gradio_inputs,
   659|         outputs=gradio_output,
   660|         title=tool.name,
   661|         article=tool.description,
   662|         description=tool.description,
   663|         api_name=tool.name,
   664|     ).launch()
   665| def load_tool(
   666|     task_or_repo_id,
   667|     model_repo_id: Optional[str] = None,
   668|     token: Optional[str] = None,
   669|     trust_remote_code: bool = False,
   670|     **kwargs,
   671| ):

# --- HUNK 8: Lines 733-779 ---
   733|         """Loads a tool collection from the Hub.
   734|         it adds a collection of tools from all Spaces in the collection to the agent's toolbox
   735|         > [!NOTE]
   736|         > Only Spaces will be fetched, so you can feel free to add models and datasets to your collection if you'd
   737|         > like for this collection to showcase them.
   738|         Args:
   739|             collection_slug (str): The collection slug referencing the collection.
   740|             token (str, *optional*): The authentication token if the collection is private.
   741|             trust_remote_code (bool, *optional*, defaults to False): Whether to trust the remote code.
   742|         Returns:
   743|             ToolCollection: A tool collection instance loaded with the tools.
   744|         Example:
   745|         ```py
   746|         >>> from smolagents import ToolCollection, CodeAgent
   747|         >>> image_tool_collection = ToolCollection.from_hub("huggingface-tools/diffusion-tools-6630bb19a942c2306a2cdb6f")
   748|         >>> agent = CodeAgent(tools=[*image_tool_collection.tools], add_base_tools=True)
   749|         >>> agent.run("Please draw me a picture of rivers and lakes.")
   750|         ```
   751|         """
   752|         _collection = get_collection(collection_slug, token=token)
   753|         _hub_repo_ids = {
   754|             item.item_id for item in _collection.items if item.item_type == "space"
   755|         }
   756|         tools = {
   757|             Tool.from_hub(repo_id, token, trust_remote_code)
   758|             for repo_id in _hub_repo_ids
   759|         }
   760|         return cls(tools)
   761|     @classmethod
   762|     @contextmanager
   763|     def from_mcp(cls, server_parameters) -> "ToolCollection":
   764|         """Automatically load a tool collection from an MCP server.
   765|         Note: a separate thread will be spawned to run an asyncio event loop handling
   766|         the MCP server.
   767|         Args:
   768|             server_parameters (mcp.StdioServerParameters): The server parameters to use to
   769|             connect to the MCP server.
   770|         Returns:
   771|             ToolCollection: A tool collection instance.
   772|         Example:
   773|         ```py
   774|         >>> from smolagents import ToolCollection, CodeAgent
   775|         >>> from mcp import StdioServerParameters
   776|         >>> server_parameters = StdioServerParameters(
   777|         >>>     command="uv",
   778|         >>>     args=["--quiet", "pubmedmcp@0.1.3"],
   779|         >>>     env={"UV_PYTHON": "3.12", **os.environ},

# --- HUNK 9: Lines 782-974 ---
   782|         >>>     agent = CodeAgent(tools=[*tool_collection.tools], add_base_tools=True)
   783|         >>>     agent.run("Please find a remedy for hangover.")
   784|         ```
   785|         """
   786|         try:
   787|             from mcpadapt.core import MCPAdapt
   788|             from mcpadapt.smolagents_adapter import SmolAgentsAdapter
   789|         except ImportError:
   790|             raise ImportError(
   791|                 """Please install 'mcp' extra to use ToolCollection.from_mcp: `pip install "smolagents[mcp]"`."""
   792|             )
   793|         with MCPAdapt(server_parameters, SmolAgentsAdapter()) as tools:
   794|             yield cls(tools)
   795| def tool(tool_function: Callable) -> Tool:
   796|     """
   797|     Converts a function into an instance of a Tool subclass.
   798|     Args:
   799|         tool_function: Your function. Should have type hints for each input and a type hint for the output.
   800|         Should also have a docstring description including an 'Args:' part where each argument is described.
   801|     """
   802|     parameters = get_json_schema(tool_function)["function"]
   803|     if "return" not in parameters:
   804|         raise TypeHintParsingException(
   805|             "Tool return type not found: make sure your function has a return type hint!"
   806|         )
   807|     class SimpleTool(Tool):
   808|         def __init__(self, name, description, inputs, output_type, function):
   809|             self.name = name
   810|             self.description = description
   811|             self.inputs = inputs
   812|             self.output_type = output_type
   813|             self.forward = function
   814|             self.is_initialized = True
   815|     simple_tool = SimpleTool(
   816|         parameters["name"],
   817|         parameters["description"],
   818|         parameters["parameters"]["properties"],
   819|         parameters["return"]["type"],
   820|         function=tool_function,
   821|     )
   822|     original_signature = inspect.signature(tool_function)
   823|     new_parameters = [
   824|         inspect.Parameter("self", inspect.Parameter.POSITIONAL_ONLY)
   825|     ] + list(original_signature.parameters.values())
   826|     new_signature = original_signature.replace(parameters=new_parameters)
   827|     simple_tool.forward.__signature__ = new_signature
   828|     return simple_tool
   829| class PipelineTool(Tool):
   830|     """
   831|     A [`Tool`] tailored towards Transformer models. On top of the class attributes of the base class [`Tool`], you will
   832|     need to specify:
   833|     - **model_class** (`type`) -- The class to use to load the model in this tool.
   834|     - **default_checkpoint** (`str`) -- The default checkpoint that should be used when the user doesn't specify one.
   835|     - **pre_processor_class** (`type`, *optional*, defaults to [`AutoProcessor`]) -- The class to use to load the
   836|       pre-processor
   837|     - **post_processor_class** (`type`, *optional*, defaults to [`AutoProcessor`]) -- The class to use to load the
   838|       post-processor (when different from the pre-processor).
   839|     Args:
   840|         model (`str` or [`PreTrainedModel`], *optional*):
   841|             The name of the checkpoint to use for the model, or the instantiated model. If unset, will default to the
   842|             value of the class attribute `default_checkpoint`.
   843|         pre_processor (`str` or `Any`, *optional*):
   844|             The name of the checkpoint to use for the pre-processor, or the instantiated pre-processor (can be a
   845|             tokenizer, an image processor, a feature extractor or a processor). Will default to the value of `model` if
   846|             unset.
   847|         post_processor (`str` or `Any`, *optional*):
   848|             The name of the checkpoint to use for the post-processor, or the instantiated pre-processor (can be a
   849|             tokenizer, an image processor, a feature extractor or a processor). Will default to the `pre_processor` if
   850|             unset.
   851|         device (`int`, `str` or `torch.device`, *optional*):
   852|             The device on which to execute the model. Will default to any accelerator available (GPU, MPS etc...), the
   853|             CPU otherwise.
   854|         device_map (`str` or `dict`, *optional*):
   855|             If passed along, will be used to instantiate the model.
   856|         model_kwargs (`dict`, *optional*):
   857|             Any keyword argument to send to the model instantiation.
   858|         token (`str`, *optional*):
   859|             The token to use as HTTP bearer authorization for remote files. If unset, will use the token generated when
   860|             running `huggingface-cli login` (stored in `~/.huggingface`).
   861|         hub_kwargs (additional keyword arguments, *optional*):
   862|             Any additional keyword argument to send to the methods that will load the data from the Hub.
   863|     """
   864|     pre_processor_class = AutoProcessor
   865|     model_class = None
   866|     post_processor_class = AutoProcessor
   867|     default_checkpoint = None
   868|     description = "This is a pipeline tool"
   869|     name = "pipeline"
   870|     inputs = {"prompt": str}
   871|     output_type = str
   872|     skip_forward_signature_validation = True
   873|     def __init__(
   874|         self,
   875|         model=None,
   876|         pre_processor=None,
   877|         post_processor=None,
   878|         device=None,
   879|         device_map=None,
   880|         model_kwargs=None,
   881|         token=None,
   882|         **hub_kwargs,
   883|     ):
   884|         if not is_torch_available():
   885|             raise ImportError("Please install torch in order to use this tool.")
   886|         if not is_accelerate_available():
   887|             raise ImportError("Please install accelerate in order to use this tool.")
   888|         if model is None:
   889|             if self.default_checkpoint is None:
   890|                 raise ValueError(
   891|                     "This tool does not implement a default checkpoint, you need to pass one."
   892|                 )
   893|             model = self.default_checkpoint
   894|         if pre_processor is None:
   895|             pre_processor = model
   896|         self.model = model
   897|         self.pre_processor = pre_processor
   898|         self.post_processor = post_processor
   899|         self.device = device
   900|         self.device_map = device_map
   901|         self.model_kwargs = {} if model_kwargs is None else model_kwargs
   902|         if device_map is not None:
   903|             self.model_kwargs["device_map"] = device_map
   904|         self.hub_kwargs = hub_kwargs
   905|         self.hub_kwargs["token"] = token
   906|         super().__init__()
   907|     def setup(self):
   908|         """
   909|         Instantiates the `pre_processor`, `model` and `post_processor` if necessary.
   910|         """
   911|         if isinstance(self.pre_processor, str):
   912|             self.pre_processor = self.pre_processor_class.from_pretrained(
   913|                 self.pre_processor, **self.hub_kwargs
   914|             )
   915|         if isinstance(self.model, str):
   916|             self.model = self.model_class.from_pretrained(
   917|                 self.model, **self.model_kwargs, **self.hub_kwargs
   918|             )
   919|         if self.post_processor is None:
   920|             self.post_processor = self.pre_processor
   921|         elif isinstance(self.post_processor, str):
   922|             self.post_processor = self.post_processor_class.from_pretrained(
   923|                 self.post_processor, **self.hub_kwargs
   924|             )
   925|         if self.device is None:
   926|             if self.device_map is not None:
   927|                 self.device = list(self.model.hf_device_map.values())[0]
   928|             else:
   929|                 self.device = PartialState().default_device
   930|         if self.device_map is None:
   931|             self.model.to(self.device)
   932|         super().setup()
   933|     def encode(self, raw_inputs):
   934|         """
   935|         Uses the `pre_processor` to prepare the inputs for the `model`.
   936|         """
   937|         return self.pre_processor(raw_inputs)
   938|     def forward(self, inputs):
   939|         """
   940|         Sends the inputs through the `model`.
   941|         """
   942|         import torch
   943|         with torch.no_grad():
   944|             return self.model(**inputs)
   945|     def decode(self, outputs):
   946|         """
   947|         Uses the `post_processor` to decode the model output.
   948|         """
   949|         return self.post_processor(outputs)
   950|     def __call__(self, *args, **kwargs):
   951|         import torch
   952|         args, kwargs = handle_agent_input_types(*args, **kwargs)
   953|         if not self.is_initialized:
   954|             self.setup()
   955|         encoded_inputs = self.encode(*args, **kwargs)
   956|         tensor_inputs = {
   957|             k: v for k, v in encoded_inputs.items() if isinstance(v, torch.Tensor)
   958|         }
   959|         non_tensor_inputs = {
   960|             k: v for k, v in encoded_inputs.items() if not isinstance(v, torch.Tensor)
   961|         }
   962|         encoded_inputs = send_to_device(tensor_inputs, self.device)
   963|         outputs = self.forward({**encoded_inputs, **non_tensor_inputs})
   964|         outputs = send_to_device(outputs, "cpu")
   965|         decoded_outputs = self.decode(outputs)
   966|         return handle_agent_output_types(decoded_outputs, self.output_type)
   967| __all__ = [
   968|     "AUTHORIZED_TYPES",
   969|     "Tool",
   970|     "tool",
   971|     "load_tool",
   972|     "launch_gradio_demo",
   973|     "ToolCollection",
   974| ]


# ====================================================================
# FILE: src/smolagents/types.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 1-102 ---
     1| import importlib.util
     2| import logging
     3| import os
     4| import pathlib
     5| import tempfile
     6| import uuid
     7| from io import BytesIO
     8| import numpy as np
     9| import requests
    10| from transformers.utils import (
    11|     is_torch_available,
    12|     is_vision_available,
    13| )
    14| logger = logging.getLogger(__name__)
    15| if is_vision_available():
    16|     from PIL import Image
    17|     from PIL.Image import Image as ImageType
    18| else:
    19|     ImageType = object
    20| if is_torch_available():
    21|     import torch
    22|     from torch import Tensor
    23| else:
    24|     Tensor = object
    25| class AgentType:
    26|     """
    27|     Abstract class to be reimplemented to define types that can be returned by agents.
    28|     These objects serve three purposes:
    29|     - They behave as they were the type they're meant to be, e.g., a string for text, a PIL.Image for images
    30|     - They can be stringified: str(object) in order to return a string defining the object
    31|     - They should be displayed correctly in ipython notebooks/colab/jupyter
    32|     """
    33|     def __init__(self, value):
    34|         self._value = value
    35|     def __str__(self):
    36|         return self.to_string()
    37|     def to_raw(self):
    38|         logger.error(
    39|             "This is a raw AgentType of unknown type. Display in notebooks and string conversion will be unreliable"
    40|         )
    41|         return self._value
    42|     def to_string(self) -> str:
    43|         logger.error(
    44|             "This is a raw AgentType of unknown type. Display in notebooks and string conversion will be unreliable"
    45|         )
    46|         return str(self._value)
    47| class AgentText(AgentType, str):
    48|     """
    49|     Text type returned by the agent. Behaves as a string.
    50|     """
    51|     def to_raw(self):
    52|         return self._value
    53|     def to_string(self):
    54|         return str(self._value)
    55| class AgentImage(AgentType, ImageType):
    56|     """
    57|     Image type returned by the agent. Behaves as a PIL.Image.
    58|     """
    59|     def __init__(self, value):
    60|         AgentType.__init__(self, value)
    61|         ImageType.__init__(self)
    62|         if not is_vision_available():
    63|             raise ImportError("PIL must be installed in order to handle images.")
    64|         self._path = None
    65|         self._raw = None
    66|         self._tensor = None
    67|         if isinstance(value, AgentImage):
    68|             self._raw, self._path, self._tensor = value._raw, value._path, value._tensor
    69|         elif isinstance(value, ImageType):
    70|             self._raw = value
    71|         elif isinstance(value, bytes):
    72|             self._raw = Image.open(BytesIO(value))
    73|         elif isinstance(value, (str, pathlib.Path)):
    74|             self._path = value
    75|         elif isinstance(value, torch.Tensor):
    76|             self._tensor = value
    77|         elif isinstance(value, np.ndarray):
    78|             self._tensor = torch.from_numpy(value)
    79|         else:
    80|             raise TypeError(
    81|                 f"Unsupported type for {self.__class__.__name__}: {type(value)}"
    82|             )
    83|     def _ipython_display_(self, include=None, exclude=None):
    84|         """
    85|         Displays correctly this type in an ipython notebook (ipython, colab, jupyter, ...)
    86|         """
    87|         from IPython.display import Image, display
    88|         display(Image(self.to_string()))
    89|     def to_raw(self):
    90|         """
    91|         Returns the "raw" version of that object. In the case of an AgentImage, it is a PIL.Image.
    92|         """
    93|         if self._raw is not None:
    94|             return self._raw
    95|         if self._path is not None:
    96|             self._raw = Image.open(self._path)
    97|             return self._raw
    98|         if self._tensor is not None:
    99|             array = self._tensor.cpu().detach().numpy()
   100|             return Image.fromarray((255 - array * 255).astype(np.uint8))
   101|     def to_string(self):
   102|         """

# --- HUNK 2: Lines 115-216 ---
   115|             img = Image.fromarray((255 - array * 255).astype(np.uint8))
   116|             directory = tempfile.mkdtemp()
   117|             self._path = os.path.join(directory, str(uuid.uuid4()) + ".png")
   118|             img.save(self._path, format="png")
   119|             return self._path
   120|     def save(self, output_bytes, format: str = None, **params):
   121|         """
   122|         Saves the image to a file.
   123|         Args:
   124|             output_bytes (bytes): The output bytes to save the image to.
   125|             format (str): The format to use for the output image. The format is the same as in PIL.Image.save.
   126|             **params: Additional parameters to pass to PIL.Image.save.
   127|         """
   128|         img = self.to_raw()
   129|         img.save(output_bytes, format=format, **params)
   130| class AgentAudio(AgentType, str):
   131|     """
   132|     Audio type returned by the agent.
   133|     """
   134|     def __init__(self, value, samplerate=16_000):
   135|         if importlib.util.find_spec("soundfile") is None:
   136|             raise ModuleNotFoundError(
   137|                 "Please install 'audio' extra to use AgentAudio: `pip install 'smolagents[audio]'`"
   138|             )
   139|         super().__init__(value)
   140|         self._path = None
   141|         self._tensor = None
   142|         self.samplerate = samplerate
   143|         if isinstance(value, (str, pathlib.Path)):
   144|             self._path = value
   145|         elif is_torch_available() and isinstance(value, torch.Tensor):
   146|             self._tensor = value
   147|         elif isinstance(value, tuple):
   148|             self.samplerate = value[0]
   149|             if isinstance(value[1], np.ndarray):
   150|                 self._tensor = torch.from_numpy(value[1])
   151|             else:
   152|                 self._tensor = torch.tensor(value[1])
   153|         else:
   154|             raise ValueError(f"Unsupported audio type: {type(value)}")
   155|     def _ipython_display_(self, include=None, exclude=None):
   156|         """
   157|         Displays correctly this type in an ipython notebook (ipython, colab, jupyter, ...)
   158|         """
   159|         from IPython.display import Audio, display
   160|         display(Audio(self.to_string(), rate=self.samplerate))
   161|     def to_raw(self):
   162|         """
   163|         Returns the "raw" version of that object. It is a `torch.Tensor` object.
   164|         """
   165|         import soundfile as sf
   166|         if self._tensor is not None:
   167|             return self._tensor
   168|         if self._path is not None:
   169|             if "://" in str(self._path):
   170|                 response = requests.get(self._path)
   171|                 response.raise_for_status()
   172|                 tensor, self.samplerate = sf.read(BytesIO(response.content))
   173|             else:
   174|                 tensor, self.samplerate = sf.read(self._path)
   175|             self._tensor = torch.tensor(tensor)
   176|             return self._tensor
   177|     def to_string(self):
   178|         """
   179|         Returns the stringified version of that object. In the case of an AgentAudio, it is a path to the serialized
   180|         version of the audio.
   181|         """
   182|         import soundfile as sf
   183|         if self._path is not None:
   184|             return self._path
   185|         if self._tensor is not None:
   186|             directory = tempfile.mkdtemp()
   187|             self._path = os.path.join(directory, str(uuid.uuid4()) + ".wav")
   188|             sf.write(self._path, self._tensor, samplerate=self.samplerate)
   189|             return self._path
   190| AGENT_TYPE_MAPPING = {"string": AgentText, "image": AgentImage, "audio": AgentAudio}
   191| INSTANCE_TYPE_MAPPING = {
   192|     str: AgentText,
   193|     ImageType: AgentImage,
   194|     Tensor: AgentAudio,
   195| }
   196| if is_torch_available():
   197|     INSTANCE_TYPE_MAPPING[Tensor] = AgentAudio
   198| def handle_agent_input_types(*args, **kwargs):
   199|     args = [(arg.to_raw() if isinstance(arg, AgentType) else arg) for arg in args]
   200|     kwargs = {
   201|         k: (v.to_raw() if isinstance(v, AgentType) else v) for k, v in kwargs.items()
   202|     }
   203|     return args, kwargs
   204| def handle_agent_output_types(output, output_type=None):
   205|     if output_type in AGENT_TYPE_MAPPING:
   206|         decoded_outputs = AGENT_TYPE_MAPPING[output_type](output)
   207|         return decoded_outputs
   208|     else:
   209|         for _k, _v in INSTANCE_TYPE_MAPPING.items():
   210|             if isinstance(output, _k):
   211|                 if (
   212|                     _k is not object
   213|                 ):  # avoid converting to audio if torch is not installed
   214|                     return _v(output)
   215|         return output
   216| __all__ = ["AgentType", "AgentImage", "AgentText", "AgentAudio"]


# ====================================================================
# FILE: src/smolagents/utils.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-69 ---
     1| import ast
     2| import importlib.util
     3| import inspect
     4| import json
     5| import re
     6| import types
     7| from typing import Dict, Tuple, Union
     8| from rich.console import Console
     9| def is_pygments_available():
    10|     return importlib.util.find_spec("soundfile") is not None
    11| console = Console()
    12| BASE_BUILTIN_MODULES = [
    13|     "collections",
    14|     "datetime",
    15|     "itertools",
    16|     "math",
    17|     "queue",
    18|     "random",
    19|     "re",
    20|     "stat",
    21|     "statistics",
    22|     "time",
    23|     "unicodedata",
    24| ]
    25| class AgentError(Exception):
    26|     """Base class for other agent-related exceptions"""
    27|     def __init__(self, message):
    28|         super().__init__(message)
    29|         self.message = message
    30|         console.print(f"[bold red]{message}[/bold red]")
    31| class AgentParsingError(AgentError):
    32|     """Exception raised for errors in parsing in the agent"""
    33|     pass
    34| class AgentExecutionError(AgentError):
    35|     """Exception raised for errors in execution in the agent"""
    36|     pass
    37| class AgentMaxStepsError(AgentError):
    38|     """Exception raised for errors in execution in the agent"""
    39|     pass
    40| class AgentGenerationError(AgentError):
    41|     """Exception raised for errors in generation in the agent"""
    42|     pass
    43| def parse_json_blob(json_blob: str) -> Dict[str, str]:
    44|     try:
    45|         first_accolade_index = json_blob.find("{")
    46|         last_accolade_index = [a.start() for a in list(re.finditer("}", json_blob))][-1]
    47|         json_blob = json_blob[first_accolade_index : last_accolade_index + 1].replace(
    48|             '\\"', "'"
    49|         )
    50|         json_data = json.loads(json_blob, strict=False)
    51|         return json_data
    52|     except json.JSONDecodeError as e:
    53|         place = e.pos
    54|         if json_blob[place - 1 : place + 2] == "},\n":
    55|             raise ValueError(
    56|                 "JSON is invalid: you probably tried to provide multiple tool calls in one action. PROVIDE ONLY ONE TOOL CALL."
    57|             )
    58|         raise ValueError(
    59|             f"The JSON blob you used is invalid due to the following error: {e}.\n"
    60|             f"JSON blob was: {json_blob}, decoding failed on that specific part of the blob:\n"
    61|             f"'{json_blob[place - 4 : place + 5]}'."
    62|         )
    63|     except Exception as e:
    64|         raise ValueError(f"Error in parsing the JSON blob: {e}")
    65| def parse_code_blobs(code_blob: str) -> str:
    66|     """Parses the LLM's output to get any code blob inside. Will return the code directly if it's code."""
    67|     pattern = r"```(?:py|python)?\n(.*?)\n```"
    68|     matches = re.findall(pattern, code_blob, re.DOTALL)
    69|     if len(matches) == 0:

# --- HUNK 2: Lines 96-226 ---
    96|     tool_name_key, tool_arguments_key = None, None
    97|     for possible_tool_name_key in ["action", "tool_name", "tool", "name", "function"]:
    98|         if possible_tool_name_key in tool_call:
    99|             tool_name_key = possible_tool_name_key
   100|     for possible_tool_arguments_key in [
   101|         "action_input",
   102|         "tool_arguments",
   103|         "tool_args",
   104|         "parameters",
   105|     ]:
   106|         if possible_tool_arguments_key in tool_call:
   107|             tool_arguments_key = possible_tool_arguments_key
   108|     if tool_name_key is not None:
   109|         if tool_arguments_key is not None:
   110|             return tool_call[tool_name_key], tool_call[tool_arguments_key]
   111|         else:
   112|             return tool_call[tool_name_key], None
   113|     error_msg = "No tool name key found in tool call!" + f" Tool call: {json_blob}"
   114|     raise AgentParsingError(error_msg)
   115| MAX_LENGTH_TRUNCATE_CONTENT = 20000
   116| def truncate_content(
   117|     content: str, max_length: int = MAX_LENGTH_TRUNCATE_CONTENT
   118| ) -> str:
   119|     if len(content) <= max_length:
   120|         return content
   121|     else:
   122|         return (
   123|             content[: max_length // 2]
   124|             + f"\n..._This content has been truncated to stay below {max_length} characters_...\n"
   125|             + content[-max_length // 2 :]
   126|         )
   127| class ImportFinder(ast.NodeVisitor):
   128|     def __init__(self):
   129|         self.packages = set()
   130|     def visit_Import(self, node):
   131|         for alias in node.names:
   132|             base_package = alias.name.split(".")[0]
   133|             self.packages.add(base_package)
   134|     def visit_ImportFrom(self, node):
   135|         if node.module:  # for "from x import y" statements
   136|             base_package = node.module.split(".")[0]
   137|             self.packages.add(base_package)
   138| def get_method_source(method):
   139|     """Get source code for a method, including bound methods."""
   140|     if isinstance(method, types.MethodType):
   141|         method = method.__func__
   142|     return inspect.getsource(method).strip()
   143| def is_same_method(method1, method2):
   144|     """Compare two methods by their source code."""
   145|     try:
   146|         source1 = get_method_source(method1)
   147|         source2 = get_method_source(method2)
   148|         source1 = "\n".join(
   149|             line for line in source1.split("\n") if not line.strip().startswith("@")
   150|         )
   151|         source2 = "\n".join(
   152|             line for line in source2.split("\n") if not line.strip().startswith("@")
   153|         )
   154|         return source1 == source2
   155|     except (TypeError, OSError):
   156|         return False
   157| def is_same_item(item1, item2):
   158|     """Compare two class items (methods or attributes) for equality."""
   159|     if callable(item1) and callable(item2):
   160|         return is_same_method(item1, item2)
   161|     else:
   162|         return item1 == item2
   163| def instance_to_source(instance, base_cls=None):
   164|     """Convert an instance to its class source code representation."""
   165|     cls = instance.__class__
   166|     class_name = cls.__name__
   167|     class_lines = []
   168|     if base_cls:
   169|         class_lines.append(f"class {class_name}({base_cls.__name__}):")
   170|     else:
   171|         class_lines.append(f"class {class_name}:")
   172|     if cls.__doc__ and (not base_cls or cls.__doc__ != base_cls.__doc__):
   173|         class_lines.append(f'    """{cls.__doc__}"""')
   174|     class_attrs = {
   175|         name: value
   176|         for name, value in cls.__dict__.items()
   177|         if not name.startswith("__")
   178|         and not callable(value)
   179|         and not (
   180|             base_cls and hasattr(base_cls, name) and getattr(base_cls, name) == value
   181|         )
   182|     }
   183|     for name, value in class_attrs.items():
   184|         if isinstance(value, str):
   185|             if "\n" in value:
   186|                 class_lines.append(f'    {name} = """{value}"""')
   187|             else:
   188|                 class_lines.append(f'    {name} = "{value}"')
   189|         else:
   190|             class_lines.append(f"    {name} = {repr(value)}")
   191|     if class_attrs:
   192|         class_lines.append("")
   193|     methods = {
   194|         name: func
   195|         for name, func in cls.__dict__.items()
   196|         if callable(func)
   197|         and not (
   198|             base_cls
   199|             and hasattr(base_cls, name)
   200|             and getattr(base_cls, name).__code__.co_code == func.__code__.co_code
   201|         )
   202|     }
   203|     for name, method in methods.items():
   204|         method_source = inspect.getsource(method)
   205|         method_lines = method_source.split("\n")
   206|         first_line = method_lines[0]
   207|         indent = len(first_line) - len(first_line.lstrip())
   208|         method_lines = [line[indent:] for line in method_lines]
   209|         method_source = "\n".join(
   210|             ["    " + line if line.strip() else line for line in method_lines]
   211|         )
   212|         class_lines.append(method_source)
   213|         class_lines.append("")
   214|     import_finder = ImportFinder()
   215|     import_finder.visit(ast.parse("\n".join(class_lines)))
   216|     required_imports = import_finder.packages
   217|     final_lines = []
   218|     if base_cls:
   219|         final_lines.append(f"from {base_cls.__module__} import {base_cls.__name__}")
   220|     for package in required_imports:
   221|         final_lines.append(f"import {package}")
   222|     if final_lines:  # Add empty line after imports
   223|         final_lines.append("")
   224|     final_lines.extend(class_lines)
   225|     return "\n".join(final_lines)
   226| __all__ = ["AgentError"]

