--- a/contrib/cmdclient/console.py
+++ b/contrib/cmdclient/console.py
@@ -1,11 +1,12 @@
 """ Starts a synapse client console. """
+from __future__ import print_function
 import argparse
 import cmd
 import getpass
 import json
 import shlex
 import sys
 import time
 import urllib
 from http import TwistedHttpClient
 import nacl.encoding

--- a/contrib/cmdclient/http.py
+++ b/contrib/cmdclient/http.py
@@ -1,10 +1,11 @@
+from __future__ import print_function
 import json
 import urllib
 from pprint import pformat
 from twisted.internet import defer, reactor
 from twisted.web.client import Agent, readBody
 from twisted.web.http_headers import Headers
 class HttpClient:
     """ Interface for talking json over http
     """
     def put_json(self, url, data):

--- a/contrib/graph/graph.py
+++ b/contrib/graph/graph.py
@@ -1,10 +1,11 @@
+from __future__ import print_function
 import argparse
 import cgi
 import datetime
 import json
 import pydot
 import urllib2
 def make_name(pdu_id, origin):
     return "%s@%s" % (pdu_id, origin)
 def make_graph(pdus, room, filename_prefix):
     pdu_map = {}

--- a/contrib/graph/graph3.py
+++ b/contrib/graph/graph3.py
@@ -1,10 +1,11 @@
+from __future__ import print_function
 import argparse
 import cgi
 import datetime
 import pydot
 import simplejson as json
 from synapse.events import FrozenEvent
 from synapse.util.frozenutils import unfreeze
 def make_graph(file_name, room_id, file_prefix, limit):
     print("Reading lines")
     with open(file_name) as f:

--- a/contrib/jitsimeetbridge/jitsimeetbridge.py
+++ b/contrib/jitsimeetbridge/jitsimeetbridge.py
@@ -1,19 +1,20 @@
 """
 This is an attempt at bridging matrix clients into a Jitis meet room via Matrix
 video call.  It uses hard-coded xml strings overg XMPP BOSH. It can display one
 of the streams from the Jitsi bridge until the second lot of SDP comes down and
 we set the remote SDP at which point the stream ends. Our video never gets to
 the bridge.
 Requires:
 npm install jquery jsdom
 """
+from __future__ import print_function
 import json
 import subprocess
 import time
 import gevent
 import grequests
 from BeautifulSoup import BeautifulSoup
 ACCESS_TOKEN = ""
 MATRIXBASE = "https://matrix.org/_matrix/client/api/v1/"
 MYUSERNAME = "@davetest:matrix.org"
 HTTPBIND = "https://meet.jit.si/http-bind"

--- a/contrib/scripts/kick_users.py
+++ b/contrib/scripts/kick_users.py
@@ -1,15 +1,20 @@
+from __future__ import print_function
 import json
 import sys
 import urllib
 from argparse import ArgumentParser
 import requests
+try:
+    raw_input
+except NameError:  # Python 3
+    raw_input = input
 def _mkurl(template, kws):
     for key in kws:
         template = template.replace(key, kws[key])
     return template
 def main(hs, room_id, access_token, user_id_prefix, why):
     if not why:
         why = "Automated kick."
     print(
         "Kicking members on %s in room %s matching %s" % (hs, room_id, user_id_prefix)
     )
@@ -35,21 +40,21 @@
         if not event["content"].get("membership") == "join":
             continue
         if event["state_key"].startswith(user_id_prefix):
             kick_list.append(event["state_key"])
     if len(kick_list) == 0:
         print("No user IDs match the prefix '%s'" % user_id_prefix)
         return
     print("The following user IDs will be kicked from %s" % room_name)
     for uid in kick_list:
         print(uid)
-    doit = input("Continue? [Y]es\n")
+    doit = raw_input("Continue? [Y]es\n")
     if len(doit) > 0 and doit.lower() == "y":
         print("Kicking members...")
         kick_list = [urllib.quote(uid) for uid in kick_list]
         for uid in kick_list:
             kick_url = _mkurl(
                 "$HS/_matrix/client/api/v1/rooms/$ROOM/state/m.room.member/$UID?access_token=$TOKEN",
                 {"$HS": hs, "$UID": uid, "$ROOM": room_id, "$TOKEN": access_token},
             )
             kick_body = {"membership": "leave", "reason": why}
             print("Kicking %s" % uid)

--- a/scripts-dev/complement.sh
+++ b//dev/null
@@ -1,7 +0,0 @@
-cd "$(dirname $0)/.."
-docker build -t matrixdotorg/synapse:latest -f docker/Dockerfile .
-wget -N https://github.com/matrix-org/complement/archive/master.tar.gz
-tar -xzf master.tar.gz
-cd complement-master
-docker build -t complement-synapse -f dockerfiles/Synapse.Dockerfile ./dockerfiles
-COMPLEMENT_BASE_IMAGE=complement-synapse go test -v -count=1 ./tests

--- a/scripts-dev/definitions.py
+++ b/scripts-dev/definitions.py
@@ -1,19 +1,20 @@
+from __future__ import print_function
 import argparse
 import ast
 import os
 import re
 import sys
 import yaml
 class DefinitionVisitor(ast.NodeVisitor):
     def __init__(self):
-        super().__init__()
+        super(DefinitionVisitor, self).__init__()
         self.functions = {}
         self.classes = {}
         self.names = {}
         self.attrs = set()
         self.definitions = {
             "def": self.functions,
             "class": self.classes,
             "names": self.names,
             "attrs": self.attrs,
         }

--- a/scripts-dev/dump_macaroon.py
+++ b/scripts-dev/dump_macaroon.py
@@ -1,10 +1,11 @@
+from __future__ import print_function
 import sys
 import pymacaroons
 if len(sys.argv) == 1:
     sys.stderr.write("usage: %s macaroon [key]\n" % (sys.argv[0],))
     sys.exit(1)
 macaroon_string = sys.argv[1]
 key = sys.argv[2] if len(sys.argv) > 2 else None
 macaroon = pymacaroons.Macaroon.deserialize(macaroon_string)
 print(macaroon.inspect())
 print("")

--- a/scripts-dev/federation_client.py
+++ b/scripts-dev/federation_client.py
@@ -1,10 +1,11 @@
+from __future__ import print_function
 import argparse
 import base64
 import json
 import sys
 from typing import Any, Optional
 from urllib import parse as urlparse
 import nacl.signing
 import requests
 import signedjson.types
 import srvlookup
@@ -224,13 +225,13 @@
             print("Invalid response from %s: %s" % (uri, e), file=sys.stderr)
         return None
     def get_connection(self, url, proxies=None):
         parsed = urlparse.urlparse(url)
         (host, port) = self.lookup(parsed.netloc)
         netloc = "%s:%d" % (host, port)
         print("Connecting to %s" % (netloc,), file=sys.stderr)
         url = urlparse.urlunparse(
             ("https", netloc, parsed.path, parsed.params, parsed.query, parsed.fragment)
         )
-        return super().get_connection(url, proxies)
+        return super(MatrixConnectionAdapter, self).get_connection(url, proxies)
 if __name__ == "__main__":
     main()

--- a/scripts-dev/hash_history.py
+++ b/scripts-dev/hash_history.py
@@ -1,10 +1,11 @@
+from __future__ import print_function
 import sqlite3
 import sys
 from unpaddedbase64 import decode_base64, encode_base64
 from synapse.crypto.event_signing import (
     add_event_pdu_content_hash,
     compute_pdu_event_reference_hash,
 )
 from synapse.federation.units import Pdu
 from synapse.storage._base import SQLBaseStore
 from synapse.storage.pdu import PduStore

--- a/scripts/move_remote_media_to_new_store.py
+++ b/scripts/move_remote_media_to_new_store.py
@@ -2,20 +2,21 @@
 Moves a list of remote media from one media store to another.
 The input should be a list of media files to be moved, one per line. Each line
 should be formatted::
     <origin server>|<file id>
 This can be extracted from postgres with::
     psql --tuples-only -A -c "select media_origin, filesystem_id from
         matrix.remote_media_cache where ..."
 To use, pipe the above into::
     PYTHON_PATH=. ./scripts/move_remote_media_to_new_store.py <source repo> <dest repo>
 """
+from __future__ import print_function
 import argparse
 import logging
 import os
 import shutil
 import sys
 from synapse.rest.media.v1.filepath import MediaFilePaths
 logger = logging.getLogger()
 def main(src_repo, dest_repo):
     src_paths = MediaFilePaths(src_repo)
     dest_paths = MediaFilePaths(dest_repo)

--- a/setup.py
+++ b/setup.py
@@ -27,27 +27,20 @@
     code = read_file(path_segments)
     exec(code, result)
     return result
 version = exec_file(("synapse", "__init__.py"))["__version__"]
 dependencies = exec_file(("synapse", "python_dependencies.py"))
 long_description = read_file(("README.rst",))
 REQUIREMENTS = dependencies["REQUIREMENTS"]
 CONDITIONAL_REQUIREMENTS = dependencies["CONDITIONAL_REQUIREMENTS"]
 ALL_OPTIONAL_REQUIREMENTS = dependencies["ALL_OPTIONAL_REQUIREMENTS"]
 CONDITIONAL_REQUIREMENTS["all"] = list(ALL_OPTIONAL_REQUIREMENTS)
-CONDITIONAL_REQUIREMENTS["lint"] = [
-    "isort==5.0.3",
-    "black==19.10b0",
-    "flake8-comprehensions",
-    "flake8",
-]
-CONDITIONAL_REQUIREMENTS["test"] = ["mock>=2.0", "parameterized>=0.7.0"]
 setup(
     name="matrix-synapse",
     version=version,
     packages=find_packages(exclude=["tests", "tests.*"]),
     description="Reference homeserver for the Matrix decentralised comms protocol",
     install_requires=REQUIREMENTS,
     extras_require=CONDITIONAL_REQUIREMENTS,
     include_package_data=True,
     zip_safe=False,
     long_description=long_description,

--- a/synapse/__init__.py
+++ b/synapse/__init__.py
@@ -13,14 +13,14 @@
     protocol.Factory.noisy = False
     Factory.noisy = False
     DNSDatagramProtocol.noisy = False
 except ImportError:
     pass
 try:
     from canonicaljson import set_json_library
     set_json_library(json)
 except ImportError:
     pass
-__version__ = "1.21.0"
+__version__ = "1.20.1"
 if bool(os.environ.get("SYNAPSE_TEST_PATCH_LOG_CONTEXTS", False)):
     from synapse.util.patch_inline_callbacks import do_patch
     do_patch()

--- a/synapse/_scripts/register_new_matrix_user.py
+++ b/synapse/_scripts/register_new_matrix_user.py
@@ -1,10 +1,11 @@
+from __future__ import print_function
 import argparse
 import getpass
 import hashlib
 import hmac
 import logging
 import sys
 import requests as _requests
 import yaml
 def request_registration(
     user,

--- a/synapse/api/auth.py
+++ b/synapse/api/auth.py
@@ -154,21 +154,25 @@
                 return synapse.types.create_requester(user_id, app_service=app_service)
             user_info = await self.get_user_by_access_token(
                 access_token, rights, allow_expired=allow_expired
             )
             user = user_info["user"]
             token_id = user_info["token_id"]
             is_guest = user_info["is_guest"]
             shadow_banned = user_info["shadow_banned"]
             if self._account_validity.enabled and not allow_expired:
                 user_id = user.to_string()
-                if await self.store.is_account_expired(user_id, self.clock.time_msec()):
+                expiration_ts = await self.store.get_expiration_ts_for_user(user_id)
+                if (
+                    expiration_ts is not None
+                    and self.clock.time_msec() >= expiration_ts
+                ):
                     raise AuthError(
                         403, "User account has expired", errcode=Codes.EXPIRED_ACCOUNT
                     )
             device_id = user_info.get("device_id")
             if user and access_token and ip_addr:
                 await self.store.insert_client_ip(
                     user_id=user.to_string(),
                     access_token=access_token,
                     ip=ip_addr,
                     user_agent=user_agent,

--- a/synapse/api/errors.py
+++ b/synapse/api/errors.py
@@ -53,21 +53,21 @@
     INVALID_SIGNATURE = "M_INVALID_SIGNATURE"
     USER_DEACTIVATED = "M_USER_DEACTIVATED"
     BAD_ALIAS = "M_BAD_ALIAS"
 class CodeMessageException(RuntimeError):
     """An exception with integer code and message string attributes.
     Attributes:
         code: HTTP error code
         msg: string describing the error
     """
     def __init__(self, code: Union[int, HTTPStatus], msg: str):
-        super().__init__("%d: %s" % (code, msg))
+        super(CodeMessageException, self).__init__("%d: %s" % (code, msg))
         self.code = int(code)
         self.msg = msg
 class RedirectException(CodeMessageException):
     """A pseudo-error indicating that we want to redirect the client to a different
     location
     Attributes:
         cookies: a list of set-cookies values to add to the response. For example:
            b"sessionId=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT"
     """
     def __init__(self, location: bytes, http_code: int = http.FOUND):
@@ -86,124 +86,126 @@
     Attributes:
         errcode: Matrix error code e.g 'M_FORBIDDEN'
     """
     def __init__(self, code: int, msg: str, errcode: str = Codes.UNKNOWN):
         """Constructs a synapse error.
         Args:
             code: The integer error code (an HTTP response code)
             msg: The human-readable error message.
             errcode: The matrix error code e.g 'M_FORBIDDEN'
         """
-        super().__init__(code, msg)
+        super(SynapseError, self).__init__(code, msg)
         self.errcode = errcode
     def error_dict(self):
         return cs_error(self.msg, self.errcode)
 class ProxiedRequestError(SynapseError):
     """An error from a general matrix endpoint, eg. from a proxied Matrix API call.
     Attributes:
         errcode: Matrix error code e.g 'M_FORBIDDEN'
     """
     def __init__(
         self,
         code: int,
         msg: str,
         errcode: str = Codes.UNKNOWN,
         additional_fields: Optional[Dict] = None,
     ):
-        super().__init__(code, msg, errcode)
+        super(ProxiedRequestError, self).__init__(code, msg, errcode)
         if additional_fields is None:
             self._additional_fields = {}  # type: Dict
         else:
             self._additional_fields = dict(additional_fields)
     def error_dict(self):
         return cs_error(self.msg, self.errcode, **self._additional_fields)
 class ConsentNotGivenError(SynapseError):
     """The error returned to the client when the user has not consented to the
     privacy policy.
     """
     def __init__(self, msg: str, consent_uri: str):
         """Constructs a ConsentNotGivenError
         Args:
             msg: The human-readable error message
             consent_url: The URL where the user can give their consent
         """
-        super().__init__(
+        super(ConsentNotGivenError, self).__init__(
             code=HTTPStatus.FORBIDDEN, msg=msg, errcode=Codes.CONSENT_NOT_GIVEN
         )
         self._consent_uri = consent_uri
     def error_dict(self):
         return cs_error(self.msg, self.errcode, consent_uri=self._consent_uri)
 class UserDeactivatedError(SynapseError):
     """The error returned to the client when the user attempted to access an
     authenticated endpoint, but the account has been deactivated.
     """
     def __init__(self, msg: str):
         """Constructs a UserDeactivatedError
         Args:
             msg: The human-readable error message
         """
-        super().__init__(
+        super(UserDeactivatedError, self).__init__(
             code=HTTPStatus.FORBIDDEN, msg=msg, errcode=Codes.USER_DEACTIVATED
         )
 class FederationDeniedError(SynapseError):
     """An error raised when the server tries to federate with a server which
     is not on its federation whitelist.
     Attributes:
         destination: The destination which has been denied
     """
     def __init__(self, destination: Optional[str]):
         """Raised by federation client or server to indicate that we are
         are deliberately not attempting to contact a given server because it is
         not on our federation whitelist.
         Args:
             destination: the domain in question
         """
         self.destination = destination
-        super().__init__(
+        super(FederationDeniedError, self).__init__(
             code=403,
             msg="Federation denied with %s." % (self.destination,),
             errcode=Codes.FORBIDDEN,
         )
 class InteractiveAuthIncompleteError(Exception):
     """An error raised when UI auth is not yet complete
     (This indicates we should return a 401 with 'result' as the body)
     Attributes:
         session_id: The ID of the ongoing interactive auth session.
         result: the server response to the request, which should be
             passed back to the client
     """
     def __init__(self, session_id: str, result: "JsonDict"):
-        super().__init__("Interactive auth not yet complete")
+        super(InteractiveAuthIncompleteError, self).__init__(
+            "Interactive auth not yet complete"
+        )
         self.session_id = session_id
         self.result = result
 class UnrecognizedRequestError(SynapseError):
     """An error indicating we don't understand the request you're trying to make"""
     def __init__(self, *args, **kwargs):
         if "errcode" not in kwargs:
             kwargs["errcode"] = Codes.UNRECOGNIZED
         if len(args) == 0:
             message = "Unrecognized request"
         else:
             message = args[0]
-        super().__init__(400, message, **kwargs)
+        super(UnrecognizedRequestError, self).__init__(400, message, **kwargs)
 class NotFoundError(SynapseError):
     """An error indicating we can't find the thing you asked for"""
     def __init__(self, msg: str = "Not found", errcode: str = Codes.NOT_FOUND):
-        super().__init__(404, msg, errcode=errcode)
+        super(NotFoundError, self).__init__(404, msg, errcode=errcode)
 class AuthError(SynapseError):
     """An error raised when there was a problem authorising an event, and at various
     other poorly-defined times.
     """
     def __init__(self, *args, **kwargs):
         if "errcode" not in kwargs:
             kwargs["errcode"] = Codes.FORBIDDEN
-        super().__init__(*args, **kwargs)
+        super(AuthError, self).__init__(*args, **kwargs)
 class InvalidClientCredentialsError(SynapseError):
     """An error raised when there was a problem with the authorisation credentials
     in a client request.
     https://matrix.org/docs/spec/client_server/r0.5.0#using-access-tokens:
     When credentials are required but missing or invalid, the HTTP call will
     return with a status of 401 and the error code, M_MISSING_TOKEN or
     M_UNKNOWN_TOKEN respectively.
     """
     def __init__(self, msg: str, errcode: str):
         super().__init__(code=401, msg=msg, errcode=errcode)
@@ -230,130 +232,132 @@
     def __init__(
         self,
         code: int,
         msg: str,
         errcode: str = Codes.RESOURCE_LIMIT_EXCEEDED,
         admin_contact: Optional[str] = None,
         limit_type: Optional[str] = None,
     ):
         self.admin_contact = admin_contact
         self.limit_type = limit_type
-        super().__init__(code, msg, errcode=errcode)
+        super(ResourceLimitError, self).__init__(code, msg, errcode=errcode)
     def error_dict(self):
         return cs_error(
             self.msg,
             self.errcode,
             admin_contact=self.admin_contact,
             limit_type=self.limit_type,
         )
 class EventSizeError(SynapseError):
     """An error raised when an event is too big."""
     def __init__(self, *args, **kwargs):
         if "errcode" not in kwargs:
             kwargs["errcode"] = Codes.TOO_LARGE
-        super().__init__(413, *args, **kwargs)
+        super(EventSizeError, self).__init__(413, *args, **kwargs)
 class EventStreamError(SynapseError):
     """An error raised when there a problem with the event stream."""
     def __init__(self, *args, **kwargs):
         if "errcode" not in kwargs:
             kwargs["errcode"] = Codes.BAD_PAGINATION
-        super().__init__(*args, **kwargs)
+        super(EventStreamError, self).__init__(*args, **kwargs)
 class LoginError(SynapseError):
     """An error raised when there was a problem logging in."""
     pass
 class StoreError(SynapseError):
     """An error raised when there was a problem storing some data."""
     pass
 class InvalidCaptchaError(SynapseError):
     def __init__(
         self,
         code: int = 400,
         msg: str = "Invalid captcha.",
         error_url: Optional[str] = None,
         errcode: str = Codes.CAPTCHA_INVALID,
     ):
-        super().__init__(code, msg, errcode)
+        super(InvalidCaptchaError, self).__init__(code, msg, errcode)
         self.error_url = error_url
     def error_dict(self):
         return cs_error(self.msg, self.errcode, error_url=self.error_url)
 class LimitExceededError(SynapseError):
     """A client has sent too many requests and is being throttled.
     """
     def __init__(
         self,
         code: int = 429,
         msg: str = "Too Many Requests",
         retry_after_ms: Optional[int] = None,
         errcode: str = Codes.LIMIT_EXCEEDED,
     ):
-        super().__init__(code, msg, errcode)
+        super(LimitExceededError, self).__init__(code, msg, errcode)
         self.retry_after_ms = retry_after_ms
     def error_dict(self):
         return cs_error(self.msg, self.errcode, retry_after_ms=self.retry_after_ms)
 class RoomKeysVersionError(SynapseError):
     """A client has tried to upload to a non-current version of the room_keys store
     """
     def __init__(self, current_version: str):
         """
         Args:
             current_version: the current version of the store they should have used
         """
-        super().__init__(403, "Wrong room_keys version", Codes.WRONG_ROOM_KEYS_VERSION)
+        super(RoomKeysVersionError, self).__init__(
+            403, "Wrong room_keys version", Codes.WRONG_ROOM_KEYS_VERSION
+        )
         self.current_version = current_version
 class UnsupportedRoomVersionError(SynapseError):
     """The client's request to create a room used a room version that the server does
     not support."""
     def __init__(self, msg: str = "Homeserver does not support this room version"):
-        super().__init__(
+        super(UnsupportedRoomVersionError, self).__init__(
             code=400, msg=msg, errcode=Codes.UNSUPPORTED_ROOM_VERSION,
         )
 class ThreepidValidationError(SynapseError):
     """An error raised when there was a problem authorising an event."""
     def __init__(self, *args, **kwargs):
         if "errcode" not in kwargs:
             kwargs["errcode"] = Codes.FORBIDDEN
-        super().__init__(*args, **kwargs)
+        super(ThreepidValidationError, self).__init__(*args, **kwargs)
 class IncompatibleRoomVersionError(SynapseError):
     """A server is trying to join a room whose version it does not support.
     Unlike UnsupportedRoomVersionError, it is specific to the case of the make_join
     failing.
     """
     def __init__(self, room_version: str):
-        super().__init__(
+        super(IncompatibleRoomVersionError, self).__init__(
             code=400,
             msg="Your homeserver does not support the features required to "
             "join this room",
             errcode=Codes.INCOMPATIBLE_ROOM_VERSION,
         )
         self._room_version = room_version
     def error_dict(self):
         return cs_error(self.msg, self.errcode, room_version=self._room_version)
 class PasswordRefusedError(SynapseError):
     """A password has been refused, either during password reset/change or registration.
     """
     def __init__(
         self,
         msg: str = "This password doesn't comply with the server's policy",
         errcode: str = Codes.WEAK_PASSWORD,
     ):
-        super().__init__(
+        super(PasswordRefusedError, self).__init__(
             code=400, msg=msg, errcode=errcode,
         )
 class RequestSendFailed(RuntimeError):
     """Sending a HTTP request over federation failed due to not being able to
     talk to the remote server for some reason.
     This exception is used to differentiate "expected" errors that arise due to
     networking (e.g. DNS failures, connection timeouts etc), versus unexpected
     errors (like programming errors).
     """
     def __init__(self, inner_exception, can_retry):
-        super().__init__(
+        super(RequestSendFailed, self).__init__(
             "Failed to send request: %s: %s"
             % (type(inner_exception).__name__, inner_exception)
         )
         self.inner_exception = inner_exception
         self.can_retry = can_retry
 def cs_error(msg: str, code: str = Codes.UNKNOWN, **kwargs):
     """ Utility method for constructing an error response for client-server
     interactions.
     Args:
         msg: The error message.
@@ -385,21 +389,21 @@
         source: Optional[str] = None,
     ):
         if level not in ["FATAL", "ERROR", "WARN"]:
             raise ValueError("Level is not valid: %s" % (level,))
         self.level = level
         self.code = code
         self.reason = reason
         self.affected = affected
         self.source = source
         msg = "%s %s: %s" % (level, code, reason)
-        super().__init__(msg)
+        super(FederationError, self).__init__(msg)
     def get_dict(self):
         return {
             "level": self.level,
             "code": self.code,
             "reason": self.reason,
             "affected": self.affected,
             "source": self.source if self.source else self.affected,
         }
 class HttpResponseException(CodeMessageException):
     """
@@ -407,21 +411,21 @@
     Attributes:
         response: body of response
     """
     def __init__(self, code: int, msg: str, response: bytes):
         """
         Args:
             code: HTTP status code
             msg: reason phrase from HTTP response status line
             response: body of response
         """
-        super().__init__(code, msg)
+        super(HttpResponseException, self).__init__(code, msg)
         self.response = response
     def to_synapse_error(self):
         """Make a SynapseError based on an HTTPResponseException
         This is useful when a proxied request has failed, and we need to
         decide how to map the failure onto a matrix error to send back to the
         client.
         An attempt is made to parse the body of the http response as a matrix
         error. If that succeeds, the errcode and error message from the body
         are used as the errcode and error message in the new synapse error.
         Otherwise, the errcode is set to M_UNKNOWN, and the error message is

--- a/synapse/api/filtering.py
+++ b/synapse/api/filtering.py
@@ -1,13 +1,13 @@
-import json
 from typing import List
 import jsonschema
+from canonicaljson import json
 from jsonschema import FormatChecker
 from synapse.api.constants import EventContentFields
 from synapse.api.errors import SynapseError
 from synapse.api.presence import UserPresenceState
 from synapse.types import RoomID, UserID
 FILTER_SCHEMA = {
     "additionalProperties": False,
     "type": "object",
     "properties": {
         "limit": {"type": "number"},
@@ -83,21 +83,21 @@
     "additionalProperties": False,
 }
 @FormatChecker.cls_checks("matrix_room_id")
 def matrix_room_id_validator(room_id_str):
     return RoomID.from_string(room_id_str)
 @FormatChecker.cls_checks("matrix_user_id")
 def matrix_user_id_validator(user_id_str):
     return UserID.from_string(user_id_str)
 class Filtering:
     def __init__(self, hs):
-        super().__init__()
+        super(Filtering, self).__init__()
         self.store = hs.get_datastore()
     async def get_user_filter(self, user_localpart, filter_id):
         result = await self.store.get_user_filter(user_localpart, filter_id)
         return FilterCollection(result)
     def add_user_filter(self, user_localpart, user_filter):
         self.check_valid_filter(user_filter)
         return self.store.add_user_filter(user_localpart, user_filter)
     def check_valid_filter(self, user_filter_json):
         """Check if the provided filter is valid.
         This inspects all definitions contained within the filter.

--- a/synapse/api/urls.py
+++ b/synapse/api/urls.py
@@ -1,16 +1,15 @@
 """Contains the URL paths to prefix various aspects of the server with. """
 import hmac
 from hashlib import sha256
 from urllib.parse import urlencode
 from synapse.config import ConfigError
-SYNAPSE_CLIENT_API_PREFIX = "/_synapse/client"
 CLIENT_API_PREFIX = "/_matrix/client"
 FEDERATION_PREFIX = "/_matrix/federation"
 FEDERATION_V1_PREFIX = FEDERATION_PREFIX + "/v1"
 FEDERATION_V2_PREFIX = FEDERATION_PREFIX + "/v2"
 FEDERATION_UNSTABLE_PREFIX = FEDERATION_PREFIX + "/unstable"
 STATIC_PREFIX = "/_matrix/static"
 WEB_CLIENT_PREFIX = "/_matrix/client"
 SERVER_KEY_V2_PREFIX = "/_matrix/key/v2"
 MEDIA_PREFIX = "/_matrix/media/r0"
 LEGACY_MEDIA_PREFIX = "/_matrix/media/v1"

--- a/synapse/app/admin_cmd.py
+++ b/synapse/app/admin_cmd.py
@@ -1,16 +1,16 @@
 import argparse
-import json
 import logging
 import os
 import sys
 import tempfile
+from canonicaljson import json
 from twisted.internet import defer, task
 import synapse
 from synapse.app import _base
 from synapse.config._base import ConfigError
 from synapse.config.homeserver import HomeServerConfig
 from synapse.config.logger import setup_logging
 from synapse.handlers.admin import ExfiltrationWriter
 from synapse.replication.slave.storage._base import BaseSlavedStore
 from synapse.replication.slave.storage.account_data import SlavedAccountDataStore
 from synapse.replication.slave.storage.appservice import SlavedApplicationServiceStore

--- a/synapse/app/generic_worker.py
+++ b/synapse/app/generic_worker.py
@@ -121,39 +121,39 @@
 from synapse.util.httpresourcetree import create_resource_tree
 from synapse.util.manhole import manhole
 from synapse.util.versionstring import get_version_string
 logger = logging.getLogger("synapse.app.generic_worker")
 class PresenceStatusStubServlet(RestServlet):
     """If presence is disabled this servlet can be used to stub out setting
     presence status.
     """
     PATTERNS = client_patterns("/presence/(?P<user_id>[^/]*)/status")
     def __init__(self, hs):
-        super().__init__()
+        super(PresenceStatusStubServlet, self).__init__()
         self.auth = hs.get_auth()
     async def on_GET(self, request, user_id):
         await self.auth.get_user_by_req(request)
         return 200, {"presence": "offline"}
     async def on_PUT(self, request, user_id):
         await self.auth.get_user_by_req(request)
         return 200, {}
 class KeyUploadServlet(RestServlet):
     """An implementation of the `KeyUploadServlet` that responds to read only
     requests, but otherwise proxies through to the master instance.
     """
     PATTERNS = client_patterns("/keys/upload(/(?P<device_id>[^/]+))?$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(KeyUploadServlet, self).__init__()
         self.auth = hs.get_auth()
         self.store = hs.get_datastore()
         self.http_client = hs.get_simple_http_client()
         self.main_uri = hs.config.worker_main_http_uri
     async def on_POST(self, request, device_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         user_id = requester.user.to_string()
         body = parse_json_object_from_request(request)
         if device_id is not None:
             if requester.device_id is not None and device_id != requester.device_id:
@@ -496,21 +496,21 @@
     async def remove_pusher(self, app_id, push_key, user_id):
         self.get_tcp_replication().send_remove_pusher(app_id, push_key, user_id)
     @cache_in_self
     def get_replication_data_handler(self):
         return GenericWorkerReplicationHandler(self)
     @cache_in_self
     def get_presence_handler(self):
         return GenericWorkerPresence(self)
 class GenericWorkerReplicationHandler(ReplicationDataHandler):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(GenericWorkerReplicationHandler, self).__init__(hs)
         self.store = hs.get_datastore()
         self.presence_handler = hs.get_presence_handler()  # type: GenericWorkerPresence
         self.notifier = hs.get_notifier()
         self.notify_pushers = hs.config.start_pushers
         self.pusher_pool = hs.get_pusherpool()
         self.send_handler = None  # type: Optional[FederationSenderHandler]
         if hs.config.send_federation:
             self.send_handler = FederationSenderHandler(hs)
     async def on_rdata(self, stream_name, instance_name, token, rows):
         await super().on_rdata(stream_name, instance_name, token, rows)

--- a/synapse/app/homeserver.py
+++ b/synapse/app/homeserver.py
@@ -1,10 +1,11 @@
+from __future__ import print_function
 import gc
 import logging
 import math
 import os
 import resource
 import sys
 from typing import Iterable
 from prometheus_client import Gauge
 from twisted.application import service
 from twisted.internet import defer, reactor
@@ -19,21 +20,20 @@
     FEDERATION_PREFIX,
     LEGACY_MEDIA_PREFIX,
     MEDIA_PREFIX,
     SERVER_KEY_V2_PREFIX,
     STATIC_PREFIX,
     WEB_CLIENT_PREFIX,
 )
 from synapse.app import _base
 from synapse.app._base import listen_ssl, listen_tcp, quit_with_error
 from synapse.config._base import ConfigError
-from synapse.config.emailconfig import ThreepidBehaviour
 from synapse.config.homeserver import HomeServerConfig
 from synapse.config.server import ListenerConfig
 from synapse.federation.transport.server import TransportLayerServer
 from synapse.http.additional_resource import AdditionalResource
 from synapse.http.server import (
     OptionsResource,
     RootOptionsRedirectResource,
     RootRedirect,
     StaticResource,
 )
@@ -154,27 +154,20 @@
                     "/.well-known/matrix/client": WellKnownResource(self),
                     "/_synapse/admin": AdminRestResource(self),
                 }
             )
             if self.get_config().oidc_enabled:
                 from synapse.rest.oidc import OIDCResource
                 resources["/_synapse/oidc"] = OIDCResource(self)
             if self.get_config().saml2_enabled:
                 from synapse.rest.saml2 import SAML2Resource
                 resources["/_matrix/saml2"] = SAML2Resource(self)
-            if self.get_config().threepid_behaviour_email == ThreepidBehaviour.LOCAL:
-                from synapse.rest.synapse.client.password_reset import (
-                    PasswordResetSubmitTokenResource,
-                )
-                resources[
-                    "/_synapse/client/password_reset/email/submit_token"
-                ] = PasswordResetSubmitTokenResource(self)
         if name == "consent":
             from synapse.rest.consent.consent_resource import ConsentResource
             consent_resource = ConsentResource(self)
             if compress:
                 consent_resource = gz_wrap(consent_resource)
             resources.update({"/_matrix/consent": consent_resource})
         if name == "federation":
             resources.update({FEDERATION_PREFIX: TransportLayerServer(self)})
         if name == "openid":
             resources.update(

--- a/synapse/appservice/api.py
+++ b/synapse/appservice/api.py
@@ -47,21 +47,21 @@
         return False
     for k in fields.keys():
         if not isinstance(fields[k], str):
             return False
     return True
 class ApplicationServiceApi(SimpleHttpClient):
     """This class manages HS -> AS communications, including querying and
     pushing.
     """
     def __init__(self, hs):
-        super().__init__(hs)
+        super(ApplicationServiceApi, self).__init__(hs)
         self.clock = hs.get_clock()
         self.protocol_meta_cache = ResponseCache(
             hs, "as_protocol_meta", timeout_ms=HOUR_IN_MS
         )
     async def query_user(self, service, user_id):
         if service.url is None:
             return False
         uri = service.url + ("/users/%s" % urllib.parse.quote(user_id))
         try:
             response = await self.get_json(uri, {"access_token": service.hs_token})
@@ -128,21 +128,21 @@
     ) -> Optional[JsonDict]:
         if service.url is None:
             return {}
         async def _get() -> Optional[JsonDict]:
             uri = "%s%s/thirdparty/protocol/%s" % (
                 service.url,
                 APP_SERVICE_PREFIX,
                 urllib.parse.quote(protocol),
             )
             try:
-                info = await self.get_json(uri)
+                info = await self.get_json(uri, {})
                 if not _is_valid_3pe_metadata(info):
                     logger.warning(
                         "query_3pe_protocol to %s did not return a valid result", uri
                     )
                     return None
                 for instance in info.get("instances", []):
                     network_id = instance.get("network_id", None)
                     if network_id is not None:
                         instance["instance_id"] = ThirdPartyInstanceID(
                             service.id, network_id

--- a/synapse/config/_base.py
+++ b/synapse/config/_base.py
@@ -160,25 +160,26 @@
         search_directories = [self.default_template_dir]
         if custom_template_directory:
             if not self.path_exists(custom_template_directory):
                 raise ConfigError(
                     "Configured template directory does not exist: %s"
                     % (custom_template_directory,)
                 )
             search_directories.insert(0, custom_template_directory)
         loader = jinja2.FileSystemLoader(search_directories)
         env = jinja2.Environment(loader=loader, autoescape=autoescape)
-        env.filters.update({"format_ts": _format_ts_filter})
-        if self.public_baseurl:
-            env.filters.update(
-                {"mxc_to_http": _create_mxc_to_http_filter(self.public_baseurl)}
-            )
+        env.filters.update(
+            {
+                "format_ts": _format_ts_filter,
+                "mxc_to_http": _create_mxc_to_http_filter(self.public_baseurl),
+            }
+        )
         for filename in filenames:
             template = env.get_template(filename)
             templates.append(template)
         return templates
 def _format_ts_filter(value: int, format: str):
     return time.strftime(format, time.localtime(value / 1000))
 def _create_mxc_to_http_filter(public_baseurl: str) -> Callable:
     """Create and return a jinja2 filter that converts MXC urls to HTTP
     Args:
         public_baseurl: The public, accessible base URL of the homeserver
@@ -635,26 +636,15 @@
     For example, the federation senders use this to determine which instances
     handles sending stuff to a given destination (which is used as the `key`
     below).
     """
     instances = attr.ib(type=List[str])
     def should_handle(self, instance_name: str, key: str) -> bool:
         """Whether this instance is responsible for handling the given key.
         """
         if not self.instances or len(self.instances) == 1:
             return True
-        return self.get_instance(key) == instance_name
-    def get_instance(self, key: str) -> str:
-        """Get the instance responsible for handling the given key.
-        Note: For things like federation sending the config for which instance
-        is sending is known only to the sender instance if there is only one.
-        Therefore `should_handle` should be used where possible.
-        """
-        if not self.instances:
-            return "master"
-        if len(self.instances) == 1:
-            return self.instances[0]
         dest_hash = sha256(key.encode("utf8")).digest()
         dest_int = int.from_bytes(dest_hash, byteorder="little")
         remainder = dest_int % (len(self.instances))
-        return self.instances[remainder]
+        return self.instances[remainder] == instance_name
 __all__ = ["Config", "RootConfig", "ShardedWorkerHandlingConfig"]

--- a/synapse/config/_util.py
+++ b/synapse/config/_util.py
@@ -1,17 +1,15 @@
-from typing import Any, Iterable
+from typing import Any, List
 import jsonschema
 from synapse.config._base import ConfigError
 from synapse.types import JsonDict
-def validate_config(
-    json_schema: JsonDict, config: Any, config_path: Iterable[str]
-) -> None:
+def validate_config(json_schema: JsonDict, config: Any, config_path: List[str]) -> None:
     """Validates a config setting against a JsonSchema definition
     This can be used to validate a section of the config file against a schema
     definition. If the validation fails, a ConfigError is raised with a textual
     description of the problem.
     Args:
         json_schema: the schema to validate against
         config: the configuration value to be validated
         config_path: the path within the config file. This will be used as a basis
            for the error message.
     """

--- a/synapse/config/captcha.py
+++ b/synapse/config/captcha.py
@@ -4,16 +4,13 @@
     def read_config(self, config, **kwargs):
         self.recaptcha_private_key = config.get("recaptcha_private_key")
         self.recaptcha_public_key = config.get("recaptcha_public_key")
         self.enable_registration_captcha = config.get(
             "enable_registration_captcha", False
         )
         self.recaptcha_siteverify_api = config.get(
             "recaptcha_siteverify_api",
             "https://www.recaptcha.net/recaptcha/api/siteverify",
         )
-        self.recaptcha_template = self.read_templates(
-            ["recaptcha.html"], autoescape=True
-        )[0]
     def generate_config_section(self, **kwargs):
         return """\
         """

--- a/synapse/config/consent_config.py
+++ b/synapse/config/consent_config.py
@@ -1,29 +1,28 @@
 from os import path
 from synapse.config import ConfigError
 from ._base import Config
 DEFAULT_CONFIG = """\
 """
 class ConsentConfig(Config):
     section = "consent"
     def __init__(self, *args):
-        super().__init__(*args)
+        super(ConsentConfig, self).__init__(*args)
         self.user_consent_version = None
         self.user_consent_template_dir = None
         self.user_consent_server_notice_content = None
         self.user_consent_server_notice_to_guests = False
         self.block_events_without_consent_error = None
         self.user_consent_at_registration = False
         self.user_consent_policy_name = "Privacy Policy"
     def read_config(self, config, **kwargs):
         consent_config = config.get("user_consent")
-        self.terms_template = self.read_templates(["terms.html"], autoescape=True)[0]
         if consent_config is None:
             return
         self.user_consent_version = str(consent_config["version"])
         self.user_consent_template_dir = self.abspath(consent_config["template_dir"])
         if not path.isdir(self.user_consent_template_dir):
             raise ConfigError(
                 "Could not find template directory '%s'"
                 % (self.user_consent_template_dir,)
             )
         self.user_consent_server_notice_content = consent_config.get(

--- a/synapse/config/emailconfig.py
+++ b/synapse/config/emailconfig.py
@@ -1,10 +1,11 @@
+from __future__ import print_function
 import email.utils
 import os
 from enum import Enum
 from typing import Optional
 import attr
 from ._base import Config, ConfigError
 MISSING_PASSWORD_RESET_CONFIG_ERROR = """\
 Password reset emails are enabled on this homeserver due to a partial
 'email' block. However, the following required keys are missing:
     %s
@@ -139,36 +140,34 @@
             add_threepid_template_success_html = email_config.get(
                 "add_threepid_template_success_html", "add_threepid_success.html"
             )
             (
                 self.email_password_reset_template_html,
                 self.email_password_reset_template_text,
                 self.email_registration_template_html,
                 self.email_registration_template_text,
                 self.email_add_threepid_template_html,
                 self.email_add_threepid_template_text,
-                self.email_password_reset_template_confirmation_html,
                 self.email_password_reset_template_failure_html,
                 self.email_registration_template_failure_html,
                 self.email_add_threepid_template_failure_html,
                 password_reset_template_success_html_template,
                 registration_template_success_html_template,
                 add_threepid_template_success_html_template,
             ) = self.read_templates(
                 [
                     password_reset_template_html,
                     password_reset_template_text,
                     registration_template_html,
                     registration_template_text,
                     add_threepid_template_html,
                     add_threepid_template_text,
-                    "password_reset_confirmation.html",
                     password_reset_template_failure_html,
                     registration_template_failure_html,
                     add_threepid_template_failure_html,
                     password_reset_template_success_html,
                     registration_template_success_html,
                     add_threepid_template_success_html,
                 ],
                 template_dir,
             )
             self.email_password_reset_template_success_html_content = (

--- a/synapse/config/federation.py
+++ b/synapse/config/federation.py
@@ -1,14 +1,13 @@
 from typing import Optional
 from netaddr import IPSet
-from synapse.config._base import Config, ConfigError
-from synapse.config._util import validate_config
+from ._base import Config, ConfigError
 class FederationConfig(Config):
     section = "federation"
     def read_config(self, config, **kwargs):
         self.federation_domain_whitelist = None  # type: Optional[dict]
         federation_domain_whitelist = config.get("federation_domain_whitelist", None)
         if federation_domain_whitelist is not None:
             self.federation_domain_whitelist = {}
             for domain in federation_domain_whitelist:
                 self.federation_domain_whitelist[domain] = True
         self.federation_ip_range_blacklist = config.get(
@@ -16,31 +15,23 @@
         )
         try:
             self.federation_ip_range_blacklist = IPSet(
                 self.federation_ip_range_blacklist
             )
             self.federation_ip_range_blacklist.update(["0.0.0.0", "::"])
         except Exception as e:
             raise ConfigError(
                 "Invalid range(s) provided in federation_ip_range_blacklist: %s" % e
             )
-        federation_metrics_domains = config.get("federation_metrics_domains") or []
-        validate_config(
-            _METRICS_FOR_DOMAINS_SCHEMA,
-            federation_metrics_domains,
-            ("federation_metrics_domains",),
-        )
-        self.federation_metrics_domains = set(federation_metrics_domains)
     def generate_config_section(self, config_dir_path, server_name, **kwargs):
         return """\
         federation_ip_range_blacklist:
           - '127.0.0.0/8'
           - '10.0.0.0/8'
           - '172.16.0.0/12'
           - '192.168.0.0/16'
           - '100.64.0.0/10'
           - '169.254.0.0/16'
           - '::1/128'
           - 'fe80::/64'
           - 'fc00::/7'
         """
-_METRICS_FOR_DOMAINS_SCHEMA = {"type": "array", "items": {"type": "string"}}

--- a/synapse/config/homeserver.py
+++ b/synapse/config/homeserver.py
@@ -66,11 +66,12 @@
         GroupsConfig,
         UserDirectoryConfig,
         ConsentConfig,
         StatsConfig,
         ServerNoticesConfig,
         RoomDirectoryConfig,
         ThirdPartyRulesConfig,
         TracerConfig,
         WorkerConfig,
         RedisConfig,
+        FederationConfig,
     ]

--- a/synapse/config/logger.py
+++ b/synapse/config/logger.py
@@ -1,23 +1,21 @@
 import argparse
 import logging
 import logging.config
 import os
 import sys
-import threading
 from string import Template
 import yaml
 from twisted.logger import (
     ILogObserver,
     LogBeginner,
     STDLibLogObserver,
-    eventAsText,
     globalLogBeginner,
 )
 import synapse
 from synapse.app import _base as appbase
 from synapse.logging._structured import (
     reload_structured_logging,
     setup_structured_logging,
 )
 from synapse.logging.context import LoggingContextFilter
 from synapse.util.versionstring import get_version_string
@@ -123,41 +121,29 @@
     else:
         logging.config.dictConfig(log_config)
     log_filter = LoggingContextFilter(request="")
     old_factory = logging.getLogRecordFactory()
     def factory(*args, **kwargs):
         record = old_factory(*args, **kwargs)
         log_filter.filter(record)
         return record
     logging.setLogRecordFactory(factory)
     observer = STDLibLogObserver()
-    threadlocal = threading.local()
     def _log(event):
         if "log_text" in event:
             if event["log_text"].startswith("DNSDatagramProtocol starting on "):
                 return
             if event["log_text"].startswith("(UDP Port "):
                 return
             if event["log_text"].startswith("Timing out client"):
                 return
-        if getattr(threadlocal, "active", False):
-            try:
-                event_text = eventAsText(event)
-                print("logging during logging: %s" % event_text, file=sys.__stderr__)
-            except Exception:
-                pass
-            return
-        try:
-            threadlocal.active = True
-            return observer(event)
-        finally:
-            threadlocal.active = False
+        return observer(event)
     logBeginner.beginLoggingTo([_log], redirectStandardIO=not config.no_redirect_stdio)
     if not config.no_redirect_stdio:
         print("Redirected stdout/stderr to logs")
     return observer
 def _reload_stdlib_logging(*args, log_config=None):
     logger = logging.getLogger("")
     if not log_config:
         logger.warning("Reloaded a blank config?")
     logging.config.dictConfig(log_config)
 def setup_logging(

--- a/synapse/config/oidc_config.py
+++ b/synapse/config/oidc_config.py
@@ -24,21 +24,20 @@
         self.oidc_client_secret = oidc_config["client_secret"]
         self.oidc_client_auth_method = oidc_config.get(
             "client_auth_method", "client_secret_basic"
         )
         self.oidc_scopes = oidc_config.get("scopes", ["openid"])
         self.oidc_authorization_endpoint = oidc_config.get("authorization_endpoint")
         self.oidc_token_endpoint = oidc_config.get("token_endpoint")
         self.oidc_userinfo_endpoint = oidc_config.get("userinfo_endpoint")
         self.oidc_jwks_uri = oidc_config.get("jwks_uri")
         self.oidc_skip_verification = oidc_config.get("skip_verification", False)
-        self.oidc_allow_existing_users = oidc_config.get("allow_existing_users", False)
         ump_config = oidc_config.get("user_mapping_provider", {})
         ump_config.setdefault("module", DEFAULT_USER_MAPPING_PROVIDER)
         ump_config.setdefault("config", {})
         (
             self.oidc_user_mapping_provider_class,
             self.oidc_user_mapping_provider_config,
         ) = load_module(ump_config)
         required_methods = [
             "get_remote_user_id",
             "map_user_attributes",

--- a/synapse/config/registration.py
+++ b/synapse/config/registration.py
@@ -3,21 +3,21 @@
 import pkg_resources
 from synapse.api.constants import RoomCreationPreset
 from synapse.config._base import Config, ConfigError
 from synapse.types import RoomAlias, UserID
 from synapse.util.stringutils import random_string_with_symbols
 class AccountValidityConfig(Config):
     section = "accountvalidity"
     def __init__(self, config, synapse_config):
         if config is None:
             return
-        super().__init__()
+        super(AccountValidityConfig, self).__init__()
         self.enabled = config.get("enabled", False)
         self.renew_by_email_enabled = "renew_at" in config
         if self.enabled:
             if "period" in config:
                 self.period = self.parse_duration(config["period"])
             else:
                 raise ConfigError("'period' is required when using account validity")
             if "renew_at" in config:
                 self.renew_at = self.parse_duration(config["renew_at"])
             if "renew_email_subject" in config:
@@ -125,23 +125,20 @@
         self.enable_set_displayname = config.get("enable_set_displayname", True)
         self.enable_set_avatar_url = config.get("enable_set_avatar_url", True)
         self.enable_3pid_changes = config.get("enable_3pid_changes", True)
         self.disable_msisdn_registration = config.get(
             "disable_msisdn_registration", False
         )
         session_lifetime = config.get("session_lifetime")
         if session_lifetime is not None:
             session_lifetime = self.parse_duration(session_lifetime)
         self.session_lifetime = session_lifetime
-        self.fallback_success_template = self.read_templates(
-            ["auth_success.html"], autoescape=True
-        )[0]
     def generate_config_section(self, generate_secrets=False, **kwargs):
         if generate_secrets:
             registration_shared_secret = 'registration_shared_secret: "%s"' % (
                 random_string_with_symbols(50),
             )
         else:
             registration_shared_secret = "#registration_shared_secret: <PRIVATE STRING>"
         return (
             """\
         account_validity:

--- a/synapse/config/saml2_config.py
+++ b/synapse/config/saml2_config.py
@@ -100,20 +100,23 @@
         config_path = saml2_config.get("config_path", None)
         if config_path is not None:
             mod = load_python_module(config_path)
             _dict_merge(merge_dict=mod.CONFIG, into_dict=saml2_config_dict)
         import saml2.config
         self.saml2_sp_config = saml2.config.SPConfig()
         self.saml2_sp_config.load(saml2_config_dict)
         self.saml2_session_lifetime = self.parse_duration(
             saml2_config.get("saml_session_lifetime", "15m")
         )
+        self.saml2_error_html_template = self.read_templates(
+            ["saml_error.html"], saml2_config.get("template_dir"), autoescape=True
+        )[0]
     def _default_saml_config_dict(
         self, required_attributes: set, optional_attributes: set
     ):
         """Generate a configuration dictionary with required and optional attributes that
         will be needed to process new user registration
         Args:
             required_attributes: SAML auth response attributes that are
                 necessary to function
             optional_attributes: SAML auth response attributes that can be used to add
                 additional information to Synapse user accounts, but are not required

--- a/synapse/config/server.py
+++ b/synapse/config/server.py
@@ -1,15 +1,15 @@
 import logging
 import os.path
 import re
 from textwrap import indent
-from typing import Any, Dict, Iterable, List, Optional, Set
+from typing import Any, Dict, Iterable, List, Optional
 import attr
 import yaml
 from synapse.api.room_versions import KNOWN_ROOM_VERSIONS
 from synapse.http.endpoint import parse_and_validate_server_name
 from ._base import Config, ConfigError
 logger = logging.Logger(__name__)
 DEFAULT_BIND_ADDRESSES = ["::", "0.0.0.0"]
 DEFAULT_ROOM_VERSION = "5"
 ROOM_COMPLEXITY_TOO_GREAT = (
     "Your homeserver is unable to join rooms this large or complex. "
@@ -364,28 +364,20 @@
             "request_token_inhibit_3pid_errors", False,
         )
         users_new_default_push_rules = (
             config.get("users_new_default_push_rules") or []
         )  # type: list
         if not isinstance(users_new_default_push_rules, list):
             raise ConfigError("'users_new_default_push_rules' must be a list")
         self.users_new_default_push_rules = set(
             users_new_default_push_rules
         )  # type: set
-        next_link_domain_whitelist = config.get(
-            "next_link_domain_whitelist"
-        )  # type: Optional[List[str]]
-        self.next_link_domain_whitelist = None  # type: Optional[Set[str]]
-        if next_link_domain_whitelist is not None:
-            if not isinstance(next_link_domain_whitelist, list):
-                raise ConfigError("'next_link_domain_whitelist' must be a list")
-            self.next_link_domain_whitelist = set(next_link_domain_whitelist)
     def has_tls_listener(self) -> bool:
         return any(listener.tls for listener in self.listeners)
     def generate_config_section(
         self, server_name, data_dir_path, open_private_ports, listeners, **kwargs
     ):
         _, bind_port = parse_and_validate_server_name(server_name)
         if bind_port is not None:
             unsecure_port = bind_port - 400
         else:
             bind_port = 8448

--- a/synapse/config/server_notices_config.py
+++ b/synapse/config/server_notices_config.py
@@ -13,21 +13,21 @@
             None if server notices are not enabled.
         server_notices_mxid_avatar_url (str|None):
             The MXC URL for the avatar of the server notices user.
             None if server notices are not enabled.
         server_notices_room_name (str|None):
             The name to use for the server notices room.
             None if server notices are not enabled.
     """
     section = "servernotices"
     def __init__(self, *args):
-        super().__init__(*args)
+        super(ServerNoticesConfig, self).__init__(*args)
         self.server_notices_mxid = None
         self.server_notices_mxid_display_name = None
         self.server_notices_mxid_avatar_url = None
         self.server_notices_room_name = None
     def read_config(self, config, **kwargs):
         c = config.get("server_notices")
         if c is None:
             return
         mxid_localpart = c["system_mxid_localpart"]
         self.server_notices_mxid = UserID(mxid_localpart, self.server_name).to_string()

--- a/synapse/config/stats.py
+++ b/synapse/config/stats.py
@@ -1,10 +1,11 @@
+from __future__ import division
 import sys
 from ._base import Config
 class StatsConfig(Config):
     """Stats Configuration
     Configuration for the behaviour of synapse's stats engine
     """
     section = "stats"
     def read_config(self, config, **kwargs):
         self.stats_enabled = True
         self.stats_bucket_size = 86400 * 1000

--- a/synapse/config/workers.py
+++ b/synapse/config/workers.py
@@ -1,37 +1,27 @@
-from typing import List, Union
 import attr
 from ._base import Config, ConfigError, ShardedWorkerHandlingConfig
 from .server import ListenerConfig, parse_listener_def
-def _instance_to_list_converter(obj: Union[str, List[str]]) -> List[str]:
-    """Helper for allowing parsing a string or list of strings to a config
-    option expecting a list of strings.
-    """
-    if isinstance(obj, str):
-        return [obj]
-    return obj
 @attr.s
 class InstanceLocationConfig:
     """The host and port to talk to an instance via HTTP replication.
     """
     host = attr.ib(type=str)
     port = attr.ib(type=int)
 @attr.s
 class WriterLocations:
     """Specifies the instances that write various streams.
     Attributes:
-        events: The instances that write to the event and backfill streams.
-        typing: The instance that writes to the typing stream.
+        events: The instance that writes to the event and backfill streams.
+        events: The instance that writes to the typing stream.
     """
-    events = attr.ib(
-        default=["master"], type=List[str], converter=_instance_to_list_converter
-    )
+    events = attr.ib(default="master", type=str)
     typing = attr.ib(default="master", type=str)
 class WorkerConfig(Config):
     """The workers are processes run separately to the main synapse process.
     They have their own pid_file and listener configuration. They use the
     replication_url to talk to the main synapse process."""
     section = "worker"
     def read_config(self, config, **kwargs):
         self.worker_app = config.get("worker_app")
         if self.worker_app == "synapse.app.homeserver":
             self.worker_app = None
@@ -58,26 +48,24 @@
         self.federation_shard_config = ShardedWorkerHandlingConfig(
             federation_sender_instances
         )
         instance_map = config.get("instance_map") or {}
         self.instance_map = {
             name: InstanceLocationConfig(**c) for name, c in instance_map.items()
         }
         writers = config.get("stream_writers") or {}
         self.writers = WriterLocations(**writers)
         for stream in ("events", "typing"):
-            instances = _instance_to_list_converter(getattr(self.writers, stream))
-            for instance in instances:
-                if instance != "master" and instance not in self.instance_map:
-                    raise ConfigError(
-                        "Instance %r is configured to write %s but does not appear in `instance_map` config."
-                        % (instance, stream)
-                    )
-        self.events_shard_config = ShardedWorkerHandlingConfig(self.writers.events)
+            instance = getattr(self.writers, stream)
+            if instance != "master" and instance not in self.instance_map:
+                raise ConfigError(
+                    "Instance %r is configured to write %s but does not appear in `instance_map` config."
+                    % (instance, stream)
+                )
     def generate_config_section(self, config_dir_path, server_name, **kwargs):
         return """\
         """
     def read_arguments(self, args):
         if args.daemonize is not None:
             self.worker_daemonize = args.daemonize
         if args.manhole is not None:
             self.worker_manhole = args.worker_manhole

--- a/synapse/crypto/context_factory.py
+++ b/synapse/crypto/context_factory.py
@@ -16,24 +16,21 @@
 from twisted.web.iweb import IPolicyForHTTPS
 logger = logging.getLogger(__name__)
 _TLS_VERSION_MAP = {
     "1": TLSVersion.TLSv1_0,
     "1.1": TLSVersion.TLSv1_1,
     "1.2": TLSVersion.TLSv1_2,
     "1.3": TLSVersion.TLSv1_3,
 }
 class ServerContextFactory(ContextFactory):
     """Factory for PyOpenSSL SSL contexts that are used to handle incoming
-    connections.
-    TODO: replace this with an implementation of IOpenSSLServerConnectionCreator,
-    per https://github.com/matrix-org/synapse/issues/1691
-    """
+    connections."""
     def __init__(self, config):
         self._context = SSL.Context(SSL.SSLv23_METHOD)
         self.configure_context(self._context, config)
     @staticmethod
     def configure_context(context, config):
         try:
             _ecCurve = crypto.get_elliptic_curve(_defaultCurveName)
             context.set_tmp_ecdh(_ecCurve)
         except Exception:
             logger.exception("Failed to enable elliptic curve for TLS")

--- a/synapse/crypto/keyring.py
+++ b/synapse/crypto/keyring.py
@@ -16,20 +16,21 @@
 from unpaddedbase64 import decode_base64
 from twisted.internet import defer
 from synapse.api.errors import (
     Codes,
     HttpResponseException,
     RequestSendFailed,
     SynapseError,
 )
 from synapse.logging.context import (
     PreserveLoggingContext,
+    current_context,
     make_deferred_yieldable,
     preserve_fn,
     run_in_background,
 )
 from synapse.storage.keys import FetchKeyResult
 from synapse.util import unwrapFirstError
 from synapse.util.async_helpers import yieldable_gather_results
 from synapse.util.metrics import Measure
 from synapse.util.retryutils import NotRetryingDestination
 logger = logging.getLogger(__name__)
@@ -150,39 +151,41 @@
         if key_lookups:
             run_in_background(self._start_key_lookups, key_lookups)
         return results
     async def _start_key_lookups(self, verify_requests):
         """Sets off the key fetches for each verify request
         Once each fetch completes, verify_request.key_ready will be resolved.
         Args:
             verify_requests (List[VerifyJsonRequest]):
         """
         try:
+            ctx = current_context()
             server_to_request_ids = {}
             for verify_request in verify_requests:
                 server_name = verify_request.server_name
                 request_id = id(verify_request)
                 server_to_request_ids.setdefault(server_name, set()).add(request_id)
             await self.wait_for_previous_lookups(server_to_request_ids.keys())
             for server_name in server_to_request_ids.keys():
                 self.key_downloads[server_name] = defer.Deferred()
                 logger.debug("Got key lookup lock on %s", server_name)
             def drop_server_lock(server_name):
                 d = self.key_downloads.pop(server_name)
                 d.callback(None)
             def lookup_done(res, verify_request):
                 server_name = verify_request.server_name
                 server_requests = server_to_request_ids[server_name]
                 server_requests.remove(id(verify_request))
                 if not server_requests:
-                    logger.debug("Releasing key lookup lock on %s", server_name)
-                    drop_server_lock(server_name)
+                    with PreserveLoggingContext(ctx):
+                        logger.debug("Releasing key lookup lock on %s", server_name)
+                    self.clock.call_later(0, drop_server_lock, server_name)
                 return res
             for verify_request in verify_requests:
                 verify_request.key_ready.addBoth(lookup_done, verify_request)
             self._get_server_verify_keys(verify_requests)
         except Exception:
             logger.exception("Error starting key lookups")
     async def wait_for_previous_lookups(self, server_names) -> None:
         """Waits for any previous key lookups for the given servers to finish.
         Args:
             server_names (Iterable[str]): list of servers which we want to look up
@@ -218,39 +221,34 @@
         remaining_requests = {rq for rq in verify_requests if not rq.key_ready.called}
         async def do_iterations():
             try:
                 with Measure(self.clock, "get_server_verify_keys"):
                     for f in self._key_fetchers:
                         if not remaining_requests:
                             return
                         await self._attempt_key_fetches_with_fetcher(
                             f, remaining_requests
                         )
-                    while remaining_requests:
-                        verify_request = remaining_requests.pop()
-                        rq_str = (
-                            "VerifyJsonRequest(server=%s, key_ids=%s, min_valid=%i)"
-                            % (
-                                verify_request.server_name,
-                                verify_request.key_ids,
-                                verify_request.minimum_valid_until_ts,
+                    with PreserveLoggingContext():
+                        for verify_request in remaining_requests:
+                            verify_request.key_ready.errback(
+                                SynapseError(
+                                    401,
+                                    "No key for %s with ids in %s (min_validity %i)"
+                                    % (
+                                        verify_request.server_name,
+                                        verify_request.key_ids,
+                                        verify_request.minimum_valid_until_ts,
+                                    ),
+                                    Codes.UNAUTHORIZED,
+                                )
                             )
-                        )
-                        self.clock.call_later(
-                            0,
-                            verify_request.key_ready.errback,
-                            SynapseError(
-                                401,
-                                "Failed to find any key to satisfy %s" % (rq_str,),
-                                Codes.UNAUTHORIZED,
-                            ),
-                        )
             except Exception as err:
                 logger.error("Unexpected error in _get_server_verify_keys: %s", err)
                 with PreserveLoggingContext():
                     for verify_request in remaining_requests:
                         if not verify_request.key_ready.called:
                             verify_request.key_ready.errback(err)
         run_in_background(do_iterations)
     async def _attempt_key_fetches_with_fetcher(self, fetcher, remaining_requests):
         """Use a key fetcher to attempt to satisfy some key requests
         Args:
@@ -274,31 +272,24 @@
             result_keys = results.get(server_name, {})
             for key_id in verify_request.key_ids:
                 fetch_key_result = result_keys.get(key_id)
                 if not fetch_key_result:
                     continue
                 if (
                     fetch_key_result.valid_until_ts
                     < verify_request.minimum_valid_until_ts
                 ):
                     continue
-                logger.debug(
-                    "Found key %s:%s for %s",
-                    server_name,
-                    key_id,
-                    verify_request.request_name,
-                )
-                self.clock.call_later(
-                    0,
-                    verify_request.key_ready.callback,
-                    (server_name, key_id, fetch_key_result.verify_key),
-                )
+                with PreserveLoggingContext():
+                    verify_request.key_ready.callback(
+                        (server_name, key_id, fetch_key_result.verify_key)
+                    )
                 completed.append(verify_request)
                 break
         remaining_requests.difference_update(completed)
 class KeyFetcher:
     async def get_keys(self, keys_to_fetch):
         """
         Args:
             keys_to_fetch (dict[str, dict[str, int]]):
                 the keys to be fetched. server_name -> key_id -> min_valid_ts
         Returns:
@@ -391,21 +382,21 @@
                     )
                     for key_id in verify_keys
                 ],
                 consumeErrors=True,
             ).addErrback(unwrapFirstError)
         )
         return verify_keys
 class PerspectivesKeyFetcher(BaseV2KeyFetcher):
     """KeyFetcher impl which fetches keys from the "perspectives" servers"""
     def __init__(self, hs):
-        super().__init__(hs)
+        super(PerspectivesKeyFetcher, self).__init__(hs)
         self.clock = hs.get_clock()
         self.client = hs.get_http_client()
         self.key_servers = self.config.key_servers
     async def get_keys(self, keys_to_fetch):
         """see KeyFetcher.get_keys"""
         async def get_key(key_server):
             try:
                 result = await self.get_server_verify_key_v2_indirect(
                     keys_to_fetch, key_server
                 )
@@ -528,21 +519,21 @@
             raise KeyLookupError(
                 "Response not signed with a known key: signed with: %r, known keys: %r"
                 % (
                     list(response["signatures"][perspective_name].keys()),
                     list(perspective_keys.keys()),
                 )
             )
 class ServerKeyFetcher(BaseV2KeyFetcher):
     """KeyFetcher impl which fetches keys from the origin servers"""
     def __init__(self, hs):
-        super().__init__(hs)
+        super(ServerKeyFetcher, self).__init__(hs)
         self.clock = hs.get_clock()
         self.client = hs.get_http_client()
     async def get_keys(self, keys_to_fetch):
         """
         Args:
             keys_to_fetch (dict[str, iterable[str]]):
                 the keys to be fetched. server_name -> key_ids
         Returns:
             dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]:
                 map from server_name -> key_id -> FetchKeyResult

--- a/synapse/events/__init__.py
+++ b/synapse/events/__init__.py
@@ -1,17 +1,17 @@
 import abc
 import os
 from distutils.util import strtobool
 from typing import Dict, Optional, Tuple, Type
 from unpaddedbase64 import encode_base64
 from synapse.api.room_versions import EventFormatVersions, RoomVersion, RoomVersions
-from synapse.types import JsonDict, RoomStreamToken
+from synapse.types import JsonDict
 from synapse.util.caches import intern_dict
 from synapse.util.frozenutils import freeze
 USE_FROZEN_DICTS = strtobool(os.environ.get("SYNAPSE_USE_FROZEN_DICTS", "0"))
 class DictProperty:
     """An object property which delegates to the `_dict` within its parent object."""
     __slots__ = ["key"]
     def __init__(self, key: str):
         self.key = key
     def __get__(self, instance, owner=None):
         if instance is None:
@@ -51,22 +51,22 @@
     outlier = DictProperty("outlier")  # type: bool
     out_of_band_membership = DictProperty("out_of_band_membership")  # type: bool
     send_on_behalf_of = DictProperty("send_on_behalf_of")  # type: str
     recheck_redaction = DictProperty("recheck_redaction")  # type: bool
     soft_failed = DictProperty("soft_failed")  # type: bool
     proactively_send = DictProperty("proactively_send")  # type: bool
     redacted = DictProperty("redacted")  # type: bool
     txn_id = DictProperty("txn_id")  # type: str
     token_id = DictProperty("token_id")  # type: str
     stream_ordering = DictProperty("stream_ordering")  # type: int
-    before = DictProperty("before")  # type: RoomStreamToken
-    after = DictProperty("after")  # type: RoomStreamToken
+    before = DictProperty("before")  # type: str
+    after = DictProperty("after")  # type: str
     order = DictProperty("order")  # type: Tuple[int, int]
     def get_dict(self) -> JsonDict:
         return dict(self._dict)
     def is_outlier(self) -> bool:
         return self._dict.get("outlier", False)
     def is_out_of_band_membership(self) -> bool:
         """Whether this is an out of band membership, like an invite or an invite
         rejection. This is needed as those events are marked as outliers, but
         they still need to be processed as if they're new events (e.g. updating
         invite state in the database, relaying to clients, etc).

--- a/synapse/federation/federation_client.py
+++ b/synapse/federation/federation_client.py
@@ -1,26 +1,24 @@
 import copy
 import itertools
 import logging
 from typing import (
     Any,
     Awaitable,
     Callable,
     Dict,
     Iterable,
     List,
-    Mapping,
     Optional,
     Sequence,
     Tuple,
     TypeVar,
-    Union,
 )
 from prometheus_client import Counter
 from twisted.internet import defer
 from twisted.internet.defer import Deferred
 from synapse.api.constants import EventTypes, Membership
 from synapse.api.errors import (
     CodeMessageException,
     Codes,
     FederationDeniedError,
     HttpResponseException,
@@ -45,21 +43,21 @@
 sent_queries_counter = Counter("synapse_federation_client_sent_queries", "", ["type"])
 PDU_RETRY_TIME_MS = 1 * 60 * 1000
 T = TypeVar("T")
 class InvalidResponseError(RuntimeError):
     """Helper for _try_destination_list: indicates that the server returned a response
     we couldn't parse
     """
     pass
 class FederationClient(FederationBase):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(FederationClient, self).__init__(hs)
         self.pdu_destination_tried = {}
         self._clock.looping_call(self._clear_tried_cache, 60 * 1000)
         self.state = hs.get_state_handler()
         self.transport_layer = hs.get_federation_transport_client()
         self.hostname = hs.hostname
         self.signing_key = hs.signing_key
         self._get_pdu_cache = ExpiringCache(
             cache_name="get_pdu_cache",
             clock=self._clock,
             max_len=1000,
@@ -382,21 +380,21 @@
                     "Failed to %s via %s", description, destination, exc_info=True
                 )
         raise SynapseError(502, "Failed to %s via any server" % (description,))
     async def make_membership_event(
         self,
         destinations: Iterable[str],
         room_id: str,
         user_id: str,
         membership: str,
         content: dict,
-        params: Optional[Mapping[str, Union[str, Iterable[str]]]],
+        params: Dict[str, str],
     ) -> Tuple[str, EventBase, RoomVersion]:
         """
         Creates an m.room.member event, with context, without participating in the room.
         Does so by asking one of the already participating servers to create an
         event with proper context.
         Returns a fully signed and hashed event.
         Note that this does not append any events to any graphs.
         Args:
             destinations: Candidate homeservers which are probably
                 participating in the room.

--- a/synapse/federation/federation_server.py
+++ b/synapse/federation/federation_server.py
@@ -4,21 +4,21 @@
     Any,
     Awaitable,
     Callable,
     Dict,
     List,
     Match,
     Optional,
     Tuple,
     Union,
 )
-from prometheus_client import Counter, Gauge, Histogram
+from prometheus_client import Counter, Histogram
 from twisted.internet import defer
 from twisted.internet.abstract import isIPAddress
 from twisted.python import failure
 from synapse.api.constants import EventTypes, Membership
 from synapse.api.errors import (
     AuthError,
     Codes,
     FederationError,
     IncompatibleRoomVersionError,
     NotFoundError,
@@ -51,46 +51,38 @@
 TRANSACTION_CONCURRENCY_LIMIT = 10
 logger = logging.getLogger(__name__)
 received_pdus_counter = Counter("synapse_federation_server_received_pdus", "")
 received_edus_counter = Counter("synapse_federation_server_received_edus", "")
 received_queries_counter = Counter(
     "synapse_federation_server_received_queries", "", ["type"]
 )
 pdu_process_time = Histogram(
     "synapse_federation_server_pdu_process_time", "Time taken to process an event",
 )
-last_pdu_age_metric = Gauge(
-    "synapse_federation_last_received_pdu_age",
-    "The age (in seconds) of the last PDU successfully received from the given domain",
-    labelnames=("server_name",),
-)
 class FederationServer(FederationBase):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(FederationServer, self).__init__(hs)
         self.auth = hs.get_auth()
         self.handler = hs.get_handlers().federation_handler
         self.state = hs.get_state_handler()
         self.device_handler = hs.get_device_handler()
         self._federation_ratelimiter = hs.get_federation_ratelimiter()
         self._server_linearizer = Linearizer("fed_server")
         self._transaction_linearizer = Linearizer("fed_txn_handler")
         self._transaction_resp_cache = ResponseCache(
             hs, "fed_txn_handler", timeout_ms=30000
         )
         self.transaction_actions = TransactionActions(self.store)
         self.registry = hs.get_federation_registry()
         self._state_resp_cache = ResponseCache(hs, "state_resp", timeout_ms=30000)
         self._state_ids_resp_cache = ResponseCache(
             hs, "state_ids_resp", timeout_ms=30000
-        )
-        self._federation_metrics_domains = (
-            hs.get_config().federation.federation_metrics_domains
         )
     async def on_backfill_request(
         self, origin: str, room_id: str, versions: List[str], limit: int
     ) -> Tuple[int, Dict[str, Any]]:
         with (await self._server_linearizer.queue((origin, room_id))):
             origin_host, _ = parse_server_name(origin)
             await self.check_server_matches_acl(origin_host, room_id)
             pdus = await self.handler.on_backfill_request(
                 origin, room_id, versions, limit
             )
@@ -173,21 +165,20 @@
             origin: the server making the request
             transaction: incoming transaction
             request_time: timestamp that the HTTP request arrived at
         Returns:
             A map from event ID of a processed PDU to any errors we should
             report back to the sending server.
         """
         received_pdus_counter.inc(len(transaction.pdus))  # type: ignore
         origin_host, _ = parse_server_name(origin)
         pdus_by_room = {}  # type: Dict[str, List[EventBase]]
-        newest_pdu_ts = 0
         for p in transaction.pdus:  # type: ignore
             if "unsigned" in p:
                 unsigned = p["unsigned"]
                 if "age" in unsigned:
                     p["age"] = unsigned["age"]
             if "age" in p:
                 p["age_ts"] = request_time - int(p["age"])
                 del p["age"]
             possible_event_id = p.get("event_id", "<Unknown>")
             room_id = p.get("room_id")
@@ -200,22 +191,20 @@
             try:
                 room_version = await self.store.get_room_version(room_id)
             except NotFoundError:
                 logger.info("Ignoring PDU for unknown room_id: %s", room_id)
                 continue
             except UnsupportedRoomVersionError as e:
                 logger.info("Ignoring PDU: %s", e)
                 continue
             event = event_from_pdu_json(p, room_version)
             pdus_by_room.setdefault(room_id, []).append(event)
-            if event.origin_server_ts > newest_pdu_ts:
-                newest_pdu_ts = event.origin_server_ts
         pdu_results = {}
         async def process_pdus_for_room(room_id: str):
             logger.debug("Processing PDUs for %s", room_id)
             try:
                 await self.check_server_matches_acl(origin_host, room_id)
             except AuthError as e:
                 logger.warning("Ignoring PDUs for room %s from banned server", room_id)
                 for pdu in pdus_by_room[room_id]:
                     event_id = pdu.event_id
                     pdu_results[event_id] = e.error_dict()
@@ -234,23 +223,20 @@
                             f = failure.Failure()
                             pdu_results[event_id] = {"error": str(e)}
                             logger.error(
                                 "Failed to handle PDU %s",
                                 event_id,
                                 exc_info=(f.type, f.value, f.getTracebackObject()),
                             )
         await concurrently_execute(
             process_pdus_for_room, pdus_by_room.keys(), TRANSACTION_CONCURRENCY_LIMIT
         )
-        if newest_pdu_ts and origin in self._federation_metrics_domains:
-            newest_pdu_age = self._clock.time_msec() - newest_pdu_ts
-            last_pdu_age_metric.labels(server_name=origin).set(newest_pdu_age / 1000)
         return pdu_results
     async def _handle_edus_in_txn(self, origin: str, transaction: Transaction):
         """Process the EDUs in a received transaction.
         """
         async def _process_edu(edu_dict):
             received_edus_counter.inc()
             edu = Edu(
                 origin=origin,
                 destination=self.server_name,
                 edu_type=edu_dict["edu_type"],

--- a/synapse/federation/sender/__init__.py
+++ b/synapse/federation/sender/__init__.py
@@ -26,22 +26,20 @@
 from synapse.util.metrics import Measure, measure_func
 logger = logging.getLogger(__name__)
 sent_pdus_destination_dist_count = Counter(
     "synapse_federation_client_sent_pdu_destinations:count",
     "Number of PDUs queued for sending to one or more destinations",
 )
 sent_pdus_destination_dist_total = Counter(
     "synapse_federation_client_sent_pdu_destinations:total",
     "Total number of PDUs queued for sending across all destinations",
 )
-CATCH_UP_STARTUP_DELAY_SEC = 15
-CATCH_UP_STARTUP_INTERVAL_SEC = 5
 class FederationSender:
     def __init__(self, hs: "synapse.server.HomeServer"):
         self.hs = hs
         self.server_name = hs.hostname
         self.store = hs.get_datastore()
         self.state = hs.get_state_handler()
         self.clock = hs.get_clock()
         self.is_mine_id = hs.is_mine_id
         self._transaction_manager = TransactionManager(hs)
         self._instance_name = hs.get_instance_name()
@@ -75,26 +73,20 @@
             ),
         )
         self._is_processing = False
         self._last_poked_id = -1
         self._processing_pending_presence = False
         self._queues_awaiting_rr_flush_by_room = (
             {}
         )  # type: Dict[str, Set[PerDestinationQueue]]
         self._rr_txn_interval_per_room_ms = (
             1000.0 / hs.config.federation_rr_transactions_per_room_per_second
-        )
-        self._catchup_after_startup_timer = self.clock.call_later(
-            CATCH_UP_STARTUP_DELAY_SEC,
-            run_as_background_process,
-            "wake_destinations_needing_catchup",
-            self._wake_destinations_needing_catchup,
         )
     def _get_per_destination_queue(self, destination: str) -> PerDestinationQueue:
         """Get or create a PerDestinationQueue for the given destination
         Args:
             destination: server_name of remote server
         """
         queue = self._per_destination_queues.get(destination)
         if not queue:
             queue = PerDestinationQueue(self.hs, self._transaction_manager, destination)
             self._per_destination_queues[destination] = queue
@@ -141,21 +133,21 @@
                         d
                         for d in destinations
                         if self._federation_shard_config.should_handle(
                             self._instance_name, d
                         )
                     }
                     if send_on_behalf_of is not None:
                         destinations.discard(send_on_behalf_of)
                     logger.debug("Sending %s to %r", event, destinations)
                     if destinations:
-                        await self._send_pdu(event, destinations)
+                        self._send_pdu(event, destinations)
                         now = self.clock.time_msec()
                         ts = await self.store.get_received_ts(event.event_id)
                         synapse.metrics.event_processing_lag_by_event.labels(
                             "federation_sender"
                         ).observe((now - ts) / 1000)
                 async def handle_room_events(events: Iterable[EventBase]) -> None:
                     with Measure(self.clock, "handle_room_events"):
                         for event in events:
                             await handle_event(event)
                 events_by_room = {}  # type: Dict[str, List[EventBase]]
@@ -183,31 +175,28 @@
                     events_processed_counter.inc(len(events))
                     event_processing_loop_room_count.labels("federation_sender").inc(
                         len(events_by_room)
                     )
                 event_processing_loop_counter.labels("federation_sender").inc()
                 synapse.metrics.event_processing_positions.labels(
                     "federation_sender"
                 ).set(next_token)
         finally:
             self._is_processing = False
-    async def _send_pdu(self, pdu: EventBase, destinations: Iterable[str]) -> None:
+    def _send_pdu(self, pdu: EventBase, destinations: Iterable[str]) -> None:
         destinations = set(destinations)
         destinations.discard(self.server_name)
         logger.debug("Sending to: %s", str(destinations))
         if not destinations:
             return
         sent_pdus_destination_dist_total.inc(len(destinations))
         sent_pdus_destination_dist_count.inc()
-        await self.store.store_destination_rooms_entries(
-            destinations, pdu.room_id, pdu.internal_metadata.stream_ordering,
-        )
         for destination in destinations:
             self._get_per_destination_queue(destination).send_pdu(pdu)
     async def send_read_receipt(self, receipt: ReadReceipt) -> None:
         """Send a RR to any other servers in the room
         Args:
             receipt: receipt to be sent
         """
         room_id = receipt.room_id
         domains_set = await self.state.get_current_hosts_in_room(room_id)
         domains = [
@@ -366,36 +355,10 @@
             return
         self._get_per_destination_queue(destination).attempt_new_transaction()
     @staticmethod
     def get_current_token() -> int:
         return 0
     @staticmethod
     async def get_replication_rows(
         instance_name: str, from_token: int, to_token: int, target_row_count: int
     ) -> Tuple[List[Tuple[int, Tuple]], int, bool]:
         return [], 0, False
-    async def _wake_destinations_needing_catchup(self):
-        """
-        Wakes up destinations that need catch-up and are not currently being
-        backed off from.
-        In order to reduce load spikes, adds a delay between each destination.
-        """
-        last_processed = None  # type: Optional[str]
-        while True:
-            destinations_to_wake = await self.store.get_catch_up_outstanding_destinations(
-                last_processed
-            )
-            if not destinations_to_wake:
-                self._catchup_after_startup_timer = None
-                break
-            destinations_to_wake = [
-                d
-                for d in destinations_to_wake
-                if self._federation_shard_config.should_handle(self._instance_name, d)
-            ]
-            for last_processed in destinations_to_wake:
-                logger.info(
-                    "Destination %s has outstanding catch-up, waking up.",
-                    last_processed,
-                )
-                self.wake_destination(last_processed)
-                await self.clock.sleep(CATCH_UP_STARTUP_INTERVAL_SEC)

--- a/synapse/federation/sender/per_destination_queue.py
+++ b/synapse/federation/sender/per_destination_queue.py
@@ -1,13 +1,13 @@
 import datetime
 import logging
-from typing import TYPE_CHECKING, Dict, Hashable, Iterable, List, Optional, Tuple, cast
+from typing import TYPE_CHECKING, Dict, Hashable, Iterable, List, Tuple
 from prometheus_client import Counter
 from synapse.api.errors import (
     FederationDeniedError,
     HttpResponseException,
     RequestSendFailed,
 )
 from synapse.api.presence import UserPresenceState
 from synapse.events import EventBase
 from synapse.federation.units import Edu
 from synapse.handlers.presence import format_user_presence_state
@@ -51,23 +51,20 @@
         self._should_send_on_this_instance = True
         if not self._federation_shard_config.should_handle(
             self._instance_name, destination
         ):
             logger.error(
                 "Create a per destination queue for %s on wrong worker", destination,
             )
             self._should_send_on_this_instance = False
         self._destination = destination
         self.transmission_loop_running = False
-        self._catching_up = True  # type: bool
-        self._catchup_last_skipped = 0  # type: int
-        self._last_successful_stream_ordering = None  # type: Optional[int]
         self._pending_pdus = []  # type: List[EventBase]
         self._pending_edus = []  # type: List[Edu]
         self._pending_edus_keyed = {}  # type: Dict[Tuple[str, Hashable], Edu]
         self._pending_presence = {}  # type: Dict[str, UserPresenceState]
         self._pending_rrs = {}  # type: Dict[str, Dict[str, Dict[str, dict]]]
         self._rrs_pending_flush = False
         self._last_device_stream_id = 0
         self._last_device_list_stream_id = 0
     def __str__(self) -> str:
         return "PerDestinationQueue[%s]" % self._destination
@@ -77,24 +74,21 @@
         return (
             len(self._pending_edus)
             + len(self._pending_presence)
             + len(self._pending_edus_keyed)
         )
     def send_pdu(self, pdu: EventBase) -> None:
         """Add a PDU to the queue, and start the transmission loop if necessary
         Args:
             pdu: pdu to send
         """
-        if not self._catching_up or self._last_successful_stream_ordering is None:
-            self._pending_pdus.append(pdu)
-        else:
-            self._catchup_last_skipped = pdu.internal_metadata.stream_ordering
+        self._pending_pdus.append(pdu)
         self.attempt_new_transaction()
     def send_presence(self, states: Iterable[UserPresenceState]) -> None:
         """Add presence updates to the queue. Start the transmission loop if necessary.
         Args:
             states: presence to send
         """
         self._pending_presence.update({state.user_id: state for state in states})
         self.attempt_new_transaction()
     def queue_read_receipt(self, receipt: ReadReceipt) -> None:
         """Add a RR to the list to be sent. Doesn't start the transmission loop yet
@@ -133,24 +127,20 @@
         logger.debug("TX [%s] Starting transaction loop", self._destination)
         run_as_background_process(
             "federation_transaction_transmission_loop",
             self._transaction_transmission_loop,
         )
     async def _transaction_transmission_loop(self) -> None:
         pending_pdus = []  # type: List[EventBase]
         try:
             self.transmission_loop_running = True
             await get_retry_limiter(self._destination, self._clock, self._store)
-            if self._catching_up:
-                await self._catch_up_transmission_loop()
-                if self._catching_up:
-                    return
             pending_pdus = []
             while True:
                 limit = MAX_EDUS_PER_TRANSACTION - 2
                 device_update_edus, dev_list_id = await self._get_device_update_edus(
                     limit
                 )
                 limit -= len(device_update_edus)
                 (
                     to_device_edus,
                     device_stream_id,
@@ -212,115 +202,62 @@
                         )
                     if device_update_edus:
                         logger.info(
                             "Marking as sent %r %r", self._destination, dev_list_id
                         )
                         await self._store.mark_as_sent_devices_by_remote(
                             self._destination, dev_list_id
                         )
                     self._last_device_stream_id = device_stream_id
                     self._last_device_list_stream_id = dev_list_id
-                    if pending_pdus:
-                        final_pdu = pending_pdus[-1]
-                        last_successful_stream_ordering = (
-                            final_pdu.internal_metadata.stream_ordering
-                        )
-                        await self._store.set_destination_last_successful_stream_ordering(
-                            self._destination, last_successful_stream_ordering
-                        )
                 else:
                     break
         except NotRetryingDestination as e:
             logger.debug(
                 "TX [%s] not ready for retry yet (next retry at %s) - "
                 "dropping transaction for now",
                 self._destination,
                 datetime.datetime.fromtimestamp(
                     (e.retry_last_ts + e.retry_interval) / 1000.0
                 ),
             )
             if e.retry_interval > 60 * 60 * 1000:
+                self._pending_pdus = []
                 self._pending_edus = []
                 self._pending_edus_keyed = {}
                 self._pending_presence = {}
                 self._pending_rrs = {}
-            self._start_catching_up()
         except FederationDeniedError as e:
             logger.info(e)
         except HttpResponseException as e:
             logger.warning(
                 "TX [%s] Received %d response to transaction: %s",
                 self._destination,
                 e.code,
                 e,
             )
-            self._start_catching_up()
         except RequestSendFailed as e:
             logger.warning(
                 "TX [%s] Failed to send transaction: %s", self._destination, e
             )
             for p in pending_pdus:
                 logger.info(
                     "Failed to send event %s to %s", p.event_id, self._destination
                 )
-            self._start_catching_up()
         except Exception:
             logger.exception("TX [%s] Failed to send transaction", self._destination)
             for p in pending_pdus:
                 logger.info(
                     "Failed to send event %s to %s", p.event_id, self._destination
                 )
-            self._start_catching_up()
         finally:
             self.transmission_loop_running = False
-    async def _catch_up_transmission_loop(self) -> None:
-        first_catch_up_check = self._last_successful_stream_ordering is None
-        if first_catch_up_check:
-            self._last_successful_stream_ordering = await self._store.get_destination_last_successful_stream_ordering(
-                self._destination
-            )
-        if self._last_successful_stream_ordering is None:
-            self._catching_up = False
-            return
-        while True:
-            event_ids = await self._store.get_catch_up_room_event_ids(
-                self._destination, self._last_successful_stream_ordering,
-            )
-            if not event_ids:
-                if self._catchup_last_skipped > self._last_successful_stream_ordering:
-                    continue
-                self._catching_up = False
-                break
-            if first_catch_up_check:
-                self._start_catching_up()
-            catchup_pdus = await self._store.get_events_as_list(event_ids)
-            if not catchup_pdus:
-                raise AssertionError(
-                    "No events retrieved when we asked for %r. "
-                    "This should not happen." % event_ids
-                )
-            if logger.isEnabledFor(logging.INFO):
-                rooms = [p.room_id for p in catchup_pdus]
-                logger.info("Catching up rooms to %s: %r", self._destination, rooms)
-            success = await self._transaction_manager.send_new_transaction(
-                self._destination, catchup_pdus, []
-            )
-            if not success:
-                return
-            sent_transactions_counter.inc()
-            final_pdu = catchup_pdus[-1]
-            self._last_successful_stream_ordering = cast(
-                int, final_pdu.internal_metadata.stream_ordering
-            )
-            await self._store.set_destination_last_successful_stream_ordering(
-                self._destination, self._last_successful_stream_ordering
-            )
     def _get_rr_edus(self, force_flush: bool) -> Iterable[Edu]:
         if not self._pending_rrs:
             return
         if not force_flush and not self._rrs_pending_flush:
             return
         edu = Edu(
             origin=self._server_name,
             destination=self._destination,
             edu_type="m.receipt",
             content=self._pending_rrs,
@@ -357,17 +294,10 @@
         edus = [
             Edu(
                 origin=self._server_name,
                 destination=self._destination,
                 edu_type="m.direct_to_device",
                 content=content,
             )
             for content in contents
         ]
         return (edus, stream_id)
-    def _start_catching_up(self) -> None:
-        """
-        Marks this destination as being in catch-up mode.
-        This throws away the PDU queue.
-        """
-        self._catching_up = True
-        self._pending_pdus = []

--- a/synapse/federation/sender/transaction_manager.py
+++ b/synapse/federation/sender/transaction_manager.py
@@ -1,47 +1,38 @@
 import logging
 from typing import TYPE_CHECKING, List
-from prometheus_client import Gauge
 from synapse.api.errors import HttpResponseException
 from synapse.events import EventBase
 from synapse.federation.persistence import TransactionActions
 from synapse.federation.units import Edu, Transaction
 from synapse.logging.opentracing import (
     extract_text_map,
     set_tag,
     start_active_span_follows_from,
     tags,
     whitelisted_homeserver,
 )
 from synapse.util import json_decoder
 from synapse.util.metrics import measure_func
 if TYPE_CHECKING:
     import synapse.server
 logger = logging.getLogger(__name__)
-last_pdu_age_metric = Gauge(
-    "synapse_federation_last_sent_pdu_age",
-    "The age (in seconds) of the last PDU successfully sent to the given domain",
-    labelnames=("server_name",),
-)
 class TransactionManager:
     """Helper class which handles building and sending transactions
     shared between PerDestinationQueue objects
     """
     def __init__(self, hs: "synapse.server.HomeServer"):
         self._server_name = hs.hostname
         self.clock = hs.get_clock()  # nb must be called this for @measure_func
         self._store = hs.get_datastore()
         self._transaction_actions = TransactionActions(self._store)
         self._transport_layer = hs.get_federation_transport_client()
-        self._federation_metrics_domains = (
-            hs.get_config().federation.federation_metrics_domains
-        )
         self._next_txn_id = int(self.clock.time_msec())
     @measure_func("_send_new_transaction")
     async def send_new_transaction(
         self, destination: str, pdus: List[EventBase], edus: List[Edu],
     ) -> bool:
         """
         Args:
             destination: The destination to send to (e.g. 'example.org')
             pdus: In-order list of PDUs to send
             edus: List of EDUs to send
@@ -120,18 +111,12 @@
                         )
             else:
                 for p in pdus:
                     logger.warning(
                         "TX [%s] {%s} Failed to send event %s",
                         destination,
                         txn_id,
                         p.event_id,
                     )
                 success = False
-            if success and pdus and destination in self._federation_metrics_domains:
-                last_pdu = pdus[-1]
-                last_pdu_age = self.clock.time_msec() - last_pdu.origin_server_ts
-                last_pdu_age_metric.labels(server_name=destination).set(
-                    last_pdu_age / 1000
-                )
             set_tag(tags.ERROR, not success)
             return success

--- a/synapse/federation/transport/server.py
+++ b/synapse/federation/transport/server.py
@@ -36,21 +36,21 @@
         Will by default register all servlets. For custom behaviour, pass in
         a list of servlet_groups to register.
         Args:
             hs (synapse.server.HomeServer): homeserver
             servlet_groups (list[str], optional): List of servlet groups to register.
                 Defaults to ``DEFAULT_SERVLET_GROUPS``.
         """
         self.hs = hs
         self.clock = hs.get_clock()
         self.servlet_groups = servlet_groups
-        super().__init__(hs, canonical_json=False)
+        super(TransportLayerServer, self).__init__(hs, canonical_json=False)
         self.authenticator = Authenticator(hs)
         self.ratelimiter = hs.get_federation_ratelimiter()
         self.register_servlets()
     def register_servlets(self):
         register_servlets(
             self.hs,
             resource=self,
             ratelimiter=self.ratelimiter,
             authenticator=self.authenticator,
             servlet_groups=self.servlet_groups,
@@ -258,21 +258,23 @@
             code = getattr(self, "on_%s" % (method), None)
             if code is None:
                 continue
             server.register_paths(
                 method, (pattern,), self._wrap(code), self.__class__.__name__,
             )
 class FederationSendServlet(BaseFederationServlet):
     PATH = "/send/(?P<transaction_id>[^/]*)/?"
     RATELIMIT = False
     def __init__(self, handler, server_name, **kwargs):
-        super().__init__(handler, server_name=server_name, **kwargs)
+        super(FederationSendServlet, self).__init__(
+            handler, server_name=server_name, **kwargs
+        )
         self.server_name = server_name
     async def on_PUT(self, origin, content, query, transaction_id):
         """ Called on PUT /send/<transaction_id>/
         Args:
             request (twisted.web.http.Request): The HTTP request.
             transaction_id (str): The transaction_id associated with this
                 request. This is *not* None.
         Returns:
             Tuple of `(code, response)`, where
             `response` is a python dict to be converted into JSON that is
@@ -521,21 +523,23 @@
                 "room_id": "!whkydVegtvatLfXmPN:localhost",
                 "world_readable": false
             }
         ],
         "end": "END",
         "start": "START"
     }
     """
     PATH = "/publicRooms"
     def __init__(self, handler, authenticator, ratelimiter, server_name, allow_access):
-        super().__init__(handler, authenticator, ratelimiter, server_name)
+        super(PublicRoomList, self).__init__(
+            handler, authenticator, ratelimiter, server_name
+        )
         self.allow_access = allow_access
     async def on_GET(self, origin, content, query):
         if not self.allow_access:
             raise FederationDeniedError(origin)
         limit = parse_integer_from_args(query, "limit", 0)
         since_token = parse_string_from_args(query, "since", None)
         include_all_networks = parse_boolean_from_args(
             query, "include_all_networks", False
         )
         third_party_instance_id = parse_string_from_args(

--- a/synapse/groups/groups_server.py
+++ b/synapse/groups/groups_server.py
@@ -221,21 +221,21 @@
                 room_id, len(joined_users), with_alias=False, allow_private=True
             )
             if not entry:
                 continue
             entry["is_public"] = bool(room_result["is_public"])
             chunk.append(entry)
         chunk.sort(key=lambda e: -e["num_joined_members"])
         return {"chunk": chunk, "total_room_count_estimate": len(room_results)}
 class GroupsServerHandler(GroupsServerWorkerHandler):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(GroupsServerHandler, self).__init__(hs)
         hs.get_groups_attestation_renewer()
     async def update_group_summary_room(
         self, group_id, requester_user_id, room_id, category_id, content
     ):
         """Add/update a room to the group summary
         """
         await self.check_group_is_ours(
             group_id, requester_user_id, and_exists=True, and_is_admin=requester_user_id
         )
         RoomID.from_string(room_id)  # Ensure valid room id

--- a/synapse/handlers/acme_issuing_service.py
+++ b/synapse/handlers/acme_issuing_service.py
@@ -40,21 +40,21 @@
             lambda: Client.from_url(
                 reactor=reactor,
                 url=URL.from_text(acme_url),
                 key=load_or_create_client_key(account_key_file),
                 alg=RS256,
             )
         ),
         clock=reactor,
         responders=[responder],
     )
-@attr.s(slots=True)
+@attr.s
 @implementer(ICertificateStore)
 class ErsatzStore:
     """
     A store that only stores in memory.
     """
     certs = attr.ib(default=attr.Factory(dict))
     def store(self, server_name, pem_objects):
         self.certs[server_name] = [o.as_bytes() for o in pem_objects]
         return defer.succeed(None)
 def load_or_create_client_key(key_file):

--- a/synapse/handlers/admin.py
+++ b/synapse/handlers/admin.py
@@ -1,21 +1,21 @@
 import logging
 from typing import List
 from synapse.api.constants import Membership
 from synapse.events import FrozenEvent
 from synapse.types import RoomStreamToken, StateMap
 from synapse.visibility import filter_events_for_client
 from ._base import BaseHandler
 logger = logging.getLogger(__name__)
 class AdminHandler(BaseHandler):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(AdminHandler, self).__init__(hs)
         self.storage = hs.get_storage()
         self.state_store = self.storage.state
     async def get_whois(self, user):
         connections = []
         sessions = await self.store.get_user_ip_and_agents(user)
         for session in sessions:
             connections.append(
                 {
                     "ip": session["ip"],
                     "last_seen": session["last_seen"],
@@ -70,22 +70,22 @@
                     event_id = room.event_id
                     invite = await self.store.get_event(event_id, allow_none=True)
                     if invite:
                         invited_state = invite.unsigned["invite_room_state"]
                         writer.write_invite(room_id, invite, invited_state)
                 continue
             if room.membership == Membership.JOIN:
                 stream_ordering = self.store.get_room_max_stream_ordering()
             else:
                 stream_ordering = room.stream_ordering
-            from_key = RoomStreamToken(0, 0)
-            to_key = RoomStreamToken(None, stream_ordering)
+            from_key = str(RoomStreamToken(0, 0))
+            to_key = str(RoomStreamToken(None, stream_ordering))
             written_events = set()  # Events that we've processed in this room
             event_to_unseen_prevs = {}
             unseen_to_child_events = {}
             while True:
                 events, _ = await self.store.paginate_room_events(
                     room_id, from_key, to_key, limit=100, direction="f"
                 )
                 if not events:
                     break
                 from_key = events[-1].internal_metadata.after

--- a/synapse/handlers/auth.py
+++ b/synapse/handlers/auth.py
@@ -81,33 +81,28 @@
         raise SynapseError(
             400, "Invalid phone-type identifier", errcode=Codes.INVALID_PARAM
         )
     phone_number = identifier.get("phone", identifier["number"])
     msisdn = phone_number_to_msisdn(identifier["country"], phone_number)
     return {
         "type": "m.id.thirdparty",
         "medium": "msisdn",
         "address": msisdn,
     }
-@attr.s(slots=True)
-class SsoLoginExtraAttributes:
-    """Data we track about SAML2 sessions"""
-    creation_time = attr.ib(type=int)
-    extra_attributes = attr.ib(type=JsonDict)
 class AuthHandler(BaseHandler):
     SESSION_EXPIRE_MS = 48 * 60 * 60 * 1000
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer):
         """
-        super().__init__(hs)
+        super(AuthHandler, self).__init__(hs)
         self.checkers = {}  # type: Dict[str, UserInteractiveAuthChecker]
         for auth_checker_class in INTERACTIVE_AUTH_CHECKERS:
             inst = auth_checker_class(hs)
             if inst.is_enabled():
                 self.checkers[inst.AUTH_TYPE] = inst  # type: ignore
         self.bcrypt_rounds = hs.config.bcrypt_rounds
         account_handler = ModuleApi(hs, self)
         self.password_providers = [
             module(config=config, account_handler=account_handler)
             for module, config in hs.config.password_providers
@@ -146,21 +141,20 @@
                 self._expire_old_sessions,
             )
         self._sso_redirect_confirm_template = hs.config.sso_redirect_confirm_template
         self._sso_auth_confirm_template = hs.config.sso_auth_confirm_template
         self._sso_auth_success_template = hs.config.sso_auth_success_template
         self._sso_account_deactivated_template = (
             hs.config.sso_account_deactivated_template
         )
         self._server_name = hs.config.server_name
         self._whitelisted_sso_clients = tuple(hs.config.sso_client_whitelist)
-        self._extra_attributes = {}  # type: Dict[str, SsoLoginExtraAttributes]
     async def validate_user_via_ui_auth(
         self,
         requester: Requester,
         request: SynapseRequest,
         request_body: Dict[str, Any],
         clientip: str,
         description: str,
     ) -> Tuple[dict, str]:
         """
         Checks that the user is who they claim to be, via a UI auth.
@@ -814,102 +808,68 @@
         await self.store.mark_ui_auth_stage_complete(
             session_id, LoginType.SSO, registered_user_id
         )
         html = self._sso_auth_success_template
         respond_with_html(request, 200, html)
     async def complete_sso_login(
         self,
         registered_user_id: str,
         request: SynapseRequest,
         client_redirect_url: str,
-        extra_attributes: Optional[JsonDict] = None,
     ):
         """Having figured out a mxid for this user, complete the HTTP request
         Args:
             registered_user_id: The registered user ID to complete SSO login for.
             request: The request to complete.
             client_redirect_url: The URL to which to redirect the user at the end of the
                 process.
-            extra_attributes: Extra attributes which will be passed to the client
-                during successful login. Must be JSON serializable.
         """
         deactivated = await self.store.get_user_deactivated_status(registered_user_id)
         if deactivated:
             respond_with_html(request, 403, self._sso_account_deactivated_template)
             return
-        self._complete_sso_login(
-            registered_user_id, request, client_redirect_url, extra_attributes
-        )
+        self._complete_sso_login(registered_user_id, request, client_redirect_url)
     def _complete_sso_login(
         self,
         registered_user_id: str,
         request: SynapseRequest,
         client_redirect_url: str,
-        extra_attributes: Optional[JsonDict] = None,
     ):
         """
         The synchronous portion of complete_sso_login.
         This exists purely for backwards compatibility of synapse.module_api.ModuleApi.
         """
-        if extra_attributes:
-            self._extra_attributes[registered_user_id] = SsoLoginExtraAttributes(
-                self._clock.time_msec(), extra_attributes,
-            )
         login_token = self.macaroon_gen.generate_short_term_login_token(
             registered_user_id
         )
         redirect_url = self.add_query_param_to_url(
             client_redirect_url, "loginToken", login_token
         )
         if client_redirect_url.startswith(self._whitelisted_sso_clients):
             request.redirect(redirect_url)
             finish_request(request)
             return
         redirect_url_no_params = client_redirect_url.split("?")[0]
         html = self._sso_redirect_confirm_template.render(
             display_url=redirect_url_no_params,
             redirect_url=redirect_url,
             server_name=self._server_name,
         )
         respond_with_html(request, 200, html)
-    async def _sso_login_callback(self, login_result: JsonDict) -> None:
-        """
-        A login callback which might add additional attributes to the login response.
-        Args:
-            login_result: The data to be sent to the client. Includes the user
-                ID and access token.
-        """
-        self._expire_sso_extra_attributes()
-        extra_attributes = self._extra_attributes.get(login_result["user_id"])
-        if extra_attributes:
-            login_result.update(extra_attributes.extra_attributes)
-    def _expire_sso_extra_attributes(self) -> None:
-        """
-        Iterate through the mapping of user IDs to extra attributes and remove any that are no longer valid.
-        """
-        LOGIN_TOKEN_EXPIRATION_TIME = 2 * 60 * 1000
-        expire_before = self._clock.time_msec() - LOGIN_TOKEN_EXPIRATION_TIME
-        to_expire = set()
-        for user_id, data in self._extra_attributes.items():
-            if data.creation_time < expire_before:
-                to_expire.add(user_id)
-        for user_id in to_expire:
-            logger.debug("Expiring extra attributes for user %s", user_id)
-            del self._extra_attributes[user_id]
     @staticmethod
     def add_query_param_to_url(url: str, param_name: str, param: Any):
         url_parts = list(urllib.parse.urlparse(url))
         query = dict(urllib.parse.parse_qsl(url_parts[4]))
         query.update({param_name: param})
         url_parts[4] = urllib.parse.urlencode(query)
         return urllib.parse.urlunparse(url_parts)
-@attr.s(slots=True)
+@attr.s
 class MacaroonGenerator:
     hs = attr.ib()
     def generate_access_token(
         self, user_id: str, extra_caveats: Optional[List[str]] = None
     ) -> str:
         extra_caveats = extra_caveats or []
         macaroon = self._generate_base_macaroon(user_id)
         macaroon.add_first_party_caveat("type = access")
         macaroon.add_first_party_caveat(
             "nonce = %s" % (stringutils.random_string_with_symbols(16),)

--- a/synapse/handlers/deactivate_account.py
+++ b/synapse/handlers/deactivate_account.py
@@ -1,21 +1,21 @@
 import logging
 from typing import Optional
 from synapse.api.errors import SynapseError
 from synapse.metrics.background_process_metrics import run_as_background_process
 from synapse.types import UserID, create_requester
 from ._base import BaseHandler
 logger = logging.getLogger(__name__)
 class DeactivateAccountHandler(BaseHandler):
     """Handler which deals with deactivating user accounts."""
     def __init__(self, hs):
-        super().__init__(hs)
+        super(DeactivateAccountHandler, self).__init__(hs)
         self.hs = hs
         self._auth_handler = hs.get_auth_handler()
         self._device_handler = hs.get_device_handler()
         self._room_member_handler = hs.get_room_member_handler()
         self._identity_handler = hs.get_handlers().identity_handler
         self.user_directory_handler = hs.get_user_directory_handler()
         self._user_parter_running = False
         if hs.config.worker_app is None:
             hs.get_reactor().callWhenRunning(self._start_user_parting)
         self._account_validity_enabled = hs.config.account_validity.enabled

--- a/synapse/handlers/device.py
+++ b/synapse/handlers/device.py
@@ -1,39 +1,38 @@
 import logging
 from typing import Any, Dict, List, Optional
 from synapse.api import errors
 from synapse.api.constants import EventTypes
 from synapse.api.errors import (
-    Codes,
     FederationDeniedError,
     HttpResponseException,
     RequestSendFailed,
     SynapseError,
 )
 from synapse.logging.opentracing import log_kv, set_tag, trace
 from synapse.metrics.background_process_metrics import run_as_background_process
 from synapse.types import (
-    StreamToken,
+    RoomStreamToken,
     get_domain_from_id,
     get_verify_key_from_cross_signing_key,
 )
 from synapse.util import stringutils
 from synapse.util.async_helpers import Linearizer
 from synapse.util.caches.expiringcache import ExpiringCache
 from synapse.util.metrics import measure_func
 from synapse.util.retryutils import NotRetryingDestination
 from ._base import BaseHandler
 logger = logging.getLogger(__name__)
 MAX_DEVICE_DISPLAY_NAME_LEN = 100
 class DeviceWorkerHandler(BaseHandler):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(DeviceWorkerHandler, self).__init__(hs)
         self.hs = hs
         self.state = hs.get_state_handler()
         self.state_store = hs.get_storage().state
         self._auth_handler = hs.get_auth_handler()
     @trace
     async def get_devices_by_user(self, user_id: str) -> List[Dict[str, Any]]:
         """
         Retrieve the given user's devices
         Args:
             user_id: The user ID to query for devices.
@@ -63,42 +62,45 @@
             device = await self.store.get_device(user_id, device_id)
         except errors.StoreError:
             raise errors.NotFoundError
         ips = await self.store.get_last_client_ip_by_device(user_id, device_id)
         _update_device_from_client_ips(device, ips)
         set_tag("device", device)
         set_tag("ips", ips)
         return device
     @trace
     @measure_func("device.get_user_ids_changed")
-    async def get_user_ids_changed(self, user_id: str, from_token: StreamToken):
+    async def get_user_ids_changed(self, user_id, from_token):
         """Get list of users that have had the devices updated, or have newly
         joined a room, that `user_id` may be interested in.
+        Args:
+            user_id (str)
+            from_token (StreamToken)
         """
         set_tag("user_id", user_id)
         set_tag("from_token", from_token)
-        now_room_key = self.store.get_room_max_token()
+        now_room_key = await self.store.get_room_events_max_id()
         room_ids = await self.store.get_rooms_for_user(user_id)
         users_who_share_room = await self.store.get_users_who_share_room_with_user(
             user_id
         )
         tracked_users = set(users_who_share_room)
         tracked_users.add(user_id)
         changed = await self.store.get_users_whose_devices_changed(
             from_token.device_list_key, tracked_users
         )
         rooms_changed = self.store.get_rooms_that_changed(room_ids, from_token.room_key)
         member_events = await self.store.get_membership_changes_for_user(
             user_id, from_token.room_key, now_room_key
         )
         rooms_changed.update(event.room_id for event in member_events)
-        stream_ordering = from_token.room_key.stream
+        stream_ordering = RoomStreamToken.parse_stream_token(from_token.room_key).stream
         possibly_changed = set(changed)
         possibly_left = set()
         for room_id in rooms_changed:
             current_state_ids = await self.store.get_current_state_ids(room_id)
             if room_id not in room_ids:
                 for key, event_id in current_state_ids.items():
                     etype, state_key = key
                     if etype != EventTypes.Member:
                         continue
                     possibly_left.add(state_key)
@@ -161,59 +163,43 @@
         )
         return {
             "user_id": user_id,
             "stream_id": stream_id,
             "devices": devices,
             "master_key": master_key,
             "self_signing_key": self_signing_key,
         }
 class DeviceHandler(DeviceWorkerHandler):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(DeviceHandler, self).__init__(hs)
         self.federation_sender = hs.get_federation_sender()
         self.device_list_updater = DeviceListUpdater(hs, self)
         federation_registry = hs.get_federation_registry()
         federation_registry.register_edu_handler(
             "m.device_list_update", self.device_list_updater.incoming_device_list_update
         )
         hs.get_distributor().observe("user_left_room", self.user_left_room)
-    def _check_device_name_length(self, name: str):
-        """
-        Checks whether a device name is longer than the maximum allowed length.
-        Args:
-            name: The name of the device.
-        Raises:
-            SynapseError: if the device name is too long.
-        """
-        if name and len(name) > MAX_DEVICE_DISPLAY_NAME_LEN:
-            raise SynapseError(
-                400,
-                "Device display name is too long (max %i)"
-                % (MAX_DEVICE_DISPLAY_NAME_LEN,),
-                errcode=Codes.TOO_LARGE,
-            )
     async def check_device_registered(
         self, user_id, device_id, initial_device_display_name=None
     ):
         """
         If the given device has not been registered, register it with the
         supplied display name.
         If no device_id is supplied, we make one up.
         Args:
             user_id (str):  @user:id
             device_id (str | None): device id supplied by client
             initial_device_display_name (str | None): device display name from
                  client
         Returns:
             str: device id (generated if none was supplied)
         """
-        self._check_device_name_length(initial_device_display_name)
         if device_id is not None:
             new_device = await self.store.store_device(
                 user_id=user_id,
                 device_id=device_id,
                 initial_device_display_name=initial_device_display_name,
             )
             if new_device:
                 await self.notify_device_update(user_id, [device_id])
             return device_id
         attempts = 0
@@ -290,21 +276,26 @@
             )
         await self.notify_device_update(user_id, device_ids)
     async def update_device(self, user_id: str, device_id: str, content: dict) -> None:
         """ Update the given device
         Args:
             user_id: The user to update devices of.
             device_id: The device to update.
             content: body of update request
         """
         new_display_name = content.get("display_name")
-        self._check_device_name_length(new_display_name)
+        if new_display_name and len(new_display_name) > MAX_DEVICE_DISPLAY_NAME_LEN:
+            raise SynapseError(
+                400,
+                "Device display name is too long (max %i)"
+                % (MAX_DEVICE_DISPLAY_NAME_LEN,),
+            )
         try:
             await self.store.update_device(
                 user_id, device_id, new_display_name=new_display_name
             )
             await self.notify_device_update(user_id, [device_id])
         except errors.StoreError as e:
             if e.code == 404:
                 raise errors.NotFoundError()
             else:
                 raise

--- a/synapse/handlers/directory.py
+++ b/synapse/handlers/directory.py
@@ -10,21 +10,21 @@
     ShadowBanError,
     StoreError,
     SynapseError,
 )
 from synapse.appservice import ApplicationService
 from synapse.types import Requester, RoomAlias, UserID, get_domain_from_id
 from ._base import BaseHandler
 logger = logging.getLogger(__name__)
 class DirectoryHandler(BaseHandler):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(DirectoryHandler, self).__init__(hs)
         self.state = hs.get_state_handler()
         self.appservice_handler = hs.get_application_service_handler()
         self.event_creation_handler = hs.get_event_creation_handler()
         self.store = hs.get_datastore()
         self.config = hs.config
         self.enable_room_list_search = hs.config.enable_room_list_search
         self.require_membership = hs.config.require_membership_for_aliases
         self.federation = hs.get_federation_client()
         hs.get_federation_registry().register_query_handler(
             "directory", self.on_directory_query

--- a/synapse/handlers/e2e_keys.py
+++ b/synapse/handlers/e2e_keys.py
@@ -867,21 +867,21 @@
         return {"status": 503, "message": "Not ready for retry"}
     return {"status": 503, "message": str(e)}
 def _one_time_keys_match(old_key_json, new_key):
     old_key = json_decoder.decode(old_key_json)
     if not isinstance(old_key, dict) or not isinstance(new_key, dict):
         return old_key == new_key
     old_key.pop("signatures", None)
     new_key_copy = dict(new_key)
     new_key_copy.pop("signatures", None)
     return old_key == new_key_copy
-@attr.s(slots=True)
+@attr.s
 class SignatureListItem:
     """An item in the signature list as used by upload_signatures_for_device_keys.
     """
     signing_key_id = attr.ib()
     target_user_id = attr.ib()
     target_device_id = attr.ib()
     signature = attr.ib()
 class SigningKeyEduUpdater:
     """Handles incoming signing key updates from federation and updates the DB"""
     def __init__(self, hs, e2e_keys_handler):

--- a/synapse/handlers/events.py
+++ b/synapse/handlers/events.py
@@ -8,21 +8,24 @@
 from synapse.logging.utils import log_function
 from synapse.streams.config import PaginationConfig
 from synapse.types import JsonDict, UserID
 from synapse.visibility import filter_events_for_client
 from ._base import BaseHandler
 if TYPE_CHECKING:
     from synapse.server import HomeServer
 logger = logging.getLogger(__name__)
 class EventStreamHandler(BaseHandler):
     def __init__(self, hs: "HomeServer"):
-        super().__init__(hs)
+        super(EventStreamHandler, self).__init__(hs)
+        self.distributor = hs.get_distributor()
+        self.distributor.declare("started_user_eventstream")
+        self.distributor.declare("stopped_user_eventstream")
         self.clock = hs.get_clock()
         self.notifier = hs.get_notifier()
         self.state = hs.get_state_handler()
         self._server_notices_sender = hs.get_server_notices_sender()
         self._event_serializer = hs.get_event_client_serializer()
     @log_function
     async def get_stream(
         self,
         auth_user_id: str,
         pagin_config: PaginationConfig,
@@ -79,27 +82,27 @@
                     )
             events.extend(to_add)
             chunks = await self._event_serializer.serialize_events(
                 events,
                 time_now,
                 as_client_event=as_client_event,
                 bundle_aggregations=False,
             )
             chunk = {
                 "chunk": chunks,
-                "start": await tokens[0].to_string(self.store),
-                "end": await tokens[1].to_string(self.store),
+                "start": tokens[0].to_string(),
+                "end": tokens[1].to_string(),
             }
             return chunk
 class EventHandler(BaseHandler):
     def __init__(self, hs: "HomeServer"):
-        super().__init__(hs)
+        super(EventHandler, self).__init__(hs)
         self.storage = hs.get_storage()
     async def get_event(
         self, user: UserID, room_id: Optional[str], event_id: str
     ) -> Optional[EventBase]:
         """Retrieve a single specified event.
         Args:
             user: The user requesting the event
             room_id: The expected room id. We'll return None if the
                 event's room does not match.
             event_id: The event ID to obtain.

--- a/synapse/handlers/federation.py
+++ b/synapse/handlers/federation.py
@@ -1,16 +1,16 @@
 """Contains handlers for federation events."""
 import itertools
 import logging
 from collections.abc import Container
 from http import HTTPStatus
-from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple, Union
 import attr
 from signedjson.key import decode_verify_key_bytes
 from signedjson.sign import verify_signed_json
 from unpaddedbase64 import decode_base64
 from twisted.internet import defer
 from synapse import event_auth
 from synapse.api.constants import (
     EventTypes,
     Membership,
     RejectedReason,
@@ -41,95 +41,96 @@
     run_in_background,
 )
 from synapse.logging.utils import log_function
 from synapse.metrics.background_process_metrics import run_as_background_process
 from synapse.replication.http.devices import ReplicationUserDevicesResyncRestServlet
 from synapse.replication.http.federation import (
     ReplicationCleanRoomRestServlet,
     ReplicationFederationSendEventsRestServlet,
     ReplicationStoreRoomOnInviteRestServlet,
 )
-from synapse.state import StateResolutionStore
+from synapse.replication.http.membership import ReplicationUserJoinedLeftRoomRestServlet
+from synapse.state import StateResolutionStore, resolve_events_with_store
 from synapse.storage.databases.main.events_worker import EventRedactBehaviour
 from synapse.types import (
     JsonDict,
     MutableStateMap,
-    PersistedEventPosition,
-    RoomStreamToken,
     StateMap,
     UserID,
     get_domain_from_id,
 )
 from synapse.util.async_helpers import Linearizer, concurrently_execute
+from synapse.util.distributor import user_joined_room
 from synapse.util.retryutils import NotRetryingDestination
 from synapse.util.stringutils import shortstr
 from synapse.visibility import filter_events_for_server
-if TYPE_CHECKING:
-    from synapse.server import HomeServer
 logger = logging.getLogger(__name__)
-@attr.s(slots=True)
+@attr.s
 class _NewEventInfo:
     """Holds information about a received event, ready for passing to _handle_new_events
     Attributes:
         event: the received event
         state: the state at that event
         auth_events: the auth_event map for that event
     """
     event = attr.ib(type=EventBase)
     state = attr.ib(type=Optional[Sequence[EventBase]], default=None)
     auth_events = attr.ib(type=Optional[MutableStateMap[EventBase]], default=None)
 class FederationHandler(BaseHandler):
     """Handles events that originated from federation.
         Responsible for:
         a) handling received Pdus before handing them on as Events to the rest
         of the homeserver (including auth and state conflict resoultion)
         b) converting events that were produced by local clients that may need
         to be sent to remote homeservers.
         c) doing the necessary dances to invite remote users and join remote
         rooms.
     """
-    def __init__(self, hs: "HomeServer"):
-        super().__init__(hs)
+    def __init__(self, hs):
+        super(FederationHandler, self).__init__(hs)
         self.hs = hs
         self.store = hs.get_datastore()
         self.storage = hs.get_storage()
         self.state_store = self.storage.state
         self.federation_client = hs.get_federation_client()
         self.state_handler = hs.get_state_handler()
-        self._state_resolution_handler = hs.get_state_resolution_handler()
         self.server_name = hs.hostname
         self.keyring = hs.get_keyring()
         self.action_generator = hs.get_action_generator()
         self.is_mine_id = hs.is_mine_id
+        self.pusher_pool = hs.get_pusherpool()
         self.spam_checker = hs.get_spam_checker()
         self.event_creation_handler = hs.get_event_creation_handler()
         self._message_handler = hs.get_message_handler()
         self._server_notices_mxid = hs.config.server_notices_mxid
         self.config = hs.config
         self.http_client = hs.get_simple_http_client()
         self._instance_name = hs.get_instance_name()
         self._replication = hs.get_replication_data_handler()
         self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)
+        self._notify_user_membership_change = ReplicationUserJoinedLeftRoomRestServlet.make_client(
+            hs
+        )
         self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(
             hs
         )
         if hs.config.worker_app:
             self._user_device_resync = ReplicationUserDevicesResyncRestServlet.make_client(
                 hs
             )
             self._maybe_store_room_on_invite = ReplicationStoreRoomOnInviteRestServlet.make_client(
                 hs
             )
         else:
             self._device_list_updater = hs.get_device_handler().device_list_updater
             self._maybe_store_room_on_invite = self.store.maybe_store_room_on_invite
-        self.room_queues = {}  # type: Dict[str, List[Tuple[EventBase, str]]]
+        self.room_queues = {}
         self._room_pdu_linearizer = Linearizer("fed_room_pdu")
         self.third_party_event_rules = hs.get_third_party_event_rules()
         self._ephemeral_messages_enabled = hs.config.enable_ephemeral_messages
     async def on_receive_pdu(self, origin, pdu, sent_to_us_directly=False) -> None:
         """ Process a PDU received via a federation /send/ transaction, or
         via backfill of missing prev_events
         Args:
             origin (str): server which initiated the /send/ transaction. Will
                 be used to fetch missing events or state.
             pdu (FrozenEvent): received PDU
@@ -200,21 +201,21 @@
                             len(missing_prevs),
                         )
                         try:
                             await self._get_missing_events_for_pdu(
                                 origin, pdu, prevs, min_depth
                             )
                         except Exception as e:
                             raise Exception(
                                 "Error fetching missing prev_events for %s: %s"
                                 % (event_id, e)
-                            ) from e
+                            )
                         seen = await self.store.have_events_in_timeline(prevs)
                         if not prevs - seen:
                             logger.info(
                                 "[%s %s] Found all missing prev_events",
                                 room_id,
                                 event_id,
                             )
             if prevs - seen:
                 if sent_to_us_directly:
                     logger.warning(
@@ -251,21 +252,22 @@
                             (remote_state, _,) = await self._get_state_for_room(
                                 origin, room_id, p, include_event_in_state=True
                             )
                             remote_state_map = {
                                 (x.type, x.state_key): x.event_id for x in remote_state
                             }
                             state_maps.append(remote_state_map)
                             for x in remote_state:
                                 event_map[x.event_id] = x
                     room_version = await self.store.get_room_version_id(room_id)
-                    state_map = await self._state_resolution_handler.resolve_events_with_store(
+                    state_map = await resolve_events_with_store(
+                        self.clock,
                         room_id,
                         room_version,
                         state_maps,
                         event_map,
                         state_res_store=StateResolutionStore(self.store),
                     )
                     evs = await self.store.get_events(
                         list(state_map.values()),
                         get_prev_content=False,
                         redact_behaviour=EventRedactBehaviour.AS_IS,
@@ -456,23 +458,37 @@
             origin: server sending the event
             event: event to be persisted
             state: Normally None, but if we are handling a gap in the graph
                 (ie, we are missing one or more prev_events), the resolved state at the
                 event
         """
         room_id = event.room_id
         event_id = event.event_id
         logger.debug("[%s %s] Processing event: %s", room_id, event_id, event)
         try:
-            await self._handle_new_event(origin, event, state=state)
+            context = await self._handle_new_event(origin, event, state=state)
         except AuthError as e:
             raise FederationError("ERROR", e.code, e.msg, affected=event.event_id)
+        if event.type == EventTypes.Member:
+            if event.membership == Membership.JOIN:
+                newly_joined = True
+                prev_state_ids = await context.get_prev_state_ids()
+                prev_state_id = prev_state_ids.get((event.type, event.state_key))
+                if prev_state_id:
+                    prev_state = await self.store.get_event(
+                        prev_state_id, allow_none=True
+                    )
+                    if prev_state and prev_state.membership == Membership.JOIN:
+                        newly_joined = False
+                if newly_joined:
+                    user = UserID.from_string(event.state_key)
+                    await self.user_joined_room(user, room_id)
         if event.type == EventTypes.Encrypted:
             device_id = event.content.get("device_id")
             sender_key = event.content.get("sender_key")
             cached_devices = await self.store.get_cached_devices_for_user(event.sender)
             resync = False  # Whether we should resync device lists.
             device = None
             if device_id is not None:
                 device = cached_devices.get(device_id)
                 if device is None:
                     logger.info(
@@ -535,22 +551,20 @@
         this method throws a SynapseError.
         TODO: make this more useful to distinguish failures of the remote
         server from invalid events (there is probably no point in trying to
         re-fetch invalid events from every other HS in the room.)
         """
         if dest == self.server_name:
             raise SynapseError(400, "Can't backfill from self.")
         events = await self.federation_client.backfill(
             dest, room_id, limit=limit, extremities=extremities
         )
-        if not events:
-            return []
         seen_events = await self.store.have_events_in_timeline(
             {e.event_id for e in events}
         )
         events = [e for e in events if e.event_id not in seen_events]
         if not events:
             return []
         event_map = {e.event_id: e for e in events}
         event_ids = {e.event_id for e in events}
         edges = [ev.event_id for ev in events if set(ev.prev_event_ids()) - event_ids]
         logger.info("backfill: Got %d events with %d edges", len(events), len(edges))
@@ -589,22 +603,21 @@
                     auth_events={
                         (
                             auth_events[a_id].type,
                             auth_events[a_id].state_key,
                         ): auth_events[a_id]
                         for a_id in ev.auth_event_ids()
                         if a_id in auth_events
                     },
                 )
             )
-        if ev_infos:
-            await self._handle_new_events(dest, room_id, ev_infos, backfilled=True)
+        await self._handle_new_events(dest, ev_infos, backfilled=True)
         events.sort(key=lambda e: e.depth)
         for event in events:
             if event in events_to_state:
                 continue
             assert not event.internal_metadata.is_outlier()
             await self._handle_new_event(dest, event, backfilled=True)
         return events
     async def maybe_backfill(
         self, room_id: str, current_depth: int, limit: int
     ) -> bool:
@@ -804,21 +817,21 @@
         for event in event_map.values():
             auth = {}
             for auth_event_id in event.auth_event_ids():
                 ae = persisted_events.get(auth_event_id) or event_map.get(auth_event_id)
                 if ae:
                     auth[(ae.type, ae.state_key)] = ae
                 else:
                     logger.info("Missing auth event %s", auth_event_id)
             event_infos.append(_NewEventInfo(event, None, auth))
         await self._handle_new_events(
-            destination, room_id, event_infos,
+            destination, event_infos,
         )
     def _sanity_check_event(self, ev):
         """
         Do some early sanity checks of a received event
         In particular, checks it doesn't have an excessive number of
         prev_events or auth_events, which could cause a huge state resolution
         or cascade of event fetches.
         Args:
             ev (synapse.events.EventBase): event to be checked
         Returns: None
@@ -904,26 +917,24 @@
             handled_events.update([s.event_id for s in state])
             handled_events.update([a.event_id for a in auth_chain])
             handled_events.add(event.event_id)
             logger.debug("do_invite_join auth_chain: %s", auth_chain)
             logger.debug("do_invite_join state: %s", state)
             logger.debug("do_invite_join event: %s", event)
             await self.store.upsert_room_on_join(
                 room_id=room_id, room_version=room_version_obj,
             )
             max_stream_id = await self._persist_auth_tree(
-                origin, room_id, auth_chain, state, event, room_version_obj
+                origin, auth_chain, state, event, room_version_obj
             )
             await self._replication.wait_for_stream_position(
-                self.config.worker.events_shard_config.get_instance(room_id),
-                "events",
-                max_stream_id,
+                self.config.worker.writers.events, "events", max_stream_id
             )
             predecessor = await self.store.get_room_predecessor(room_id)
             if not predecessor or not isinstance(predecessor.get("room_id"), str):
                 return event.event_id, max_stream_id
             old_room_id = predecessor["room_id"]
             logger.debug(
                 "Found predecessor for %s during remote join: %s", room_id, old_room_id
             )
             member_handler = self.hs.get_room_member_handler()
             await member_handler.transfer_room_state_on_room_upgrade(
@@ -1037,20 +1048,24 @@
         if not event_allowed:
             logger.info("Sending of join %s forbidden by third-party rules", event)
             raise SynapseError(
                 403, "This event is not allowed in this context", Codes.FORBIDDEN
             )
         logger.debug(
             "on_send_join_request: After _handle_new_event: %s, sigs: %s",
             event.event_id,
             event.signatures,
         )
+        if event.type == EventTypes.Member:
+            if event.content["membership"] == Membership.JOIN:
+                user = UserID.from_string(event.state_key)
+                await self.user_joined_room(user, event.room_id)
         prev_state_ids = await context.get_prev_state_ids()
         state_ids = list(prev_state_ids.values())
         auth_chain = await self.store.get_auth_chain(state_ids)
         state = await self.store.get_events(list(prev_state_ids.values()))
         return {"state": list(state.values()), "auth_chain": auth_chain}
     async def on_invite_request(
         self, origin: str, event: EventBase, room_version: RoomVersion
     ):
         """ We've got an invite event. Process and persist it. Sign it.
         Respond with the now signed event.
@@ -1087,41 +1102,39 @@
         event.internal_metadata.out_of_band_membership = True
         event.signatures.update(
             compute_event_signature(
                 room_version,
                 event.get_pdu_json(),
                 self.hs.hostname,
                 self.hs.signing_key,
             )
         )
         context = await self.state_handler.compute_event_context(event)
-        await self.persist_events_and_notify(event.room_id, [(event, context)])
+        await self.persist_events_and_notify([(event, context)])
         return event
     async def do_remotely_reject_invite(
         self, target_hosts: Iterable[str], room_id: str, user_id: str, content: JsonDict
     ) -> Tuple[EventBase, int]:
         origin, event, room_version = await self._make_and_verify_event(
             target_hosts, room_id, user_id, "leave", content=content
         )
         event.internal_metadata.outlier = True
         event.internal_metadata.out_of_band_membership = True
         host_list = list(target_hosts)
         try:
             host_list.remove(origin)
             host_list.insert(0, origin)
         except ValueError:
             pass
         await self.federation_client.send_leave(host_list, event)
         context = await self.state_handler.compute_event_context(event)
-        stream_id = await self.persist_events_and_notify(
-            event.room_id, [(event, context)]
-        )
+        stream_id = await self.persist_events_and_notify([(event, context)])
         return event, stream_id
     async def _make_and_verify_event(
         self,
         target_hosts: Iterable[str],
         room_id: str,
         user_id: str,
         membership: str,
         content: JsonDict = {},
         params: Optional[Dict[str, Union[str, Iterable[str]]]] = None,
     ) -> Tuple[str, EventBase, RoomVersion]:
@@ -1303,32 +1316,31 @@
         try:
             if (
                 not event.internal_metadata.is_outlier()
                 and not backfilled
                 and not context.rejected
             ):
                 await self.action_generator.handle_push_actions_for_event(
                     event, context
                 )
             await self.persist_events_and_notify(
-                event.room_id, [(event, context)], backfilled=backfilled
+                [(event, context)], backfilled=backfilled
             )
         except Exception:
             run_in_background(
                 self.store.remove_push_actions_from_staging, event.event_id
             )
             raise
         return context
     async def _handle_new_events(
         self,
         origin: str,
-        room_id: str,
         event_infos: Iterable[_NewEventInfo],
         backfilled: bool = False,
     ) -> None:
         """Creates the appropriate contexts and persists events. The events
         should not depend on one another, e.g. this should be used to persist
         a bunch of outliers, but not a chunk of individual events that depend
         on each other for state calculations.
         Notifies about the events where appropriate.
         """
         async def prep(ev_info: _NewEventInfo):
@@ -1342,44 +1354,41 @@
                     backfilled=backfilled,
                 )
             return res
         contexts = await make_deferred_yieldable(
             defer.gatherResults(
                 [run_in_background(prep, ev_info) for ev_info in event_infos],
                 consumeErrors=True,
             )
         )
         await self.persist_events_and_notify(
-            room_id,
             [
                 (ev_info.event, context)
                 for ev_info, context in zip(event_infos, contexts)
             ],
             backfilled=backfilled,
         )
     async def _persist_auth_tree(
         self,
         origin: str,
-        room_id: str,
         auth_events: List[EventBase],
         state: List[EventBase],
         event: EventBase,
         room_version: RoomVersion,
     ) -> int:
         """Checks the auth chain is valid (and passes auth checks) for the
         state and event. Then persists the auth chain and state atomically.
         Persists the event separately. Notifies about the persisted events
         where appropriate.
         Will attempt to fetch missing auth events.
         Args:
             origin: Where the events came from
-            room_id,
             auth_events
             state
             event
             room_version: The room version we expect this room to have, and
                 will raise if it doesn't match the version in the create event.
         """
         events_to_context = {}
         for e in itertools.chain(auth_events, state):
             e.internal_metadata.outlier = True
             ctx = await self.state_handler.compute_event_context(e)
@@ -1421,32 +1430,29 @@
             if create_event:
                 auth_for_e[(EventTypes.Create, "")] = create_event
             try:
                 event_auth.check(room_version, e, auth_events=auth_for_e)
             except SynapseError as err:
                 logger.warning("Rejecting %s because %s", e.event_id, err.msg)
                 if e == event:
                     raise
                 events_to_context[e.event_id].rejected = RejectedReason.AUTH_ERROR
         await self.persist_events_and_notify(
-            room_id,
             [
                 (e, events_to_context[e.event_id])
                 for e in itertools.chain(auth_events, state)
-            ],
+            ]
         )
         new_event_context = await self.state_handler.compute_event_context(
             event, old_state=state
         )
-        return await self.persist_events_and_notify(
-            room_id, [(event, new_event_context)]
-        )
+        return await self.persist_events_and_notify([(event, new_event_context)])
     async def _prep_event(
         self,
         origin: str,
         event: EventBase,
         state: Optional[Iterable[EventBase]],
         auth_events: Optional[MutableStateMap[EventBase]],
         backfilled: bool,
     ) -> EventContext:
         context = await self.state_handler.compute_event_context(event, old_state=state)
         if not auth_events:
@@ -1482,24 +1488,24 @@
         if backfilled or event.internal_metadata.is_outlier():
             return
         extrem_ids_list = await self.store.get_latest_event_ids_in_room(event.room_id)
         extrem_ids = set(extrem_ids_list)
         prev_event_ids = set(event.prev_event_ids())
         if extrem_ids == prev_event_ids:
             return
         room_version = await self.store.get_room_version_id(event.room_id)
         room_version_obj = KNOWN_ROOM_VERSIONS[room_version]
         if state is not None:
-            state_sets_d = await self.state_store.get_state_groups(
+            state_sets = await self.state_store.get_state_groups(
                 event.room_id, extrem_ids
             )
-            state_sets = list(state_sets_d.values())  # type: List[Iterable[EventBase]]
+            state_sets = list(state_sets.values())
             state_sets.append(state)
             current_states = await self.state_handler.resolve_events(
                 room_version, state_sets, event
             )
             current_state_ids = {
                 k: e.event_id for k, e in current_states.items()
             }  # type: StateMap[str]
         else:
             current_state_ids = await self.state_handler.get_current_state_ids(
                 event.room_id, latest_event_ids=extrem_ids
@@ -2040,92 +2046,93 @@
                 for revocation.
         """
         try:
             response = await self.http_client.get_json(url, {"public_key": public_key})
         except Exception:
             raise SynapseError(502, "Third party certificate could not be checked")
         if "valid" not in response or not response["valid"]:
             raise AuthError(403, "Third party certificate was invalid")
     async def persist_events_and_notify(
         self,
-        room_id: str,
         event_and_contexts: Sequence[Tuple[EventBase, EventContext]],
         backfilled: bool = False,
     ) -> int:
         """Persists events and tells the notifier/pushers about them, if
         necessary.
         Args:
-            room_id: The room ID of events being persisted.
-            event_and_contexts: Sequence of events with their associated
-                context that should be persisted. All events must belong to
-                the same room.
+            event_and_contexts:
             backfilled: Whether these events are a result of
                 backfilling or not
         """
-        instance = self.config.worker.events_shard_config.get_instance(room_id)
-        if instance != self._instance_name:
+        if self.config.worker.writers.events != self._instance_name:
             result = await self._send_events(
-                instance_name=instance,
+                instance_name=self.config.worker.writers.events,
                 store=self.store,
-                room_id=room_id,
                 event_and_contexts=event_and_contexts,
                 backfilled=backfilled,
             )
             return result["max_stream_id"]
         else:
-            assert self.storage.persistence
-            max_stream_token = await self.storage.persistence.persist_events(
+            max_stream_id = await self.storage.persistence.persist_events(
                 event_and_contexts, backfilled=backfilled
             )
             if self._ephemeral_messages_enabled:
                 for (event, context) in event_and_contexts:
                     self._message_handler.maybe_schedule_expiry(event)
             if not backfilled:  # Never notify for backfilled events
                 for event, _ in event_and_contexts:
-                    await self._notify_persisted_event(event, max_stream_token)
-            return max_stream_token.stream
+                    await self._notify_persisted_event(event, max_stream_id)
+            return max_stream_id
     async def _notify_persisted_event(
-        self, event: EventBase, max_stream_token: RoomStreamToken
+        self, event: EventBase, max_stream_id: int
     ) -> None:
         """Checks to see if notifier/pushers should be notified about the
         event or not.
         Args:
             event:
             max_stream_id: The max_stream_id returned by persist_events
         """
         extra_users = []
         if event.type == EventTypes.Member:
             target_user_id = event.state_key
             if event.internal_metadata.is_outlier():
                 if event.membership != Membership.INVITE:
                     if not self.is_mine_id(target_user_id):
                         return
             target_user = UserID.from_string(target_user_id)
             extra_users.append(target_user)
         elif event.internal_metadata.is_outlier():
             return
-        event_pos = PersistedEventPosition(
-            self._instance_name, event.internal_metadata.stream_ordering
-        )
+        event_stream_id = event.internal_metadata.stream_ordering
         self.notifier.on_new_room_event(
-            event, event_pos, max_stream_token, extra_users=extra_users
-        )
+            event, event_stream_id, max_stream_id, extra_users=extra_users
+        )
+        await self.pusher_pool.on_new_notifications(event_stream_id, max_stream_id)
     async def _clean_room_for_join(self, room_id: str) -> None:
         """Called to clean up any data in DB for a given room, ready for the
         server to join the room.
         Args:
             room_id
         """
         if self.config.worker_app:
             await self._clean_room_for_join_client(room_id)
         else:
             await self.store.clean_room_for_join(room_id)
+    async def user_joined_room(self, user: UserID, room_id: str) -> None:
+        """Called when a new user has joined the room
+        """
+        if self.config.worker_app:
+            await self._notify_user_membership_change(
+                room_id=room_id, user_id=user.to_string(), change="joined"
+            )
+        else:
+            user_joined_room(self.distributor, user, room_id)
     async def get_room_complexity(
         self, remote_room_hosts: List[str], room_id: str
     ) -> Optional[dict]:
         """
         Fetch the complexity of a remote room over federation.
         Args:
             remote_room_hosts (list[str]): The remote servers to ask.
             room_id (str): The room ID to ask about.
         Returns:
             Dict contains the complexity

--- a/synapse/handlers/groups_local.py
+++ b/synapse/handlers/groups_local.py
@@ -161,21 +161,21 @@
                 results.update(r["users"])
             except Exception:
                 failed_results.extend(dest_user_ids)
         for uid in local_users:
             results[uid] = await self.store.get_publicised_groups_for_user(uid)
             for app_service in self.store.get_app_services():
                 results[uid].extend(app_service.get_groups_for_user(uid))
         return {"users": results}
 class GroupsLocalHandler(GroupsLocalWorkerHandler):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(GroupsLocalHandler, self).__init__(hs)
         hs.get_groups_attestation_renewer()
     update_group_profile = _create_rerouter("update_group_profile")
     add_room_to_group = _create_rerouter("add_room_to_group")
     update_room_in_group = _create_rerouter("update_room_in_group")
     remove_room_from_group = _create_rerouter("remove_room_from_group")
     update_group_summary_room = _create_rerouter("update_group_summary_room")
     delete_group_summary_room = _create_rerouter("delete_group_summary_room")
     update_group_category = _create_rerouter("update_group_category")
     delete_group_category = _create_rerouter("delete_group_category")
     update_group_summary_user = _create_rerouter("update_group_summary_user")

--- a/synapse/handlers/identity.py
+++ b/synapse/handlers/identity.py
@@ -1,33 +1,33 @@
 """Utilities for interacting with Identity Servers"""
 import logging
 import urllib.parse
 from typing import Awaitable, Callable, Dict, List, Optional, Tuple
+from twisted.internet.error import TimeoutError
 from synapse.api.errors import (
     CodeMessageException,
     Codes,
     HttpResponseException,
     SynapseError,
 )
 from synapse.config.emailconfig import ThreepidBehaviour
-from synapse.http import RequestTimedOutError
 from synapse.http.client import SimpleHttpClient
 from synapse.types import JsonDict, Requester
 from synapse.util import json_decoder
 from synapse.util.hash import sha256_and_url_safe_base64
 from synapse.util.stringutils import assert_valid_client_secret, random_string
 from ._base import BaseHandler
 logger = logging.getLogger(__name__)
 id_server_scheme = "https://"
 class IdentityHandler(BaseHandler):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(IdentityHandler, self).__init__(hs)
         self.http_client = SimpleHttpClient(hs)
         self.blacklisting_http_client = SimpleHttpClient(
             hs, ip_blacklist=hs.config.federation_ip_range_blacklist
         )
         self.federation_http_client = hs.get_http_client()
         self.hs = hs
     async def threepid_from_creds(
         self, id_server: str, creds: Dict[str, str]
     ) -> Optional[JsonDict]:
         """
@@ -51,21 +51,21 @@
         assert_valid_client_secret(client_secret)
         session_id = creds.get("sid")
         if not session_id:
             raise SynapseError(
                 400, "Missing param session_id in creds", errcode=Codes.MISSING_PARAM
             )
         query_params = {"sid": session_id, "client_secret": client_secret}
         url = id_server + "/_matrix/identity/api/v1/3pid/getValidated3pid"
         try:
             data = await self.http_client.get_json(url, query_params)
-        except RequestTimedOutError:
+        except TimeoutError:
             raise SynapseError(500, "Timed out contacting identity server")
         except HttpResponseException as e:
             logger.info(
                 "%s returned %i for threepid validation for: %s",
                 id_server,
                 e.code,
                 creds,
             )
             return None
         if "medium" in data:
@@ -111,21 +111,21 @@
                 user_id=mxid,
                 medium=data["medium"],
                 address=data["address"],
                 id_server=id_server,
             )
             return data
         except HttpResponseException as e:
             if e.code != 404 or not use_v2:
                 logger.error("3PID bind failed with Matrix error: %r", e)
                 raise e.to_synapse_error()
-        except RequestTimedOutError:
+        except TimeoutError:
             raise SynapseError(500, "Timed out contacting identity server")
         except CodeMessageException as e:
             data = json_decoder.decode(e.msg)  # XXX WAT?
             return data
         logger.info("Got 404 when POSTing JSON %s, falling back to v1 URL", bind_url)
         res = await self.bind_threepid(
             client_secret, sid, mxid, id_server, id_access_token, use_v2=False
         )
         return res
     async def try_unbind_threepid(self, mxid: str, threepid: dict) -> bool:
@@ -189,21 +189,21 @@
                 url, content, headers
             )
             changed = True
         except HttpResponseException as e:
             changed = False
             if e.code in (400, 404, 501):
                 logger.warning("Received %d response while unbinding threepid", e.code)
             else:
                 logger.error("Failed to unbind threepid on identity server: %s", e)
                 raise SynapseError(500, "Failed to contact identity server")
-        except RequestTimedOutError:
+        except TimeoutError:
             raise SynapseError(500, "Timed out contacting identity server")
         await self.store.remove_user_bound_threepid(
             user_id=mxid,
             medium=threepid["medium"],
             address=threepid["address"],
             id_server=id_server,
         )
         return changed
     async def send_threepid_validation(
         self,
@@ -302,21 +302,21 @@
             )
         try:
             data = await self.http_client.post_json_get_json(
                 id_server + "/_matrix/identity/api/v1/validate/email/requestToken",
                 params,
             )
             return data
         except HttpResponseException as e:
             logger.info("Proxied requestToken failed: %r", e)
             raise e.to_synapse_error()
-        except RequestTimedOutError:
+        except TimeoutError:
             raise SynapseError(500, "Timed out contacting identity server")
     async def requestMsisdnToken(
         self,
         id_server: str,
         country: str,
         phone_number: str,
         client_secret: str,
         send_attempt: int,
         next_link: Optional[str] = None,
     ) -> JsonDict:
@@ -349,21 +349,21 @@
                 "details and update your config file."
             )
         try:
             data = await self.http_client.post_json_get_json(
                 id_server + "/_matrix/identity/api/v1/validate/msisdn/requestToken",
                 params,
             )
         except HttpResponseException as e:
             logger.info("Proxied requestToken failed: %r", e)
             raise e.to_synapse_error()
-        except RequestTimedOutError:
+        except TimeoutError:
             raise SynapseError(500, "Timed out contacting identity server")
         assert self.hs.config.public_baseurl
         data["submit_url"] = (
             self.hs.config.public_baseurl
             + "_matrix/client/unstable/add_threepid/msisdn/submit_token"
         )
         return data
     async def validate_threepid_session(
         self, client_secret: str, sid: str
     ) -> Optional[JsonDict]:
@@ -405,21 +405,21 @@
             SynapseError: If we failed to contact the identity server
         Returns:
             The response dict from the identity server
         """
         body = {"client_secret": client_secret, "sid": sid, "token": token}
         try:
             return await self.http_client.post_json_get_json(
                 id_server + "/_matrix/identity/api/v1/validate/msisdn/submitToken",
                 body,
             )
-        except RequestTimedOutError:
+        except TimeoutError:
             raise SynapseError(500, "Timed out contacting identity server")
         except HttpResponseException as e:
             logger.warning("Error contacting msisdn account_threepid_delegate: %s", e)
             raise SynapseError(400, "Error contacting the identity server")
     async def lookup_3pid(
         self,
         id_server: str,
         medium: str,
         address: str,
         id_access_token: Optional[str] = None,
@@ -464,21 +464,21 @@
         Returns:
             the matrix ID of the 3pid, or None if it is not recognized.
         """
         try:
             data = await self.blacklisting_http_client.get_json(
                 "%s%s/_matrix/identity/api/v1/lookup" % (id_server_scheme, id_server),
                 {"medium": medium, "address": address},
             )
             if "mxid" in data:
                 return data["mxid"]
-        except RequestTimedOutError:
+        except TimeoutError:
             raise SynapseError(500, "Timed out contacting identity server")
         except IOError as e:
             logger.warning("Error from v1 identity server lookup: %s" % (e,))
         return None
     async def _lookup_3pid_v2(
         self, id_server: str, id_access_token: str, medium: str, address: str
     ) -> Optional[str]:
         """Looks up a 3pid in the passed identity server using v2 lookup.
         Args:
             id_server: The server name (including port, if required)
@@ -487,21 +487,21 @@
             medium: The type of the third party identifier (e.g. "email").
             address: The third party identifier (e.g. "foo@example.com").
         Returns:
             the matrix ID of the 3pid, or None if it is not recognised.
         """
         try:
             hash_details = await self.blacklisting_http_client.get_json(
                 "%s%s/_matrix/identity/v2/hash_details" % (id_server_scheme, id_server),
                 {"access_token": id_access_token},
             )
-        except RequestTimedOutError:
+        except TimeoutError:
             raise SynapseError(500, "Timed out contacting identity server")
         if not isinstance(hash_details, dict):
             logger.warning(
                 "Got non-dict object when checking hash details of %s%s: %s",
                 id_server_scheme,
                 id_server,
                 hash_details,
             )
             raise SynapseError(
                 400,
@@ -543,21 +543,21 @@
         try:
             lookup_results = await self.blacklisting_http_client.post_json_get_json(
                 "%s%s/_matrix/identity/v2/lookup" % (id_server_scheme, id_server),
                 {
                     "addresses": [lookup_value],
                     "algorithm": lookup_algorithm,
                     "pepper": lookup_pepper,
                 },
                 headers=headers,
             )
-        except RequestTimedOutError:
+        except TimeoutError:
             raise SynapseError(500, "Timed out contacting identity server")
         except Exception as e:
             logger.warning("Error when performing a v2 3pid lookup: %s", e)
             raise SynapseError(
                 500, "Unknown error occurred during identity server lookup"
             )
         if "mappings" not in lookup_results or not isinstance(
             lookup_results["mappings"], dict
         ):
             logger.warning("No results from 3pid lookup")
@@ -626,37 +626,37 @@
                 id_server_scheme,
                 id_server,
             )
             url = base_url + "/v2/store-invite"
             try:
                 data = await self.blacklisting_http_client.post_json_get_json(
                     url,
                     invite_config,
                     {"Authorization": create_id_access_token_header(id_access_token)},
                 )
-            except RequestTimedOutError:
+            except TimeoutError:
                 raise SynapseError(500, "Timed out contacting identity server")
             except HttpResponseException as e:
                 if e.code != 404:
                     logger.info("Failed to POST %s with JSON: %s", url, e)
                     raise e
         if data is None:
             key_validity_url = "%s%s/_matrix/identity/api/v1/pubkey/isvalid" % (
                 id_server_scheme,
                 id_server,
             )
             url = base_url + "/api/v1/store-invite"
             try:
                 data = await self.blacklisting_http_client.post_json_get_json(
                     url, invite_config
                 )
-            except RequestTimedOutError:
+            except TimeoutError:
                 raise SynapseError(500, "Timed out contacting identity server")
             except HttpResponseException as e:
                 logger.warning(
                     "Error trying to call /store-invite on %s%s: %s",
                     id_server_scheme,
                     id_server,
                     e,
                 )
             if data is None:
                 try:

--- a/synapse/handlers/initial_sync.py
+++ b/synapse/handlers/initial_sync.py
@@ -1,32 +1,32 @@
 import logging
 from typing import TYPE_CHECKING
 from twisted.internet import defer
 from synapse.api.constants import EventTypes, Membership
 from synapse.api.errors import SynapseError
 from synapse.events.validator import EventValidator
 from synapse.handlers.presence import format_user_presence_state
 from synapse.logging.context import make_deferred_yieldable, run_in_background
 from synapse.storage.roommember import RoomsForUser
 from synapse.streams.config import PaginationConfig
-from synapse.types import JsonDict, Requester, RoomStreamToken, StreamToken, UserID
+from synapse.types import JsonDict, Requester, StreamToken, UserID
 from synapse.util import unwrapFirstError
 from synapse.util.async_helpers import concurrently_execute
 from synapse.util.caches.response_cache import ResponseCache
 from synapse.visibility import filter_events_for_client
 from ._base import BaseHandler
 if TYPE_CHECKING:
     from synapse.server import HomeServer
 logger = logging.getLogger(__name__)
 class InitialSyncHandler(BaseHandler):
     def __init__(self, hs: "HomeServer"):
-        super().__init__(hs)
+        super(InitialSyncHandler, self).__init__(hs)
         self.hs = hs
         self.state = hs.get_state_handler()
         self.clock = hs.get_clock()
         self.validator = EventValidator()
         self.snapshot_cache = ResponseCache(hs, "initial_sync_cache")
         self._event_serializer = hs.get_event_client_serializer()
         self.storage = hs.get_storage()
         self.state_store = self.storage.state
     def snapshot_all_rooms(
         self,
@@ -75,26 +75,27 @@
         memberships = [Membership.INVITE, Membership.JOIN]
         if include_archived:
             memberships.append(Membership.LEAVE)
         room_list = await self.store.get_rooms_for_local_user_where_membership_is(
             user_id=user_id, membership_list=memberships
         )
         user = UserID.from_string(user_id)
         rooms_ret = []
         now_token = self.hs.get_event_sources().get_current_token()
         presence_stream = self.hs.get_event_sources().sources["presence"]
-        presence, _ = await presence_stream.get_new_events(
-            user, from_key=None, include_offline=False
-        )
-        joined_rooms = [r.room_id for r in room_list if r.membership == Membership.JOIN]
-        receipt = await self.store.get_linearized_receipts_for_rooms(
-            joined_rooms, to_key=int(now_token.receipt_key),
+        pagination_config = PaginationConfig(from_token=now_token)
+        presence, _ = await presence_stream.get_pagination_rows(
+            user, pagination_config.get_source_config("presence"), None
+        )
+        receipt_stream = self.hs.get_event_sources().sources["receipt"]
+        receipt, _ = await receipt_stream.get_pagination_rows(
+            user, pagination_config.get_source_config("receipt"), None
         )
         tags_by_room = await self.store.get_tags_for_user(user_id)
         account_data, account_data_by_room = await self.store.get_account_data_for_user(
             user_id
         )
         public_room_ids = await self.store.get_public_room_ids()
         limit = pagin_config.limit
         if limit is None:
             limit = 10
         async def handle_room(event: RoomsForUser):
@@ -115,21 +116,21 @@
             rooms_ret.append(d)
             if event.membership not in (Membership.JOIN, Membership.LEAVE):
                 return
             try:
                 if event.membership == Membership.JOIN:
                     room_end_token = now_token.room_key
                     deferred_room_state = run_in_background(
                         self.state_handler.get_current_state, event.room_id
                     )
                 elif event.membership == Membership.LEAVE:
-                    room_end_token = RoomStreamToken(None, event.stream_ordering,)
+                    room_end_token = "s%d" % (event.stream_ordering,)
                     deferred_room_state = run_in_background(
                         self.state_store.get_state_for_events, [event.event_id]
                     )
                     deferred_room_state.addCallback(
                         lambda states: states[event.event_id]
                     )
                 (messages, token), current_state = await make_deferred_yieldable(
                     defer.gatherResults(
                         [
                             run_in_background(
@@ -147,22 +148,22 @@
                 )
                 start_token = now_token.copy_and_replace("room_key", token)
                 end_token = now_token.copy_and_replace("room_key", room_end_token)
                 time_now = self.clock.time_msec()
                 d["messages"] = {
                     "chunk": (
                         await self._event_serializer.serialize_events(
                             messages, time_now=time_now, as_client_event=as_client_event
                         )
                     ),
-                    "start": await start_token.to_string(self.store),
-                    "end": await end_token.to_string(self.store),
+                    "start": start_token.to_string(),
+                    "end": end_token.to_string(),
                 }
                 d["state"] = await self._event_serializer.serialize_events(
                     current_state.values(),
                     time_now=time_now,
                     as_client_event=as_client_event,
                 )
                 account_data_events = []
                 tags = tags_by_room.get(event.room_id)
                 if tags:
                     account_data_events.append(
@@ -185,21 +186,21 @@
             "rooms": rooms_ret,
             "presence": [
                 {
                     "type": "m.presence",
                     "content": format_user_presence_state(event, now),
                 }
                 for event in presence
             ],
             "account_data": account_data_events,
             "receipts": receipt,
-            "end": await now_token.to_string(self.store),
+            "end": now_token.to_string(),
         }
         return ret
     async def room_initial_sync(
         self, requester: Requester, room_id: str, pagin_config: PaginationConfig
     ) -> JsonDict:
         """Capture the a snapshot of a room. If user is currently a member of
         the room this will be what is currently in the room. If the user left
         the room this will be what was in the room when they left.
         Args:
             requester: The user to get a snapshot for.
@@ -246,40 +247,39 @@
         pagin_config: PaginationConfig,
         membership: Membership,
         member_event_id: str,
         is_peeking: bool,
     ) -> JsonDict:
         room_state = await self.state_store.get_state_for_events([member_event_id])
         room_state = room_state[member_event_id]
         limit = pagin_config.limit if pagin_config else None
         if limit is None:
             limit = 10
-        leave_position = await self.store.get_position_for_event(member_event_id)
-        stream_token = leave_position.to_room_stream_token()
+        stream_token = await self.store.get_stream_token_for_event(member_event_id)
         messages, token = await self.store.get_recent_events_for_room(
             room_id, limit=limit, end_token=stream_token
         )
         messages = await filter_events_for_client(
             self.storage, user_id, messages, is_peeking=is_peeking
         )
         start_token = StreamToken.START.copy_and_replace("room_key", token)
         end_token = StreamToken.START.copy_and_replace("room_key", stream_token)
         time_now = self.clock.time_msec()
         return {
             "membership": membership,
             "room_id": room_id,
             "messages": {
                 "chunk": (
                     await self._event_serializer.serialize_events(messages, time_now)
                 ),
-                "start": await start_token.to_string(self.store),
-                "end": await end_token.to_string(self.store),
+                "start": start_token.to_string(),
+                "end": end_token.to_string(),
             },
             "state": (
                 await self._event_serializer.serialize_events(
                     room_state.values(), time_now
                 )
             ),
             "presence": [],
             "receipts": [],
         }
     async def _room_initial_sync_joined(
@@ -346,20 +346,20 @@
         )
         start_token = now_token.copy_and_replace("room_key", token)
         end_token = now_token
         time_now = self.clock.time_msec()
         ret = {
             "room_id": room_id,
             "messages": {
                 "chunk": (
                     await self._event_serializer.serialize_events(messages, time_now)
                 ),
-                "start": await start_token.to_string(self.store),
-                "end": await end_token.to_string(self.store),
+                "start": start_token.to_string(),
+                "end": end_token.to_string(),
             },
             "state": state,
             "presence": presence,
             "receipts": receipts,
         }
         if not is_peeking:
             ret["membership"] = membership
         return ret

--- a/synapse/handlers/message.py
+++ b/synapse/handlers/message.py
@@ -268,25 +268,27 @@
         self.storage = hs.get_storage()
         self.state = hs.get_state_handler()
         self.clock = hs.get_clock()
         self.validator = EventValidator()
         self.profile_handler = hs.get_profile_handler()
         self.event_builder_factory = hs.get_event_builder_factory()
         self.server_name = hs.hostname
         self.notifier = hs.get_notifier()
         self.config = hs.config
         self.require_membership_for_aliases = hs.config.require_membership_for_aliases
-        self._events_shard_config = self.config.worker.events_shard_config
-        self._instance_name = hs.get_instance_name()
+        self._is_event_writer = (
+            self.config.worker.writers.events == hs.get_instance_name()
+        )
         self.room_invite_state_types = self.hs.config.room_invite_state_types
         self.send_event = ReplicationSendEventRestServlet.make_client(hs)
         self.base_handler = BaseHandler(hs)
+        self.pusher_pool = hs.get_pusherpool()
         self.limiter = Linearizer(max_count=5, name="room_event_creation_limit")
         self.action_generator = hs.get_action_generator()
         self.spam_checker = hs.get_spam_checker()
         self.third_party_event_rules = hs.get_third_party_event_rules()
         self._block_events_without_consent_error = (
             self.config.block_events_without_consent_error
         )
         self._rooms_to_exclude_from_dummy_event_insertion = {}  # type: Dict[str, int]
         if self._block_events_without_consent_error:
             self._consent_uri_builder = ConsentURIBuilder(self.config)
@@ -657,24 +659,23 @@
                 logger.warning("Denying new event %r because %s", event, err)
                 raise err
         try:
             dump = frozendict_json_encoder.encode(event.content)
             json_decoder.decode(dump)
         except Exception:
             logger.exception("Failed to encode content: %r", event.content)
             raise
         await self.action_generator.handle_push_actions_for_event(event, context)
         try:
-            writer_instance = self._events_shard_config.get_instance(event.room_id)
-            if writer_instance != self._instance_name:
+            if not self._is_event_writer:
                 result = await self.send_event(
-                    instance_name=writer_instance,
+                    instance_name=self.config.worker.writers.events,
                     event_id=event.event_id,
                     store=self.store,
                     requester=requester,
                     event=event,
                     context=context,
                     ratelimit=ratelimit,
                     extra_users=extra_users,
                 )
                 stream_id = result["stream_id"]
                 event.internal_metadata.stream_ordering = stream_id
@@ -718,24 +719,21 @@
         requester: Requester,
         event: EventBase,
         context: EventContext,
         ratelimit: bool = True,
         extra_users: List[UserID] = [],
     ) -> int:
         """Called when we have fully built the event, have already
         calculated the push actions for the event, and checked auth.
         This should only be run on the instance in charge of persisting events.
         """
-        assert self.storage.persistence is not None
-        assert self._events_shard_config.should_handle(
-            self._instance_name, event.room_id
-        )
+        assert self._is_event_writer
         if ratelimit:
             is_admin_redaction = False
             if event.type == EventTypes.Redaction:
                 original_event = await self.store.get_event(
                     event.redacts,
                     redact_behaviour=EventRedactBehaviour.AS_IS,
                     get_prev_content=False,
                     allow_rejected=False,
                     allow_none=True,
                 )
@@ -830,104 +828,101 @@
             ):
                 if not original_event:
                     raise NotFoundError("Could not find event %s" % (event.redacts,))
                 if event.user_id != original_event.user_id:
                     raise AuthError(403, "You don't have permission to redact events")
                 event.internal_metadata.recheck_redaction = False
         if event.type == EventTypes.Create:
             prev_state_ids = await context.get_prev_state_ids()
             if prev_state_ids:
                 raise AuthError(403, "Changing the room create event is forbidden")
-        event_pos, max_stream_token = await self.storage.persistence.persist_event(
+        event_stream_id, max_stream_id = await self.storage.persistence.persist_event(
             event, context=context
         )
         if self._ephemeral_events_enabled:
             self._message_handler.maybe_schedule_expiry(event)
+        await self.pusher_pool.on_new_notifications(event_stream_id, max_stream_id)
         def _notify():
             try:
                 self.notifier.on_new_room_event(
-                    event, event_pos, max_stream_token, extra_users=extra_users
+                    event, event_stream_id, max_stream_id, extra_users=extra_users
                 )
             except Exception:
                 logger.exception("Error notifying about new room event")
         run_in_background(_notify)
         if event.type == EventTypes.Message:
             run_in_background(self._bump_active_time, requester.user)
-        return event_pos.stream
+        return event_stream_id
     async def _bump_active_time(self, user: UserID) -> None:
         try:
             presence = self.hs.get_presence_handler()
             await presence.bump_presence_active_time(user)
         except Exception:
             logger.exception("Error bumping presence active time")
     async def _send_dummy_events_to_fill_extremities(self):
         """Background task to send dummy events into rooms that have a large
         number of extremities
         """
         self._expire_rooms_to_exclude_from_dummy_event_insertion()
         room_ids = await self.store.get_rooms_with_many_extremities(
             min_count=self._dummy_events_threshold,
             limit=5,
             room_id_filter=self._rooms_to_exclude_from_dummy_event_insertion.keys(),
         )
         for room_id in room_ids:
-            dummy_event_sent = await self._send_dummy_event_for_room(room_id)
+            latest_event_ids = await self.store.get_prev_events_for_room(room_id)
+            members = await self.state.get_current_users_in_room(
+                room_id, latest_event_ids=latest_event_ids
+            )
+            dummy_event_sent = False
+            for user_id in members:
+                if not self.hs.is_mine_id(user_id):
+                    continue
+                requester = create_requester(user_id)
+                try:
+                    event, context = await self.create_event(
+                        requester,
+                        {
+                            "type": "org.matrix.dummy_event",
+                            "content": {},
+                            "room_id": room_id,
+                            "sender": user_id,
+                        },
+                        prev_event_ids=latest_event_ids,
+                    )
+                    event.internal_metadata.proactively_send = False
+                    await self.send_nonmember_event(
+                        requester,
+                        event,
+                        context,
+                        ratelimit=False,
+                        ignore_shadow_ban=True,
+                    )
+                    dummy_event_sent = True
+                    break
+                except ConsentNotGivenError:
+                    logger.info(
+                        "Failed to send dummy event into room %s for user %s due to "
+                        "lack of consent. Will try another user" % (room_id, user_id)
+                    )
+                except AuthError:
+                    logger.info(
+                        "Failed to send dummy event into room %s for user %s due to "
+                        "lack of power. Will try another user" % (room_id, user_id)
+                    )
             if not dummy_event_sent:
                 logger.info(
                     "Failed to send dummy event into room %s. Will exclude it from "
                     "future attempts until cache expires" % (room_id,)
                 )
                 now = self.clock.time_msec()
                 self._rooms_to_exclude_from_dummy_event_insertion[room_id] = now
-    async def _send_dummy_event_for_room(self, room_id: str) -> bool:
-        """Attempt to send a dummy event for the given room.
-        Args:
-            room_id: room to try to send an event from
-        Returns:
-            True if a dummy event was successfully sent. False if no user was able
-            to send an event.
-        """
-        latest_event_ids = await self.store.get_prev_events_for_room(room_id)
-        members = await self.state.get_current_users_in_room(
-            room_id, latest_event_ids=latest_event_ids
-        )
-        for user_id in members:
-            if not self.hs.is_mine_id(user_id):
-                continue
-            requester = create_requester(user_id)
-            try:
-                event, context = await self.create_event(
-                    requester,
-                    {
-                        "type": "org.matrix.dummy_event",
-                        "content": {},
-                        "room_id": room_id,
-                        "sender": user_id,
-                    },
-                    prev_event_ids=latest_event_ids,
-                )
-                event.internal_metadata.proactively_send = False
-                await self.send_nonmember_event(
-                    requester, event, context, ratelimit=False, ignore_shadow_ban=True,
-                )
-                return True
-            except ConsentNotGivenError:
-                logger.info(
-                    "Failed to send dummy event into room %s for user %s due to "
-                    "lack of consent. Will try another user" % (room_id, user_id)
-                )
-            except AuthError:
-                logger.info(
-                    "Failed to send dummy event into room %s for user %s due to "
-                    "lack of power. Will try another user" % (room_id, user_id)
-                )
-        return False
     def _expire_rooms_to_exclude_from_dummy_event_insertion(self):
         expire_before = self.clock.time_msec() - _DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY
         to_expire = set()
         for room_id, time in self._rooms_to_exclude_from_dummy_event_insertion.items():
             if time < expire_before:
                 to_expire.add(room_id)
         for room_id in to_expire:
             logger.debug(
                 "Expiring room id %s from dummy event insertion exclusion cache",
                 room_id,

--- a/synapse/handlers/oidc_handler.py
+++ b/synapse/handlers/oidc_handler.py
@@ -13,21 +13,21 @@
 from pymacaroons.exceptions import (
     MacaroonDeserializationException,
     MacaroonInvalidSignatureException,
 )
 from typing_extensions import TypedDict
 from twisted.web.client import readBody
 from synapse.config import ConfigError
 from synapse.http.server import respond_with_html
 from synapse.http.site import SynapseRequest
 from synapse.logging.context import make_deferred_yieldable
-from synapse.types import JsonDict, UserID, map_username_to_mxid_localpart
+from synapse.types import UserID, map_username_to_mxid_localpart
 from synapse.util import json_decoder
 if TYPE_CHECKING:
     from synapse.server import HomeServer
 logger = logging.getLogger(__name__)
 SESSION_COOKIE_NAME = b"oidc_session"
 Token = TypedDict(
     "Token",
     {
         "access_token": str,
         "token_type": str,
@@ -70,37 +70,36 @@
             authorization_endpoint=hs.config.oidc_authorization_endpoint,
             token_endpoint=hs.config.oidc_token_endpoint,
             userinfo_endpoint=hs.config.oidc_userinfo_endpoint,
             jwks_uri=hs.config.oidc_jwks_uri,
         )  # type: OpenIDProviderMetadata
         self._provider_needs_discovery = hs.config.oidc_discover  # type: bool
         self._user_mapping_provider = hs.config.oidc_user_mapping_provider_class(
             hs.config.oidc_user_mapping_provider_config
         )  # type: OidcMappingProvider
         self._skip_verification = hs.config.oidc_skip_verification  # type: bool
-        self._allow_existing_users = hs.config.oidc_allow_existing_users  # type: bool
         self._http_client = hs.get_proxied_http_client()
         self._auth_handler = hs.get_auth_handler()
         self._registration_handler = hs.get_registration_handler()
         self._datastore = hs.get_datastore()
         self._clock = hs.get_clock()
         self._hostname = hs.hostname  # type: str
         self._server_name = hs.config.server_name  # type: str
         self._macaroon_secret_key = hs.config.macaroon_secret_key
         self._error_template = hs.config.sso_error_template
         self._auth_provider_id = "oidc"
     def _render_error(
         self, request, error: str, error_description: Optional[str] = None
     ) -> None:
-        """Render the error template and respond to the request with it.
+        """Renders the error template and respond with it.
         This is used to show errors to the user. The template of this page can
-        be found under `synapse/res/templates/sso_error.html`.
+        be found under ``synapse/res/templates/sso_error.html``.
         Args:
             request: The incoming request from the browser.
                 We'll respond with an HTML page describing the error.
             error: A technical identifier for this error. Those include
                 well-known OAuth2/OIDC error types like invalid_request or
                 access_denied.
             error_description: A human-readable description of the error.
         """
         html = self._error_template.render(
             error=error, error_description=error_description
@@ -508,33 +507,27 @@
         ].decode("ascii", "surrogateescape")
         ip_address = self.hs.get_ip_from_request(request)
         try:
             user_id = await self._map_userinfo_to_user(
                 userinfo, token, user_agent, ip_address
             )
         except MappingException as e:
             logger.exception("Could not map user")
             self._render_error(request, "mapping_error", str(e))
             return
-        extra_attributes = None
-        get_extra_attributes = getattr(
-            self._user_mapping_provider, "get_extra_attributes", None
-        )
-        if get_extra_attributes:
-            extra_attributes = await get_extra_attributes(userinfo, token)
         if ui_auth_session_id:
             await self._auth_handler.complete_sso_ui_auth(
                 user_id, ui_auth_session_id, request
             )
         else:
             await self._auth_handler.complete_sso_login(
-                user_id, request, client_redirect_url, extra_attributes
+                user_id, request, client_redirect_url
             )
     def _generate_oidc_session_token(
         self,
         state: str,
         nonce: str,
         client_redirect_url: str,
         ui_auth_session_id: Optional[str],
         duration_in_ms: int = (60 * 60 * 1000),
     ) -> str:
         """Generates a signed token storing data about an OIDC session.
@@ -630,22 +623,21 @@
         return now < expiry
     async def _map_userinfo_to_user(
         self, userinfo: UserInfo, token: Token, user_agent: str, ip_address: str
     ) -> str:
         """Maps a UserInfo object to a mxid.
         UserInfo should have a claim that uniquely identifies users. This claim
         is usually `sub`, but can be configured with `oidc_config.subject_claim`.
         It is then used as an `external_id`.
         If we don't find the user that way, we should register the user,
         mapping the localpart and the display name from the UserInfo.
-        If a user already exists with the mxid we've mapped and allow_existing_users
-        is disabled, raise an exception.
+        If a user already exists with the mxid we've mapped, raise an exception.
         Args:
             userinfo: an object representing the user
             token: a dict with the tokens obtained from the provider
             user_agent: The user agent of the client making the request.
             ip_address: The IP address of the client making the request.
         Raises:
             MappingException: if there was an error while mapping some properties
         Returns:
             The mxid of the user
         """
@@ -674,42 +666,30 @@
         except Exception as e:
             raise MappingException(
                 "Could not extract user attributes from OIDC response: " + str(e)
             )
         logger.debug(
             "Retrieved user attributes from user mapping provider: %r", attributes
         )
         if not attributes["localpart"]:
             raise MappingException("localpart is empty")
         localpart = map_username_to_mxid_localpart(attributes["localpart"])
-        user_id = UserID(localpart, self._hostname).to_string()
-        users = await self._datastore.get_users_by_id_case_insensitive(user_id)
-        if users:
-            if self._allow_existing_users:
-                if len(users) == 1:
-                    registered_user_id = next(iter(users))
-                elif user_id in users:
-                    registered_user_id = user_id
-                else:
-                    raise MappingException(
-                        "Attempted to login as '{}' but it matches more than one user inexactly: {}".format(
-                            user_id, list(users.keys())
-                        )
-                    )
-            else:
-                raise MappingException("mxid '{}' is already taken".format(user_id))
-        else:
-            registered_user_id = await self._registration_handler.register_user(
-                localpart=localpart,
-                default_display_name=attributes["display_name"],
-                user_agent_ips=(user_agent, ip_address),
-            )
+        user_id = UserID(localpart, self._hostname)
+        if await self._datastore.get_users_by_id_case_insensitive(user_id.to_string()):
+            raise MappingException(
+                "mxid '{}' is already taken".format(user_id.to_string())
+            )
+        registered_user_id = await self._registration_handler.register_user(
+            localpart=localpart,
+            default_display_name=attributes["display_name"],
+            user_agent_ips=(user_agent, ip_address),
+        )
         await self._datastore.record_user_external_id(
             self._auth_provider_id, remote_user_id, registered_user_id,
         )
         return registered_user_id
 UserAttribute = TypedDict(
     "UserAttribute", {"localpart": str, "display_name": Optional[str]}
 )
 C = TypeVar("C")
 class OidcMappingProvider(Generic[C]):
     """A mapping provider maps a UserInfo object to user attributes.
@@ -734,46 +714,36 @@
         Usually, in an OIDC-compliant scenario, it should be the ``sub`` claim from the UserInfo object.
         Args:
             userinfo: An object representing the user given by the OIDC provider
         Returns:
             A unique user ID
         """
         raise NotImplementedError()
     async def map_user_attributes(
         self, userinfo: UserInfo, token: Token
     ) -> UserAttribute:
-        """Map a `UserInfo` object into user attributes.
+        """Map a ``UserInfo`` objects into user attributes.
         Args:
             userinfo: An object representing the user given by the OIDC provider
             token: A dict with the tokens returned by the provider
         Returns:
             A dict containing the ``localpart`` and (optionally) the ``display_name``
         """
         raise NotImplementedError()
-    async def get_extra_attributes(self, userinfo: UserInfo, token: Token) -> JsonDict:
-        """Map a `UserInfo` object into additional attributes passed to the client during login.
-        Args:
-            userinfo: An object representing the user given by the OIDC provider
-            token: A dict with the tokens returned by the provider
-        Returns:
-            A dict containing additional attributes. Must be JSON serializable.
-        """
-        return {}
 def jinja_finalize(thing):
     return thing if thing is not None else ""
 env = Environment(finalize=jinja_finalize)
 @attr.s
 class JinjaOidcMappingConfig:
     subject_claim = attr.ib()  # type: str
     localpart_template = attr.ib()  # type: Template
     display_name_template = attr.ib()  # type: Optional[Template]
-    extra_attributes = attr.ib()  # type: Dict[str, Template]
 class JinjaOidcMappingProvider(OidcMappingProvider[JinjaOidcMappingConfig]):
     """An implementation of a mapping provider based on Jinja templates.
     This is the default mapping provider.
     """
     def __init__(self, config: JinjaOidcMappingConfig):
         self._config = config
     @staticmethod
     def parse_config(config: dict) -> JinjaOidcMappingConfig:
         subject_claim = config.get("subject_claim", "sub")
         if "localpart_template" not in config:
@@ -789,53 +759,29 @@
             )
         display_name_template = None  # type: Optional[Template]
         if "display_name_template" in config:
             try:
                 display_name_template = env.from_string(config["display_name_template"])
             except Exception as e:
                 raise ConfigError(
                     "invalid jinja template for oidc_config.user_mapping_provider.config.display_name_template: %r"
                     % (e,)
                 )
-        extra_attributes = {}  # type Dict[str, Template]
-        if "extra_attributes" in config:
-            extra_attributes_config = config.get("extra_attributes") or {}
-            if not isinstance(extra_attributes_config, dict):
-                raise ConfigError(
-                    "oidc_config.user_mapping_provider.config.extra_attributes must be a dict"
-                )
-            for key, value in extra_attributes_config.items():
-                try:
-                    extra_attributes[key] = env.from_string(value)
-                except Exception as e:
-                    raise ConfigError(
-                        "invalid jinja template for oidc_config.user_mapping_provider.config.extra_attributes.%s: %r"
-                        % (key, e)
-                    )
         return JinjaOidcMappingConfig(
             subject_claim=subject_claim,
             localpart_template=localpart_template,
             display_name_template=display_name_template,
-            extra_attributes=extra_attributes,
         )
     def get_remote_user_id(self, userinfo: UserInfo) -> str:
         return userinfo[self._config.subject_claim]
     async def map_user_attributes(
         self, userinfo: UserInfo, token: Token
     ) -> UserAttribute:
         localpart = self._config.localpart_template.render(user=userinfo).strip()
         display_name = None  # type: Optional[str]
         if self._config.display_name_template is not None:
             display_name = self._config.display_name_template.render(
                 user=userinfo
             ).strip()
             if display_name == "":
                 display_name = None
         return UserAttribute(localpart=localpart, display_name=display_name)
-    async def get_extra_attributes(self, userinfo: UserInfo, token: Token) -> JsonDict:
-        extras = {}  # type: Dict[str, str]
-        for key, template in self._config.extra_attributes.items():
-            try:
-                extras[key] = template.render(user=userinfo).strip()
-            except Exception as e:
-                logger.error("Failed to render OIDC extra attribute %s: %s" % (key, e))
-        return extras

--- a/synapse/handlers/pagination.py
+++ b/synapse/handlers/pagination.py
@@ -1,21 +1,21 @@
 import logging
 from typing import TYPE_CHECKING, Any, Dict, Optional, Set
 from twisted.python.failure import Failure
 from synapse.api.constants import EventTypes, Membership
 from synapse.api.errors import SynapseError
 from synapse.api.filtering import Filter
 from synapse.logging.context import run_in_background
 from synapse.metrics.background_process_metrics import run_as_background_process
 from synapse.storage.state import StateFilter
 from synapse.streams.config import PaginationConfig
-from synapse.types import Requester
+from synapse.types import Requester, RoomStreamToken
 from synapse.util.async_helpers import ReadWriteLock
 from synapse.util.stringutils import random_string
 from synapse.visibility import filter_events_for_client
 if TYPE_CHECKING:
     from synapse.server import HomeServer
 logger = logging.getLogger(__name__)
 class PurgeStatus:
     """Object tracking the status of a purge request
     This class contains information on the progress of a purge request, for
     return by get_purge_status.
@@ -218,93 +218,92 @@
             requester: The user requesting messages.
             room_id: The room they want messages from.
             pagin_config: The pagination config rules to apply, if any.
             as_client_event: True to get events in client-server format.
             event_filter: Filter to apply to results or None
         Returns:
             Pagination API results
         """
         user_id = requester.user.to_string()
         if pagin_config.from_token:
-            from_token = pagin_config.from_token
+            room_token = pagin_config.from_token.room_key
         else:
-            from_token = self.hs.get_event_sources().get_current_token_for_pagination()
-        if pagin_config.limit is None:
-            raise Exception("limit not set")
-        room_token = from_token.room_key
+            pagin_config.from_token = (
+                self.hs.get_event_sources().get_current_token_for_pagination()
+            )
+            room_token = pagin_config.from_token.room_key
+        room_token = RoomStreamToken.parse(room_token)
+        pagin_config.from_token = pagin_config.from_token.copy_and_replace(
+            "room_key", str(room_token)
+        )
+        source_config = pagin_config.get_source_config("room")
         with await self.pagination_lock.read(room_id):
             (
                 membership,
                 member_event_id,
             ) = await self.auth.check_user_in_room_or_world_readable(
                 room_id, user_id, allow_departed_users=True
             )
-            if pagin_config.direction == "b":
+            if source_config.direction == "b":
                 if room_token.topological:
                     curr_topo = room_token.topological
                 else:
                     curr_topo = await self.store.get_current_topological_token(
                         room_id, room_token.stream
                     )
                 if membership == Membership.LEAVE:
                     assert member_event_id
                     leave_token = await self.store.get_topological_token_for_event(
                         member_event_id
                     )
-                    assert leave_token.topological is not None
-                    if leave_token.topological < curr_topo:
-                        from_token = from_token.copy_and_replace(
-                            "room_key", leave_token
-                        )
+                    if RoomStreamToken.parse(leave_token).topological < curr_topo:
+                        source_config.from_key = str(leave_token)
                 await self.hs.get_handlers().federation_handler.maybe_backfill(
-                    room_id, curr_topo, limit=pagin_config.limit,
-                )
-            to_room_key = None
-            if pagin_config.to_token:
-                to_room_key = pagin_config.to_token.room_key
+                    room_id, curr_topo, limit=source_config.limit,
+                )
             events, next_key = await self.store.paginate_room_events(
                 room_id=room_id,
-                from_key=from_token.room_key,
-                to_key=to_room_key,
-                direction=pagin_config.direction,
-                limit=pagin_config.limit,
+                from_key=source_config.from_key,
+                to_key=source_config.to_key,
+                direction=source_config.direction,
+                limit=source_config.limit,
                 event_filter=event_filter,
             )
-            next_token = from_token.copy_and_replace("room_key", next_key)
+            next_token = pagin_config.from_token.copy_and_replace("room_key", next_key)
         if events:
             if event_filter:
                 events = event_filter.filter(events)
             events = await filter_events_for_client(
                 self.storage, user_id, events, is_peeking=(member_event_id is None)
             )
         if not events:
             return {
                 "chunk": [],
-                "start": await from_token.to_string(self.store),
-                "end": await next_token.to_string(self.store),
+                "start": pagin_config.from_token.to_string(),
+                "end": next_token.to_string(),
             }
         state = None
         if event_filter and event_filter.lazy_load_members() and len(events) > 0:
             state_filter = StateFilter.from_types(
                 (EventTypes.Member, event.sender) for event in events
             )
             state_ids = await self.state_store.get_state_ids_for_event(
                 events[0].event_id, state_filter=state_filter
             )
             if state_ids:
                 state_dict = await self.store.get_events(list(state_ids.values()))
                 state = state_dict.values()
         time_now = self.clock.time_msec()
         chunk = {
             "chunk": (
                 await self._event_serializer.serialize_events(
                     events, time_now, as_client_event=as_client_event
                 )
             ),
-            "start": await from_token.to_string(self.store),
-            "end": await next_token.to_string(self.store),
+            "start": pagin_config.from_token.to_string(),
+            "end": next_token.to_string(),
         }
         if state:
             chunk["state"] = await self._event_serializer.serialize_events(
                 state, time_now, as_client_event=as_client_event
             )
         return chunk

--- a/synapse/handlers/presence.py
+++ b/synapse/handlers/presence.py
@@ -765,20 +765,22 @@
             updates = await presence.current_state_for_users(user_ids_changed)
         if include_offline:
             return (list(updates.values()), max_token)
         else:
             return (
                 [s for s in updates.values() if s.state != PresenceState.OFFLINE],
                 max_token,
             )
     def get_current_key(self):
         return self.store.get_current_presence_token()
+    async def get_pagination_rows(self, user, pagination_config, key):
+        return await self.get_new_events(user, from_key=None, include_offline=False)
     @cached(num_args=2, cache_context=True)
     async def _get_interested_in(self, user, explicit_room_id, cache_context):
         """Returns the set of users that the given user should see presence
         updates for
         """
         user_id = user.to_string()
         users_interested_in = set()
         users_interested_in.add(user_id)  # So that we receive our own presence
         users_who_share_room = await self.store.get_users_who_share_room_with_user(
             user_id, on_invalidate=cache_context.invalidate

--- a/synapse/handlers/profile.py
+++ b/synapse/handlers/profile.py
@@ -14,21 +14,21 @@
 logger = logging.getLogger(__name__)
 MAX_DISPLAYNAME_LEN = 256
 MAX_AVATAR_URL_LEN = 1000
 class BaseProfileHandler(BaseHandler):
     """Handles fetching and updating user profile information.
     BaseProfileHandler can be instantiated directly on workers and will
     delegate to master when necessary. The master process should use the
     subclass MasterProfileHandler
     """
     def __init__(self, hs):
-        super().__init__(hs)
+        super(BaseProfileHandler, self).__init__(hs)
         self.federation = hs.get_federation_client()
         hs.get_federation_registry().register_query_handler(
             "profile", self.on_profile_query
         )
         self.user_directory_handler = hs.get_user_directory_handler()
     async def get_profile(self, user_id):
         target_user = UserID.from_string(user_id)
         if self.hs.is_mine(target_user):
             try:
                 displayname = await self.store.get_profile_displayname(
@@ -269,21 +269,21 @@
             if requester_rooms.isdisjoint(target_user_rooms):
                 raise SynapseError(403, "Profile isn't available", Codes.FORBIDDEN)
         except StoreError as e:
             if e.code == 404:
                 raise SynapseError(403, "Profile isn't available", Codes.FORBIDDEN)
             raise
 class MasterProfileHandler(BaseProfileHandler):
     PROFILE_UPDATE_MS = 60 * 1000
     PROFILE_UPDATE_EVERY_MS = 24 * 60 * 60 * 1000
     def __init__(self, hs):
-        super().__init__(hs)
+        super(MasterProfileHandler, self).__init__(hs)
         assert hs.config.worker_app is None
         self.clock.looping_call(
             self._start_update_remote_profile_cache, self.PROFILE_UPDATE_MS
         )
     def _start_update_remote_profile_cache(self):
         return run_as_background_process(
             "Update remote profile", self._update_remote_profile_cache
         )
     async def _update_remote_profile_cache(self):
         """Called periodically to check profiles of remote users we haven't

--- a/synapse/handlers/read_marker.py
+++ b/synapse/handlers/read_marker.py
@@ -1,17 +1,17 @@
 import logging
 from synapse.util.async_helpers import Linearizer
 from ._base import BaseHandler
 logger = logging.getLogger(__name__)
 class ReadMarkerHandler(BaseHandler):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(ReadMarkerHandler, self).__init__(hs)
         self.server_name = hs.config.server_name
         self.store = hs.get_datastore()
         self.read_marker_linearizer = Linearizer(name="read_marker")
         self.notifier = hs.get_notifier()
     async def received_client_read_marker(self, room_id, user_id, event_id):
         """Updates the read marker for a given user in a given room if the event ID given
         is ahead in the stream relative to the current read marker.
         This uses a notifier to indicate that account data should be sent down /sync if
         the read marker has changed.
         """

--- a/synapse/handlers/receipts.py
+++ b/synapse/handlers/receipts.py
@@ -1,18 +1,18 @@
 import logging
 from synapse.handlers._base import BaseHandler
 from synapse.types import ReadReceipt, get_domain_from_id
 from synapse.util.async_helpers import maybe_awaitable
 logger = logging.getLogger(__name__)
 class ReceiptsHandler(BaseHandler):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(ReceiptsHandler, self).__init__(hs)
         self.server_name = hs.config.server_name
         self.store = hs.get_datastore()
         self.hs = hs
         self.federation = hs.get_federation_sender()
         hs.get_federation_registry().register_edu_handler(
             "m.receipt", self._received_remote_receipt
         )
         self.clock = self.hs.get_clock()
         self.state = hs.get_state_handler()
     async def _received_remote_receipt(self, origin, content):
@@ -91,10 +91,21 @@
         from_key = int(from_key)
         to_key = self.get_current_key()
         if from_key == to_key:
             return [], to_key
         events = await self.store.get_linearized_receipts_for_rooms(
             room_ids, from_key=from_key, to_key=to_key
         )
         return (events, to_key)
     def get_current_key(self, direction="f"):
         return self.store.get_max_receipt_stream_id()
+    async def get_pagination_rows(self, user, config, key):
+        to_key = int(config.from_key)
+        if config.to_key:
+            from_key = int(config.to_key)
+        else:
+            from_key = None
+        room_ids = await self.store.get_rooms_for_user(user.to_string())
+        events = await self.store.get_linearized_receipts_for_rooms(
+            room_ids, from_key=from_key, to_key=to_key
+        )
+        return (events, to_key)

--- a/synapse/handlers/register.py
+++ b/synapse/handlers/register.py
@@ -14,21 +14,21 @@
 from synapse.storage.state import StateFilter
 from synapse.types import RoomAlias, UserID, create_requester
 from ._base import BaseHandler
 logger = logging.getLogger(__name__)
 class RegistrationHandler(BaseHandler):
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer):
         """
-        super().__init__(hs)
+        super(RegistrationHandler, self).__init__(hs)
         self.hs = hs
         self.auth = hs.get_auth()
         self._auth_handler = hs.get_auth_handler()
         self.profile_handler = hs.get_profile_handler()
         self.user_directory_handler = hs.get_user_directory_handler()
         self.identity_handler = self.hs.get_handlers().identity_handler
         self.ratelimiter = hs.get_registration_ratelimiter()
         self.macaroon_gen = hs.get_macaroon_generator()
         self._server_notices_mxid = hs.config.server_notices_mxid
         self.spam_checker = hs.get_spam_checker()

--- a/synapse/handlers/room.py
+++ b/synapse/handlers/room.py
@@ -37,21 +37,21 @@
 from synapse.util.caches.response_cache import ResponseCache
 from synapse.visibility import filter_events_for_client
 from ._base import BaseHandler
 if TYPE_CHECKING:
     from synapse.server import HomeServer
 logger = logging.getLogger(__name__)
 id_server_scheme = "https://"
 FIVE_MINUTES_IN_MS = 5 * 60 * 1000
 class RoomCreationHandler(BaseHandler):
     def __init__(self, hs: "HomeServer"):
-        super().__init__(hs)
+        super(RoomCreationHandler, self).__init__(hs)
         self.spam_checker = hs.get_spam_checker()
         self.event_creation_handler = hs.get_event_creation_handler()
         self.room_member_handler = hs.get_room_member_handler()
         self.config = hs.config
         self._presets_dict = {
             RoomCreationPreset.PRIVATE_CHAT: {
                 "join_rules": JoinRules.INVITE,
                 "history_visibility": "shared",
                 "original_invitees_have_ops": False,
                 "guest_can_join": True,
@@ -588,23 +588,21 @@
                 address,
                 id_server,
                 requester,
                 txn_id=None,
                 id_access_token=id_access_token,
             )
         result = {"room_id": room_id}
         if room_alias:
             result["room_alias"] = room_alias.to_string()
         await self._replication.wait_for_stream_position(
-            self.hs.config.worker.events_shard_config.get_instance(room_id),
-            "events",
-            last_stream_id,
+            self.hs.config.worker.writers.events, "events", last_stream_id
         )
         return result, last_stream_id
     async def _send_events_for_new_room(
         self,
         creator: Requester,
         room_id: str,
         preset_config: str,
         invite_list: List[str],
         initial_state: MutableStateMap,
         creation_content: JsonDict,
@@ -800,43 +798,42 @@
         else:
             state_filter = StateFilter.all()
         state = await self.state_store.get_state_for_events(
             [last_event_id], state_filter=state_filter
         )
         state_events = list(state[last_event_id].values())
         if event_filter:
             state_events = event_filter.filter(state_events)
         results["state"] = await filter_evts(state_events)
         token = StreamToken.START
-        results["start"] = await token.copy_and_replace(
+        results["start"] = token.copy_and_replace(
             "room_key", results["start"]
-        ).to_string(self.store)
-        results["end"] = await token.copy_and_replace(
-            "room_key", results["end"]
-        ).to_string(self.store)
+        ).to_string()
+        results["end"] = token.copy_and_replace("room_key", results["end"]).to_string()
         return results
 class RoomEventSource:
     def __init__(self, hs: "HomeServer"):
         self.store = hs.get_datastore()
     async def get_new_events(
         self,
         user: UserID,
-        from_key: RoomStreamToken,
+        from_key: str,
         limit: int,
         room_ids: List[str],
         is_guest: bool,
         explicit_room_id: Optional[str] = None,
-    ) -> Tuple[List[EventBase], RoomStreamToken]:
+    ) -> Tuple[List[EventBase], str]:
         to_key = self.get_current_key()
-        if from_key.topological:
+        from_token = RoomStreamToken.parse(from_key)
+        if from_token.topological:
             logger.warning("Stream has topological part!!!! %r", from_key)
-            from_key = RoomStreamToken(None, from_key.stream)
+            from_key = "s%s" % (from_token.stream,)
         app_service = self.store.get_app_service_by_user_id(user.to_string())
         if app_service:
             raise NotImplementedError()
         else:
             room_events = await self.store.get_membership_changes_for_user(
                 user.to_string(), from_key, to_key
             )
             room_to_events = await self.store.get_room_events_stream_for_rooms(
                 room_ids=room_ids,
                 from_key=from_key,
@@ -847,22 +844,22 @@
             events = list(room_events)
             events.extend(e for evs, _ in room_to_events.values() for e in evs)
             events.sort(key=lambda e: e.internal_metadata.order)
             if limit:
                 events[:] = events[:limit]
             if events:
                 end_key = events[-1].internal_metadata.after
             else:
                 end_key = to_key
         return (events, end_key)
-    def get_current_key(self) -> RoomStreamToken:
-        return self.store.get_room_max_token()
+    def get_current_key(self) -> str:
+        return "s%d" % (self.store.get_room_max_stream_ordering(),)
     def get_current_key_for_room(self, room_id: str) -> Awaitable[str]:
         return self.store.get_room_events_max_id(room_id)
 class RoomShutdownHandler:
     DEFAULT_MESSAGE = (
         "Sharing illegal content on this server is not permitted and rooms in"
         " violation will be blocked."
     )
     DEFAULT_ROOM_NAME = "Content Violation Notification"
     def __init__(self, hs: "HomeServer"):
         self.hs = hs
@@ -945,23 +942,21 @@
                     "name": new_room_name,
                     "power_level_content_override": {"users_default": -10},
                 },
                 ratelimit=False,
             )
             new_room_id = info["room_id"]
             logger.info(
                 "Shutting down room %r, joining to new room: %r", room_id, new_room_id
             )
             await self._replication.wait_for_stream_position(
-                self.hs.config.worker.events_shard_config.get_instance(new_room_id),
-                "events",
-                stream_id,
+                self.hs.config.worker.writers.events, "events", stream_id
             )
         else:
             new_room_id = None
             logger.info("Shutting down room %r", room_id)
         users = await self.state.get_current_users_in_room(room_id)
         kicked_users = []
         failed_to_kick_users = []
         for user_id in users:
             if not self.hs.is_mine_id(user_id):
                 continue
@@ -971,23 +966,21 @@
                 _, stream_id = await self.room_member_handler.update_membership(
                     requester=target_requester,
                     target=target_requester.user,
                     room_id=room_id,
                     action=Membership.LEAVE,
                     content={},
                     ratelimit=False,
                     require_consent=False,
                 )
                 await self._replication.wait_for_stream_position(
-                    self.hs.config.worker.events_shard_config.get_instance(room_id),
-                    "events",
-                    stream_id,
+                    self.hs.config.worker.writers.events, "events", stream_id
                 )
                 await self.room_member_handler.forget(target_requester.user, room_id)
                 if new_room_user_id:
                     await self.room_member_handler.update_membership(
                         requester=target_requester,
                         target=target_requester.user,
                         room_id=new_room_id,
                         action=Membership.JOIN,
                         content={},
                         ratelimit=False,

--- a/synapse/handlers/room_list.py
+++ b/synapse/handlers/room_list.py
@@ -7,21 +7,21 @@
 from synapse.api.errors import Codes, HttpResponseException
 from synapse.types import ThirdPartyInstanceID
 from synapse.util.caches.descriptors import cached
 from synapse.util.caches.response_cache import ResponseCache
 from ._base import BaseHandler
 logger = logging.getLogger(__name__)
 REMOTE_ROOM_LIST_POLL_INTERVAL = 60 * 1000
 EMPTY_THIRD_PARTY_ID = ThirdPartyInstanceID(None, None)
 class RoomListHandler(BaseHandler):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(RoomListHandler, self).__init__(hs)
         self.enable_room_list_search = hs.config.enable_room_list_search
         self.response_cache = ResponseCache(hs, "room_list")
         self.remote_response_cache = ResponseCache(
             hs, "remote_room_list", timeout_ms=30 * 1000
         )
     async def get_local_public_room_list(
         self,
         limit=None,
         since_token=None,
         search_filter=None,

--- a/synapse/handlers/room_member.py
+++ b/synapse/handlers/room_member.py
@@ -16,45 +16,52 @@
 from synapse.api.ratelimiting import Ratelimiter
 from synapse.api.room_versions import EventFormatVersions
 from synapse.crypto.event_signing import compute_event_reference_hash
 from synapse.events import EventBase
 from synapse.events.builder import create_local_event_from_event_dict
 from synapse.events.snapshot import EventContext
 from synapse.events.validator import EventValidator
 from synapse.storage.roommember import RoomsForUser
 from synapse.types import JsonDict, Requester, RoomAlias, RoomID, StateMap, UserID
 from synapse.util.async_helpers import Linearizer
-from synapse.util.distributor import user_left_room
+from synapse.util.distributor import user_joined_room, user_left_room
 from ._base import BaseHandler
 if TYPE_CHECKING:
     from synapse.server import HomeServer
 logger = logging.getLogger(__name__)
-class RoomMemberHandler(metaclass=abc.ABCMeta):
+class RoomMemberHandler:
+    __metaclass__ = abc.ABCMeta
     def __init__(self, hs: "HomeServer"):
         self.hs = hs
         self.store = hs.get_datastore()
         self.auth = hs.get_auth()
         self.state_handler = hs.get_state_handler()
         self.config = hs.config
         self.federation_handler = hs.get_handlers().federation_handler
         self.directory_handler = hs.get_handlers().directory_handler
         self.identity_handler = hs.get_handlers().identity_handler
         self.registration_handler = hs.get_registration_handler()
         self.profile_handler = hs.get_profile_handler()
         self.event_creation_handler = hs.get_event_creation_handler()
         self.member_linearizer = Linearizer(name="member")
         self.clock = hs.get_clock()
         self.spam_checker = hs.get_spam_checker()
         self.third_party_event_rules = hs.get_third_party_event_rules()
         self._server_notices_mxid = self.config.server_notices_mxid
         self._enable_lookup = hs.config.enable_3pid_lookup
         self.allow_per_room_profiles = self.config.allow_per_room_profiles
+        self._event_stream_writer_instance = hs.config.worker.writers.events
+        self._is_on_event_persistence_instance = (
+            self._event_stream_writer_instance == hs.get_instance_name()
+        )
+        if self._is_on_event_persistence_instance:
+            self.persist_event_storage = hs.get_storage().persistence
         self._join_rate_limiter_local = Ratelimiter(
             clock=self.clock,
             rate_hz=hs.config.ratelimiting.rc_joins_local.per_second,
             burst_count=hs.config.ratelimiting.rc_joins_local.burst_count,
         )
         self._join_rate_limiter_remote = Ratelimiter(
             clock=self.clock,
             rate_hz=hs.config.ratelimiting.rc_joins_remote.per_second,
             burst_count=hs.config.ratelimiting.rc_joins_remote.burst_count,
         )
@@ -88,20 +95,29 @@
         """
         Rejects an out-of-band invite we have received from a remote server
         Args:
             invite_event_id: ID of the invite to be rejected
             txn_id: optional transaction ID supplied by the client
             requester: user making the rejection request, according to the access token
             content: additional content to include in the rejection event.
                Normally an empty dict.
         Returns:
             event id, stream_id of the leave event
+        """
+        raise NotImplementedError()
+    @abc.abstractmethod
+    async def _user_joined_room(self, target: UserID, room_id: str) -> None:
+        """Notifies distributor on master process that the user has joined the
+        room.
+        Args:
+            target
+            room_id
         """
         raise NotImplementedError()
     @abc.abstractmethod
     async def _user_left_room(self, target: UserID, room_id: str) -> None:
         """Notifies distributor on master process that the user has left the
         room.
         Args:
             target
             room_id
         """
@@ -140,39 +156,42 @@
             require_consent=require_consent,
         )
         duplicate = await self.event_creation_handler.deduplicate_state_event(
             event, context
         )
         if duplicate is not None:
             _, stream_id = await self.store.get_event_ordering(duplicate.event_id)
             return duplicate.event_id, stream_id
         prev_state_ids = await context.get_prev_state_ids()
         prev_member_event_id = prev_state_ids.get((EventTypes.Member, user_id), None)
+        newly_joined = False
         if event.membership == Membership.JOIN:
             newly_joined = True
             if prev_member_event_id:
                 prev_member_event = await self.store.get_event(prev_member_event_id)
                 newly_joined = prev_member_event.membership != Membership.JOIN
             if newly_joined:
                 time_now_s = self.clock.time()
                 (
                     allowed,
                     time_allowed,
                 ) = self._join_rate_limiter_local.can_requester_do_action(requester)
                 if not allowed:
                     raise LimitExceededError(
                         retry_after_ms=int(1000 * (time_allowed - time_now_s))
                     )
         stream_id = await self.event_creation_handler.handle_new_client_event(
             requester, event, context, extra_users=[target], ratelimit=ratelimit,
         )
-        if event.membership == Membership.LEAVE:
+        if event.membership == Membership.JOIN and newly_joined:
+            await self._user_joined_room(target, room_id)
+        elif event.membership == Membership.LEAVE:
             if prev_member_event_id:
                 prev_member_event = await self.store.get_event(prev_member_event_id)
                 if prev_member_event.membership == Membership.JOIN:
                     await self._user_left_room(target, room_id)
         return event.event_id, stream_id
     async def copy_room_tags_and_direct_to_room(
         self, old_room_id, new_room_id, user_id
     ) -> None:
         """Copies the tags and direct room state from one room to another.
         Args:
@@ -514,21 +533,28 @@
         if event.membership not in (Membership.LEAVE, Membership.BAN):
             is_blocked = await self.store.is_room_blocked(room_id)
             if is_blocked:
                 raise SynapseError(403, "This room has been blocked on this server")
         await self.event_creation_handler.handle_new_client_event(
             requester, event, context, extra_users=[target_user], ratelimit=ratelimit
         )
         prev_member_event_id = prev_state_ids.get(
             (EventTypes.Member, event.state_key), None
         )
-        if event.membership == Membership.LEAVE:
+        if event.membership == Membership.JOIN:
+            newly_joined = True
+            if prev_member_event_id:
+                prev_member_event = await self.store.get_event(prev_member_event_id)
+                newly_joined = prev_member_event.membership != Membership.JOIN
+            if newly_joined:
+                await self._user_joined_room(target_user, room_id)
+        elif event.membership == Membership.LEAVE:
             if prev_member_event_id:
                 prev_member_event = await self.store.get_event(prev_member_event_id)
                 if prev_member_event.membership == Membership.JOIN:
                     await self._user_left_room(target_user, room_id)
     async def _can_guest_join(self, current_state_ids: StateMap[str]) -> bool:
         """
         Returns whether a guest can join a room based on its current state.
         """
         guest_access_id = current_state_ids.get((EventTypes.GuestAccess, ""), None)
         if not guest_access_id:
@@ -731,22 +757,23 @@
             if event.membership == Membership.JOIN:
                 return True
         return False
     async def _is_server_notice_room(self, room_id: str) -> bool:
         if self._server_notices_mxid is None:
             return False
         user_ids = await self.store.get_users_in_room(room_id)
         return self._server_notices_mxid in user_ids
 class RoomMemberMasterHandler(RoomMemberHandler):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(RoomMemberMasterHandler, self).__init__(hs)
         self.distributor = hs.get_distributor()
+        self.distributor.declare("user_joined_room")
         self.distributor.declare("user_left_room")
     async def _is_remote_room_too_complex(
         self, room_id: str, remote_room_hosts: List[str]
     ) -> Optional[bool]:
         """
         Check if complexity of a remote room is too great.
         Args:
             room_id
             remote_room_hosts
         Returns: bool of whether the complexity is too great, or None
@@ -792,20 +819,21 @@
             )
             if too_complex is True:
                 raise SynapseError(
                     code=400,
                     msg=self.hs.config.limit_remote_rooms.complexity_error,
                     errcode=Codes.RESOURCE_LIMIT_EXCEEDED,
                 )
         event_id, stream_id = await self.federation_handler.do_invite_join(
             remote_room_hosts, room_id, user.to_string(), content
         )
+        await self._user_joined_room(user, room_id)
         if check_complexity:
             if too_complex is False:
                 return event_id, stream_id
             too_complex = await self._is_local_room_too_complex(room_id)
             if too_complex is False:
                 return event_id, stream_id
             requester = types.create_requester(user, None, False, False, None)
             await self.update_membership(
                 requester=requester, target=user, room_id=room_id, action="leave"
             )
@@ -892,20 +920,24 @@
             event.internal_metadata.txn_id = txn_id
         if requester.access_token_id is not None:
             event.internal_metadata.token_id = requester.access_token_id
         EventValidator().validate_new(event, self.config)
         context = await self.state_handler.compute_event_context(event)
         context.app_service = requester.app_service
         stream_id = await self.event_creation_handler.handle_new_client_event(
             requester, event, context, extra_users=[UserID.from_string(target_user)],
         )
         return event.event_id, stream_id
+    async def _user_joined_room(self, target: UserID, room_id: str) -> None:
+        """Implements RoomMemberHandler._user_joined_room
+        """
+        user_joined_room(self.distributor, target, room_id)
     async def _user_left_room(self, target: UserID, room_id: str) -> None:
         """Implements RoomMemberHandler._user_left_room
         """
         user_left_room(self.distributor, target, room_id)
     async def forget(self, user: UserID, room_id: str) -> None:
         user_id = user.to_string()
         member = await self.state_handler.get_current_state(
             room_id=room_id, event_type=EventTypes.Member, state_key=user_id
         )
         membership = member.membership if member else None

--- a/synapse/handlers/room_member_worker.py
+++ b/synapse/handlers/room_member_worker.py
@@ -4,21 +4,21 @@
 from synapse.handlers.room_member import RoomMemberHandler
 from synapse.replication.http.membership import (
     ReplicationRemoteJoinRestServlet as ReplRemoteJoin,
     ReplicationRemoteRejectInviteRestServlet as ReplRejectInvite,
     ReplicationUserJoinedLeftRoomRestServlet as ReplJoinedLeft,
 )
 from synapse.types import Requester, UserID
 logger = logging.getLogger(__name__)
 class RoomMemberWorkerHandler(RoomMemberHandler):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(RoomMemberWorkerHandler, self).__init__(hs)
         self._remote_join_client = ReplRemoteJoin.make_client(hs)
         self._remote_reject_client = ReplRejectInvite.make_client(hs)
         self._notify_change_client = ReplJoinedLeft.make_client(hs)
     async def _remote_join(
         self,
         requester: Requester,
         remote_room_hosts: List[str],
         room_id: str,
         user: UserID,
         content: dict,
@@ -27,35 +27,42 @@
         """
         if len(remote_room_hosts) == 0:
             raise SynapseError(404, "No known servers")
         ret = await self._remote_join_client(
             requester=requester,
             remote_room_hosts=remote_room_hosts,
             room_id=room_id,
             user_id=user.to_string(),
             content=content,
         )
+        await self._user_joined_room(user, room_id)
         return ret["event_id"], ret["stream_id"]
     async def remote_reject_invite(
         self,
         invite_event_id: str,
         txn_id: Optional[str],
         requester: Requester,
         content: dict,
     ) -> Tuple[str, int]:
         """
         Rejects an out-of-band invite received from a remote user
         Implements RoomMemberHandler.remote_reject_invite
         """
         ret = await self._remote_reject_client(
             invite_event_id=invite_event_id,
             txn_id=txn_id,
             requester=requester,
             content=content,
         )
         return ret["event_id"], ret["stream_id"]
+    async def _user_joined_room(self, target: UserID, room_id: str) -> None:
+        """Implements RoomMemberHandler._user_joined_room
+        """
+        await self._notify_change_client(
+            user_id=target.to_string(), room_id=room_id, change="joined"
+        )
     async def _user_left_room(self, target: UserID, room_id: str) -> None:
         """Implements RoomMemberHandler._user_left_room
         """
         await self._notify_change_client(
             user_id=target.to_string(), room_id=room_id, change="left"
         )

--- a/synapse/handlers/saml_handler.py
+++ b/synapse/handlers/saml_handler.py
@@ -1,80 +1,60 @@
 import logging
 import re
 from typing import TYPE_CHECKING, Callable, Dict, Optional, Set, Tuple
 import attr
 import saml2
 import saml2.response
 from saml2.client import Saml2Client
-from synapse.api.errors import SynapseError
+from synapse.api.errors import AuthError, SynapseError
 from synapse.config import ConfigError
 from synapse.config.saml2_config import SamlAttributeRequirement
-from synapse.http.server import respond_with_html
 from synapse.http.servlet import parse_string
 from synapse.http.site import SynapseRequest
 from synapse.module_api import ModuleApi
 from synapse.types import (
     UserID,
     map_username_to_mxid_localpart,
     mxid_localpart_allowed_characters,
 )
 from synapse.util.async_helpers import Linearizer
 from synapse.util.iterutils import chunk_seq
 if TYPE_CHECKING:
     import synapse.server
 logger = logging.getLogger(__name__)
-class MappingException(Exception):
-    """Used to catch errors when mapping the SAML2 response to a user."""
-@attr.s(slots=True)
+@attr.s
 class Saml2SessionData:
     """Data we track about SAML2 sessions"""
     creation_time = attr.ib()
     ui_auth_session_id = attr.ib(type=Optional[str], default=None)
 class SamlHandler:
     def __init__(self, hs: "synapse.server.HomeServer"):
         self.hs = hs
         self._saml_client = Saml2Client(hs.config.saml2_sp_config)
         self._auth = hs.get_auth()
         self._auth_handler = hs.get_auth_handler()
         self._registration_handler = hs.get_registration_handler()
         self._clock = hs.get_clock()
         self._datastore = hs.get_datastore()
         self._hostname = hs.hostname
         self._saml2_session_lifetime = hs.config.saml2_session_lifetime
         self._grandfathered_mxid_source_attribute = (
             hs.config.saml2_grandfathered_mxid_source_attribute
         )
         self._saml2_attribute_requirements = hs.config.saml2.attribute_requirements
-        self._error_template = hs.config.sso_error_template
         self._user_mapping_provider = hs.config.saml2_user_mapping_provider_class(
             hs.config.saml2_user_mapping_provider_config,
             ModuleApi(hs, hs.get_auth_handler()),
         )
         self._auth_provider_id = "saml"
         self._outstanding_requests_dict = {}  # type: Dict[str, Saml2SessionData]
         self._mapping_lock = Linearizer(name="saml_mapping", clock=self._clock)
-    def _render_error(
-        self, request, error: str, error_description: Optional[str] = None
-    ) -> None:
-        """Render the error template and respond to the request with it.
-        This is used to show errors to the user. The template of this page can
-        be found under `synapse/res/templates/sso_error.html`.
-        Args:
-            request: The incoming request from the browser.
-                We'll respond with an HTML page describing the error.
-            error: A technical identifier for this error.
-            error_description: A human-readable description of the error.
-        """
-        html = self._error_template.render(
-            error=error, error_description=error_description
-        )
-        respond_with_html(request, 400, html)
     def handle_redirect_request(
         self, client_redirect_url: bytes, ui_auth_session_id: Optional[str] = None
     ) -> bytes:
         """Handle an incoming request to /login/sso/redirect
         Args:
             client_redirect_url: the URL that we should redirect the
                 client to when everything is done
             ui_auth_session_id: The session ID of the ongoing UI Auth (or
                 None if this is a login).
         Returns:
@@ -96,199 +76,177 @@
         """Handle an incoming request to /_matrix/saml2/authn_response
         Args:
             request: the incoming request from the browser. We'll
                 respond to it with a redirect.
         Returns:
             Completes once we have handled the request.
         """
         resp_bytes = parse_string(request, "SAMLResponse", required=True)
         relay_state = parse_string(request, "RelayState", required=True)
         self.expire_sessions()
+        user_agent = request.requestHeaders.getRawHeaders(b"User-Agent", default=[b""])[
+            0
+        ].decode("ascii", "surrogateescape")
+        ip_address = self.hs.get_ip_from_request(request)
+        user_id, current_session = await self._map_saml_response_to_user(
+            resp_bytes, relay_state, user_agent, ip_address
+        )
+        if current_session and current_session.ui_auth_session_id:
+            await self._auth_handler.complete_sso_ui_auth(
+                user_id, current_session.ui_auth_session_id, request
+            )
+        else:
+            await self._auth_handler.complete_sso_login(user_id, request, relay_state)
+    async def _map_saml_response_to_user(
+        self,
+        resp_bytes: str,
+        client_redirect_url: str,
+        user_agent: str,
+        ip_address: str,
+    ) -> Tuple[str, Optional[Saml2SessionData]]:
+        """
+        Given a sample response, retrieve the cached session and user for it.
+        Args:
+            resp_bytes: The SAML response.
+            client_redirect_url: The redirect URL passed in by the client.
+            user_agent: The user agent of the client making the request.
+            ip_address: The IP address of the client making the request.
+        Returns:
+             Tuple of the user ID and SAML session associated with this response.
+        Raises:
+            SynapseError if there was a problem with the response.
+            RedirectException: some mapping providers may raise this if they need
+                to redirect to an interstitial page.
+        """
         try:
             saml2_auth = self._saml_client.parse_authn_request_response(
                 resp_bytes,
                 saml2.BINDING_HTTP_POST,
                 outstanding=self._outstanding_requests_dict,
             )
         except saml2.response.UnsolicitedResponse as e:
             logger.warning(str(e))
-            self._render_error(
-                request, "unsolicited_response", "Unexpected SAML2 login."
-            )
-            return
+            raise SynapseError(400, "Unexpected SAML2 login.")
         except Exception as e:
-            self._render_error(
-                request,
-                "invalid_response",
-                "Unable to parse SAML2 response: %s." % (e,),
-            )
-            return
+            raise SynapseError(400, "Unable to parse SAML2 response: %s." % (e,))
         if saml2_auth.not_signed:
-            self._render_error(
-                request, "unsigned_respond", "SAML2 response was not signed."
-            )
-            return
+            raise SynapseError(400, "SAML2 response was not signed.")
         logger.debug("SAML2 response: %s", saml2_auth.origxml)
         for assertion in saml2_auth.assertions:
             count = 0
             for part in chunk_seq(str(assertion), 10000):
                 logger.info(
                     "SAML2 assertion: %s%s", "(%i)..." % (count,) if count else "", part
                 )
                 count += 1
         logger.info("SAML2 mapped attributes: %s", saml2_auth.ava)
         current_session = self._outstanding_requests_dict.pop(
             saml2_auth.in_response_to, None
         )
         for requirement in self._saml2_attribute_requirements:
-            if not _check_attribute_requirement(saml2_auth.ava, requirement):
-                self._render_error(
-                    request, "unauthorised", "You are not authorised to log in here."
-                )
-                return
-        user_agent = request.requestHeaders.getRawHeaders(b"User-Agent", default=[b""])[
-            0
-        ].decode("ascii", "surrogateescape")
-        ip_address = self.hs.get_ip_from_request(request)
-        try:
-            user_id = await self._map_saml_response_to_user(
-                saml2_auth, relay_state, user_agent, ip_address
-            )
-        except MappingException as e:
-            logger.exception("Could not map user")
-            self._render_error(request, "mapping_error", str(e))
-            return
-        if current_session and current_session.ui_auth_session_id:
-            await self._auth_handler.complete_sso_ui_auth(
-                user_id, current_session.ui_auth_session_id, request
-            )
-        else:
-            await self._auth_handler.complete_sso_login(user_id, request, relay_state)
-    async def _map_saml_response_to_user(
-        self,
-        saml2_auth: saml2.response.AuthnResponse,
-        client_redirect_url: str,
-        user_agent: str,
-        ip_address: str,
-    ) -> str:
-        """
-        Given a SAML response, retrieve the user ID for it and possibly register the user.
-        Args:
-            saml2_auth: The parsed SAML2 response.
-            client_redirect_url: The redirect URL passed in by the client.
-            user_agent: The user agent of the client making the request.
-            ip_address: The IP address of the client making the request.
-        Returns:
-             The user ID associated with this response.
-        Raises:
-            MappingException if there was a problem mapping the response to a user.
-            RedirectException: some mapping providers may raise this if they need
-                to redirect to an interstitial page.
-        """
+            _check_attribute_requirement(saml2_auth.ava, requirement)
         remote_user_id = self._user_mapping_provider.get_remote_user_id(
             saml2_auth, client_redirect_url
         )
         if not remote_user_id:
-            raise MappingException(
-                "Failed to extract remote user id from SAML response"
-            )
+            raise Exception("Failed to extract remote user id from SAML response")
         with (await self._mapping_lock.queue(self._auth_provider_id)):
             logger.info(
                 "Looking for existing mapping for user %s:%s",
                 self._auth_provider_id,
                 remote_user_id,
             )
             registered_user_id = await self._datastore.get_user_by_external_id(
                 self._auth_provider_id, remote_user_id
             )
             if registered_user_id is not None:
                 logger.info("Found existing mapping %s", registered_user_id)
-                return registered_user_id
+                return registered_user_id, current_session
             if (
                 self._grandfathered_mxid_source_attribute
                 and self._grandfathered_mxid_source_attribute in saml2_auth.ava
             ):
                 attrval = saml2_auth.ava[self._grandfathered_mxid_source_attribute][0]
                 user_id = UserID(
                     map_username_to_mxid_localpart(attrval), self._hostname
                 ).to_string()
                 logger.info(
                     "Looking for existing account based on mapped %s %s",
                     self._grandfathered_mxid_source_attribute,
                     user_id,
                 )
                 users = await self._datastore.get_users_by_id_case_insensitive(user_id)
                 if users:
                     registered_user_id = list(users.keys())[0]
                     logger.info("Grandfathering mapping to %s", registered_user_id)
                     await self._datastore.record_user_external_id(
                         self._auth_provider_id, remote_user_id, registered_user_id
                     )
-                    return registered_user_id
+                    return registered_user_id, current_session
             for i in range(1000):
                 attribute_dict = self._user_mapping_provider.saml_response_to_user_attributes(
                     saml2_auth, i, client_redirect_url=client_redirect_url,
                 )
                 logger.debug(
                     "Retrieved SAML attributes from user mapping provider: %s "
                     "(attempt %d)",
                     attribute_dict,
                     i,
                 )
                 localpart = attribute_dict.get("mxid_localpart")
                 if not localpart:
-                    raise MappingException(
+                    raise Exception(
                         "Error parsing SAML2 response: SAML mapping provider plugin "
                         "did not return a mxid_localpart value"
                     )
                 displayname = attribute_dict.get("displayname")
                 emails = attribute_dict.get("emails", [])
                 if not await self._datastore.get_users_by_id_case_insensitive(
                     UserID(localpart, self._hostname).to_string()
                 ):
                     break
             else:
-                raise MappingException(
-                    "Unable to generate a Matrix ID from the SAML response"
+                raise SynapseError(
+                    500, "Unable to generate a Matrix ID from the SAML response"
                 )
             logger.info("Mapped SAML user to local part %s", localpart)
             registered_user_id = await self._registration_handler.register_user(
                 localpart=localpart,
                 default_display_name=displayname,
                 bind_emails=emails,
                 user_agent_ips=(user_agent, ip_address),
             )
             await self._datastore.record_user_external_id(
                 self._auth_provider_id, remote_user_id, registered_user_id
             )
-            return registered_user_id
+            return registered_user_id, current_session
     def expire_sessions(self):
         expire_before = self._clock.time_msec() - self._saml2_session_lifetime
         to_expire = set()
         for reqid, data in self._outstanding_requests_dict.items():
             if data.creation_time < expire_before:
                 to_expire.add(reqid)
         for reqid in to_expire:
             logger.debug("Expiring session id %s", reqid)
             del self._outstanding_requests_dict[reqid]
-def _check_attribute_requirement(ava: dict, req: SamlAttributeRequirement) -> bool:
+def _check_attribute_requirement(ava: dict, req: SamlAttributeRequirement):
     values = ava.get(req.attribute, [])
     for v in values:
         if v == req.value:
-            return True
+            return
     logger.info(
         "SAML2 attribute %s did not match required value '%s' (was '%s')",
         req.attribute,
         req.value,
         values,
     )
-    return False
+    raise AuthError(403, "You are not authorized to log in here.")
 DOT_REPLACE_PATTERN = re.compile(
     ("[^%s]" % (re.escape("".join(mxid_localpart_allowed_characters)),))
 )
 def dot_replace_for_mxid(username: str) -> str:
     """Replace any characters which are not allowed in Matrix IDs with a dot."""
     username = username.lower()
     username = DOT_REPLACE_PATTERN.sub(".", username)
     username = re.sub("^_", "", username)
     return username
 MXID_MAPPER_MAP = {
@@ -313,21 +271,21 @@
             module_api._hs.config.saml2_grandfathered_mxid_source_attribute
         )
     def get_remote_user_id(
         self, saml_response: saml2.response.AuthnResponse, client_redirect_url: str
     ) -> str:
         """Extracts the remote user id from the SAML response"""
         try:
             return saml_response.ava["uid"][0]
         except KeyError:
             logger.warning("SAML2 response lacks a 'uid' attestation")
-            raise MappingException("'uid' not in SAML2 response")
+            raise SynapseError(400, "'uid' not in SAML2 response")
     def saml_response_to_user_attributes(
         self,
         saml_response: saml2.response.AuthnResponse,
         failures: int,
         client_redirect_url: str,
     ) -> dict:
         """Maps some text from a SAML response to attributes of a new user
         Args:
             saml_response: A SAML auth response object
             failures: How many times a call to this function with this

--- a/synapse/handlers/search.py
+++ b/synapse/handlers/search.py
@@ -4,21 +4,21 @@
 from unpaddedbase64 import decode_base64, encode_base64
 from synapse.api.constants import EventTypes, Membership
 from synapse.api.errors import NotFoundError, SynapseError
 from synapse.api.filtering import Filter
 from synapse.storage.state import StateFilter
 from synapse.visibility import filter_events_for_client
 from ._base import BaseHandler
 logger = logging.getLogger(__name__)
 class SearchHandler(BaseHandler):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(SearchHandler, self).__init__(hs)
         self._event_serializer = hs.get_event_client_serializer()
         self.storage = hs.get_storage()
         self.state_store = self.storage.state
         self.auth = hs.get_auth()
     async def get_old_rooms_from_upgraded_room(self, room_id: str) -> Iterable[str]:
         """Retrieves room IDs of old rooms in the history of an upgraded room.
         We do so by checking the m.room.create event of the room for a
         `predecessor` key. If it exists, we add the room ID to our return
         list and then check that room for a m.room.create event and so on
         until we can no longer find any more previous rooms.
@@ -220,26 +220,26 @@
                     "Context for search returned %d and %d events",
                     len(res["events_before"]),
                     len(res["events_after"]),
                 )
                 res["events_before"] = await filter_events_for_client(
                     self.storage, user.to_string(), res["events_before"]
                 )
                 res["events_after"] = await filter_events_for_client(
                     self.storage, user.to_string(), res["events_after"]
                 )
-                res["start"] = await now_token.copy_and_replace(
+                res["start"] = now_token.copy_and_replace(
                     "room_key", res["start"]
-                ).to_string(self.store)
-                res["end"] = await now_token.copy_and_replace(
+                ).to_string()
+                res["end"] = now_token.copy_and_replace(
                     "room_key", res["end"]
-                ).to_string(self.store)
+                ).to_string()
                 if include_profile:
                     senders = {
                         ev.sender
                         for ev in itertools.chain(
                             res["events_before"], [event], res["events_after"]
                         )
                     }
                     if res["events_after"]:
                         last_event_id = res["events_after"][-1].event_id
                     else:

--- a/synapse/handlers/set_password.py
+++ b/synapse/handlers/set_password.py
@@ -1,20 +1,20 @@
 import logging
 from typing import Optional
 from synapse.api.errors import Codes, StoreError, SynapseError
 from synapse.types import Requester
 from ._base import BaseHandler
 logger = logging.getLogger(__name__)
 class SetPasswordHandler(BaseHandler):
     """Handler which deals with changing user account passwords"""
     def __init__(self, hs):
-        super().__init__(hs)
+        super(SetPasswordHandler, self).__init__(hs)
         self._auth_handler = hs.get_auth_handler()
         self._device_handler = hs.get_device_handler()
         self._password_policy_handler = hs.get_password_policy_handler()
     async def set_password(
         self,
         user_id: str,
         password_hash: str,
         logout_devices: bool,
         requester: Optional[Requester] = None,
     ):

--- a/synapse/handlers/sync.py
+++ b/synapse/handlers/sync.py
@@ -43,82 +43,88 @@
     user = attr.ib(type=UserID)
     filter_collection = attr.ib(type=FilterCollection)
     is_guest = attr.ib(type=bool)
     request_key = attr.ib(type=Tuple[Any, ...])
     device_id = attr.ib(type=str)
 @attr.s(slots=True, frozen=True)
 class TimelineBatch:
     prev_batch = attr.ib(type=StreamToken)
     events = attr.ib(type=List[EventBase])
     limited = attr.ib(bool)
-    def __bool__(self) -> bool:
+    def __nonzero__(self) -> bool:
         """Make the result appear empty if there are no updates. This is used
         to tell if room needs to be part of the sync result.
         """
         return bool(self.events)
+    __bool__ = __nonzero__  # python3
 @attr.s(slots=True)
 class JoinedSyncResult:
     room_id = attr.ib(type=str)
     timeline = attr.ib(type=TimelineBatch)
     state = attr.ib(type=StateMap[EventBase])
     ephemeral = attr.ib(type=List[JsonDict])
     account_data = attr.ib(type=List[JsonDict])
     unread_notifications = attr.ib(type=JsonDict)
     summary = attr.ib(type=Optional[JsonDict])
     unread_count = attr.ib(type=int)
-    def __bool__(self) -> bool:
+    def __nonzero__(self) -> bool:
         """Make the result appear empty if there are no updates. This is used
         to tell if room needs to be part of the sync result.
         """
         return bool(
             self.timeline
             or self.state
             or self.ephemeral
             or self.account_data
         )
+    __bool__ = __nonzero__  # python3
 @attr.s(slots=True, frozen=True)
 class ArchivedSyncResult:
     room_id = attr.ib(type=str)
     timeline = attr.ib(type=TimelineBatch)
     state = attr.ib(type=StateMap[EventBase])
     account_data = attr.ib(type=List[JsonDict])
-    def __bool__(self) -> bool:
+    def __nonzero__(self) -> bool:
         """Make the result appear empty if there are no updates. This is used
         to tell if room needs to be part of the sync result.
         """
         return bool(self.timeline or self.state or self.account_data)
+    __bool__ = __nonzero__  # python3
 @attr.s(slots=True, frozen=True)
 class InvitedSyncResult:
     room_id = attr.ib(type=str)
     invite = attr.ib(type=EventBase)
-    def __bool__(self) -> bool:
+    def __nonzero__(self) -> bool:
         """Invited rooms should always be reported to the client"""
         return True
+    __bool__ = __nonzero__  # python3
 @attr.s(slots=True, frozen=True)
 class GroupsSyncResult:
     join = attr.ib(type=JsonDict)
     invite = attr.ib(type=JsonDict)
     leave = attr.ib(type=JsonDict)
-    def __bool__(self) -> bool:
+    def __nonzero__(self) -> bool:
         return bool(self.join or self.invite or self.leave)
+    __bool__ = __nonzero__  # python3
 @attr.s(slots=True, frozen=True)
 class DeviceLists:
     """
     Attributes:
         changed: List of user_ids whose devices may have changed
         left: List of user_ids whose devices we no longer track
     """
     changed = attr.ib(type=Collection[str])
     left = attr.ib(type=Collection[str])
-    def __bool__(self) -> bool:
+    def __nonzero__(self) -> bool:
         return bool(self.changed or self.left)
-@attr.s(slots=True)
+    __bool__ = __nonzero__  # python3
+@attr.s
 class _RoomChanges:
     """The set of room entries to include in the sync, plus the set of joined
     and left room IDs since last sync.
     """
     room_entries = attr.ib(type=List["RoomSyncResultBuilder"])
     invited = attr.ib(type=List[InvitedSyncResult])
     newly_joined_rooms = attr.ib(type=List[str])
     newly_left_rooms = attr.ib(type=List[str])
 @attr.s(slots=True, frozen=True)
 class SyncResult:
@@ -139,35 +145,36 @@
     next_batch = attr.ib(type=StreamToken)
     presence = attr.ib(type=List[JsonDict])
     account_data = attr.ib(type=List[JsonDict])
     joined = attr.ib(type=List[JoinedSyncResult])
     invited = attr.ib(type=List[InvitedSyncResult])
     archived = attr.ib(type=List[ArchivedSyncResult])
     to_device = attr.ib(type=List[JsonDict])
     device_lists = attr.ib(type=DeviceLists)
     device_one_time_keys_count = attr.ib(type=JsonDict)
     groups = attr.ib(type=Optional[GroupsSyncResult])
-    def __bool__(self) -> bool:
+    def __nonzero__(self) -> bool:
         """Make the result appear empty if there are no updates. This is used
         to tell if the notifier needs to wait for more events when polling for
         events.
         """
         return bool(
             self.presence
             or self.joined
             or self.invited
             or self.archived
             or self.account_data
             or self.to_device
             or self.device_lists
             or self.groups
         )
+    __bool__ = __nonzero__  # python3
 class SyncHandler:
     def __init__(self, hs: "HomeServer"):
         self.hs_config = hs.config
         self.store = hs.get_datastore()
         self.notifier = hs.get_notifier()
         self.presence_handler = hs.get_presence_handler()
         self.event_sources = hs.get_event_sources()
         self.clock = hs.get_clock()
         self.response_cache = ResponseCache(hs, "sync")
         self.state = hs.get_state_handler()
@@ -265,37 +272,37 @@
             now_token: Where the server is currently up to.
             since_token: Where the server was when the client
                 last synced.
         Returns:
             A tuple of the now StreamToken, updated to reflect the which typing
             events are included, and a dict mapping from room_id to a list of
             typing events for that room.
         """
         sync_config = sync_result_builder.sync_config
         with Measure(self.clock, "ephemeral_by_room"):
-            typing_key = since_token.typing_key if since_token else 0
+            typing_key = since_token.typing_key if since_token else "0"
             room_ids = sync_result_builder.joined_room_ids
             typing_source = self.event_sources.sources["typing"]
             typing, typing_key = await typing_source.get_new_events(
                 user=sync_config.user,
                 from_key=typing_key,
                 limit=sync_config.filter_collection.ephemeral_limit(),
                 room_ids=room_ids,
                 is_guest=sync_config.is_guest,
             )
             now_token = now_token.copy_and_replace("typing_key", typing_key)
             ephemeral_by_room = {}  # type: JsonDict
             for event in typing:
                 room_id = event["room_id"]
                 event_copy = {k: v for (k, v) in event.items() if k != "room_id"}
                 ephemeral_by_room.setdefault(room_id, []).append(event_copy)
-            receipt_key = since_token.receipt_key if since_token else 0
+            receipt_key = since_token.receipt_key if since_token else "0"
             receipt_source = self.event_sources.sources["receipt"]
             receipts, receipt_key = await receipt_source.get_new_events(
                 user=sync_config.user,
                 from_key=receipt_key,
                 limit=sync_config.filter_collection.ephemeral_limit(),
                 room_ids=room_ids,
                 is_guest=sync_config.is_guest,
             )
             now_token = now_token.copy_and_replace("receipt_key", receipt_key)
             for event in receipts:
@@ -698,21 +705,21 @@
             sync_config.user,
             since_token,
             now_token,
         )
         user_id = sync_config.user.to_string()
         app_service = self.store.get_app_service_by_user_id(user_id)
         if app_service:
             raise NotImplementedError()
         else:
             joined_room_ids = await self.get_rooms_for_user_at(
-                user_id, now_token.room_key
+                user_id, now_token.room_stream_id
             )
         sync_result_builder = SyncResultBuilder(
             sync_config,
             full_state,
             since_token=since_token,
             now_token=now_token,
             joined_room_ids=joined_room_ids,
         )
         logger.debug("Fetching account data")
         account_data_by_room = await self._generate_sync_entry_for_account_data(
@@ -949,32 +956,32 @@
                 the last sync (or empty if an initial sync)
             newly_joined_or_invited_users: Set of users that have joined or
                 been invited to rooms since the last sync (or empty if an
                 initial sync)
         """
         now_token = sync_result_builder.now_token
         sync_config = sync_result_builder.sync_config
         user = sync_result_builder.sync_config.user
         presence_source = self.event_sources.sources["presence"]
         since_token = sync_result_builder.since_token
-        presence_key = None
-        include_offline = False
         if since_token and not sync_result_builder.full_state:
             presence_key = since_token.presence_key
             include_offline = True
+        else:
+            presence_key = None
+            include_offline = False
         presence, presence_key = await presence_source.get_new_events(
             user=user,
             from_key=presence_key,
             is_guest=sync_config.is_guest,
             include_offline=include_offline,
         )
-        assert presence_key
         sync_result_builder.now_token = now_token.copy_and_replace(
             "presence_key", presence_key
         )
         extra_users_ids = set(newly_joined_or_invited_users)
         for room_id in newly_joined_rooms:
             users = await self.state.get_current_users_in_room(room_id)
             extra_users_ids.update(users)
         extra_users_ids.discard(user.to_string())
         if extra_users_ids:
             states = await self.presence_handler.get_states(extra_users_ids)
@@ -1092,21 +1099,21 @@
         """
         user_id = sync_result_builder.sync_config.user.to_string()
         since_token = sync_result_builder.since_token
         now_token = sync_result_builder.now_token
         assert since_token
         rooms_changed = await self.store.get_membership_changes_for_user(
             user_id, since_token.room_key, now_token.room_key
         )
         if rooms_changed:
             return True
-        stream_id = since_token.room_key.stream
+        stream_id = RoomStreamToken.parse_stream_token(since_token.room_key).stream
         for room_id in sync_result_builder.joined_room_ids:
             if self.store.has_room_changed_since(room_id, stream_id):
                 return True
         return False
     async def _get_rooms_changed(
         self, sync_result_builder: "SyncResultBuilder", ignored_users: Set[str]
     ) -> _RoomChanges:
         """Gets the the changes that have happened since the last sync.
         """
         user_id = sync_result_builder.sync_config.user.to_string()
@@ -1182,30 +1189,28 @@
                     room_sync = InvitedSyncResult(room_id, invite=non_joins[-1])
                     if room_sync:
                         invited.append(room_sync)
             leave_events = [
                 e
                 for e in non_joins
                 if e.membership in (Membership.LEAVE, Membership.BAN)
             ]
             if leave_events:
                 leave_event = leave_events[-1]
-                leave_position = await self.store.get_position_for_event(
+                leave_stream_token = await self.store.get_stream_token_for_event(
                     leave_event.event_id
                 )
-                if since_token and not leave_position.persisted_after(
-                    since_token.room_key
-                ):
+                leave_token = since_token.copy_and_replace(
+                    "room_key", leave_stream_token
+                )
+                if since_token and since_token.is_after(leave_token):
                     continue
-                leave_token = since_token.copy_and_replace(
-                    "room_key", leave_position.to_room_stream_token()
-                )
                 if leave_event.internal_metadata.is_out_of_band_membership():
                     batch_events = [leave_event]  # type: Optional[List[EventBase]]
                 else:
                     batch_events = None
                 room_entries.append(
                     RoomSyncResultBuilder(
                         room_id=room_id,
                         rtype="archived",
                         events=batch_events,
                         newly_joined=room_id in newly_joined_rooms,
@@ -1294,21 +1299,21 @@
                 if event.sender in ignored_users:
                     continue
                 invite = await self.store.get_event(event.event_id)
                 invited.append(InvitedSyncResult(room_id=event.room_id, invite=invite))
             elif event.membership in (Membership.LEAVE, Membership.BAN):
                 if not sync_config.filter_collection.include_leave:
                     if event.membership == Membership.LEAVE:
                         if user_id == event.sender:
                             continue
                 leave_token = now_token.copy_and_replace(
-                    "room_key", RoomStreamToken(None, event.stream_ordering)
+                    "room_key", "s%d" % (event.stream_ordering,)
                 )
                 room_entries.append(
                     RoomSyncResultBuilder(
                         room_id=event.room_id,
                         rtype="archived",
                         events=None,
                         newly_joined=False,
                         full_state=True,
                         since_token=since_token,
                         upto_token=leave_token,
@@ -1427,41 +1432,41 @@
                 room_id=room_id,
                 timeline=batch,
                 state=state,
                 account_data=account_data_events,
             )
             if archived_room_sync or always_include:
                 sync_result_builder.archived.append(archived_room_sync)
         else:
             raise Exception("Unrecognized rtype: %r", room_builder.rtype)
     async def get_rooms_for_user_at(
-        self, user_id: str, room_key: RoomStreamToken
+        self, user_id: str, stream_ordering: int
     ) -> FrozenSet[str]:
         """Get set of joined rooms for a user at the given stream ordering.
         The stream ordering *must* be recent, otherwise this may throw an
         exception if older than a month. (This function is called with the
         current token, which should be perfectly fine).
         Args:
             user_id
             stream_ordering
         ReturnValue:
             Set of room_ids the user is in at given stream_ordering.
         """
         joined_rooms = await self.store.get_rooms_for_user_with_stream_ordering(user_id)
         joined_room_ids = set()
-        for room_id, event_pos in joined_rooms:
-            if not event_pos.persisted_after(room_key):
+        for room_id, membership_stream_ordering in joined_rooms:
+            if membership_stream_ordering <= stream_ordering:
                 joined_room_ids.add(room_id)
                 continue
             logger.info("User joined room after current token: %s", room_id)
             extrems = await self.store.get_forward_extremeties_for_room(
-                room_id, event_pos.stream
+                room_id, stream_ordering
             )
             users_in_room = await self.state.get_current_users_in_room(room_id, extrems)
             if user_id in users_in_room:
                 joined_room_ids.add(room_id)
         return frozenset(joined_room_ids)
 def _action_has_highlight(actions: List[JsonDict]) -> bool:
     for action in actions:
         try:
             if action.get("set_tweak", None) == "highlight":
                 return action.get("value", True)
@@ -1498,21 +1503,21 @@
     c_ids = set(current.values())
     ts_ids = set(timeline_start.values())
     p_ids = set(previous.values())
     tc_ids = set(timeline_contains.values())
     if lazy_load_members:
         p_ids.difference_update(
             e for t, e in timeline_start.items() if t[0] == EventTypes.Member
         )
     state_ids = ((c_ids | ts_ids) - p_ids) - tc_ids
     return {event_id_to_key[e]: e for e in state_ids}
-@attr.s(slots=True)
+@attr.s
 class SyncResultBuilder:
     """Used to help build up a new SyncResult for a user
     Attributes:
         sync_config
         full_state: The full_state flag as specified by user
         since_token: The token supplied by user, or None.
         now_token: The token to sync up to.
         joined_room_ids: List of rooms the user is joined to
         presence (list)
         account_data (list)
@@ -1527,21 +1532,21 @@
     since_token = attr.ib(type=Optional[StreamToken])
     now_token = attr.ib(type=StreamToken)
     joined_room_ids = attr.ib(type=FrozenSet[str])
     presence = attr.ib(type=List[JsonDict], default=attr.Factory(list))
     account_data = attr.ib(type=List[JsonDict], default=attr.Factory(list))
     joined = attr.ib(type=List[JoinedSyncResult], default=attr.Factory(list))
     invited = attr.ib(type=List[InvitedSyncResult], default=attr.Factory(list))
     archived = attr.ib(type=List[ArchivedSyncResult], default=attr.Factory(list))
     groups = attr.ib(type=Optional[GroupsSyncResult], default=None)
     to_device = attr.ib(type=List[JsonDict], default=attr.Factory(list))
-@attr.s(slots=True)
+@attr.s
 class RoomSyncResultBuilder:
     """Stores information needed to create either a `JoinedSyncResult` or
     `ArchivedSyncResult`.
     Attributes:
         room_id
         rtype: One of `"joined"` or `"archived"`
         events: List of events to include in the room (more events may be added
             when generating result).
         newly_joined: If the user has newly joined the room
         full_state: Whether the full state should be sent in result

--- a/synapse/handlers/user_directory.py
+++ b/synapse/handlers/user_directory.py
@@ -8,21 +8,21 @@
 logger = logging.getLogger(__name__)
 class UserDirectoryHandler(StateDeltasHandler):
     """Handles querying of and keeping updated the user_directory.
     N.B.: ASSUMES IT IS THE ONLY THING THAT MODIFIES THE USER DIRECTORY
     The user directory is filled with users who this server can see are joined to a
     world_readable or publically joinable room. We keep a database table up to date
     by streaming changes of the current state and recalculating whether users should
     be in the directory or not when necessary.
     """
     def __init__(self, hs):
-        super().__init__(hs)
+        super(UserDirectoryHandler, self).__init__(hs)
         self.store = hs.get_datastore()
         self.state = hs.get_state_handler()
         self.server_name = hs.hostname
         self.clock = hs.get_clock()
         self.notifier = hs.get_notifier()
         self.is_mine_id = hs.is_mine_id
         self.update_user_directory = hs.config.update_user_directory
         self.search_all_users = hs.config.user_directory_search_all_users
         self.spam_checker = hs.get_spam_checker()
         self.pos = None

--- a/synapse/http/__init__.py
+++ b/synapse/http/__init__.py
@@ -1,18 +1,28 @@
 import re
 from twisted.internet import task
+from twisted.internet.defer import CancelledError
+from twisted.python import failure
 from twisted.web.client import FileBodyProducer
 from synapse.api.errors import SynapseError
 class RequestTimedOutError(SynapseError):
     """Exception representing timeout of an outbound request"""
-    def __init__(self, msg):
-        super().__init__(504, msg)
+    def __init__(self):
+        super(RequestTimedOutError, self).__init__(504, "Timed out")
+def cancelled_to_request_timed_out_error(value, timeout):
+    """Turns CancelledErrors into RequestTimedOutErrors.
+    For use with async.add_timeout_to_deferred
+    """
+    if isinstance(value, failure.Failure):
+        value.trap(CancelledError)
+        raise RequestTimedOutError()
+    return value
 ACCESS_TOKEN_RE = re.compile(r"(\?.*access(_|%5[Ff])token=)[^&]*(.*)$")
 CLIENT_SECRET_RE = re.compile(r"(\?.*client(_|%5[Ff])secret=)[^&]*(.*)$")
 def redact_uri(uri):
     """Strips sensitive information from the uri replaces with <redacted>"""
     uri = ACCESS_TOKEN_RE.sub(r"\1<redacted>\3", uri)
     return CLIENT_SECRET_RE.sub(r"\1<redacted>\3", uri)
 class QuieterFileBodyProducer(FileBodyProducer):
     """Wrapper for FileBodyProducer that avoids CRITICAL errors when the connection drops.
     Workaround for https://github.com/matrix-org/synapse/issues/4003 /
     https://twistedmatrix.com/trac/ticket/6528

--- a/synapse/http/client.py
+++ b/synapse/http/client.py
@@ -1,65 +1,47 @@
 import logging
 import urllib
 from io import BytesIO
-from typing import (
-    Any,
-    BinaryIO,
-    Dict,
-    Iterable,
-    List,
-    Mapping,
-    Optional,
-    Sequence,
-    Tuple,
-    Union,
-)
 import treq
 from canonicaljson import encode_canonical_json
 from netaddr import IPAddress
 from prometheus_client import Counter
 from zope.interface import implementer, provider
 from OpenSSL import SSL
 from OpenSSL.SSL import VERIFY_NONE
-from twisted.internet import defer, error as twisted_error, protocol, ssl
+from twisted.internet import defer, protocol, ssl
 from twisted.internet.interfaces import (
     IReactorPluggableNameResolver,
     IResolutionReceiver,
 )
 from twisted.internet.task import Cooperator
 from twisted.python.failure import Failure
 from twisted.web._newclient import ResponseDone
-from twisted.web.client import (
-    Agent,
-    HTTPConnectionPool,
-    ResponseNeverReceived,
-    readBody,
-)
+from twisted.web.client import Agent, HTTPConnectionPool, readBody
 from twisted.web.http import PotentialDataLoss
 from twisted.web.http_headers import Headers
-from twisted.web.iweb import IResponse
 from synapse.api.errors import Codes, HttpResponseException, SynapseError
-from synapse.http import QuieterFileBodyProducer, RequestTimedOutError, redact_uri
+from synapse.http import (
+    QuieterFileBodyProducer,
+    cancelled_to_request_timed_out_error,
+    redact_uri,
+)
 from synapse.http.proxyagent import ProxyAgent
 from synapse.logging.context import make_deferred_yieldable
 from synapse.logging.opentracing import set_tag, start_active_span, tags
 from synapse.util import json_decoder
 from synapse.util.async_helpers import timeout_deferred
 logger = logging.getLogger(__name__)
 outgoing_requests_counter = Counter("synapse_http_client_requests", "", ["method"])
 incoming_responses_counter = Counter(
     "synapse_http_client_responses", "", ["method", "code"]
 )
-RawHeaders = Union[Mapping[str, "RawHeaderValue"], Mapping[bytes, "RawHeaderValue"]]
-RawHeaderValue = Sequence[Union[str, bytes]]
-QueryParamValue = Union[str, bytes, Iterable[Union[str, bytes]]]
-QueryParams = Union[Mapping[str, QueryParamValue], Mapping[bytes, QueryParamValue]]
 def check_against_blacklist(ip_address, ip_whitelist, ip_blacklist):
     """
     Args:
         ip_address (netaddr.IPAddress)
         ip_whitelist (netaddr.IPSet)
         ip_blacklist (netaddr.IPSet)
     """
     if ip_address in ip_blacklist:
         if ip_whitelist is None or ip_address not in ip_whitelist:
             return True
@@ -214,37 +196,27 @@
             http_proxy=http_proxy,
             https_proxy=https_proxy,
         )
         if self._ip_blacklist:
             self.agent = BlacklistingAgentWrapper(
                 self.agent,
                 self.reactor,
                 ip_whitelist=self._ip_whitelist,
                 ip_blacklist=self._ip_blacklist,
             )
-    async def request(
-        self,
-        method: str,
-        uri: str,
-        data: Optional[bytes] = None,
-        headers: Optional[Headers] = None,
-    ) -> IResponse:
-        """
-        Args:
-            method: HTTP method to use.
-            uri: URI to query.
-            data: Data to send in the request body, if applicable.
-            headers: Request headers.
-        Returns:
-            Response object, once the headers have been read.
-        Raises:
-            RequestTimedOutError if the request times out before the headers are read
+    async def request(self, method, uri, data=None, headers=None):
+        """
+        Args:
+            method (str): HTTP method to use.
+            uri (str): URI to query.
+            data (bytes): Data to send in the request body, if applicable.
+            headers (t.w.http_headers.Headers): Request headers.
         """
         outgoing_requests_counter.labels(method).inc()
         logger.debug("Sending request %s %s", method, redact_uri(uri))
         with start_active_span(
             "outgoing-client-request",
             tags={
                 tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,
                 tags.HTTP_METHOD: method,
                 tags.HTTP_URL: uri,
             },
@@ -256,25 +228,27 @@
                     body_producer = QuieterFileBodyProducer(
                         BytesIO(data), cooperator=self._cooperator,
                     )
                 request_deferred = treq.request(
                     method,
                     uri,
                     agent=self.agent,
                     data=body_producer,
                     headers=headers,
                     **self._extra_treq_args
-                )  # type: defer.Deferred
+                )
                 request_deferred = timeout_deferred(
-                    request_deferred, 60, self.hs.get_reactor(),
+                    request_deferred,
+                    60,
+                    self.hs.get_reactor(),
+                    cancelled_to_request_timed_out_error,
                 )
-                request_deferred.addErrback(_timeout_to_request_timed_out_error)
                 response = await make_deferred_yieldable(request_deferred)
                 incoming_responses_counter.labels(method, response.code).inc()
                 logger.info(
                     "Received response to %s %s: %s",
                     method,
                     redact_uri(uri),
                     response.code,
                 )
                 return response
             except Exception as e:
@@ -282,37 +256,30 @@
                 logger.info(
                     "Error sending request to  %s %s: %s %s",
                     method,
                     redact_uri(uri),
                     type(e).__name__,
                     e.args[0],
                 )
                 set_tag(tags.ERROR, True)
                 set_tag("error_reason", e.args[0])
                 raise
-    async def post_urlencoded_get_json(
-        self,
-        uri: str,
-        args: Mapping[str, Union[str, List[str]]] = {},
-        headers: Optional[RawHeaders] = None,
-    ) -> Any:
-        """
-        Args:
-            uri: uri to query
-            args: parameters to be url-encoded in the body
-            headers: a map from header name to a list of values for that header
+    async def post_urlencoded_get_json(self, uri, args={}, headers=None):
+        """
+        Args:
+            uri (str):
+            args (dict[str, str|List[str]]): query params
+            headers (dict[str|bytes, List[str|bytes]]|None): If not None, a map from
+               header name to a list of values for that header
         Returns:
-            parsed json
+            object: parsed json
         Raises:
-            RequestTimedOutError: if there is a timeout before the response headers
-               are received. Note there is currently no timeout on reading the response
-               body.
             HttpResponseException: On a non-2xx HTTP response.
             ValueError: if the response was not JSON
         """
         logger.debug("post_urlencoded_get_json args: %s", args)
         query_bytes = urllib.parse.urlencode(encode_urlencode_args(args), True).encode(
             "utf8"
         )
         actual_headers = {
             b"Content-Type": [b"application/x-www-form-urlencoded"],
             b"User-Agent": [self.user_agent],
@@ -323,34 +290,30 @@
         response = await self.request(
             "POST", uri, headers=Headers(actual_headers), data=query_bytes
         )
         body = await make_deferred_yieldable(readBody(response))
         if 200 <= response.code < 300:
             return json_decoder.decode(body.decode("utf-8"))
         else:
             raise HttpResponseException(
                 response.code, response.phrase.decode("ascii", errors="replace"), body
             )
-    async def post_json_get_json(
-        self, uri: str, post_json: Any, headers: Optional[RawHeaders] = None
-    ) -> Any:
-        """
-        Args:
-            uri: URI to query.
-            post_json: request body, to be encoded as json
-            headers: a map from header name to a list of values for that header
+    async def post_json_get_json(self, uri, post_json, headers=None):
+        """
+        Args:
+            uri (str):
+            post_json (object):
+            headers (dict[str|bytes, List[str|bytes]]|None): If not None, a map from
+               header name to a list of values for that header
         Returns:
-            parsed json
+            object: parsed json
         Raises:
-            RequestTimedOutError: if there is a timeout before the response headers
-               are received. Note there is currently no timeout on reading the response
-               body.
             HttpResponseException: On a non-2xx HTTP response.
             ValueError: if the response was not JSON
         """
         json_str = encode_canonical_json(post_json)
         logger.debug("HTTP POST %s -> %s", json_str, uri)
         actual_headers = {
             b"Content-Type": [b"application/json"],
             b"User-Agent": [self.user_agent],
             b"Accept": [b"application/json"],
         }
@@ -359,61 +322,57 @@
         response = await self.request(
             "POST", uri, headers=Headers(actual_headers), data=json_str
         )
         body = await make_deferred_yieldable(readBody(response))
         if 200 <= response.code < 300:
             return json_decoder.decode(body.decode("utf-8"))
         else:
             raise HttpResponseException(
                 response.code, response.phrase.decode("ascii", errors="replace"), body
             )
-    async def get_json(
-        self, uri: str, args: QueryParams = {}, headers: Optional[RawHeaders] = None,
-    ) -> Any:
-        """Gets some json from the given URI.
-        Args:
-            uri: The URI to request, not including query parameters
-            args: A dictionary used to create query string
-            headers: a map from header name to a list of values for that header
+    async def get_json(self, uri, args={}, headers=None):
+        """ Gets some json from the given URI.
+        Args:
+            uri (str): The URI to request, not including query parameters
+            args (dict): A dictionary used to create query strings, defaults to
+                None.
+                **Note**: The value of each key is assumed to be an iterable
+                and *not* a string.
+            headers (dict[str|bytes, List[str|bytes]]|None): If not None, a map from
+               header name to a list of values for that header
         Returns:
-            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.
+            Succeeds when we get *any* 2xx HTTP response, with the
+            HTTP body as JSON.
         Raises:
-            RequestTimedOutError: if there is a timeout before the response headers
-               are received. Note there is currently no timeout on reading the response
-               body.
             HttpResponseException On a non-2xx HTTP response.
             ValueError: if the response was not JSON
         """
         actual_headers = {b"Accept": [b"application/json"]}
         if headers:
             actual_headers.update(headers)
         body = await self.get_raw(uri, args, headers=headers)
         return json_decoder.decode(body.decode("utf-8"))
-    async def put_json(
-        self,
-        uri: str,
-        json_body: Any,
-        args: QueryParams = {},
-        headers: RawHeaders = None,
-    ) -> Any:
-        """Puts some json to the given URI.
-        Args:
-            uri: The URI to request, not including query parameters
-            json_body: The JSON to put in the HTTP body,
-            args: A dictionary used to create query strings
-            headers: a map from header name to a list of values for that header
+    async def put_json(self, uri, json_body, args={}, headers=None):
+        """ Puts some json to the given URI.
+        Args:
+            uri (str): The URI to request, not including query parameters
+            json_body (dict): The JSON to put in the HTTP body,
+            args (dict): A dictionary used to create query strings, defaults to
+                None.
+                **Note**: The value of each key is assumed to be an iterable
+                and *not* a string.
+            headers (dict[str|bytes, List[str|bytes]]|None): If not None, a map from
+               header name to a list of values for that header
         Returns:
-            Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.
+            Succeeds when we get *any* 2xx HTTP response, with the
+            HTTP body as JSON.
         Raises:
-             RequestTimedOutError: if there is a timeout before the response headers
-               are received. Note there is currently no timeout on reading the response
-               body.
             HttpResponseException On a non-2xx HTTP response.
             ValueError: if the response was not JSON
         """
         if len(args):
             query_bytes = urllib.parse.urlencode(args, True)
             uri = "%s?%s" % (uri, query_bytes)
         json_str = encode_canonical_json(json_body)
         actual_headers = {
             b"Content-Type": [b"application/json"],
             b"User-Agent": [self.user_agent],
@@ -424,72 +383,60 @@
         response = await self.request(
             "PUT", uri, headers=Headers(actual_headers), data=json_str
         )
         body = await make_deferred_yieldable(readBody(response))
         if 200 <= response.code < 300:
             return json_decoder.decode(body.decode("utf-8"))
         else:
             raise HttpResponseException(
                 response.code, response.phrase.decode("ascii", errors="replace"), body
             )
-    async def get_raw(
-        self, uri: str, args: QueryParams = {}, headers: Optional[RawHeaders] = None
-    ) -> bytes:
-        """Gets raw text from the given URI.
-        Args:
-            uri: The URI to request, not including query parameters
-            args: A dictionary used to create query strings
-            headers: a map from header name to a list of values for that header
+    async def get_raw(self, uri, args={}, headers=None):
+        """ Gets raw text from the given URI.
+        Args:
+            uri (str): The URI to request, not including query parameters
+            args (dict): A dictionary used to create query strings, defaults to
+                None.
+                **Note**: The value of each key is assumed to be an iterable
+                and *not* a string.
+            headers (dict[str|bytes, List[str|bytes]]|None): If not None, a map from
+               header name to a list of values for that header
         Returns:
-            Succeeds when we get a 2xx HTTP response, with the
+            Succeeds when we get *any* 2xx HTTP response, with the
             HTTP body as bytes.
         Raises:
-            RequestTimedOutError: if there is a timeout before the response headers
-               are received. Note there is currently no timeout on reading the response
-               body.
             HttpResponseException on a non-2xx HTTP response.
         """
         if len(args):
             query_bytes = urllib.parse.urlencode(args, True)
             uri = "%s?%s" % (uri, query_bytes)
         actual_headers = {b"User-Agent": [self.user_agent]}
         if headers:
             actual_headers.update(headers)
         response = await self.request("GET", uri, headers=Headers(actual_headers))
         body = await make_deferred_yieldable(readBody(response))
         if 200 <= response.code < 300:
             return body
         else:
             raise HttpResponseException(
                 response.code, response.phrase.decode("ascii", errors="replace"), body
             )
-    async def get_file(
-        self,
-        url: str,
-        output_stream: BinaryIO,
-        max_size: Optional[int] = None,
-        headers: Optional[RawHeaders] = None,
-    ) -> Tuple[int, Dict[bytes, List[bytes]], str, int]:
+    async def get_file(self, url, output_stream, max_size=None, headers=None):
         """GETs a file from a given URL
         Args:
-            url: The URL to GET
-            output_stream: File to write the response body to.
-            headers: A map from header name to a list of values for that header
+            url (str): The URL to GET
+            output_stream (file): File to write the response body to.
+            headers (dict[str|bytes, List[str|bytes]]|None): If not None, a map from
+               header name to a list of values for that header
         Returns:
-            A tuple of the file length, dict of the response
+            A (int,dict,string,int) tuple of the file length, dict of the response
             headers, absolute URI of the response and HTTP response code.
-        Raises:
-            RequestTimedOutError: if there is a timeout before the response headers
-               are received. Note there is currently no timeout on reading the response
-               body.
-            SynapseError: if the response is not a 2xx, the remote file is too large, or
-               another exception happens during the download.
         """
         actual_headers = {b"User-Agent": [self.user_agent]}
         if headers:
             actual_headers.update(headers)
         response = await self.request("GET", url, headers=Headers(actual_headers))
         resp_headers = dict(response.headers.getAllRawHeaders())
         if (
             b"Content-Length" in resp_headers
             and int(resp_headers[b"Content-Length"][0]) > max_size
         ):
@@ -509,26 +456,20 @@
         except SynapseError:
             raise
         except Exception as e:
             raise SynapseError(502, ("Failed to download remote body: %s" % e)) from e
         return (
             length,
             resp_headers,
             response.request.absoluteURI.decode("ascii"),
             response.code,
         )
-def _timeout_to_request_timed_out_error(f: Failure):
-    if f.check(twisted_error.TimeoutError, twisted_error.ConnectingCancelledError):
-        raise RequestTimedOutError("Timeout connecting to remote server")
-    elif f.check(defer.TimeoutError, ResponseNeverReceived):
-        raise RequestTimedOutError("Timeout waiting for response from remote server")
-    return f
 class _ReadBodyToFileProtocol(protocol.Protocol):
     def __init__(self, stream, deferred, max_size):
         self.stream = stream
         self.deferred = deferred
         self.length = 0
         self.max_size = max_size
     def dataReceived(self, data):
         self.stream.write(data)
         self.length += len(data)
         if self.max_size is not None and self.length >= self.max_size:

--- a/synapse/http/federation/well_known_resolver.py
+++ b/synapse/http/federation/well_known_resolver.py
@@ -188,13 +188,13 @@
     return None
 def _parse_cache_control(headers: Headers) -> Dict[bytes, Optional[bytes]]:
     cache_controls = {}
     for hdr in headers.getRawHeaders(b"cache-control", []):
         for directive in hdr.split(b","):
             splits = [x.strip() for x in directive.split(b"=", 1)]
             k = splits[0].lower()
             v = splits[1] if len(splits) > 1 else None
             cache_controls[k] = v
     return cache_controls
-@attr.s(slots=True)
+@attr.s()
 class _FetchWellKnownFailure(Exception):
     temporary = attr.ib()

--- a/synapse/http/matrixfederationclient.py
+++ b/synapse/http/matrixfederationclient.py
@@ -43,21 +43,21 @@
 outgoing_requests_counter = Counter(
     "synapse_http_matrixfederationclient_requests", "", ["method"]
 )
 incoming_responses_counter = Counter(
     "synapse_http_matrixfederationclient_responses", "", ["method", "code"]
 )
 MAX_LONG_RETRIES = 10
 MAX_SHORT_RETRIES = 3
 MAXINT = sys.maxsize
 _next_id = 1
-@attr.s(slots=True, frozen=True)
+@attr.s(frozen=True)
 class MatrixFederationRequest:
     method = attr.ib()
     """HTTP method
     :type: str
     """
     path = attr.ib()
     """HTTP path
     :type: str
     """
     destination = attr.ib()
@@ -118,21 +118,21 @@
         start_ms: Timestamp when request was made
     Returns:
         dict: parsed JSON response
     """
     try:
         check_content_type_is_json(response.headers)
         d = treq.text_content(response, encoding="utf-8")
         d.addCallback(json_decoder.decode)
         d = timeout_deferred(d, timeout=timeout_sec, reactor=reactor)
         body = await make_deferred_yieldable(d)
-    except defer.TimeoutError as e:
+    except TimeoutError as e:
         logger.warning(
             "{%s} [%s] Timed out reading response - %s %s",
             request.txn_id,
             request.destination,
             request.method,
             request.uri.decode("ascii"),
         )
         raise RequestSendFailed(e, can_retry=True) from e
     except Exception as e:
         logger.warning(
@@ -351,20 +351,22 @@
                                 url_bytes,
                                 headers=Headers(headers_dict),
                                 bodyProducer=producer,
                             )
                             request_deferred = timeout_deferred(
                                 request_deferred,
                                 timeout=_sec_timeout,
                                 reactor=self.reactor,
                             )
                             response = await request_deferred
+                    except TimeoutError as e:
+                        raise RequestSendFailed(e, can_retry=True) from e
                     except DNSLookupError as e:
                         raise RequestSendFailed(e, can_retry=retry_on_dns_fail) from e
                     except Exception as e:
                         raise RequestSendFailed(e, can_retry=True) from e
                     incoming_responses_counter.labels(
                         request.method, response.code
                     ).inc()
                     set_tag(tags.HTTP_STATUS_CODE, response.code)
                     response_phrase = response.phrase.decode("ascii", errors="replace")
                     if 200 <= response.code < 300:
@@ -503,26 +505,23 @@
             destination (str): The remote server to send the HTTP request
                 to.
             path (str): The HTTP path.
             args (dict): query params
             data (dict): A dict containing the data that will be used as
                 the request body. This will be encoded as JSON.
             json_data_callback (callable): A callable returning the dict to
                 use as the request body.
             long_retries (bool): whether to use the long retry algorithm. See
                 docs on _send_request for details.
-            timeout (int|None): number of milliseconds to wait for the response.
+            timeout (int|None): number of milliseconds to wait for the response headers
+                (including connecting to the server), *for each attempt*.
                 self._default_timeout (60s) by default.
-                Note that we may make several attempts to send the request; this
-                timeout applies to the time spent waiting for response headers for
-                *each* attempt (including connection time) as well as the time spent
-                reading the response body after a 200 response.
             ignore_backoff (bool): true to ignore the historical backoff data
                 and try the request anyway.
             backoff_on_404 (bool): True if we should count a 404 response as
                 a failure of the server (and should therefore back off future
                 requests).
             try_trailing_slash_on_400 (bool): True if on a 400 M_UNRECOGNIZED
                 response we should try appending a trailing slash to the end
                 of the request. Workaround for #3622 in Synapse <= v0.99.3. This
                 will be attempted before backing off if backing off has been
                 enabled.
@@ -549,26 +548,22 @@
         )
         start_ms = self.clock.time_msec()
         response = await self._send_request_with_optional_trailing_slash(
             request,
             try_trailing_slash_on_400,
             backoff_on_404=backoff_on_404,
             ignore_backoff=ignore_backoff,
             long_retries=long_retries,
             timeout=timeout,
         )
-        if timeout is not None:
-            _sec_timeout = timeout / 1000
-        else:
-            _sec_timeout = self.default_timeout
         body = await _handle_json_response(
-            self.reactor, _sec_timeout, request, response, start_ms
+            self.reactor, self.default_timeout, request, response, start_ms
         )
         return body
     async def post_json(
         self,
         destination,
         path,
         data={},
         long_retries=False,
         timeout=None,
         ignore_backoff=False,
@@ -576,26 +571,23 @@
     ):
         """ Sends the specifed json data using POST
         Args:
             destination (str): The remote server to send the HTTP request
                 to.
             path (str): The HTTP path.
             data (dict): A dict containing the data that will be used as
                 the request body. This will be encoded as JSON.
             long_retries (bool): whether to use the long retry algorithm. See
                 docs on _send_request for details.
-            timeout (int|None): number of milliseconds to wait for the response.
+            timeout (int|None): number of milliseconds to wait for the response headers
+                (including connecting to the server), *for each attempt*.
                 self._default_timeout (60s) by default.
-                Note that we may make several attempts to send the request; this
-                timeout applies to the time spent waiting for response headers for
-                *each* attempt (including connection time) as well as the time spent
-                reading the response body after a 200 response.
             ignore_backoff (bool): true to ignore the historical backoff data and
                 try the request anyway.
             args (dict): query params
         Returns:
             dict|list: Succeeds when we get a 2xx HTTP response. The
             result will be the decoded JSON body.
         Raises:
             HttpResponseException: If we get an HTTP response code >= 300
                 (except 429).
             NotRetryingDestination: If we are not yet ready to retry this
@@ -633,26 +625,23 @@
         ignore_backoff=False,
         try_trailing_slash_on_400=False,
     ):
         """ GETs some json from the given host homeserver and path
         Args:
             destination (str): The remote server to send the HTTP request
                 to.
             path (str): The HTTP path.
             args (dict|None): A dictionary used to create query strings, defaults to
                 None.
-            timeout (int|None): number of milliseconds to wait for the response.
+            timeout (int|None): number of milliseconds to wait for the response headers
+                (including connecting to the server), *for each attempt*.
                 self._default_timeout (60s) by default.
-                Note that we may make several attempts to send the request; this
-                timeout applies to the time spent waiting for response headers for
-                *each* attempt (including connection time) as well as the time spent
-                reading the response body after a 200 response.
             ignore_backoff (bool): true to ignore the historical backoff data
                 and try the request anyway.
             try_trailing_slash_on_400 (bool): True if on a 400 M_UNRECOGNIZED
                 response we should try appending a trailing slash to the end of
                 the request. Workaround for #3622 in Synapse <= v0.99.3.
         Returns:
             dict|list: Succeeds when we get a 2xx HTTP response. The
             result will be the decoded JSON body.
         Raises:
             HttpResponseException: If we get an HTTP response code >= 300
@@ -669,50 +658,43 @@
         )
         start_ms = self.clock.time_msec()
         response = await self._send_request_with_optional_trailing_slash(
             request,
             try_trailing_slash_on_400,
             backoff_on_404=False,
             ignore_backoff=ignore_backoff,
             retry_on_dns_fail=retry_on_dns_fail,
             timeout=timeout,
         )
-        if timeout is not None:
-            _sec_timeout = timeout / 1000
-        else:
-            _sec_timeout = self.default_timeout
         body = await _handle_json_response(
-            self.reactor, _sec_timeout, request, response, start_ms
+            self.reactor, self.default_timeout, request, response, start_ms
         )
         return body
     async def delete_json(
         self,
         destination,
         path,
         long_retries=False,
         timeout=None,
         ignore_backoff=False,
         args={},
     ):
         """Send a DELETE request to the remote expecting some json response
         Args:
             destination (str): The remote server to send the HTTP request
                 to.
             path (str): The HTTP path.
             long_retries (bool): whether to use the long retry algorithm. See
                 docs on _send_request for details.
-            timeout (int|None): number of milliseconds to wait for the response.
+            timeout (int|None): number of milliseconds to wait for the response headers
+                (including connecting to the server), *for each attempt*.
                 self._default_timeout (60s) by default.
-                Note that we may make several attempts to send the request; this
-                timeout applies to the time spent waiting for response headers for
-                *each* attempt (including connection time) as well as the time spent
-                reading the response body after a 200 response.
             ignore_backoff (bool): true to ignore the historical backoff data and
                 try the request anyway.
             args (dict): query params
         Returns:
             dict|list: Succeeds when we get a 2xx HTTP response. The
             result will be the decoded JSON body.
         Raises:
             HttpResponseException: If we get an HTTP response code >= 300
                 (except 429).
             NotRetryingDestination: If we are not yet ready to retry this
@@ -725,26 +707,22 @@
         request = MatrixFederationRequest(
             method="DELETE", destination=destination, path=path, query=args
         )
         start_ms = self.clock.time_msec()
         response = await self._send_request(
             request,
             long_retries=long_retries,
             timeout=timeout,
             ignore_backoff=ignore_backoff,
         )
-        if timeout is not None:
-            _sec_timeout = timeout / 1000
-        else:
-            _sec_timeout = self.default_timeout
         body = await _handle_json_response(
-            self.reactor, _sec_timeout, request, response, start_ms
+            self.reactor, self.default_timeout, request, response, start_ms
         )
         return body
     async def get_file(
         self,
         destination,
         path,
         output_stream,
         args={},
         retry_on_dns_fail=True,
         max_size=None,

--- a/synapse/http/proxyagent.py
+++ b/synapse/http/proxyagent.py
@@ -13,24 +13,22 @@
 @implementer(IAgent)
 class ProxyAgent(_AgentBase):
     """An Agent implementation which will use an HTTP proxy if one was requested
     Args:
         reactor: twisted reactor to place outgoing
             connections.
         contextFactory (IPolicyForHTTPS): A factory for TLS contexts, to control the
             verification parameters of OpenSSL.  The default is to use a
             `BrowserLikePolicyForHTTPS`, so unless you have special
             requirements you can leave this as-is.
-        connectTimeout (Optional[float]): The amount of time that this Agent will wait
-            for the peer to accept a connection, in seconds. If 'None',
-            HostnameEndpoint's default (30s) will be used.
-            This is used for connections to both proxies and destination servers.
+        connectTimeout (float): The amount of time that this Agent will wait
+            for the peer to accept a connection.
         bindAddress (bytes): The local address for client sockets to bind to.
         pool (HTTPConnectionPool|None): connection pool to be used. If None, a
             non-persistent pool instance will be created.
     """
     def __init__(
         self,
         reactor,
         contextFactory=BrowserLikePolicyForHTTPS(),
         connectTimeout=None,
         bindAddress=None,
@@ -63,26 +61,20 @@
             method (bytes): The request method to use, such as `GET`, `POST`, etc
             uri (bytes): The location of the resource to request.
             headers (Headers|None): Extra headers to send with the request
             bodyProducer (IBodyProducer|None): An object which can generate bytes to
                 make up the body of this request (for example, the properly encoded
                 contents of a file for a file upload). Or, None if the request is to
                 have no body.
         Returns:
             Deferred[IResponse]: completes when the header of the response has
                  been received (regardless of the response status code).
-                 Can fail with:
-                    SchemeNotSupported: if the uri is not http or https
-                    twisted.internet.error.TimeoutError if the server we are connecting
-                        to (proxy or destination) does not accept a connection before
-                        connectTimeout.
-                    ... other things too.
         """
         uri = uri.strip()
         if not _VALID_URI.match(uri):
             raise ValueError("Invalid URI {!r}".format(uri))
         parsed_uri = URI.fromBytes(uri)
         pool_key = (parsed_uri.scheme, parsed_uri.host, parsed_uri.port)
         request_path = parsed_uri.originForm
         if parsed_uri.scheme == b"http" and self.http_proxy_endpoint:
             pool_key = ("http-proxy", self.http_proxy_endpoint)
             endpoint = self.http_proxy_endpoint

--- a/synapse/logging/context.py
+++ b/synapse/logging/context.py
@@ -20,22 +20,20 @@
     import resource
     RUSAGE_THREAD = 1
     resource.getrusage(RUSAGE_THREAD)
     is_thread_resource_usage_supported = True
     def get_thread_resource_usage() -> "Optional[resource._RUsage]":
         return resource.getrusage(RUSAGE_THREAD)
 except Exception:
     is_thread_resource_usage_supported = False
     def get_thread_resource_usage() -> "Optional[resource._RUsage]":
         return None
-def logcontext_error(msg: str):
-    logger.warning(msg)
 get_thread_id = threading.get_ident
 class ContextResourceUsage:
     """Object for tracking the resources used by a log context
     Attributes:
         ru_utime (float): user CPU time (in seconds)
         ru_stime (float): system CPU time (in seconds)
         db_txn_count (int): number of database transactions done
         db_sched_duration_sec (float): amount of time spent waiting for a
             database connection
         db_txn_duration_sec (float): amount of time spent doing database
@@ -135,22 +133,23 @@
     def start(self, rusage: "Optional[resource._RUsage]"):
         pass
     def stop(self, rusage: "Optional[resource._RUsage]"):
         pass
     def add_database_transaction(self, duration_sec):
         pass
     def add_database_scheduled(self, sched_sec):
         pass
     def record_event_fetch(self, event_count):
         pass
-    def __bool__(self):
+    def __nonzero__(self):
         return False
+    __bool__ = __nonzero__  # python3
 SENTINEL_CONTEXT = _Sentinel()
 class LoggingContext:
     """Additional context for log formatting. Contexts are scoped within a
     "with" block.
     If a parent is given when creating a new context, then:
         - logging fields are copied from the parent to the new context on entry
         - when the new context exits, the cpu usage stats are copied from the
           child to the parent
     Args:
         name (str): Name for the context for debugging.
@@ -218,38 +217,39 @@
             "synapse.logging.context.LoggingContext.set_current_context() is deprecated "
             "in favor of synapse.logging.context.set_current_context().",
             DeprecationWarning,
             stacklevel=2,
         )
         return set_current_context(context)
     def __enter__(self) -> "LoggingContext":
         """Enters this logging context into thread local storage"""
         old_context = set_current_context(self)
         if self.previous_context != old_context:
-            logcontext_error(
-                "Expected previous context %r, found %r"
-                % (self.previous_context, old_context,)
+            logger.warning(
+                "Expected previous context %r, found %r",
+                self.previous_context,
+                old_context,
             )
         return self
     def __exit__(self, type, value, traceback) -> None:
         """Restore the logging context in thread local storage to the state it
         was before this context was entered.
         Returns:
             None to avoid suppressing any exceptions that were thrown.
         """
         current = set_current_context(self.previous_context)
         if current is not self:
             if current is SENTINEL_CONTEXT:
-                logcontext_error("Expected logging context %s was lost" % (self,))
+                logger.warning("Expected logging context %s was lost", self)
             else:
-                logcontext_error(
-                    "Expected logging context %s but found %s" % (self, current)
+                logger.warning(
+                    "Expected logging context %s but found %s", self, current
                 )
         self.finished = True
     def copy_to(self, record) -> None:
         """Copy logging fields from this context to a log record or
         another LoggingContext
         """
         record.request = self.request
         record.scope = self.scope
     def copy_to_twisted_log_entry(self, record) -> None:
         """
@@ -260,47 +260,47 @@
     def start(self, rusage: "Optional[resource._RUsage]") -> None:
         """
         Record that this logcontext is currently running.
         This should not be called directly: use set_current_context
         Args:
             rusage: the resources used by the current thread, at the point of
                 switching to this logcontext. May be None if this platform doesn't
                 support getrusuage.
         """
         if get_thread_id() != self.main_thread:
-            logcontext_error("Started logcontext %s on different thread" % (self,))
+            logger.warning("Started logcontext %s on different thread", self)
             return
         if self.finished:
-            logcontext_error("Re-starting finished log context %s" % (self,))
+            logger.warning("Re-starting finished log context %s", self)
         if self.usage_start:
-            logcontext_error("Re-starting already-active log context %s" % (self,))
+            logger.warning("Re-starting already-active log context %s", self)
         else:
             self.usage_start = rusage
     def stop(self, rusage: "Optional[resource._RUsage]") -> None:
         """
         Record that this logcontext is no longer running.
         This should not be called directly: use set_current_context
         Args:
             rusage: the resources used by the current thread, at the point of
                 switching away from this logcontext. May be None if this platform
                 doesn't support getrusuage.
         """
         try:
             if get_thread_id() != self.main_thread:
-                logcontext_error("Stopped logcontext %s on different thread" % (self,))
+                logger.warning("Stopped logcontext %s on different thread", self)
                 return
             if not rusage:
                 return
             if not self.usage_start:
-                logcontext_error(
-                    "Called stop on logcontext %s without recording a start rusage"
-                    % (self,)
+                logger.warning(
+                    "Called stop on logcontext %s without recording a start rusage",
+                    self,
                 )
                 return
             utime_delta, stime_delta = self._get_cputime(rusage)
             self.add_cputime(utime_delta, stime_delta)
         finally:
             self.usage_start = None
     def get_resource_usage(self) -> ContextResourceUsage:
         """Get resources used by this logcontext so far.
         Returns:
             ContextResourceUsage: a *copy* of the object tracking resource
@@ -405,27 +405,28 @@
     def __init__(
         self, new_context: LoggingContextOrSentinel = SENTINEL_CONTEXT
     ) -> None:
         self._new_context = new_context
     def __enter__(self) -> None:
         self._old_context = set_current_context(self._new_context)
     def __exit__(self, type, value, traceback) -> None:
         context = set_current_context(self._old_context)
         if context != self._new_context:
             if not context:
-                logcontext_error(
-                    "Expected logging context %s was lost" % (self._new_context,)
+                logger.warning(
+                    "Expected logging context %s was lost", self._new_context
                 )
             else:
-                logcontext_error(
-                    "Expected logging context %s but found %s"
-                    % (self._new_context, context,)
+                logger.warning(
+                    "Expected logging context %s but found %s",
+                    self._new_context,
+                    context,
                 )
 _thread_local = threading.local()
 _thread_local.current_context = SENTINEL_CONTEXT
 def current_context() -> LoggingContextOrSentinel:
     """Get the current logging context from thread local storage"""
     return getattr(_thread_local, "current_context", SENTINEL_CONTEXT)
 def set_current_context(context: LoggingContextOrSentinel) -> LoggingContextOrSentinel:
     """Set the current logging context in thread local storage
     Args:
         context(LoggingContext): The context to activate.

--- a/synapse/logging/formatter.py
+++ b/synapse/logging/formatter.py
@@ -3,21 +3,21 @@
 from io import StringIO
 class LogFormatter(logging.Formatter):
     """Log formatter which gives more detail for exceptions
     This is the same as the standard log formatter, except that when logging
     exceptions [typically via log.foo("msg", exc_info=1)], it prints the
     sequence that led up to the point at which the exception was caught.
     (Normally only stack frames between the point the exception was raised and
     where it was caught are logged).
     """
     def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
+        super(LogFormatter, self).__init__(*args, **kwargs)
     def formatException(self, ei):
         sio = StringIO()
         (typ, val, tb) = ei
         if tb and hasattr(tb.tb_frame, "f_back"):
             sio.write("Capture point (most recent call last):\n")
             traceback.print_stack(tb.tb_frame.f_back, None, sio)
         traceback.print_exception(typ, val, tb, None, sio)
         s = sio.getvalue()
         sio.close()
         if s[-1:] == "\n":

--- a/synapse/logging/scopecontextmanager.py
+++ b/synapse/logging/scopecontextmanager.py
@@ -65,33 +65,33 @@
                 the opentracing span which this scope represents the local
                 lifetime for.
             logcontext (LogContext):
                 the logcontext to which this scope is attached.
             enter_logcontext (Boolean):
                 if True the logcontext will be entered and exited when the scope
                 is entered and exited respectively
             finish_on_close (Boolean):
                 if True finish the span when the scope is closed
         """
-        super().__init__(manager, span)
+        super(_LogContextScope, self).__init__(manager, span)
         self.logcontext = logcontext
         self._finish_on_close = finish_on_close
         self._enter_logcontext = enter_logcontext
     def __enter__(self):
         if self._enter_logcontext:
             self.logcontext.__enter__()
         return self
     def __exit__(self, type, value, traceback):
         if type == twisted.internet.defer._DefGen_Return:
-            super().__exit__(None, None, None)
+            super(_LogContextScope, self).__exit__(None, None, None)
         else:
-            super().__exit__(type, value, traceback)
+            super(_LogContextScope, self).__exit__(type, value, traceback)
         if self._enter_logcontext:
             self.logcontext.__exit__(type, value, traceback)
         else:  # the logcontext existed before the creation of the scope
             self.logcontext.scope = None
     def close(self):
         if self.manager.active is not self:
             logger.error("Tried to close a non-active scope!")
             return
         if self._finish_on_close:
             self.span.finish()

--- a/synapse/logging/utils.py
+++ b/synapse/logging/utils.py
@@ -1,25 +1,25 @@
 import logging
 from functools import wraps
 from inspect import getcallargs
 _TIME_FUNC_ID = 0
 def _log_debug_as_f(f, msg, msg_args):
     name = f.__module__
     logger = logging.getLogger(name)
     if logger.isEnabledFor(logging.DEBUG):
         lineno = f.__code__.co_firstlineno
         pathname = f.__code__.co_filename
-        record = logger.makeRecord(
+        record = logging.LogRecord(
             name=name,
             level=logging.DEBUG,
-            fn=pathname,
-            lno=lineno,
+            pathname=pathname,
+            lineno=lineno,
             msg=msg,
             args=msg_args,
             exc_info=None,
         )
         logger.handle(record)
 def log_function(f):
     """ Function decorator that logs every call to that function.
     """
     func_name = f.__name__
     @wraps(f)

--- a/synapse/metrics/__init__.py
+++ b/synapse/metrics/__init__.py
@@ -1,47 +1,46 @@
 import functools
 import gc
-import itertools
 import logging
 import os
 import platform
 import threading
 import time
 from typing import Callable, Dict, Iterable, Optional, Tuple, Union
 import attr
 from prometheus_client import Counter, Gauge, Histogram
 from prometheus_client.core import (
     REGISTRY,
     CounterMetricFamily,
-    GaugeHistogramMetricFamily,
     GaugeMetricFamily,
+    HistogramMetricFamily,
 )
 from twisted.internet import reactor
 import synapse
 from synapse.metrics._exposition import (
     MetricsResource,
     generate_latest,
     start_http_server,
 )
 from synapse.util.versionstring import get_version_string
 logger = logging.getLogger(__name__)
 METRICS_PREFIX = "/_synapse/metrics"
 running_on_pypy = platform.python_implementation() == "PyPy"
-all_gauges = {}  # type: Dict[str, Union[LaterGauge, InFlightGauge]]
+all_gauges = {}  # type: Dict[str, Union[LaterGauge, InFlightGauge, BucketCollector]]
 HAVE_PROC_SELF_STAT = os.path.exists("/proc/self/stat")
 class RegistryProxy:
     @staticmethod
     def collect():
         for metric in REGISTRY.collect():
             if not metric.name.startswith("__"):
                 yield metric
-@attr.s(slots=True, hash=True)
+@attr.s(hash=True)
 class LaterGauge:
     name = attr.ib(type=str)
     desc = attr.ib(type=str)
     labels = attr.ib(hash=False, type=Optional[Iterable[str]])
     caller = attr.ib(type=Callable[[], Union[Dict[Tuple[str, ...], float], float]])
     def collect(self):
         g = GaugeMetricFamily(self.name, self.desc, labels=self.labels)
         try:
             calls = self.caller()
         except Exception:
@@ -129,77 +128,62 @@
             )
             for key, metrics in metrics_by_key.items():
                 gauge.add_metric(key, getattr(metrics, name))
             yield gauge
     def _register_with_collector(self):
         if self.name in all_gauges.keys():
             logger.warning("%s already registered, reregistering" % (self.name,))
             REGISTRY.unregister(all_gauges.pop(self.name))
         REGISTRY.register(self)
         all_gauges[self.name] = self
-class GaugeBucketCollector:
-    """Like a Histogram, but the buckets are Gauges which are updated atomically.
-    The data is updated by calling `update_data` with an iterable of measurements.
-    We assume that the data is updated less frequently than it is reported to
-    Prometheus, and optimise for that case.
+@attr.s(hash=True)
+class BucketCollector:
     """
-    __slots__ = ("_name", "_documentation", "_bucket_bounds", "_metric")
-    def __init__(
-        self,
-        name: str,
-        documentation: str,
-        buckets: Iterable[float],
-        registry=REGISTRY,
-    ):
-        """
-        Args:
-            name: base name of metric to be exported to Prometheus. (a _bucket suffix
-               will be added.)
-            documentation: help text for the metric
-            buckets: The top bounds of the buckets to report
-            registry: metric registry to register with
-        """
-        self._name = name
-        self._documentation = documentation
-        self._bucket_bounds = [float(b) for b in buckets]
-        if self._bucket_bounds != sorted(self._bucket_bounds):
-            raise ValueError("Buckets not in sorted order")
-        if self._bucket_bounds[-1] != float("inf"):
-            self._bucket_bounds.append(float("inf"))
-        self._metric = self._values_to_metric([])
-        registry.register(self)
-    def collect(self):
-        yield self._metric
-    def update_data(self, values: Iterable[float]):
-        """Update the data to be reported by the metric
-        The existing data is cleared, and each measurement in the input is assigned
-        to the relevant bucket.
-        """
-        self._metric = self._values_to_metric(values)
-    def _values_to_metric(self, values: Iterable[float]) -> GaugeHistogramMetricFamily:
-        total = 0.0
-        bucket_values = [0 for _ in self._bucket_bounds]
-        for v in values:
-            for i, bound in enumerate(self._bucket_bounds):
-                if v <= bound:
-                    bucket_values[i] += 1
-                    break
-            total += v
-        accumulated_values = itertools.accumulate(bucket_values)
-        return GaugeHistogramMetricFamily(
-            self._name,
-            self._documentation,
-            buckets=list(
-                zip((str(b) for b in self._bucket_bounds), accumulated_values)
-            ),
-            gsum_value=total,
-        )
+    Like a Histogram, but allows buckets to be point-in-time instead of
+    incrementally added to.
+    Args:
+        name (str): Base name of metric to be exported to Prometheus.
+        data_collector (callable -> dict): A synchronous callable that
+            returns a dict mapping bucket to number of items in the
+            bucket. If these buckets are not the same as the buckets
+            given to this class, they will be remapped into them.
+        buckets (list[float]): List of floats/ints of the buckets to
+            give to Prometheus. +Inf is ignored, if given.
+    """
+    name = attr.ib()
+    data_collector = attr.ib()
+    buckets = attr.ib()
+    def collect(self):
+        data = self.data_collector()
+        buckets = {}  # type: Dict[float, int]
+        res = []
+        for x in data.keys():
+            for i, bound in enumerate(self.buckets):
+                if x <= bound:
+                    buckets[bound] = buckets.get(bound, 0) + data[x]
+        for i in self.buckets:
+            res.append([str(i), buckets.get(i, 0)])
+        res.append(["+Inf", sum(data.values())])
+        metric = HistogramMetricFamily(
+            self.name, "", buckets=res, sum_value=sum(x * y for x, y in data.items())
+        )
+        yield metric
+    def __attrs_post_init__(self):
+        self.buckets = [float(x) for x in self.buckets if x != "+Inf"]
+        if self.buckets != sorted(self.buckets):
+            raise ValueError("Buckets not sorted")
+        self.buckets = tuple(self.buckets)
+        if self.name in all_gauges.keys():
+            logger.warning("%s already registered, reregistering" % (self.name,))
+            REGISTRY.unregister(all_gauges.pop(self.name))
+        REGISTRY.register(self)
+        all_gauges[self.name] = self
 class CPUMetrics:
     def __init__(self):
         ticks_per_sec = 100
         try:
             ticks_per_sec = os.sysconf("SC_CLK_TCK")
         except (ValueError, TypeError, AttributeError):
             pass
         self.ticks_per_sec = ticks_per_sec
     def collect(self):
         if not HAVE_PROC_SELF_STAT:

--- a/synapse/metrics/_exposition.py
+++ b/synapse/metrics/_exposition.py
@@ -1,25 +1,31 @@
 """
 This code is based off `prometheus_client/exposition.py` from version 0.7.1.
 Due to the renaming of metrics in prometheus_client 0.4.0, this customised
 vendoring of the code will emit both the old versions that Synapse dashboards
 expect, and the newer "best practice" version of the up-to-date official client.
 """
 import math
 import threading
+from collections import namedtuple
 from http.server import BaseHTTPRequestHandler, HTTPServer
 from socketserver import ThreadingMixIn
-from typing import Dict, List
 from urllib.parse import parse_qs, urlparse
 from prometheus_client import REGISTRY
 from twisted.web.resource import Resource
 from synapse.util import caches
+try:
+    from prometheus_client.samples import Sample
+except ImportError:
+    Sample = namedtuple(  # type: ignore[no-redef] # noqa
+        "Sample", ["name", "labels", "value", "timestamp", "exemplar"]
+    )
 CONTENT_TYPE_LATEST = str("text/plain; version=0.0.4; charset=utf-8")
 INF = float("inf")
 MINUS_INF = float("-inf")
 def floatToGoString(d):
     d = float(d)
     if d == INF:
         return "+Inf"
     elif d == MINUS_INF:
         return "-Inf"
     elif math.isnan(d):
@@ -45,20 +51,28 @@
             )
         )
     else:
         labelstr = ""
     timestamp = ""
     if line.timestamp is not None:
         timestamp = " {0:d}".format(int(float(line.timestamp) * 1000))
     return "{0}{1} {2}{3}\n".format(
         name, labelstr, floatToGoString(line.value), timestamp
     )
+def nameify_sample(sample):
+    """
+    If we get a prometheus_client<0.4.0 sample as a tuple, transform it into a
+    namedtuple which has the names we expect.
+    """
+    if not isinstance(sample, Sample):
+        sample = Sample(*sample, None, None)
+    return sample
 def generate_latest(registry, emit_help=False):
     for collector in caches.collectors_by_name.values():
         collector.collect()
     output = []
     for metric in registry.collect():
         if not metric.samples:
             continue
         mname = metric.name
         mnewname = metric.name
         mtype = metric.type
@@ -74,62 +88,51 @@
         elif mtype == "unknown":
             mtype = "untyped"
         if emit_help:
             output.append(
                 "# HELP {0} {1}\n".format(
                     mname,
                     metric.documentation.replace("\\", r"\\").replace("\n", r"\n"),
                 )
             )
         output.append("# TYPE {0} {1}\n".format(mname, mtype))
-        om_samples = {}  # type: Dict[str, List[str]]
-        for s in metric.samples:
+        for sample in map(nameify_sample, metric.samples):
             for suffix in ["_created", "_gsum", "_gcount"]:
-                if s.name == metric.name + suffix:
-                    om_samples.setdefault(suffix, []).append(sample_line(s, s.name))
+                if sample.name.endswith(suffix):
                     break
             else:
-                newname = s.name.replace(mnewname, mname)
+                newname = sample.name.replace(mnewname, mname)
                 if ":" in newname and newname.endswith("_total"):
                     newname = newname[: -len("_total")]
-                output.append(sample_line(s, newname))
-        for suffix, lines in sorted(om_samples.items()):
-            if emit_help:
-                output.append(
-                    "# HELP {0}{1} {2}\n".format(
-                        metric.name,
-                        suffix,
-                        metric.documentation.replace("\\", r"\\").replace("\n", r"\n"),
-                    )
-                )
-            output.append("# TYPE {0}{1} gauge\n".format(metric.name, suffix))
-            output.extend(lines)
+                output.append(sample_line(sample, newname))
         if mtype == "counter":
             mnewname = mnewname.replace(":total", "")
         mnewname = mnewname.replace(":", "_")
         if mname == mnewname:
             continue
         if emit_help:
             output.append(
                 "# HELP {0} {1}\n".format(
                     mnewname,
                     metric.documentation.replace("\\", r"\\").replace("\n", r"\n"),
                 )
             )
         output.append("# TYPE {0} {1}\n".format(mnewname, mtype))
-        for s in metric.samples:
+        for sample in map(nameify_sample, metric.samples):
             for suffix in ["_created", "_gsum", "_gcount"]:
-                if s.name == metric.name + suffix:
+                if sample.name.endswith(suffix):
                     break
             else:
                 output.append(
-                    sample_line(s, s.name.replace(":total", "").replace(":", "_"))
+                    sample_line(
+                        sample, sample.name.replace(":total", "").replace(":", "_")
+                    )
                 )
     return "".join(output).encode("utf-8")
 class MetricsHandler(BaseHTTPRequestHandler):
     """HTTP handler that gives metrics from ``REGISTRY``."""
     registry = REGISTRY
     def do_GET(self):
         registry = self.registry
         params = parse_qs(urlparse(self.path).query)
         if "help" in params:
             emit_help = True

--- a/synapse/notifier.py
+++ b/synapse/notifier.py
@@ -17,27 +17,21 @@
 import synapse.server
 from synapse.api.constants import EventTypes, Membership
 from synapse.api.errors import AuthError
 from synapse.events import EventBase
 from synapse.handlers.presence import format_user_presence_state
 from synapse.logging.context import PreserveLoggingContext
 from synapse.logging.utils import log_function
 from synapse.metrics import LaterGauge
 from synapse.metrics.background_process_metrics import run_as_background_process
 from synapse.streams.config import PaginationConfig
-from synapse.types import (
-    Collection,
-    PersistedEventPosition,
-    RoomStreamToken,
-    StreamToken,
-    UserID,
-)
+from synapse.types import Collection, StreamToken, UserID
 from synapse.util.async_helpers import ObservableDeferred, timeout_deferred
 from synapse.util.metrics import Measure
 from synapse.visibility import filter_events_for_client
 logger = logging.getLogger(__name__)
 notified_events_counter = Counter("synapse_notifier_notified_events", "")
 users_woken_by_stream_counter = Counter(
     "synapse_notifier_users_woken_by_stream", "", ["stream"]
 )
 T = TypeVar("T")
 def count(func: Callable[[T], bool], it: Iterable[T]) -> int:
@@ -70,23 +64,21 @@
         current_token: StreamToken,
         time_now_ms: int,
     ):
         self.user_id = user_id
         self.rooms = set(rooms)
         self.current_token = current_token
         self.last_notified_token = current_token
         self.last_notified_ms = time_now_ms
         with PreserveLoggingContext():
             self.notify_deferred = ObservableDeferred(defer.Deferred())
-    def notify(
-        self, stream_key: str, stream_id: Union[int, RoomStreamToken], time_now_ms: int,
-    ):
+    def notify(self, stream_key: str, stream_id: int, time_now_ms: int):
         """Notify any listeners for this user of a new event from an
         event source.
         Args:
             stream_key: The stream the event came from.
             stream_id: The new id for the stream the event came from.
             time_now_ms: The current time in milliseconds.
         """
         self.current_token = self.current_token.copy_and_advance(stream_key, stream_id)
         self.last_notified_token = self.current_token
         self.last_notified_ms = time_now_ms
@@ -105,48 +97,48 @@
         notifier.user_to_user_stream.pop(self.user_id)
     def count_listeners(self) -> int:
         return len(self.notify_deferred.observers())
     def new_listener(self, token: StreamToken) -> _NotificationListener:
         """Returns a deferred that is resolved when there is a new token
         greater than the given token.
         Args:
             token: The token from which we are streaming from, i.e. we shouldn't
                 notify for things that happened before this.
         """
-        if self.last_notified_token != token:
+        if self.last_notified_token.is_after(token):
             return _NotificationListener(defer.succeed(self.current_token))
         else:
             return _NotificationListener(self.notify_deferred.observe())
 class EventStreamResult(namedtuple("EventStreamResult", ("events", "tokens"))):
-    def __bool__(self):
+    def __nonzero__(self):
         return bool(self.events)
+    __bool__ = __nonzero__  # python3
 class Notifier:
     """ This class is responsible for notifying any listeners when there are
     new events available for it.
     Primarily used from the /events stream.
     """
     UNUSED_STREAM_EXPIRY_MS = 10 * 60 * 1000
     def __init__(self, hs: "synapse.server.HomeServer"):
         self.user_to_user_stream = {}  # type: Dict[str, _NotifierUserStream]
         self.room_to_user_streams = {}  # type: Dict[str, Set[_NotifierUserStream]]
         self.hs = hs
         self.storage = hs.get_storage()
         self.event_sources = hs.get_event_sources()
         self.store = hs.get_datastore()
         self.pending_new_room_events = (
             []
-        )  # type: List[Tuple[PersistedEventPosition, EventBase, Collection[UserID]]]
+        )  # type: List[Tuple[int, EventBase, Collection[Union[str, UserID]]]]
         self.replication_callbacks = []  # type: List[Callable[[], None]]
         self.remote_server_up_callbacks = []  # type: List[Callable[[str], None]]
         self.clock = hs.get_clock()
         self.appservice_handler = hs.get_application_service_handler()
-        self._pusher_pool = hs.get_pusherpool()
         self.federation_sender = None
         if hs.should_send_federation():
             self.federation_sender = hs.get_federation_sender()
         self.state_handler = hs.get_state_handler()
         self.clock.looping_call(
             self.remove_expired_streams, self.UNUSED_STREAM_EXPIRY_MS
         )
         def count_listeners():
             all_user_streams = set()  # type: Set[_NotifierUserStream]
             for streams in list(self.room_to_user_streams.values()):
@@ -167,92 +159,79 @@
     def add_replication_callback(self, cb: Callable[[], None]):
         """Add a callback that will be called when some new data is available.
         Callback is not given any arguments. It should *not* return a Deferred - if
         it needs to do any asynchronous work, a background thread should be started and
         wrapped with run_as_background_process.
         """
         self.replication_callbacks.append(cb)
     def on_new_room_event(
         self,
         event: EventBase,
-        event_pos: PersistedEventPosition,
-        max_room_stream_token: RoomStreamToken,
-        extra_users: Collection[UserID] = [],
+        room_stream_id: int,
+        max_room_stream_id: int,
+        extra_users: Collection[Union[str, UserID]] = [],
     ):
         """ Used by handlers to inform the notifier something has happened
         in the room, room event wise.
         This triggers the notifier to wake up any listeners that are
         listening to the room, and any listeners for the users in the
         `extra_users` param.
         The events can be peristed out of order. The notifier will wait
         until all previous events have been persisted before notifying
         the client streams.
         """
-        self.pending_new_room_events.append((event_pos, event, extra_users))
-        self._notify_pending_new_room_events(max_room_stream_token)
+        self.pending_new_room_events.append((room_stream_id, event, extra_users))
+        self._notify_pending_new_room_events(max_room_stream_id)
         self.notify_replication()
-    def _notify_pending_new_room_events(self, max_room_stream_token: RoomStreamToken):
+    def _notify_pending_new_room_events(self, max_room_stream_id: int):
         """Notify for the room events that were queued waiting for a previous
         event to be persisted.
         Args:
-            max_room_stream_token: The highest stream_id below which all
+            max_room_stream_id: The highest stream_id below which all
                 events have been persisted.
         """
         pending = self.pending_new_room_events
         self.pending_new_room_events = []
-        users = set()  # type: Set[UserID]
-        rooms = set()  # type: Set[str]
-        for event_pos, event, extra_users in pending:
-            if event_pos.persisted_after(max_room_stream_token):
-                self.pending_new_room_events.append((event_pos, event, extra_users))
+        for room_stream_id, event, extra_users in pending:
+            if room_stream_id > max_room_stream_id:
+                self.pending_new_room_events.append(
+                    (room_stream_id, event, extra_users)
+                )
             else:
-                if (
-                    event.type == EventTypes.Member
-                    and event.membership == Membership.JOIN
-                ):
-                    self._user_joined_room(event.state_key, event.room_id)
-                users.update(extra_users)
-                rooms.add(event.room_id)
-        if users or rooms:
-            self.on_new_event(
-                "room_key", max_room_stream_token, users=users, rooms=rooms,
-            )
-            self._on_updated_room_token(max_room_stream_token)
-    def _on_updated_room_token(self, max_room_stream_token: RoomStreamToken):
-        """Poke services that might care that the room position has been
-        updated.
-        """
+                self._on_new_room_event(event, room_stream_id, extra_users)
+    def _on_new_room_event(
+        self,
+        event: EventBase,
+        room_stream_id: int,
+        extra_users: Collection[Union[str, UserID]] = [],
+    ):
+        """Notify any user streams that are interested in this room event"""
         run_as_background_process(
-            "_notify_app_services", self._notify_app_services, max_room_stream_token
-        )
-        run_as_background_process(
-            "_notify_pusher_pool", self._notify_pusher_pool, max_room_stream_token
+            "notify_app_services", self._notify_app_services, room_stream_id
         )
         if self.federation_sender:
-            self.federation_sender.notify_new_events(max_room_stream_token.stream)
-    async def _notify_app_services(self, max_room_stream_token: RoomStreamToken):
+            self.federation_sender.notify_new_events(room_stream_id)
+        if event.type == EventTypes.Member and event.membership == Membership.JOIN:
+            self._user_joined_room(event.state_key, event.room_id)
+        self.on_new_event(
+            "room_key", room_stream_id, users=extra_users, rooms=[event.room_id]
+        )
+    async def _notify_app_services(self, room_stream_id: int):
         try:
-            await self.appservice_handler.notify_interested_services(
-                max_room_stream_token.stream
-            )
+            await self.appservice_handler.notify_interested_services(room_stream_id)
         except Exception:
             logger.exception("Error notifying application services of event")
-    async def _notify_pusher_pool(self, max_room_stream_token: RoomStreamToken):
-        try:
-            await self._pusher_pool.on_new_notifications(max_room_stream_token.stream)
-        except Exception:
-            logger.exception("Error pusher pool of event")
     def on_new_event(
         self,
         stream_key: str,
-        new_token: Union[int, RoomStreamToken],
-        users: Collection[UserID] = [],
+        new_token: int,
+        users: Collection[Union[str, UserID]] = [],
         rooms: Collection[str] = [],
     ):
         """ Used to inform listeners that something has happened event wise.
         Will wake up all listeners for the given users and rooms.
         """
         with PreserveLoggingContext():
             with Measure(self.clock, "on_new_event"):
                 user_streams = set()
                 for user in users:
                     user_stream = self.user_to_user_stream.get(str(user))
@@ -333,31 +312,30 @@
         explicit_room_id: str = None,
     ) -> EventStreamResult:
         """ For the given user and rooms, return any new events for them. If
         there are no new events wait for up to `timeout` milliseconds for any
         new events to happen before returning.
         If explicit_room_id is not set, the user's joined rooms will be polled
         for events.
         If explicit_room_id is set, that room will be polled for events only if
         it is world readable or the user has joined the room.
         """
-        if pagination_config.from_token:
-            from_token = pagination_config.from_token
-        else:
+        from_token = pagination_config.from_token
+        if not from_token:
             from_token = self.event_sources.get_current_token()
         limit = pagination_config.limit
         room_ids, is_joined = await self._get_room_ids(user, explicit_room_id)
         is_peeking = not is_joined
         async def check_for_updates(
             before_token: StreamToken, after_token: StreamToken
         ) -> EventStreamResult:
-            if after_token == before_token:
+            if not after_token.is_after(before_token):
                 return EventStreamResult([], (from_token, from_token))
             events = []  # type: List[EventBase]
             end_token = from_token
             for name, source in self.event_sources.sources.items():
                 keyname = "%s_key" % name
                 before_id = getattr(before_token, keyname)
                 after_id = getattr(after_token, keyname)
                 if before_id == after_id:
                     continue
                 new_events, new_key = await source.get_new_events(

--- a/synapse/push/__init__.py
+++ b/synapse/push/__init__.py
@@ -1,3 +1,3 @@
 class PusherConfigException(Exception):
     def __init__(self, msg):
-        super().__init__(msg)
+        super(PusherConfigException, self).__init__(msg)

--- a/synapse/push/emailpusher.py
+++ b/synapse/push/emailpusher.py
@@ -38,21 +38,21 @@
         """
         if should_check_for_notifs and self.mailer is not None:
             self._start_processing()
     def on_stop(self):
         if self.timed_call:
             try:
                 self.timed_call.cancel()
             except (AlreadyCalled, AlreadyCancelled):
                 pass
             self.timed_call = None
-    def on_new_notifications(self, max_stream_ordering):
+    def on_new_notifications(self, min_stream_ordering, max_stream_ordering):
         if self.max_stream_ordering:
             self.max_stream_ordering = max(
                 max_stream_ordering, self.max_stream_ordering
             )
         else:
             self.max_stream_ordering = max_stream_ordering
         self._start_processing()
     def on_new_receipts(self, min_stream_id, max_stream_id):
         pass
     def on_timer(self):

--- a/synapse/push/httppusher.py
+++ b/synapse/push/httppusher.py
@@ -65,21 +65,21 @@
         del self.data_minus_url["url"]
     def on_started(self, should_check_for_notifs):
         """Called when this pusher has been started.
         Args:
             should_check_for_notifs (bool): Whether we should immediately
                 check for push to send. Set to False only if it's known there
                 is nothing to send
         """
         if should_check_for_notifs:
             self._start_processing()
-    def on_new_notifications(self, max_stream_ordering):
+    def on_new_notifications(self, min_stream_ordering, max_stream_ordering):
         self.max_stream_ordering = max(
             max_stream_ordering, self.max_stream_ordering or 0
         )
         self._start_processing()
     def on_new_receipts(self, min_stream_id, max_stream_id):
         run_as_background_process("http_pusher.on_new_receipts", self._update_badge)
     async def _update_badge(self):
         badge = await push_tools.get_badge_count(self.hs.get_datastore(), self.user_id)
         await self._send_badge(badge)
     def on_timer(self):

--- a/synapse/push/mailer.py
+++ b/synapse/push/mailer.py
@@ -81,21 +81,21 @@
                 reset to
             token (str): Unique token generated by the server to verify
                 the email was received
             client_secret (str): Unique token generated by the client to
                 group together multiple email sending attempts
             sid (str): The generated session ID
         """
         params = {"token": token, "client_secret": client_secret, "sid": sid}
         link = (
             self.hs.config.public_baseurl
-            + "_synapse/client/password_reset/email/submit_token?%s"
+            + "_matrix/client/unstable/password_reset/email/submit_token?%s"
             % urllib.parse.urlencode(params)
         )
         template_vars = {"link": link}
         await self.send_email(
             email_address,
             self.email_subjects.password_reset
             % {"server_name": self.hs.config.server_name},
             template_vars,
         )
     async def send_registration_mail(self, email_address, token, client_secret, sid):

--- a/synapse/push/pusherpool.py
+++ b/synapse/push/pusherpool.py
@@ -24,24 +24,22 @@
     will send out the notifications in the background, rather than blocking until the
     notifications are sent; accordingly Pusher.on_started, Pusher.on_new_notifications and
     Pusher.on_new_receipts are not expected to return awaitables.
     """
     def __init__(self, hs: "HomeServer"):
         self.hs = hs
         self.pusher_factory = PusherFactory(hs)
         self._should_start_pushers = hs.config.start_pushers
         self.store = self.hs.get_datastore()
         self.clock = self.hs.get_clock()
-        self._account_validity = hs.config.account_validity
         self._pusher_shard_config = hs.config.push.pusher_shard_config
         self._instance_name = hs.get_instance_name()
-        self._last_room_stream_id_seen = self.store.get_room_max_stream_ordering()
         self.pushers = {}  # type: Dict[str, Dict[str, Union[HttpPusher, EmailPusher]]]
     def start(self):
         """Starts the pushers off in a background process.
         """
         if not self._should_start_pushers:
             logger.info("Not starting pushers because they are disabled in the config")
             return
         run_as_background_process("start_pushers", self._start_pushers)
     async def add_pusher(
         self,
@@ -121,57 +119,41 @@
         tokens = set(access_tokens)
         for p in await self.store.get_pushers_by_user_id(user_id):
             if p["access_token"] in tokens:
                 logger.info(
                     "Removing pusher for app id %s, pushkey %s, user %s",
                     p["app_id"],
                     p["pushkey"],
                     p["user_name"],
                 )
                 await self.remove_pusher(p["app_id"], p["pushkey"], p["user_name"])
-    async def on_new_notifications(self, max_stream_id: int):
+    async def on_new_notifications(self, min_stream_id, max_stream_id):
         if not self.pushers:
             return
-        if max_stream_id < self._last_room_stream_id_seen:
-            return
-        prev_stream_id = self._last_room_stream_id_seen
-        self._last_room_stream_id_seen = max_stream_id
         try:
             users_affected = await self.store.get_push_action_users_in_range(
-                prev_stream_id, max_stream_id
+                min_stream_id, max_stream_id
             )
             for u in users_affected:
-                if self._account_validity.enabled:
-                    expired = await self.store.is_account_expired(
-                        u, self.clock.time_msec()
-                    )
-                    if expired:
-                        continue
                 if u in self.pushers:
                     for p in self.pushers[u].values():
-                        p.on_new_notifications(max_stream_id)
+                        p.on_new_notifications(min_stream_id, max_stream_id)
         except Exception:
             logger.exception("Exception in pusher on_new_notifications")
     async def on_new_receipts(self, min_stream_id, max_stream_id, affected_room_ids):
         if not self.pushers:
             return
         try:
             users_affected = await self.store.get_users_sent_receipts_between(
                 min_stream_id - 1, max_stream_id
             )
             for u in users_affected:
-                if self._account_validity.enabled:
-                    expired = await self.store.is_account_expired(
-                        u, self.clock.time_msec()
-                    )
-                    if expired:
-                        continue
                 if u in self.pushers:
                     for p in self.pushers[u].values():
                         p.on_new_receipts(min_stream_id, max_stream_id)
         except Exception:
             logger.exception("Exception in pusher on_new_receipts")
     async def start_pusher_by_id(self, app_id, pushkey, user_id):
         """Look up the details for the given pusher, and start it
         Returns:
             EmailPusher|HttpPusher|None: The pusher started, if any
         """

--- a/synapse/python_dependencies.py
+++ b/synapse/python_dependencies.py
@@ -4,55 +4,56 @@
     DistributionNotFound,
     Requirement,
     VersionConflict,
     get_provider,
 )
 logger = logging.getLogger(__name__)
 REQUIREMENTS = [
     "jsonschema>=2.5.1",
     "frozendict>=1",
     "unpaddedbase64>=1.1.0",
-    "canonicaljson>=1.4.0",
+    "canonicaljson>=1.3.0",
     "signedjson>=1.1.0",
     "pynacl>=1.2.1",
     "idna>=2.5",
     "service_identity>=18.1.0",
     "Twisted>=18.9.0",
     "treq>=15.1",
     "pyopenssl>=16.0.0",
     "pyyaml>=3.11",
     "pyasn1>=0.1.9",
     "pyasn1-modules>=0.0.7",
     "bcrypt>=3.1.0",
     "pillow>=4.3.0",
     "sortedcontainers>=1.4.4",
     "pymacaroons>=0.13.0",
     "msgpack>=0.5.2",
     "phonenumbers>=8.2.0",
-    "prometheus_client>=0.4.0,<0.9.0",
+    "prometheus_client>=0.0.18,<0.9.0",
     "attrs>=19.1.0",
     "netaddr>=0.7.18",
     "Jinja2>=2.9",
     "bleach>=1.4.3",
     "typing-extensions>=3.7.4",
 ]
 CONDITIONAL_REQUIREMENTS = {
     "matrix-synapse-ldap3": ["matrix-synapse-ldap3>=0.1"],
     "postgres": ["psycopg2>=2.7"],
     "acme": [
         "txacme>=0.9.2",
         'eliot<1.8.0;python_version<"3.5.3"',
     ],
     "saml2": ["pysaml2>=4.5.0"],
     "oidc": ["authlib>=0.14.0"],
     "systemd": ["systemd-python>=231"],
     "url_preview": ["lxml>=3.5.0"],
+    "test": ["mock>=2.0", "parameterized>=0.7.0"],
     "sentry": ["sentry-sdk>=0.7.2"],
     "opentracing": ["jaeger-client>=4.0.0", "opentracing>=2.2.0"],
     "jwt": ["pyjwt>=1.6.4"],
     "redis": ["txredisapi>=1.4.7", "hiredis"],
 }
 ALL_OPTIONAL_REQUIREMENTS = set()  # type: Set[str]
 for name, optional_deps in CONDITIONAL_REQUIREMENTS.items():
     if name not in ["systemd"]:
         ALL_OPTIONAL_REQUIREMENTS = set(optional_deps) | ALL_OPTIONAL_REQUIREMENTS
 def list_requirements():

--- a/synapse/replication/http/_base.py
+++ b/synapse/replication/http/_base.py
@@ -1,34 +1,27 @@
 import abc
 import logging
 import re
 import urllib
 from inspect import signature
 from typing import Dict, List, Tuple
-from prometheus_client import Counter, Gauge
-from synapse.api.errors import HttpResponseException, SynapseError
-from synapse.http import RequestTimedOutError
+from synapse.api.errors import (
+    CodeMessageException,
+    HttpResponseException,
+    RequestSendFailed,
+    SynapseError,
+)
 from synapse.logging.opentracing import inject_active_span_byte_dict, trace
 from synapse.util.caches.response_cache import ResponseCache
 from synapse.util.stringutils import random_string
 logger = logging.getLogger(__name__)
-_pending_outgoing_requests = Gauge(
-    "synapse_pending_outgoing_replication_requests",
-    "Number of active outgoing replication requests, by replication method name",
-    ["name"],
-)
-_outgoing_request_counter = Counter(
-    "synapse_outgoing_replication_requests",
-    "Number of outgoing replication requests, by replication method name and result",
-    ["name", "code"],
-)
-class ReplicationEndpoint(metaclass=abc.ABCMeta):
+class ReplicationEndpoint:
     """Helper base class for defining new replication HTTP endpoints.
     This creates an endpoint under `/_synapse/replication/:NAME/:PATH_ARGS..`
     (with a `/:txn_id` suffix for cached requests), where NAME is a name,
     PATH_ARGS are a tuple of parameters to be encoded in the URL.
     For example, if `NAME` is "send_event" and `PATH_ARGS` is `("event_id",)`,
     with `CACHE` set to true then this generates an endpoint:
         /_synapse/replication/send_event/:event_id/:txn_id
     For POST/PUT requests the payload is serialized to json and sent as the
     body, while for GET requests the payload is added as query parameters. See
     `_serialize_payload` for details.
@@ -45,20 +38,21 @@
             easier to follow along in the log files.
         METHOD (str): The method of the HTTP request, defaults to POST. Can be
             one of POST, PUT or GET. If GET then the payload is sent as query
             parameters rather than a JSON body.
         CACHE (bool): Whether server should cache the result of the request/
             If true then transparently adds a txn_id to all requests, and
             `_handle_request` must return a Deferred.
         RETRY_ON_TIMEOUT(bool): Whether or not to retry the request when a 504
             is received.
     """
+    __metaclass__ = abc.ABCMeta
     NAME = abc.abstractproperty()  # type: str  # type: ignore
     PATH_ARGS = abc.abstractproperty()  # type: Tuple[str, ...]  # type: ignore
     METHOD = "POST"
     CACHE = True
     RETRY_ON_TIMEOUT = True
     def __init__(self, hs):
         if self.CACHE:
             self.response_cache = ResponseCache(
                 hs, "repl." + self.NAME, timeout_ms=30 * 60 * 1000
             )
@@ -95,23 +89,21 @@
     def make_client(cls, hs):
         """Create a client that makes requests.
         Returns a callable that accepts the same parameters as `_serialize_payload`.
         """
         clock = hs.get_clock()
         client = hs.get_simple_http_client()
         local_instance_name = hs.get_instance_name()
         master_host = hs.config.worker_replication_host
         master_port = hs.config.worker_replication_http_port
         instance_map = hs.config.worker.instance_map
-        outgoing_gauge = _pending_outgoing_requests.labels(cls.NAME)
         @trace(opname="outgoing_replication_request")
-        @outgoing_gauge.track_inprogress()
         async def send_request(instance_name="master", **kwargs):
             if instance_name == local_instance_name:
                 raise Exception("Trying to send HTTP request to self")
             if instance_name == "master":
                 host = master_host
                 port = master_port
             elif instance_name in instance_map:
                 host = instance_map[instance_name].host
                 port = instance_map[instance_name].port
             else:
@@ -141,32 +133,29 @@
                 cls.NAME,
                 "/".join(url_args),
             )
             try:
                 while True:
                     headers = {}  # type: Dict[bytes, List[bytes]]
                     inject_active_span_byte_dict(headers, None, check_destination=False)
                     try:
                         result = await request_func(uri, data, headers=headers)
                         break
-                    except RequestTimedOutError:
-                        if not cls.RETRY_ON_TIMEOUT:
+                    except CodeMessageException as e:
+                        if e.code != 504 or not cls.RETRY_ON_TIMEOUT:
                             raise
-                    logger.warning("%s request timed out; retrying", cls.NAME)
+                    logger.warning("%s request timed out", cls.NAME)
                     await clock.sleep(1)
             except HttpResponseException as e:
-                _outgoing_request_counter.labels(cls.NAME, e.code).inc()
                 raise e.to_synapse_error()
-            except Exception as e:
-                _outgoing_request_counter.labels(cls.NAME, "ERR").inc()
-                raise SynapseError(502, "Failed to talk to main process") from e
-            _outgoing_request_counter.labels(cls.NAME, 200).inc()
+            except RequestSendFailed as e:
+                raise SynapseError(502, "Failed to talk to master") from e
             return result
         return send_request
     def register(self, http_server):
         """Called by the server to register this as a handler to the
         appropriate path.
         """
         url_args = list(self.PATH_ARGS)
         handler = self._handle_request
         method = self.METHOD
         if self.CACHE:

--- a/synapse/replication/http/devices.py
+++ b/synapse/replication/http/devices.py
@@ -19,21 +19,21 @@
                     "keys": { ... },
                     "device_display_name": "Alice's Mobile Phone"
                 }
             ]
         }
     """
     NAME = "user_device_resync"
     PATH_ARGS = ("user_id",)
     CACHE = False
     def __init__(self, hs):
-        super().__init__(hs)
+        super(ReplicationUserDevicesResyncRestServlet, self).__init__(hs)
         self.device_list_updater = hs.get_device_handler().device_list_updater
         self.store = hs.get_datastore()
         self.clock = hs.get_clock()
     @staticmethod
     async def _serialize_payload(user_id):
         return {}
     async def _handle_request(self, request, user_id):
         user_devices = await self.device_list_updater.user_device_resync(user_id)
         return 200, user_devices
 def register_servlets(hs, http_server):

--- a/synapse/replication/http/federation.py
+++ b/synapse/replication/http/federation.py
@@ -24,92 +24,86 @@
             "backfilled": false
         }
         200 OK
         {
             "max_stream_id": 32443,
         }
     """
     NAME = "fed_send_events"
     PATH_ARGS = ()
     def __init__(self, hs):
-        super().__init__(hs)
+        super(ReplicationFederationSendEventsRestServlet, self).__init__(hs)
         self.store = hs.get_datastore()
         self.storage = hs.get_storage()
         self.clock = hs.get_clock()
         self.federation_handler = hs.get_handlers().federation_handler
     @staticmethod
-    async def _serialize_payload(store, room_id, event_and_contexts, backfilled):
+    async def _serialize_payload(store, event_and_contexts, backfilled):
         """
         Args:
             store
-            room_id (str)
             event_and_contexts (list[tuple[FrozenEvent, EventContext]])
             backfilled (bool): Whether or not the events are the result of
                 backfilling
         """
         event_payloads = []
         for event, context in event_and_contexts:
             serialized_context = await context.serialize(event, store)
             event_payloads.append(
                 {
                     "event": event.get_pdu_json(),
                     "room_version": event.room_version.identifier,
                     "event_format_version": event.format_version,
                     "internal_metadata": event.internal_metadata.get_dict(),
                     "rejected_reason": event.rejected_reason,
                     "context": serialized_context,
                 }
             )
-        payload = {
-            "events": event_payloads,
-            "backfilled": backfilled,
-            "room_id": room_id,
-        }
+        payload = {"events": event_payloads, "backfilled": backfilled}
         return payload
     async def _handle_request(self, request):
         with Measure(self.clock, "repl_fed_send_events_parse"):
             content = parse_json_object_from_request(request)
-            room_id = content["room_id"]
             backfilled = content["backfilled"]
             event_payloads = content["events"]
             event_and_contexts = []
             for event_payload in event_payloads:
                 event_dict = event_payload["event"]
                 room_ver = KNOWN_ROOM_VERSIONS[event_payload["room_version"]]
                 internal_metadata = event_payload["internal_metadata"]
                 rejected_reason = event_payload["rejected_reason"]
                 event = make_event_from_dict(
                     event_dict, room_ver, internal_metadata, rejected_reason
                 )
                 context = EventContext.deserialize(
                     self.storage, event_payload["context"]
                 )
                 event_and_contexts.append((event, context))
         logger.info("Got %d events from federation", len(event_and_contexts))
         max_stream_id = await self.federation_handler.persist_events_and_notify(
-            room_id, event_and_contexts, backfilled
+            event_and_contexts, backfilled
         )
         return 200, {"max_stream_id": max_stream_id}
 class ReplicationFederationSendEduRestServlet(ReplicationEndpoint):
     """Handles EDUs newly received from federation, including persisting and
     notifying.
     Request format:
         POST /_synapse/replication/fed_send_edu/:edu_type/:txn_id
         {
             "origin": ...,
             "content: { ... }
         }
     """
     NAME = "fed_send_edu"
     PATH_ARGS = ("edu_type",)
     def __init__(self, hs):
-        super().__init__(hs)
+        super(ReplicationFederationSendEduRestServlet, self).__init__(hs)
         self.store = hs.get_datastore()
         self.clock = hs.get_clock()
         self.registry = hs.get_federation_registry()
     @staticmethod
     async def _serialize_payload(edu_type, origin, content):
         return {"origin": origin, "content": content}
     async def _handle_request(self, request, edu_type):
         with Measure(self.clock, "repl_fed_send_edu_parse"):
             content = parse_json_object_from_request(request)
             origin = content["origin"]
@@ -122,21 +116,21 @@
     Request format:
         POST /_synapse/replication/fed_query/:query_type
         {
             "args": { ... }
         }
     """
     NAME = "fed_query"
     PATH_ARGS = ("query_type",)
     CACHE = False
     def __init__(self, hs):
-        super().__init__(hs)
+        super(ReplicationGetQueryRestServlet, self).__init__(hs)
         self.store = hs.get_datastore()
         self.clock = hs.get_clock()
         self.registry = hs.get_federation_registry()
     @staticmethod
     async def _serialize_payload(query_type, args):
         """
         Args:
             query_type (str)
             args (dict): The arguments received for the given query type
         """
@@ -151,21 +145,21 @@
 class ReplicationCleanRoomRestServlet(ReplicationEndpoint):
     """Called to clean up any data in DB for a given room, ready for the
     server to join the room.
     Request format:
         POST /_synapse/replication/fed_cleanup_room/:room_id/:txn_id
         {}
     """
     NAME = "fed_cleanup_room"
     PATH_ARGS = ("room_id",)
     def __init__(self, hs):
-        super().__init__(hs)
+        super(ReplicationCleanRoomRestServlet, self).__init__(hs)
         self.store = hs.get_datastore()
     @staticmethod
     async def _serialize_payload(room_id, args):
         """
         Args:
             room_id (str)
         """
         return {}
     async def _handle_request(self, request, room_id):
         await self.store.clean_room_for_join(room_id)

--- a/synapse/replication/http/login.py
+++ b/synapse/replication/http/login.py
@@ -3,21 +3,21 @@
 from synapse.replication.http._base import ReplicationEndpoint
 logger = logging.getLogger(__name__)
 class RegisterDeviceReplicationServlet(ReplicationEndpoint):
     """Ensure a device is registered, generating a new access token for the
     device.
     Used during registration and login.
     """
     NAME = "device_check_registered"
     PATH_ARGS = ("user_id",)
     def __init__(self, hs):
-        super().__init__(hs)
+        super(RegisterDeviceReplicationServlet, self).__init__(hs)
         self.registration_handler = hs.get_registration_handler()
     @staticmethod
     async def _serialize_payload(user_id, device_id, initial_display_name, is_guest):
         """
         Args:
             device_id (str|None): Device ID to use, if None a new one is
                 generated.
             initial_display_name (str|None)
             is_guest (bool)
         """

--- a/synapse/replication/http/membership.py
+++ b/synapse/replication/http/membership.py
@@ -1,33 +1,33 @@
 import logging
 from typing import TYPE_CHECKING, Optional
 from synapse.http.servlet import parse_json_object_from_request
 from synapse.replication.http._base import ReplicationEndpoint
 from synapse.types import JsonDict, Requester, UserID
-from synapse.util.distributor import user_left_room
+from synapse.util.distributor import user_joined_room, user_left_room
 if TYPE_CHECKING:
     from synapse.server import HomeServer
 logger = logging.getLogger(__name__)
 class ReplicationRemoteJoinRestServlet(ReplicationEndpoint):
     """Does a remote join for the given user to the given room
     Request format:
         POST /_synapse/replication/remote_join/:room_id/:user_id
         {
             "requester": ...,
             "remote_room_hosts": [...],
             "content": { ... }
         }
     """
     NAME = "remote_join"
     PATH_ARGS = ("room_id", "user_id")
     def __init__(self, hs):
-        super().__init__(hs)
+        super(ReplicationRemoteJoinRestServlet, self).__init__(hs)
         self.federation_handler = hs.get_handlers().federation_handler
         self.store = hs.get_datastore()
         self.clock = hs.get_clock()
     @staticmethod
     async def _serialize_payload(
         requester, room_id, user_id, remote_room_hosts, content
     ):
         """
         Args:
             requester(Requester)
@@ -59,21 +59,21 @@
         POST /_synapse/replication/remote_reject_invite/:event_id
         {
             "txn_id": ...,
             "requester": ...,
             "content": { ... }
         }
     """
     NAME = "remote_reject_invite"
     PATH_ARGS = ("invite_event_id",)
     def __init__(self, hs: "HomeServer"):
-        super().__init__(hs)
+        super(ReplicationRemoteRejectInviteRestServlet, self).__init__(hs)
         self.store = hs.get_datastore()
         self.clock = hs.get_clock()
         self.member_handler = hs.get_room_member_handler()
     @staticmethod
     async def _serialize_payload(  # type: ignore
         invite_event_id: str,
         txn_id: Optional[str],
         requester: Requester,
         content: JsonDict,
     ):
@@ -104,37 +104,39 @@
 class ReplicationUserJoinedLeftRoomRestServlet(ReplicationEndpoint):
     """Notifies that a user has joined or left the room
     Request format:
         POST /_synapse/replication/membership_change/:room_id/:user_id/:change
         {}
     """
     NAME = "membership_change"
     PATH_ARGS = ("room_id", "user_id", "change")
     CACHE = False  # No point caching as should return instantly.
     def __init__(self, hs):
-        super().__init__(hs)
+        super(ReplicationUserJoinedLeftRoomRestServlet, self).__init__(hs)
         self.registeration_handler = hs.get_registration_handler()
         self.store = hs.get_datastore()
         self.clock = hs.get_clock()
         self.distributor = hs.get_distributor()
     @staticmethod
     async def _serialize_payload(room_id, user_id, change):
         """
         Args:
             room_id (str)
             user_id (str)
-            change (str): "left"
+            change (str): Either "joined" or "left"
         """
-        assert change == "left"
+        assert change in ("joined", "left")
         return {}
     def _handle_request(self, request, room_id, user_id, change):
         logger.info("user membership change: %s in %s", user_id, room_id)
         user = UserID.from_string(user_id)
-        if change == "left":
+        if change == "joined":
+            user_joined_room(self.distributor, user, room_id)
+        elif change == "left":
             user_left_room(self.distributor, user, room_id)
         else:
             raise Exception("Unrecognized change: %r", change)
         return 200, {}
 def register_servlets(hs, http_server):
     ReplicationRemoteJoinRestServlet(hs).register(http_server)
     ReplicationRemoteRejectInviteRestServlet(hs).register(http_server)
     ReplicationUserJoinedLeftRoomRestServlet(hs).register(http_server)

--- a/synapse/replication/http/register.py
+++ b/synapse/replication/http/register.py
@@ -1,21 +1,21 @@
 import logging
 from synapse.http.servlet import parse_json_object_from_request
 from synapse.replication.http._base import ReplicationEndpoint
 logger = logging.getLogger(__name__)
 class ReplicationRegisterServlet(ReplicationEndpoint):
     """Register a new user
     """
     NAME = "register_user"
     PATH_ARGS = ("user_id",)
     def __init__(self, hs):
-        super().__init__(hs)
+        super(ReplicationRegisterServlet, self).__init__(hs)
         self.store = hs.get_datastore()
         self.registration_handler = hs.get_registration_handler()
     @staticmethod
     async def _serialize_payload(
         user_id,
         password_hash,
         was_guest,
         make_guest,
         appservice_id,
         create_profile_with_displayname,
@@ -67,21 +67,21 @@
             address=content["address"],
             shadow_banned=content["shadow_banned"],
         )
         return 200, {}
 class ReplicationPostRegisterActionsServlet(ReplicationEndpoint):
     """Run any post registration actions
     """
     NAME = "post_register"
     PATH_ARGS = ("user_id",)
     def __init__(self, hs):
-        super().__init__(hs)
+        super(ReplicationPostRegisterActionsServlet, self).__init__(hs)
         self.store = hs.get_datastore()
         self.registration_handler = hs.get_registration_handler()
     @staticmethod
     async def _serialize_payload(user_id, auth_result, access_token):
         """
         Args:
             user_id (str): The user ID that consented
             auth_result (dict): The authenticated credentials of the newly
                 registered user.
             access_token (str|None): The access token of the newly logged in

--- a/synapse/replication/http/send_event.py
+++ b/synapse/replication/http/send_event.py
@@ -21,21 +21,21 @@
             "rejected_reason": ..,   // The event.rejected_reason field
             "context": { .. serialized event context .. },
             "requester": { .. serialized requester .. },
             "ratelimit": true,
             "extra_users": [],
         }
     """
     NAME = "send_event"
     PATH_ARGS = ("event_id",)
     def __init__(self, hs):
-        super().__init__(hs)
+        super(ReplicationSendEventRestServlet, self).__init__(hs)
         self.event_creation_handler = hs.get_event_creation_handler()
         self.store = hs.get_datastore()
         self.storage = hs.get_storage()
         self.clock = hs.get_clock()
     @staticmethod
     async def _serialize_payload(
         event_id, store, event, context, requester, ratelimit, extra_users
     ):
         """
         Args:

--- a/synapse/replication/slave/storage/_base.py
+++ b/synapse/replication/slave/storage/_base.py
@@ -1,25 +1,23 @@
 import logging
 from typing import Optional
 from synapse.storage.database import DatabasePool
 from synapse.storage.databases.main.cache import CacheInvalidationWorkerStore
 from synapse.storage.engines import PostgresEngine
 from synapse.storage.util.id_generators import MultiWriterIdGenerator
 logger = logging.getLogger(__name__)
 class BaseSlavedStore(CacheInvalidationWorkerStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(BaseSlavedStore, self).__init__(database, db_conn, hs)
         if isinstance(self.database_engine, PostgresEngine):
             self._cache_id_gen = MultiWriterIdGenerator(
                 db_conn,
                 database,
-                stream_name="caches",
                 instance_name=hs.get_instance_name(),
                 table="cache_invalidation_stream_by_instance",
                 instance_column="instance_name",
                 id_column="stream_id",
                 sequence_name="cache_invalidation_stream_seq",
-                writers=[],
             )  # type: Optional[MultiWriterIdGenerator]
         else:
             self._cache_id_gen = None
         self.hs = hs

--- a/synapse/replication/slave/storage/account_data.py
+++ b/synapse/replication/slave/storage/account_data.py
@@ -8,21 +8,21 @@
     def __init__(self, database: DatabasePool, db_conn, hs):
         self._account_data_id_gen = SlavedIdTracker(
             db_conn,
             "account_data",
             "stream_id",
             extra_tables=[
                 ("room_account_data", "stream_id"),
                 ("room_tags_revisions", "stream_id"),
             ],
         )
-        super().__init__(database, db_conn, hs)
+        super(SlavedAccountDataStore, self).__init__(database, db_conn, hs)
     def get_max_account_data_stream_id(self):
         return self._account_data_id_gen.get_current_token()
     def process_replication_rows(self, stream_name, instance_name, token, rows):
         if stream_name == TagAccountDataStream.NAME:
             self._account_data_id_gen.advance(instance_name, token)
             for row in rows:
                 self.get_tags_for_user.invalidate((row.user_id,))
                 self._account_data_stream_cache.entity_has_changed(row.user_id, token)
         elif stream_name == AccountDataStream.NAME:
             self._account_data_id_gen.advance(instance_name, token)

--- a/synapse/replication/slave/storage/client_ips.py
+++ b/synapse/replication/slave/storage/client_ips.py
@@ -1,17 +1,17 @@
 from synapse.storage.database import DatabasePool
 from synapse.storage.databases.main.client_ips import LAST_SEEN_GRANULARITY
 from synapse.util.caches.descriptors import Cache
 from ._base import BaseSlavedStore
 class SlavedClientIpStore(BaseSlavedStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(SlavedClientIpStore, self).__init__(database, db_conn, hs)
         self.client_ip_last_seen = Cache(
             name="client_ip_last_seen", keylen=4, max_entries=50000
         )
     async def insert_client_ip(self, user_id, access_token, ip, user_agent, device_id):
         now = int(self._clock.time_msec())
         key = (user_id, access_token, ip)
         try:
             last_seen = self.client_ip_last_seen.get(key)
         except KeyError:
             last_seen = None

--- a/synapse/replication/slave/storage/deviceinbox.py
+++ b/synapse/replication/slave/storage/deviceinbox.py
@@ -1,20 +1,20 @@
 from synapse.replication.slave.storage._base import BaseSlavedStore
 from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
 from synapse.replication.tcp.streams import ToDeviceStream
 from synapse.storage.database import DatabasePool
 from synapse.storage.databases.main.deviceinbox import DeviceInboxWorkerStore
 from synapse.util.caches.expiringcache import ExpiringCache
 from synapse.util.caches.stream_change_cache import StreamChangeCache
 class SlavedDeviceInboxStore(DeviceInboxWorkerStore, BaseSlavedStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(SlavedDeviceInboxStore, self).__init__(database, db_conn, hs)
         self._device_inbox_id_gen = SlavedIdTracker(
             db_conn, "device_inbox", "stream_id"
         )
         self._device_inbox_stream_cache = StreamChangeCache(
             "DeviceInboxStreamChangeCache",
             self._device_inbox_id_gen.get_current_token(),
         )
         self._device_federation_outbox_stream_cache = StreamChangeCache(
             "DeviceFederationOutboxStreamChangeCache",
             self._device_inbox_id_gen.get_current_token(),

--- a/synapse/replication/slave/storage/devices.py
+++ b/synapse/replication/slave/storage/devices.py
@@ -1,20 +1,20 @@
 from synapse.replication.slave.storage._base import BaseSlavedStore
 from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
 from synapse.replication.tcp.streams._base import DeviceListsStream, UserSignatureStream
 from synapse.storage.database import DatabasePool
 from synapse.storage.databases.main.devices import DeviceWorkerStore
 from synapse.storage.databases.main.end_to_end_keys import EndToEndKeyWorkerStore
 from synapse.util.caches.stream_change_cache import StreamChangeCache
 class SlavedDeviceStore(EndToEndKeyWorkerStore, DeviceWorkerStore, BaseSlavedStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(SlavedDeviceStore, self).__init__(database, db_conn, hs)
         self.hs = hs
         self._device_list_id_gen = SlavedIdTracker(
             db_conn,
             "device_lists_stream",
             "stream_id",
             extra_tables=[
                 ("user_signature_stream", "stream_id"),
                 ("device_lists_outbound_pokes", "stream_id"),
             ],
         )

--- a/synapse/replication/slave/storage/events.py
+++ b/synapse/replication/slave/storage/events.py
@@ -20,21 +20,21 @@
     EventPushActionsWorkerStore,
     StreamWorkerStore,
     StateGroupWorkerStore,
     EventsWorkerStore,
     SignatureWorkerStore,
     UserErasureWorkerStore,
     RelationsWorkerStore,
     BaseSlavedStore,
 ):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(SlavedEventStore, self).__init__(database, db_conn, hs)
         events_max = self._stream_id_gen.get_current_token()
         curr_state_delta_prefill, min_curr_state_delta_id = self.db_pool.get_cache_dict(
             db_conn,
             "current_state_delta_stream",
             entity_column="room_id",
             stream_column="stream_id",
             max_value=events_max,  # As we share the stream id with events token
             limit=1000,
         )
         self._curr_state_delta_stream_cache = StreamChangeCache(

--- a/synapse/replication/slave/storage/filtering.py
+++ b/synapse/replication/slave/storage/filtering.py
@@ -1,7 +1,7 @@
 from synapse.storage.database import DatabasePool
 from synapse.storage.databases.main.filtering import FilteringStore
 from ._base import BaseSlavedStore
 class SlavedFilteringStore(BaseSlavedStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(SlavedFilteringStore, self).__init__(database, db_conn, hs)
     get_user_filter = FilteringStore.__dict__["get_user_filter"]

--- a/synapse/replication/slave/storage/groups.py
+++ b/synapse/replication/slave/storage/groups.py
@@ -1,19 +1,19 @@
 from synapse.replication.slave.storage._base import BaseSlavedStore
 from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
 from synapse.replication.tcp.streams import GroupServerStream
 from synapse.storage.database import DatabasePool
 from synapse.storage.databases.main.group_server import GroupServerWorkerStore
 from synapse.util.caches.stream_change_cache import StreamChangeCache
 class SlavedGroupServerStore(GroupServerWorkerStore, BaseSlavedStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(SlavedGroupServerStore, self).__init__(database, db_conn, hs)
         self.hs = hs
         self._group_updates_id_gen = SlavedIdTracker(
             db_conn, "local_group_updates", "stream_id"
         )
         self._group_updates_stream_cache = StreamChangeCache(
             "_group_updates_stream_cache",
             self._group_updates_id_gen.get_current_token(),
         )
     def get_group_stream_token(self):
         return self._group_updates_id_gen.get_current_token()

--- a/synapse/replication/slave/storage/presence.py
+++ b/synapse/replication/slave/storage/presence.py
@@ -1,20 +1,20 @@
 from synapse.replication.tcp.streams import PresenceStream
 from synapse.storage import DataStore
 from synapse.storage.database import DatabasePool
 from synapse.storage.databases.main.presence import PresenceStore
 from synapse.util.caches.stream_change_cache import StreamChangeCache
 from ._base import BaseSlavedStore
 from ._slaved_id_tracker import SlavedIdTracker
 class SlavedPresenceStore(BaseSlavedStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(SlavedPresenceStore, self).__init__(database, db_conn, hs)
         self._presence_id_gen = SlavedIdTracker(db_conn, "presence_stream", "stream_id")
         self._presence_on_startup = self._get_active_presence(db_conn)  # type: ignore
         self.presence_stream_cache = StreamChangeCache(
             "PresenceStreamChangeCache", self._presence_id_gen.get_current_token()
         )
     _get_active_presence = DataStore._get_active_presence
     take_presence_startup_info = DataStore.take_presence_startup_info
     _get_presence_for_user = PresenceStore.__dict__["_get_presence_for_user"]
     get_presence_for_users = PresenceStore.__dict__["get_presence_for_users"]
     def get_current_presence_token(self):

--- a/synapse/replication/slave/storage/pushers.py
+++ b/synapse/replication/slave/storage/pushers.py
@@ -1,17 +1,17 @@
 from synapse.replication.tcp.streams import PushersStream
 from synapse.storage.database import DatabasePool
 from synapse.storage.databases.main.pusher import PusherWorkerStore
 from ._base import BaseSlavedStore
 from ._slaved_id_tracker import SlavedIdTracker
 class SlavedPusherStore(PusherWorkerStore, BaseSlavedStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(SlavedPusherStore, self).__init__(database, db_conn, hs)
         self._pushers_id_gen = SlavedIdTracker(
             db_conn, "pushers", "id", extra_tables=[("deleted_pushers", "stream_id")]
         )
     def get_pushers_stream_token(self):
         return self._pushers_id_gen.get_current_token()
     def process_replication_rows(self, stream_name, instance_name, token, rows):
         if stream_name == PushersStream.NAME:
             self._pushers_id_gen.advance(instance_name, token)
         return super().process_replication_rows(stream_name, instance_name, token, rows)

--- a/synapse/replication/slave/storage/receipts.py
+++ b/synapse/replication/slave/storage/receipts.py
@@ -1,21 +1,21 @@
 from synapse.replication.tcp.streams import ReceiptsStream
 from synapse.storage.database import DatabasePool
 from synapse.storage.databases.main.receipts import ReceiptsWorkerStore
 from ._base import BaseSlavedStore
 from ._slaved_id_tracker import SlavedIdTracker
 class SlavedReceiptsStore(ReceiptsWorkerStore, BaseSlavedStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
         self._receipts_id_gen = SlavedIdTracker(
             db_conn, "receipts_linearized", "stream_id"
         )
-        super().__init__(database, db_conn, hs)
+        super(SlavedReceiptsStore, self).__init__(database, db_conn, hs)
     def get_max_receipt_stream_id(self):
         return self._receipts_id_gen.get_current_token()
     def invalidate_caches_for_receipt(self, room_id, receipt_type, user_id):
         self.get_receipts_for_user.invalidate((user_id, receipt_type))
         self._get_linearized_receipts_for_room.invalidate_many((room_id,))
         self.get_last_receipt_event_id_for_user.invalidate(
             (user_id, room_id, receipt_type)
         )
         self._invalidate_get_users_with_receipts_in_room(room_id, receipt_type, user_id)
         self.get_receipts_for_room.invalidate((room_id, receipt_type))

--- a/synapse/replication/slave/storage/room.py
+++ b/synapse/replication/slave/storage/room.py
@@ -1,17 +1,17 @@
 from synapse.replication.tcp.streams import PublicRoomsStream
 from synapse.storage.database import DatabasePool
 from synapse.storage.databases.main.room import RoomWorkerStore
 from ._base import BaseSlavedStore
 from ._slaved_id_tracker import SlavedIdTracker
 class RoomStore(RoomWorkerStore, BaseSlavedStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(RoomStore, self).__init__(database, db_conn, hs)
         self._public_room_id_gen = SlavedIdTracker(
             db_conn, "public_room_list_stream", "stream_id"
         )
     def get_current_public_room_stream_id(self):
         return self._public_room_id_gen.get_current_token()
     def process_replication_rows(self, stream_name, instance_name, token, rows):
         if stream_name == PublicRoomsStream.NAME:
             self._public_room_id_gen.advance(instance_name, token)
         return super().process_replication_rows(stream_name, instance_name, token, rows)

--- a/synapse/replication/tcp/client.py
+++ b/synapse/replication/tcp/client.py
@@ -6,21 +6,20 @@
 from twisted.internet.protocol import ReconnectingClientFactory
 from synapse.api.constants import EventTypes
 from synapse.logging.context import PreserveLoggingContext, make_deferred_yieldable
 from synapse.replication.tcp.protocol import ClientReplicationStreamProtocol
 from synapse.replication.tcp.streams import TypingStream
 from synapse.replication.tcp.streams.events import (
     EventsStream,
     EventsStreamEventRow,
     EventsStreamRow,
 )
-from synapse.types import PersistedEventPosition, UserID
 from synapse.util.async_helpers import timeout_deferred
 from synapse.util.metrics import Measure
 if TYPE_CHECKING:
     from synapse.replication.tcp.handler import ReplicationCommandHandler
     from synapse.server import HomeServer
 logger = logging.getLogger(__name__)
 _WAIT_FOR_REPLICATION_TIMEOUT_SECONDS = 30
 class DirectTcpReplicationClientFactory(ReconnectingClientFactory):
     """Factory for building connections to the master. Will reconnect if the
     connection is lost.
@@ -57,20 +56,21 @@
     def clientConnectionFailed(self, connector, reason):
         logger.error("Failed to connect to replication: %r", reason)
         ReconnectingClientFactory.clientConnectionFailed(self, connector, reason)
 class ReplicationDataHandler:
     """Handles incoming stream updates from replication.
     This instance notifies the slave data store about updates. Can be subclassed
     to handle updates in additional ways.
     """
     def __init__(self, hs: "HomeServer"):
         self.store = hs.get_datastore()
+        self.pusher_pool = hs.get_pusherpool()
         self.notifier = hs.get_notifier()
         self._reactor = hs.get_reactor()
         self._clock = hs.get_clock()
         self._streams = hs.get_replication_streams()
         self._instance_name = hs.get_instance_name()
         self._typing_handler = hs.get_typing_handler()
         self._streams_to_waiters = (
             {}
         )  # type: Dict[str, List[Tuple[int, Deferred[None]]]]
     async def on_rdata(
@@ -94,28 +94,26 @@
         if stream_name == EventsStream.NAME:
             for row in rows:
                 if row.type != EventsStreamEventRow.TypeId:
                     continue
                 assert isinstance(row, EventsStreamRow)
                 event = await self.store.get_event(
                     row.data.event_id, allow_rejected=True
                 )
                 if event.rejected_reason:
                     continue
-                extra_users = ()  # type: Tuple[UserID, ...]
+                extra_users = ()  # type: Tuple[str, ...]
                 if event.type == EventTypes.Member:
-                    extra_users = (UserID.from_string(event.state_key),)
-                max_token = self.store.get_room_max_token()
-                event_pos = PersistedEventPosition(instance_name, token)
-                self.notifier.on_new_room_event(
-                    event, event_pos, max_token, extra_users
-                )
+                    extra_users = (event.state_key,)
+                max_token = self.store.get_room_max_stream_ordering()
+                self.notifier.on_new_room_event(event, token, max_token, extra_users)
+            await self.pusher_pool.on_new_notifications(token, token)
         waiting_list = self._streams_to_waiters.get(stream_name, [])
         index_of_first_deferred_not_called = len(waiting_list)
         for idx, (position, deferred) in enumerate(waiting_list):
             if position <= token:
                 try:
                     with PreserveLoggingContext():
                         deferred.callback(None)
                 except Exception:
                     pass
             else:

--- a/synapse/replication/tcp/handler.py
+++ b/synapse/replication/tcp/handler.py
@@ -65,21 +65,21 @@
         self._instance_name = hs.get_instance_name()
         self._streams = {
             stream.NAME: stream(hs) for stream in STREAMS_MAP.values()
         }  # type: Dict[str, Stream]
         self._streams_to_replicate = []  # type: List[Stream]
         for stream in self._streams.values():
             if stream.NAME == CachesStream.NAME:
                 self._streams_to_replicate.append(stream)
                 continue
             if isinstance(stream, (EventsStream, BackfillStream)):
-                if hs.get_instance_name() in hs.config.worker.writers.events:
+                if hs.config.worker.writers.events == hs.get_instance_name():
                     self._streams_to_replicate.append(stream)
                 continue
             if isinstance(stream, TypingStream):
                 if hs.config.worker.writers.typing == hs.get_instance_name():
                     self._streams_to_replicate.append(stream)
                 continue
             if hs.config.worker_app is not None:
                 continue
             if stream.NAME == FederationStream.NAME and hs.config.send_federation:
                 continue

--- a/synapse/replication/tcp/streams/_base.py
+++ b/synapse/replication/tcp/streams/_base.py
@@ -219,21 +219,21 @@
             store.get_all_updated_receipts,
         )
 class PushRulesStream(Stream):
     """A user has changed their push rules
     """
     PushRulesStreamRow = namedtuple("PushRulesStreamRow", ("user_id",))  # str
     NAME = "push_rules"
     ROW_TYPE = PushRulesStreamRow
     def __init__(self, hs):
         self.store = hs.get_datastore()
-        super().__init__(
+        super(PushRulesStream, self).__init__(
             hs.get_instance_name(),
             self._current_token,
             self.store.get_all_push_rule_updates,
         )
     def _current_token(self, instance_name: str) -> int:
         push_rules_token = self.store.get_max_push_rules_stream_id()
         return push_rules_token
 class PushersStream(Stream):
     """A user has added/changed/removed a pusher
     """
@@ -247,21 +247,21 @@
         store = hs.get_datastore()
         super().__init__(
             hs.get_instance_name(),
             current_token_without_instance(store.get_pushers_stream_token),
             store.get_all_updated_pushers_rows,
         )
 class CachesStream(Stream):
     """A cache was invalidated on the master and no other stream would invalidate
     the cache on the workers
     """
-    @attr.s(slots=True)
+    @attr.s
     class CachesStreamRow:
         """Stream to inform workers they should invalidate their cache.
         Attributes:
             cache_func: Name of the cached function.
             keys: The entry in the cache to invalidate. If None then will
                 invalidate all.
             invalidation_ts: Timestamp of when the invalidation took place.
         """
         cache_func = attr.ib(type=str)
         keys = attr.ib(type=Optional[List[Any]])
@@ -293,21 +293,21 @@
         store = hs.get_datastore()
         super().__init__(
             hs.get_instance_name(),
             current_token_without_instance(store.get_current_public_room_stream_id),
             store.get_all_new_public_rooms,
         )
 class DeviceListsStream(Stream):
     """Either a user has updated their devices or a remote server needs to be
     told about a device update.
     """
-    @attr.s(slots=True)
+    @attr.s
     class DeviceListsStreamRow:
         entity = attr.ib(type=str)
     NAME = "device_lists"
     ROW_TYPE = DeviceListsStreamRow
     def __init__(self, hs):
         store = hs.get_datastore()
         super().__init__(
             hs.get_instance_name(),
             current_token_without_instance(store.get_device_stream_token),
             store.get_all_device_list_changes_for_remotes,

--- a/synapse/replication/tcp/streams/events.py
+++ b/synapse/replication/tcp/streams/events.py
@@ -1,15 +1,15 @@
 import heapq
 from collections.abc import Iterable
 from typing import List, Tuple, Type
 import attr
-from ._base import Stream, StreamUpdateResult, Token
+from ._base import Stream, StreamUpdateResult, Token, current_token_without_instance
 """Handling of the 'events' replication stream
 This stream contains rows of various types. Each row therefore contains a 'type'
 identifier before the real data. For example::
     RDATA events batch ["state", ["!room:id", "m.type", "", "$event:id"]]
     RDATA events 12345 ["ev", ["$event:id", "!room:id", "m.type", null, null]]
 An "ev" row is sent for each new event. The fields in the data part are:
  * The new event id
  * The room id for the event
  * The type of the new event
  * The state key of the event, for state events
@@ -61,21 +61,21 @@
 )  # type: Tuple[Type[BaseEventsStreamRow], ...]
 TypeToRow = {Row.TypeId: Row for Row in _EventRows}
 class EventsStream(Stream):
     """We received a new event, or an event went from being an outlier to not
     """
     NAME = "events"
     def __init__(self, hs):
         self._store = hs.get_datastore()
         super().__init__(
             hs.get_instance_name(),
-            self._store._stream_id_gen.get_current_token_for_writer,
+            current_token_without_instance(self._store.get_current_events_token),
             self._update_function,
         )
     async def _update_function(
         self,
         instance_name: str,
         from_token: Token,
         current_token: Token,
         target_row_count: int,
     ) -> StreamUpdateResult:
         target_row_count //= 2

--- a/synapse/rest/__init__.py
+++ b/synapse/rest/__init__.py
@@ -1,12 +1,12 @@
+import synapse.rest.admin
 from synapse.http.server import JsonResource
-from synapse.rest import admin
 from synapse.rest.client import versions
 from synapse.rest.client.v1 import (
     directory,
     events,
     initial_sync,
     login as v1_login,
     logout,
     presence,
     profile,
     push_rule,
@@ -86,12 +86,14 @@
         devices.register_servlets(hs, client_resource)
         thirdparty.register_servlets(hs, client_resource)
         sendtodevice.register_servlets(hs, client_resource)
         user_directory.register_servlets(hs, client_resource)
         groups.register_servlets(hs, client_resource)
         room_upgrade_rest_servlet.register_servlets(hs, client_resource)
         capabilities.register_servlets(hs, client_resource)
         account_validity.register_servlets(hs, client_resource)
         relations.register_servlets(hs, client_resource)
         password_policy.register_servlets(hs, client_resource)
-        admin.register_servlets_for_client_rest_resource(hs, client_resource)
+        synapse.rest.admin.register_servlets_for_client_rest_resource(
+            hs, client_resource
+        )
         shared_rooms.register_servlets(hs, client_resource)

--- a/synapse/rest/admin/__init__.py
+++ b/synapse/rest/admin/__init__.py
@@ -1,56 +1,54 @@
 import logging
 import platform
+import re
 import synapse
 from synapse.api.errors import Codes, NotFoundError, SynapseError
 from synapse.http.server import JsonResource
 from synapse.http.servlet import RestServlet, parse_json_object_from_request
 from synapse.rest.admin._base import (
-    admin_patterns,
     assert_requester_is_admin,
     historical_admin_path_patterns,
 )
 from synapse.rest.admin.devices import (
     DeleteDevicesRestServlet,
     DeviceRestServlet,
     DevicesRestServlet,
 )
-from synapse.rest.admin.event_reports import EventReportsRestServlet
 from synapse.rest.admin.groups import DeleteGroupAdminRestServlet
 from synapse.rest.admin.media import ListMediaInRoom, register_servlets_for_media_repo
 from synapse.rest.admin.purge_room_servlet import PurgeRoomServlet
 from synapse.rest.admin.rooms import (
     DeleteRoomRestServlet,
     JoinRoomAliasServlet,
     ListRoomRestServlet,
     RoomMembersRestServlet,
     RoomRestServlet,
     ShutdownRoomRestServlet,
 )
 from synapse.rest.admin.server_notice_servlet import SendServerNoticeServlet
 from synapse.rest.admin.users import (
     AccountValidityRenewServlet,
     DeactivateAccountRestServlet,
     ResetPasswordRestServlet,
     SearchUsersRestServlet,
     UserAdminServlet,
-    UserMembershipRestServlet,
     UserRegisterServlet,
     UserRestServletV2,
     UsersRestServlet,
     UsersRestServletV2,
     WhoisRestServlet,
 )
 from synapse.util.versionstring import get_version_string
 logger = logging.getLogger(__name__)
 class VersionServlet(RestServlet):
-    PATTERNS = admin_patterns("/server_version$")
+    PATTERNS = (re.compile("^/_synapse/admin/v1/server_version$"),)
     def __init__(self, hs):
         self.res = {
             "server_version": get_version_string(synapse),
             "python_version": platform.python_version(),
         }
     def on_GET(self, request):
         return 200, self.res
 class PurgeHistoryRestServlet(RestServlet):
     PATTERNS = historical_admin_path_patterns(
         "/purge_history/(?P<room_id>[^/]*)(/(?P<event_id>[^/]+))?"
@@ -66,22 +64,21 @@
     async def on_POST(self, request, room_id, event_id):
         await assert_requester_is_admin(self.auth, request)
         body = parse_json_object_from_request(request, allow_empty_body=True)
         delete_local_events = bool(body.get("delete_local_events", False))
         if event_id is None:
             event_id = body.get("purge_up_to_event_id")
         if event_id is not None:
             event = await self.store.get_event(event_id)
             if event.room_id != room_id:
                 raise SynapseError(400, "Event is for wrong room.")
-            room_token = await self.store.get_topological_token_for_event(event_id)
-            token = await room_token.to_string(self.store)
+            token = await self.store.get_topological_token_for_event(event_id)
             logger.info("[purge] purging up to token %s (event_id %s)", token, event_id)
         elif "purge_up_to_ts" in body:
             ts = body["purge_up_to_ts"]
             if not isinstance(ts, int):
                 raise SynapseError(
                     400, "purge_up_to_ts must be an int", errcode=Codes.BAD_JSON
                 )
             stream_ordering = await self.store.find_first_stream_ordering_after_ts(ts)
             r = await self.store.get_room_event_before_stream_ordering(
                 room_id, stream_ordering
@@ -144,27 +141,25 @@
     register_servlets_for_client_rest_resource(hs, http_server)
     ListRoomRestServlet(hs).register(http_server)
     RoomRestServlet(hs).register(http_server)
     RoomMembersRestServlet(hs).register(http_server)
     DeleteRoomRestServlet(hs).register(http_server)
     JoinRoomAliasServlet(hs).register(http_server)
     PurgeRoomServlet(hs).register(http_server)
     SendServerNoticeServlet(hs).register(http_server)
     VersionServlet(hs).register(http_server)
     UserAdminServlet(hs).register(http_server)
-    UserMembershipRestServlet(hs).register(http_server)
     UserRestServletV2(hs).register(http_server)
     UsersRestServletV2(hs).register(http_server)
     DeviceRestServlet(hs).register(http_server)
     DevicesRestServlet(hs).register(http_server)
     DeleteDevicesRestServlet(hs).register(http_server)
-    EventReportsRestServlet(hs).register(http_server)
 def register_servlets_for_client_rest_resource(hs, http_server):
     """Register only the servlets which need to be exposed on /_matrix/client/xxx"""
     WhoisRestServlet(hs).register(http_server)
     PurgeHistoryStatusRestServlet(hs).register(http_server)
     DeactivateAccountRestServlet(hs).register(http_server)
     PurgeHistoryRestServlet(hs).register(http_server)
     UsersRestServlet(hs).register(http_server)
     ResetPasswordRestServlet(hs).register(http_server)
     SearchUsersRestServlet(hs).register(http_server)
     ShutdownRoomRestServlet(hs).register(http_server)

--- a/synapse/rest/admin/_base.py
+++ b/synapse/rest/admin/_base.py
@@ -14,29 +14,29 @@
     """
     return [
         re.compile(prefix + path_regex)
         for prefix in (
             "^/_synapse/admin/v1",
             "^/_matrix/client/api/v1/admin",
             "^/_matrix/client/unstable/admin",
             "^/_matrix/client/r0/admin",
         )
     ]
-def admin_patterns(path_regex: str, version: str = "v1"):
+def admin_patterns(path_regex: str):
     """Returns the list of patterns for an admin endpoint
     Args:
         path_regex: The regex string to match. This should NOT have a ^
             as this will be prefixed.
     Returns:
         A list of regex patterns.
     """
-    admin_prefix = "^/_synapse/admin/" + version
+    admin_prefix = "^/_synapse/admin/v1"
     patterns = [re.compile(admin_prefix + path_regex)]
     return patterns
 async def assert_requester_is_admin(
     auth: synapse.api.auth.Auth, request: twisted.web.server.Request
 ) -> None:
     """Verify that the requester is an admin user
     Args:
         auth: api.auth.Auth singleton
         request: incoming request
     Raises:

--- a/synapse/rest/admin/devices.py
+++ b/synapse/rest/admin/devices.py
@@ -1,29 +1,32 @@
 import logging
+import re
 from synapse.api.errors import NotFoundError, SynapseError
 from synapse.http.servlet import (
     RestServlet,
     assert_params_in_dict,
     parse_json_object_from_request,
 )
-from synapse.rest.admin._base import admin_patterns, assert_requester_is_admin
+from synapse.rest.admin._base import assert_requester_is_admin
 from synapse.types import UserID
 logger = logging.getLogger(__name__)
 class DeviceRestServlet(RestServlet):
     """
     Get, update or delete the given user's device
     """
-    PATTERNS = admin_patterns(
-        "/users/(?P<user_id>[^/]*)/devices/(?P<device_id>[^/]*)$", "v2"
+    PATTERNS = (
+        re.compile(
+            "^/_synapse/admin/v2/users/(?P<user_id>[^/]*)/devices/(?P<device_id>[^/]*)$"
+        ),
     )
     def __init__(self, hs):
-        super().__init__()
+        super(DeviceRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.device_handler = hs.get_device_handler()
         self.store = hs.get_datastore()
     async def on_GET(self, request, user_id, device_id):
         await assert_requester_is_admin(self.auth, request)
         target_user = UserID.from_string(user_id)
         if not self.hs.is_mine(target_user):
             raise SynapseError(400, "Can only lookup local users")
         u = await self.store.get_user_by_id(target_user.to_string())
@@ -53,21 +56,21 @@
             raise NotFoundError("Unknown user")
         body = parse_json_object_from_request(request, allow_empty_body=True)
         await self.device_handler.update_device(
             target_user.to_string(), device_id, body
         )
         return 200, {}
 class DevicesRestServlet(RestServlet):
     """
     Retrieve the given user's devices
     """
-    PATTERNS = admin_patterns("/users/(?P<user_id>[^/]*)/devices$", "v2")
+    PATTERNS = (re.compile("^/_synapse/admin/v2/users/(?P<user_id>[^/]*)/devices$"),)
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
         self.hs = hs
         self.auth = hs.get_auth()
         self.device_handler = hs.get_device_handler()
         self.store = hs.get_datastore()
     async def on_GET(self, request, user_id):
@@ -78,21 +81,23 @@
         u = await self.store.get_user_by_id(target_user.to_string())
         if u is None:
             raise NotFoundError("Unknown user")
         devices = await self.device_handler.get_devices_by_user(target_user.to_string())
         return 200, {"devices": devices}
 class DeleteDevicesRestServlet(RestServlet):
     """
     API for bulk deletion of devices. Accepts a JSON object with a devices
     key which lists the device_ids to delete.
     """
-    PATTERNS = admin_patterns("/users/(?P<user_id>[^/]*)/delete_devices$", "v2")
+    PATTERNS = (
+        re.compile("^/_synapse/admin/v2/users/(?P<user_id>[^/]*)/delete_devices$"),
+    )
     def __init__(self, hs):
         self.hs = hs
         self.auth = hs.get_auth()
         self.device_handler = hs.get_device_handler()
         self.store = hs.get_datastore()
     async def on_POST(self, request, user_id):
         await assert_requester_is_admin(self.auth, request)
         target_user = UserID.from_string(user_id)
         if not self.hs.is_mine(target_user):
             raise SynapseError(400, "Can only lookup local users")

--- a/synapse/rest/admin/event_reports.py
+++ b//dev/null
@@ -1,58 +0,0 @@
-import logging
-from synapse.api.errors import Codes, SynapseError
-from synapse.http.servlet import RestServlet, parse_integer, parse_string
-from synapse.rest.admin._base import admin_patterns, assert_requester_is_admin
-logger = logging.getLogger(__name__)
-class EventReportsRestServlet(RestServlet):
-    """
-    List all reported events that are known to the homeserver. Results are returned
-    in a dictionary containing report information. Supports pagination.
-    The requester must have administrator access in Synapse.
-    GET /_synapse/admin/v1/event_reports
-    returns:
-        200 OK with list of reports if success otherwise an error.
-    Args:
-        The parameters `from` and `limit` are required only for pagination.
-        By default, a `limit` of 100 is used.
-        The parameter `dir` can be used to define the order of results.
-        The parameter `user_id` can be used to filter by user id.
-        The parameter `room_id` can be used to filter by room id.
-    Returns:
-        A list of reported events and an integer representing the total number of
-        reported events that exist given this query
-    """
-    PATTERNS = admin_patterns("/event_reports$")
-    def __init__(self, hs):
-        self.hs = hs
-        self.auth = hs.get_auth()
-        self.store = hs.get_datastore()
-    async def on_GET(self, request):
-        await assert_requester_is_admin(self.auth, request)
-        start = parse_integer(request, "from", default=0)
-        limit = parse_integer(request, "limit", default=100)
-        direction = parse_string(request, "dir", default="b")
-        user_id = parse_string(request, "user_id")
-        room_id = parse_string(request, "room_id")
-        if start < 0:
-            raise SynapseError(
-                400,
-                "The start parameter must be a positive integer.",
-                errcode=Codes.INVALID_PARAM,
-            )
-        if limit < 0:
-            raise SynapseError(
-                400,
-                "The limit parameter must be a positive integer.",
-                errcode=Codes.INVALID_PARAM,
-            )
-        if direction not in ("f", "b"):
-            raise SynapseError(
-                400, "Unknown direction: %s" % (direction,), errcode=Codes.INVALID_PARAM
-            )
-        event_reports, total = await self.store.get_event_reports_paginate(
-            start, limit, direction, user_id, room_id
-        )
-        ret = {"event_reports": event_reports, "total": total}
-        if (start + limit) < total:
-            ret["next_token"] = start + len(event_reports)
-        return 200, ret

--- a/synapse/rest/admin/purge_room_servlet.py
+++ b/synapse/rest/admin/purge_room_servlet.py
@@ -1,27 +1,27 @@
+import re
 from synapse.http.servlet import (
     RestServlet,
     assert_params_in_dict,
     parse_json_object_from_request,
 )
 from synapse.rest.admin import assert_requester_is_admin
-from synapse.rest.admin._base import admin_patterns
 class PurgeRoomServlet(RestServlet):
     """Servlet which will remove all trace of a room from the database
     POST /_synapse/admin/v1/purge_room
     {
         "room_id": "!room:id"
     }
     returns:
     {}
     """
-    PATTERNS = admin_patterns("/purge_room$")
+    PATTERNS = (re.compile("^/_synapse/admin/v1/purge_room$"),)
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
         self.hs = hs
         self.auth = hs.get_auth()
         self.pagination_handler = hs.get_pagination_handler()
     async def on_POST(self, request):
         await assert_requester_is_admin(self.auth, request)

--- a/synapse/rest/admin/server_notice_servlet.py
+++ b/synapse/rest/admin/server_notice_servlet.py
@@ -1,19 +1,19 @@
+import re
 from synapse.api.constants import EventTypes
 from synapse.api.errors import SynapseError
 from synapse.http.servlet import (
     RestServlet,
     assert_params_in_dict,
     parse_json_object_from_request,
 )
 from synapse.rest.admin import assert_requester_is_admin
-from synapse.rest.admin._base import admin_patterns
 from synapse.rest.client.transactions import HttpTransactionCache
 from synapse.types import UserID
 class SendServerNoticeServlet(RestServlet):
     """Servlet which will send a server notice to a given user
     POST /_synapse/admin/v1/send_server_notice
     {
         "user_id": "@target_user:server_name",
         "content": {
             "msgtype": "m.text",
             "body": "This is my message"
@@ -27,27 +27,27 @@
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
         self.hs = hs
         self.auth = hs.get_auth()
         self.txns = HttpTransactionCache(hs)
         self.snm = hs.get_server_notices_manager()
     def register(self, json_resource):
-        PATTERN = "/send_server_notice"
+        PATTERN = "^/_synapse/admin/v1/send_server_notice"
         json_resource.register_paths(
-            "POST", admin_patterns(PATTERN + "$"), self.on_POST, self.__class__.__name__
+            "POST", (re.compile(PATTERN + "$"),), self.on_POST, self.__class__.__name__
         )
         json_resource.register_paths(
             "PUT",
-            admin_patterns(PATTERN + "/(?P<txn_id>[^/]*)$"),
+            (re.compile(PATTERN + "/(?P<txn_id>[^/]*)$"),),
             self.on_PUT,
             self.__class__.__name__,
         )
     async def on_POST(self, request, txn_id=None):
         await assert_requester_is_admin(self.auth, request)
         body = parse_json_object_from_request(request)
         assert_params_in_dict(body, ("user_id", "content"))
         event_type = body.get("type", EventTypes.Message)
         state_key = body.get("state_key")
         if not self.snm.is_enabled():

--- a/synapse/rest/admin/users.py
+++ b/synapse/rest/admin/users.py
@@ -1,26 +1,26 @@
 import hashlib
 import hmac
 import logging
+import re
 from http import HTTPStatus
 from synapse.api.constants import UserTypes
 from synapse.api.errors import Codes, NotFoundError, SynapseError
 from synapse.http.servlet import (
     RestServlet,
     assert_params_in_dict,
     parse_boolean,
     parse_integer,
     parse_json_object_from_request,
     parse_string,
 )
 from synapse.rest.admin._base import (
-    admin_patterns,
     assert_requester_is_admin,
     assert_user_is_admin,
     historical_admin_path_patterns,
 )
 from synapse.types import UserID
 logger = logging.getLogger(__name__)
 class UsersRestServlet(RestServlet):
     PATTERNS = historical_admin_path_patterns("/users/(?P<user_id>[^/]*)$")
     def __init__(self, hs):
         self.hs = hs
@@ -28,21 +28,21 @@
         self.auth = hs.get_auth()
         self.admin_handler = hs.get_handlers().admin_handler
     async def on_GET(self, request, user_id):
         target_user = UserID.from_string(user_id)
         await assert_requester_is_admin(self.auth, request)
         if not self.hs.is_mine(target_user):
             raise SynapseError(400, "Can only users a local user")
         ret = await self.store.get_users()
         return 200, ret
 class UsersRestServletV2(RestServlet):
-    PATTERNS = admin_patterns("/users$", "v2")
+    PATTERNS = (re.compile("^/_synapse/admin/v2/users$"),)
     """Get request to list all local users.
     This needs user to have administrator access in Synapse.
     GET /_synapse/admin/v2/users?from=0&limit=10&guests=false
     returns:
         200 OK with list of users if success otherwise an error.
     The parameters `from` and `limit` are required only for pagination.
     By default, a `limit` of 100 is used.
     The parameter `user_id` can be used to filter by user id.
     The parameter `name` can be used to filter by user id or display name.
     The parameter `guests` can be used to exclude guest users.
@@ -62,21 +62,21 @@
         guests = parse_boolean(request, "guests", default=True)
         deactivated = parse_boolean(request, "deactivated", default=False)
         users, total = await self.store.get_users_paginate(
             start, limit, user_id, name, guests, deactivated
         )
         ret = {"users": users, "total": total}
         if len(users) >= limit:
             ret["next_token"] = str(start + len(users))
         return 200, ret
 class UserRestServletV2(RestServlet):
-    PATTERNS = admin_patterns("/users/(?P<user_id>[^/]+)$", "v2")
+    PATTERNS = (re.compile("^/_synapse/admin/v2/users/(?P<user_id>[^/]+)$"),)
     """Get request to list user details.
     This needs user to have administrator access in Synapse.
     GET /_synapse/admin/v2/users/<user_id>
     returns:
         200 OK with user details if success otherwise an error.
     Put request to allow an administrator to add or modify a user.
     This needs user to have administrator access in Synapse.
     We use PUT instead of POST since we already know the id of the user
     object to create. POST could be used to create guests.
     PUT /_synapse/admin/v2/users/<user_id>
@@ -473,21 +473,21 @@
                 }
         * Set
             PUT /_synapse/admin/v1/users/@reivilibre:librepush.net/admin
             request body:
                 {
                     "admin": true
                 }
             response on success:
                 {}
     """
-    PATTERNS = admin_patterns("/users/(?P<user_id>[^/]*)/admin$")
+    PATTERNS = (re.compile("^/_synapse/admin/v1/users/(?P<user_id>[^/]*)/admin$"),)
     def __init__(self, hs):
         self.hs = hs
         self.store = hs.get_datastore()
         self.auth = hs.get_auth()
     async def on_GET(self, request, user_id):
         await assert_requester_is_admin(self.auth, request)
         target_user = UserID.from_string(user_id)
         if not self.hs.is_mine(target_user):
             raise SynapseError(400, "Only local users can be admins of this homeserver")
         is_admin = await self.store.is_server_admin(target_user)
@@ -499,28 +499,10 @@
         target_user = UserID.from_string(user_id)
         body = parse_json_object_from_request(request)
         assert_params_in_dict(body, ["admin"])
         if not self.hs.is_mine(target_user):
             raise SynapseError(400, "Only local users can be admins of this homeserver")
         set_admin_to = bool(body["admin"])
         if target_user == auth_user and not set_admin_to:
             raise SynapseError(400, "You may not demote yourself.")
         await self.store.set_server_admin(target_user, set_admin_to)
         return 200, {}
-class UserMembershipRestServlet(RestServlet):
-    """
-    Get room list of an user.
-    """
-    PATTERNS = admin_patterns("/users/(?P<user_id>[^/]+)/joined_rooms$")
-    def __init__(self, hs):
-        self.is_mine = hs.is_mine
-        self.auth = hs.get_auth()
-        self.store = hs.get_datastore()
-    async def on_GET(self, request, user_id):
-        await assert_requester_is_admin(self.auth, request)
-        if not self.is_mine(UserID.from_string(user_id)):
-            raise SynapseError(400, "Can only lookup local users")
-        room_ids = await self.store.get_rooms_for_user(user_id)
-        if not room_ids:
-            raise NotFoundError("User not found")
-        ret = {"joined_rooms": list(room_ids), "total": len(room_ids)}
-        return 200, ret

--- a/synapse/rest/client/v1/directory.py
+++ b/synapse/rest/client/v1/directory.py
@@ -10,21 +10,21 @@
 from synapse.rest.client.v2_alpha._base import client_patterns
 from synapse.types import RoomAlias
 logger = logging.getLogger(__name__)
 def register_servlets(hs, http_server):
     ClientDirectoryServer(hs).register(http_server)
     ClientDirectoryListServer(hs).register(http_server)
     ClientAppserviceDirectoryListServer(hs).register(http_server)
 class ClientDirectoryServer(RestServlet):
     PATTERNS = client_patterns("/directory/room/(?P<room_alias>[^/]*)$", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(ClientDirectoryServer, self).__init__()
         self.store = hs.get_datastore()
         self.handlers = hs.get_handlers()
         self.auth = hs.get_auth()
     async def on_GET(self, request, room_alias):
         room_alias = RoomAlias.from_string(room_alias)
         dir_handler = self.handlers.directory_handler
         res = await dir_handler.get_association(room_alias)
         return 200, res
     async def on_PUT(self, request, room_alias):
         room_alias = RoomAlias.from_string(room_alias)
@@ -65,21 +65,21 @@
         user = requester.user
         room_alias = RoomAlias.from_string(room_alias)
         await dir_handler.delete_association(requester, room_alias)
         logger.info(
             "User %s deleted alias %s", user.to_string(), room_alias.to_string()
         )
         return 200, {}
 class ClientDirectoryListServer(RestServlet):
     PATTERNS = client_patterns("/directory/list/room/(?P<room_id>[^/]*)$", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(ClientDirectoryListServer, self).__init__()
         self.store = hs.get_datastore()
         self.handlers = hs.get_handlers()
         self.auth = hs.get_auth()
     async def on_GET(self, request, room_id):
         room = await self.store.get_room(room_id)
         if room is None:
             raise NotFoundError("Unknown room")
         return 200, {"visibility": "public" if room["is_public"] else "private"}
     async def on_PUT(self, request, room_id):
         requester = await self.auth.get_user_by_req(request)
@@ -93,21 +93,21 @@
         requester = await self.auth.get_user_by_req(request)
         await self.handlers.directory_handler.edit_published_room_list(
             requester, room_id, "private"
         )
         return 200, {}
 class ClientAppserviceDirectoryListServer(RestServlet):
     PATTERNS = client_patterns(
         "/directory/list/appservice/(?P<network_id>[^/]*)/(?P<room_id>[^/]*)$", v1=True
     )
     def __init__(self, hs):
-        super().__init__()
+        super(ClientAppserviceDirectoryListServer, self).__init__()
         self.store = hs.get_datastore()
         self.handlers = hs.get_handlers()
         self.auth = hs.get_auth()
     def on_PUT(self, request, network_id, room_id):
         content = parse_json_object_from_request(request)
         visibility = content.get("visibility", "public")
         return self._edit(request, network_id, room_id, visibility)
     def on_DELETE(self, request, network_id, room_id):
         return self._edit(request, network_id, room_id, "private")
     async def _edit(self, request, network_id, room_id, visibility):

--- a/synapse/rest/client/v1/events.py
+++ b/synapse/rest/client/v1/events.py
@@ -2,34 +2,33 @@
 import logging
 from synapse.api.errors import SynapseError
 from synapse.http.servlet import RestServlet
 from synapse.rest.client.v2_alpha._base import client_patterns
 from synapse.streams.config import PaginationConfig
 logger = logging.getLogger(__name__)
 class EventStreamRestServlet(RestServlet):
     PATTERNS = client_patterns("/events$", v1=True)
     DEFAULT_LONGPOLL_TIME_MS = 30000
     def __init__(self, hs):
-        super().__init__()
+        super(EventStreamRestServlet, self).__init__()
         self.event_stream_handler = hs.get_event_stream_handler()
         self.auth = hs.get_auth()
-        self.store = hs.get_datastore()
     async def on_GET(self, request):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         is_guest = requester.is_guest
         room_id = None
         if is_guest:
             if b"room_id" not in request.args:
                 raise SynapseError(400, "Guest users must specify room_id param")
         if b"room_id" in request.args:
             room_id = request.args[b"room_id"][0].decode("ascii")
-        pagin_config = await PaginationConfig.from_request(self.store, request)
+        pagin_config = PaginationConfig.from_request(request)
         timeout = EventStreamRestServlet.DEFAULT_LONGPOLL_TIME_MS
         if b"timeout" in request.args:
             try:
                 timeout = int(request.args[b"timeout"][0])
             except ValueError:
                 raise SynapseError(400, "timeout must be in milliseconds.")
         as_client_event = b"raw" not in request.args
         chunk = await self.event_stream_handler.get_stream(
             requester.user.to_string(),
             pagin_config,
@@ -38,21 +37,21 @@
             affect_presence=(not is_guest),
             room_id=room_id,
             is_guest=is_guest,
         )
         return 200, chunk
     def on_OPTIONS(self, request):
         return 200, {}
 class EventRestServlet(RestServlet):
     PATTERNS = client_patterns("/events/(?P<event_id>[^/]*)$", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(EventRestServlet, self).__init__()
         self.clock = hs.get_clock()
         self.event_handler = hs.get_event_handler()
         self.auth = hs.get_auth()
         self._event_serializer = hs.get_event_client_serializer()
     async def on_GET(self, request, event_id):
         requester = await self.auth.get_user_by_req(request)
         event = await self.event_handler.get_event(requester.user, None, event_id)
         time_now = self.clock.time_msec()
         if event:
             event = await self._event_serializer.serialize_event(event, time_now)

--- a/synapse/rest/client/v1/initial_sync.py
+++ b/synapse/rest/client/v1/initial_sync.py
@@ -1,24 +1,23 @@
 from synapse.http.servlet import RestServlet, parse_boolean
 from synapse.rest.client.v2_alpha._base import client_patterns
 from synapse.streams.config import PaginationConfig
 class InitialSyncRestServlet(RestServlet):
     PATTERNS = client_patterns("/initialSync$", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(InitialSyncRestServlet, self).__init__()
         self.initial_sync_handler = hs.get_initial_sync_handler()
         self.auth = hs.get_auth()
-        self.store = hs.get_datastore()
     async def on_GET(self, request):
         requester = await self.auth.get_user_by_req(request)
         as_client_event = b"raw" not in request.args
-        pagination_config = await PaginationConfig.from_request(self.store, request)
+        pagination_config = PaginationConfig.from_request(request)
         include_archived = parse_boolean(request, "archived", default=False)
         content = await self.initial_sync_handler.snapshot_all_rooms(
             user_id=requester.user.to_string(),
             pagin_config=pagination_config,
             as_client_event=as_client_event,
             include_archived=include_archived,
         )
         return 200, content
 def register_servlets(hs, http_server):
     InitialSyncRestServlet(hs).register(http_server)

--- a/synapse/rest/client/v1/login.py
+++ b/synapse/rest/client/v1/login.py
@@ -1,15 +1,14 @@
 import logging
 from typing import Awaitable, Callable, Dict, Optional
 from synapse.api.errors import Codes, LoginError, SynapseError
 from synapse.api.ratelimiting import Ratelimiter
-from synapse.appservice import ApplicationService
 from synapse.handlers.auth import (
     convert_client_dict_legacy_fields_to_identifier,
     login_id_phone_to_thirdparty,
 )
 from synapse.http.server import finish_request
 from synapse.http.servlet import (
     RestServlet,
     parse_json_object_from_request,
     parse_string,
 )
@@ -19,33 +18,31 @@
 from synapse.types import JsonDict, UserID
 from synapse.util.threepids import canonicalise_email
 logger = logging.getLogger(__name__)
 class LoginRestServlet(RestServlet):
     PATTERNS = client_patterns("/login$", v1=True)
     CAS_TYPE = "m.login.cas"
     SSO_TYPE = "m.login.sso"
     TOKEN_TYPE = "m.login.token"
     JWT_TYPE = "org.matrix.login.jwt"
     JWT_TYPE_DEPRECATED = "m.login.jwt"
-    APPSERVICE_TYPE = "uk.half-shot.msc2778.login.application_service"
-    def __init__(self, hs):
-        super().__init__()
+    def __init__(self, hs):
+        super(LoginRestServlet, self).__init__()
         self.hs = hs
         self.jwt_enabled = hs.config.jwt_enabled
         self.jwt_secret = hs.config.jwt_secret
         self.jwt_algorithm = hs.config.jwt_algorithm
         self.jwt_issuer = hs.config.jwt_issuer
         self.jwt_audiences = hs.config.jwt_audiences
         self.saml2_enabled = hs.config.saml2_enabled
         self.cas_enabled = hs.config.cas_enabled
         self.oidc_enabled = hs.config.oidc_enabled
-        self.auth = hs.get_auth()
         self.auth_handler = self.hs.get_auth_handler()
         self.registration_handler = hs.get_registration_handler()
         self.handlers = hs.get_handlers()
         self._well_known_builder = WellKnownBuilder(hs)
         self._address_ratelimiter = Ratelimiter(
             clock=hs.get_clock(),
             rate_hz=self.hs.config.rc_login_address.per_second,
             burst_count=self.hs.config.rc_login_address.burst_count,
         )
         self._account_ratelimiter = Ratelimiter(
@@ -71,59 +68,35 @@
         flows.extend(
             ({"type": t} for t in self.auth_handler.get_supported_login_types())
         )
         return 200, {"flows": flows}
     def on_OPTIONS(self, request: SynapseRequest):
         return 200, {}
     async def on_POST(self, request: SynapseRequest):
         self._address_ratelimiter.ratelimit(request.getClientIP())
         login_submission = parse_json_object_from_request(request)
         try:
-            if login_submission["type"] == LoginRestServlet.APPSERVICE_TYPE:
-                appservice = self.auth.get_appservice_by_req(request)
-                result = await self._do_appservice_login(login_submission, appservice)
-            elif self.jwt_enabled and (
+            if self.jwt_enabled and (
                 login_submission["type"] == LoginRestServlet.JWT_TYPE
                 or login_submission["type"] == LoginRestServlet.JWT_TYPE_DEPRECATED
             ):
                 result = await self._do_jwt_login(login_submission)
             elif login_submission["type"] == LoginRestServlet.TOKEN_TYPE:
                 result = await self._do_token_login(login_submission)
             else:
                 result = await self._do_other_login(login_submission)
         except KeyError:
             raise SynapseError(400, "Missing JSON keys.")
         well_known_data = self._well_known_builder.get_well_known()
         if well_known_data:
             result["well_known"] = well_known_data
         return 200, result
-    def _get_qualified_user_id(self, identifier):
-        if identifier["type"] != "m.id.user":
-            raise SynapseError(400, "Unknown login identifier type")
-        if "user" not in identifier:
-            raise SynapseError(400, "User identifier is missing 'user' key")
-        if identifier["user"].startswith("@"):
-            return identifier["user"]
-        else:
-            return UserID(identifier["user"], self.hs.hostname).to_string()
-    async def _do_appservice_login(
-        self, login_submission: JsonDict, appservice: ApplicationService
-    ):
-        logger.info(
-            "Got appservice login request with identifier: %r",
-            login_submission.get("identifier"),
-        )
-        identifier = convert_client_dict_legacy_fields_to_identifier(login_submission)
-        qualified_user_id = self._get_qualified_user_id(identifier)
-        if not appservice.is_interested_in_user(qualified_user_id):
-            raise LoginError(403, "Invalid access_token", errcode=Codes.FORBIDDEN)
-        return await self._complete_login(qualified_user_id, login_submission)
     async def _do_other_login(self, login_submission: JsonDict) -> Dict[str, str]:
         """Handle non-token/saml/jwt logins
         Args:
             login_submission:
         Returns:
             HTTP response
         """
         logger.info(
             "Got login request with identifier: %r, medium: %r, address: %r, user: %r",
             login_submission.get("identifier"),
@@ -159,55 +132,64 @@
             user_id = await self.hs.get_datastore().get_user_id_by_threepid(
                 medium, address
             )
             if not user_id:
                 logger.warning(
                     "unknown 3pid identifier medium %s, address %r", medium, address
                 )
                 self._failed_attempts_ratelimiter.can_do_action((medium, address))
                 raise LoginError(403, "", errcode=Codes.FORBIDDEN)
             identifier = {"type": "m.id.user", "user": user_id}
-        qualified_user_id = self._get_qualified_user_id(identifier)
+        if identifier["type"] != "m.id.user":
+            raise SynapseError(400, "Unknown login identifier type")
+        if "user" not in identifier:
+            raise SynapseError(400, "User identifier is missing 'user' key")
+        if identifier["user"].startswith("@"):
+            qualified_user_id = identifier["user"]
+        else:
+            qualified_user_id = UserID(identifier["user"], self.hs.hostname).to_string()
         self._failed_attempts_ratelimiter.ratelimit(
             qualified_user_id.lower(), update=False
         )
         try:
             canonical_user_id, callback = await self.auth_handler.validate_login(
                 identifier["user"], login_submission
             )
         except LoginError:
             self._failed_attempts_ratelimiter.can_do_action(qualified_user_id.lower())
             raise
         result = await self._complete_login(
             canonical_user_id, login_submission, callback
         )
         return result
     async def _complete_login(
         self,
         user_id: str,
         login_submission: JsonDict,
-        callback: Optional[Callable[[Dict[str, str]], Awaitable[None]]] = None,
+        callback: Optional[
+            Callable[[Dict[str, str]], Awaitable[Dict[str, str]]]
+        ] = None,
         create_non_existent_users: bool = False,
     ) -> Dict[str, str]:
         """Called when we've successfully authed the user and now need to
         actually login them in (e.g. create devices). This gets called on
         all successful logins.
         Applies the ratelimiting for successful login attempts against an
         account.
         Args:
             user_id: ID of the user to register.
             login_submission: Dictionary of login information.
-            callback: Callback function to run after login.
+            callback: Callback function to run after registration.
             create_non_existent_users: Whether to create the user if they don't
                 exist. Defaults to False.
         Returns:
-            result: Dictionary of account information after successful login.
+            result: Dictionary of account information after successful registration.
         """
         self._account_ratelimiter.ratelimit(user_id.lower())
         if create_non_existent_users:
             canonical_uid = await self.auth_handler.check_user_exists(user_id)
             if not canonical_uid:
                 canonical_uid = await self.registration_handler.register_user(
                     localpart=UserID.from_string(user_id).localpart
                 )
             user_id = canonical_uid
         device_id = login_submission.get("device_id")
@@ -218,35 +200,27 @@
         result = {
             "user_id": user_id,
             "access_token": access_token,
             "home_server": self.hs.hostname,
             "device_id": device_id,
         }
         if callback is not None:
             await callback(result)
         return result
     async def _do_token_login(self, login_submission: JsonDict) -> Dict[str, str]:
-        """
-        Handle the final stage of SSO login.
-        Args:
-             login_submission: The JSON request body.
-        Returns:
-            The body of the JSON response.
-        """
         token = login_submission["token"]
         auth_handler = self.auth_handler
         user_id = await auth_handler.validate_short_term_login_token_and_get_user_id(
             token
         )
-        return await self._complete_login(
-            user_id, login_submission, self.auth_handler._sso_login_callback
-        )
+        result = await self._complete_login(user_id, login_submission)
+        return result
     async def _do_jwt_login(self, login_submission: JsonDict) -> Dict[str, str]:
         token = login_submission.get("token", None)
         if token is None:
             raise LoginError(
                 403, "Token field for JWT is missing", errcode=Codes.FORBIDDEN
             )
         import jwt
         try:
             payload = jwt.decode(
                 token,
@@ -295,21 +269,21 @@
         self._cas_handler = hs.get_cas_handler()
     async def get_sso_url(
         self, request: SynapseRequest, client_redirect_url: bytes
     ) -> bytes:
         return self._cas_handler.get_redirect_url(
             {"redirectUrl": client_redirect_url}
         ).encode("ascii")
 class CasTicketServlet(RestServlet):
     PATTERNS = client_patterns("/login/cas/ticket", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(CasTicketServlet, self).__init__()
         self._cas_handler = hs.get_cas_handler()
     async def on_GET(self, request: SynapseRequest) -> None:
         client_redirect_url = parse_string(request, "redirectUrl")
         ticket = parse_string(request, "ticket", required=True)
         session = parse_string(request, "session")
         if not client_redirect_url and not session:
             message = "Missing string query parameter redirectUrl or session"
             raise SynapseError(400, message, errcode=Codes.MISSING_PARAM)
         await self._cas_handler.handle_ticket(
             request, ticket, client_redirect_url, session

--- a/synapse/rest/client/v1/logout.py
+++ b/synapse/rest/client/v1/logout.py
@@ -1,37 +1,37 @@
 import logging
 from synapse.http.servlet import RestServlet
 from synapse.rest.client.v2_alpha._base import client_patterns
 logger = logging.getLogger(__name__)
 class LogoutRestServlet(RestServlet):
     PATTERNS = client_patterns("/logout$", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(LogoutRestServlet, self).__init__()
         self.auth = hs.get_auth()
         self._auth_handler = hs.get_auth_handler()
         self._device_handler = hs.get_device_handler()
     def on_OPTIONS(self, request):
         return 200, {}
     async def on_POST(self, request):
         requester = await self.auth.get_user_by_req(request, allow_expired=True)
         if requester.device_id is None:
             access_token = self.auth.get_access_token_from_request(request)
             await self._auth_handler.delete_access_token(access_token)
         else:
             await self._device_handler.delete_device(
                 requester.user.to_string(), requester.device_id
             )
         return 200, {}
 class LogoutAllRestServlet(RestServlet):
     PATTERNS = client_patterns("/logout/all$", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(LogoutAllRestServlet, self).__init__()
         self.auth = hs.get_auth()
         self._auth_handler = hs.get_auth_handler()
         self._device_handler = hs.get_device_handler()
     def on_OPTIONS(self, request):
         return 200, {}
     async def on_POST(self, request):
         requester = await self.auth.get_user_by_req(request, allow_expired=True)
         user_id = requester.user.to_string()
         await self._device_handler.delete_all_devices_for_user(user_id)
         await self._auth_handler.delete_access_tokens_for_user(user_id)

--- a/synapse/rest/client/v1/presence.py
+++ b/synapse/rest/client/v1/presence.py
@@ -3,21 +3,21 @@
 import logging
 from synapse.api.errors import AuthError, SynapseError
 from synapse.handlers.presence import format_user_presence_state
 from synapse.http.servlet import RestServlet, parse_json_object_from_request
 from synapse.rest.client.v2_alpha._base import client_patterns
 from synapse.types import UserID
 logger = logging.getLogger(__name__)
 class PresenceStatusRestServlet(RestServlet):
     PATTERNS = client_patterns("/presence/(?P<user_id>[^/]*)/status", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(PresenceStatusRestServlet, self).__init__()
         self.hs = hs
         self.presence_handler = hs.get_presence_handler()
         self.clock = hs.get_clock()
         self.auth = hs.get_auth()
     async def on_GET(self, request, user_id):
         requester = await self.auth.get_user_by_req(request)
         user = UserID.from_string(user_id)
         if requester.user != user:
             allowed = await self.presence_handler.is_visible(
                 observed_user=user, observer_user=requester.user

--- a/synapse/rest/client/v1/profile.py
+++ b/synapse/rest/client/v1/profile.py
@@ -1,19 +1,19 @@
 """ This module contains REST servlets to do with profile: /profile/<paths> """
 from synapse.api.errors import Codes, SynapseError
 from synapse.http.servlet import RestServlet, parse_json_object_from_request
 from synapse.rest.client.v2_alpha._base import client_patterns
 from synapse.types import UserID
 class ProfileDisplaynameRestServlet(RestServlet):
     PATTERNS = client_patterns("/profile/(?P<user_id>[^/]*)/displayname", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(ProfileDisplaynameRestServlet, self).__init__()
         self.hs = hs
         self.profile_handler = hs.get_profile_handler()
         self.auth = hs.get_auth()
     async def on_GET(self, request, user_id):
         requester_user = None
         if self.hs.config.require_auth_for_profile_requests:
             requester = await self.auth.get_user_by_req(request)
             requester_user = requester.user
         user = UserID.from_string(user_id)
         await self.profile_handler.check_profile_query_allowed(user, requester_user)
@@ -31,21 +31,21 @@
             new_name = content["displayname"]
         except Exception:
             return 400, "Unable to parse name"
         await self.profile_handler.set_displayname(user, requester, new_name, is_admin)
         return 200, {}
     def on_OPTIONS(self, request, user_id):
         return 200, {}
 class ProfileAvatarURLRestServlet(RestServlet):
     PATTERNS = client_patterns("/profile/(?P<user_id>[^/]*)/avatar_url", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(ProfileAvatarURLRestServlet, self).__init__()
         self.hs = hs
         self.profile_handler = hs.get_profile_handler()
         self.auth = hs.get_auth()
     async def on_GET(self, request, user_id):
         requester_user = None
         if self.hs.config.require_auth_for_profile_requests:
             requester = await self.auth.get_user_by_req(request)
             requester_user = requester.user
         user = UserID.from_string(user_id)
         await self.profile_handler.check_profile_query_allowed(user, requester_user)
@@ -67,21 +67,21 @@
             )
         await self.profile_handler.set_avatar_url(
             user, requester, new_avatar_url, is_admin
         )
         return 200, {}
     def on_OPTIONS(self, request, user_id):
         return 200, {}
 class ProfileRestServlet(RestServlet):
     PATTERNS = client_patterns("/profile/(?P<user_id>[^/]*)", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(ProfileRestServlet, self).__init__()
         self.hs = hs
         self.profile_handler = hs.get_profile_handler()
         self.auth = hs.get_auth()
     async def on_GET(self, request, user_id):
         requester_user = None
         if self.hs.config.require_auth_for_profile_requests:
             requester = await self.auth.get_user_by_req(request)
             requester_user = requester.user
         user = UserID.from_string(user_id)
         await self.profile_handler.check_profile_query_allowed(user, requester_user)

--- a/synapse/rest/client/v1/push_rule.py
+++ b/synapse/rest/client/v1/push_rule.py
@@ -13,21 +13,21 @@
 from synapse.push.clientformat import format_push_rules_for_user
 from synapse.push.rulekinds import PRIORITY_CLASS_MAP
 from synapse.rest.client.v2_alpha._base import client_patterns
 from synapse.storage.push_rule import InconsistentRuleException, RuleNotFoundException
 class PushRuleRestServlet(RestServlet):
     PATTERNS = client_patterns("/(?P<path>pushrules/.*)$", v1=True)
     SLIGHTLY_PEDANTIC_TRAILING_SLASH_ERROR = (
         "Unrecognised request: You probably wanted a trailing slash"
     )
     def __init__(self, hs):
-        super().__init__()
+        super(PushRuleRestServlet, self).__init__()
         self.auth = hs.get_auth()
         self.store = hs.get_datastore()
         self.notifier = hs.get_notifier()
         self._is_worker = hs.config.worker_app is not None
         self._users_new_default_push_rules = hs.config.users_new_default_push_rules
     async def on_PUT(self, request, path):
         if self._is_worker:
             raise Exception("Cannot handle PUT /push_rules on worker")
         spec = _rule_spec_from_path(path.split("/"))
         try:
@@ -105,35 +105,28 @@
             result = _filter_ruleset_with_path(rules["global"], path[1:])
             return 200, result
         else:
             raise UnrecognizedRequestError()
     def on_OPTIONS(self, request, path):
         return 200, {}
     def notify_user(self, user_id):
         stream_id = self.store.get_max_push_rules_stream_id()
         self.notifier.on_new_event("push_rules_key", stream_id, users=[user_id])
     async def set_rule_attr(self, user_id, spec, val):
-        if spec["attr"] not in ("enabled", "actions"):
-            raise UnrecognizedRequestError()
-        namespaced_rule_id = _namespaced_rule_id_from_spec(spec)
-        rule_id = spec["rule_id"]
-        is_default_rule = rule_id.startswith(".")
-        if is_default_rule:
-            if namespaced_rule_id not in BASE_RULE_IDS:
-                raise NotFoundError("Unknown rule %s" % (namespaced_rule_id,))
         if spec["attr"] == "enabled":
             if isinstance(val, dict) and "enabled" in val:
                 val = val["enabled"]
             if not isinstance(val, bool):
                 raise SynapseError(400, "Value for 'enabled' must be boolean")
+            namespaced_rule_id = _namespaced_rule_id_from_spec(spec)
             return await self.store.set_push_rule_enabled(
-                user_id, namespaced_rule_id, val, is_default_rule
+                user_id, namespaced_rule_id, val
             )
         elif spec["attr"] == "actions":
             actions = val.get("actions")
             _check_actions(actions)
             namespaced_rule_id = _namespaced_rule_id_from_spec(spec)
             rule_id = spec["rule_id"]
             is_default_rule = rule_id.startswith(".")
             if is_default_rule:
                 if user_id in self._users_new_default_push_rules:
                     rule_ids = NEW_RULE_IDS

--- a/synapse/rest/client/v1/pusher.py
+++ b/synapse/rest/client/v1/pusher.py
@@ -16,37 +16,37 @@
     "data",
     "device_display_name",
     "kind",
     "lang",
     "profile_tag",
     "pushkey",
 }
 class PushersRestServlet(RestServlet):
     PATTERNS = client_patterns("/pushers$", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(PushersRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
     async def on_GET(self, request):
         requester = await self.auth.get_user_by_req(request)
         user = requester.user
         pushers = await self.hs.get_datastore().get_pushers_by_user_id(user.to_string())
         filtered_pushers = [
             {k: v for k, v in p.items() if k in ALLOWED_KEYS} for p in pushers
         ]
         return 200, {"pushers": filtered_pushers}
     def on_OPTIONS(self, _):
         return 200, {}
 class PushersSetRestServlet(RestServlet):
     PATTERNS = client_patterns("/pushers/set$", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(PushersSetRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.notifier = hs.get_notifier()
         self.pusher_pool = self.hs.get_pusherpool()
     async def on_POST(self, request):
         requester = await self.auth.get_user_by_req(request)
         user = requester.user
         content = parse_json_object_from_request(request)
         if (
             "pushkey" in content
@@ -102,21 +102,21 @@
         return 200, {}
     def on_OPTIONS(self, _):
         return 200, {}
 class PushersRemoveRestServlet(RestServlet):
     """
     To allow pusher to be delete by clicking a link (ie. GET request)
     """
     PATTERNS = client_patterns("/pushers/remove$", v1=True)
     SUCCESS_HTML = b"<html><body>You have been unsubscribed</body><html>"
     def __init__(self, hs):
-        super().__init__()
+        super(PushersRemoveRestServlet, self).__init__()
         self.hs = hs
         self.notifier = hs.get_notifier()
         self.auth = hs.get_auth()
         self.pusher_pool = self.hs.get_pusherpool()
     async def on_GET(self, request):
         requester = await self.auth.get_user_by_req(request, rights="delete_pusher")
         user = requester.user
         app_id = parse_string(request, "app_id", required=True)
         pushkey = parse_string(request, "pushkey", required=True)
         try:

--- a/synapse/rest/client/v1/room.py
+++ b/synapse/rest/client/v1/room.py
@@ -28,25 +28,25 @@
 from synapse.streams.config import PaginationConfig
 from synapse.types import RoomAlias, RoomID, StreamToken, ThirdPartyInstanceID, UserID
 from synapse.util import json_decoder
 from synapse.util.stringutils import random_string
 MYPY = False
 if MYPY:
     import synapse.server
 logger = logging.getLogger(__name__)
 class TransactionRestServlet(RestServlet):
     def __init__(self, hs):
-        super().__init__()
+        super(TransactionRestServlet, self).__init__()
         self.txns = HttpTransactionCache(hs)
 class RoomCreateRestServlet(TransactionRestServlet):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(RoomCreateRestServlet, self).__init__(hs)
         self._room_creation_handler = hs.get_room_creation_handler()
         self.auth = hs.get_auth()
     def register(self, http_server):
         PATTERNS = "/createRoom"
         register_txn_path(self, PATTERNS, http_server)
         http_server.register_paths(
             "OPTIONS",
             client_patterns("/rooms(?:/.*)?$", v1=True),
             self.on_OPTIONS,
             self.__class__.__name__,
@@ -66,21 +66,21 @@
             requester, self.get_room_config(request)
         )
         return 200, info
     def get_room_config(self, request):
         user_supplied_config = parse_json_object_from_request(request)
         return user_supplied_config
     def on_OPTIONS(self, request):
         return 200, {}
 class RoomStateEventRestServlet(TransactionRestServlet):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(RoomStateEventRestServlet, self).__init__(hs)
         self.handlers = hs.get_handlers()
         self.event_creation_handler = hs.get_event_creation_handler()
         self.room_member_handler = hs.get_room_member_handler()
         self.message_handler = hs.get_message_handler()
         self.auth = hs.get_auth()
     def register(self, http_server):
         no_state_key = "/rooms/(?P<room_id>[^/]*)/state/(?P<event_type>[^/]*)$"
         state_key = (
             "/rooms/(?P<room_id>[^/]*)/state/"
             "(?P<event_type>[^/]*)/(?P<state_key>[^/]*)$"
@@ -163,21 +163,21 @@
                     requester, event_dict, txn_id=txn_id
                 )
                 event_id = event.event_id
         except ShadowBanError:
             event_id = "$" + random_string(43)
         set_tag("event_id", event_id)
         ret = {"event_id": event_id}
         return 200, ret
 class RoomSendEventRestServlet(TransactionRestServlet):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(RoomSendEventRestServlet, self).__init__(hs)
         self.event_creation_handler = hs.get_event_creation_handler()
         self.auth = hs.get_auth()
     def register(self, http_server):
         PATTERNS = "/rooms/(?P<room_id>[^/]*)/send/(?P<event_type>[^/]*)"
         register_txn_path(self, PATTERNS, http_server, with_get=True)
     async def on_POST(self, request, room_id, event_type, txn_id=None):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         content = parse_json_object_from_request(request)
         event_dict = {
             "type": event_type,
@@ -201,21 +201,21 @@
         return 200, {"event_id": event_id}
     def on_GET(self, request, room_id, event_type, txn_id):
         return 200, "Not implemented"
     def on_PUT(self, request, room_id, event_type, txn_id):
         set_tag("txn_id", txn_id)
         return self.txns.fetch_or_execute_request(
             request, self.on_POST, request, room_id, event_type, txn_id
         )
 class JoinRoomAliasServlet(TransactionRestServlet):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(JoinRoomAliasServlet, self).__init__(hs)
         self.room_member_handler = hs.get_room_member_handler()
         self.auth = hs.get_auth()
     def register(self, http_server):
         PATTERNS = "/join/(?P<room_identifier>[^/]*)"
         register_txn_path(self, PATTERNS, http_server)
     async def on_POST(self, request, room_identifier, txn_id=None):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         try:
             content = parse_json_object_from_request(request)
         except Exception:
@@ -249,21 +249,21 @@
         )
         return 200, {"room_id": room_id}
     def on_PUT(self, request, room_identifier, txn_id):
         set_tag("txn_id", txn_id)
         return self.txns.fetch_or_execute_request(
             request, self.on_POST, request, room_identifier, txn_id
         )
 class PublicRoomListRestServlet(TransactionRestServlet):
     PATTERNS = client_patterns("/publicRooms$", v1=True)
     def __init__(self, hs):
-        super().__init__(hs)
+        super(PublicRoomListRestServlet, self).__init__(hs)
         self.hs = hs
         self.auth = hs.get_auth()
     async def on_GET(self, request):
         server = parse_string(request, "server", default=None)
         try:
             await self.auth.get_user_by_req(request, allow_guest=True)
         except InvalidClientCredentialsError as e:
             if not self.hs.config.allow_public_rooms_without_auth:
                 raise
             if server:
@@ -325,72 +325,68 @@
             data = await handler.get_local_public_room_list(
                 limit=limit,
                 since_token=since_token,
                 search_filter=search_filter,
                 network_tuple=network_tuple,
             )
         return 200, data
 class RoomMemberListRestServlet(RestServlet):
     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/members$", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(RoomMemberListRestServlet, self).__init__()
         self.message_handler = hs.get_message_handler()
         self.auth = hs.get_auth()
-        self.store = hs.get_datastore()
     async def on_GET(self, request, room_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         handler = self.message_handler
         at_token_string = parse_string(request, "at")
         if at_token_string is None:
             at_token = None
         else:
-            at_token = await StreamToken.from_string(self.store, at_token_string)
+            at_token = StreamToken.from_string(at_token_string)
         membership = parse_string(request, "membership")
         not_membership = parse_string(request, "not_membership")
         events = await handler.get_state_events(
             room_id=room_id,
             user_id=requester.user.to_string(),
             at_token=at_token,
             state_filter=StateFilter.from_types([(EventTypes.Member, None)]),
         )
         chunk = []
         for event in events:
             if (membership and event["content"].get("membership") != membership) or (
                 not_membership and event["content"].get("membership") == not_membership
             ):
                 continue
             chunk.append(event)
         return 200, {"chunk": chunk}
 class JoinedRoomMemberListRestServlet(RestServlet):
     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/joined_members$", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(JoinedRoomMemberListRestServlet, self).__init__()
         self.message_handler = hs.get_message_handler()
         self.auth = hs.get_auth()
     async def on_GET(self, request, room_id):
         requester = await self.auth.get_user_by_req(request)
         users_with_profile = await self.message_handler.get_joined_members(
             requester, room_id
         )
         return 200, {"joined": users_with_profile}
 class RoomMessageListRestServlet(RestServlet):
     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/messages$", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(RoomMessageListRestServlet, self).__init__()
         self.pagination_handler = hs.get_pagination_handler()
         self.auth = hs.get_auth()
-        self.store = hs.get_datastore()
     async def on_GET(self, request, room_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
-        pagination_config = await PaginationConfig.from_request(
-            self.store, request, default_limit=10
-        )
+        pagination_config = PaginationConfig.from_request(request, default_limit=10)
         as_client_event = b"raw" not in request.args
         filter_str = parse_string(request, b"filter", encoding="utf-8")
         if filter_str:
             filter_json = urlparse.unquote(filter_str)
             event_filter = Filter(
                 json_decoder.decode(filter_json)
             )  # type: Optional[Filter]
             if (
                 event_filter
                 and event_filter.filter_json.get("event_format", "client")
@@ -403,51 +399,50 @@
             room_id=room_id,
             requester=requester,
             pagin_config=pagination_config,
             as_client_event=as_client_event,
             event_filter=event_filter,
         )
         return 200, msgs
 class RoomStateRestServlet(RestServlet):
     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/state$", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(RoomStateRestServlet, self).__init__()
         self.message_handler = hs.get_message_handler()
         self.auth = hs.get_auth()
     async def on_GET(self, request, room_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         events = await self.message_handler.get_state_events(
             room_id=room_id,
             user_id=requester.user.to_string(),
             is_guest=requester.is_guest,
         )
         return 200, events
 class RoomInitialSyncRestServlet(RestServlet):
     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/initialSync$", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(RoomInitialSyncRestServlet, self).__init__()
         self.initial_sync_handler = hs.get_initial_sync_handler()
         self.auth = hs.get_auth()
-        self.store = hs.get_datastore()
     async def on_GET(self, request, room_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
-        pagination_config = await PaginationConfig.from_request(self.store, request)
+        pagination_config = PaginationConfig.from_request(request)
         content = await self.initial_sync_handler.room_initial_sync(
             room_id=room_id, requester=requester, pagin_config=pagination_config
         )
         return 200, content
 class RoomEventServlet(RestServlet):
     PATTERNS = client_patterns(
         "/rooms/(?P<room_id>[^/]*)/event/(?P<event_id>[^/]*)$", v1=True
     )
     def __init__(self, hs):
-        super().__init__()
+        super(RoomEventServlet, self).__init__()
         self.clock = hs.get_clock()
         self.event_handler = hs.get_event_handler()
         self._event_serializer = hs.get_event_client_serializer()
         self.auth = hs.get_auth()
     async def on_GET(self, request, room_id, event_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         try:
             event = await self.event_handler.get_event(
                 requester.user, room_id, event_id
             )
@@ -456,21 +451,21 @@
         time_now = self.clock.time_msec()
         if event:
             event = await self._event_serializer.serialize_event(event, time_now)
             return 200, event
         return SynapseError(404, "Event not found.", errcode=Codes.NOT_FOUND)
 class RoomEventContextServlet(RestServlet):
     PATTERNS = client_patterns(
         "/rooms/(?P<room_id>[^/]*)/context/(?P<event_id>[^/]*)$", v1=True
     )
     def __init__(self, hs):
-        super().__init__()
+        super(RoomEventContextServlet, self).__init__()
         self.clock = hs.get_clock()
         self.room_context_handler = hs.get_room_context_handler()
         self._event_serializer = hs.get_event_client_serializer()
         self.auth = hs.get_auth()
     async def on_GET(self, request, room_id, event_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         limit = parse_integer(request, "limit", default=10)
         filter_str = parse_string(request, b"filter", encoding="utf-8")
         if filter_str:
             filter_json = urlparse.unquote(filter_str)
@@ -493,38 +488,38 @@
         )
         results["events_after"] = await self._event_serializer.serialize_events(
             results["events_after"], time_now
         )
         results["state"] = await self._event_serializer.serialize_events(
             results["state"], time_now
         )
         return 200, results
 class RoomForgetRestServlet(TransactionRestServlet):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(RoomForgetRestServlet, self).__init__(hs)
         self.room_member_handler = hs.get_room_member_handler()
         self.auth = hs.get_auth()
     def register(self, http_server):
         PATTERNS = "/rooms/(?P<room_id>[^/]*)/forget"
         register_txn_path(self, PATTERNS, http_server)
     async def on_POST(self, request, room_id, txn_id=None):
         requester = await self.auth.get_user_by_req(request, allow_guest=False)
         await self.room_member_handler.forget(user=requester.user, room_id=room_id)
         return 200, {}
     def on_PUT(self, request, room_id, txn_id):
         set_tag("txn_id", txn_id)
         return self.txns.fetch_or_execute_request(
             request, self.on_POST, request, room_id, txn_id
         )
 class RoomMembershipRestServlet(TransactionRestServlet):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(RoomMembershipRestServlet, self).__init__(hs)
         self.room_member_handler = hs.get_room_member_handler()
         self.auth = hs.get_auth()
     def register(self, http_server):
         PATTERNS = (
             "/rooms/(?P<room_id>[^/]*)/"
             "(?P<membership_action>join|invite|leave|ban|unban|kick)"
         )
         register_txn_path(self, PATTERNS, http_server)
     async def on_POST(self, request, room_id, membership_action, txn_id=None):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
@@ -580,21 +575,21 @@
             if key not in content:
                 return False
         return True
     def on_PUT(self, request, room_id, membership_action, txn_id):
         set_tag("txn_id", txn_id)
         return self.txns.fetch_or_execute_request(
             request, self.on_POST, request, room_id, membership_action, txn_id
         )
 class RoomRedactEventRestServlet(TransactionRestServlet):
     def __init__(self, hs):
-        super().__init__(hs)
+        super(RoomRedactEventRestServlet, self).__init__(hs)
         self.handlers = hs.get_handlers()
         self.event_creation_handler = hs.get_event_creation_handler()
         self.auth = hs.get_auth()
     def register(self, http_server):
         PATTERNS = "/rooms/(?P<room_id>[^/]*)/redact/(?P<event_id>[^/]*)"
         register_txn_path(self, PATTERNS, http_server)
     async def on_POST(self, request, room_id, event_id, txn_id=None):
         requester = await self.auth.get_user_by_req(request)
         content = parse_json_object_from_request(request)
         try:
@@ -620,21 +615,21 @@
     def on_PUT(self, request, room_id, event_id, txn_id):
         set_tag("txn_id", txn_id)
         return self.txns.fetch_or_execute_request(
             request, self.on_POST, request, room_id, event_id, txn_id
         )
 class RoomTypingRestServlet(RestServlet):
     PATTERNS = client_patterns(
         "/rooms/(?P<room_id>[^/]*)/typing/(?P<user_id>[^/]*)$", v1=True
     )
     def __init__(self, hs):
-        super().__init__()
+        super(RoomTypingRestServlet, self).__init__()
         self.presence_handler = hs.get_presence_handler()
         self.typing_handler = hs.get_typing_handler()
         self.auth = hs.get_auth()
         self._is_typing_writer = (
             hs.config.worker.writers.typing == hs.get_instance_name()
         )
     async def on_PUT(self, request, room_id, user_id):
         requester = await self.auth.get_user_by_req(request)
         if not self._is_typing_writer:
             raise Exception("Got /typing request on instance that is not typing writer")
@@ -671,35 +666,35 @@
         self.directory_handler = hs.get_handlers().directory_handler
     async def on_GET(self, request, room_id):
         requester = await self.auth.get_user_by_req(request)
         alias_list = await self.directory_handler.get_aliases_for_room(
             requester, room_id
         )
         return 200, {"aliases": alias_list}
 class SearchRestServlet(RestServlet):
     PATTERNS = client_patterns("/search$", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(SearchRestServlet, self).__init__()
         self.handlers = hs.get_handlers()
         self.auth = hs.get_auth()
     async def on_POST(self, request):
         requester = await self.auth.get_user_by_req(request)
         content = parse_json_object_from_request(request)
         batch = parse_string(request, "next_batch")
         results = await self.handlers.search_handler.search(
             requester.user, content, batch
         )
         return 200, results
 class JoinedRoomsRestServlet(RestServlet):
     PATTERNS = client_patterns("/joined_rooms$", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(JoinedRoomsRestServlet, self).__init__()
         self.store = hs.get_datastore()
         self.auth = hs.get_auth()
     async def on_GET(self, request):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         room_ids = await self.store.get_rooms_for_user(requester.user.to_string())
         return 200, {"joined_rooms": list(room_ids)}
 def register_txn_path(servlet, regex_string, http_server, with_get=False):
     """Registers a transaction-based path.
     This registers two paths:
         PUT regex_string/$txnid

--- a/synapse/rest/client/v1/voip.py
+++ b/synapse/rest/client/v1/voip.py
@@ -1,19 +1,19 @@
 import base64
 import hashlib
 import hmac
 from synapse.http.servlet import RestServlet
 from synapse.rest.client.v2_alpha._base import client_patterns
 class VoipRestServlet(RestServlet):
     PATTERNS = client_patterns("/voip/turnServer$", v1=True)
     def __init__(self, hs):
-        super().__init__()
+        super(VoipRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
     async def on_GET(self, request):
         requester = await self.auth.get_user_by_req(
             request, self.hs.config.turn_allow_guests
         )
         turnUris = self.hs.config.turn_uris
         turnSecret = self.hs.config.turn_shared_secret
         turnUsername = self.hs.config.turn_username
         turnPassword = self.hs.config.turn_password

--- a/synapse/rest/client/v2_alpha/account.py
+++ b/synapse/rest/client/v2_alpha/account.py
@@ -1,17 +1,13 @@
 import logging
 import random
 from http import HTTPStatus
-from typing import TYPE_CHECKING
-from urllib.parse import urlparse
-if TYPE_CHECKING:
-    from synapse.app.homeserver import HomeServer
 from synapse.api.constants import LoginType
 from synapse.api.errors import (
     Codes,
     InteractiveAuthIncompleteError,
     SynapseError,
     ThreepidValidationError,
 )
 from synapse.config.emailconfig import ThreepidBehaviour
 from synapse.http.server import finish_request, respond_with_html
 from synapse.http.servlet import (
@@ -22,21 +18,21 @@
 )
 from synapse.push.mailer import Mailer
 from synapse.util.msisdn import phone_number_to_msisdn
 from synapse.util.stringutils import assert_valid_client_secret, random_string
 from synapse.util.threepids import canonicalise_email, check_3pid_allowed
 from ._base import client_patterns, interactive_auth_handler
 logger = logging.getLogger(__name__)
 class EmailPasswordRequestTokenRestServlet(RestServlet):
     PATTERNS = client_patterns("/account/password/email/requestToken$")
     def __init__(self, hs):
-        super().__init__()
+        super(EmailPasswordRequestTokenRestServlet, self).__init__()
         self.hs = hs
         self.datastore = hs.get_datastore()
         self.config = hs.config
         self.identity_handler = hs.get_handlers().identity_handler
         if self.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:
             self.mailer = Mailer(
                 hs=self.hs,
                 app_name=self.config.email_app_name,
                 template_html=self.config.email_password_reset_template_html,
                 template_text=self.config.email_password_reset_template_text,
@@ -53,22 +49,26 @@
         body = parse_json_object_from_request(request)
         assert_params_in_dict(body, ["client_secret", "email", "send_attempt"])
         client_secret = body["client_secret"]
         assert_valid_client_secret(client_secret)
         try:
             email = canonicalise_email(body["email"])
         except ValueError as e:
             raise SynapseError(400, str(e))
         send_attempt = body["send_attempt"]
         next_link = body.get("next_link")  # Optional param
-        if next_link:
-            assert_valid_next_link(self.hs, next_link)
+        if not check_3pid_allowed(self.hs, "email", email):
+            raise SynapseError(
+                403,
+                "Your email domain is not authorized on this server",
+                Codes.THREEPID_DENIED,
+            )
         existing_user_id = await self.hs.get_datastore().get_user_id_by_threepid(
             "email", email
         )
         if existing_user_id is None:
             if self.config.request_token_inhibit_3pid_errors:
                 await self.hs.clock.sleep(random.randint(1, 10) / 10)
                 return 200, {"sid": random_string(16)}
             raise SynapseError(400, "Email not found", Codes.THREEPID_NOT_FOUND)
         if self.config.threepid_behaviour_email == ThreepidBehaviour.REMOTE:
             assert self.hs.config.account_threepid_delegate_email
@@ -82,24 +82,82 @@
         else:
             sid = await self.identity_handler.send_threepid_validation(
                 email,
                 client_secret,
                 send_attempt,
                 self.mailer.send_password_reset_mail,
                 next_link,
             )
             ret = {"sid": sid}
         return 200, ret
+class PasswordResetSubmitTokenServlet(RestServlet):
+    """Handles 3PID validation token submission"""
+    PATTERNS = client_patterns(
+        "/password_reset/(?P<medium>[^/]*)/submit_token$", releases=(), unstable=True
+    )
+    def __init__(self, hs):
+        """
+        Args:
+            hs (synapse.server.HomeServer): server
+        """
+        super(PasswordResetSubmitTokenServlet, self).__init__()
+        self.hs = hs
+        self.auth = hs.get_auth()
+        self.config = hs.config
+        self.clock = hs.get_clock()
+        self.store = hs.get_datastore()
+        if self.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:
+            self._failure_email_template = (
+                self.config.email_password_reset_template_failure_html
+            )
+    async def on_GET(self, request, medium):
+        if medium != "email":
+            raise SynapseError(
+                400, "This medium is currently not supported for password resets"
+            )
+        if self.config.threepid_behaviour_email == ThreepidBehaviour.OFF:
+            if self.config.local_threepid_handling_disabled_due_to_email_config:
+                logger.warning(
+                    "Password reset emails have been disabled due to lack of an email config"
+                )
+            raise SynapseError(
+                400, "Email-based password resets are disabled on this server"
+            )
+        sid = parse_string(request, "sid", required=True)
+        token = parse_string(request, "token", required=True)
+        client_secret = parse_string(request, "client_secret", required=True)
+        assert_valid_client_secret(client_secret)
+        try:
+            next_link = await self.store.validate_threepid_session(
+                sid, client_secret, token, self.clock.time_msec()
+            )
+            if next_link:
+                if next_link.startswith("file:///"):
+                    logger.warning(
+                        "Not redirecting to next_link as it is a local file: address"
+                    )
+                else:
+                    request.setResponseCode(302)
+                    request.setHeader("Location", next_link)
+                    finish_request(request)
+                    return None
+            html = self.config.email_password_reset_template_success_html_content
+            status_code = 200
+        except ThreepidValidationError as e:
+            status_code = e.code
+            template_vars = {"failure_reason": e.msg}
+            html = self._failure_email_template.render(**template_vars)
+        respond_with_html(request, status_code, html)
 class PasswordRestServlet(RestServlet):
     PATTERNS = client_patterns("/account/password$")
     def __init__(self, hs):
-        super().__init__()
+        super(PasswordRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.auth_handler = hs.get_auth_handler()
         self.datastore = self.hs.get_datastore()
         self.password_policy_handler = hs.get_password_policy_handler()
         self._set_password_handler = hs.get_set_password_handler()
     @interactive_auth_handler
     async def on_POST(self, request):
         body = parse_json_object_from_request(request)
         new_password = body.pop("new_password", None)
@@ -171,21 +229,21 @@
         logout_devices = params.get("logout_devices", True)
         await self._set_password_handler.set_password(
             user_id, password_hash, logout_devices, requester
         )
         return 200, {}
     def on_OPTIONS(self, _):
         return 200, {}
 class DeactivateAccountRestServlet(RestServlet):
     PATTERNS = client_patterns("/account/deactivate$")
     def __init__(self, hs):
-        super().__init__()
+        super(DeactivateAccountRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.auth_handler = hs.get_auth_handler()
         self._deactivate_account_handler = hs.get_deactivate_account_handler()
     @interactive_auth_handler
     async def on_POST(self, request):
         body = parse_json_object_from_request(request)
         erase = body.get("erase", False)
         if not isinstance(erase, bool):
             raise SynapseError(
@@ -210,21 +268,21 @@
             requester.user.to_string(), erase, id_server=body.get("id_server")
         )
         if result:
             id_server_unbind_result = "success"
         else:
             id_server_unbind_result = "no-support"
         return 200, {"id_server_unbind_result": id_server_unbind_result}
 class EmailThreepidRequestTokenRestServlet(RestServlet):
     PATTERNS = client_patterns("/account/3pid/email/requestToken$")
     def __init__(self, hs):
-        super().__init__()
+        super(EmailThreepidRequestTokenRestServlet, self).__init__()
         self.hs = hs
         self.config = hs.config
         self.identity_handler = hs.get_handlers().identity_handler
         self.store = self.hs.get_datastore()
         if self.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:
             self.mailer = Mailer(
                 hs=self.hs,
                 app_name=self.config.email_app_name,
                 template_html=self.config.email_add_threepid_template_html,
                 template_text=self.config.email_add_threepid_template_text,
@@ -247,22 +305,20 @@
         except ValueError as e:
             raise SynapseError(400, str(e))
         send_attempt = body["send_attempt"]
         next_link = body.get("next_link")  # Optional param
         if not check_3pid_allowed(self.hs, "email", email):
             raise SynapseError(
                 403,
                 "Your email domain is not authorized on this server",
                 Codes.THREEPID_DENIED,
             )
-        if next_link:
-            assert_valid_next_link(self.hs, next_link)
         existing_user_id = await self.store.get_user_id_by_threepid("email", email)
         if existing_user_id is not None:
             if self.config.request_token_inhibit_3pid_errors:
                 await self.hs.clock.sleep(random.randint(1, 10) / 10)
                 return 200, {"sid": random_string(16)}
             raise SynapseError(400, "Email is already in use", Codes.THREEPID_IN_USE)
         if self.config.threepid_behaviour_email == ThreepidBehaviour.REMOTE:
             assert self.hs.config.account_threepid_delegate_email
             ret = await self.identity_handler.requestEmailToken(
                 self.hs.config.account_threepid_delegate_email,
@@ -278,43 +334,41 @@
                 send_attempt,
                 self.mailer.send_add_threepid_mail,
                 next_link,
             )
             ret = {"sid": sid}
         return 200, ret
 class MsisdnThreepidRequestTokenRestServlet(RestServlet):
     PATTERNS = client_patterns("/account/3pid/msisdn/requestToken$")
     def __init__(self, hs):
         self.hs = hs
-        super().__init__()
+        super(MsisdnThreepidRequestTokenRestServlet, self).__init__()
         self.store = self.hs.get_datastore()
         self.identity_handler = hs.get_handlers().identity_handler
     async def on_POST(self, request):
         body = parse_json_object_from_request(request)
         assert_params_in_dict(
             body, ["client_secret", "country", "phone_number", "send_attempt"]
         )
         client_secret = body["client_secret"]
         assert_valid_client_secret(client_secret)
         country = body["country"]
         phone_number = body["phone_number"]
         send_attempt = body["send_attempt"]
         next_link = body.get("next_link")  # Optional param
         msisdn = phone_number_to_msisdn(country, phone_number)
         if not check_3pid_allowed(self.hs, "msisdn", msisdn):
             raise SynapseError(
                 403,
                 "Account phone numbers are not authorized on this server",
                 Codes.THREEPID_DENIED,
             )
-        if next_link:
-            assert_valid_next_link(self.hs, next_link)
         existing_user_id = await self.store.get_user_id_by_threepid("msisdn", msisdn)
         if existing_user_id is not None:
             if self.hs.config.request_token_inhibit_3pid_errors:
                 await self.hs.clock.sleep(random.randint(1, 10) / 10)
                 return 200, {"sid": random_string(16)}
             raise SynapseError(400, "MSISDN is already in use", Codes.THREEPID_IN_USE)
         if not self.hs.config.account_threepid_delegate_msisdn:
             logger.warning(
                 "No upstream msisdn account_threepid_delegate configured on the server to "
                 "handle this request"
@@ -367,24 +421,29 @@
             )
         sid = parse_string(request, "sid", required=True)
         token = parse_string(request, "token", required=True)
         client_secret = parse_string(request, "client_secret", required=True)
         assert_valid_client_secret(client_secret)
         try:
             next_link = await self.store.validate_threepid_session(
                 sid, client_secret, token, self.clock.time_msec()
             )
             if next_link:
-                request.setResponseCode(302)
-                request.setHeader("Location", next_link)
-                finish_request(request)
-                return None
+                if next_link.startswith("file:///"):
+                    logger.warning(
+                        "Not redirecting to next_link as it is a local file: address"
+                    )
+                else:
+                    request.setResponseCode(302)
+                    request.setHeader("Location", next_link)
+                    finish_request(request)
+                    return None
             html = self.config.email_add_threepid_template_success_html_content
             status_code = 200
         except ThreepidValidationError as e:
             status_code = e.code
             template_vars = {"failure_reason": e.msg}
             html = self._failure_email_template.render(**template_vars)
         respond_with_html(request, status_code, html)
 class AddThreepidMsisdnSubmitTokenServlet(RestServlet):
     """Handles 3PID validation token submission for adding a phone number to a user's
     account
@@ -415,21 +474,21 @@
         response = await self.identity_handler.proxy_msisdn_submit_token(
             self.config.account_threepid_delegate_msisdn,
             body["client_secret"],
             body["sid"],
             body["token"],
         )
         return 200, response
 class ThreepidRestServlet(RestServlet):
     PATTERNS = client_patterns("/account/3pid$")
     def __init__(self, hs):
-        super().__init__()
+        super(ThreepidRestServlet, self).__init__()
         self.hs = hs
         self.identity_handler = hs.get_handlers().identity_handler
         self.auth = hs.get_auth()
         self.auth_handler = hs.get_auth_handler()
         self.datastore = self.hs.get_datastore()
     async def on_GET(self, request):
         requester = await self.auth.get_user_by_req(request)
         threepids = await self.datastore.user_get_threepids(requester.user.to_string())
         return 200, {"threepids": threepids}
     async def on_POST(self, request):
@@ -459,21 +518,21 @@
                 validation_session["address"],
                 validation_session["validated_at"],
             )
             return 200, {}
         raise SynapseError(
             400, "No validated 3pid session found", Codes.THREEPID_AUTH_FAILED
         )
 class ThreepidAddRestServlet(RestServlet):
     PATTERNS = client_patterns("/account/3pid/add$")
     def __init__(self, hs):
-        super().__init__()
+        super(ThreepidAddRestServlet, self).__init__()
         self.hs = hs
         self.identity_handler = hs.get_handlers().identity_handler
         self.auth = hs.get_auth()
         self.auth_handler = hs.get_auth_handler()
     @interactive_auth_handler
     async def on_POST(self, request):
         if not self.hs.config.enable_3pid_changes:
             raise SynapseError(
                 400, "3PID changes are disabled on this server", Codes.FORBIDDEN
             )
@@ -501,42 +560,42 @@
                 validation_session["address"],
                 validation_session["validated_at"],
             )
             return 200, {}
         raise SynapseError(
             400, "No validated 3pid session found", Codes.THREEPID_AUTH_FAILED
         )
 class ThreepidBindRestServlet(RestServlet):
     PATTERNS = client_patterns("/account/3pid/bind$")
     def __init__(self, hs):
-        super().__init__()
+        super(ThreepidBindRestServlet, self).__init__()
         self.hs = hs
         self.identity_handler = hs.get_handlers().identity_handler
         self.auth = hs.get_auth()
     async def on_POST(self, request):
         body = parse_json_object_from_request(request)
         assert_params_in_dict(body, ["id_server", "sid", "client_secret"])
         id_server = body["id_server"]
         sid = body["sid"]
         id_access_token = body.get("id_access_token")  # optional
         client_secret = body["client_secret"]
         assert_valid_client_secret(client_secret)
         requester = await self.auth.get_user_by_req(request)
         user_id = requester.user.to_string()
         await self.identity_handler.bind_threepid(
             client_secret, sid, user_id, id_server, id_access_token
         )
         return 200, {}
 class ThreepidUnbindRestServlet(RestServlet):
     PATTERNS = client_patterns("/account/3pid/unbind$")
     def __init__(self, hs):
-        super().__init__()
+        super(ThreepidUnbindRestServlet, self).__init__()
         self.hs = hs
         self.identity_handler = hs.get_handlers().identity_handler
         self.auth = hs.get_auth()
         self.datastore = self.hs.get_datastore()
     async def on_POST(self, request):
         """Unbind the given 3pid from a specific identity server, or identity servers that are
         known to have this 3pid bound
         """
         requester = await self.auth.get_user_by_req(request)
         body = parse_json_object_from_request(request)
@@ -545,21 +604,21 @@
         address = body.get("address")
         id_server = body.get("id_server")
         result = await self.identity_handler.try_unbind_threepid(
             requester.user.to_string(),
             {"address": address, "medium": medium, "id_server": id_server},
         )
         return 200, {"id_server_unbind_result": "success" if result else "no-support"}
 class ThreepidDeleteRestServlet(RestServlet):
     PATTERNS = client_patterns("/account/3pid/delete$")
     def __init__(self, hs):
-        super().__init__()
+        super(ThreepidDeleteRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.auth_handler = hs.get_auth_handler()
     async def on_POST(self, request):
         if not self.hs.config.enable_3pid_changes:
             raise SynapseError(
                 400, "3PID changes are disabled on this server", Codes.FORBIDDEN
             )
         body = parse_json_object_from_request(request)
         assert_params_in_dict(body, ["medium", "address"])
@@ -570,57 +629,31 @@
                 user_id, body["medium"], body["address"], body.get("id_server")
             )
         except Exception:
             logger.exception("Failed to remove threepid")
             raise SynapseError(500, "Failed to remove threepid")
         if ret:
             id_server_unbind_result = "success"
         else:
             id_server_unbind_result = "no-support"
         return 200, {"id_server_unbind_result": id_server_unbind_result}
-def assert_valid_next_link(hs: "HomeServer", next_link: str):
-    """
-    Raises a SynapseError if a given next_link value is invalid
-    next_link is valid if the scheme is http(s) and the next_link.domain_whitelist config
-    option is either empty or contains a domain that matches the one in the given next_link
-    Args:
-        hs: The homeserver object
-        next_link: The next_link value given by the client
-    Raises:
-        SynapseError: If the next_link is invalid
-    """
-    valid = True
-    next_link_parsed = urlparse(next_link)
-    if next_link_parsed.scheme == "file":
-        valid = False
-    if (
-        valid
-        and hs.config.next_link_domain_whitelist is not None
-        and next_link_parsed.hostname not in hs.config.next_link_domain_whitelist
-    ):
-        valid = False
-    if not valid:
-        raise SynapseError(
-            400,
-            "'next_link' domain not included in whitelist, or not http(s)",
-            errcode=Codes.INVALID_PARAM,
-        )
 class WhoamiRestServlet(RestServlet):
     PATTERNS = client_patterns("/account/whoami$")
     def __init__(self, hs):
-        super().__init__()
+        super(WhoamiRestServlet, self).__init__()
         self.auth = hs.get_auth()
     async def on_GET(self, request):
         requester = await self.auth.get_user_by_req(request)
         return 200, {"user_id": requester.user.to_string()}
 def register_servlets(hs, http_server):
     EmailPasswordRequestTokenRestServlet(hs).register(http_server)
+    PasswordResetSubmitTokenServlet(hs).register(http_server)
     PasswordRestServlet(hs).register(http_server)
     DeactivateAccountRestServlet(hs).register(http_server)
     EmailThreepidRequestTokenRestServlet(hs).register(http_server)
     MsisdnThreepidRequestTokenRestServlet(hs).register(http_server)
     AddThreepidEmailSubmitTokenServlet(hs).register(http_server)
     AddThreepidMsisdnSubmitTokenServlet(hs).register(http_server)
     ThreepidRestServlet(hs).register(http_server)
     ThreepidAddRestServlet(hs).register(http_server)
     ThreepidBindRestServlet(hs).register(http_server)
     ThreepidUnbindRestServlet(hs).register(http_server)

--- a/synapse/rest/client/v2_alpha/account_data.py
+++ b/synapse/rest/client/v2_alpha/account_data.py
@@ -5,21 +5,21 @@
 logger = logging.getLogger(__name__)
 class AccountDataServlet(RestServlet):
     """
     PUT /user/{user_id}/account_data/{account_dataType} HTTP/1.1
     GET /user/{user_id}/account_data/{account_dataType} HTTP/1.1
     """
     PATTERNS = client_patterns(
         "/user/(?P<user_id>[^/]*)/account_data/(?P<account_data_type>[^/]*)"
     )
     def __init__(self, hs):
-        super().__init__()
+        super(AccountDataServlet, self).__init__()
         self.auth = hs.get_auth()
         self.store = hs.get_datastore()
         self.notifier = hs.get_notifier()
         self._is_worker = hs.config.worker_app is not None
     async def on_PUT(self, request, user_id, account_data_type):
         if self._is_worker:
             raise Exception("Cannot handle PUT /account_data on worker")
         requester = await self.auth.get_user_by_req(request)
         if user_id != requester.user.to_string():
             raise AuthError(403, "Cannot add account data for other users.")
@@ -43,21 +43,21 @@
     """
     PUT /user/{user_id}/rooms/{room_id}/account_data/{account_dataType} HTTP/1.1
     GET /user/{user_id}/rooms/{room_id}/account_data/{account_dataType} HTTP/1.1
     """
     PATTERNS = client_patterns(
         "/user/(?P<user_id>[^/]*)"
         "/rooms/(?P<room_id>[^/]*)"
         "/account_data/(?P<account_data_type>[^/]*)"
     )
     def __init__(self, hs):
-        super().__init__()
+        super(RoomAccountDataServlet, self).__init__()
         self.auth = hs.get_auth()
         self.store = hs.get_datastore()
         self.notifier = hs.get_notifier()
         self._is_worker = hs.config.worker_app is not None
     async def on_PUT(self, request, user_id, room_id, account_data_type):
         if self._is_worker:
             raise Exception("Cannot handle PUT /account_data on worker")
         requester = await self.auth.get_user_by_req(request)
         if user_id != requester.user.to_string():
             raise AuthError(403, "Cannot add account data for other users.")

--- a/synapse/rest/client/v2_alpha/account_validity.py
+++ b/synapse/rest/client/v2_alpha/account_validity.py
@@ -4,21 +4,21 @@
 from synapse.http.servlet import RestServlet
 from ._base import client_patterns
 logger = logging.getLogger(__name__)
 class AccountValidityRenewServlet(RestServlet):
     PATTERNS = client_patterns("/account_validity/renew$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(AccountValidityRenewServlet, self).__init__()
         self.hs = hs
         self.account_activity_handler = hs.get_account_validity_handler()
         self.auth = hs.get_auth()
         self.success_html = hs.config.account_validity.account_renewed_html_content
         self.failure_html = hs.config.account_validity.invalid_token_html_content
     async def on_GET(self, request):
         if b"token" not in request.args:
             raise SynapseError(400, "Missing renewal token")
         renewal_token = request.args[b"token"][0]
         token_valid = await self.account_activity_handler.renew_account(
@@ -31,21 +31,21 @@
             status_code = 404
             response = self.failure_html
         respond_with_html(request, status_code, response)
 class AccountValiditySendMailServlet(RestServlet):
     PATTERNS = client_patterns("/account_validity/send_mail$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(AccountValiditySendMailServlet, self).__init__()
         self.hs = hs
         self.account_activity_handler = hs.get_account_validity_handler()
         self.auth = hs.get_auth()
         self.account_validity = self.hs.config.account_validity
     async def on_POST(self, request):
         if not self.account_validity.renew_by_email_enabled:
             raise AuthError(
                 403, "Account renewal via email is disabled on this server."
             )
         requester = await self.auth.get_user_by_req(request, allow_expired=True)

--- a/synapse/rest/client/v2_alpha/auth.py
+++ b/synapse/rest/client/v2_alpha/auth.py
@@ -1,66 +1,148 @@
 import logging
 from synapse.api.constants import LoginType
 from synapse.api.errors import SynapseError
 from synapse.api.urls import CLIENT_API_PREFIX
 from synapse.http.server import respond_with_html
 from synapse.http.servlet import RestServlet, parse_string
 from ._base import client_patterns
 logger = logging.getLogger(__name__)
+RECAPTCHA_TEMPLATE = """
+<html>
+<head>
+<title>Authentication</title>
+<meta name='viewport' content='width=device-width, initial-scale=1,
+    user-scalable=no, minimum-scale=1.0, maximum-scale=1.0'>
+<script src="https://www.recaptcha.net/recaptcha/api.js"
+    async defer></script>
+<script src="//code.jquery.com/jquery-1.11.2.min.js"></script>
+<link rel="stylesheet" href="/_matrix/static/client/register/style.css">
+<script>
+function captchaDone() {
+    $('#registrationForm').submit();
+}
+</script>
+</head>
+<body>
+<form id="registrationForm" method="post" action="%(myurl)s">
+    <div>
+        <p>
+        Hello! We need to prevent computer programs and other automated
+        things from creating accounts on this server.
+        </p>
+        <p>
+        Please verify that you're not a robot.
+        </p>
+        <input type="hidden" name="session" value="%(session)s" />
+        <div class="g-recaptcha"
+            data-sitekey="%(sitekey)s"
+            data-callback="captchaDone">
+        </div>
+        <noscript>
+        <input type="submit" value="All Done" />
+        </noscript>
+        </div>
+    </div>
+</form>
+</body>
+</html>
+"""
+TERMS_TEMPLATE = """
+<html>
+<head>
+<title>Authentication</title>
+<meta name='viewport' content='width=device-width, initial-scale=1,
+    user-scalable=no, minimum-scale=1.0, maximum-scale=1.0'>
+<link rel="stylesheet" href="/_matrix/static/client/register/style.css">
+</head>
+<body>
+<form id="registrationForm" method="post" action="%(myurl)s">
+    <div>
+        <p>
+            Please click the button below if you agree to the
+            <a href="%(terms_url)s">privacy policy of this homeserver.</a>
+        </p>
+        <input type="hidden" name="session" value="%(session)s" />
+        <input type="submit" value="Agree" />
+    </div>
+</form>
+</body>
+</html>
+"""
+SUCCESS_TEMPLATE = """
+<html>
+<head>
+<title>Success!</title>
+<meta name='viewport' content='width=device-width, initial-scale=1,
+    user-scalable=no, minimum-scale=1.0, maximum-scale=1.0'>
+<link rel="stylesheet" href="/_matrix/static/client/register/style.css">
+<script>
+if (window.onAuthDone) {
+    window.onAuthDone();
+} else if (window.opener && window.opener.postMessage) {
+     window.opener.postMessage("authDone", "*");
+}
+</script>
+</head>
+<body>
+    <div>
+        <p>Thank you</p>
+        <p>You may now close this window and return to the application</p>
+    </div>
+</body>
+</html>
+"""
 class AuthRestServlet(RestServlet):
     """
     Handles Client / Server API authentication in any situations where it
     cannot be handled in the normal flow (with requests to the same endpoint).
     Current use is for web fallback auth.
     """
     PATTERNS = client_patterns(r"/auth/(?P<stagetype>[\w\.]*)/fallback/web")
     def __init__(self, hs):
-        super().__init__()
+        super(AuthRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.auth_handler = hs.get_auth_handler()
         self.registration_handler = hs.get_registration_handler()
         self._cas_enabled = hs.config.cas_enabled
         if self._cas_enabled:
             self._cas_handler = hs.get_cas_handler()
             self._cas_server_url = hs.config.cas_server_url
             self._cas_service_url = hs.config.cas_service_url
         self._saml_enabled = hs.config.saml2_enabled
         if self._saml_enabled:
             self._saml_handler = hs.get_saml_handler()
         self._oidc_enabled = hs.config.oidc_enabled
         if self._oidc_enabled:
             self._oidc_handler = hs.get_oidc_handler()
             self._cas_server_url = hs.config.cas_server_url
             self._cas_service_url = hs.config.cas_service_url
-        self.recaptcha_template = hs.config.recaptcha_template
-        self.terms_template = hs.config.terms_template
-        self.success_template = hs.config.fallback_success_template
     async def on_GET(self, request, stagetype):
         session = parse_string(request, "session")
         if not session:
             raise SynapseError(400, "No session supplied")
         if stagetype == LoginType.RECAPTCHA:
-            html = self.recaptcha_template.render(
-                session=session,
-                myurl="%s/r0/auth/%s/fallback/web"
+            html = RECAPTCHA_TEMPLATE % {
+                "session": session,
+                "myurl": "%s/r0/auth/%s/fallback/web"
                 % (CLIENT_API_PREFIX, LoginType.RECAPTCHA),
-                sitekey=self.hs.config.recaptcha_public_key,
-            )
+                "sitekey": self.hs.config.recaptcha_public_key,
+            }
         elif stagetype == LoginType.TERMS:
-            html = self.terms_template.render(
-                session=session,
-                terms_url="%s_matrix/consent?v=%s"
+            html = TERMS_TEMPLATE % {
+                "session": session,
+                "terms_url": "%s_matrix/consent?v=%s"
                 % (self.hs.config.public_baseurl, self.hs.config.user_consent_version),
-                myurl="%s/r0/auth/%s/fallback/web"
+                "myurl": "%s/r0/auth/%s/fallback/web"
                 % (CLIENT_API_PREFIX, LoginType.TERMS),
-            )
+            }
         elif stagetype == LoginType.SSO:
             if self._cas_enabled:
                 sso_redirect_url = self._cas_handler.get_redirect_url(
                     {"session": session},
                 )
             elif self._saml_enabled:
                 client_redirect_url = b"unused"
                 sso_redirect_url = self._saml_handler.handle_redirect_request(
                     client_redirect_url, session
                 )
@@ -82,46 +164,46 @@
             raise SynapseError(400, "No session supplied")
         if stagetype == LoginType.RECAPTCHA:
             response = parse_string(request, "g-recaptcha-response")
             if not response:
                 raise SynapseError(400, "No captcha response supplied")
             authdict = {"response": response, "session": session}
             success = await self.auth_handler.add_oob_auth(
                 LoginType.RECAPTCHA, authdict, self.hs.get_ip_from_request(request)
             )
             if success:
-                html = self.success_template.render()
+                html = SUCCESS_TEMPLATE
             else:
-                html = self.recaptcha_template.render(
-                    session=session,
-                    myurl="%s/r0/auth/%s/fallback/web"
+                html = RECAPTCHA_TEMPLATE % {
+                    "session": session,
+                    "myurl": "%s/r0/auth/%s/fallback/web"
                     % (CLIENT_API_PREFIX, LoginType.RECAPTCHA),
-                    sitekey=self.hs.config.recaptcha_public_key,
-                )
+                    "sitekey": self.hs.config.recaptcha_public_key,
+                }
         elif stagetype == LoginType.TERMS:
             authdict = {"session": session}
             success = await self.auth_handler.add_oob_auth(
                 LoginType.TERMS, authdict, self.hs.get_ip_from_request(request)
             )
             if success:
-                html = self.success_template.render()
+                html = SUCCESS_TEMPLATE
             else:
-                html = self.terms_template.render(
-                    session=session,
-                    terms_url="%s_matrix/consent?v=%s"
+                html = TERMS_TEMPLATE % {
+                    "session": session,
+                    "terms_url": "%s_matrix/consent?v=%s"
                     % (
                         self.hs.config.public_baseurl,
                         self.hs.config.user_consent_version,
                     ),
-                    myurl="%s/r0/auth/%s/fallback/web"
+                    "myurl": "%s/r0/auth/%s/fallback/web"
                     % (CLIENT_API_PREFIX, LoginType.TERMS),
-                )
+                }
         elif stagetype == LoginType.SSO:
             raise SynapseError(404, "Fallback SSO auth does not support POST requests.")
         else:
             raise SynapseError(404, "Unknown auth stage type")
         respond_with_html(request, 200, html)
         return None
     def on_OPTIONS(self, _):
         return 200, {}
 def register_servlets(hs, http_server):
     AuthRestServlet(hs).register(http_server)

--- a/synapse/rest/client/v2_alpha/capabilities.py
+++ b/synapse/rest/client/v2_alpha/capabilities.py
@@ -4,21 +4,21 @@
 from ._base import client_patterns
 logger = logging.getLogger(__name__)
 class CapabilitiesRestServlet(RestServlet):
     """End point to expose the capabilities of the server."""
     PATTERNS = client_patterns("/capabilities$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(CapabilitiesRestServlet, self).__init__()
         self.hs = hs
         self.config = hs.config
         self.auth = hs.get_auth()
         self.store = hs.get_datastore()
     async def on_GET(self, request):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         user = await self.store.get_user_by_id(requester.user.to_string())
         change_password = bool(user["password_hash"])
         response = {
             "capabilities": {

--- a/synapse/rest/client/v2_alpha/devices.py
+++ b/synapse/rest/client/v2_alpha/devices.py
@@ -7,38 +7,38 @@
 )
 from ._base import client_patterns, interactive_auth_handler
 logger = logging.getLogger(__name__)
 class DevicesRestServlet(RestServlet):
     PATTERNS = client_patterns("/devices$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(DevicesRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.device_handler = hs.get_device_handler()
     async def on_GET(self, request):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         devices = await self.device_handler.get_devices_by_user(
             requester.user.to_string()
         )
         return 200, {"devices": devices}
 class DeleteDevicesRestServlet(RestServlet):
     """
     API for bulk deletion of devices. Accepts a JSON object with a devices
     key which lists the device_ids to delete. Requires user interactive auth.
     """
     PATTERNS = client_patterns("/delete_devices")
     def __init__(self, hs):
-        super().__init__()
+        super(DeleteDevicesRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.device_handler = hs.get_device_handler()
         self.auth_handler = hs.get_auth_handler()
     @interactive_auth_handler
     async def on_POST(self, request):
         requester = await self.auth.get_user_by_req(request)
         try:
             body = parse_json_object_from_request(request)
         except errors.SynapseError as e:
@@ -58,21 +58,21 @@
             requester.user.to_string(), body["devices"]
         )
         return 200, {}
 class DeviceRestServlet(RestServlet):
     PATTERNS = client_patterns("/devices/(?P<device_id>[^/]*)$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(DeviceRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.device_handler = hs.get_device_handler()
         self.auth_handler = hs.get_auth_handler()
     async def on_GET(self, request, device_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         device = await self.device_handler.get_device(
             requester.user.to_string(), device_id
         )
         return 200, device

--- a/synapse/rest/client/v2_alpha/filter.py
+++ b/synapse/rest/client/v2_alpha/filter.py
@@ -1,20 +1,20 @@
 import logging
 from synapse.api.errors import AuthError, NotFoundError, StoreError, SynapseError
 from synapse.http.servlet import RestServlet, parse_json_object_from_request
 from synapse.types import UserID
 from ._base import client_patterns, set_timeline_upper_limit
 logger = logging.getLogger(__name__)
 class GetFilterRestServlet(RestServlet):
     PATTERNS = client_patterns("/user/(?P<user_id>[^/]*)/filter/(?P<filter_id>[^/]*)")
     def __init__(self, hs):
-        super().__init__()
+        super(GetFilterRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.filtering = hs.get_filtering()
     async def on_GET(self, request, user_id, filter_id):
         target_user = UserID.from_string(user_id)
         requester = await self.auth.get_user_by_req(request)
         if target_user != requester.user:
             raise AuthError(403, "Cannot get filters for other users")
         if not self.hs.is_mine(target_user):
             raise AuthError(403, "Can only get filters for local users")
@@ -27,21 +27,21 @@
                 user_localpart=target_user.localpart, filter_id=filter_id
             )
         except StoreError as e:
             if e.code != 404:
                 raise
             raise NotFoundError("No such filter")
         return 200, filter_collection.get_filter_json()
 class CreateFilterRestServlet(RestServlet):
     PATTERNS = client_patterns("/user/(?P<user_id>[^/]*)/filter")
     def __init__(self, hs):
-        super().__init__()
+        super(CreateFilterRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.filtering = hs.get_filtering()
     async def on_POST(self, request, user_id):
         target_user = UserID.from_string(user_id)
         requester = await self.auth.get_user_by_req(request)
         if target_user != requester.user:
             raise AuthError(403, "Cannot create filters for other users")
         if not self.hs.is_mine(target_user):
             raise AuthError(403, "Can only create filters for local users")

--- a/synapse/rest/client/v2_alpha/groups.py
+++ b/synapse/rest/client/v2_alpha/groups.py
@@ -2,21 +2,21 @@
 from synapse.api.errors import SynapseError
 from synapse.http.servlet import RestServlet, parse_json_object_from_request
 from synapse.types import GroupID
 from ._base import client_patterns
 logger = logging.getLogger(__name__)
 class GroupServlet(RestServlet):
     """Get the group profile
     """
     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/profile$")
     def __init__(self, hs):
-        super().__init__()
+        super(GroupServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_GET(self, request, group_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         requester_user_id = requester.user.to_string()
         group_description = await self.groups_handler.get_group_profile(
             group_id, requester_user_id
         )
         return 200, group_description
@@ -26,21 +26,21 @@
         content = parse_json_object_from_request(request)
         await self.groups_handler.update_group_profile(
             group_id, requester_user_id, content
         )
         return 200, {}
 class GroupSummaryServlet(RestServlet):
     """Get the full group summary
     """
     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/summary$")
     def __init__(self, hs):
-        super().__init__()
+        super(GroupSummaryServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_GET(self, request, group_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         requester_user_id = requester.user.to_string()
         get_group_summary = await self.groups_handler.get_group_summary(
             group_id, requester_user_id
         )
         return 200, get_group_summary
@@ -49,21 +49,21 @@
     Matches both:
         - /groups/:group/summary/rooms/:room_id
         - /groups/:group/summary/categories/:category/rooms/:room_id
     """
     PATTERNS = client_patterns(
         "/groups/(?P<group_id>[^/]*)/summary"
         "(/categories/(?P<category_id>[^/]+))?"
         "/rooms/(?P<room_id>[^/]*)$"
     )
     def __init__(self, hs):
-        super().__init__()
+        super(GroupSummaryRoomsCatServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_PUT(self, request, group_id, category_id, room_id):
         requester = await self.auth.get_user_by_req(request)
         requester_user_id = requester.user.to_string()
         content = parse_json_object_from_request(request)
         resp = await self.groups_handler.update_group_summary_room(
             group_id,
             requester_user_id,
@@ -79,21 +79,21 @@
             group_id, requester_user_id, room_id=room_id, category_id=category_id
         )
         return 200, resp
 class GroupCategoryServlet(RestServlet):
     """Get/add/update/delete a group category
     """
     PATTERNS = client_patterns(
         "/groups/(?P<group_id>[^/]*)/categories/(?P<category_id>[^/]+)$"
     )
     def __init__(self, hs):
-        super().__init__()
+        super(GroupCategoryServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_GET(self, request, group_id, category_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         requester_user_id = requester.user.to_string()
         category = await self.groups_handler.get_group_category(
             group_id, requester_user_id, category_id=category_id
         )
         return 200, category
@@ -110,37 +110,37 @@
         requester_user_id = requester.user.to_string()
         resp = await self.groups_handler.delete_group_category(
             group_id, requester_user_id, category_id=category_id
         )
         return 200, resp
 class GroupCategoriesServlet(RestServlet):
     """Get all group categories
     """
     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/categories/$")
     def __init__(self, hs):
-        super().__init__()
+        super(GroupCategoriesServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_GET(self, request, group_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         requester_user_id = requester.user.to_string()
         category = await self.groups_handler.get_group_categories(
             group_id, requester_user_id
         )
         return 200, category
 class GroupRoleServlet(RestServlet):
     """Get/add/update/delete a group role
     """
     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/roles/(?P<role_id>[^/]+)$")
     def __init__(self, hs):
-        super().__init__()
+        super(GroupRoleServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_GET(self, request, group_id, role_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         requester_user_id = requester.user.to_string()
         category = await self.groups_handler.get_group_role(
             group_id, requester_user_id, role_id=role_id
         )
         return 200, category
@@ -157,21 +157,21 @@
         requester_user_id = requester.user.to_string()
         resp = await self.groups_handler.delete_group_role(
             group_id, requester_user_id, role_id=role_id
         )
         return 200, resp
 class GroupRolesServlet(RestServlet):
     """Get all group roles
     """
     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/roles/$")
     def __init__(self, hs):
-        super().__init__()
+        super(GroupRolesServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_GET(self, request, group_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         requester_user_id = requester.user.to_string()
         category = await self.groups_handler.get_group_roles(
             group_id, requester_user_id
         )
         return 200, category
@@ -180,21 +180,21 @@
     Matches both:
         - /groups/:group/summary/users/:room_id
         - /groups/:group/summary/roles/:role/users/:user_id
     """
     PATTERNS = client_patterns(
         "/groups/(?P<group_id>[^/]*)/summary"
         "(/roles/(?P<role_id>[^/]+))?"
         "/users/(?P<user_id>[^/]*)$"
     )
     def __init__(self, hs):
-        super().__init__()
+        super(GroupSummaryUsersRoleServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_PUT(self, request, group_id, role_id, user_id):
         requester = await self.auth.get_user_by_req(request)
         requester_user_id = requester.user.to_string()
         content = parse_json_object_from_request(request)
         resp = await self.groups_handler.update_group_summary_user(
             group_id,
             requester_user_id,
@@ -208,87 +208,87 @@
         requester_user_id = requester.user.to_string()
         resp = await self.groups_handler.delete_group_summary_user(
             group_id, requester_user_id, user_id=user_id, role_id=role_id
         )
         return 200, resp
 class GroupRoomServlet(RestServlet):
     """Get all rooms in a group
     """
     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/rooms$")
     def __init__(self, hs):
-        super().__init__()
+        super(GroupRoomServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_GET(self, request, group_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         requester_user_id = requester.user.to_string()
         if not GroupID.is_valid(group_id):
             raise SynapseError(400, "%s was not legal group ID" % (group_id,))
         result = await self.groups_handler.get_rooms_in_group(
             group_id, requester_user_id
         )
         return 200, result
 class GroupUsersServlet(RestServlet):
     """Get all users in a group
     """
     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/users$")
     def __init__(self, hs):
-        super().__init__()
+        super(GroupUsersServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_GET(self, request, group_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         requester_user_id = requester.user.to_string()
         result = await self.groups_handler.get_users_in_group(
             group_id, requester_user_id
         )
         return 200, result
 class GroupInvitedUsersServlet(RestServlet):
     """Get users invited to a group
     """
     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/invited_users$")
     def __init__(self, hs):
-        super().__init__()
+        super(GroupInvitedUsersServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_GET(self, request, group_id):
         requester = await self.auth.get_user_by_req(request)
         requester_user_id = requester.user.to_string()
         result = await self.groups_handler.get_invited_users_in_group(
             group_id, requester_user_id
         )
         return 200, result
 class GroupSettingJoinPolicyServlet(RestServlet):
     """Set group join policy
     """
     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/settings/m.join_policy$")
     def __init__(self, hs):
-        super().__init__()
+        super(GroupSettingJoinPolicyServlet, self).__init__()
         self.auth = hs.get_auth()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_PUT(self, request, group_id):
         requester = await self.auth.get_user_by_req(request)
         requester_user_id = requester.user.to_string()
         content = parse_json_object_from_request(request)
         result = await self.groups_handler.set_group_join_policy(
             group_id, requester_user_id, content
         )
         return 200, result
 class GroupCreateServlet(RestServlet):
     """Create a group
     """
     PATTERNS = client_patterns("/create_group$")
     def __init__(self, hs):
-        super().__init__()
+        super(GroupCreateServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
         self.server_name = hs.hostname
     async def on_POST(self, request):
         requester = await self.auth.get_user_by_req(request)
         requester_user_id = requester.user.to_string()
         content = parse_json_object_from_request(request)
         localpart = content.pop("localpart")
         group_id = GroupID(localpart, self.server_name).to_string()
@@ -296,21 +296,21 @@
             group_id, requester_user_id, content
         )
         return 200, result
 class GroupAdminRoomsServlet(RestServlet):
     """Add a room to the group
     """
     PATTERNS = client_patterns(
         "/groups/(?P<group_id>[^/]*)/admin/rooms/(?P<room_id>[^/]*)$"
     )
     def __init__(self, hs):
-        super().__init__()
+        super(GroupAdminRoomsServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_PUT(self, request, group_id, room_id):
         requester = await self.auth.get_user_by_req(request)
         requester_user_id = requester.user.to_string()
         content = parse_json_object_from_request(request)
         result = await self.groups_handler.add_room_to_group(
             group_id, requester_user_id, room_id, content
         )
@@ -323,40 +323,40 @@
         )
         return 200, result
 class GroupAdminRoomsConfigServlet(RestServlet):
     """Update the config of a room in a group
     """
     PATTERNS = client_patterns(
         "/groups/(?P<group_id>[^/]*)/admin/rooms/(?P<room_id>[^/]*)"
         "/config/(?P<config_key>[^/]*)$"
     )
     def __init__(self, hs):
-        super().__init__()
+        super(GroupAdminRoomsConfigServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_PUT(self, request, group_id, room_id, config_key):
         requester = await self.auth.get_user_by_req(request)
         requester_user_id = requester.user.to_string()
         content = parse_json_object_from_request(request)
         result = await self.groups_handler.update_room_in_group(
             group_id, requester_user_id, room_id, config_key, content
         )
         return 200, result
 class GroupAdminUsersInviteServlet(RestServlet):
     """Invite a user to the group
     """
     PATTERNS = client_patterns(
         "/groups/(?P<group_id>[^/]*)/admin/users/invite/(?P<user_id>[^/]*)$"
     )
     def __init__(self, hs):
-        super().__init__()
+        super(GroupAdminUsersInviteServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
         self.store = hs.get_datastore()
         self.is_mine_id = hs.is_mine_id
     async def on_PUT(self, request, group_id, user_id):
         requester = await self.auth.get_user_by_req(request)
         requester_user_id = requester.user.to_string()
         content = parse_json_object_from_request(request)
         config = content.get("config", {})
@@ -364,135 +364,135 @@
             group_id, user_id, requester_user_id, config
         )
         return 200, result
 class GroupAdminUsersKickServlet(RestServlet):
     """Kick a user from the group
     """
     PATTERNS = client_patterns(
         "/groups/(?P<group_id>[^/]*)/admin/users/remove/(?P<user_id>[^/]*)$"
     )
     def __init__(self, hs):
-        super().__init__()
+        super(GroupAdminUsersKickServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_PUT(self, request, group_id, user_id):
         requester = await self.auth.get_user_by_req(request)
         requester_user_id = requester.user.to_string()
         content = parse_json_object_from_request(request)
         result = await self.groups_handler.remove_user_from_group(
             group_id, user_id, requester_user_id, content
         )
         return 200, result
 class GroupSelfLeaveServlet(RestServlet):
     """Leave a joined group
     """
     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/self/leave$")
     def __init__(self, hs):
-        super().__init__()
+        super(GroupSelfLeaveServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_PUT(self, request, group_id):
         requester = await self.auth.get_user_by_req(request)
         requester_user_id = requester.user.to_string()
         content = parse_json_object_from_request(request)
         result = await self.groups_handler.remove_user_from_group(
             group_id, requester_user_id, requester_user_id, content
         )
         return 200, result
 class GroupSelfJoinServlet(RestServlet):
     """Attempt to join a group, or knock
     """
     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/self/join$")
     def __init__(self, hs):
-        super().__init__()
+        super(GroupSelfJoinServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_PUT(self, request, group_id):
         requester = await self.auth.get_user_by_req(request)
         requester_user_id = requester.user.to_string()
         content = parse_json_object_from_request(request)
         result = await self.groups_handler.join_group(
             group_id, requester_user_id, content
         )
         return 200, result
 class GroupSelfAcceptInviteServlet(RestServlet):
     """Accept a group invite
     """
     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/self/accept_invite$")
     def __init__(self, hs):
-        super().__init__()
+        super(GroupSelfAcceptInviteServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_PUT(self, request, group_id):
         requester = await self.auth.get_user_by_req(request)
         requester_user_id = requester.user.to_string()
         content = parse_json_object_from_request(request)
         result = await self.groups_handler.accept_invite(
             group_id, requester_user_id, content
         )
         return 200, result
 class GroupSelfUpdatePublicityServlet(RestServlet):
     """Update whether we publicise a users membership of a group
     """
     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/self/update_publicity$")
     def __init__(self, hs):
-        super().__init__()
+        super(GroupSelfUpdatePublicityServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.store = hs.get_datastore()
     async def on_PUT(self, request, group_id):
         requester = await self.auth.get_user_by_req(request)
         requester_user_id = requester.user.to_string()
         content = parse_json_object_from_request(request)
         publicise = content["publicise"]
         await self.store.update_group_publicity(group_id, requester_user_id, publicise)
         return 200, {}
 class PublicisedGroupsForUserServlet(RestServlet):
     """Get the list of groups a user is advertising
     """
     PATTERNS = client_patterns("/publicised_groups/(?P<user_id>[^/]*)$")
     def __init__(self, hs):
-        super().__init__()
+        super(PublicisedGroupsForUserServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.store = hs.get_datastore()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_GET(self, request, user_id):
         await self.auth.get_user_by_req(request, allow_guest=True)
         result = await self.groups_handler.get_publicised_groups_for_user(user_id)
         return 200, result
 class PublicisedGroupsForUsersServlet(RestServlet):
     """Get the list of groups a user is advertising
     """
     PATTERNS = client_patterns("/publicised_groups$")
     def __init__(self, hs):
-        super().__init__()
+        super(PublicisedGroupsForUsersServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.store = hs.get_datastore()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_POST(self, request):
         await self.auth.get_user_by_req(request, allow_guest=True)
         content = parse_json_object_from_request(request)
         user_ids = content["user_ids"]
         result = await self.groups_handler.bulk_get_publicised_groups(user_ids)
         return 200, result
 class GroupsForUserServlet(RestServlet):
     """Get all groups the logged in user is joined to
     """
     PATTERNS = client_patterns("/joined_groups$")
     def __init__(self, hs):
-        super().__init__()
+        super(GroupsForUserServlet, self).__init__()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.groups_handler = hs.get_groups_local_handler()
     async def on_GET(self, request):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         requester_user_id = requester.user.to_string()
         result = await self.groups_handler.get_joined_groups(requester_user_id)
         return 200, result
 def register_servlets(hs, http_server):
     GroupServlet(hs).register(http_server)

--- a/synapse/rest/client/v2_alpha/keys.py
+++ b/synapse/rest/client/v2_alpha/keys.py
@@ -33,21 +33,21 @@
         "<algorithm>:<key_id>": "<key_base64>"
       },
     }
     """
     PATTERNS = client_patterns("/keys/upload(/(?P<device_id>[^/]+))?$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(KeyUploadServlet, self).__init__()
         self.auth = hs.get_auth()
         self.e2e_keys_handler = hs.get_e2e_keys_handler()
     @trace(opname="upload_keys")
     async def on_POST(self, request, device_id):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         user_id = requester.user.to_string()
         body = parse_json_object_from_request(request)
         if device_id is not None:
             if requester.device_id is not None and device_id != requester.device_id:
                 set_tag("error", True)
@@ -105,21 +105,21 @@
               "<server_name>": {
                 "<algorithm>:<key_id>": "<signature_base64>"
     } } } } } }
     """
     PATTERNS = client_patterns("/keys/query$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer):
         """
-        super().__init__()
+        super(KeyQueryServlet, self).__init__()
         self.auth = hs.get_auth()
         self.e2e_keys_handler = hs.get_e2e_keys_handler()
     async def on_POST(self, request):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         user_id = requester.user.to_string()
         timeout = parse_integer(request, "timeout", 10 * 1000)
         body = parse_json_object_from_request(request)
         result = await self.e2e_keys_handler.query_devices(body, timeout, user_id)
         return 200, result
 class KeyChangesServlet(RestServlet):
@@ -128,30 +128,29 @@
         GET /keys/changes?from=...&to=...
         200 OK
         { "changed": ["@foo:example.com"] }
     """
     PATTERNS = client_patterns("/keys/changes$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer):
         """
-        super().__init__()
+        super(KeyChangesServlet, self).__init__()
         self.auth = hs.get_auth()
         self.device_handler = hs.get_device_handler()
-        self.store = hs.get_datastore()
     async def on_GET(self, request):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         from_token_string = parse_string(request, "from")
         set_tag("from", from_token_string)
         set_tag("to", parse_string(request, "to"))
-        from_token = await StreamToken.from_string(self.store, from_token_string)
+        from_token = StreamToken.from_string(from_token_string)
         user_id = requester.user.to_string()
         results = await self.device_handler.get_user_ids_changed(user_id, from_token)
         return 200, results
 class OneTimeKeyServlet(RestServlet):
     """
     POST /keys/claim HTTP/1.1
     {
       "one_time_keys": {
         "<user_id>": {
           "<device_id>": "<algorithm>"
@@ -159,21 +158,21 @@
     HTTP/1.1 200 OK
     {
       "one_time_keys": {
         "<user_id>": {
           "<device_id>": {
             "<algorithm>:<key_id>": "<key_base64>"
     } } } }
     """
     PATTERNS = client_patterns("/keys/claim$")
     def __init__(self, hs):
-        super().__init__()
+        super(OneTimeKeyServlet, self).__init__()
         self.auth = hs.get_auth()
         self.e2e_keys_handler = hs.get_e2e_keys_handler()
     async def on_POST(self, request):
         await self.auth.get_user_by_req(request, allow_guest=True)
         timeout = parse_integer(request, "timeout", 10 * 1000)
         body = parse_json_object_from_request(request)
         result = await self.e2e_keys_handler.claim_one_time_keys(body, timeout)
         return 200, result
 class SigningKeyUploadServlet(RestServlet):
     """
@@ -181,21 +180,21 @@
     Content-Type: application/json
     {
     }
     """
     PATTERNS = client_patterns("/keys/device_signing/upload$", releases=())
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(SigningKeyUploadServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.e2e_keys_handler = hs.get_e2e_keys_handler()
         self.auth_handler = hs.get_auth_handler()
     @interactive_auth_handler
     async def on_POST(self, request):
         requester = await self.auth.get_user_by_req(request)
         user_id = requester.user.to_string()
         body = parse_json_object_from_request(request)
         await self.auth_handler.validate_user_via_ui_auth(
@@ -231,21 +230,21 @@
         }
       }
     }
     """
     PATTERNS = client_patterns("/keys/signatures/upload$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(SignaturesUploadServlet, self).__init__()
         self.auth = hs.get_auth()
         self.e2e_keys_handler = hs.get_e2e_keys_handler()
     async def on_POST(self, request):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         user_id = requester.user.to_string()
         body = parse_json_object_from_request(request)
         result = await self.e2e_keys_handler.upload_signatures_for_device_keys(
             user_id, body
         )
         return 200, result

--- a/synapse/rest/client/v2_alpha/notifications.py
+++ b/synapse/rest/client/v2_alpha/notifications.py
@@ -1,19 +1,19 @@
 import logging
 from synapse.events.utils import format_event_for_client_v2_without_room_id
 from synapse.http.servlet import RestServlet, parse_integer, parse_string
 from ._base import client_patterns
 logger = logging.getLogger(__name__)
 class NotificationsServlet(RestServlet):
     PATTERNS = client_patterns("/notifications$")
     def __init__(self, hs):
-        super().__init__()
+        super(NotificationsServlet, self).__init__()
         self.store = hs.get_datastore()
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self._event_serializer = hs.get_event_client_serializer()
     async def on_GET(self, request):
         requester = await self.auth.get_user_by_req(request)
         user_id = requester.user.to_string()
         from_token = parse_string(request, "from", required=False)
         limit = parse_integer(request, "limit", default=50)
         only = parse_string(request, "only", required=False)

--- a/synapse/rest/client/v2_alpha/openid.py
+++ b/synapse/rest/client/v2_alpha/openid.py
@@ -22,21 +22,21 @@
     {
         "access_token": "ABDEFGH",
         "token_type": "Bearer",
         "matrix_server_name": "example.com",
         "expires_in": 3600,
     }
     """
     PATTERNS = client_patterns("/user/(?P<user_id>[^/]*)/openid/request_token")
     EXPIRES_MS = 3600 * 1000
     def __init__(self, hs):
-        super().__init__()
+        super(IdTokenServlet, self).__init__()
         self.auth = hs.get_auth()
         self.store = hs.get_datastore()
         self.clock = hs.get_clock()
         self.server_name = hs.config.server_name
     async def on_POST(self, request, user_id):
         requester = await self.auth.get_user_by_req(request)
         if user_id != requester.user.to_string():
             raise AuthError(403, "Cannot request tokens for other users.")
         parse_json_object_from_request(request)
         token = random_string(24)

--- a/synapse/rest/client/v2_alpha/password_policy.py
+++ b/synapse/rest/client/v2_alpha/password_policy.py
@@ -2,21 +2,21 @@
 from synapse.http.servlet import RestServlet
 from ._base import client_patterns
 logger = logging.getLogger(__name__)
 class PasswordPolicyServlet(RestServlet):
     PATTERNS = client_patterns("/password_policy$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(PasswordPolicyServlet, self).__init__()
         self.policy = hs.config.password_policy
         self.enabled = hs.config.password_policy_enabled
     def on_GET(self, request):
         if not self.enabled or not self.policy:
             return (200, {})
         policy = {}
         for param in [
             "minimum_length",
             "require_digit",
             "require_symbol",

--- a/synapse/rest/client/v2_alpha/read_marker.py
+++ b/synapse/rest/client/v2_alpha/read_marker.py
@@ -1,18 +1,18 @@
 import logging
 from synapse.http.servlet import RestServlet, parse_json_object_from_request
 from ._base import client_patterns
 logger = logging.getLogger(__name__)
 class ReadMarkerRestServlet(RestServlet):
     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/read_markers$")
     def __init__(self, hs):
-        super().__init__()
+        super(ReadMarkerRestServlet, self).__init__()
         self.auth = hs.get_auth()
         self.receipts_handler = hs.get_receipts_handler()
         self.read_marker_handler = hs.get_read_marker_handler()
         self.presence_handler = hs.get_presence_handler()
     async def on_POST(self, request, room_id):
         requester = await self.auth.get_user_by_req(request)
         await self.presence_handler.bump_presence_active_time(requester.user)
         body = parse_json_object_from_request(request)
         read_event_id = body.get("m.read", None)
         if read_event_id:

--- a/synapse/rest/client/v2_alpha/receipts.py
+++ b/synapse/rest/client/v2_alpha/receipts.py
@@ -3,21 +3,21 @@
 from synapse.http.servlet import RestServlet
 from ._base import client_patterns
 logger = logging.getLogger(__name__)
 class ReceiptRestServlet(RestServlet):
     PATTERNS = client_patterns(
         "/rooms/(?P<room_id>[^/]*)"
         "/receipt/(?P<receipt_type>[^/]*)"
         "/(?P<event_id>[^/]*)$"
     )
     def __init__(self, hs):
-        super().__init__()
+        super(ReceiptRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.receipts_handler = hs.get_receipts_handler()
         self.presence_handler = hs.get_presence_handler()
     async def on_POST(self, request, room_id, receipt_type, event_id):
         requester = await self.auth.get_user_by_req(request)
         if receipt_type != "m.read":
             raise SynapseError(400, "Receipt type must be 'm.read'")
         await self.presence_handler.bump_presence_active_time(requester.user)
         await self.receipts_handler.received_client_receipt(

--- a/synapse/rest/client/v2_alpha/register.py
+++ b/synapse/rest/client/v2_alpha/register.py
@@ -40,21 +40,21 @@
     def compare_digest(a, b):
         return a == b
 logger = logging.getLogger(__name__)
 class EmailRegisterRequestTokenRestServlet(RestServlet):
     PATTERNS = client_patterns("/register/email/requestToken$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(EmailRegisterRequestTokenRestServlet, self).__init__()
         self.hs = hs
         self.identity_handler = hs.get_handlers().identity_handler
         self.config = hs.config
         if self.hs.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:
             self.mailer = Mailer(
                 hs=self.hs,
                 app_name=self.config.email_app_name,
                 template_html=self.config.email_registration_template_html,
                 template_text=self.config.email_registration_template_text,
             )
@@ -110,21 +110,21 @@
             )
             ret = {"sid": sid}
         return 200, ret
 class MsisdnRegisterRequestTokenRestServlet(RestServlet):
     PATTERNS = client_patterns("/register/msisdn/requestToken$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(MsisdnRegisterRequestTokenRestServlet, self).__init__()
         self.hs = hs
         self.identity_handler = hs.get_handlers().identity_handler
     async def on_POST(self, request):
         body = parse_json_object_from_request(request)
         assert_params_in_dict(
             body, ["client_secret", "country", "phone_number", "send_attempt"]
         )
         client_secret = body["client_secret"]
         country = body["country"]
         phone_number = body["phone_number"]
@@ -167,21 +167,21 @@
 class RegistrationSubmitTokenServlet(RestServlet):
     """Handles registration 3PID validation token submission"""
     PATTERNS = client_patterns(
         "/registration/(?P<medium>[^/]*)/submit_token$", releases=(), unstable=True
     )
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(RegistrationSubmitTokenServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.config = hs.config
         self.clock = hs.get_clock()
         self.store = hs.get_datastore()
         if self.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:
             self._failure_email_template = (
                 self.config.email_registration_template_failure_html
             )
     async def on_GET(self, request, medium):
@@ -221,21 +221,21 @@
             template_vars = {"failure_reason": e.msg}
             html = self._failure_email_template.render(**template_vars)
         respond_with_html(request, status_code, html)
 class UsernameAvailabilityRestServlet(RestServlet):
     PATTERNS = client_patterns("/register/available")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(UsernameAvailabilityRestServlet, self).__init__()
         self.hs = hs
         self.registration_handler = hs.get_registration_handler()
         self.ratelimiter = FederationRateLimiter(
             hs.get_clock(),
             FederationRateLimitConfig(
                 window_size=2000,
                 sleep_limit=1,
                 sleep_msec=1000,
                 reject_limit=1,
                 concurrent_requests=1,
@@ -252,21 +252,21 @@
             username = parse_string(request, "username", required=True)
             await self.registration_handler.check_username(username)
             return 200, {"available": True}
 class RegisterRestServlet(RestServlet):
     PATTERNS = client_patterns("/register$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(RegisterRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.store = hs.get_datastore()
         self.auth_handler = hs.get_auth_handler()
         self.registration_handler = hs.get_registration_handler()
         self.identity_handler = hs.get_handlers().identity_handler
         self.room_member_handler = hs.get_room_member_handler()
         self.macaroon_gen = hs.get_macaroon_generator()
         self.ratelimiter = hs.get_registration_ratelimiter()
         self.password_policy_handler = hs.get_password_policy_handler()
@@ -294,26 +294,25 @@
         if "username" in body:
             if not isinstance(body["username"], str) or len(body["username"]) > 512:
                 raise SynapseError(400, "Invalid username")
             desired_username = body["username"]
         appservice = None
         if self.auth.has_access_token(request):
             appservice = self.auth.get_appservice_by_req(request)
         if appservice:
             desired_username = body.get("user", desired_username)
             access_token = self.auth.get_access_token_from_request(request)
-            if not isinstance(desired_username, str):
-                raise SynapseError(400, "Desired Username is missing or not a string")
-            result = await self._do_appservice_registration(
-                desired_username, access_token, body
-            )
-            return 200, result
+            if isinstance(desired_username, str):
+                result = await self._do_appservice_registration(
+                    desired_username, access_token, body
+                )
+            return 200, result  # we throw for non 200 responses
         if not self._registration_enabled:
             raise SynapseError(403, "Registration has been disabled")
         if desired_username is not None:
             desired_username = desired_username.lower()
         guest_access_token = body.get("guest_access_token", None)
         password = body.pop("password", None)
         if password is not None:
             if not isinstance(password, str) or len(password) > 512:
                 raise SynapseError(400, "Invalid password")
             self.password_policy_handler.validate_password(password)

--- a/synapse/rest/client/v2_alpha/relations.py
+++ b/synapse/rest/client/v2_alpha/relations.py
@@ -27,21 +27,21 @@
         {}
         {
             "event_id": "$foobar"
         }
     """
     PATTERN = (
         "/rooms/(?P<room_id>[^/]*)/send_relation"
         "/(?P<parent_id>[^/]*)/(?P<relation_type>[^/]*)/(?P<event_type>[^/]*)"
     )
     def __init__(self, hs):
-        super().__init__()
+        super(RelationSendServlet, self).__init__()
         self.auth = hs.get_auth()
         self.event_creation_handler = hs.get_event_creation_handler()
         self.txns = HttpTransactionCache(hs)
     def register(self, http_server):
         http_server.register_paths(
             "POST",
             client_patterns(self.PATTERN + "$", releases=()),
             self.on_PUT_or_POST,
             self.__class__.__name__,
         )
@@ -88,21 +88,21 @@
 class RelationPaginationServlet(RestServlet):
     """API to paginate relations on an event by topological ordering, optionally
     filtered by relation type and event type.
     """
     PATTERNS = client_patterns(
         "/rooms/(?P<room_id>[^/]*)/relations/(?P<parent_id>[^/]*)"
         "(/(?P<relation_type>[^/]*)(/(?P<event_type>[^/]*))?)?$",
         releases=(),
     )
     def __init__(self, hs):
-        super().__init__()
+        super(RelationPaginationServlet, self).__init__()
         self.auth = hs.get_auth()
         self.store = hs.get_datastore()
         self.clock = hs.get_clock()
         self._event_serializer = hs.get_event_client_serializer()
         self.event_handler = hs.get_event_handler()
     async def on_GET(
         self, request, room_id, parent_id, relation_type=None, event_type=None
     ):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         await self.auth.check_user_in_room_or_world_readable(
@@ -155,21 +155,21 @@
                 }
             ]
         }
     """
     PATTERNS = client_patterns(
         "/rooms/(?P<room_id>[^/]*)/aggregations/(?P<parent_id>[^/]*)"
         "(/(?P<relation_type>[^/]*)(/(?P<event_type>[^/]*))?)?$",
         releases=(),
     )
     def __init__(self, hs):
-        super().__init__()
+        super(RelationAggregationPaginationServlet, self).__init__()
         self.auth = hs.get_auth()
         self.store = hs.get_datastore()
         self.event_handler = hs.get_event_handler()
     async def on_GET(
         self, request, room_id, parent_id, relation_type=None, event_type=None
     ):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         await self.auth.check_user_in_room_or_world_readable(
             room_id, requester.user.to_string(), allow_departed_users=True,
         )
@@ -213,21 +213,21 @@
                 ...
             ]
         }
     """
     PATTERNS = client_patterns(
         "/rooms/(?P<room_id>[^/]*)/aggregations/(?P<parent_id>[^/]*)"
         "/(?P<relation_type>[^/]*)/(?P<event_type>[^/]*)/(?P<key>[^/]*)$",
         releases=(),
     )
     def __init__(self, hs):
-        super().__init__()
+        super(RelationAggregationGroupPaginationServlet, self).__init__()
         self.auth = hs.get_auth()
         self.store = hs.get_datastore()
         self.clock = hs.get_clock()
         self._event_serializer = hs.get_event_client_serializer()
         self.event_handler = hs.get_event_handler()
     async def on_GET(self, request, room_id, parent_id, relation_type, event_type, key):
         requester = await self.auth.get_user_by_req(request, allow_guest=True)
         await self.auth.check_user_in_room_or_world_readable(
             room_id, requester.user.to_string(), allow_departed_users=True,
         )

--- a/synapse/rest/client/v2_alpha/report_event.py
+++ b/synapse/rest/client/v2_alpha/report_event.py
@@ -4,21 +4,21 @@
 from synapse.http.servlet import (
     RestServlet,
     assert_params_in_dict,
     parse_json_object_from_request,
 )
 from ._base import client_patterns
 logger = logging.getLogger(__name__)
 class ReportEventRestServlet(RestServlet):
     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/report/(?P<event_id>[^/]*)$")
     def __init__(self, hs):
-        super().__init__()
+        super(ReportEventRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.clock = hs.get_clock()
         self.store = hs.get_datastore()
     async def on_POST(self, request, room_id, event_id):
         requester = await self.auth.get_user_by_req(request)
         user_id = requester.user.to_string()
         body = parse_json_object_from_request(request)
         assert_params_in_dict(body, ("reason", "score"))
         if not isinstance(body["reason"], str):

--- a/synapse/rest/client/v2_alpha/room_keys.py
+++ b/synapse/rest/client/v2_alpha/room_keys.py
@@ -9,21 +9,21 @@
 logger = logging.getLogger(__name__)
 class RoomKeysServlet(RestServlet):
     PATTERNS = client_patterns(
         "/room_keys/keys(/(?P<room_id>[^/]+))?(/(?P<session_id>[^/]+))?$"
     )
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(RoomKeysServlet, self).__init__()
         self.auth = hs.get_auth()
         self.e2e_room_keys_handler = hs.get_e2e_room_keys_handler()
     async def on_PUT(self, request, room_id, session_id):
         """
         Uploads one or more encrypted E2E room keys for backup purposes.
         room_id: the ID of the room the keys are for (optional)
         session_id: the ID for the E2E room keys for the room (optional)
         version: the version of the user's backup which this data is for.
         the version must already have been created via the /room_keys/version API.
         Each session has:
@@ -176,21 +176,21 @@
             user_id, version, room_id, session_id
         )
         return 200, ret
 class RoomKeysNewVersionServlet(RestServlet):
     PATTERNS = client_patterns("/room_keys/version$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(RoomKeysNewVersionServlet, self).__init__()
         self.auth = hs.get_auth()
         self.e2e_room_keys_handler = hs.get_e2e_room_keys_handler()
     async def on_POST(self, request):
         """
         Create a new backup version for this user's room_keys with the given
         info.  The version is allocated by the server and returned to the user
         in the response.  This API is intended to be used whenever the user
         changes the encryption key for their backups, ensuring that backups
         encrypted with different keys don't collide.
         It takes out an exclusive lock on this user's room_key backups, to ensure
@@ -216,21 +216,21 @@
         info = parse_json_object_from_request(request)
         new_version = await self.e2e_room_keys_handler.create_version(user_id, info)
         return 200, {"version": new_version}
 class RoomKeysVersionServlet(RestServlet):
     PATTERNS = client_patterns("/room_keys/version(/(?P<version>[^/]+))?$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(RoomKeysVersionServlet, self).__init__()
         self.auth = hs.get_auth()
         self.e2e_room_keys_handler = hs.get_e2e_room_keys_handler()
     async def on_GET(self, request, version):
         """
         Retrieve the version information about a given version of the user's
         room_keys backup.  If the version part is missing, returns info about the
         most current backup version (if any)
         It takes out an exclusive lock on this user's room_key backups, to ensure
         clients only upload to the current backup.
         Returns 404 if the given version does not exist.

--- a/synapse/rest/client/v2_alpha/room_upgrade_rest_servlet.py
+++ b/synapse/rest/client/v2_alpha/room_upgrade_rest_servlet.py
@@ -18,21 +18,21 @@
             "new_version": "2",
         }
     Creates a new room and shuts down the old one. Returns the ID of the new room.
     Args:
         hs (synapse.server.HomeServer):
     """
     PATTERNS = client_patterns(
         "/rooms/(?P<room_id>[^/]*)/upgrade$"
     )
     def __init__(self, hs):
-        super().__init__()
+        super(RoomUpgradeRestServlet, self).__init__()
         self._hs = hs
         self._room_creation_handler = hs.get_room_creation_handler()
         self._auth = hs.get_auth()
     async def on_POST(self, request, room_id):
         requester = await self._auth.get_user_by_req(request)
         content = parse_json_object_from_request(request)
         assert_params_in_dict(content, ("new_version",))
         new_version = KNOWN_ROOM_VERSIONS.get(content["new_version"])
         if new_version is None:
             raise SynapseError(

--- a/synapse/rest/client/v2_alpha/sendtodevice.py
+++ b/synapse/rest/client/v2_alpha/sendtodevice.py
@@ -8,21 +8,21 @@
 logger = logging.getLogger(__name__)
 class SendToDeviceRestServlet(servlet.RestServlet):
     PATTERNS = client_patterns(
         "/sendToDevice/(?P<message_type>[^/]*)/(?P<txn_id>[^/]*)$"
     )
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(SendToDeviceRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.txns = HttpTransactionCache(hs)
         self.device_message_handler = hs.get_device_message_handler()
     @trace(opname="sendToDevice")
     def on_PUT(self, request, message_type, txn_id):
         set_tag("message_type", message_type)
         set_tag("txn_id", txn_id)
         return self.txns.fetch_or_execute_request(
             request, self._put, request, message_type, txn_id

--- a/synapse/rest/client/v2_alpha/shared_rooms.py
+++ b/synapse/rest/client/v2_alpha/shared_rooms.py
@@ -6,21 +6,21 @@
 logger = logging.getLogger(__name__)
 class UserSharedRoomsServlet(RestServlet):
     """
     GET /uk.half-shot.msc2666/user/shared_rooms/{user_id} HTTP/1.1
     """
     PATTERNS = client_patterns(
         "/uk.half-shot.msc2666/user/shared_rooms/(?P<user_id>[^/]*)",
         releases=(),  # This is an unstable feature
     )
     def __init__(self, hs):
-        super().__init__()
+        super(UserSharedRoomsServlet, self).__init__()
         self.auth = hs.get_auth()
         self.store = hs.get_datastore()
         self.user_directory_active = hs.config.update_user_directory
     async def on_GET(self, request, user_id):
         if not self.user_directory_active:
             raise SynapseError(
                 code=400,
                 msg="The user directory is disabled on this server. Cannot determine shared rooms.",
                 errcode=Codes.FORBIDDEN,
             )

--- a/synapse/rest/client/v2_alpha/sync.py
+++ b/synapse/rest/client/v2_alpha/sync.py
@@ -43,24 +43,23 @@
               }
             },
             "invite": {}, // Invited rooms being updated.
             "leave": {} // Archived rooms being updated.
           }
         }
     """
     PATTERNS = client_patterns("/sync$")
     ALLOWED_PRESENCE = {"online", "offline", "unavailable"}
     def __init__(self, hs):
-        super().__init__()
+        super(SyncRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
-        self.store = hs.get_datastore()
         self.sync_handler = hs.get_sync_handler()
         self.clock = hs.get_clock()
         self.filtering = hs.get_filtering()
         self.presence_handler = hs.get_presence_handler()
         self._server_notices_sender = hs.get_server_notices_sender()
         self._event_serializer = hs.get_event_client_serializer()
     async def on_GET(self, request):
         if b"from" in request.args:
             raise SynapseError(
                 400, "'from' is not a valid query parameter. Did you mean 'since'?"
@@ -110,23 +109,24 @@
                 if err.code != 404:
                     raise
                 raise SynapseError(400, "No such filter", errcode=Codes.INVALID_PARAM)
         sync_config = SyncConfig(
             user=user,
             filter_collection=filter_collection,
             is_guest=requester.is_guest,
             request_key=request_key,
             device_id=device_id,
         )
-        since_token = None
         if since is not None:
-            since_token = await StreamToken.from_string(self.store, since)
+            since_token = StreamToken.from_string(since)
+        else:
+            since_token = None
         await self._server_notices_sender.on_user_syncing(user.to_string())
         affect_presence = set_presence != PresenceState.OFFLINE
         if affect_presence:
             await self.presence_handler.set_state(
                 user, {"presence": set_presence}, True
             )
         context = await self.presence_handler.user_syncing(
             user.to_string(), affect_presence=affect_presence
         )
         with context:
@@ -179,21 +179,21 @@
                 "left": list(sync_result.device_lists.left),
             },
             "presence": SyncRestServlet.encode_presence(sync_result.presence, time_now),
             "rooms": {"join": joined, "invite": invited, "leave": archived},
             "groups": {
                 "join": sync_result.groups.join,
                 "invite": sync_result.groups.invite,
                 "leave": sync_result.groups.leave,
             },
             "device_one_time_keys_count": sync_result.device_one_time_keys_count,
-            "next_batch": await sync_result.next_batch.to_string(self.store),
+            "next_batch": sync_result.next_batch.to_string(),
         }
     @staticmethod
     def encode_presence(events, time_now):
         return {
             "events": [
                 {
                     "type": "m.presence",
                     "sender": event.user_id,
                     "content": format_user_presence_state(
                         event, time_now, include_user_id=False
@@ -333,21 +333,21 @@
                     event.event_id,
                     room.room_id,
                     event.room_id,
                 )
         serialized_state = await serialize(state_events)
         serialized_timeline = await serialize(timeline_events)
         account_data = room.account_data
         result = {
             "timeline": {
                 "events": serialized_timeline,
-                "prev_batch": await room.timeline.prev_batch.to_string(self.store),
+                "prev_batch": room.timeline.prev_batch.to_string(),
                 "limited": room.timeline.limited,
             },
             "state": {"events": serialized_state},
             "account_data": {"events": account_data},
         }
         if joined:
             ephemeral_events = room.ephemeral
             result["ephemeral"] = {"events": ephemeral_events}
             result["unread_notifications"] = room.unread_notifications
             result["summary"] = room.summary

--- a/synapse/rest/client/v2_alpha/tags.py
+++ b/synapse/rest/client/v2_alpha/tags.py
@@ -2,39 +2,39 @@
 from synapse.api.errors import AuthError
 from synapse.http.servlet import RestServlet, parse_json_object_from_request
 from ._base import client_patterns
 logger = logging.getLogger(__name__)
 class TagListServlet(RestServlet):
     """
     GET /user/{user_id}/rooms/{room_id}/tags HTTP/1.1
     """
     PATTERNS = client_patterns("/user/(?P<user_id>[^/]*)/rooms/(?P<room_id>[^/]*)/tags")
     def __init__(self, hs):
-        super().__init__()
+        super(TagListServlet, self).__init__()
         self.auth = hs.get_auth()
         self.store = hs.get_datastore()
     async def on_GET(self, request, user_id, room_id):
         requester = await self.auth.get_user_by_req(request)
         if user_id != requester.user.to_string():
             raise AuthError(403, "Cannot get tags for other users.")
         tags = await self.store.get_tags_for_room(user_id, room_id)
         return 200, {"tags": tags}
 class TagServlet(RestServlet):
     """
     PUT /user/{user_id}/rooms/{room_id}/tags/{tag} HTTP/1.1
     DELETE /user/{user_id}/rooms/{room_id}/tags/{tag} HTTP/1.1
     """
     PATTERNS = client_patterns(
         "/user/(?P<user_id>[^/]*)/rooms/(?P<room_id>[^/]*)/tags/(?P<tag>[^/]*)"
     )
     def __init__(self, hs):
-        super().__init__()
+        super(TagServlet, self).__init__()
         self.auth = hs.get_auth()
         self.store = hs.get_datastore()
         self.notifier = hs.get_notifier()
     async def on_PUT(self, request, user_id, room_id, tag):
         requester = await self.auth.get_user_by_req(request)
         if user_id != requester.user.to_string():
             raise AuthError(403, "Cannot add tags for other users.")
         body = parse_json_object_from_request(request)
         max_id = await self.store.add_tag_to_room(user_id, room_id, tag, body)
         self.notifier.on_new_event("account_data_key", max_id, users=[user_id])

--- a/synapse/rest/client/v2_alpha/thirdparty.py
+++ b/synapse/rest/client/v2_alpha/thirdparty.py
@@ -1,58 +1,58 @@
 import logging
 from synapse.api.constants import ThirdPartyEntityKind
 from synapse.http.servlet import RestServlet
 from ._base import client_patterns
 logger = logging.getLogger(__name__)
 class ThirdPartyProtocolsServlet(RestServlet):
     PATTERNS = client_patterns("/thirdparty/protocols")
     def __init__(self, hs):
-        super().__init__()
+        super(ThirdPartyProtocolsServlet, self).__init__()
         self.auth = hs.get_auth()
         self.appservice_handler = hs.get_application_service_handler()
     async def on_GET(self, request):
         await self.auth.get_user_by_req(request, allow_guest=True)
         protocols = await self.appservice_handler.get_3pe_protocols()
         return 200, protocols
 class ThirdPartyProtocolServlet(RestServlet):
     PATTERNS = client_patterns("/thirdparty/protocol/(?P<protocol>[^/]+)$")
     def __init__(self, hs):
-        super().__init__()
+        super(ThirdPartyProtocolServlet, self).__init__()
         self.auth = hs.get_auth()
         self.appservice_handler = hs.get_application_service_handler()
     async def on_GET(self, request, protocol):
         await self.auth.get_user_by_req(request, allow_guest=True)
         protocols = await self.appservice_handler.get_3pe_protocols(
             only_protocol=protocol
         )
         if protocol in protocols:
             return 200, protocols[protocol]
         else:
             return 404, {"error": "Unknown protocol"}
 class ThirdPartyUserServlet(RestServlet):
     PATTERNS = client_patterns("/thirdparty/user(/(?P<protocol>[^/]+))?$")
     def __init__(self, hs):
-        super().__init__()
+        super(ThirdPartyUserServlet, self).__init__()
         self.auth = hs.get_auth()
         self.appservice_handler = hs.get_application_service_handler()
     async def on_GET(self, request, protocol):
         await self.auth.get_user_by_req(request, allow_guest=True)
         fields = request.args
         fields.pop(b"access_token", None)
         results = await self.appservice_handler.query_3pe(
             ThirdPartyEntityKind.USER, protocol, fields
         )
         return 200, results
 class ThirdPartyLocationServlet(RestServlet):
     PATTERNS = client_patterns("/thirdparty/location(/(?P<protocol>[^/]+))?$")
     def __init__(self, hs):
-        super().__init__()
+        super(ThirdPartyLocationServlet, self).__init__()
         self.auth = hs.get_auth()
         self.appservice_handler = hs.get_application_service_handler()
     async def on_GET(self, request, protocol):
         await self.auth.get_user_by_req(request, allow_guest=True)
         fields = request.args
         fields.pop(b"access_token", None)
         results = await self.appservice_handler.query_3pe(
             ThirdPartyEntityKind.LOCATION, protocol, fields
         )
         return 200, results

--- a/synapse/rest/client/v2_alpha/tokenrefresh.py
+++ b/synapse/rest/client/v2_alpha/tokenrefresh.py
@@ -1,15 +1,15 @@
 from synapse.api.errors import AuthError
 from synapse.http.servlet import RestServlet
 from ._base import client_patterns
 class TokenRefreshRestServlet(RestServlet):
     """
     Exchanges refresh tokens for a pair of an access token and a new refresh
     token.
     """
     PATTERNS = client_patterns("/tokenrefresh")
     def __init__(self, hs):
-        super().__init__()
+        super(TokenRefreshRestServlet, self).__init__()
     async def on_POST(self, request):
         raise AuthError(403, "tokenrefresh is no longer supported.")
 def register_servlets(hs, http_server):
     TokenRefreshRestServlet(hs).register(http_server)

--- a/synapse/rest/client/v2_alpha/user_directory.py
+++ b/synapse/rest/client/v2_alpha/user_directory.py
@@ -3,21 +3,21 @@
 from synapse.http.servlet import RestServlet, parse_json_object_from_request
 from ._base import client_patterns
 logger = logging.getLogger(__name__)
 class UserDirectorySearchRestServlet(RestServlet):
     PATTERNS = client_patterns("/user_directory/search$")
     def __init__(self, hs):
         """
         Args:
             hs (synapse.server.HomeServer): server
         """
-        super().__init__()
+        super(UserDirectorySearchRestServlet, self).__init__()
         self.hs = hs
         self.auth = hs.get_auth()
         self.user_directory_handler = hs.get_user_directory_handler()
     async def on_POST(self, request):
         """Searches for users in directory
         Returns:
             dict of the form::
                 {
                     "limited": <bool>,  # whether there were more results or not
                     "results": [  # Ordered by best match first

--- a/synapse/rest/client/versions.py
+++ b/synapse/rest/client/versions.py
@@ -1,19 +1,19 @@
 import logging
 import re
 from synapse.api.constants import RoomCreationPreset
 from synapse.http.servlet import RestServlet
 logger = logging.getLogger(__name__)
 class VersionsRestServlet(RestServlet):
     PATTERNS = [re.compile("^/_matrix/client/versions$")]
     def __init__(self, hs):
-        super().__init__()
+        super(VersionsRestServlet, self).__init__()
         self.config = hs.config
         self.e2ee_forced_public = (
             RoomCreationPreset.PUBLIC_CHAT
             in self.config.encryption_enabled_by_default_for_room_presets
         )
         self.e2ee_forced_private = (
             RoomCreationPreset.PRIVATE_CHAT
             in self.config.encryption_enabled_by_default_for_room_presets
         )
         self.e2ee_forced_trusted_private = (

--- a/synapse/rest/key/v2/remote_key_resource.py
+++ b/synapse/rest/key/v2/remote_key_resource.py
@@ -7,21 +7,21 @@
 from synapse.http.servlet import parse_integer, parse_json_object_from_request
 from synapse.util import json_decoder
 logger = logging.getLogger(__name__)
 class RemoteKey(DirectServeJsonResource):
     """HTTP resource for retrieving the TLS certificate and NACL signature
     verification keys for a collection of servers. Checks that the reported
     X.509 TLS certificate matches the one used in the HTTPS connection. Checks
     that the NACL signature for the remote server is valid. Returns a dict of
     JSON signed by both the remote server and by this server.
     Supports individual GET APIs and a bulk query POST API.
-    Requests:
+    Requsts:
     GET /_matrix/key/v2/query/remote.server.example.com HTTP/1.1
     GET /_matrix/key/v2/query/remote.server.example.com/a.key.id HTTP/1.1
     POST /_matrix/v2/query HTTP/1.1
     Content-Type: application/json
     {
         "server_keys": {
             "remote.server.example.com": {
                 "a.key.id": {
                     "minimum_valid_until_ts": 1234567890123
                 }

--- a/synapse/rest/media/v1/filepath.py
+++ b/synapse/rest/media/v1/filepath.py
@@ -40,43 +40,30 @@
     local_media_thumbnail = _wrap_in_base_path(local_media_thumbnail_rel)
     def remote_media_filepath_rel(self, server_name, file_id):
         return os.path.join(
             "remote_content", server_name, file_id[0:2], file_id[2:4], file_id[4:]
         )
     remote_media_filepath = _wrap_in_base_path(remote_media_filepath_rel)
     def remote_media_thumbnail_rel(
         self, server_name, file_id, width, height, content_type, method
     ):
         top_level_type, sub_type = content_type.split("/")
-        file_name = "%i-%i-%s-%s-%s" % (width, height, top_level_type, sub_type, method)
+        file_name = "%i-%i-%s-%s" % (width, height, top_level_type, sub_type)
         return os.path.join(
             "remote_thumbnail",
             server_name,
             file_id[0:2],
             file_id[2:4],
             file_id[4:],
             file_name,
         )
     remote_media_thumbnail = _wrap_in_base_path(remote_media_thumbnail_rel)
-    def remote_media_thumbnail_rel_legacy(
-        self, server_name, file_id, width, height, content_type
-    ):
-        top_level_type, sub_type = content_type.split("/")
-        file_name = "%i-%i-%s-%s" % (width, height, top_level_type, sub_type)
-        return os.path.join(
-            "remote_thumbnail",
-            server_name,
-            file_id[0:2],
-            file_id[2:4],
-            file_id[4:],
-            file_name,
-        )
     def remote_media_thumbnail_dir(self, server_name, file_id):
         return os.path.join(
             self.base_path,
             "remote_thumbnail",
             server_name,
             file_id[0:2],
             file_id[2:4],
             file_id[4:],
         )
     def url_cache_filepath_rel(self, media_id):

--- a/synapse/rest/media/v1/media_repository.py
+++ b/synapse/rest/media/v1/media_repository.py
@@ -27,21 +27,21 @@
     respond_404,
     respond_with_responder,
 )
 from .config_resource import MediaConfigResource
 from .download_resource import DownloadResource
 from .filepath import MediaFilePaths
 from .media_storage import MediaStorage
 from .preview_url_resource import PreviewUrlResource
 from .storage_provider import StorageProviderWrapper
 from .thumbnail_resource import ThumbnailResource
-from .thumbnailer import Thumbnailer, ThumbnailError
+from .thumbnailer import Thumbnailer
 from .upload_resource import UploadResource
 logger = logging.getLogger(__name__)
 UPDATE_RECENTLY_ACCESSED_TS = 60 * 1000
 class MediaRepository:
     def __init__(self, hs):
         self.hs = hs
         self.auth = hs.get_auth()
         self.client = hs.get_http_client()
         self.clock = hs.get_clock()
         self.server_name = hs.hostname
@@ -90,29 +90,29 @@
             server_name (str|None): Origin server of media, or None if local
             media_id (str): The media ID of the content
         """
         if server_name:
             self.recently_accessed_remotes.add((server_name, media_id))
         else:
             self.recently_accessed_locals.add(media_id)
     async def create_content(
         self,
         media_type: str,
-        upload_name: Optional[str],
+        upload_name: str,
         content: IO,
         content_length: int,
         auth_user: str,
     ) -> str:
         """Store uploaded content for a local user and return the mxc URL
         Args:
-            media_type: The content type of the file.
-            upload_name: The name of the file, if provided.
+            media_type: The content type of the file
+            upload_name: The name of the file
             content: A file like object that is the content to store
             content_length: The length of the content
             auth_user: The user_id of the uploader
         Returns:
             The mxc url of the stored content
         """
         media_id = random_string(24)
         file_info = FileInfo(server_name=None, file_id=media_id)
         fname = await self.media_storage.store_file(content, file_info)
         logger.info("Stored local media in file %r", fname)
@@ -339,42 +339,26 @@
             t_byte_source = thumbnailer.crop(t_width, t_height, t_type)
         elif t_method == "scale":
             t_width, t_height = thumbnailer.aspect(t_width, t_height)
             t_width = min(m_width, t_width)
             t_height = min(m_height, t_height)
             t_byte_source = thumbnailer.scale(t_width, t_height, t_type)
         else:
             t_byte_source = None
         return t_byte_source
     async def generate_local_exact_thumbnail(
-        self,
-        media_id: str,
-        t_width: int,
-        t_height: int,
-        t_method: str,
-        t_type: str,
-        url_cache: str,
-    ) -> Optional[str]:
+        self, media_id, t_width, t_height, t_method, t_type, url_cache
+    ):
         input_path = await self.media_storage.ensure_media_is_in_local_cache(
             FileInfo(None, media_id, url_cache=url_cache)
         )
-        try:
-            thumbnailer = Thumbnailer(input_path)
-        except ThumbnailError as e:
-            logger.warning(
-                "Unable to generate a thumbnail for local media %s using a method of %s and type of %s: %s",
-                media_id,
-                t_method,
-                t_type,
-                e,
-            )
-            return None
+        thumbnailer = Thumbnailer(input_path)
         t_byte_source = await defer_to_thread(
             self.hs.get_reactor(),
             self._generate_thumbnail,
             thumbnailer,
             t_width,
             t_height,
             t_method,
             t_type,
         )
         if t_byte_source:
@@ -393,46 +377,27 @@
                     t_byte_source, file_info
                 )
             finally:
                 t_byte_source.close()
             logger.info("Stored thumbnail in file %r", output_path)
             t_len = os.path.getsize(output_path)
             await self.store.store_local_thumbnail(
                 media_id, t_width, t_height, t_type, t_method, t_len
             )
             return output_path
-        return None
     async def generate_remote_exact_thumbnail(
-        self,
-        server_name: str,
-        file_id: str,
-        media_id: str,
-        t_width: int,
-        t_height: int,
-        t_method: str,
-        t_type: str,
-    ) -> Optional[str]:
+        self, server_name, file_id, media_id, t_width, t_height, t_method, t_type
+    ):
         input_path = await self.media_storage.ensure_media_is_in_local_cache(
             FileInfo(server_name, file_id, url_cache=False)
         )
-        try:
-            thumbnailer = Thumbnailer(input_path)
-        except ThumbnailError as e:
-            logger.warning(
-                "Unable to generate a thumbnail for remote media %s from %s using a method of %s and type of %s: %s",
-                media_id,
-                server_name,
-                t_method,
-                t_type,
-                e,
-            )
-            return None
+        thumbnailer = Thumbnailer(input_path)
         t_byte_source = await defer_to_thread(
             self.hs.get_reactor(),
             self._generate_thumbnail,
             thumbnailer,
             t_width,
             t_height,
             t_method,
             t_type,
         )
         if t_byte_source:
@@ -457,21 +422,20 @@
                 server_name,
                 media_id,
                 file_id,
                 t_width,
                 t_height,
                 t_type,
                 t_method,
                 t_len,
             )
             return output_path
-        return None
     async def _generate_thumbnails(
         self,
         server_name: Optional[str],
         media_id: str,
         file_id: str,
         media_type: str,
         url_cache: bool = False,
     ) -> Optional[dict]:
         """Generate and store thumbnails for an image.
         Args:
@@ -485,31 +449,21 @@
         Returns:
             Dict with "width" and "height" keys of original image or None if the
             media cannot be thumbnailed.
         """
         requirements = self._get_thumbnail_requirements(media_type)
         if not requirements:
             return None
         input_path = await self.media_storage.ensure_media_is_in_local_cache(
             FileInfo(server_name, file_id, url_cache=url_cache)
         )
-        try:
-            thumbnailer = Thumbnailer(input_path)
-        except ThumbnailError as e:
-            logger.warning(
-                "Unable to generate thumbnails for remote media %s from %s of type %s: %s",
-                media_id,
-                server_name,
-                media_type,
-                e,
-            )
-            return None
+        thumbnailer = Thumbnailer(input_path)
         m_width = thumbnailer.width
         m_height = thumbnailer.height
         if m_width * m_height >= self.max_image_pixels:
             logger.info(
                 "Image too large to thumbnail %r x %r > %r",
                 m_width,
                 m_height,
                 self.max_image_pixels,
             )
             return None

--- a/synapse/rest/media/v1/media_storage.py
+++ b/synapse/rest/media/v1/media_storage.py
@@ -86,68 +86,42 @@
         if not finished_called:
             raise Exception("Finished callback not called")
     async def fetch_media(self, file_info: FileInfo) -> Optional[Responder]:
         """Attempts to fetch media described by file_info from the local cache
         and configured storage providers.
         Args:
             file_info
         Returns:
             Returns a Responder if the file was found, otherwise None.
         """
-        paths = [self._file_info_to_path(file_info)]
-        if file_info.thumbnail and file_info.server_name:
-            paths.append(
-                self.filepaths.remote_media_thumbnail_rel_legacy(
-                    server_name=file_info.server_name,
-                    file_id=file_info.file_id,
-                    width=file_info.thumbnail_width,
-                    height=file_info.thumbnail_height,
-                    content_type=file_info.thumbnail_type,
-                )
-            )
-        for path in paths:
-            local_path = os.path.join(self.local_media_directory, path)
-            if os.path.exists(local_path):
-                logger.debug("responding with local file %s", local_path)
-                return FileResponder(open(local_path, "rb"))
-            logger.debug("local file %s did not exist", local_path)
+        path = self._file_info_to_path(file_info)
+        local_path = os.path.join(self.local_media_directory, path)
+        if os.path.exists(local_path):
+            return FileResponder(open(local_path, "rb"))
         for provider in self.storage_providers:
-            for path in paths:
-                res = await provider.fetch(path, file_info)  # type: Any
-                if res:
-                    logger.debug("Streaming %s from %s", path, provider)
-                    return res
-                logger.debug("%s not found on %s", path, provider)
+            res = await provider.fetch(path, file_info)  # type: Any
+            if res:
+                logger.debug("Streaming %s from %s", path, provider)
+                return res
         return None
     async def ensure_media_is_in_local_cache(self, file_info: FileInfo) -> str:
         """Ensures that the given file is in the local cache. Attempts to
         download it from storage providers if it isn't.
         Args:
             file_info
         Returns:
             Full path to local file
         """
         path = self._file_info_to_path(file_info)
         local_path = os.path.join(self.local_media_directory, path)
         if os.path.exists(local_path):
             return local_path
-        if file_info.thumbnail and file_info.server_name:
-            legacy_path = self.filepaths.remote_media_thumbnail_rel_legacy(
-                server_name=file_info.server_name,
-                file_id=file_info.file_id,
-                width=file_info.thumbnail_width,
-                height=file_info.thumbnail_height,
-                content_type=file_info.thumbnail_type,
-            )
-            legacy_local_path = os.path.join(self.local_media_directory, legacy_path)
-            if os.path.exists(legacy_local_path):
-                return legacy_local_path
         dirname = os.path.dirname(local_path)
         if not os.path.exists(dirname):
             os.makedirs(dirname)
         for provider in self.storage_providers:
             res = await provider.fetch(path, file_info)  # type: Any
             if res:
                 with res:
                     consumer = BackgroundFileConsumer(
                         open(local_path, "wb"), self.hs.get_reactor()
                     )

--- a/synapse/rest/media/v1/preview_url_resource.py
+++ b/synapse/rest/media/v1/preview_url_resource.py
@@ -53,21 +53,21 @@
         if results.scheme not in {"http", "https"}:
             raise ValueError("Insecure oEmbed glob scheme: %s" % (results.scheme,))
         pattern = urlparse.urlunparse(
             [
                 results.scheme,
                 re.escape(results.netloc).replace("\\*", "[a-zA-Z0-9_-]+"),
             ]
             + [re.escape(part).replace("\\*", ".+") for part in results[2:]]
         )
         _oembed_patterns[re.compile(pattern)] = endpoint
-@attr.s(slots=True)
+@attr.s
 class OEmbedResult:
     html = attr.ib(type=Optional[str])
     url = attr.ib(type=Optional[str])
     title = attr.ib(type=Optional[str])
     cache_age = attr.ib(type=int)
 class OEmbedError(Exception):
     """An error occurred processing the oEmbed object."""
 class PreviewUrlResource(DirectServeJsonResource):
     isLeaf = True
     def __init__(self, hs, media_repo, media_storage):
@@ -292,24 +292,24 @@
             if "thumbnail_url" in result:
                 oembed_result.url = result.get("thumbnail_url")
                 return oembed_result
             raise OEmbedError("Incompatible oEmbed information.")
         except OEmbedError as e:
             logger.warning("Error parsing oEmbed metadata from %s: %r", url, e)
             raise
         except Exception as e:
             logger.warning("Error downloading oEmbed metadata from %s: %r", url, e)
             raise OEmbedError() from e
-    async def _download_url(self, url: str, user):
+    async def _download_url(self, url, user):
         file_id = datetime.date.today().isoformat() + "_" + random_string(16)
         file_info = FileInfo(server_name=None, file_id=file_id, url_cache=True)
-        url_to_download = url  # type: Optional[str]
+        url_to_download = url
         oembed_url = self._get_oembed_url(url)
         if oembed_url:
             try:
                 oembed_result = await self._get_oembed_content(oembed_url, url)
                 if oembed_result.url:
                     url_to_download = oembed_result.url
                 elif oembed_result.html:
                     url_to_download = None
             except OEmbedError:
                 pass
@@ -339,27 +339,23 @@
                         % (traceback.format_exception_only(sys.exc_info()[0], e),),
                         Codes.UNKNOWN,
                     )
                 await finish()
                 if b"Content-Type" in headers:
                     media_type = headers[b"Content-Type"][0].decode("ascii")
                 else:
                     media_type = "application/octet-stream"
                 download_name = get_filename_from_headers(headers)
                 expires = ONE_HOUR
-                etag = (
-                    headers[b"ETag"][0].decode("ascii") if b"ETag" in headers else None
-                )
-        else:
-            assert oembed_result.html is not None
-            assert oembed_url is not None
-            html_bytes = oembed_result.html.encode("utf-8")
+                etag = headers["ETag"][0] if "ETag" in headers else None
+        else:
+            html_bytes = oembed_result.html.encode("utf-8")  # type: ignore
             with self.media_storage.store_into_file(file_info) as (f, fname, finish):
                 f.write(html_bytes)
                 await finish()
             media_type = "text/html"
             download_name = oembed_result.title
             length = len(html_bytes)
             expires = oembed_result.cache_age or ONE_HOUR
             uri = oembed_url
             code = 200
             etag = None

--- a/synapse/rest/media/v1/thumbnail_resource.py
+++ b/synapse/rest/media/v1/thumbnail_resource.py
@@ -1,12 +1,11 @@
 import logging
-from synapse.api.errors import SynapseError
 from synapse.http.server import DirectServeJsonResource, set_cors_headers
 from synapse.http.servlet import parse_integer, parse_string
 from ._base import (
     FileInfo,
     parse_media_id,
     respond_404,
     respond_with_file,
     respond_with_responder,
 )
 logger = logging.getLogger(__name__)
@@ -125,21 +124,21 @@
             desired_width,
             desired_height,
             desired_method,
             desired_type,
             url_cache=media_info["url_cache"],
         )
         if file_path:
             await respond_with_file(request, desired_type, file_path)
         else:
             logger.warning("Failed to generate thumbnail")
-            raise SynapseError(400, "Failed to generate thumbnail.")
+            respond_404(request)
     async def _select_or_generate_remote_thumbnail(
         self,
         request,
         server_name,
         media_id,
         desired_width,
         desired_height,
         desired_method,
         desired_type,
     ):
@@ -176,21 +175,21 @@
             media_id,
             desired_width,
             desired_height,
             desired_method,
             desired_type,
         )
         if file_path:
             await respond_with_file(request, desired_type, file_path)
         else:
             logger.warning("Failed to generate thumbnail")
-            raise SynapseError(400, "Failed to generate thumbnail.")
+            respond_404(request)
     async def _respond_remote_thumbnail(
         self, request, server_name, media_id, width, height, method, m_type
     ):
         media_info = await self.media_repo.get_remote_media_info(server_name, media_id)
         thumbnail_infos = await self.store.get_remote_media_thumbnails(
             server_name, media_id
         )
         if thumbnail_infos:
             thumbnail_info = self._select_thumbnail(
                 width, height, method, m_type, thumbnail_infos

--- a/synapse/rest/media/v1/thumbnailer.py
+++ b/synapse/rest/media/v1/thumbnailer.py
@@ -1,33 +1,28 @@
 import logging
 from io import BytesIO
-from PIL import Image
+from PIL import Image as Image
 logger = logging.getLogger(__name__)
 EXIF_ORIENTATION_TAG = 0x0112
 EXIF_TRANSPOSE_MAPPINGS = {
     2: Image.FLIP_LEFT_RIGHT,
     3: Image.ROTATE_180,
     4: Image.FLIP_TOP_BOTTOM,
     5: Image.TRANSPOSE,
     6: Image.ROTATE_270,
     7: Image.TRANSVERSE,
     8: Image.ROTATE_90,
 }
-class ThumbnailError(Exception):
-    """An error occurred generating a thumbnail."""
 class Thumbnailer:
     FORMATS = {"image/jpeg": "JPEG", "image/png": "PNG"}
     def __init__(self, input_path):
-        try:
-            self.image = Image.open(input_path)
-        except OSError as e:
-            raise ThumbnailError from e
+        self.image = Image.open(input_path)
         self.width, self.height = self.image.size
         self.transpose_method = None
         try:
             image_exif = self.image._getexif()
             if image_exif is not None:
                 image_orientation = image_exif.get(EXIF_ORIENTATION_TAG)
                 self.transpose_method = EXIF_TRANSPOSE_MAPPINGS.get(image_orientation)
         except Exception as e:
             logger.info("Error parsing image EXIF information: %s", e)
     def transpose(self):
@@ -42,21 +37,21 @@
             self.image.info["exif"] = None
         return self.image.size
     def aspect(self, max_width, max_height):
         """Calculate the largest size that preserves aspect ratio which
         fits within the given rectangle::
             (w_in / h_in) = (w_out / h_out)
             w_out = min(w_max, h_max * (w_in / h_in))
             h_out = min(h_max, w_max * (h_in / w_in))
         Args:
             max_width: The largest possible width.
-            max_height: The largest possible height.
+            max_height: The larget possible height.
         """
         if max_width * self.height < max_height * self.width:
             return max_width, (max_width * self.height) // self.width
         else:
             return (max_height * self.width) // self.height, max_height
     def _resize(self, width, height):
         if self.image.mode in ["1", "P"]:
             self.image = self.image.convert("RGB")
         return self.image.resize((width, height), Image.ANTIALIAS)
     def scale(self, width, height, output_type):
@@ -67,21 +62,21 @@
         scaled = self._resize(width, height)
         return self._encode_image(scaled, output_type)
     def crop(self, width, height, output_type):
         """Rescales and crops the image to the given dimensions preserving
         aspect::
             (w_in / h_in) = (w_scaled / h_scaled)
             w_scaled = max(w_out, h_out * (w_in / h_in))
             h_scaled = max(h_out, w_out * (h_in / w_in))
         Args:
             max_width: The largest possible width.
-            max_height: The largest possible height.
+            max_height: The larget possible height.
         Returns:
             BytesIO: the bytes of the encoded image ready to be written to disk
         """
         if width * self.height > height * self.width:
             scaled_height = (width * self.height) // self.width
             scaled_image = self._resize(width, scaled_height)
             crop_top = (scaled_height - height) // 2
             crop_bottom = height + crop_top
             cropped = scaled_image.crop((0, crop_top, width, crop_bottom))
         else:

--- a/synapse/rest/media/v1/upload_resource.py
+++ b/synapse/rest/media/v1/upload_resource.py
@@ -29,22 +29,20 @@
                 errcode=Codes.TOO_LARGE,
             )
         upload_name = parse_string(request, b"filename", encoding=None)
         if upload_name:
             try:
                 upload_name = upload_name.decode("utf8")
             except UnicodeDecodeError:
                 raise SynapseError(
                     msg="Invalid UTF-8 filename parameter: %r" % (upload_name), code=400
                 )
-        else:
-            upload_name = None
         headers = request.requestHeaders
         if headers.hasHeader(b"Content-Type"):
             media_type = headers.getRawHeaders(b"Content-Type")[0].decode("ascii")
         else:
             raise SynapseError(msg="Upload request missing 'Content-Type'", code=400)
         content_uri = await self.media_repo.create_content(
             media_type, upload_name, request.content, content_length, requester.user
         )
         logger.info("Uploaded content with URI %r", content_uri)
         respond_with_json(request, 200, {"content_uri": content_uri}, send_cors=True)

--- a/synapse/rest/saml2/response_resource.py
+++ b/synapse/rest/saml2/response_resource.py
@@ -1,13 +1,21 @@
-from synapse.http.server import DirectServeHtmlResource
+from twisted.python import failure
+from synapse.api.errors import SynapseError
+from synapse.http.server import DirectServeHtmlResource, return_html_error
 class SAML2ResponseResource(DirectServeHtmlResource):
     """A Twisted web resource which handles the SAML response"""
     isLeaf = 1
     def __init__(self, hs):
         super().__init__()
         self._saml_handler = hs.get_saml_handler()
+        self._error_html_template = hs.config.saml2.saml2_error_html_template
     async def _async_render_GET(self, request):
-        self._saml_handler._render_error(
-            request, "unexpected_get", "Unexpected GET request on /saml2/authn_response"
+        f = failure.Failure(
+            SynapseError(400, "Unexpected GET request on /saml2/authn_response")
         )
+        return_html_error(f, request, self._error_html_template)
     async def _async_render_POST(self, request):
-        await self._saml_handler.handle_saml_response(request)
+        try:
+            await self._saml_handler.handle_saml_response(request)
+        except Exception:
+            f = failure.Failure()
+            return_html_error(f, request, self._error_html_template)

--- a/synapse/rest/synapse/client/password_reset.py
+++ b//dev/null
@@ -1,85 +0,0 @@
-import logging
-from typing import TYPE_CHECKING, Tuple
-from twisted.web.http import Request
-from synapse.api.errors import ThreepidValidationError
-from synapse.config.emailconfig import ThreepidBehaviour
-from synapse.http.server import DirectServeHtmlResource
-from synapse.http.servlet import parse_string
-from synapse.util.stringutils import assert_valid_client_secret
-if TYPE_CHECKING:
-    from synapse.server import HomeServer
-logger = logging.getLogger(__name__)
-class PasswordResetSubmitTokenResource(DirectServeHtmlResource):
-    """Handles 3PID validation token submission
-    This resource gets mounted under /_synapse/client/password_reset/email/submit_token
-    """
-    isLeaf = 1
-    def __init__(self, hs: "HomeServer"):
-        """
-        Args:
-            hs: server
-        """
-        super().__init__()
-        self.clock = hs.get_clock()
-        self.store = hs.get_datastore()
-        self._local_threepid_handling_disabled_due_to_email_config = (
-            hs.config.local_threepid_handling_disabled_due_to_email_config
-        )
-        self._confirmation_email_template = (
-            hs.config.email_password_reset_template_confirmation_html
-        )
-        self._email_password_reset_template_success_html = (
-            hs.config.email_password_reset_template_success_html_content
-        )
-        self._failure_email_template = (
-            hs.config.email_password_reset_template_failure_html
-        )
-        assert hs.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL
-    async def _async_render_GET(self, request: Request) -> Tuple[int, bytes]:
-        sid = parse_string(request, "sid", required=True)
-        token = parse_string(request, "token", required=True)
-        client_secret = parse_string(request, "client_secret", required=True)
-        assert_valid_client_secret(client_secret)
-        template_vars = {
-            "sid": sid,
-            "token": token,
-            "client_secret": client_secret,
-        }
-        return (
-            200,
-            self._confirmation_email_template.render(**template_vars).encode("utf-8"),
-        )
-    async def _async_render_POST(self, request: Request) -> Tuple[int, bytes]:
-        sid = parse_string(request, "sid", required=True)
-        token = parse_string(request, "token", required=True)
-        client_secret = parse_string(request, "client_secret", required=True)
-        try:
-            next_link = await self.store.validate_threepid_session(
-                sid, client_secret, token, self.clock.time_msec()
-            )
-            if next_link:
-                if next_link.startswith("file:///"):
-                    logger.warning(
-                        "Not redirecting to next_link as it is a local file: address"
-                    )
-                else:
-                    next_link_bytes = next_link.encode("utf-8")
-                    request.setHeader("Location", next_link_bytes)
-                    return (
-                        302,
-                        (
-                            b'You are being redirected to <a src="%s">%s</a>.'
-                            % (next_link_bytes, next_link_bytes)
-                        ),
-                    )
-            html_bytes = self._email_password_reset_template_success_html.encode(
-                "utf-8"
-            )
-            status_code = 200
-        except ThreepidValidationError as e:
-            status_code = e.code
-            template_vars = {"failure_reason": e.msg}
-            html_bytes = self._failure_email_template.render(**template_vars).encode(
-                "utf-8"
-            )
-        return status_code, html_bytes

--- a/synapse/state/__init__.py
+++ b/synapse/state/__init__.py
@@ -1,47 +1,42 @@
-import heapq
 import logging
-from collections import defaultdict, namedtuple
+from collections import namedtuple
 from typing import (
-    Any,
     Awaitable,
-    Callable,
-    DefaultDict,
     Dict,
     Iterable,
     List,
     Optional,
     Sequence,
     Set,
-    Tuple,
     Union,
+    cast,
     overload,
 )
 import attr
 from frozendict import frozendict
-from prometheus_client import Counter, Histogram
+from prometheus_client import Histogram
 from typing_extensions import Literal
 from synapse.api.constants import EventTypes
 from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, StateResolutionVersions
 from synapse.events import EventBase
 from synapse.events.snapshot import EventContext
-from synapse.logging.context import ContextResourceUsage
 from synapse.logging.utils import log_function
 from synapse.state import v1, v2
 from synapse.storage.databases.main.events_worker import EventRedactBehaviour
 from synapse.storage.roommember import ProfileInfo
-from synapse.types import Collection, StateMap
+from synapse.types import Collection, MutableStateMap, StateMap
+from synapse.util import Clock
 from synapse.util.async_helpers import Linearizer
 from synapse.util.caches.expiringcache import ExpiringCache
 from synapse.util.metrics import Measure, measure_func
 logger = logging.getLogger(__name__)
-metrics_logger = logging.getLogger("synapse.state.metrics")
 state_groups_histogram = Histogram(
     "synapse_state_number_state_groups_in_resolution",
     "Number of state groups used when performing a state resolution",
     buckets=(1, 2, 3, 5, 7, 10, 15, 20, 50, 100, 200, 500, "+Inf"),
 )
 KeyStateTuple = namedtuple("KeyStateTuple", ("context", "type", "state_key"))
 EVICTION_TIMEOUT_SECONDS = 60 * 60
 _NEXT_STATE_ID = 1
 POWER_KEY = (EventTypes.PowerLevels, "")
 def _gen_state_id():
@@ -312,64 +307,47 @@
         state_sets: Collection[Iterable[EventBase]],
         event: EventBase,
     ) -> StateMap[EventBase]:
         logger.info(
             "Resolving state for %s with %d groups", event.room_id, len(state_sets)
         )
         state_set_ids = [
             {(ev.type, ev.state_key): ev.event_id for ev in st} for st in state_sets
         ]
         state_map = {ev.event_id: ev for st in state_sets for ev in st}
-        new_state = await self._state_resolution_handler.resolve_events_with_store(
-            event.room_id,
-            room_version,
-            state_set_ids,
-            event_map=state_map,
-            state_res_store=StateResolutionStore(self.store),
-        )
+        with Measure(self.clock, "state._resolve_events"):
+            new_state = await resolve_events_with_store(
+                self.clock,
+                event.room_id,
+                room_version,
+                state_set_ids,
+                event_map=state_map,
+                state_res_store=StateResolutionStore(self.store),
+            )
         return {key: state_map[ev_id] for key, ev_id in new_state.items()}
-@attr.s(slots=True)
-class _StateResMetrics:
-    """Keeps track of some usage metrics about state res."""
-    cpu_time = attr.ib(type=float, default=0.0)
-    db_time = attr.ib(type=float, default=0.0)
-    db_events = attr.ib(type=int, default=0)
-_biggest_room_by_cpu_counter = Counter(
-    "synapse_state_res_cpu_for_biggest_room_seconds",
-    "CPU time spent performing state resolution for the single most expensive "
-    "room for state resolution",
-)
-_biggest_room_by_db_counter = Counter(
-    "synapse_state_res_db_for_biggest_room_seconds",
-    "Database time spent performing state resolution for the single most "
-    "expensive room for state resolution",
-)
 class StateResolutionHandler:
     """Responsible for doing state conflict resolution.
     Note that the storage layer depends on this handler, so all functions must
     be storage-independent.
     """
     def __init__(self, hs):
         self.clock = hs.get_clock()
+        self._state_cache = None
         self.resolve_linearizer = Linearizer(name="state_resolve_lock")
         self._state_cache = ExpiringCache(
             cache_name="state_cache",
             clock=self.clock,
             max_len=100000,
             expiry_ms=EVICTION_TIMEOUT_SECONDS * 1000,
             iterable=True,
             reset_expiry_on_get=True,
         )
-        self._state_res_metrics = defaultdict(
-            _StateResMetrics
-        )  # type: DefaultDict[str, _StateResMetrics]
-        self.clock.looping_call(self._report_metrics, 120 * 1000)
     @log_function
     async def resolve_state_groups(
         self,
         room_id: str,
         room_version: str,
         state_groups_ids: Dict[int, StateMap[str]],
         event_map: Optional[Dict[str, EventBase]],
         state_res_store: "StateResolutionStore",
     ):
         """Resolves conflicts between a set of state groups
@@ -384,126 +362,60 @@
             event_map:
                 a dict from event_id to event, for any events that we happen to
                 have in flight (eg, those currently being persisted). This will be
                 used as a starting point fof finding the state we need; any missing
                 events will be requested via state_res_store.
                 If None, all events will be fetched via state_res_store.
             state_res_store
         Returns:
             The resolved state
         """
+        logger.debug("resolve_state_groups state_groups %s", state_groups_ids.keys())
         group_names = frozenset(state_groups_ids.keys())
         with (await self.resolve_linearizer.queue(group_names)):
-            cache = self._state_cache.get(group_names, None)
-            if cache:
-                return cache
+            if self._state_cache is not None:
+                cache = self._state_cache.get(group_names, None)
+                if cache:
+                    return cache
             logger.info(
-                "Resolving state for %s with groups %s", room_id, list(group_names),
+                "Resolving state for %s with %d groups", room_id, len(state_groups_ids)
             )
             state_groups_histogram.observe(len(state_groups_ids))
-            new_state = await self.resolve_events_with_store(
-                room_id,
-                room_version,
-                list(state_groups_ids.values()),
-                event_map=event_map,
-                state_res_store=state_res_store,
-            )
+            new_state = {}  # type: MutableStateMap[str]
+            conflicted_state = False
+            for st in state_groups_ids.values():
+                for key, e_id in st.items():
+                    if key in new_state:
+                        conflicted_state = True
+                        break
+                    new_state[key] = e_id
+                if conflicted_state:
+                    break
+            if conflicted_state:
+                logger.info("Resolving conflicted state for %r", room_id)
+                with Measure(self.clock, "state._resolve_events"):
+                    new_state = cast(
+                        MutableStateMap,
+                        await resolve_events_with_store(
+                            self.clock,
+                            room_id,
+                            room_version,
+                            list(state_groups_ids.values()),
+                            event_map=event_map,
+                            state_res_store=state_res_store,
+                        ),
+                    )
             with Measure(self.clock, "state.create_group_ids"):
                 cache = _make_state_cache_entry(new_state, state_groups_ids)
-            self._state_cache[group_names] = cache
+            if self._state_cache is not None:
+                self._state_cache[group_names] = cache
             return cache
-    async def resolve_events_with_store(
-        self,
-        room_id: str,
-        room_version: str,
-        state_sets: Sequence[StateMap[str]],
-        event_map: Optional[Dict[str, EventBase]],
-        state_res_store: "StateResolutionStore",
-    ) -> StateMap[str]:
-        """
-        Args:
-            room_id: the room we are working in
-            room_version: Version of the room
-            state_sets: List of dicts of (type, state_key) -> event_id,
-                which are the different state groups to resolve.
-            event_map:
-                a dict from event_id to event, for any events that we happen to
-                have in flight (eg, those currently being persisted). This will be
-                used as a starting point fof finding the state we need; any missing
-                events will be requested via state_map_factory.
-                If None, all events will be fetched via state_res_store.
-            state_res_store: a place to fetch events from
-        Returns:
-            a map from (type, state_key) to event_id.
-        """
-        try:
-            with Measure(self.clock, "state._resolve_events") as m:
-                v = KNOWN_ROOM_VERSIONS[room_version]
-                if v.state_res == StateResolutionVersions.V1:
-                    return await v1.resolve_events_with_store(
-                        room_id, state_sets, event_map, state_res_store.get_events
-                    )
-                else:
-                    return await v2.resolve_events_with_store(
-                        self.clock,
-                        room_id,
-                        room_version,
-                        state_sets,
-                        event_map,
-                        state_res_store,
-                    )
-        finally:
-            self._record_state_res_metrics(room_id, m.get_resource_usage())
-    def _record_state_res_metrics(self, room_id: str, rusage: ContextResourceUsage):
-        room_metrics = self._state_res_metrics[room_id]
-        room_metrics.cpu_time += rusage.ru_utime + rusage.ru_stime
-        room_metrics.db_time += rusage.db_txn_duration_sec
-        room_metrics.db_events += rusage.evt_db_fetch_count
-    def _report_metrics(self):
-        if not self._state_res_metrics:
-            return
-        self._report_biggest(
-            lambda i: i.cpu_time, "CPU time", _biggest_room_by_cpu_counter,
-        )
-        self._report_biggest(
-            lambda i: i.db_time, "DB time", _biggest_room_by_db_counter,
-        )
-        self._state_res_metrics.clear()
-    def _report_biggest(
-        self,
-        extract_key: Callable[[_StateResMetrics], Any],
-        metric_name: str,
-        prometheus_counter_metric: Counter,
-    ) -> None:
-        """Report metrics on the biggest rooms for state res
-        Args:
-            extract_key: a callable which, given a _StateResMetrics, extracts a single
-                metric to sort by.
-            metric_name: the name of the metric we have extracted, for the log line
-            prometheus_counter_metric: a prometheus metric recording the sum of the
-                the extracted metric
-        """
-        n_to_log = 10
-        if not metrics_logger.isEnabledFor(logging.DEBUG):
-            n_to_log = 1
-        items = self._state_res_metrics.items()
-        biggest = heapq.nlargest(
-            n_to_log, items, key=lambda i: extract_key(i[1])
-        )  # type: List[Tuple[str, _StateResMetrics]]
-        metrics_logger.debug(
-            "%i biggest rooms for state-res by %s: %s",
-            len(biggest),
-            metric_name,
-            ["%s (%gs)" % (r, extract_key(m)) for (r, m) in biggest],
-        )
-        _, biggest_metrics = biggest[0]
-        prometheus_counter_metric.inc(extract_key(biggest_metrics))
 def _make_state_cache_entry(
     new_state: StateMap[str], state_groups_ids: Dict[int, StateMap[str]]
 ) -> _StateCacheEntry:
     """Given a resolved state, and a set of input state groups, pick one to base
     a new state group on (if any), and return an appropriately-constructed
     _StateCacheEntry.
     Args:
         new_state: resolved state map (mapping from (type, state_key) to event_id)
         state_groups_ids:
             map from state group id to the state in that state group (where
@@ -521,21 +433,54 @@
     prev_group = None
     delta_ids = None
     for old_group, old_state in state_groups_ids.items():
         n_delta_ids = {k: v for k, v in new_state.items() if old_state.get(k) != v}
         if not delta_ids or len(n_delta_ids) < len(delta_ids):
             prev_group = old_group
             delta_ids = n_delta_ids
     return _StateCacheEntry(
         state=new_state, state_group=None, prev_group=prev_group, delta_ids=delta_ids
     )
-@attr.s(slots=True)
+def resolve_events_with_store(
+    clock: Clock,
+    room_id: str,
+    room_version: str,
+    state_sets: Sequence[StateMap[str]],
+    event_map: Optional[Dict[str, EventBase]],
+    state_res_store: "StateResolutionStore",
+) -> Awaitable[StateMap[str]]:
+    """
+    Args:
+        room_id: the room we are working in
+        room_version: Version of the room
+        state_sets: List of dicts of (type, state_key) -> event_id,
+            which are the different state groups to resolve.
+        event_map:
+            a dict from event_id to event, for any events that we happen to
+            have in flight (eg, those currently being persisted). This will be
+            used as a starting point fof finding the state we need; any missing
+            events will be requested via state_map_factory.
+            If None, all events will be fetched via state_res_store.
+        state_res_store: a place to fetch events from
+    Returns:
+        a map from (type, state_key) to event_id.
+    """
+    v = KNOWN_ROOM_VERSIONS[room_version]
+    if v.state_res == StateResolutionVersions.V1:
+        return v1.resolve_events_with_store(
+            room_id, state_sets, event_map, state_res_store.get_events
+        )
+    else:
+        return v2.resolve_events_with_store(
+            clock, room_id, room_version, state_sets, event_map, state_res_store
+        )
+@attr.s
 class StateResolutionStore:
     """Interface that allows state resolution algorithms to access the database
     in well defined way.
     Args:
         store (DataStore)
     """
     store = attr.ib()
     def get_events(
         self, event_ids: Iterable[str], allow_rejected: bool = False
     ) -> Awaitable[Dict[str, EventBase]]:

--- a/synapse/storage/__init__.py
+++ b/synapse/storage/__init__.py
@@ -14,15 +14,13 @@
 from synapse.storage.databases.main import DataStore
 from synapse.storage.persist_events import EventsPersistenceStorage
 from synapse.storage.purge_events import PurgeEventsStorage
 from synapse.storage.state import StateGroupStorage
 __all__ = ["DataStores", "DataStore"]
 class Storage:
     """The high level interfaces for talking to various storage layers.
     """
     def __init__(self, hs, stores: Databases):
         self.main = stores.main
+        self.persistence = EventsPersistenceStorage(hs, stores)
         self.purge_events = PurgeEventsStorage(hs, stores)
         self.state = StateGroupStorage(hs, stores)
-        self.persistence = None
-        if stores.persist_events:
-            self.persistence = EventsPersistenceStorage(hs, stores)

--- a/synapse/storage/database.py
+++ b/synapse/storage/database.py
@@ -272,35 +272,20 @@
     def new_transaction(
         self,
         conn: Connection,
         desc: str,
         after_callbacks: List[_CallbackListEntry],
         exception_callbacks: List[_CallbackListEntry],
         func: "Callable[..., R]",
         *args: Any,
         **kwargs: Any
     ) -> R:
-        """Start a new database transaction with the given connection.
-        Note: The given func may be called multiple times under certain
-        failure modes. This is normally fine when in a standard transaction,
-        but care must be taken if the connection is in `autocommit` mode that
-        the function will correctly handle being aborted and retried half way
-        through its execution.
-        Args:
-            conn
-            desc
-            after_callbacks
-            exception_callbacks
-            func
-            *args
-            **kwargs
-        """
         start = monotonic_time()
         txn_id = self._TXN_ID
         self._TXN_ID = (self._TXN_ID + 1) % (MAX_TXN_ID)
         name = "%s-%x" % (desc, txn_id)
         transaction_logger.debug("[TXN START] {%s}", name)
         try:
             i = 0
             N = 5
             while True:
                 cursor = LoggingTransaction(
@@ -348,112 +333,83 @@
             raise
         finally:
             end = monotonic_time()
             duration = end - start
             current_context().add_database_transaction(duration)
             transaction_logger.debug("[TXN END] {%s} %f sec", name, duration)
             self._current_txn_total_time += duration
             self._txn_perf_counters.update(desc, duration)
             sql_txn_timer.labels(desc).observe(duration)
     async def runInteraction(
-        self,
-        desc: str,
-        func: "Callable[..., R]",
-        *args: Any,
-        db_autocommit: bool = False,
-        **kwargs: Any
+        self, desc: str, func: "Callable[..., R]", *args: Any, **kwargs: Any
     ) -> R:
         """Starts a transaction on the database and runs a given function
         Arguments:
             desc: description of the transaction, for logging and metrics
             func: callback function, which will be called with a
                 database transaction (twisted.enterprise.adbapi.Transaction) as
                 its first argument, followed by `args` and `kwargs`.
-            db_autocommit: Whether to run the function in "autocommit" mode,
-                i.e. outside of a transaction. This is useful for transactions
-                that are only a single query.
-                Currently, this is only implemented for Postgres. SQLite will still
-                run the function inside a transaction.
-                WARNING: This means that if func fails half way through then
-                the changes will *not* be rolled back. `func` may also get
-                called multiple times if the transaction is retried, so must
-                correctly handle that case.
             args: positional args to pass to `func`
             kwargs: named args to pass to `func`
         Returns:
             The result of func
         """
         after_callbacks = []  # type: List[_CallbackListEntry]
         exception_callbacks = []  # type: List[_CallbackListEntry]
         if not current_context():
             logger.warning("Starting db txn '%s' from sentinel context", desc)
         try:
             result = await self.runWithConnection(
                 self.new_transaction,
                 desc,
                 after_callbacks,
                 exception_callbacks,
                 func,
                 *args,
-                db_autocommit=db_autocommit,
                 **kwargs
             )
             for after_callback, after_args, after_kwargs in after_callbacks:
                 after_callback(*after_args, **after_kwargs)
         except:  # noqa: E722, as we reraise the exception this is fine.
             for after_callback, after_args, after_kwargs in exception_callbacks:
                 after_callback(*after_args, **after_kwargs)
             raise
         return cast(R, result)
     async def runWithConnection(
-        self,
-        func: "Callable[..., R]",
-        *args: Any,
-        db_autocommit: bool = False,
-        **kwargs: Any
+        self, func: "Callable[..., R]", *args: Any, **kwargs: Any
     ) -> R:
         """Wraps the .runWithConnection() method on the underlying db_pool.
         Arguments:
             func: callback function, which will be called with a
                 database connection (twisted.enterprise.adbapi.Connection) as
                 its first argument, followed by `args` and `kwargs`.
             args: positional args to pass to `func`
-            db_autocommit: Whether to run the function in "autocommit" mode,
-                i.e. outside of a transaction. This is useful for transaction
-                that are only a single query. Currently only affects postgres.
             kwargs: named args to pass to `func`
         Returns:
             The result of func
         """
         parent_context = current_context()  # type: Optional[LoggingContextOrSentinel]
         if not parent_context:
             logger.warning(
                 "Starting db connection from sentinel context: metrics will be lost"
             )
             parent_context = None
         start_time = monotonic_time()
         def inner_func(conn, *args, **kwargs):
-            assert not self.engine.in_transaction(conn)
             with LoggingContext("runWithConnection", parent_context) as context:
                 sched_duration_sec = monotonic_time() - start_time
                 sql_scheduling_timer.observe(sched_duration_sec)
                 context.add_database_scheduled(sched_duration_sec)
                 if self.engine.is_connection_closed(conn):
                     logger.debug("Reconnecting closed database connection")
                     conn.reconnect()
-                try:
-                    if db_autocommit:
-                        self.engine.attempt_to_set_autocommit(conn, True)
-                    return func(conn, *args, **kwargs)
-                finally:
-                    if db_autocommit:
-                        self.engine.attempt_to_set_autocommit(conn, False)
+                return func(conn, *args, **kwargs)
         return await make_deferred_yieldable(
             self._db_pool.runWithConnection(inner_func, *args, **kwargs)
         )
     @staticmethod
     def cursor_to_dict(cursor: Cursor) -> List[Dict[str, Any]]:
         """Converts a SQL cursor into an list of dicts.
         Args:
             cursor: The DBAPI cursor which has executed a query.
         Returns:
             A list of dicts where the key is the column header.
@@ -738,21 +694,21 @@
             latter,
         )
         txn.execute(sql, list(allvalues.values()))
     def simple_upsert_many_txn(
         self,
         txn: LoggingTransaction,
         table: str,
         key_names: Collection[str],
         key_values: Collection[Iterable[Any]],
         value_names: Collection[str],
-        value_values: Iterable[Iterable[Any]],
+        value_values: Iterable[Iterable[str]],
     ) -> None:
         """
         Upsert, many times.
         Args:
             table: The table to upsert into
             key_names: The key column names.
             key_values: A list of each row's key column values.
             value_names: The value column names
             value_values: A list of each row's value column values.
                 Ignored if value_names is empty.
@@ -765,21 +721,21 @@
             return self.simple_upsert_many_txn_emulated(
                 txn, table, key_names, key_values, value_names, value_values
             )
     def simple_upsert_many_txn_emulated(
         self,
         txn: LoggingTransaction,
         table: str,
         key_names: Iterable[str],
         key_values: Collection[Iterable[Any]],
         value_names: Collection[str],
-        value_values: Iterable[Iterable[Any]],
+        value_values: Iterable[Iterable[str]],
     ) -> None:
         """
         Upsert, many times, but without native UPSERT support or batching.
         Args:
             table: The table to upsert into
             key_names: The key column names.
             key_values: A list of each row's key column values.
             value_names: The value column names
             value_values: A list of each row's value column values.
                 Ignored if value_names is empty.

--- a/synapse/storage/databases/__init__.py
+++ b/synapse/storage/databases/__init__.py
@@ -31,21 +31,21 @@
                     db_conn, engine, hs.config, databases=database_config.databases,
                 )
                 database = DatabasePool(hs, database_config, engine)
                 if "main" in database_config.databases:
                     logger.info(
                         "[database config %r]: Starting 'main' database", db_name
                     )
                     if main:
                         raise Exception("'main' data store already configured")
                     main = main_store_class(database, db_conn, hs)
-                    if hs.get_instance_name() in hs.config.worker.writers.events:
+                    if hs.config.worker.writers.events == hs.get_instance_name():
                         persist_events = PersistEventsStore(hs, database, main)
                 if "state" in database_config.databases:
                     logger.info(
                         "[database config %r]: Starting 'state' database", db_name
                     )
                     if state:
                         raise Exception("'state' data store already configured")
                     state = StateGroupDataStore(database, db_conn, hs)
                 db_conn.commit()
                 self.databases.append(database)

--- a/synapse/storage/databases/main/__init__.py
+++ b/synapse/storage/databases/main/__init__.py
@@ -131,31 +131,29 @@
         self._pushers_id_gen = StreamIdGenerator(
             db_conn, "pushers", "id", extra_tables=[("deleted_pushers", "stream_id")]
         )
         self._group_updates_id_gen = StreamIdGenerator(
             db_conn, "local_group_updates", "stream_id"
         )
         if isinstance(self.database_engine, PostgresEngine):
             self._cache_id_gen = MultiWriterIdGenerator(
                 db_conn,
                 database,
-                stream_name="caches",
-                instance_name=hs.get_instance_name(),
+                instance_name="master",
                 table="cache_invalidation_stream_by_instance",
                 instance_column="instance_name",
                 id_column="stream_id",
                 sequence_name="cache_invalidation_stream_seq",
-                writers=[],
             )
         else:
             self._cache_id_gen = None
-        super().__init__(database, db_conn, hs)
+        super(DataStore, self).__init__(database, db_conn, hs)
         self._presence_on_startup = self._get_active_presence(db_conn)
         presence_cache_prefill, min_presence_val = self.db_pool.get_cache_dict(
             db_conn,
             "presence_stream",
             entity_column="user_id",
             stream_column="stream_id",
             max_value=self._presence_id_gen.get_current_token(),
         )
         self.presence_stream_cache = StreamChangeCache(
             "PresenceStreamChangeCache",

--- a/synapse/storage/databases/main/account_data.py
+++ b/synapse/storage/databases/main/account_data.py
@@ -2,30 +2,31 @@
 import logging
 from typing import Dict, List, Optional, Tuple
 from synapse.storage._base import SQLBaseStore, db_to_json
 from synapse.storage.database import DatabasePool
 from synapse.storage.util.id_generators import StreamIdGenerator
 from synapse.types import JsonDict
 from synapse.util import json_encoder
 from synapse.util.caches.descriptors import _CacheContext, cached
 from synapse.util.caches.stream_change_cache import StreamChangeCache
 logger = logging.getLogger(__name__)
-class AccountDataWorkerStore(SQLBaseStore, metaclass=abc.ABCMeta):
+class AccountDataWorkerStore(SQLBaseStore):
     """This is an abstract base class where subclasses must implement
     `get_max_account_data_stream_id` which can be called in the initializer.
     """
+    __metaclass__ = abc.ABCMeta
     def __init__(self, database: DatabasePool, db_conn, hs):
         account_max = self.get_max_account_data_stream_id()
         self._account_data_stream_cache = StreamChangeCache(
             "AccountDataAndTagsChangeCache", account_max
         )
-        super().__init__(database, db_conn, hs)
+        super(AccountDataWorkerStore, self).__init__(database, db_conn, hs)
     @abc.abstractmethod
     def get_max_account_data_stream_id(self):
         """Get the current max stream ID for account data stream
         Returns:
             int
         """
         raise NotImplementedError()
     @cached()
     async def get_account_data_for_user(
         self, user_id: str
@@ -234,41 +235,41 @@
     def __init__(self, database: DatabasePool, db_conn, hs):
         self._account_data_id_gen = StreamIdGenerator(
             db_conn,
             "account_data_max_stream_id",
             "stream_id",
             extra_tables=[
                 ("room_account_data", "stream_id"),
                 ("room_tags_revisions", "stream_id"),
             ],
         )
-        super().__init__(database, db_conn, hs)
+        super(AccountDataStore, self).__init__(database, db_conn, hs)
     def get_max_account_data_stream_id(self) -> int:
         """Get the current max stream id for the private user data stream
         Returns:
             The maximum stream ID.
         """
         return self._account_data_id_gen.get_current_token()
     async def add_account_data_to_room(
         self, user_id: str, room_id: str, account_data_type: str, content: JsonDict
     ) -> int:
         """Add some account_data to a room for a user.
         Args:
             user_id: The user to add a tag for.
             room_id: The room to add a tag for.
             account_data_type: The type of account_data to add.
             content: A json object to associate with the tag.
         Returns:
             The maximum stream ID.
         """
         content_json = json_encoder.encode(content)
-        async with self._account_data_id_gen.get_next() as next_id:
+        with await self._account_data_id_gen.get_next() as next_id:
             await self.db_pool.simple_upsert(
                 desc="add_room_account_data",
                 table="room_account_data",
                 keyvalues={
                     "user_id": user_id,
                     "room_id": room_id,
                     "account_data_type": account_data_type,
                 },
                 values={"stream_id": next_id, "content": content_json},
                 lock=False,
@@ -286,21 +287,21 @@
     ) -> int:
         """Add some account_data to a room for a user.
         Args:
             user_id: The user to add a tag for.
             account_data_type: The type of account_data to add.
             content: A json object to associate with the tag.
         Returns:
             The maximum stream ID.
         """
         content_json = json_encoder.encode(content)
-        async with self._account_data_id_gen.get_next() as next_id:
+        with await self._account_data_id_gen.get_next() as next_id:
             await self.db_pool.simple_upsert(
                 desc="add_user_account_data",
                 table="account_data",
                 keyvalues={"user_id": user_id, "account_data_type": account_data_type},
                 values={"stream_id": next_id, "content": content_json},
                 lock=False,
             )
             await self._update_max_stream_id(next_id)
             self._account_data_stream_cache.entity_has_changed(user_id, next_id)
             self.get_account_data_for_user.invalidate((user_id,))

--- a/synapse/storage/databases/main/appservice.py
+++ b/synapse/storage/databases/main/appservice.py
@@ -18,21 +18,21 @@
         exclusive_user_regex = re.compile(exclusive_user_regex)
     else:
         exclusive_user_regex = None
     return exclusive_user_regex
 class ApplicationServiceWorkerStore(SQLBaseStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
         self.services_cache = load_appservices(
             hs.hostname, hs.config.app_service_config_files
         )
         self.exclusive_user_regex = _make_exclusive_regex(self.services_cache)
-        super().__init__(database, db_conn, hs)
+        super(ApplicationServiceWorkerStore, self).__init__(database, db_conn, hs)
     def get_app_services(self):
         return self.services_cache
     def get_if_app_services_interested_in_user(self, user_id):
         """Check if the user is one associated with an app service (exclusively)
         """
         if self.exclusive_user_regex:
             return bool(self.exclusive_user_regex.match(user_id))
         else:
             return False
     def get_app_service_by_user_id(self, user_id):

--- a/synapse/storage/databases/main/client_ips.py
+++ b/synapse/storage/databases/main/client_ips.py
@@ -1,21 +1,21 @@
 import logging
 from typing import Dict, Optional, Tuple
 from synapse.metrics.background_process_metrics import wrap_as_background_process
 from synapse.storage._base import SQLBaseStore
 from synapse.storage.database import DatabasePool, make_tuple_comparison_clause
 from synapse.util.caches.descriptors import Cache
 logger = logging.getLogger(__name__)
 LAST_SEEN_GRANULARITY = 120 * 1000
 class ClientIpBackgroundUpdateStore(SQLBaseStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(ClientIpBackgroundUpdateStore, self).__init__(database, db_conn, hs)
         self.db_pool.updates.register_background_index_update(
             "user_ips_device_index",
             index_name="user_ips_device_id",
             table="user_ips",
             columns=["user_id", "device_id", "last_seen"],
         )
         self.db_pool.updates.register_background_index_update(
             "user_ips_last_seen_index",
             index_name="user_ips_last_seen",
             table="user_ips",
@@ -200,21 +200,21 @@
             "_devices_last_seen_update", _devices_last_seen_update_txn
         )
         if not updated:
             await self.db_pool.updates._end_background_update("devices_last_seen")
         return updated
 class ClientIpStore(ClientIpBackgroundUpdateStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
         self.client_ip_last_seen = Cache(
             name="client_ip_last_seen", keylen=4, max_entries=50000
         )
-        super().__init__(database, db_conn, hs)
+        super(ClientIpStore, self).__init__(database, db_conn, hs)
         self.user_ips_max_age = hs.config.user_ips_max_age
         self._batch_row_update = {}
         self._client_ip_looper = self._clock.looping_call(
             self._update_client_ips_batch, 5 * 1000
         )
         self.hs.get_reactor().addSystemEventTrigger(
             "before", "shutdown", self._update_client_ips_batch
         )
         if self.user_ips_max_age:
             self._clock.looping_call(self._prune_old_user_ips, 5 * 1000)

--- a/synapse/storage/databases/main/deviceinbox.py
+++ b/synapse/storage/databases/main/deviceinbox.py
@@ -211,42 +211,42 @@
             if len(updates) >= limit:
                 upto_token = updates[-1][0]
                 limited = True
             return updates, upto_token, limited
         return await self.db_pool.runInteraction(
             "get_all_new_device_messages", get_all_new_device_messages_txn
         )
 class DeviceInboxBackgroundUpdateStore(SQLBaseStore):
     DEVICE_INBOX_STREAM_ID = "device_inbox_stream_drop"
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(DeviceInboxBackgroundUpdateStore, self).__init__(database, db_conn, hs)
         self.db_pool.updates.register_background_index_update(
             "device_inbox_stream_index",
             index_name="device_inbox_stream_id_user_id",
             table="device_inbox",
             columns=["stream_id", "user_id"],
         )
         self.db_pool.updates.register_background_update_handler(
             self.DEVICE_INBOX_STREAM_ID, self._background_drop_index_device_inbox
         )
     async def _background_drop_index_device_inbox(self, progress, batch_size):
         def reindex_txn(conn):
             txn = conn.cursor()
             txn.execute("DROP INDEX IF EXISTS device_inbox_stream_id")
             txn.close()
         await self.db_pool.runWithConnection(reindex_txn)
         await self.db_pool.updates._end_background_update(self.DEVICE_INBOX_STREAM_ID)
         return 1
 class DeviceInboxStore(DeviceInboxWorkerStore, DeviceInboxBackgroundUpdateStore):
     DEVICE_INBOX_STREAM_ID = "device_inbox_stream_drop"
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(DeviceInboxStore, self).__init__(database, db_conn, hs)
         self._last_device_delete_cache = ExpiringCache(
             cache_name="last_device_delete_cache",
             clock=self._clock,
             max_len=10000,
             expiry_ms=30 * 60 * 1000,
         )
     @trace
     async def add_messages_to_device_inbox(
         self,
         local_messages_by_user_then_device: dict,
@@ -268,21 +268,21 @@
             sql = (
                 "INSERT INTO device_federation_outbox"
                 " (destination, stream_id, queued_ts, messages_json)"
                 " VALUES (?,?,?,?)"
             )
             rows = []
             for destination, edu in remote_messages_by_destination.items():
                 edu_json = json_encoder.encode(edu)
                 rows.append((destination, stream_id, now_ms, edu_json))
             txn.executemany(sql, rows)
-        async with self._device_inbox_id_gen.get_next() as stream_id:
+        with await self._device_inbox_id_gen.get_next() as stream_id:
             now_ms = self.clock.time_msec()
             await self.db_pool.runInteraction(
                 "add_messages_to_device_inbox", add_messages_txn, now_ms, stream_id
             )
             for user_id in local_messages_by_user_then_device.keys():
                 self._device_inbox_stream_cache.entity_has_changed(user_id, stream_id)
             for destination in remote_messages_by_destination.keys():
                 self._device_federation_outbox_stream_cache.entity_has_changed(
                     destination, stream_id
                 )
@@ -305,21 +305,21 @@
                 table="device_federation_inbox",
                 values={
                     "origin": origin,
                     "message_id": message_id,
                     "received_ts": now_ms,
                 },
             )
             self._add_messages_to_local_device_inbox_txn(
                 txn, stream_id, local_messages_by_user_then_device
             )
-        async with self._device_inbox_id_gen.get_next() as stream_id:
+        with await self._device_inbox_id_gen.get_next() as stream_id:
             now_ms = self.clock.time_msec()
             await self.db_pool.runInteraction(
                 "add_messages_from_remote_to_device_inbox",
                 add_messages_txn,
                 now_ms,
                 stream_id,
             )
             for user_id in local_messages_by_user_then_device.keys():
                 self._device_inbox_stream_cache.entity_has_changed(user_id, stream_id)
         return stream_id

--- a/synapse/storage/databases/main/devices.py
+++ b/synapse/storage/databases/main/devices.py
@@ -276,21 +276,21 @@
     async def add_user_signature_change_to_streams(
         self, from_user_id: str, user_ids: List[str]
     ) -> int:
         """Persist that a user has made new signatures
         Args:
             from_user_id: the user who made the signatures
             user_ids: the users who were signed
         Returns:
             THe new stream ID.
         """
-        async with self._device_list_id_gen.get_next() as stream_id:
+        with await self._device_list_id_gen.get_next() as stream_id:
             await self.db_pool.runInteraction(
                 "add_user_sig_change_to_streams",
                 self._add_user_signature_change_txn,
                 from_user_id,
                 user_ids,
                 stream_id,
             )
         return stream_id
     def _add_user_signature_change_txn(
         self,
@@ -365,30 +365,31 @@
         devices = await self.db_pool.simple_select_list(
             table="device_lists_remote_cache",
             keyvalues={"user_id": user_id},
             retcols=("device_id", "content"),
             desc="get_cached_devices_for_user",
         )
         return {
             device["device_id"]: db_to_json(device["content"]) for device in devices
         }
     async def get_users_whose_devices_changed(
-        self, from_key: int, user_ids: Iterable[str]
+        self, from_key: str, user_ids: Iterable[str]
     ) -> Set[str]:
         """Get set of users whose devices have changed since `from_key` that
         are in the given list of user_ids.
         Args:
             from_key: The device lists stream token
             user_ids: The user IDs to query for devices.
         Returns:
             The set of user_ids whose devices have changed since `from_key`
         """
+        from_key = int(from_key)
         to_check = self._device_list_stream_cache.get_entities_changed(
             user_ids, from_key
         )
         if not to_check:
             return set()
         def _get_users_whose_devices_changed_txn(txn):
             changes = set()
             sql = """
                 SELECT DISTINCT user_id FROM device_lists_stream
                 WHERE stream_id > ?
@@ -398,30 +399,31 @@
                 clause, args = make_in_list_sql_clause(
                     txn.database_engine, "user_id", chunk
                 )
                 txn.execute(sql + clause, (from_key,) + tuple(args))
                 changes.update(user_id for user_id, in txn)
             return changes
         return await self.db_pool.runInteraction(
             "get_users_whose_devices_changed", _get_users_whose_devices_changed_txn
         )
     async def get_users_whose_signatures_changed(
-        self, user_id: str, from_key: int
+        self, user_id: str, from_key: str
     ) -> Set[str]:
         """Get the users who have new cross-signing signatures made by `user_id` since
         `from_key`.
         Args:
             user_id: the user who made the signatures
             from_key: The device lists stream token
         Returns:
             A set of user IDs with updated signatures.
         """
+        from_key = int(from_key)
         if self._user_signature_stream_cache.has_entity_changed(user_id, from_key):
             sql = """
                 SELECT DISTINCT user_ids FROM user_signature_stream
                 WHERE from_user_id = ? AND stream_id > ?
             """
             rows = await self.db_pool.execute(
                 "get_users_whose_signatures_changed", None, sql, user_id, from_key
             )
             return {user for row in rows for user in db_to_json(row[0])}
         else:
@@ -545,21 +547,21 @@
             )
             self._invalidate_cache_and_stream(
                 txn, self.get_device_list_last_stream_id_for_remote, (user_id,)
             )
         await self.db_pool.runInteraction(
             "mark_remote_user_device_list_as_unsubscribed",
             _mark_remote_user_device_list_as_unsubscribed_txn,
         )
 class DeviceBackgroundUpdateStore(SQLBaseStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(DeviceBackgroundUpdateStore, self).__init__(database, db_conn, hs)
         self.db_pool.updates.register_background_index_update(
             "device_lists_stream_idx",
             index_name="device_lists_stream_user_id",
             table="device_lists_stream",
             columns=["user_id", "device_id"],
         )
         self.db_pool.updates.register_background_index_update(
             "device_lists_remote_cache_unique_idx",
             index_name="device_lists_remote_cache_unique_id",
             table="device_lists_remote_cache",
@@ -639,21 +641,21 @@
         rows = await self.db_pool.runInteraction(
             BG_UPDATE_REMOVE_DUP_OUTBOUND_POKES, _txn
         )
         if not rows:
             await self.db_pool.updates._end_background_update(
                 BG_UPDATE_REMOVE_DUP_OUTBOUND_POKES
             )
         return rows
 class DeviceStore(DeviceWorkerStore, DeviceBackgroundUpdateStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(DeviceStore, self).__init__(database, db_conn, hs)
         self.device_id_exists_cache = Cache(
             name="device_id_exists", keylen=2, max_entries=10000
         )
         self._clock.looping_call(self._prune_old_outbound_device_pokes, 60 * 60 * 1000)
     async def store_device(
         self, user_id: str, device_id: str, initial_device_display_name: str
     ) -> bool:
         """Ensure the given device is known; add it to the store if not
         Args:
             user_id: id of user associated with the device
@@ -862,34 +864,34 @@
             txn, table="device_lists_remote_resync", keyvalues={"user_id": user_id},
         )
     async def add_device_change_to_streams(
         self, user_id: str, device_ids: Collection[str], hosts: List[str]
     ):
         """Persist that a user's devices have been updated, and which hosts
         (if any) should be poked.
         """
         if not device_ids:
             return
-        async with self._device_list_id_gen.get_next_mult(
+        with await self._device_list_id_gen.get_next_mult(
             len(device_ids)
         ) as stream_ids:
             await self.db_pool.runInteraction(
                 "add_device_change_to_stream",
                 self._add_device_change_to_stream_txn,
                 user_id,
                 device_ids,
                 stream_ids,
             )
         if not hosts:
             return stream_ids[-1]
         context = get_active_span_text_map()
-        async with self._device_list_id_gen.get_next_mult(
+        with await self._device_list_id_gen.get_next_mult(
             len(hosts) * len(device_ids)
         ) as stream_ids:
             await self.db_pool.runInteraction(
                 "add_device_outbound_poke_to_stream",
                 self._add_device_outbound_poke_to_stream_txn,
                 user_id,
                 device_ids,
                 hosts,
                 stream_ids,
                 context,

--- a/synapse/storage/databases/main/end_to_end_keys.py
+++ b/synapse/storage/databases/main/end_to_end_keys.py
@@ -6,21 +6,21 @@
 from synapse.logging.opentracing import log_kv, set_tag, trace
 from synapse.storage._base import SQLBaseStore, db_to_json
 from synapse.storage.database import make_in_list_sql_clause
 from synapse.storage.types import Cursor
 from synapse.types import JsonDict
 from synapse.util import json_encoder
 from synapse.util.caches.descriptors import cached, cachedList
 from synapse.util.iterutils import batch_iter
 if TYPE_CHECKING:
     from synapse.handlers.e2e_keys import SignatureListItem
-@attr.s(slots=True)
+@attr.s
 class DeviceKeyLookupResult:
     """The type returned by get_e2e_device_keys_and_signatures"""
     display_name = attr.ib(type=Optional[str])
     keys = attr.ib(type=Optional[JsonDict])
 class EndToEndKeyWorkerStore(SQLBaseStore):
     async def get_e2e_device_keys_for_federation_query(
         self, user_id: str
     ) -> Tuple[int, List[JsonDict]]:
         """Get all devices (with any device keys) for a user
         Returns:
@@ -635,21 +635,21 @@
         self._invalidate_cache_and_stream(
             txn, self._get_bare_e2e_cross_signing_keys, (user_id,)
         )
     async def set_e2e_cross_signing_key(self, user_id, key_type, key):
         """Set a user's cross-signing key.
         Args:
             user_id (str): the user to set the user-signing key for
             key_type (str): the type of cross-signing key to set
             key (dict): the key data
         """
-        async with self._cross_signing_id_gen.get_next() as stream_id:
+        with await self._cross_signing_id_gen.get_next() as stream_id:
             return await self.db_pool.runInteraction(
                 "add_e2e_cross_signing_key",
                 self._set_e2e_cross_signing_key_txn,
                 user_id,
                 key_type,
                 key,
                 stream_id,
             )
     async def store_e2e_cross_signing_signatures(
         self, user_id: str, signatures: "Iterable[SignatureListItem]"

--- a/synapse/storage/databases/main/event_federation.py
+++ b/synapse/storage/databases/main/event_federation.py
@@ -270,21 +270,21 @@
             stream_ordering = min(last_change, stream_ordering)
         return await self._get_forward_extremeties_for_room(room_id, stream_ordering)
     @cached(max_entries=5000, num_args=2)
     async def _get_forward_extremeties_for_room(self, room_id, stream_ordering):
         """For a given room_id and stream_ordering, return the forward
         extremeties of the room at that point in "time".
         Throws a StoreError if we have since purged the index for
         stream_orderings from that point.
         """
         if stream_ordering <= self.stream_ordering_month_ago:
-            raise StoreError(400, "stream_ordering too old %s" % (stream_ordering,))
+            raise StoreError(400, "stream_ordering too old")
         sql = """
                 SELECT event_id FROM stream_ordering_to_exterm
                 INNER JOIN (
                     SELECT room_id, MAX(stream_ordering) AS stream_ordering
                     FROM stream_ordering_to_exterm
                     WHERE stream_ordering <= ? GROUP BY room_id
                 ) AS rms USING (room_id, stream_ordering)
                 WHERE room_id = ?
         """
         def get_forward_extremeties_for_room_txn(txn):
@@ -393,21 +393,21 @@
 class EventFederationStore(EventFederationWorkerStore):
     """ Responsible for storing and serving up the various graphs associated
     with an event. Including the main event graph and the auth chains for an
     event.
     Also has methods for getting the front (latest) and back (oldest) edges
     of the event graphs. These are used to generate the parents for new events
     and backfilling from another server respectively.
     """
     EVENT_AUTH_STATE_ONLY = "event_auth_state_only"
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(EventFederationStore, self).__init__(database, db_conn, hs)
         self.db_pool.updates.register_background_update_handler(
             self.EVENT_AUTH_STATE_ONLY, self._background_delete_non_state_event_auth
         )
         hs.get_clock().looping_call(
             self._delete_old_forward_extrem_cache, 60 * 60 * 1000
         )
     def _delete_old_forward_extrem_cache(self):
         def _delete_old_forward_extrem_cache_txn(txn):
             sql = """
                 DELETE FROM stream_ordering_to_exterm

--- a/synapse/storage/databases/main/event_push_actions.py
+++ b/synapse/storage/databases/main/event_push_actions.py
@@ -32,21 +32,21 @@
     """Custom deserializer for actions. This allows us to "compress" common actions
     """
     if actions:
         return db_to_json(actions)
     if is_highlight:
         return DEFAULT_HIGHLIGHT_ACTION
     else:
         return DEFAULT_NOTIF_ACTION
 class EventPushActionsWorkerStore(SQLBaseStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(EventPushActionsWorkerStore, self).__init__(database, db_conn, hs)
         self.stream_ordering_month_ago = None
         self.stream_ordering_day_ago = None
         cur = LoggingTransaction(
             db_conn.cursor(),
             name="_find_stream_orderings_for_times_txn",
             database_engine=self.database_engine,
         )
         self._find_stream_orderings_for_times_txn(cur)
         cur.close()
         self.find_stream_orderings_looping_call = self._clock.looping_call(
@@ -486,21 +486,21 @@
             )
             txn.execute(sql, (stream_ordering,))
             return txn.fetchone()
         result = await self.db_pool.runInteraction(
             "get_time_of_last_push_action_before", f
         )
         return result[0] if result else None
 class EventPushActionsStore(EventPushActionsWorkerStore):
     EPA_HIGHLIGHT_INDEX = "epa_highlight_index"
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(EventPushActionsStore, self).__init__(database, db_conn, hs)
         self.db_pool.updates.register_background_index_update(
             self.EPA_HIGHLIGHT_INDEX,
             index_name="event_push_actions_u_highlight",
             table="event_push_actions",
             columns=["user_id", "stream_ordering"],
         )
         self.db_pool.updates.register_background_index_update(
             "event_push_actions_highlights_index",
             index_name="event_push_actions_highlights_index",
             table="event_push_actions",
@@ -725,19 +725,19 @@
             (rotate_to_stream_ordering,),
         )
 def _action_has_highlight(actions):
     for action in actions:
         try:
             if action.get("set_tweak", None) == "highlight":
                 return action.get("value", True)
         except AttributeError:
             pass
     return False
-@attr.s(slots=True)
+@attr.s
 class _EventPushSummary:
     """Summary of pending event push actions for a given user in a given room.
     Used in _rotate_notifs_before_txn to manipulate results from event_push_actions.
     """
     unread_count = attr.ib(type=int)
     stream_ordering = attr.ib(type=int)
     old_user_id = attr.ib(type=str)
     notif_count = attr.ib(type=int)

--- a/synapse/storage/databases/main/events.py
+++ b/synapse/storage/databases/main/events.py
@@ -1,27 +1,27 @@
 import itertools
 import logging
 from collections import OrderedDict, namedtuple
-from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Set, Tuple
+from typing import TYPE_CHECKING, Dict, Iterable, List, Set, Tuple
 import attr
 from prometheus_client import Counter
 import synapse.metrics
 from synapse.api.constants import EventContentFields, EventTypes, RelationTypes
 from synapse.api.room_versions import RoomVersions
 from synapse.crypto.event_signing import compute_event_reference_hash
 from synapse.events import EventBase  # noqa: F401
 from synapse.events.snapshot import EventContext  # noqa: F401
 from synapse.logging.utils import log_function
 from synapse.storage._base import db_to_json, make_in_list_sql_clause
 from synapse.storage.database import DatabasePool, LoggingTransaction
 from synapse.storage.databases.main.search import SearchEntry
-from synapse.storage.util.id_generators import MultiWriterIdGenerator
+from synapse.storage.util.id_generators import StreamIdGenerator
 from synapse.types import StateMap, get_domain_from_id
 from synapse.util.frozenutils import frozendict_json_encoder
 from synapse.util.iterutils import batch_iter
 if TYPE_CHECKING:
     from synapse.server import HomeServer
     from synapse.storage.databases.main import DataStore
 logger = logging.getLogger(__name__)
 persist_event_counter = Counter("synapse_storage_events_persisted_events", "")
 event_counter = Counter(
     "synapse_storage_events_persisted_events_sep",
@@ -55,29 +55,26 @@
     Note: This is not part of the `DataStore` mixin.
     """
     def __init__(
         self, hs: "HomeServer", db: DatabasePool, main_data_store: "DataStore"
     ):
         self.hs = hs
         self.db_pool = db
         self.store = main_data_store
         self.database_engine = db.engine
         self._clock = hs.get_clock()
-        self._instance_name = hs.get_instance_name()
         self._ephemeral_messages_enabled = hs.config.enable_ephemeral_messages
         self.is_mine_id = hs.is_mine_id
-        self._backfill_id_gen = (
-            self.store._backfill_id_gen
-        )  # type: MultiWriterIdGenerator
-        self._stream_id_gen = self.store._stream_id_gen  # type: MultiWriterIdGenerator
+        self._backfill_id_gen = self.store._backfill_id_gen  # type: StreamIdGenerator
+        self._stream_id_gen = self.store._stream_id_gen  # type: StreamIdGenerator
         assert (
-            hs.get_instance_name() in hs.config.worker.writers.events
+            hs.config.worker.writers.events == hs.get_instance_name()
         ), "Can only instantiate EventsStore on master"
     async def _persist_events_and_state_updates(
         self,
         events_and_contexts: List[Tuple[EventBase, EventContext]],
         current_state_for_room: Dict[str, StateMap[str]],
         state_delta_for_room: Dict[str, DeltaState],
         new_forward_extremeties: Dict[str, List[str]],
         backfilled: bool = False,
     ) -> None:
         """Persist a set of events alongside updates to the current state and
@@ -88,28 +85,28 @@
                 the room based on forward extremities
             state_delta_for_room: Map from room_id to the delta to apply to
                 room state
             new_forward_extremities: Map from room_id to list of event IDs
                 that are the new forward extremities of the room.
             backfilled
         Returns:
             Resolves when the events have been persisted
         """
         if backfilled:
-            stream_ordering_manager = self._backfill_id_gen.get_next_mult(
+            stream_ordering_manager = await self._backfill_id_gen.get_next_mult(
                 len(events_and_contexts)
             )
         else:
-            stream_ordering_manager = self._stream_id_gen.get_next_mult(
+            stream_ordering_manager = await self._stream_id_gen.get_next_mult(
                 len(events_and_contexts)
             )
-        async with stream_ordering_manager as stream_orderings:
+        with stream_ordering_manager as stream_orderings:
             for (event, context), stream in zip(events_and_contexts, stream_orderings):
                 event.internal_metadata.stream_ordering = stream
             await self.db_pool.runInteraction(
                 "persist_events",
                 self._persist_events_txn,
                 events_and_contexts=events_and_contexts,
                 backfilled=backfilled,
                 state_delta_for_room=state_delta_for_room,
                 new_forward_extremeties=new_forward_extremeties,
             )
@@ -136,21 +133,21 @@
                     (room_id,), list(latest_event_ids)
                 )
     async def _get_events_which_are_prevs(self, event_ids: Iterable[str]) -> List[str]:
         """Filter the supplied list of event_ids to get those which are prev_events of
         existing (non-outlier/rejected) events.
         Args:
             event_ids: event ids to filter
         Returns:
             Filtered event ids
         """
-        results = []  # type: List[str]
+        results = []
         def _get_events_which_are_prevs_txn(txn, batch):
             sql = """
             SELECT prev_event_id, internal_metadata
             FROM event_edges
                 INNER JOIN events USING (event_id)
                 LEFT JOIN rejections USING (event_id)
                 LEFT JOIN event_json USING (event_id)
             WHERE
                 NOT events.outlier
                 AND rejections.event_id IS NULL
@@ -437,57 +434,48 @@
                 {
                     "room_id": room_id,
                     "event_id": event_id,
                     "stream_ordering": max_stream_order,
                 }
                 for room_id, new_extrem in new_forward_extremities.items()
                 for event_id in new_extrem
             ],
         )
     @classmethod
-    def _filter_events_and_contexts_for_duplicates(
-        cls, events_and_contexts: List[Tuple[EventBase, EventContext]]
-    ) -> List[Tuple[EventBase, EventContext]]:
+    def _filter_events_and_contexts_for_duplicates(cls, events_and_contexts):
         """Ensure that we don't have the same event twice.
         Pick the earliest non-outlier if there is one, else the earliest one.
         Args:
             events_and_contexts (list[(EventBase, EventContext)]):
         Returns:
             list[(EventBase, EventContext)]: filtered list
         """
-        new_events_and_contexts = (
-            OrderedDict()
-        )  # type: OrderedDict[str, Tuple[EventBase, EventContext]]
+        new_events_and_contexts = OrderedDict()
         for event, context in events_and_contexts:
             prev_event_context = new_events_and_contexts.get(event.event_id)
             if prev_event_context:
                 if not event.internal_metadata.is_outlier():
                     if prev_event_context[0].internal_metadata.is_outlier():
                         new_events_and_contexts.pop(event.event_id, None)
                         new_events_and_contexts[event.event_id] = (event, context)
             else:
                 new_events_and_contexts[event.event_id] = (event, context)
         return list(new_events_and_contexts.values())
-    def _update_room_depths_txn(
-        self,
-        txn,
-        events_and_contexts: List[Tuple[EventBase, EventContext]],
-        backfilled: bool,
-    ):
+    def _update_room_depths_txn(self, txn, events_and_contexts, backfilled):
         """Update min_depth for each room
         Args:
             txn (twisted.enterprise.adbapi.Connection): db connection
             events_and_contexts (list[(EventBase, EventContext)]): events
                 we are persisting
             backfilled (bool): True if the events were backfilled
         """
-        depth_updates = {}  # type: Dict[str, int]
+        depth_updates = {}
         for event, context in events_and_contexts:
             txn.call_after(self.store._invalidate_get_event_cache, event.event_id)
             if not backfilled:
                 txn.call_after(
                     self.store._events_stream_cache.entity_has_changed,
                     event.room_id,
                     event.internal_metadata.stream_ordering,
                 )
             if not event.internal_metadata.is_outlier() and not context.rejected:
                 depth_updates[event.room_id] = max(
@@ -573,21 +561,20 @@
                     "format_version": event.format_version,
                 }
                 for event, _ in events_and_contexts
             ],
         )
         self.db_pool.simple_insert_many_txn(
             txn,
             table="events",
             values=[
                 {
-                    "instance_name": self._instance_name,
                     "stream_ordering": event.internal_metadata.stream_ordering,
                     "topological_ordering": event.depth,
                     "depth": event.depth,
                     "event_id": event.event_id,
                     "room_id": event.room_id,
                     "type": event.type,
                     "processed": True,
                     "outlier": event.internal_metadata.is_outlier(),
                     "origin_server_ts": int(event.origin_server_ts),
                     "received_ts": self._clock.time_msec(),
@@ -803,34 +790,32 @@
                     "algorithm": ref_alg,
                     "hash": memoryview(ref_hash_bytes),
                 }
             )
         self.db_pool.simple_insert_many_txn(
             txn, table="event_reference_hashes", values=vals
         )
     def _store_room_members_txn(self, txn, events, backfilled):
         """Store a room member in the database.
         """
-        def str_or_none(val: Any) -> Optional[str]:
-            return val if isinstance(val, str) else None
         self.db_pool.simple_insert_many_txn(
             txn,
             table="room_memberships",
             values=[
                 {
                     "event_id": event.event_id,
                     "user_id": event.state_key,
                     "sender": event.user_id,
                     "room_id": event.room_id,
                     "membership": event.membership,
-                    "display_name": str_or_none(event.content.get("displayname")),
-                    "avatar_url": str_or_none(event.content.get("avatar_url")),
+                    "display_name": event.content.get("displayname", None),
+                    "avatar_url": event.content.get("avatar_url", None),
                 }
                 for event in events
             ],
         )
         for event in events:
             txn.call_after(
                 self.store._membership_stream_cache.entity_has_changed,
                 event.state_key,
                 event.internal_metadata.stream_ordering,
             )
@@ -1087,21 +1072,21 @@
             ],
         )
         self._update_backward_extremeties(txn, events)
     def _update_backward_extremeties(self, txn, events):
         """Updates the event_backward_extremities tables based on the new/updated
         events being persisted.
         This is called for new events *and* for events that were outliers, but
         are now being persisted as non-outliers.
         Forward extremities are handled when we first start persisting the events.
         """
-        events_by_room = {}  # type: Dict[str, List[EventBase]]
+        events_by_room = {}
         for ev in events:
             events_by_room.setdefault(ev.room_id, []).append(ev)
         query = (
             "INSERT INTO event_backward_extremities (event_id, room_id)"
             " SELECT ?, ? WHERE NOT EXISTS ("
             " SELECT 1 FROM event_backward_extremities"
             " WHERE event_id = ? AND room_id = ?"
             " )"
             " AND NOT EXISTS ("
             " SELECT 1 FROM events WHERE event_id = ? AND room_id = ? "

--- a/synapse/storage/databases/main/events_bg_updates.py
+++ b/synapse/storage/databases/main/events_bg_updates.py
@@ -1,21 +1,21 @@
 import logging
 from synapse.api.constants import EventContentFields
 from synapse.storage._base import SQLBaseStore, db_to_json, make_in_list_sql_clause
 from synapse.storage.database import DatabasePool
 logger = logging.getLogger(__name__)
 class EventsBackgroundUpdatesStore(SQLBaseStore):
     EVENT_ORIGIN_SERVER_TS_NAME = "event_origin_server_ts"
     EVENT_FIELDS_SENDER_URL_UPDATE_NAME = "event_fields_sender_url"
     DELETE_SOFT_FAILED_EXTREMITIES = "delete_soft_failed_extremities"
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(EventsBackgroundUpdatesStore, self).__init__(database, db_conn, hs)
         self.db_pool.updates.register_background_update_handler(
             self.EVENT_ORIGIN_SERVER_TS_NAME, self._background_reindex_origin_server_ts
         )
         self.db_pool.updates.register_background_update_handler(
             self.EVENT_FIELDS_SENDER_URL_UPDATE_NAME,
             self._background_reindex_fields_sender,
         )
         self.db_pool.updates.register_background_index_update(
             "event_contains_url_index",
             index_name="event_contains_url_index",

--- a/synapse/storage/databases/main/events_worker.py
+++ b/synapse/storage/databases/main/events_worker.py
@@ -1,10 +1,11 @@
+from __future__ import division
 import itertools
 import logging
 import threading
 from collections import namedtuple
 from typing import Dict, Iterable, List, Optional, Tuple, overload
 from constantly import NamedConstant, Names
 from typing_extensions import Literal
 from twisted.internet import defer
 from synapse.api.constants import EventTypes
 from synapse.api.errors import NotFoundError, SynapseError
@@ -15,84 +16,56 @@
 )
 from synapse.events import EventBase, make_event_from_dict
 from synapse.events.utils import prune_event
 from synapse.logging.context import PreserveLoggingContext, current_context
 from synapse.metrics.background_process_metrics import run_as_background_process
 from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
 from synapse.replication.tcp.streams import BackfillStream
 from synapse.replication.tcp.streams.events import EventsStream
 from synapse.storage._base import SQLBaseStore, db_to_json, make_in_list_sql_clause
 from synapse.storage.database import DatabasePool
-from synapse.storage.engines import PostgresEngine
-from synapse.storage.util.id_generators import MultiWriterIdGenerator, StreamIdGenerator
+from synapse.storage.util.id_generators import StreamIdGenerator
 from synapse.types import Collection, get_domain_from_id
 from synapse.util.caches.descriptors import Cache, cached
 from synapse.util.iterutils import batch_iter
 from synapse.util.metrics import Measure
 logger = logging.getLogger(__name__)
 EVENT_QUEUE_THREADS = 3  # Max number of threads that will fetch events
 EVENT_QUEUE_ITERATIONS = 3  # No. times we block waiting for requests for events
 EVENT_QUEUE_TIMEOUT_S = 0.1  # Timeout when waiting for requests for events
 _EventCacheEntry = namedtuple("_EventCacheEntry", ("event", "redacted_event"))
 class EventRedactBehaviour(Names):
     """
     What to do when retrieving a redacted event from the database.
     """
     AS_IS = NamedConstant()
     REDACT = NamedConstant()
     BLOCK = NamedConstant()
 class EventsWorkerStore(SQLBaseStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
-        if isinstance(database.engine, PostgresEngine):
-            self._stream_id_gen = MultiWriterIdGenerator(
-                db_conn=db_conn,
-                db=database,
-                stream_name="events",
-                instance_name=hs.get_instance_name(),
-                table="events",
-                instance_column="instance_name",
-                id_column="stream_ordering",
-                sequence_name="events_stream_seq",
-                writers=hs.config.worker.writers.events,
-            )
-            self._backfill_id_gen = MultiWriterIdGenerator(
-                db_conn=db_conn,
-                db=database,
-                stream_name="backfill",
-                instance_name=hs.get_instance_name(),
-                table="events",
-                instance_column="instance_name",
-                id_column="stream_ordering",
-                sequence_name="events_backfill_stream_seq",
-                positive=False,
-                writers=hs.config.worker.writers.events,
+        super(EventsWorkerStore, self).__init__(database, db_conn, hs)
+        if hs.config.worker.writers.events == hs.get_instance_name():
+            self._stream_id_gen = StreamIdGenerator(
+                db_conn, "events", "stream_ordering",
+            )
+            self._backfill_id_gen = StreamIdGenerator(
+                db_conn,
+                "events",
+                "stream_ordering",
+                step=-1,
+                extra_tables=[("ex_outlier_stream", "event_stream_ordering")],
             )
         else:
-            if hs.get_instance_name() in hs.config.worker.writers.events:
-                self._stream_id_gen = StreamIdGenerator(
-                    db_conn, "events", "stream_ordering",
-                )
-                self._backfill_id_gen = StreamIdGenerator(
-                    db_conn,
-                    "events",
-                    "stream_ordering",
-                    step=-1,
-                    extra_tables=[("ex_outlier_stream", "event_stream_ordering")],
-                )
-            else:
-                self._stream_id_gen = SlavedIdTracker(
-                    db_conn, "events", "stream_ordering"
-                )
-                self._backfill_id_gen = SlavedIdTracker(
-                    db_conn, "events", "stream_ordering", step=-1
-                )
+            self._stream_id_gen = SlavedIdTracker(db_conn, "events", "stream_ordering")
+            self._backfill_id_gen = SlavedIdTracker(
+                db_conn, "events", "stream_ordering", step=-1
+            )
         self._get_event_cache = Cache(
             "*getEvent*",
             keylen=3,
             max_entries=hs.config.caches.event_cache_size,
             apply_cache_factor_from_config=False,
         )
         self._event_fetch_lock = threading.Condition()
         self._event_fetch_list = []
         self._event_fetch_ongoing = 0
     def process_replication_rows(self, stream_name, instance_name, token, rows):

--- a/synapse/storage/databases/main/group_server.py
+++ b/synapse/storage/databases/main/group_server.py
@@ -1078,21 +1078,21 @@
                     txn,
                     table="group_attestations_renewals",
                     keyvalues={"group_id": group_id, "user_id": user_id},
                 )
                 self.db_pool.simple_delete_txn(
                     txn,
                     table="group_attestations_remote",
                     keyvalues={"group_id": group_id, "user_id": user_id},
                 )
             return next_id
-        async with self._group_updates_id_gen.get_next() as next_id:
+        with await self._group_updates_id_gen.get_next() as next_id:
             res = await self.db_pool.runInteraction(
                 "register_user_group_membership",
                 _register_user_group_membership_txn,
                 next_id,
             )
         return res
     async def create_group(
         self, group_id, user_id, name, avatar_url, short_description, long_description
     ) -> None:
         await self.db_pool.simple_insert(

--- a/synapse/storage/databases/main/media_repository.py
+++ b/synapse/storage/databases/main/media_repository.py
@@ -1,74 +1,29 @@
 from typing import Any, Dict, Iterable, List, Optional, Tuple
 from synapse.storage._base import SQLBaseStore
 from synapse.storage.database import DatabasePool
-BG_UPDATE_REMOVE_MEDIA_REPO_INDEX_WITHOUT_METHOD = (
-    "media_repository_drop_index_wo_method"
-)
 class MediaRepositoryBackgroundUpdateStore(SQLBaseStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(MediaRepositoryBackgroundUpdateStore, self).__init__(
+            database, db_conn, hs
+        )
         self.db_pool.updates.register_background_index_update(
             update_name="local_media_repository_url_idx",
             index_name="local_media_repository_url_idx",
             table="local_media_repository",
             columns=["created_ts"],
             where_clause="url_cache IS NOT NULL",
         )
-        self.db_pool.updates.register_background_index_update(
-            update_name="local_media_repository_thumbnails_method_idx",
-            index_name="local_media_repository_thumbn_media_id_width_height_method_key",
-            table="local_media_repository_thumbnails",
-            columns=[
-                "media_id",
-                "thumbnail_width",
-                "thumbnail_height",
-                "thumbnail_type",
-                "thumbnail_method",
-            ],
-            unique=True,
-        )
-        self.db_pool.updates.register_background_index_update(
-            update_name="remote_media_repository_thumbnails_method_idx",
-            index_name="remote_media_repository_thumbn_media_origin_id_width_height_method_key",
-            table="remote_media_cache_thumbnails",
-            columns=[
-                "media_origin",
-                "media_id",
-                "thumbnail_width",
-                "thumbnail_height",
-                "thumbnail_type",
-                "thumbnail_method",
-            ],
-            unique=True,
-        )
-        self.db_pool.updates.register_background_update_handler(
-            BG_UPDATE_REMOVE_MEDIA_REPO_INDEX_WITHOUT_METHOD,
-            self._drop_media_index_without_method,
-        )
-    async def _drop_media_index_without_method(self, progress, batch_size):
-        def f(txn):
-            txn.execute(
-                "ALTER TABLE local_media_repository_thumbnails DROP CONSTRAINT IF EXISTS local_media_repository_thumbn_media_id_thumbnail_width_thum_key"
-            )
-            txn.execute(
-                "ALTER TABLE remote_media_cache_thumbnails DROP CONSTRAINT IF EXISTS remote_media_repository_thumbn_media_id_thumbnail_width_thum_key"
-            )
-        await self.db_pool.runInteraction("drop_media_indices_without_method", f)
-        await self.db_pool.updates._end_background_update(
-            BG_UPDATE_REMOVE_MEDIA_REPO_INDEX_WITHOUT_METHOD
-        )
-        return 1
 class MediaRepositoryStore(MediaRepositoryBackgroundUpdateStore):
     """Persistence for attachments and avatars"""
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(MediaRepositoryStore, self).__init__(database, db_conn, hs)
     async def get_local_media(self, media_id: str) -> Optional[Dict[str, Any]]:
         """Get the metadata for a local piece of media
         Returns:
             None if the media_id doesn't exist.
         """
         return await self.db_pool.simple_select_one(
             "local_media_repository",
             {"media_id": media_id},
             (
                 "media_type",

--- a/synapse/storage/databases/main/metrics.py
+++ b/synapse/storage/databases/main/metrics.py
@@ -1,60 +1,49 @@
-from synapse.metrics import GaugeBucketCollector
+import typing
+from collections import Counter
+from synapse.metrics import BucketCollector
 from synapse.metrics.background_process_metrics import run_as_background_process
 from synapse.storage._base import SQLBaseStore
 from synapse.storage.database import DatabasePool
 from synapse.storage.databases.main.event_push_actions import (
     EventPushActionsWorkerStore,
-)
-_extremities_collecter = GaugeBucketCollector(
-    "synapse_forward_extremities",
-    "Number of rooms on the server with the given number of forward extremities"
-    " or fewer",
-    buckets=[1, 2, 3, 5, 7, 10, 15, 20, 50, 100, 200, 500],
-)
-_excess_state_events_collecter = GaugeBucketCollector(
-    "synapse_excess_extremity_events",
-    "Number of rooms on the server with the given number of excess extremity "
-    "events, or fewer",
-    buckets=[0] + [1 << n for n in range(12)],
 )
 class ServerMetricsStore(EventPushActionsWorkerStore, SQLBaseStore):
     """Functions to pull various metrics from the DB, for e.g. phone home
     stats and prometheus metrics.
     """
     def __init__(self, database: DatabasePool, db_conn, hs):
         super().__init__(database, db_conn, hs)
+        self._current_forward_extremities_amount = (
+            Counter()
+        )  # type: typing.Counter[int]
+        BucketCollector(
+            "synapse_forward_extremities",
+            lambda: self._current_forward_extremities_amount,
+            buckets=[1, 2, 3, 5, 7, 10, 15, 20, 50, 100, 200, 500, "+Inf"],
+        )
         def read_forward_extremities():
             return run_as_background_process(
                 "read_forward_extremities", self._read_forward_extremities
             )
         hs.get_clock().looping_call(read_forward_extremities, 60 * 60 * 1000)
     async def _read_forward_extremities(self):
         def fetch(txn):
             txn.execute(
                 """
-                SELECT t1.c, t2.c
-                FROM (
-                    SELECT room_id, COUNT(*) c FROM event_forward_extremities
-                    GROUP BY room_id
-                ) t1 LEFT JOIN (
-                    SELECT room_id, COUNT(*) c FROM current_state_events
-                    GROUP BY room_id
-                ) t2 ON t1.room_id = t2.room_id
+                select count(*) c from event_forward_extremities
+                group by room_id
                 """
             )
             return txn.fetchall()
         res = await self.db_pool.runInteraction("read_forward_extremities", fetch)
-        _extremities_collecter.update_data(x[0] for x in res)
-        _excess_state_events_collecter.update_data(
-            (x[0] - 1) * x[1] for x in res if x[1]
-        )
+        self._current_forward_extremities_amount = Counter([x[0] for x in res])
     async def count_daily_messages(self):
         """
         Returns an estimate of the number of messages sent in the last day.
         If it has been significantly less or more than one day since the last
         call to this function, it will return None.
         """
         def _count_messages(txn):
             sql = """
                 SELECT COALESCE(COUNT(*), 0) FROM events
                 WHERE type = 'm.room.message'

--- a/synapse/storage/databases/main/monthly_active_users.py
+++ b/synapse/storage/databases/main/monthly_active_users.py
@@ -1,36 +1,30 @@
 import logging
 from typing import Dict, List
 from synapse.storage._base import SQLBaseStore
 from synapse.storage.database import DatabasePool, make_in_list_sql_clause
 from synapse.util.caches.descriptors import cached
 logger = logging.getLogger(__name__)
 LAST_SEEN_GRANULARITY = 60 * 60 * 1000
 class MonthlyActiveUsersWorkerStore(SQLBaseStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(MonthlyActiveUsersWorkerStore, self).__init__(database, db_conn, hs)
         self._clock = hs.get_clock()
         self.hs = hs
     @cached(num_args=0)
     async def get_monthly_active_count(self) -> int:
         """Generates current count of monthly active users
         Returns:
             Number of current monthly active users
         """
         def _count_users(txn):
-            sql = """
-                SELECT COALESCE(count(*), 0)
-                FROM monthly_active_users
-                    LEFT JOIN users
-                    ON monthly_active_users.user_id=users.name
-                WHERE (users.appservice_id IS NULL OR users.appservice_id = '');
-            """
+            sql = "SELECT COALESCE(count(*), 0) FROM monthly_active_users"
             txn.execute(sql)
             (count,) = txn.fetchone()
             return count
         return await self.db_pool.runInteraction("count_users", _count_users)
     @cached(num_args=0)
     async def get_monthly_active_count_by_service(self) -> Dict[str, int]:
         """Generates current count of monthly active users broken down by service.
         A service is typically an appservice but also includes native matrix users.
         Since the `monthly_active_users` table is populated from the `user_ips` table
         `config.track_appservice_user_ips` must be set to `true` for this
@@ -78,21 +72,21 @@
         """
         return await self.db_pool.simple_select_one_onecol(
             table="monthly_active_users",
             keyvalues={"user_id": user_id},
             retcol="timestamp",
             allow_none=True,
             desc="user_last_seen_monthly_active",
         )
 class MonthlyActiveUsersStore(MonthlyActiveUsersWorkerStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(MonthlyActiveUsersStore, self).__init__(database, db_conn, hs)
         self._limit_usage_by_mau = hs.config.limit_usage_by_mau
         self._mau_stats_only = hs.config.mau_stats_only
         self._max_mau_value = hs.config.max_mau_value
         self.db_pool.new_transaction(
             db_conn,
             "initialise_mau_threepids",
             [],
             [],
             self._initialise_reserved_users,
             hs.config.mau_limits_reserved_threepids[: self._max_mau_value],

--- a/synapse/storage/databases/main/presence.py
+++ b/synapse/storage/databases/main/presence.py
@@ -1,21 +1,21 @@
 from typing import List, Tuple
 from synapse.api.presence import UserPresenceState
 from synapse.storage._base import SQLBaseStore, make_in_list_sql_clause
 from synapse.util.caches.descriptors import cached, cachedList
 from synapse.util.iterutils import batch_iter
 class PresenceStore(SQLBaseStore):
     async def update_presence(self, presence_states):
-        stream_ordering_manager = self._presence_id_gen.get_next_mult(
+        stream_ordering_manager = await self._presence_id_gen.get_next_mult(
             len(presence_states)
         )
-        async with stream_ordering_manager as stream_orderings:
+        with stream_ordering_manager as stream_orderings:
             await self.db_pool.runInteraction(
                 "update_presence",
                 self._update_presence_txn,
                 stream_orderings,
                 presence_states,
             )
         return stream_orderings[-1], self._presence_id_gen.get_current_token()
     def _update_presence_txn(self, txn, stream_orderings, presence_states):
         for stream_id, state in zip(stream_orderings, presence_states):
             txn.call_after(

--- a/synapse/storage/databases/main/purge_events.py
+++ b/synapse/storage/databases/main/purge_events.py
@@ -13,29 +13,29 @@
         Args:
             room_id:
             token: A topological token to delete events before
             delete_local_events:
                 if True, we will delete local events as well as remote ones
                 (instead of just marking them as outliers and deleting their
                 state groups).
         Returns:
             The set of state groups that are referenced by deleted events.
         """
-        parsed_token = await RoomStreamToken.parse(self, token)
         return await self.db_pool.runInteraction(
             "purge_history",
             self._purge_history_txn,
             room_id,
-            parsed_token,
+            token,
             delete_local_events,
         )
-    def _purge_history_txn(self, txn, room_id, token, delete_local_events):
+    def _purge_history_txn(self, txn, room_id, token_str, delete_local_events):
+        token = RoomStreamToken.parse(token_str)
         txn.execute("DROP TABLE IF EXISTS events_to_purge")
         txn.execute(
             "CREATE TEMPORARY TABLE events_to_purge ("
             "    event_id TEXT NOT NULL,"
             "    should_delete BOOLEAN NOT NULL"
             ")"
         )
         txn.execute(
             "SELECT e.event_id, e.depth FROM events as e "
             "INNER JOIN event_forward_extremities as f "
@@ -201,21 +201,20 @@
                 """
                 DELETE FROM %s WHERE event_id IN (
                   SELECT event_id FROM events WHERE room_id=?
                 )
                 """
                 % (table,),
                 (room_id,),
             )
         for table in (
             "current_state_events",
-            "destination_rooms",
             "event_backward_extremities",
             "event_forward_extremities",
             "event_json",
             "event_push_actions",
             "event_search",
             "events",
             "group_rooms",
             "public_room_list_stream",
             "receipts_graph",
             "receipts_linearized",

--- a/synapse/storage/databases/main/push_rule.py
+++ b/synapse/storage/databases/main/push_rule.py
@@ -1,24 +1,22 @@
 import abc
 import logging
 from typing import List, Tuple, Union
-from synapse.api.errors import NotFoundError, StoreError
 from synapse.push.baserules import list_with_base_rules
 from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
 from synapse.storage._base import SQLBaseStore, db_to_json
 from synapse.storage.database import DatabasePool
 from synapse.storage.databases.main.appservice import ApplicationServiceWorkerStore
 from synapse.storage.databases.main.events_worker import EventsWorkerStore
 from synapse.storage.databases.main.pusher import PusherWorkerStore
 from synapse.storage.databases.main.receipts import ReceiptsWorkerStore
 from synapse.storage.databases.main.roommember import RoomMemberWorkerStore
-from synapse.storage.engines import PostgresEngine, Sqlite3Engine
 from synapse.storage.push_rule import InconsistentRuleException, RuleNotFoundException
 from synapse.storage.util.id_generators import StreamIdGenerator
 from synapse.util import json_encoder
 from synapse.util.caches.descriptors import cached, cachedList
 from synapse.util.caches.stream_change_cache import StreamChangeCache
 logger = logging.getLogger(__name__)
 def _load_rules(rawrules, enabled_map, use_new_defaults=False):
     ruleslist = []
     for rawrule in rawrules:
         rule = dict(rawrule)
@@ -35,27 +33,27 @@
                 rule["enabled"] = bool(enabled_map[rule_id])
                 rules[i] = rule
     return rules
 class PushRulesWorkerStore(
     ApplicationServiceWorkerStore,
     ReceiptsWorkerStore,
     PusherWorkerStore,
     RoomMemberWorkerStore,
     EventsWorkerStore,
     SQLBaseStore,
-    metaclass=abc.ABCMeta,
 ):
     """This is an abstract base class where subclasses must implement
     `get_max_push_rules_stream_id` which can be called in the initializer.
     """
+    __metaclass__ = abc.ABCMeta
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(PushRulesWorkerStore, self).__init__(database, db_conn, hs)
         if hs.config.worker.worker_app is None:
             self._push_rules_stream_id_gen = StreamIdGenerator(
                 db_conn, "push_rules_stream", "stream_id"
             )  # type: Union[StreamIdGenerator, SlavedIdTracker]
         else:
             self._push_rules_stream_id_gen = SlavedIdTracker(
                 db_conn, "push_rules_stream", "stream_id"
             )
         push_rules_prefill, push_rules_id = self.db_pool.get_cache_dict(
             db_conn,
@@ -251,21 +249,21 @@
         user_id,
         rule_id,
         priority_class,
         conditions,
         actions,
         before=None,
         after=None,
     ) -> None:
         conditions_json = json_encoder.encode(conditions)
         actions_json = json_encoder.encode(actions)
-        async with self._push_rules_stream_id_gen.get_next() as stream_id:
+        with await self._push_rules_stream_id_gen.get_next() as stream_id:
             event_stream_ordering = self._stream_id_gen.get_current_token()
             if before or after:
                 await self.db_pool.runInteraction(
                     "_add_push_rule_relative_txn",
                     self._add_push_rule_relative_txn,
                     stream_id,
                     event_stream_ordering,
                     user_id,
                     rule_id,
                     priority_class,
@@ -419,197 +417,112 @@
                 user_id,
                 rule_id,
                 op="ADD",
                 data={
                     "priority_class": priority_class,
                     "priority": priority,
                     "conditions": conditions_json,
                     "actions": actions_json,
                 },
             )
-        if isinstance(self.database_engine, PostgresEngine):
-            sql = """
-                INSERT INTO push_rules_enable (id, user_name, rule_id, enabled)
-                VALUES (?, ?, ?, ?)
-                ON CONFLICT DO NOTHING
-            """
-        elif isinstance(self.database_engine, Sqlite3Engine):
-            sql = """
-                INSERT OR IGNORE INTO push_rules_enable (id, user_name, rule_id, enabled)
-                VALUES (?, ?, ?, ?)
-            """
-        else:
-            raise RuntimeError("Unknown database engine")
-        new_enable_id = self._push_rules_enable_id_gen.get_next()
-        txn.execute(sql, (new_enable_id, user_id, rule_id, 1))
     async def delete_push_rule(self, user_id: str, rule_id: str) -> None:
         """
         Delete a push rule. Args specify the row to be deleted and can be
         any of the columns in the push_rule table, but below are the
         standard ones
         Args:
             user_id: The matrix ID of the push rule owner
             rule_id: The rule_id of the rule to be deleted
         """
         def delete_push_rule_txn(txn, stream_id, event_stream_ordering):
-            self.db_pool.simple_delete_txn(
-                txn, "push_rules_enable", {"user_name": user_id, "rule_id": rule_id}
-            )
             self.db_pool.simple_delete_one_txn(
                 txn, "push_rules", {"user_name": user_id, "rule_id": rule_id}
             )
             self._insert_push_rules_update_txn(
                 txn, stream_id, event_stream_ordering, user_id, rule_id, op="DELETE"
             )
-        async with self._push_rules_stream_id_gen.get_next() as stream_id:
+        with await self._push_rules_stream_id_gen.get_next() as stream_id:
             event_stream_ordering = self._stream_id_gen.get_current_token()
             await self.db_pool.runInteraction(
                 "delete_push_rule",
                 delete_push_rule_txn,
                 stream_id,
                 event_stream_ordering,
             )
-    async def set_push_rule_enabled(
-        self, user_id: str, rule_id: str, enabled: bool, is_default_rule: bool
-    ) -> None:
-        """
-        Sets the `enabled` state of a push rule.
-        Args:
-            user_id: the user ID of the user who wishes to enable/disable the rule
-                e.g. '@tina:example.org'
-            rule_id: the full rule ID of the rule to be enabled/disabled
-                e.g. 'global/override/.m.rule.roomnotif'
-                  or 'global/override/myCustomRule'
-            enabled: True if the rule is to be enabled, False if it is to be
-                disabled
-            is_default_rule: True if and only if this is a server-default rule.
-                This skips the check for existence (as only user-created rules
-                are always stored in the database `push_rules` table).
-        Raises:
-            NotFoundError if the rule does not exist.
-        """
-        async with self._push_rules_stream_id_gen.get_next() as stream_id:
+    async def set_push_rule_enabled(self, user_id, rule_id, enabled) -> None:
+        with await self._push_rules_stream_id_gen.get_next() as stream_id:
             event_stream_ordering = self._stream_id_gen.get_current_token()
             await self.db_pool.runInteraction(
                 "_set_push_rule_enabled_txn",
                 self._set_push_rule_enabled_txn,
                 stream_id,
                 event_stream_ordering,
                 user_id,
                 rule_id,
                 enabled,
-                is_default_rule,
             )
     def _set_push_rule_enabled_txn(
-        self,
-        txn,
-        stream_id,
-        event_stream_ordering,
-        user_id,
-        rule_id,
-        enabled,
-        is_default_rule,
+        self, txn, stream_id, event_stream_ordering, user_id, rule_id, enabled
     ):
         new_id = self._push_rules_enable_id_gen.get_next()
-        if not is_default_rule:
-            for_key_share = "FOR KEY SHARE"
-            if not isinstance(self.database_engine, PostgresEngine):
-                for_key_share = ""
-            sql = (
-                """
-                SELECT 1 FROM push_rules
-                WHERE user_name = ? AND rule_id = ?
-                %s
-            """
-                % for_key_share
-            )
-            txn.execute(sql, (user_id, rule_id))
-            if txn.fetchone() is None:
-                raise NotFoundError("Push rule does not exist.")
         self.db_pool.simple_upsert_txn(
             txn,
             "push_rules_enable",
             {"user_name": user_id, "rule_id": rule_id},
             {"enabled": 1 if enabled else 0},
             {"id": new_id},
         )
         self._insert_push_rules_update_txn(
             txn,
             stream_id,
             event_stream_ordering,
             user_id,
             rule_id,
             op="ENABLE" if enabled else "DISABLE",
         )
     async def set_push_rule_actions(
-        self,
-        user_id: str,
-        rule_id: str,
-        actions: List[Union[dict, str]],
-        is_default_rule: bool,
+        self, user_id, rule_id, actions, is_default_rule
     ) -> None:
-        """
-        Sets the `actions` state of a push rule.
-        Will throw NotFoundError if the rule does not exist; the Code for this
-        is NOT_FOUND.
-        Args:
-            user_id: the user ID of the user who wishes to enable/disable the rule
-                e.g. '@tina:example.org'
-            rule_id: the full rule ID of the rule to be enabled/disabled
-                e.g. 'global/override/.m.rule.roomnotif'
-                  or 'global/override/myCustomRule'
-            actions: A list of actions (each action being a dict or string),
-                e.g. ["notify", {"set_tweak": "highlight", "value": false}]
-            is_default_rule: True if and only if this is a server-default rule.
-                This skips the check for existence (as only user-created rules
-                are always stored in the database `push_rules` table).
-        """
         actions_json = json_encoder.encode(actions)
         def set_push_rule_actions_txn(txn, stream_id, event_stream_ordering):
             if is_default_rule:
                 priority_class = -1
                 priority = 1
                 self._upsert_push_rule_txn(
                     txn,
                     stream_id,
                     event_stream_ordering,
                     user_id,
                     rule_id,
                     priority_class,
                     priority,
                     "[]",
                     actions_json,
                     update_stream=False,
                 )
             else:
-                try:
-                    self.db_pool.simple_update_one_txn(
-                        txn,
-                        "push_rules",
-                        {"user_name": user_id, "rule_id": rule_id},
-                        {"actions": actions_json},
-                    )
-                except StoreError as serr:
-                    if serr.code == 404:
-                        raise NotFoundError("Push rule does not exist")
-                    else:
-                        raise
+                self.db_pool.simple_update_one_txn(
+                    txn,
+                    "push_rules",
+                    {"user_name": user_id, "rule_id": rule_id},
+                    {"actions": actions_json},
+                )
             self._insert_push_rules_update_txn(
                 txn,
                 stream_id,
                 event_stream_ordering,
                 user_id,
                 rule_id,
                 op="ACTIONS",
                 data={"actions": actions_json},
             )
-        async with self._push_rules_stream_id_gen.get_next() as stream_id:
+        with await self._push_rules_stream_id_gen.get_next() as stream_id:
             event_stream_ordering = self._stream_id_gen.get_current_token()
             await self.db_pool.runInteraction(
                 "set_push_rule_actions",
                 set_push_rule_actions_txn,
                 stream_id,
                 event_stream_ordering,
             )
     def _insert_push_rules_update_txn(
         self, txn, stream_id, event_stream_ordering, user_id, rule_id, op, data=None
     ):

--- a/synapse/storage/databases/main/pusher.py
+++ b/synapse/storage/databases/main/pusher.py
@@ -212,21 +212,21 @@
         app_id,
         app_display_name,
         device_display_name,
         pushkey,
         pushkey_ts,
         lang,
         data,
         last_stream_ordering,
         profile_tag="",
     ) -> None:
-        async with self._pushers_id_gen.get_next() as stream_id:
+        with await self._pushers_id_gen.get_next() as stream_id:
             await self.db_pool.simple_upsert(
                 table="pushers",
                 keyvalues={"app_id": app_id, "pushkey": pushkey, "user_name": user_id},
                 values={
                     "access_token": access_token,
                     "kind": kind,
                     "app_display_name": app_display_name,
                     "device_display_name": device_display_name,
                     "ts": pushkey_ts,
                     "lang": lang,
@@ -263,14 +263,14 @@
             self.db_pool.simple_insert_txn(
                 txn,
                 table="deleted_pushers",
                 values={
                     "stream_id": stream_id,
                     "app_id": app_id,
                     "pushkey": pushkey,
                     "user_id": user_id,
                 },
             )
-        async with self._pushers_id_gen.get_next() as stream_id:
+        with await self._pushers_id_gen.get_next() as stream_id:
             await self.db_pool.runInteraction(
                 "delete_pusher", delete_pusher_txn, stream_id
             )

--- a/synapse/storage/databases/main/receipts.py
+++ b/synapse/storage/databases/main/receipts.py
@@ -3,26 +3,27 @@
 from typing import Any, Dict, List, Optional, Tuple
 from twisted.internet import defer
 from synapse.storage._base import SQLBaseStore, db_to_json, make_in_list_sql_clause
 from synapse.storage.database import DatabasePool
 from synapse.storage.util.id_generators import StreamIdGenerator
 from synapse.util import json_encoder
 from synapse.util.async_helpers import ObservableDeferred
 from synapse.util.caches.descriptors import cached, cachedList
 from synapse.util.caches.stream_change_cache import StreamChangeCache
 logger = logging.getLogger(__name__)
-class ReceiptsWorkerStore(SQLBaseStore, metaclass=abc.ABCMeta):
+class ReceiptsWorkerStore(SQLBaseStore):
     """This is an abstract base class where subclasses must implement
     `get_max_receipt_stream_id` which can be called in the initializer.
     """
+    __metaclass__ = abc.ABCMeta
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(ReceiptsWorkerStore, self).__init__(database, db_conn, hs)
         self._receipts_stream_cache = StreamChangeCache(
             "ReceiptsRoomChangeCache", self.get_max_receipt_stream_id()
         )
     @abc.abstractmethod
     def get_max_receipt_stream_id(self):
         """Get the current max stream ID for receipts stream
         Returns:
             int
         """
         raise NotImplementedError()
@@ -274,21 +275,21 @@
             else:
                 res = None
         if res and user_id in res:
             return
         self.get_users_with_read_receipts_in_room.invalidate((room_id,))
 class ReceiptsStore(ReceiptsWorkerStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
         self._receipts_id_gen = StreamIdGenerator(
             db_conn, "receipts_linearized", "stream_id"
         )
-        super().__init__(database, db_conn, hs)
+        super(ReceiptsStore, self).__init__(database, db_conn, hs)
     def get_max_receipt_stream_id(self):
         return self._receipts_id_gen.get_current_token()
     def insert_linearized_receipt_txn(
         self, txn, room_id, receipt_type, user_id, event_id, data, stream_id
     ):
         """Inserts a read-receipt into the database if it's newer than the current RR
         Returns: int|None
             None if the RR is older than the current RR
             otherwise, the rx timestamp of the event that the RR corresponds to
                 (or 0 if the event is unknown)
@@ -386,21 +387,21 @@
                 )
                 txn.execute(sql, [room_id] + list(args))
                 rows = txn.fetchall()
                 if rows:
                     return rows[0][0]
                 else:
                     raise RuntimeError("Unrecognized event_ids: %r" % (event_ids,))
             linearized_event_id = await self.db_pool.runInteraction(
                 "insert_receipt_conv", graph_to_linear
             )
-        async with self._receipts_id_gen.get_next() as stream_id:
+        with await self._receipts_id_gen.get_next() as stream_id:
             event_ts = await self.db_pool.runInteraction(
                 "insert_linearized_receipt",
                 self.insert_linearized_receipt_txn,
                 room_id,
                 receipt_type,
                 user_id,
                 linearized_event_id,
                 data,
                 stream_id=stream_id,
             )

--- a/synapse/storage/databases/main/registration.py
+++ b/synapse/storage/databases/main/registration.py
@@ -7,21 +7,21 @@
 from synapse.storage._base import SQLBaseStore
 from synapse.storage.database import DatabasePool
 from synapse.storage.types import Cursor
 from synapse.storage.util.sequence import build_sequence_generator
 from synapse.types import UserID
 from synapse.util.caches.descriptors import cached
 THIRTY_MINUTES_IN_MS = 30 * 60 * 1000
 logger = logging.getLogger(__name__)
 class RegistrationWorkerStore(SQLBaseStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(RegistrationWorkerStore, self).__init__(database, db_conn, hs)
         self.config = hs.config
         self.clock = hs.get_clock()
         self._user_id_seq = build_sequence_generator(
             database.engine, find_max_generated_user_id_localpart, "user_id_seq",
         )
     @cached()
     async def get_user_by_id(self, user_id: str) -> Optional[Dict[str, Any]]:
         return await self.db_pool.simple_select_one(
             table="users",
             keyvalues={"name": user_id},
@@ -75,31 +75,20 @@
             None, if the account has no expiration timestamp, otherwise int
             representation of the timestamp (as a number of milliseconds since epoch).
         """
         return await self.db_pool.simple_select_one_onecol(
             table="account_validity",
             keyvalues={"user_id": user_id},
             retcol="expiration_ts_ms",
             allow_none=True,
             desc="get_expiration_ts_for_user",
         )
-    async def is_account_expired(self, user_id: str, current_ts: int) -> bool:
-        """
-        Returns whether an user account is expired.
-        Args:
-            user_id: The user's ID
-            current_ts: The current timestamp
-        Returns:
-            Whether the user account has expired
-        """
-        expiration_ts = await self.get_expiration_ts_for_user(user_id)
-        return expiration_ts is not None and current_ts >= expiration_ts
     async def set_account_validity_for_user(
         self,
         user_id: str,
         expiration_ts: int,
         email_sent: bool,
         renewal_token: Optional[str] = None,
     ) -> None:
         """Updates the account validity properties of the given account, with the
         given values.
         Args:
@@ -305,27 +294,27 @@
         Returns:
              A mapping of user_id -> password_hash.
         """
         def f(txn):
             sql = "SELECT name, password_hash FROM users WHERE lower(name) = lower(?)"
             txn.execute(sql, (user_id,))
             return dict(txn)
         return await self.db_pool.runInteraction("get_users_by_id_case_insensitive", f)
     async def get_user_by_external_id(
         self, auth_provider: str, external_id: str
-    ) -> Optional[str]:
+    ) -> str:
         """Look up a user by their external auth id
         Args:
             auth_provider: identifier for the remote auth provider
             external_id: id on that system
         Returns:
-            the mxid of the user, or None if they are not known
+            str|None: the mxid of the user, or None if they are not known
         """
         return await self.db_pool.simple_select_one_onecol(
             table="user_external_ids",
             keyvalues={"auth_provider": auth_provider, "external_id": external_id},
             retcol="user_id",
             allow_none=True,
             desc="get_user_by_external_id",
         )
     async def count_all_users(self):
         """Counts all users registered on the homeserver."""
@@ -625,21 +614,21 @@
             self.db_pool.simple_delete_txn(
                 txn,
                 table="threepid_validation_session",
                 keyvalues={"session_id": session_id},
             )
         await self.db_pool.runInteraction(
             "delete_threepid_session", delete_threepid_session_txn
         )
 class RegistrationBackgroundUpdateStore(RegistrationWorkerStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(RegistrationBackgroundUpdateStore, self).__init__(database, db_conn, hs)
         self.clock = hs.get_clock()
         self.config = hs.config
         self.db_pool.updates.register_background_index_update(
             "access_tokens_device_index",
             index_name="access_tokens_device_id",
             table="access_tokens",
             columns=["user_id", "device_id"],
         )
         self.db_pool.updates.register_background_index_update(
             "users_creation_ts",
@@ -722,21 +711,21 @@
             """
             txn.executemany(sql, [(id_server,) for id_server in id_servers])
         if id_servers:
             await self.db_pool.runInteraction(
                 "_bg_user_threepids_grandfather", _bg_user_threepids_grandfather_txn
             )
         await self.db_pool.updates._end_background_update("user_threepids_grandfather")
         return 1
 class RegistrationStore(RegistrationBackgroundUpdateStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(RegistrationStore, self).__init__(database, db_conn, hs)
         self._account_validity = hs.config.account_validity
         self._ignore_unknown_session_error = hs.config.request_token_inhibit_3pid_errors
         if self._account_validity.enabled:
             self._clock.call_later(
                 0.0,
                 run_as_background_process,
                 "account_validity_set_expiration_dates",
                 self._set_expiration_date_when_missing,
             )
         def start_cull():

--- a/synapse/storage/databases/main/room.py
+++ b/synapse/storage/databases/main/room.py
@@ -33,21 +33,21 @@
     CREATOR = "creator"
     ENCRYPTION = "encryption"
     FEDERATABLE = "federatable"
     PUBLIC = "public"
     JOIN_RULES = "join_rules"
     GUEST_ACCESS = "guest_access"
     HISTORY_VISIBILITY = "history_visibility"
     STATE_EVENTS = "state_events"
 class RoomWorkerStore(SQLBaseStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(RoomWorkerStore, self).__init__(database, db_conn, hs)
         self.config = hs.config
     async def get_room(self, room_id: str) -> dict:
         """Retrieve a room.
         Args:
             room_id: The ID of the room to retrieve.
         Returns:
             A dict containing the room information, or None if the room is unknown.
         """
         return await self.db_pool.simple_select_one(
             table="rooms",
@@ -62,22 +62,21 @@
             room_id: The ID of the room to retrieve.
         Returns:
             A dict containing the room information, or None if the room is unknown.
         """
         def get_room_with_stats_txn(txn, room_id):
             sql = """
                 SELECT room_id, state.name, state.canonical_alias, curr.joined_members,
                   curr.local_users_in_room AS joined_local_members, rooms.room_version AS version,
                   rooms.creator, state.encryption, state.is_federatable AS federatable,
                   rooms.is_public AS public, state.join_rules, state.guest_access,
-                  state.history_visibility, curr.current_state_events AS state_events,
-                  state.avatar, state.topic
+                  state.history_visibility, curr.current_state_events AS state_events
                 FROM rooms
                 LEFT JOIN room_stats_state state USING (room_id)
                 LEFT JOIN room_stats_current curr USING (room_id)
                 WHERE room_id = ?
                 """
             txn.execute(sql, [room_id])
             try:
                 res = self.db_pool.cursor_to_dict(txn)[0]
             except IndexError:
                 return None
@@ -673,21 +672,21 @@
                 upto_token = updates[-1][0]
                 limited = True
             return updates, upto_token, limited
         return await self.db_pool.runInteraction(
             "get_all_new_public_rooms", get_all_new_public_rooms
         )
 class RoomBackgroundUpdateStore(SQLBaseStore):
     REMOVE_TOMESTONED_ROOMS_BG_UPDATE = "remove_tombstoned_rooms_from_directory"
     ADD_ROOMS_ROOM_VERSION_COLUMN = "add_rooms_room_version_column"
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(RoomBackgroundUpdateStore, self).__init__(database, db_conn, hs)
         self.config = hs.config
         self.db_pool.updates.register_background_update_handler(
             "insert_room_retention", self._background_insert_retention,
         )
         self.db_pool.updates.register_background_update_handler(
             self.REMOVE_TOMESTONED_ROOMS_BG_UPDATE,
             self._remove_tombstoned_rooms_from_directory,
         )
         self.db_pool.updates.register_background_update_handler(
             self.ADD_ROOMS_ROOM_VERSION_COLUMN,
@@ -832,21 +831,21 @@
             await self.set_room_is_public(room_id, False)
         await self.db_pool.updates._background_update_progress(
             self.REMOVE_TOMESTONED_ROOMS_BG_UPDATE, {"room_id": rooms[-1]}
         )
         return len(rooms)
     @abstractmethod
     def set_room_is_public(self, room_id, is_public):
         raise NotImplementedError()
 class RoomStore(RoomBackgroundUpdateStore, RoomWorkerStore, SearchStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(RoomStore, self).__init__(database, db_conn, hs)
         self.config = hs.config
     async def upsert_room_on_join(self, room_id: str, room_version: RoomVersion):
         """Ensure that the room is stored in the table
         Called when we join a room over federation, and overwrites any room version
         currently in the table.
         """
         await self.db_pool.simple_upsert(
             desc="upsert_room_on_join",
             table="rooms",
             keyvalues={"room_id": room_id},
@@ -886,21 +885,21 @@
                 if is_public:
                     self.db_pool.simple_insert_txn(
                         txn,
                         table="public_room_list_stream",
                         values={
                             "stream_id": next_id,
                             "room_id": room_id,
                             "visibility": is_public,
                         },
                     )
-            async with self._public_room_id_gen.get_next() as next_id:
+            with await self._public_room_id_gen.get_next() as next_id:
                 await self.db_pool.runInteraction(
                     "store_room_txn", store_room_txn, next_id
                 )
         except Exception as e:
             logger.error("store_room with room_id=%s failed: %s", room_id, e)
             raise StoreError(500, "Problem creating room.")
     async def maybe_store_room_on_invite(self, room_id: str, room_version: RoomVersion):
         """
         When we receive an invite over federation, store the version of the room if we
         don't already know the room version.
@@ -944,21 +943,21 @@
                     txn,
                     table="public_room_list_stream",
                     values={
                         "stream_id": next_id,
                         "room_id": room_id,
                         "visibility": is_public,
                         "appservice_id": None,
                         "network_id": None,
                     },
                 )
-        async with self._public_room_id_gen.get_next() as next_id:
+        with await self._public_room_id_gen.get_next() as next_id:
             await self.db_pool.runInteraction(
                 "set_room_is_public", set_room_is_public_txn, next_id
             )
         self.hs.get_notifier().on_new_replication_data()
     async def set_room_is_public_appservice(
         self, room_id, appservice_id, network_id, is_public
     ):
         """Edit the appservice/network specific public room list.
         Each appservice can have a number of published room lists associated
         with them, keyed off of an appservice defined `network_id`, which
@@ -1014,21 +1013,21 @@
                     txn,
                     table="public_room_list_stream",
                     values={
                         "stream_id": next_id,
                         "room_id": room_id,
                         "visibility": is_public,
                         "appservice_id": appservice_id,
                         "network_id": network_id,
                     },
                 )
-        async with self._public_room_id_gen.get_next() as next_id:
+        with await self._public_room_id_gen.get_next() as next_id:
             await self.db_pool.runInteraction(
                 "set_room_is_public_appservice",
                 set_room_is_public_appservice_txn,
                 next_id,
             )
         self.hs.get_notifier().on_new_replication_data()
     async def get_room_count(self) -> int:
         """Retrieve the total number of rooms.
         """
         def f(txn):
@@ -1052,103 +1051,20 @@
             values={
                 "id": next_id,
                 "received_ts": received_ts,
                 "room_id": room_id,
                 "event_id": event_id,
                 "user_id": user_id,
                 "reason": reason,
                 "content": json_encoder.encode(content),
             },
             desc="add_event_report",
-        )
-    async def get_event_reports_paginate(
-        self,
-        start: int,
-        limit: int,
-        direction: str = "b",
-        user_id: Optional[str] = None,
-        room_id: Optional[str] = None,
-    ) -> Tuple[List[Dict[str, Any]], int]:
-        """Retrieve a paginated list of event reports
-        Args:
-            start: event offset to begin the query from
-            limit: number of rows to retrieve
-            direction: Whether to fetch the most recent first (`"b"`) or the
-                oldest first (`"f"`)
-            user_id: search for user_id. Ignored if user_id is None
-            room_id: search for room_id. Ignored if room_id is None
-        Returns:
-            event_reports: json list of event reports
-            count: total number of event reports matching the filter criteria
-        """
-        def _get_event_reports_paginate_txn(txn):
-            filters = []
-            args = []
-            if user_id:
-                filters.append("er.user_id LIKE ?")
-                args.extend(["%" + user_id + "%"])
-            if room_id:
-                filters.append("er.room_id LIKE ?")
-                args.extend(["%" + room_id + "%"])
-            if direction == "b":
-                order = "DESC"
-            else:
-                order = "ASC"
-            where_clause = "WHERE " + " AND ".join(filters) if len(filters) > 0 else ""
-            sql = """
-                SELECT COUNT(*) as total_event_reports
-                FROM event_reports AS er
-                {}
-                """.format(
-                where_clause
-            )
-            txn.execute(sql, args)
-            count = txn.fetchone()[0]
-            sql = """
-                SELECT
-                    er.id,
-                    er.received_ts,
-                    er.room_id,
-                    er.event_id,
-                    er.user_id,
-                    er.reason,
-                    er.content,
-                    events.sender,
-                    room_aliases.room_alias,
-                    event_json.json AS event_json
-                FROM event_reports AS er
-                LEFT JOIN room_aliases
-                    ON room_aliases.room_id = er.room_id
-                JOIN events
-                    ON events.event_id = er.event_id
-                JOIN event_json
-                    ON event_json.event_id = er.event_id
-                {where_clause}
-                ORDER BY er.received_ts {order}
-                LIMIT ?
-                OFFSET ?
-            """.format(
-                where_clause=where_clause, order=order,
-            )
-            args += [limit, start]
-            txn.execute(sql, args)
-            event_reports = self.db_pool.cursor_to_dict(txn)
-            if count > 0:
-                for row in event_reports:
-                    try:
-                        row["content"] = db_to_json(row["content"])
-                        row["event_json"] = db_to_json(row["event_json"])
-                    except Exception:
-                        continue
-            return event_reports, count
-        return await self.db_pool.runInteraction(
-            "get_event_reports_paginate", _get_event_reports_paginate_txn
         )
     def get_current_public_room_stream_id(self):
         return self._public_room_id_gen.get_current_token()
     async def block_room(self, room_id: str, user_id: str) -> None:
         """Marks the room as blocked. Can be called multiple times.
         Args:
             room_id: Room to block
             user_id: Who blocked it
         """
         await self.db_pool.simple_upsert(

--- a/synapse/storage/databases/main/roommember.py
+++ b/synapse/storage/databases/main/roommember.py
@@ -13,33 +13,33 @@
 )
 from synapse.storage.database import DatabasePool
 from synapse.storage.databases.main.events_worker import EventsWorkerStore
 from synapse.storage.engines import Sqlite3Engine
 from synapse.storage.roommember import (
     GetRoomsForUserWithStreamOrdering,
     MemberSummary,
     ProfileInfo,
     RoomsForUser,
 )
-from synapse.types import Collection, PersistedEventPosition, get_domain_from_id
+from synapse.types import Collection, get_domain_from_id
 from synapse.util.async_helpers import Linearizer
 from synapse.util.caches import intern_string
 from synapse.util.caches.descriptors import _CacheContext, cached, cachedList
 from synapse.util.metrics import Measure
 if TYPE_CHECKING:
     from synapse.state import _StateCacheEntry
 logger = logging.getLogger(__name__)
 _MEMBERSHIP_PROFILE_UPDATE_NAME = "room_membership_profile_update"
 _CURRENT_STATE_MEMBERSHIP_UPDATE_NAME = "current_state_events_membership"
 class RoomMemberWorkerStore(EventsWorkerStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(RoomMemberWorkerStore, self).__init__(database, db_conn, hs)
         self._current_state_events_membership_up_to_date = False
         txn = LoggingTransaction(
             db_conn.cursor(),
             name="_check_safe_current_state_events_membership_updated",
             database_engine=self.database_engine,
         )
         self._check_safe_current_state_events_membership_updated_txn(txn)
         txn.close()
         if self.hs.config.metrics_flags.known_servers:
             self._known_servers_count = 1
@@ -283,46 +283,41 @@
         return await self.db_pool.runInteraction(
             "get_rooms_for_user_with_stream_ordering",
             self._get_rooms_for_user_with_stream_ordering_txn,
             user_id,
         )
     def _get_rooms_for_user_with_stream_ordering_txn(
         self, txn, user_id: str
     ) -> FrozenSet[GetRoomsForUserWithStreamOrdering]:
         if self._current_state_events_membership_up_to_date:
             sql = """
-                SELECT room_id, e.instance_name, e.stream_ordering
+                SELECT room_id, e.stream_ordering
                 FROM current_state_events AS c
                 INNER JOIN events AS e USING (room_id, event_id)
                 WHERE
                     c.type = 'm.room.member'
                     AND state_key = ?
                     AND c.membership = ?
             """
         else:
             sql = """
-                SELECT room_id, e.instance_name, e.stream_ordering
+                SELECT room_id, e.stream_ordering
                 FROM current_state_events AS c
                 INNER JOIN room_memberships AS m USING (room_id, event_id)
                 INNER JOIN events AS e USING (room_id, event_id)
                 WHERE
                     c.type = 'm.room.member'
                     AND state_key = ?
                     AND m.membership = ?
             """
         txn.execute(sql, (user_id, Membership.JOIN))
-        return frozenset(
-            GetRoomsForUserWithStreamOrdering(
-                room_id, PersistedEventPosition(instance, stream_id)
-            )
-            for room_id, instance, stream_id in txn
-        )
+        return frozenset(GetRoomsForUserWithStreamOrdering(*row) for row in txn)
     async def get_users_server_still_shares_room_with(
         self, user_ids: Collection[str]
     ) -> Set[str]:
         """Given a list of users return the set that the server still share a
         room with.
         """
         if not user_ids:
             return set()
         def _get_users_server_still_shares_room_with_txn(txn):
             sql = """
@@ -611,21 +606,21 @@
         )
         def _is_local_host_in_room_ignoring_users_txn(txn):
             txn.execute(sql, (room_id, Membership.JOIN, *args))
             return bool(txn.fetchone())
         return await self.db_pool.runInteraction(
             "is_local_host_in_room_ignoring_users",
             _is_local_host_in_room_ignoring_users_txn,
         )
 class RoomMemberBackgroundUpdateStore(SQLBaseStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(RoomMemberBackgroundUpdateStore, self).__init__(database, db_conn, hs)
         self.db_pool.updates.register_background_update_handler(
             _MEMBERSHIP_PROFILE_UPDATE_NAME, self._background_add_membership_profile
         )
         self.db_pool.updates.register_background_update_handler(
             _CURRENT_STATE_MEMBERSHIP_UPDATE_NAME,
             self._background_current_state_membership,
         )
         self.db_pool.updates.register_background_index_update(
             "room_membership_forgotten_idx",
             index_name="room_memberships_user_room_forgotten",
@@ -733,21 +728,21 @@
             _background_current_state_membership_txn,
             last_processed_room,
         )
         if finished:
             await self.db_pool.updates._end_background_update(
                 _CURRENT_STATE_MEMBERSHIP_UPDATE_NAME
             )
         return row_count
 class RoomMemberStore(RoomMemberWorkerStore, RoomMemberBackgroundUpdateStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(RoomMemberStore, self).__init__(database, db_conn, hs)
     async def forget(self, user_id: str, room_id: str) -> None:
         """Indicate that user_id wishes to discard history for room_id."""
         def f(txn):
             sql = (
                 "UPDATE"
                 "  room_memberships"
                 " SET"
                 "  forgotten = 1"
                 " WHERE"
                 "  user_id = ?"

--- a/synapse/storage/databases/main/search.py
+++ b/synapse/storage/databases/main/search.py
@@ -52,21 +52,21 @@
             )
             txn.executemany(sql, args)
         else:
             raise Exception("Unrecognized database engine")
 class SearchBackgroundUpdateStore(SearchWorkerStore):
     EVENT_SEARCH_UPDATE_NAME = "event_search"
     EVENT_SEARCH_ORDER_UPDATE_NAME = "event_search_order"
     EVENT_SEARCH_USE_GIST_POSTGRES_NAME = "event_search_postgres_gist"
     EVENT_SEARCH_USE_GIN_POSTGRES_NAME = "event_search_postgres_gin"
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(SearchBackgroundUpdateStore, self).__init__(database, db_conn, hs)
         if not hs.config.enable_search:
             return
         self.db_pool.updates.register_background_update_handler(
             self.EVENT_SEARCH_UPDATE_NAME, self._background_reindex_search
         )
         self.db_pool.updates.register_background_update_handler(
             self.EVENT_SEARCH_ORDER_UPDATE_NAME, self._background_reindex_search_order
         )
         self.db_pool.updates.register_noop_background_update(
             self.EVENT_SEARCH_USE_GIST_POSTGRES_NAME
@@ -233,21 +233,21 @@
         num_rows, finished = await self.db_pool.runInteraction(
             self.EVENT_SEARCH_ORDER_UPDATE_NAME, reindex_search_txn
         )
         if not finished:
             await self.db_pool.updates._end_background_update(
                 self.EVENT_SEARCH_ORDER_UPDATE_NAME
             )
         return num_rows
 class SearchStore(SearchBackgroundUpdateStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(SearchStore, self).__init__(database, db_conn, hs)
     async def search_msgs(self, room_ids, search_term, keys):
         """Performs a full text search over events with given keys.
         Args:
             room_ids (list): List of room ids to search in
             search_term (str): Search term to search for
             keys (list): List of keys to search in, currently supports
                 "content.body", "content.name", "content.topic"
         Returns:
             list of dicts
         """

--- a/synapse/storage/databases/main/state.py
+++ b/synapse/storage/databases/main/state.py
@@ -22,21 +22,21 @@
     """Return type of get_state_group_delta that implements __len__, which lets
     us use the itrable flag when caching
     """
     __slots__ = []
     def __len__(self):
         return len(self.delta_ids) if self.delta_ids else 0
 class StateGroupWorkerStore(EventsWorkerStore, SQLBaseStore):
     """The parts of StateGroupStore that can be called from workers.
     """
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(StateGroupWorkerStore, self).__init__(database, db_conn, hs)
     async def get_room_version(self, room_id: str) -> RoomVersion:
         """Get the room_version of a given room
         Raises:
             NotFoundError: if the room is unknown
             UnsupportedRoomVersionError: if the room uses an unknown room version.
                 Typically this happens if support for the room's version has been
                 removed from Synapse.
         """
         room_version_id = await self.get_room_version_id(room_id)
         v = KNOWN_ROOM_VERSIONS.get(room_version_id)
@@ -212,21 +212,21 @@
             keyvalues={},
             retcols=("DISTINCT state_group",),
             desc="get_referenced_state_groups",
         )
         return {row["state_group"] for row in rows}
 class MainStateBackgroundUpdateStore(RoomMemberWorkerStore):
     CURRENT_STATE_INDEX_UPDATE_NAME = "current_state_members_idx"
     EVENT_STATE_GROUP_INDEX_UPDATE_NAME = "event_to_state_groups_sg_index"
     DELETE_CURRENT_STATE_UPDATE_NAME = "delete_old_current_state_events"
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(MainStateBackgroundUpdateStore, self).__init__(database, db_conn, hs)
         self.server_name = hs.hostname
         self.db_pool.updates.register_background_index_update(
             self.CURRENT_STATE_INDEX_UPDATE_NAME,
             index_name="current_state_events_member_index",
             table="current_state_events",
             columns=["state_key"],
             where_clause="type='m.room.member'",
         )
         self.db_pool.updates.register_background_index_update(
             self.EVENT_STATE_GROUP_INDEX_UPDATE_NAME,
@@ -330,11 +330,11 @@
     Hence, every change in the current state causes a new state group to be
     generated. However, if no change happens (e.g., if we get a message event
     with only one parent it inherits the state group from its parent.)
     There are three tables:
       * `state_groups`: Stores group name, first event with in the group and
         room id.
       * `event_to_state_groups`: Maps events to state groups.
       * `state_groups_state`: Maps state group to state events.
     """
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(StateStore, self).__init__(database, db_conn, hs)

--- a/synapse/storage/databases/main/stats.py
+++ b/synapse/storage/databases/main/stats.py
@@ -21,28 +21,31 @@
     "user": ("joined_rooms",),
 }
 PER_SLICE_FIELDS = {
     "room": ("total_events", "total_event_bytes"),
     "user": ("invites_sent", "rooms_created", "total_events", "total_event_bytes"),
 }
 TYPE_TO_TABLE = {"room": ("room_stats", "room_id"), "user": ("user_stats", "user_id")}
 TYPE_TO_ORIGIN_TABLE = {"room": ("rooms", "room_id"), "user": ("users", "name")}
 class StatsStore(StateDeltasStore):
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(StatsStore, self).__init__(database, db_conn, hs)
         self.server_name = hs.hostname
         self.clock = self.hs.get_clock()
         self.stats_enabled = hs.config.stats_enabled
         self.stats_bucket_size = hs.config.stats_bucket_size
         self.stats_delta_processing_lock = DeferredLock()
         self.db_pool.updates.register_background_update_handler(
             "populate_stats_process_rooms", self._populate_stats_process_rooms
+        )
+        self.db_pool.updates.register_background_update_handler(
+            "populate_stats_process_rooms_2", self._populate_stats_process_rooms_2
         )
         self.db_pool.updates.register_background_update_handler(
             "populate_stats_process_users", self._populate_stats_process_users
         )
         self.db_pool.updates.register_noop_background_update("populate_stats_cleanup")
         self.db_pool.updates.register_noop_background_update("populate_stats_prepare")
     def quantise_stats_time(self, ts):
         """
         Quantises a timestamp to be a multiple of the bucket size.
         Args:
@@ -86,51 +89,68 @@
             await self._calculate_and_set_initial_state_for_user(user_id)
             progress["last_user_id"] = user_id
         await self.db_pool.runInteraction(
             "populate_stats_process_users",
             self.db_pool.updates._background_update_progress_txn,
             "populate_stats_process_users",
             progress,
         )
         return len(users_to_work_on)
     async def _populate_stats_process_rooms(self, progress, batch_size):
-        """This is a background update which regenerates statistics for rooms."""
+        """
+        This was a background update which regenerated statistics for rooms.
+        It has been replaced by StatsStore._populate_stats_process_rooms_2. This background
+        job has been scheduled to run as part of Synapse v1.0.0, and again now. To ensure
+        someone upgrading from <v1.0.0, this background task has been turned into a no-op
+        so that the potentially expensive task is not run twice.
+        Further context: https://github.com/matrix-org/synapse/pull/7977
+        """
+        await self.db_pool.updates._end_background_update(
+            "populate_stats_process_rooms"
+        )
+        return 1
+    async def _populate_stats_process_rooms_2(self, progress, batch_size):
+        """
+        This is a background update which regenerates statistics for rooms.
+        It replaces StatsStore._populate_stats_process_rooms. See its docstring for the
+        reasoning.
+        """
         if not self.stats_enabled:
             await self.db_pool.updates._end_background_update(
-                "populate_stats_process_rooms"
+                "populate_stats_process_rooms_2"
             )
             return 1
         last_room_id = progress.get("last_room_id", "")
         def _get_next_batch(txn):
             sql = """
                     SELECT DISTINCT room_id FROM current_state_events
                     WHERE room_id > ?
                     ORDER BY room_id ASC
                     LIMIT ?
                 """
             txn.execute(sql, (last_room_id, batch_size))
             return [r for r, in txn]
         rooms_to_work_on = await self.db_pool.runInteraction(
-            "populate_stats_rooms_get_batch", _get_next_batch
+            "populate_stats_rooms_2_get_batch", _get_next_batch
         )
         if not rooms_to_work_on:
             await self.db_pool.updates._end_background_update(
-                "populate_stats_process_rooms"
+                "populate_stats_process_rooms_2"
             )
             return 1
         for room_id in rooms_to_work_on:
             await self._calculate_and_set_initial_state_for_room(room_id)
             progress["last_room_id"] = room_id
         await self.db_pool.runInteraction(
-            "_populate_stats_process_rooms",
+            "_populate_stats_process_rooms_2",
             self.db_pool.updates._background_update_progress_txn,
-            "populate_stats_process_rooms",
+            "populate_stats_process_rooms_2",
             progress,
         )
         return len(rooms_to_work_on)
     async def get_stats_positions(self) -> int:
         """
         Returns the stats processor positions.
         """
         return await self.db_pool.simple_select_one_onecol(
             table="stats_incremental_position",
             keyvalues={},
@@ -140,37 +160,35 @@
     async def update_room_state(self, room_id: str, fields: Dict[str, Any]) -> None:
         """Update the state of a room.
         fields can contain the following keys with string values:
         * join_rules
         * history_visibility
         * encryption
         * name
         * topic
         * avatar
         * canonical_alias
-        * guest_access
         A is_federatable key can also be included with a boolean value.
         Args:
             room_id: The room ID to update the state of.
             fields: The fields to update. This can include a partial list of the
                 above fields to only update some room information.
         """
         sentinel = object()
         for col in (
             "join_rules",
             "history_visibility",
             "encryption",
             "name",
             "topic",
             "avatar",
             "canonical_alias",
-            "guest_access",
         ):
             field = fields.get(col, sentinel)
             if field is not sentinel and (not isinstance(field, str) or "\0" in field):
                 fields[col] = None
         await self.db_pool.simple_upsert(
             table="room_stats_state",
             keyvalues={"room_id": room_id},
             values=fields,
             desc="update_room_state",
         )

--- a/synapse/storage/databases/main/stream.py
+++ b/synapse/storage/databases/main/stream.py
@@ -23,36 +23,36 @@
 from synapse.events import EventBase
 from synapse.logging.context import make_deferred_yieldable, run_in_background
 from synapse.storage._base import SQLBaseStore
 from synapse.storage.database import (
     DatabasePool,
     LoggingTransaction,
     make_in_list_sql_clause,
 )
 from synapse.storage.databases.main.events_worker import EventsWorkerStore
 from synapse.storage.engines import BaseDatabaseEngine, PostgresEngine
-from synapse.types import Collection, PersistedEventPosition, RoomStreamToken
+from synapse.types import Collection, RoomStreamToken
 from synapse.util.caches.stream_change_cache import StreamChangeCache
 if TYPE_CHECKING:
     from synapse.server import HomeServer
 logger = logging.getLogger(__name__)
 MAX_STREAM_SIZE = 1000
 _STREAM_TOKEN = "stream"
 _TOPOLOGICAL_TOKEN = "topological"
 _EventDictReturn = namedtuple(
     "_EventDictReturn", ("event_id", "topological_ordering", "stream_ordering")
 )
 def generate_pagination_where_clause(
     direction: str,
     column_names: Tuple[str, str],
-    from_token: Optional[Tuple[Optional[int], int]],
-    to_token: Optional[Tuple[Optional[int], int]],
+    from_token: Optional[Tuple[int, int]],
+    to_token: Optional[Tuple[int, int]],
     engine: BaseDatabaseEngine,
 ) -> str:
     """Creates an SQL expression to bound the columns by the pagination
     tokens.
     For example creates an SQL expression like:
         (6, 7) >= (topological_ordering, stream_ordering)
         AND (5, 3) < (topological_ordering, stream_ordering)
     would be generated for dir=b, from_token=(6, 7) and to_token=(5, 3).
     Note that tokens are considered to be after the row they are in, e.g. if
     a row A has a token T, then we consider A to be before T. This convention
@@ -165,27 +165,28 @@
     for room_id in event_filter.not_rooms:
         clauses.append("room_id != ?")
         args.append(room_id)
     if event_filter.contains_url:
         clauses.append("contains_url = ?")
         args.append(event_filter.contains_url)
     if event_filter.labels:
         clauses.append("(%s)" % " OR ".join("label = ?" for _ in event_filter.labels))
         args.extend(event_filter.labels)
     return " AND ".join(clauses), args
-class StreamWorkerStore(EventsWorkerStore, SQLBaseStore, metaclass=abc.ABCMeta):
+class StreamWorkerStore(EventsWorkerStore, SQLBaseStore):
     """This is an abstract base class where subclasses must implement
     `get_room_max_stream_ordering` and `get_room_min_stream_ordering`
     which can be called in the initializer.
     """
+    __metaclass__ = abc.ABCMeta
     def __init__(self, database: DatabasePool, db_conn, hs: "HomeServer"):
-        super().__init__(database, db_conn, hs)
+        super(StreamWorkerStore, self).__init__(database, db_conn, hs)
         self._instance_name = hs.get_instance_name()
         self._send_federation = hs.should_send_federation()
         self._federation_shard_config = hs.config.worker.federation_shard_config
         self._need_to_reset_federation_stream_positions = self._send_federation
         events_max = self.get_room_max_stream_ordering()
         event_cache_prefill, min_event_val = self.db_pool.get_cache_dict(
             db_conn,
             "events",
             entity_column="room_id",
             stream_column="stream_ordering",
@@ -199,49 +200,46 @@
         self._membership_stream_cache = StreamChangeCache(
             "MembershipStreamChangeCache", events_max
         )
         self._stream_order_on_start = self.get_room_max_stream_ordering()
     @abc.abstractmethod
     def get_room_max_stream_ordering(self) -> int:
         raise NotImplementedError()
     @abc.abstractmethod
     def get_room_min_stream_ordering(self) -> int:
         raise NotImplementedError()
-    def get_room_max_token(self) -> RoomStreamToken:
-        return RoomStreamToken(None, self.get_room_max_stream_ordering())
     async def get_room_events_stream_for_rooms(
         self,
         room_ids: Collection[str],
-        from_key: RoomStreamToken,
-        to_key: RoomStreamToken,
+        from_key: str,
+        to_key: str,
         limit: int = 0,
         order: str = "DESC",
-    ) -> Dict[str, Tuple[List[EventBase], RoomStreamToken]]:
+    ) -> Dict[str, Tuple[List[EventBase], str]]:
         """Get new room events in stream ordering since `from_key`.
         Args:
             room_ids
             from_key: Token from which no events are returned before
             to_key: Token from which no events are returned after. (This
                 is typically the current stream token)
             limit: Maximum number of events to return
             order: Either "DESC" or "ASC". Determines which events are
                 returned when the result is limited. If "DESC" then the most
                 recent `limit` events are returned, otherwise returns the
                 oldest `limit` events.
         Returns:
             A map from room id to a tuple containing:
                 - list of recent events in the room
                 - stream ordering key for the start of the chunk of events returned.
         """
-        room_ids = self._events_stream_cache.get_entities_changed(
-            room_ids, from_key.stream
-        )
+        from_id = RoomStreamToken.parse_stream_token(from_key).stream
+        room_ids = self._events_stream_cache.get_entities_changed(room_ids, from_id)
         if not room_ids:
             return {}
         results = {}
         room_ids = list(room_ids)
         for rm_ids in (room_ids[i : i + 20] for i in range(0, len(room_ids), 20)):
             res = await make_deferred_yieldable(
                 defer.gatherResults(
                     [
                         run_in_background(
                             self.get_room_events_stream_for_room,
@@ -252,58 +250,61 @@
                             order=order,
                         )
                         for room_id in rm_ids
                     ],
                     consumeErrors=True,
                 )
             )
             results.update(dict(zip(rm_ids, res)))
         return results
     def get_rooms_that_changed(
-        self, room_ids: Collection[str], from_key: RoomStreamToken
+        self, room_ids: Collection[str], from_key: str
     ) -> Set[str]:
         """Given a list of rooms and a token, return rooms where there may have
         been changes.
-        """
-        from_id = from_key.stream
+        Args:
+            room_ids
+            from_key: The room_key portion of a StreamToken
+        """
+        from_id = RoomStreamToken.parse_stream_token(from_key).stream
         return {
             room_id
             for room_id in room_ids
             if self._events_stream_cache.has_entity_changed(room_id, from_id)
         }
     async def get_room_events_stream_for_room(
         self,
         room_id: str,
-        from_key: RoomStreamToken,
-        to_key: RoomStreamToken,
+        from_key: str,
+        to_key: str,
         limit: int = 0,
         order: str = "DESC",
-    ) -> Tuple[List[EventBase], RoomStreamToken]:
+    ) -> Tuple[List[EventBase], str]:
         """Get new room events in stream ordering since `from_key`.
         Args:
             room_id
             from_key: Token from which no events are returned before
             to_key: Token from which no events are returned after. (This
                 is typically the current stream token)
             limit: Maximum number of events to return
             order: Either "DESC" or "ASC". Determines which events are
                 returned when the result is limited. If "DESC" then the most
                 recent `limit` events are returned, otherwise returns the
                 oldest `limit` events.
         Returns:
             The list of events (in ascending order) and the token from the start
             of the chunk of events returned.
         """
         if from_key == to_key:
             return [], from_key
-        from_id = from_key.stream
-        to_id = to_key.stream
+        from_id = RoomStreamToken.parse_stream_token(from_key).stream
+        to_id = RoomStreamToken.parse_stream_token(to_key).stream
         has_changed = self._events_stream_cache.has_entity_changed(room_id, from_id)
         if not has_changed:
             return [], from_key
         def f(txn):
             sql = (
                 "SELECT event_id, stream_ordering FROM events WHERE"
                 " room_id = ?"
                 " AND not outlier"
                 " AND stream_ordering > ? AND stream_ordering <= ?"
                 " ORDER BY stream_ordering %s LIMIT ?"
@@ -312,29 +313,29 @@
             rows = [_EventDictReturn(row[0], None, row[1]) for row in txn]
             return rows
         rows = await self.db_pool.runInteraction("get_room_events_stream_for_room", f)
         ret = await self.get_events_as_list(
             [r.event_id for r in rows], get_prev_content=True
         )
         self._set_before_and_after(ret, rows, topo_order=from_id is None)
         if order.lower() == "desc":
             ret.reverse()
         if rows:
-            key = RoomStreamToken(None, min(r.stream_ordering for r in rows))
+            key = "s%d" % min(r.stream_ordering for r in rows)
         else:
             key = from_key
         return ret, key
     async def get_membership_changes_for_user(
-        self, user_id: str, from_key: RoomStreamToken, to_key: RoomStreamToken
+        self, user_id: str, from_key: str, to_key: str
     ) -> List[EventBase]:
-        from_id = from_key.stream
-        to_id = to_key.stream
+        from_id = RoomStreamToken.parse_stream_token(from_key).stream
+        to_id = RoomStreamToken.parse_stream_token(to_key).stream
         if from_key == to_key:
             return []
         if from_id:
             has_changed = self._membership_stream_cache.has_entity_changed(
                 user_id, int(from_id)
             )
             if not has_changed:
                 return []
         def f(txn):
             sql = (
@@ -348,53 +349,54 @@
             txn.execute(sql, (user_id, from_id, to_id))
             rows = [_EventDictReturn(row[0], None, row[1]) for row in txn]
             return rows
         rows = await self.db_pool.runInteraction("get_membership_changes_for_user", f)
         ret = await self.get_events_as_list(
             [r.event_id for r in rows], get_prev_content=True
         )
         self._set_before_and_after(ret, rows, topo_order=False)
         return ret
     async def get_recent_events_for_room(
-        self, room_id: str, limit: int, end_token: RoomStreamToken
-    ) -> Tuple[List[EventBase], RoomStreamToken]:
+        self, room_id: str, limit: int, end_token: str
+    ) -> Tuple[List[EventBase], str]:
         """Get the most recent events in the room in topological ordering.
         Args:
             room_id
             limit
             end_token: The stream token representing now.
         Returns:
             A list of events and a token pointing to the start of the returned
             events. The events returned are in ascending order.
         """
         rows, token = await self.get_recent_event_ids_for_room(
             room_id, limit, end_token
         )
         events = await self.get_events_as_list(
             [r.event_id for r in rows], get_prev_content=True
         )
         self._set_before_and_after(events, rows)
         return (events, token)
     async def get_recent_event_ids_for_room(
-        self, room_id: str, limit: int, end_token: RoomStreamToken
-    ) -> Tuple[List[_EventDictReturn], RoomStreamToken]:
+        self, room_id: str, limit: int, end_token: str
+    ) -> Tuple[List[_EventDictReturn], str]:
         """Get the most recent events in the room in topological ordering.
         Args:
             room_id
             limit
             end_token: The stream token representing now.
         Returns:
             A list of _EventDictReturn and a token pointing to the start of the
             returned events. The events returned are in ascending order.
         """
         if limit == 0:
             return [], end_token
+        end_token = RoomStreamToken.parse(end_token)
         rows, token = await self.db_pool.runInteraction(
             "get_recent_event_ids_for_room",
             self._paginate_room_events_txn,
             room_id,
             from_token=end_token,
             limit=limit,
         )
         rows.reverse()
         return rows, token
     async def get_room_event_before_stream_ordering(
@@ -450,48 +452,47 @@
     def get_stream_id_for_event_txn(
         self, txn: LoggingTransaction, event_id: str, allow_none=False,
     ) -> int:
         return self.db_pool.simple_select_one_onecol_txn(
             txn=txn,
             table="events",
             keyvalues={"event_id": event_id},
             retcol="stream_ordering",
             allow_none=allow_none,
         )
-    async def get_position_for_event(self, event_id: str) -> PersistedEventPosition:
-        """Get the persisted position for an event
-        """
-        row = await self.db_pool.simple_select_one(
-            table="events",
-            keyvalues={"event_id": event_id},
-            retcols=("stream_ordering", "instance_name"),
-            desc="get_position_for_event",
-        )
-        return PersistedEventPosition(
-            row["instance_name"] or "master", row["stream_ordering"]
-        )
-    async def get_topological_token_for_event(self, event_id: str) -> RoomStreamToken:
+    async def get_stream_token_for_event(self, event_id: str) -> str:
         """The stream token for an event
         Args:
             event_id: The id of the event to look up a stream token for.
         Raises:
             StoreError if the event wasn't in the database.
         Returns:
-            A `RoomStreamToken` topological token.
+            A "s%d" stream token.
+        """
+        stream_id = await self.get_stream_id_for_event(event_id)
+        return "s%d" % (stream_id,)
+    async def get_topological_token_for_event(self, event_id: str) -> str:
+        """The stream token for an event
+        Args:
+            event_id: The id of the event to look up a stream token for.
+        Raises:
+            StoreError if the event wasn't in the database.
+        Returns:
+            A "t%d-%d" topological token.
         """
         row = await self.db_pool.simple_select_one(
             table="events",
             keyvalues={"event_id": event_id},
             retcols=("stream_ordering", "topological_ordering"),
             desc="get_topological_token_for_event",
         )
-        return RoomStreamToken(row["topological_ordering"], row["stream_ordering"])
+        return "t%d-%d" % (row["topological_ordering"], row["stream_ordering"])
     async def get_current_topological_token(self, room_id: str, stream_key: int) -> int:
         """Gets the topological token in a room after or at the given stream
         ordering.
         Args:
             room_id
             stream_key
         """
         sql = (
             "SELECT coalesce(MIN(topological_ordering), 0) FROM events"
             " WHERE room_id = ? AND stream_ordering >= ?"
@@ -520,22 +521,22 @@
                 ordering. If true then all rows should have a non null
                 topological_ordering.
         """
         for event, row in zip(events, rows):
             stream = row.stream_ordering
             if topo_order and row.topological_ordering:
                 topo = row.topological_ordering
             else:
                 topo = None
             internal = event.internal_metadata
-            internal.before = RoomStreamToken(topo, stream - 1)
-            internal.after = RoomStreamToken(topo, stream)
+            internal.before = str(RoomStreamToken(topo, stream - 1))
+            internal.after = str(RoomStreamToken(topo, stream))
             internal.order = (int(topo) if topo else 0, int(stream))
     async def get_events_around(
         self,
         room_id: str,
         event_id: str,
         before_limit: int,
         after_limit: int,
         event_filter: Optional[Filter] = None,
     ) -> dict:
         """Retrieve events and pagination tokens around a given event in a
@@ -721,21 +722,21 @@
         return self._events_stream_cache.has_entity_changed(room_id, stream_id)
     def _paginate_room_events_txn(
         self,
         txn: LoggingTransaction,
         room_id: str,
         from_token: RoomStreamToken,
         to_token: Optional[RoomStreamToken] = None,
         direction: str = "b",
         limit: int = -1,
         event_filter: Optional[Filter] = None,
-    ) -> Tuple[List[_EventDictReturn], RoomStreamToken]:
+    ) -> Tuple[List[_EventDictReturn], str]:
         """Returns list of events before or after a given token.
         Args:
             txn
             room_id
             from_token: The token used to stream from
             to_token: A token which if given limits the results to only those before
             direction: Either 'b' or 'f' to indicate whether we are paginating
                 forwards or backwards from `from_key`.
             limit: The maximum number of events to return.
             event_filter: If provided filters the events to
@@ -748,22 +749,22 @@
         """
         assert int(limit) >= 0
         args = [False, room_id]
         if direction == "b":
             order = "DESC"
         else:
             order = "ASC"
         bounds = generate_pagination_where_clause(
             direction=direction,
             column_names=("topological_ordering", "stream_ordering"),
-            from_token=from_token.as_tuple(),
-            to_token=to_token.as_tuple() if to_token else None,
+            from_token=from_token,
+            to_token=to_token,
             engine=self.database_engine,
         )
         filter_clause, filter_args = filter_to_clause(event_filter)
         if filter_clause:
             bounds += " AND " + filter_clause
             args.extend(filter_args)
         args.append(int(limit))
         select_keywords = "SELECT"
         join_clause = ""
         if event_filter and event_filter.labels:
@@ -789,45 +790,48 @@
         txn.execute(sql, args)
         rows = [_EventDictReturn(row[0], row[1], row[2]) for row in txn]
         if rows:
             topo = rows[-1].topological_ordering
             toke = rows[-1].stream_ordering
             if direction == "b":
                 toke -= 1
             next_token = RoomStreamToken(topo, toke)
         else:
             next_token = to_token if to_token else from_token
-        return rows, next_token
+        return rows, str(next_token)
     async def paginate_room_events(
         self,
         room_id: str,
-        from_key: RoomStreamToken,
-        to_key: Optional[RoomStreamToken] = None,
+        from_key: str,
+        to_key: Optional[str] = None,
         direction: str = "b",
         limit: int = -1,
         event_filter: Optional[Filter] = None,
-    ) -> Tuple[List[EventBase], RoomStreamToken]:
+    ) -> Tuple[List[EventBase], str]:
         """Returns list of events before or after a given token.
         Args:
             room_id
             from_key: The token used to stream from
             to_key: A token which if given limits the results to only those before
             direction: Either 'b' or 'f' to indicate whether we are paginating
                 forwards or backwards from `from_key`.
             limit: The maximum number of events to return.
             event_filter: If provided filters the events to those that match the filter.
         Returns:
             The results as a list of events and a token that points to the end
             of the result set. If no events are returned then the end of the
             stream has been reached (i.e. there are no events between `from_key`
             and `to_key`).
         """
+        from_key = RoomStreamToken.parse(from_key)
+        if to_key:
+            to_key = RoomStreamToken.parse(to_key)
         rows, token = await self.db_pool.runInteraction(
             "paginate_room_events",
             self._paginate_room_events_txn,
             room_id,
             from_key,
             to_key,
             direction,
             limit,
             event_filter,
         )

--- a/synapse/storage/databases/main/tags.py
+++ b/synapse/storage/databases/main/tags.py
@@ -149,37 +149,37 @@
         """
         content_json = json_encoder.encode(content)
         def add_tag_txn(txn, next_id):
             self.db_pool.simple_upsert_txn(
                 txn,
                 table="room_tags",
                 keyvalues={"user_id": user_id, "room_id": room_id, "tag": tag},
                 values={"content": content_json},
             )
             self._update_revision_txn(txn, user_id, room_id, next_id)
-        async with self._account_data_id_gen.get_next() as next_id:
+        with await self._account_data_id_gen.get_next() as next_id:
             await self.db_pool.runInteraction("add_tag", add_tag_txn, next_id)
         self.get_tags_for_user.invalidate((user_id,))
         return self._account_data_id_gen.get_current_token()
     async def remove_tag_from_room(self, user_id: str, room_id: str, tag: str) -> int:
         """Remove a tag from a room for a user.
         Returns:
             The next account data ID.
         """
         def remove_tag_txn(txn, next_id):
             sql = (
                 "DELETE FROM room_tags "
                 " WHERE user_id = ? AND room_id = ? AND tag = ?"
             )
             txn.execute(sql, (user_id, room_id, tag))
             self._update_revision_txn(txn, user_id, room_id, next_id)
-        async with self._account_data_id_gen.get_next() as next_id:
+        with await self._account_data_id_gen.get_next() as next_id:
             await self.db_pool.runInteraction("remove_tag", remove_tag_txn, next_id)
         self.get_tags_for_user.invalidate((user_id,))
         return self._account_data_id_gen.get_current_token()
     def _update_revision_txn(
         self, txn, user_id: str, room_id: str, next_id: int
     ) -> None:
         """Update the latest revision of the tags for the given user and room.
         Args:
             txn: The database cursor
             user_id: The ID of the user.

--- a/synapse/storage/databases/main/transactions.py
+++ b/synapse/storage/databases/main/transactions.py
@@ -1,35 +1,34 @@
 import logging
 from collections import namedtuple
-from typing import Iterable, List, Optional, Tuple
+from typing import Optional, Tuple
 from canonicaljson import encode_canonical_json
 from synapse.metrics.background_process_metrics import run_as_background_process
 from synapse.storage._base import SQLBaseStore, db_to_json
-from synapse.storage.database import DatabasePool, LoggingTransaction
-from synapse.storage.engines import PostgresEngine, Sqlite3Engine
+from synapse.storage.database import DatabasePool
 from synapse.types import JsonDict
 from synapse.util.caches.expiringcache import ExpiringCache
 db_binary_type = memoryview
 logger = logging.getLogger(__name__)
 _TransactionRow = namedtuple(
     "_TransactionRow",
     ("id", "transaction_id", "destination", "ts", "response_code", "response_json"),
 )
 _UpdateTransactionRow = namedtuple(
     "_TransactionRow", ("response_code", "response_json")
 )
 SENTINEL = object()
 class TransactionStore(SQLBaseStore):
     """A collection of queries for handling PDUs.
     """
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(TransactionStore, self).__init__(database, db_conn, hs)
         self._clock.looping_call(self._start_cleanup_transactions, 30 * 60 * 1000)
         self._destination_retry_cache = ExpiringCache(
             cache_name="get_destination_retry_timings",
             clock=self._clock,
             expiry_ms=5 * 60 * 1000,
         )
     async def get_received_txn_response(
         self, transaction_id: str, origin: str
     ) -> Optional[Tuple[int, JsonDict]]:
         """For an incoming transaction from a given origin, check if we have
@@ -110,21 +109,21 @@
         self._destination_retry_cache[destination] = result
         return result
     def _get_destination_retry_timings(self, txn, destination):
         result = self.db_pool.simple_select_one_txn(
             txn,
             table="destinations",
             keyvalues={"destination": destination},
             retcols=("destination", "failure_ts", "retry_last_ts", "retry_interval"),
             allow_none=True,
         )
-        if result and result["retry_last_ts"]:
+        if result and result["retry_last_ts"] > 0:
             return result
         else:
             return None
     async def set_destination_retry_timings(
         self,
         destination: str,
         failure_ts: Optional[int],
         retry_last_ts: int,
         retry_interval: int,
     ) -> None:
@@ -153,21 +152,20 @@
                 INSERT INTO destinations (
                     destination, failure_ts, retry_last_ts, retry_interval
                 )
                     VALUES (?, ?, ?, ?)
                 ON CONFLICT (destination) DO UPDATE SET
                         failure_ts = EXCLUDED.failure_ts,
                         retry_last_ts = EXCLUDED.retry_last_ts,
                         retry_interval = EXCLUDED.retry_interval
                     WHERE
                         EXCLUDED.retry_interval = 0
-                        OR destinations.retry_interval IS NULL
                         OR destinations.retry_interval < EXCLUDED.retry_interval
             """
             txn.execute(sql, (destination, failure_ts, retry_last_ts, retry_interval))
             return
         self.database_engine.lock_table(txn, "destinations")
         prev_row = self.db_pool.simple_select_one_txn(
             txn,
             table="destinations",
             keyvalues={"destination": destination},
             retcols=("failure_ts", "retry_last_ts", "retry_interval"),
@@ -177,25 +175,21 @@
             self.db_pool.simple_insert_txn(
                 txn,
                 table="destinations",
                 values={
                     "destination": destination,
                     "failure_ts": failure_ts,
                     "retry_last_ts": retry_last_ts,
                     "retry_interval": retry_interval,
                 },
             )
-        elif (
-            retry_interval == 0
-            or prev_row["retry_interval"] is None
-            or prev_row["retry_interval"] < retry_interval
-        ):
+        elif retry_interval == 0 or prev_row["retry_interval"] < retry_interval:
             self.db_pool.simple_update_one_txn(
                 txn,
                 "destinations",
                 keyvalues={"destination": destination},
                 updatevalues={
                     "failure_ts": failure_ts,
                     "retry_last_ts": retry_last_ts,
                     "retry_interval": retry_interval,
                 },
             )
@@ -204,178 +198,10 @@
             "cleanup_transactions", self._cleanup_transactions
         )
     async def _cleanup_transactions(self) -> None:
         now = self._clock.time_msec()
         month_ago = now - 30 * 24 * 60 * 60 * 1000
         def _cleanup_transactions_txn(txn):
             txn.execute("DELETE FROM received_transactions WHERE ts < ?", (month_ago,))
         await self.db_pool.runInteraction(
             "_cleanup_transactions", _cleanup_transactions_txn
         )
-    async def store_destination_rooms_entries(
-        self, destinations: Iterable[str], room_id: str, stream_ordering: int,
-    ) -> None:
-        """
-        Updates or creates `destination_rooms` entries in batch for a single event.
-        Args:
-            destinations: list of destinations
-            room_id: the room_id of the event
-            stream_ordering: the stream_ordering of the event
-        """
-        return await self.db_pool.runInteraction(
-            "store_destination_rooms_entries",
-            self._store_destination_rooms_entries_txn,
-            destinations,
-            room_id,
-            stream_ordering,
-        )
-    def _store_destination_rooms_entries_txn(
-        self,
-        txn: LoggingTransaction,
-        destinations: Iterable[str],
-        room_id: str,
-        stream_ordering: int,
-    ) -> None:
-        if isinstance(self.database_engine, PostgresEngine):
-            q = """
-                INSERT INTO destinations (destination)
-                    VALUES (?)
-                    ON CONFLICT DO NOTHING;
-            """
-        elif isinstance(self.database_engine, Sqlite3Engine):
-            q = """
-                INSERT OR IGNORE INTO destinations (destination)
-                    VALUES (?);
-            """
-        else:
-            raise RuntimeError("Unknown database engine")
-        txn.execute_batch(q, ((destination,) for destination in destinations))
-        rows = [(destination, room_id) for destination in destinations]
-        self.db_pool.simple_upsert_many_txn(
-            txn,
-            "destination_rooms",
-            ["destination", "room_id"],
-            rows,
-            ["stream_ordering"],
-            [(stream_ordering,)] * len(rows),
-        )
-    async def get_destination_last_successful_stream_ordering(
-        self, destination: str
-    ) -> Optional[int]:
-        """
-        Gets the stream ordering of the PDU most-recently successfully sent
-        to the specified destination, or None if this information has not been
-        tracked yet.
-        Args:
-            destination: the destination to query
-        """
-        return await self.db_pool.simple_select_one_onecol(
-            "destinations",
-            {"destination": destination},
-            "last_successful_stream_ordering",
-            allow_none=True,
-            desc="get_last_successful_stream_ordering",
-        )
-    async def set_destination_last_successful_stream_ordering(
-        self, destination: str, last_successful_stream_ordering: int
-    ) -> None:
-        """
-        Marks that we have successfully sent the PDUs up to and including the
-        one specified.
-        Args:
-            destination: the destination we have successfully sent to
-            last_successful_stream_ordering: the stream_ordering of the most
-                recent successfully-sent PDU
-        """
-        return await self.db_pool.simple_upsert(
-            "destinations",
-            keyvalues={"destination": destination},
-            values={"last_successful_stream_ordering": last_successful_stream_ordering},
-            desc="set_last_successful_stream_ordering",
-        )
-    async def get_catch_up_room_event_ids(
-        self, destination: str, last_successful_stream_ordering: int,
-    ) -> List[str]:
-        """
-        Returns at most 50 event IDs and their corresponding stream_orderings
-        that correspond to the oldest events that have not yet been sent to
-        the destination.
-        Args:
-            destination: the destination in question
-            last_successful_stream_ordering: the stream_ordering of the
-                most-recently successfully-transmitted event to the destination
-        Returns:
-            list of event_ids
-        """
-        return await self.db_pool.runInteraction(
-            "get_catch_up_room_event_ids",
-            self._get_catch_up_room_event_ids_txn,
-            destination,
-            last_successful_stream_ordering,
-        )
-    @staticmethod
-    def _get_catch_up_room_event_ids_txn(
-        txn: LoggingTransaction, destination: str, last_successful_stream_ordering: int,
-    ) -> List[str]:
-        q = """
-                SELECT event_id FROM destination_rooms
-                 JOIN events USING (stream_ordering)
-                WHERE destination = ?
-                  AND stream_ordering > ?
-                ORDER BY stream_ordering
-                LIMIT 50
-            """
-        txn.execute(
-            q, (destination, last_successful_stream_ordering),
-        )
-        event_ids = [row[0] for row in txn]
-        return event_ids
-    async def get_catch_up_outstanding_destinations(
-        self, after_destination: Optional[str]
-    ) -> List[str]:
-        """
-        Gets at most 25 destinations which have outstanding PDUs to be caught up,
-        and are not being backed off from
-        Args:
-            after_destination:
-                If provided, all destinations must be lexicographically greater
-                than this one.
-        Returns:
-            list of up to 25 destinations with outstanding catch-up.
-                These are the lexicographically first destinations which are
-                lexicographically greater than after_destination (if provided).
-        """
-        time = self.hs.get_clock().time_msec()
-        return await self.db_pool.runInteraction(
-            "get_catch_up_outstanding_destinations",
-            self._get_catch_up_outstanding_destinations_txn,
-            time,
-            after_destination,
-        )
-    @staticmethod
-    def _get_catch_up_outstanding_destinations_txn(
-        txn: LoggingTransaction, now_time_ms: int, after_destination: Optional[str]
-    ) -> List[str]:
-        q = """
-            SELECT destination FROM destinations
-                WHERE destination IN (
-                    SELECT destination FROM destination_rooms
-                        WHERE destination_rooms.stream_ordering >
-                            destinations.last_successful_stream_ordering
-                )
-                AND destination > ?
-                AND (
-                    retry_last_ts IS NULL OR
-                    retry_last_ts + retry_interval < ?
-                )
-                ORDER BY destination
-                LIMIT 25
-        """
-        txn.execute(
-            q,
-            (
-                after_destination or "",
-                now_time_ms,
-            ),
-        )
-        destinations = [row[0] for row in txn]
-        return destinations

--- a/synapse/storage/databases/main/ui_auth.py
+++ b/synapse/storage/databases/main/ui_auth.py
@@ -1,18 +1,18 @@
 from typing import Any, Dict, List, Optional, Tuple, Union
 import attr
 from synapse.api.errors import StoreError
 from synapse.storage._base import SQLBaseStore, db_to_json
 from synapse.storage.database import LoggingTransaction
 from synapse.types import JsonDict
 from synapse.util import json_encoder, stringutils
-@attr.s(slots=True)
+@attr.s
 class UIAuthSessionData:
     session_id = attr.ib(type=str)
     clientdict = attr.ib(type=JsonDict)
     uri = attr.ib(type=str)
     method = attr.ib(type=str)
     description = attr.ib(type=str)
 class UIAuthWorkerStore(SQLBaseStore):
     """
     Manage user interactive authentication sessions.
     """

--- a/synapse/storage/databases/main/user_directory.py
+++ b/synapse/storage/databases/main/user_directory.py
@@ -6,21 +6,21 @@
 from synapse.storage.databases.main.state import StateFilter
 from synapse.storage.databases.main.state_deltas import StateDeltasStore
 from synapse.storage.engines import PostgresEngine, Sqlite3Engine
 from synapse.types import get_domain_from_id, get_localpart_from_id
 from synapse.util.caches.descriptors import cached
 logger = logging.getLogger(__name__)
 TEMP_TABLE = "_temp_populate_user_directory"
 class UserDirectoryBackgroundUpdateStore(StateDeltasStore):
     SHARE_PRIVATE_WORKING_SET = 500
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(UserDirectoryBackgroundUpdateStore, self).__init__(database, db_conn, hs)
         self.server_name = hs.hostname
         self.db_pool.updates.register_background_update_handler(
             "populate_user_directory_createtables",
             self._populate_user_directory_createtables,
         )
         self.db_pool.updates.register_background_update_handler(
             "populate_user_directory_process_rooms",
             self._populate_user_directory_process_rooms,
         )
         self.db_pool.updates.register_background_update_handler(
@@ -419,21 +419,21 @@
     async def update_user_directory_stream_pos(self, stream_id: str) -> None:
         await self.db_pool.simple_update_one(
             table="user_directory_stream_pos",
             keyvalues={},
             updatevalues={"stream_id": stream_id},
             desc="update_user_directory_stream_pos",
         )
 class UserDirectoryStore(UserDirectoryBackgroundUpdateStore):
     SHARE_PRIVATE_WORKING_SET = 500
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(UserDirectoryStore, self).__init__(database, db_conn, hs)
     async def remove_from_user_dir(self, user_id: str) -> None:
         def _remove_from_user_dir_txn(txn):
             self.db_pool.simple_delete_txn(
                 txn, table="user_directory", keyvalues={"user_id": user_id}
             )
             self.db_pool.simple_delete_txn(
                 txn, table="user_directory_search", keyvalues={"user_id": user_id}
             )
             self.db_pool.simple_delete_txn(
                 txn, table="users_in_public_rooms", keyvalues={"user_id": user_id}

--- a/synapse/storage/databases/main/user_erasure_store.py
+++ b/synapse/storage/databases/main/user_erasure_store.py
@@ -52,15 +52,15 @@
         await self.db_pool.runInteraction("mark_user_erased", f)
     async def mark_user_not_erased(self, user_id: str) -> None:
         """Indicate that user_id is no longer erased.
         Args:
             user_id: full user_id to be un-erased
         """
         def f(txn):
             txn.execute("SELECT 1 FROM erased_users WHERE user_id = ?", (user_id,))
             if not txn.fetchone():
                 return
-            self.db_pool.simple_delete_one_txn(
+            self.simple_delete_one_txn(
                 txn, "erased_users", keyvalues={"user_id": user_id}
             )
             self._invalidate_cache_and_stream(txn, self.is_user_erased, (user_id,))
         await self.db_pool.runInteraction("mark_user_not_erased", f)

--- a/synapse/storage/databases/state/bg_updates.py
+++ b/synapse/storage/databases/state/bg_updates.py
@@ -103,21 +103,21 @@
                         keyvalues={"state_group": next_group},
                         retcol="prev_state_group",
                         allow_none=True,
                     )
         return results
 class StateBackgroundUpdateStore(StateGroupBackgroundUpdateStore):
     STATE_GROUP_DEDUPLICATION_UPDATE_NAME = "state_group_state_deduplication"
     STATE_GROUP_INDEX_UPDATE_NAME = "state_group_state_type_index"
     STATE_GROUPS_ROOM_INDEX_UPDATE_NAME = "state_groups_room_id_idx"
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(StateBackgroundUpdateStore, self).__init__(database, db_conn, hs)
         self.db_pool.updates.register_background_update_handler(
             self.STATE_GROUP_DEDUPLICATION_UPDATE_NAME,
             self._background_deduplicate_state,
         )
         self.db_pool.updates.register_background_update_handler(
             self.STATE_GROUP_INDEX_UPDATE_NAME, self._background_index_state
         )
         self.db_pool.updates.register_background_index_update(
             self.STATE_GROUPS_ROOM_INDEX_UPDATE_NAME,
             index_name="state_groups_room_id_idx",

--- a/synapse/storage/databases/state/store.py
+++ b/synapse/storage/databases/state/store.py
@@ -1,54 +1,51 @@
 import logging
 from collections import namedtuple
 from typing import Dict, Iterable, List, Set, Tuple
 from synapse.api.constants import EventTypes
 from synapse.storage._base import SQLBaseStore
 from synapse.storage.database import DatabasePool
 from synapse.storage.databases.state.bg_updates import StateBackgroundUpdateStore
 from synapse.storage.state import StateFilter
 from synapse.storage.types import Cursor
 from synapse.storage.util.sequence import build_sequence_generator
-from synapse.types import MutableStateMap, StateMap
+from synapse.types import StateMap
 from synapse.util.caches.descriptors import cached
 from synapse.util.caches.dictionary_cache import DictionaryCache
 logger = logging.getLogger(__name__)
 MAX_STATE_DELTA_HOPS = 100
 class _GetStateGroupDelta(
     namedtuple("_GetStateGroupDelta", ("prev_group", "delta_ids"))
 ):
     """Return type of get_state_group_delta that implements __len__, which lets
     us use the itrable flag when caching
     """
     __slots__ = []
     def __len__(self):
         return len(self.delta_ids) if self.delta_ids else 0
 class StateGroupDataStore(StateBackgroundUpdateStore, SQLBaseStore):
     """A data store for fetching/storing state groups.
     """
     def __init__(self, database: DatabasePool, db_conn, hs):
-        super().__init__(database, db_conn, hs)
+        super(StateGroupDataStore, self).__init__(database, db_conn, hs)
         self._state_group_cache = DictionaryCache(
             "*stateGroupCache*",
             50000,
         )
         self._state_group_members_cache = DictionaryCache(
             "*stateGroupMembersCache*", 500000,
         )
         def get_max_state_group_txn(txn: Cursor):
             txn.execute("SELECT COALESCE(max(id), 0) FROM state_groups")
             return txn.fetchone()[0]
         self._state_group_seq_gen = build_sequence_generator(
             self.database_engine, get_max_state_group_txn, "state_group_id_seq"
-        )
-        self._state_group_seq_gen.check_consistency(
-            db_conn, table="state_groups", id_column="id"
         )
     @cached(max_entries=10000, iterable=True)
     async def get_state_group_delta(self, state_group):
         """Given a state group try to return a previous group and a delta between
         the old and the new.
         Returns:
             (prev_group, delta_ids), where both may be None.
         """
         def _get_state_group_delta_txn(txn):
             prev_group = self.db_pool.simple_select_one_onecol_txn(
@@ -115,21 +112,21 @@
         if state_filter.has_wildcards():
             missing_types = True
         else:
             for key in state_filter.concrete_types():
                 if key not in state_dict_ids and key not in known_absent:
                     missing_types = True
                     break
         return state_filter.filter_state(state_dict_ids), not missing_types
     async def _get_state_for_groups(
         self, groups: Iterable[int], state_filter: StateFilter = StateFilter.all()
-    ) -> Dict[int, MutableStateMap[str]]:
+    ) -> Dict[int, StateMap[str]]:
         """Gets the state at each of a list of state groups, optionally
         filtering by type/state_key
         Args:
             groups: list of state groups for which we want
                 to get the state.
             state_filter: The state filter used to fetch state
                 from the database.
         Returns:
             Dict of state group to state map.
         """

--- a/synapse/storage/engines/_base.py
+++ b/synapse/storage/engines/_base.py
@@ -57,23 +57,10 @@
         ...
     @abc.abstractmethod
     def lock_table(self, txn, table: str) -> None:
         ...
     @property
     @abc.abstractmethod
     def server_version(self) -> str:
         """Gets a string giving the server version. For example: '3.22.0'
         """
         ...
-    @abc.abstractmethod
-    def in_transaction(self, conn: Connection) -> bool:
-        """Whether the connection is currently in a transaction.
-        """
-        ...
-    @abc.abstractmethod
-    def attempt_to_set_autocommit(self, conn: Connection, autocommit: bool):
-        """Attempt to set the connections autocommit mode.
-        When True queries are run outside of transactions.
-        Note: This has no effect on SQLite3, so callers still need to
-        commit/rollback the connections.
-        """
-        ...

--- a/synapse/storage/engines/postgres.py
+++ b/synapse/storage/engines/postgres.py
@@ -1,13 +1,12 @@
 import logging
-from synapse.storage.engines._base import BaseDatabaseEngine, IncorrectDatabaseSetup
-from synapse.storage.types import Connection
+from ._base import BaseDatabaseEngine, IncorrectDatabaseSetup
 logger = logging.getLogger(__name__)
 class PostgresEngine(BaseDatabaseEngine):
     def __init__(self, database_module, database_config):
         super().__init__(database_module, database_config)
         self.module.extensions.register_type(self.module.extensions.UNICODE)
         def _disable_bytes_adapter(_):
             raise Exception("Passing bytes to DB is disabled.")
         self.module.extensions.register_adapter(bytes, _disable_bytes_adapter)
         self.synchronous_commit = database_config.get("synchronous_commit", True)
         self._version = None  # unknown as yet
@@ -64,21 +63,20 @@
         return sql.replace("?", "%s")
     def on_new_connection(self, db_conn):
         db_conn.set_isolation_level(
             self.module.extensions.ISOLATION_LEVEL_REPEATABLE_READ
         )
         cursor = db_conn.cursor()
         cursor.execute("SET bytea_output TO escape")
         if not self.synchronous_commit:
             cursor.execute("SET synchronous_commit TO OFF")
         cursor.close()
-        db_conn.commit()
     @property
     def can_native_upsert(self):
         """
         Can we use native UPSERTs?
         """
         return True
     @property
     def supports_tuple_comparison(self):
         """
         Do we support comparing tuples, i.e. `(a, b) > (c, d)`?
@@ -102,14 +100,10 @@
         """Returns a string giving the server version. For example: '8.1.5'
         Returns:
             string
         """
         numver = self._version
         assert numver is not None
         if numver >= 100000:
             return "%i.%i" % (numver / 10000, numver % 10000)
         else:
             return "%i.%i.%i" % (numver / 10000, (numver % 10000) / 100, numver % 100)
-    def in_transaction(self, conn: Connection) -> bool:
-        return conn.status != self.module.extensions.STATUS_READY  # type: ignore
-    def attempt_to_set_autocommit(self, conn: Connection, autocommit: bool):
-        return conn.set_session(autocommit=autocommit)  # type: ignore

--- a/synapse/storage/engines/sqlite.py
+++ b/synapse/storage/engines/sqlite.py
@@ -1,15 +1,14 @@
 import struct
 import threading
 import typing
 from synapse.storage.engines import BaseDatabaseEngine
-from synapse.storage.types import Connection
 if typing.TYPE_CHECKING:
     import sqlite3  # noqa: F401
 class Sqlite3Engine(BaseDatabaseEngine["sqlite3.Connection"]):
     def __init__(self, database_module, database_config):
         super().__init__(database_module, database_config)
         database = database_config.get("args", {}).get("database")
         self._is_in_memory = database in (None, ":memory:",)
         self._current_state_group_id = None
         self._current_state_group_id_lock = threading.Lock()
     @property
@@ -44,38 +43,33 @@
         apply stricter checks on new databases versus existing database.
         """
     def convert_param_style(self, sql):
         return sql
     def on_new_connection(self, db_conn):
         from synapse.storage.prepare_database import prepare_database
         if self._is_in_memory:
             prepare_database(db_conn, self, config=None)
         db_conn.create_function("rank", 1, _rank)
         db_conn.execute("PRAGMA foreign_keys = ON;")
-        db_conn.commit()
     def is_deadlock(self, error):
         return False
     def is_connection_closed(self, conn):
         return False
     def lock_table(self, txn, table):
         return
     @property
     def server_version(self):
         """Gets a string giving the server version. For example: '3.22.0'
         Returns:
             string
         """
         return "%i.%i.%i" % self.module.sqlite_version_info
-    def in_transaction(self, conn: Connection) -> bool:
-        return conn.in_transaction  # type: ignore
-    def attempt_to_set_autocommit(self, conn: Connection, autocommit: bool):
-        pass
 def _parse_match_info(buf):
     bufsize = len(buf)
     return [struct.unpack("@I", buf[i : i + 4])[0] for i in range(0, bufsize, 4)]
 def _rank(raw_match_info):
     """Handle match_info called w/default args 'pcx' - based on the example rank
     function http://sqlite.org/fts3.html#appendix_a
     """
     match_info = _parse_match_info(raw_match_info)
     score = 0.0
     p, c = match_info[:2]

--- a/synapse/storage/persist_events.py
+++ b/synapse/storage/persist_events.py
@@ -1,24 +1,24 @@
 import itertools
 import logging
 from collections import deque, namedtuple
-from typing import Dict, Iterable, List, Optional, Set, Tuple
+from typing import Iterable, List, Optional, Set, Tuple
 from prometheus_client import Counter, Histogram
 from twisted.internet import defer
 from synapse.api.constants import EventTypes, Membership
 from synapse.events import EventBase
 from synapse.events.snapshot import EventContext
 from synapse.logging.context import PreserveLoggingContext, make_deferred_yieldable
 from synapse.metrics.background_process_metrics import run_as_background_process
 from synapse.storage.databases import Databases
 from synapse.storage.databases.main.events import DeltaState
-from synapse.types import Collection, PersistedEventPosition, RoomStreamToken, StateMap
+from synapse.types import StateMap
 from synapse.util.async_helpers import ObservableDeferred
 from synapse.util.metrics import Measure
 logger = logging.getLogger(__name__)
 state_delta_counter = Counter("synapse_storage_events_state_delta", "")
 state_delta_single_event_counter = Counter(
     "synapse_storage_events_state_delta_single_event", ""
 )
 state_delta_reuse_delta_counter = Counter(
     "synapse_storage_events_state_delta_reuse_delta", ""
 )
@@ -111,73 +111,70 @@
         except IndexError:
             pass
 class EventsPersistenceStorage:
     """High level interface for handling persisting newly received events.
     Takes care of batching up events by room, and calculating the necessary
     current state and forward extremity changes.
     """
     def __init__(self, hs, stores: Databases):
         self.main_store = stores.main
         self.state_store = stores.state
-        assert stores.persist_events
         self.persist_events_store = stores.persist_events
         self._clock = hs.get_clock()
-        self._instance_name = hs.get_instance_name()
         self.is_mine_id = hs.is_mine_id
         self._event_persist_queue = _EventPeristenceQueue()
         self._state_resolution_handler = hs.get_state_resolution_handler()
     async def persist_events(
         self,
-        events_and_contexts: Iterable[Tuple[EventBase, EventContext]],
+        events_and_contexts: List[Tuple[EventBase, EventContext]],
         backfilled: bool = False,
-    ) -> RoomStreamToken:
+    ) -> int:
         """
         Write events to the database
         Args:
             events_and_contexts: list of tuples of (event, context)
             backfilled: Whether the results are retrieved from federation
                 via backfill or not. Used to determine if they're "new" events
                 which might update the current state etc.
         Returns:
             the stream ordering of the latest persisted event
         """
-        partitioned = {}  # type: Dict[str, List[Tuple[EventBase, EventContext]]]
+        partitioned = {}
         for event, ctx in events_and_contexts:
             partitioned.setdefault(event.room_id, []).append((event, ctx))
         deferreds = []
         for room_id, evs_ctxs in partitioned.items():
             d = self._event_persist_queue.add_to_queue(
                 room_id, evs_ctxs, backfilled=backfilled
             )
             deferreds.append(d)
         for room_id in partitioned:
             self._maybe_start_persisting(room_id)
         await make_deferred_yieldable(
             defer.gatherResults(deferreds, consumeErrors=True)
         )
-        return self.main_store.get_room_max_token()
+        return self.main_store.get_current_events_token()
     async def persist_event(
         self, event: EventBase, context: EventContext, backfilled: bool = False
-    ) -> Tuple[PersistedEventPosition, RoomStreamToken]:
+    ) -> Tuple[int, int]:
         """
         Returns:
             The stream ordering of `event`, and the stream ordering of the
             latest persisted event
         """
         deferred = self._event_persist_queue.add_to_queue(
             event.room_id, [(event, context)], backfilled=backfilled
         )
         self._maybe_start_persisting(event.room_id)
         await make_deferred_yieldable(deferred)
-        event_stream_id = event.internal_metadata.stream_ordering
-        pos = PersistedEventPosition(self._instance_name, event_stream_id)
-        return pos, self.main_store.get_room_max_token()
+        max_persisted_id = self.main_store.get_current_events_token()
+        return (event.internal_metadata.stream_ordering, max_persisted_id)
     def _maybe_start_persisting(self, room_id: str):
         async def persisting_queue(item):
             with Measure(self._clock, "persist_events"):
                 await self._persist_events(
                     item.events_and_contexts, backfilled=item.backfilled
                 )
         self._event_persist_queue.handle_queue(room_id, persisting_queue)
     async def _persist_events(
         self,
         events_and_contexts: List[Tuple[EventBase, EventContext]],
@@ -192,23 +189,21 @@
             events_and_contexts[x : x + 100]
             for x in range(0, len(events_and_contexts), 100)
         ]
         for chunk in chunks:
             new_forward_extremeties = {}
             current_state_for_room = {}
             state_delta_for_room = {}
             potentially_left_users = set()  # type: Set[str]
             if not backfilled:
                 with Measure(self._clock, "_calculate_state_and_extrem"):
-                    events_by_room = (
-                        {}
-                    )  # type: Dict[str, List[Tuple[EventBase, EventContext]]]
+                    events_by_room = {}
                     for event, context in chunk:
                         events_by_room.setdefault(event.room_id, []).append(
                             (event, context)
                         )
                     for room_id, ev_ctx_rm in events_by_room.items():
                         latest_event_ids = await self.main_store.get_latest_event_ids_in_room(
                             room_id
                         )
                         new_latest_event_ids = await self._calculate_new_extremities(
                             room_id, ev_ctx_rm, latest_event_ids
@@ -280,42 +275,42 @@
                 current_state_for_room=current_state_for_room,
                 state_delta_for_room=state_delta_for_room,
                 new_forward_extremeties=new_forward_extremeties,
                 backfilled=backfilled,
             )
             await self._handle_potentially_left_users(potentially_left_users)
     async def _calculate_new_extremities(
         self,
         room_id: str,
         event_contexts: List[Tuple[EventBase, EventContext]],
-        latest_event_ids: Collection[str],
+        latest_event_ids: List[str],
     ):
         """Calculates the new forward extremities for a room given events to
         persist.
         Assumes that we are only persisting events for one room at a time.
         """
         new_events = [
             event
             for event, ctx in event_contexts
             if not event.internal_metadata.is_outlier()
             and not ctx.rejected
             and not event.internal_metadata.is_soft_failed()
         ]
         latest_event_ids = set(latest_event_ids)
         result = set(latest_event_ids)
         result.update(event.event_id for event in new_events)
         result.difference_update(
             e_id for event in new_events for e_id in event.prev_event_ids()
         )
         existing_prevs = await self.persist_events_store._get_events_which_are_prevs(
             result
-        )  # type: Collection[str]
+        )
         result.difference_update(existing_prevs)
         existing_prevs = await self.persist_events_store._get_prevs_before_rejected(
             e_id for event in new_events for e_id in event.prev_event_ids()
         )
         result.difference_update(existing_prevs)
         if result != latest_event_ids:
             forward_extremities_counter.observe(len(result))
             stale = latest_event_ids & result
             stale_forward_extremities_counter.observe(len(stale))
         return result

--- a/synapse/storage/prepare_database.py
+++ b/synapse/storage/prepare_database.py
@@ -1,63 +1,54 @@
 import imp
 import logging
 import os
 import re
 from collections import Counter
-from typing import Optional, TextIO
+from typing import TextIO
 import attr
-from synapse.config.homeserver import HomeServerConfig
-from synapse.storage.engines import BaseDatabaseEngine
 from synapse.storage.engines.postgres import PostgresEngine
-from synapse.storage.types import Connection, Cursor
-from synapse.types import Collection
+from synapse.storage.types import Cursor
 logger = logging.getLogger(__name__)
 SCHEMA_VERSION = 58
 dir_path = os.path.abspath(os.path.dirname(__file__))
 class PrepareDatabaseException(Exception):
     pass
 class UpgradeDatabaseException(PrepareDatabaseException):
     pass
 OUTDATED_SCHEMA_ON_WORKER_ERROR = (
     "Expected database schema version %i but got %i: run the main synapse process to "
     "upgrade the database schema before starting worker processes."
 )
 EMPTY_DATABASE_ON_WORKER_ERROR = (
     "Uninitialised database: run the main synapse process to prepare the database "
     "schema before starting worker processes."
 )
 UNAPPLIED_DELTA_ON_WORKER_ERROR = (
     "Database schema delta %s has not been applied: run the main synapse process to "
     "upgrade the database schema before starting worker processes."
 )
-def prepare_database(
-    db_conn: Connection,
-    database_engine: BaseDatabaseEngine,
-    config: Optional[HomeServerConfig],
-    databases: Collection[str] = ["main", "state"],
-):
+def prepare_database(db_conn, database_engine, config, databases=["main", "state"]):
     """Prepares a physical database for usage. Will either create all necessary tables
     or upgrade from an older schema version.
     If `config` is None then prepare_database will assert that no upgrade is
     necessary, *or* will create a fresh database if the database is empty.
     Args:
         db_conn:
         database_engine:
-        config :
+        config (synapse.config.homeserver.HomeServerConfig|None):
             application config, or None if we are connecting to an existing
             database which we expect to be configured already
-        databases: The name of the databases that will be used
+        databases (list[str]): The name of the databases that will be used
             with this physical database. Defaults to all databases.
     """
     try:
         cur = db_conn.cursor()
-        cur.execute("BEGIN TRANSACTION")
         logger.info("%r: Checking existing schema version", databases)
         version_info = _get_or_create_schema_state(cur, database_engine)
         if version_info:
             user_version, delta_files, upgraded = version_info
             logger.info(
                 "%r: Existing schema is %i (+%i deltas)",
                 databases,
                 user_version,
                 len(delta_files),
             )
@@ -442,19 +433,19 @@
     if current_version:
         txn.execute(
             database_engine.convert_param_style(
                 "SELECT file FROM applied_schema_deltas WHERE version >= ?"
             ),
             (current_version,),
         )
         applied_deltas = [d for d, in txn]
         return current_version, applied_deltas, upgraded
     return None
-@attr.s(slots=True)
+@attr.s()
 class _DirectoryListing:
     """Helper class to store schema file name and the
     absolute path to it.
     These entries get sorted, so for consistency we want to ensure that
     `file_name` attr is kept first.
     """
     file_name = attr.ib()
     absolute_path = attr.ib()

--- a/synapse/storage/relations.py
+++ b/synapse/storage/relations.py
@@ -1,15 +1,15 @@
 import logging
 import attr
 from synapse.api.errors import SynapseError
 logger = logging.getLogger(__name__)
-@attr.s(slots=True)
+@attr.s
 class PaginationChunk:
     """Returned by relation pagination APIs.
     Attributes:
         chunk (list): The rows returned by pagination
         next_batch (Any|None): Token to fetch next set of results with, if
             None then there are no more results.
         prev_batch (Any|None): Token to fetch previous set of results with, if
             None then there are no previous results.
     """
     chunk = attr.ib()

--- a/synapse/storage/roommember.py
+++ b/synapse/storage/roommember.py
@@ -1,11 +1,11 @@
 import logging
 from collections import namedtuple
 logger = logging.getLogger(__name__)
 RoomsForUser = namedtuple(
     "RoomsForUser", ("room_id", "sender", "membership", "event_id", "stream_ordering")
 )
 GetRoomsForUserWithStreamOrdering = namedtuple(
-    "_GetRoomsForUserWithStreamOrdering", ("room_id", "event_pos")
+    "_GetRoomsForUserWithStreamOrdering", ("room_id", "stream_ordering")
 )
 ProfileInfo = namedtuple("ProfileInfo", ("avatar_url", "display_name"))
 MemberSummary = namedtuple("MemberSummary", ("members", "count"))

--- a/synapse/storage/state.py
+++ b/synapse/storage/state.py
@@ -1,16 +1,16 @@
 import logging
 from typing import Awaitable, Dict, Iterable, List, Optional, Set, Tuple, TypeVar
 import attr
 from synapse.api.constants import EventTypes
 from synapse.events import EventBase
-from synapse.types import MutableStateMap, StateMap
+from synapse.types import StateMap
 logger = logging.getLogger(__name__)
 T = TypeVar("T")
 @attr.s(slots=True)
 class StateFilter:
     """A filter used when querying for state.
     Attributes:
         types: Map from type to set of state keys (or None). This specifies
             which state_keys for the given type to fetch from the DB. If None
             then all events with that type are fetched. If the set is empty
             then no events with that type are fetched.
@@ -235,21 +235,21 @@
         the old and the new.
         Args:
             state_group: The state group used to retrieve state deltas.
         Returns:
             Tuple[Optional[int], Optional[StateMap[str]]]:
                 (prev_group, delta_ids)
         """
         return await self.stores.state.get_state_group_delta(state_group)
     async def get_state_groups_ids(
         self, _room_id: str, event_ids: Iterable[str]
-    ) -> Dict[int, MutableStateMap[str]]:
+    ) -> Dict[int, StateMap[str]]:
         """Get the event IDs of all the state for the state groups for the given events
         Args:
             _room_id: id of the room for these events
             event_ids: ids of the events
         Returns:
             dict of state_group_id -> (dict of (type, state_key) -> event id)
         """
         if not event_ids:
             return {}
         event_to_groups = await self.stores.main._get_state_group_for_events(event_ids)
@@ -379,21 +379,21 @@
         Args:
             event_id: event whose state should be returned
             state_filter: The state filter used to fetch state from the database.
         Returns:
             A dict from (type, state_key) -> state_event
         """
         state_map = await self.get_state_ids_for_events([event_id], state_filter)
         return state_map[event_id]
     def _get_state_for_groups(
         self, groups: Iterable[int], state_filter: StateFilter = StateFilter.all()
-    ) -> Awaitable[Dict[int, MutableStateMap[str]]]:
+    ) -> Awaitable[Dict[int, StateMap[str]]]:
         """Gets the state at each of a list of state groups, optionally
         filtering by type/state_key
         Args:
             groups: list of state groups for which we want to get the state.
             state_filter: The state filter used to fetch state.
                 from the database.
         Returns:
             Dict of state group to state map.
         """
         return self.stores.state._get_state_for_groups(groups, state_filter)

--- a/synapse/storage/util/id_generators.py
+++ b/synapse/storage/util/id_generators.py
@@ -1,21 +1,18 @@
+import contextlib
 import heapq
 import logging
 import threading
 from collections import deque
-from contextlib import contextmanager
-from typing import Dict, List, Optional, Set, Union
-import attr
+from typing import Dict, List, Set
 from typing_extensions import Deque
-from synapse.metrics.background_process_metrics import run_as_background_process
 from synapse.storage.database import DatabasePool, LoggingTransaction
-from synapse.storage.types import Cursor
 from synapse.storage.util.sequence import PostgresSequenceGenerator
 logger = logging.getLogger(__name__)
 class IdGenerator:
     def __init__(self, db_conn, table, column):
         self._lock = threading.Lock()
         self._next_id = _load_current_id(db_conn, table, column)
     def get_next(self):
         with self._lock:
             self._next_id += 1
             return self._next_id
@@ -51,72 +48,72 @@
         table(str): A database table to read the initial value of the id
             generator from.
         column(str): The column of the database table to read the initial
             value from the id generator from.
         extra_tables(list): List of pairs of database tables and columns to
             use to source the initial value of the generator from. The value
             with the largest magnitude is used.
         step(int): which direction the stream ids grow in. +1 to grow
             upwards, -1 to grow downwards.
     Usage:
-        async with stream_id_gen.get_next() as stream_id:
+        with await stream_id_gen.get_next() as stream_id:
     """
     def __init__(self, db_conn, table, column, extra_tables=[], step=1):
         assert step != 0
         self._lock = threading.Lock()
         self._step = step
         self._current = _load_current_id(db_conn, table, column, step)
         for table, column in extra_tables:
             self._current = (max if step > 0 else min)(
                 self._current, _load_current_id(db_conn, table, column, step)
             )
         self._unfinished_ids = deque()  # type: Deque[int]
-    def get_next(self):
-        """
-        Usage:
-            async with stream_id_gen.get_next() as stream_id:
+    async def get_next(self):
+        """
+        Usage:
+            with await stream_id_gen.get_next() as stream_id:
         """
         with self._lock:
             self._current += self._step
             next_id = self._current
             self._unfinished_ids.append(next_id)
-        @contextmanager
+        @contextlib.contextmanager
         def manager():
             try:
                 yield next_id
             finally:
                 with self._lock:
                     self._unfinished_ids.remove(next_id)
-        return _AsyncCtxManagerWrapper(manager())
-    def get_next_mult(self, n):
-        """
-        Usage:
-            async with stream_id_gen.get_next(n) as stream_ids:
+        return manager()
+    async def get_next_mult(self, n):
+        """
+        Usage:
+            with await stream_id_gen.get_next(n) as stream_ids:
         """
         with self._lock:
             next_ids = range(
                 self._current + self._step,
                 self._current + self._step * (n + 1),
                 self._step,
             )
             self._current += n * self._step
             for next_id in next_ids:
                 self._unfinished_ids.append(next_id)
-        @contextmanager
+        @contextlib.contextmanager
         def manager():
             try:
                 yield next_ids
             finally:
                 with self._lock:
                     for next_id in next_ids:
                         self._unfinished_ids.remove(next_id)
-        return _AsyncCtxManagerWrapper(manager())
+        return manager()
     def get_current_token(self):
         """Returns the maximum stream id such that all stream ids less than or
         equal to it have been successfully persisted.
         Returns:
             int
         """
         with self._lock:
             if self._unfinished_ids:
                 return self._unfinished_ids[0] - self._step
             return self._current
@@ -127,198 +124,143 @@
         """
         return self.get_current_token()
 class MultiWriterIdGenerator:
     """An ID generator that tracks a stream that can have multiple writers.
     Uses a Postgres sequence to coordinate ID assignment, but positions of other
     writers will only get updated when `advance` is called (by replication).
     Note: Only works with Postgres.
     Args:
         db_conn
         db
-        stream_name: A name for the stream.
         instance_name: The name of this instance.
         table: Database table associated with stream.
         instance_column: Column that stores the row's writer's instance name
         id_column: Column that stores the stream ID.
         sequence_name: The name of the postgres sequence used to generate new
             IDs.
-        writers: A list of known writers to use to populate current positions
-            on startup. Can be empty if nothing uses `get_current_token` or
-            `get_positions` (e.g. caches stream).
         positive: Whether the IDs are positive (true) or negative (false).
             When using negative IDs we go backwards from -1 to -2, -3, etc.
     """
     def __init__(
         self,
         db_conn,
         db: DatabasePool,
-        stream_name: str,
         instance_name: str,
         table: str,
         instance_column: str,
         id_column: str,
         sequence_name: str,
-        writers: List[str],
         positive: bool = True,
     ):
         self._db = db
-        self._stream_name = stream_name
         self._instance_name = instance_name
         self._positive = positive
-        self._writers = writers
         self._return_factor = 1 if positive else -1
         self._lock = threading.Lock()
-        self._current_positions = {}  # type: Dict[str, int]
+        self._current_positions = self._load_current_ids(
+            db_conn, table, instance_column, id_column
+        )
         self._unfinished_ids = set()  # type: Set[int]
-        self._finished_ids = set()  # type: Set[int]
         self._persisted_upto_position = (
-            min(self._current_positions.values()) if self._current_positions else 1
+            min(self._current_positions.values()) if self._current_positions else 0
         )
         self._known_persisted_positions = []  # type: List[int]
         self._sequence_gen = PostgresSequenceGenerator(sequence_name)
-        self._sequence_gen.check_consistency(
-            db_conn, table=table, id_column=id_column, positive=positive
-        )
-        self._load_current_ids(db_conn, table, instance_column, id_column)
     def _load_current_ids(
         self, db_conn, table: str, instance_column: str, id_column: str
-    ):
+    ) -> Dict[str, int]:
+        sql = """
+            SELECT %(instance)s, %(agg)s(%(id)s) FROM %(table)s
+            GROUP BY %(instance)s
+        """ % {
+            "instance": instance_column,
+            "id": id_column,
+            "table": table,
+            "agg": "MAX" if self._positive else "-MIN",
+        }
         cur = db_conn.cursor()
-        if self._writers:
-            sql = """
-                DELETE FROM stream_positions
-                WHERE
-                    stream_name = ?
-                    AND instance_name != ALL(?)
-            """
-            sql = self._db.engine.convert_param_style(sql)
-            cur.execute(sql, (self._stream_name, self._writers))
-            sql = """
-                SELECT instance_name, stream_id FROM stream_positions
-                WHERE stream_name = ?
-            """
-            sql = self._db.engine.convert_param_style(sql)
-            cur.execute(sql, (self._stream_name,))
-            self._current_positions = {
-                instance: stream_id * self._return_factor
-                for instance, stream_id in cur
-                if instance in self._writers
-            }
-        min_stream_id = min(self._current_positions.values(), default=None)
-        if min_stream_id is None:
-            sql = """
-                SELECT GREATEST(COALESCE(%(agg)s(%(id)s), 1), 1)
-                FROM %(table)s
-            """ % {
-                "id": id_column,
-                "table": table,
-                "agg": "MAX" if self._positive else "-MIN",
-            }
-            cur.execute(sql)
-            (stream_id,) = cur.fetchone()
-            self._persisted_upto_position = stream_id
-        else:
-            sql = """
-                SELECT %(instance)s, %(id)s FROM %(table)s
-                WHERE ? %(cmp)s %(id)s
-            """ % {
-                "id": id_column,
-                "table": table,
-                "instance": instance_column,
-                "cmp": "<=" if self._positive else ">=",
-            }
-            sql = self._db.engine.convert_param_style(sql)
-            cur.execute(sql, (min_stream_id * self._return_factor,))
-            self._persisted_upto_position = min_stream_id
-            with self._lock:
-                for (instance, stream_id,) in cur:
-                    stream_id = self._return_factor * stream_id
-                    self._add_persisted_position(stream_id)
-                    if instance == self._instance_name:
-                        self._current_positions[instance] = stream_id
+        cur.execute(sql)
+        current_positions = dict(cur)
         cur.close()
+        return current_positions
     def _load_next_id_txn(self, txn) -> int:
         return self._sequence_gen.get_next_id_txn(txn)
     def _load_next_mult_id_txn(self, txn, n: int) -> List[int]:
         return self._sequence_gen.get_next_mult_txn(txn, n)
-    def get_next(self):
-        """
-        Usage:
-            async with stream_id_gen.get_next() as stream_id:
-        """
-        return _MultiWriterCtxManager(self)
-    def get_next_mult(self, n: int):
-        """
-        Usage:
-            async with stream_id_gen.get_next_mult(5) as stream_ids:
-        """
-        return _MultiWriterCtxManager(self, n)
+    async def get_next(self):
+        """
+        Usage:
+            with await stream_id_gen.get_next() as stream_id:
+        """
+        next_id = await self._db.runInteraction("_load_next_id", self._load_next_id_txn)
+        with self._lock:
+            assert self._current_positions.get(self._instance_name, 0) < next_id
+            self._unfinished_ids.add(next_id)
+        @contextlib.contextmanager
+        def manager():
+            try:
+                yield self._return_factor * next_id
+            finally:
+                self._mark_id_as_finished(next_id)
+        return manager()
+    async def get_next_mult(self, n: int):
+        """
+        Usage:
+            with await stream_id_gen.get_next_mult(5) as stream_ids:
+        """
+        next_ids = await self._db.runInteraction(
+            "_load_next_mult_id", self._load_next_mult_id_txn, n
+        )
+        with self._lock:
+            assert max(self._current_positions.values(), default=0) < min(next_ids)
+            self._unfinished_ids.update(next_ids)
+        @contextlib.contextmanager
+        def manager():
+            try:
+                yield [self._return_factor * i for i in next_ids]
+            finally:
+                for i in next_ids:
+                    self._mark_id_as_finished(i)
+        return manager()
     def get_next_txn(self, txn: LoggingTransaction):
         """
         Usage:
             stream_id = stream_id_gen.get_next(txn)
         """
         next_id = self._load_next_id_txn(txn)
         with self._lock:
             self._unfinished_ids.add(next_id)
         txn.call_after(self._mark_id_as_finished, next_id)
         txn.call_on_exception(self._mark_id_as_finished, next_id)
-        if self._writers:
-            txn.call_after(
-                run_as_background_process,
-                "MultiWriterIdGenerator._update_table",
-                self._db.runInteraction,
-                "MultiWriterIdGenerator._update_table",
-                self._update_stream_positions_table_txn,
-            )
         return self._return_factor * next_id
     def _mark_id_as_finished(self, next_id: int):
         """The ID has finished being processed so we should advance the
-        current position if possible.
+        current poistion if possible.
         """
         with self._lock:
             self._unfinished_ids.discard(next_id)
-            self._finished_ids.add(next_id)
-            new_cur = None
-            if self._unfinished_ids:
-                finished = set()
-                min_unfinshed = min(self._unfinished_ids)
-                for s in self._finished_ids:
-                    if s < min_unfinshed:
-                        if new_cur is None or new_cur < s:
-                            new_cur = s
-                    else:
-                        finished.add(s)
-                self._finished_ids = finished
-            else:
-                new_cur = max(self._finished_ids)
-                self._finished_ids.clear()
-            if new_cur:
+            if all(c > next_id for c in self._unfinished_ids):
                 curr = self._current_positions.get(self._instance_name, 0)
-                self._current_positions[self._instance_name] = max(curr, new_cur)
+                self._current_positions[self._instance_name] = max(curr, next_id)
             self._add_persisted_position(next_id)
     def get_current_token(self) -> int:
         """Returns the maximum stream id such that all stream ids less than or
         equal to it have been successfully persisted.
         """
-        return self.get_persisted_upto_position()
+        raise NotImplementedError()
     def get_current_token_for_writer(self, instance_name: str) -> int:
         """Returns the position of the given writer.
         """
         with self._lock:
-            return self._return_factor * self._current_positions.get(
-                instance_name, self._persisted_upto_position
-            )
+            return self._return_factor * self._current_positions.get(instance_name, 0)
     def get_positions(self) -> Dict[str, int]:
         """Get a copy of the current positon map.
-        Note that this won't necessarily include all configured writers if some
-        writers haven't written anything yet.
         """
         with self._lock:
             return {
                 name: self._return_factor * i
                 for name, i in self._current_positions.items()
             }
     def advance(self, instance_name: str, new_id: int):
         """Advance the postion of the named writer to the given ID, if greater
         than existing entry.
         """
@@ -336,84 +278,22 @@
         lag if one writer doesn't write very often.
         """
         with self._lock:
             return self._return_factor * self._persisted_upto_position
     def _add_persisted_position(self, new_id: int):
         """Record that we have persisted a position.
         This is used to keep the `_current_positions` up to date.
         """
         assert self._lock.locked()
         heapq.heappush(self._known_persisted_positions, new_id)
-        min_curr = min(self._current_positions.values(), default=0)
+        min_curr = min(self._current_positions.values())
         self._persisted_upto_position = max(min_curr, self._persisted_upto_position)
         while self._known_persisted_positions:
             if self._known_persisted_positions[0] <= self._persisted_upto_position:
                 heapq.heappop(self._known_persisted_positions)
             elif (
                 self._known_persisted_positions[0] == self._persisted_upto_position + 1
             ):
                 heapq.heappop(self._known_persisted_positions)
                 self._persisted_upto_position += 1
             else:
                 break
-    def _update_stream_positions_table_txn(self, txn: Cursor):
-        """Update the `stream_positions` table with newly persisted position.
-        """
-        if not self._writers:
-            return
-        sql = """
-            INSERT INTO stream_positions (stream_name, instance_name, stream_id)
-            VALUES (?, ?, ?)
-            ON CONFLICT (stream_name, instance_name)
-            DO UPDATE SET
-                stream_id = %(agg)s(stream_positions.stream_id, EXCLUDED.stream_id)
-        """ % {
-            "agg": "GREATEST" if self._positive else "LEAST",
-        }
-        pos = (self.get_current_token_for_writer(self._instance_name),)
-        txn.execute(sql, (self._stream_name, self._instance_name, pos))
-@attr.s(slots=True)
-class _AsyncCtxManagerWrapper:
-    """Helper class to convert a plain context manager to an async one.
-    This is mainly useful if you have a plain context manager but the interface
-    requires an async one.
-    """
-    inner = attr.ib()
-    async def __aenter__(self):
-        return self.inner.__enter__()
-    async def __aexit__(self, exc_type, exc, tb):
-        return self.inner.__exit__(exc_type, exc, tb)
-@attr.s(slots=True)
-class _MultiWriterCtxManager:
-    """Async context manager returned by MultiWriterIdGenerator
-    """
-    id_gen = attr.ib(type=MultiWriterIdGenerator)
-    multiple_ids = attr.ib(type=Optional[int], default=None)
-    stream_ids = attr.ib(type=List[int], factory=list)
-    async def __aenter__(self) -> Union[int, List[int]]:
-        self.stream_ids = await self.id_gen._db.runInteraction(
-            "_load_next_mult_id",
-            self.id_gen._load_next_mult_id_txn,
-            self.multiple_ids or 1,
-            db_autocommit=True,
-        )
-        with self.id_gen._lock:
-            assert max(self.id_gen._current_positions.values(), default=0) < min(
-                self.stream_ids
-            )
-            self.id_gen._unfinished_ids.update(self.stream_ids)
-        if self.multiple_ids is None:
-            return self.stream_ids[0] * self.id_gen._return_factor
-        else:
-            return [i * self.id_gen._return_factor for i in self.stream_ids]
-    async def __aexit__(self, exc_type, exc, tb):
-        for i in self.stream_ids:
-            self.id_gen._mark_id_as_finished(i)
-        if exc_type is not None:
-            return False
-        if self.id_gen._writers:
-            await self.id_gen._db.runInteraction(
-                "MultiWriterIdGenerator._update_table",
-                self.id_gen._update_stream_positions_table_txn,
-                db_autocommit=True,
-            )
-        return False

--- a/synapse/storage/util/sequence.py
+++ b/synapse/storage/util/sequence.py
@@ -1,93 +1,33 @@
 import abc
-import logging
 import threading
 from typing import Callable, List, Optional
-from synapse.storage.engines import (
-    BaseDatabaseEngine,
-    IncorrectDatabaseSetup,
-    PostgresEngine,
-)
-from synapse.storage.types import Connection, Cursor
-logger = logging.getLogger(__name__)
-_INCONSISTENT_SEQUENCE_ERROR = """
-Postgres sequence '%(seq)s' is inconsistent with associated
-table '%(table)s'. This can happen if Synapse has been downgraded and
-then upgraded again, or due to a bad migration.
-To fix this error, shut down Synapse (including any and all workers)
-and run the following SQL:
-    SELECT setval('%(seq)s', (
-        %(max_id_sql)s
-    ));
-See docs/postgres.md for more information.
-"""
+from synapse.storage.engines import BaseDatabaseEngine, PostgresEngine
+from synapse.storage.types import Cursor
 class SequenceGenerator(metaclass=abc.ABCMeta):
     """A class which generates a unique sequence of integers"""
     @abc.abstractmethod
     def get_next_id_txn(self, txn: Cursor) -> int:
         """Gets the next ID in the sequence"""
-        ...
-    @abc.abstractmethod
-    def check_consistency(
-        self, db_conn: Connection, table: str, id_column: str, positive: bool = True
-    ):
-        """Should be called during start up to test that the current value of
-        the sequence is greater than or equal to the maximum ID in the table.
-        This is to handle various cases where the sequence value can get out
-        of sync with the table, e.g. if Synapse gets rolled back to a previous
-        version and the rolled forwards again.
-        """
         ...
 class PostgresSequenceGenerator(SequenceGenerator):
     """An implementation of SequenceGenerator which uses a postgres sequence"""
     def __init__(self, sequence_name: str):
         self._sequence_name = sequence_name
     def get_next_id_txn(self, txn: Cursor) -> int:
         txn.execute("SELECT nextval(?)", (self._sequence_name,))
         return txn.fetchone()[0]
     def get_next_mult_txn(self, txn: Cursor, n: int) -> List[int]:
         txn.execute(
             "SELECT nextval(?) FROM generate_series(1, ?)", (self._sequence_name, n)
         )
         return [i for (i,) in txn]
-    def check_consistency(
-        self, db_conn: Connection, table: str, id_column: str, positive: bool = True
-    ):
-        txn = db_conn.cursor()
-        table_sql = "SELECT GREATEST(%(agg)s(%(id)s), 0) FROM %(table)s" % {
-            "id": id_column,
-            "table": table,
-            "agg": "MAX" if positive else "-MIN",
-        }
-        txn.execute(table_sql)
-        row = txn.fetchone()
-        if not row:
-            txn.close()
-            return
-        max_stream_id = row[0]
-        txn.execute(
-            "SELECT last_value, is_called FROM %(seq)s" % {"seq": self._sequence_name}
-        )
-        last_value, is_called = txn.fetchone()
-        txn.close()
-        if not is_called:
-            last_value -= 1
-        if max_stream_id > last_value:
-            logger.warning(
-                "Postgres sequence %s is behind table %s: %d < %d",
-                last_value,
-                max_stream_id,
-            )
-            raise IncorrectDatabaseSetup(
-                _INCONSISTENT_SEQUENCE_ERROR
-                % {"seq": self._sequence_name, "table": table, "max_id_sql": table_sql}
-            )
 GetFirstCallbackType = Callable[[Cursor], int]
 class LocalSequenceGenerator(SequenceGenerator):
     """An implementation of SequenceGenerator which uses local locking
     This only works reliably if there are no other worker processes generating IDs at
     the same time.
     """
     def __init__(self, get_first_callback: GetFirstCallbackType):
         """
         Args:
             get_first_callback: a callback which is called on the first call to
@@ -97,24 +37,20 @@
         self._current_max_id = None  # type: Optional[int]
         self._lock = threading.Lock()
     def get_next_id_txn(self, txn: Cursor) -> int:
         with self._lock:
             if self._current_max_id is None:
                 assert self._callback is not None
                 self._current_max_id = self._callback(txn)
                 self._callback = None
             self._current_max_id += 1
             return self._current_max_id
-    def check_consistency(
-        self, db_conn: Connection, table: str, id_column: str, positive: bool = True
-    ):
-        pass
 def build_sequence_generator(
     database_engine: BaseDatabaseEngine,
     get_first_callback: GetFirstCallbackType,
     sequence_name: str,
 ) -> SequenceGenerator:
     """Get the best impl of SequenceGenerator available
     This uses PostgresSequenceGenerator on postgres, and a locally-locked impl on
     sqlite.
     Args:
         database_engine: the database engine we are connected to

--- a/synapse/streams/config.py
+++ b/synapse/streams/config.py
@@ -1,57 +1,68 @@
 import logging
-from typing import Optional
-import attr
 from synapse.api.errors import SynapseError
 from synapse.http.servlet import parse_integer, parse_string
-from synapse.http.site import SynapseRequest
-from synapse.storage.databases.main import DataStore
 from synapse.types import StreamToken
 logger = logging.getLogger(__name__)
 MAX_LIMIT = 1000
-@attr.s(slots=True)
+class SourcePaginationConfig:
+    """A configuration object which stores pagination parameters for a
+    specific event source."""
+    def __init__(self, from_key=None, to_key=None, direction="f", limit=None):
+        self.from_key = from_key
+        self.to_key = to_key
+        self.direction = "f" if direction == "f" else "b"
+        self.limit = min(int(limit), MAX_LIMIT) if limit is not None else None
+    def __repr__(self):
+        return "StreamConfig(from_key=%r, to_key=%r, direction=%r, limit=%r)" % (
+            self.from_key,
+            self.to_key,
+            self.direction,
+            self.limit,
+        )
 class PaginationConfig:
     """A configuration object which stores pagination parameters."""
-    from_token = attr.ib(type=Optional[StreamToken])
-    to_token = attr.ib(type=Optional[StreamToken])
-    direction = attr.ib(type=str)
-    limit = attr.ib(type=Optional[int])
+    def __init__(self, from_token=None, to_token=None, direction="f", limit=None):
+        self.from_token = from_token
+        self.to_token = to_token
+        self.direction = "f" if direction == "f" else "b"
+        self.limit = min(int(limit), MAX_LIMIT) if limit is not None else None
     @classmethod
-    async def from_request(
-        cls,
-        store: "DataStore",
-        request: SynapseRequest,
-        raise_invalid_params: bool = True,
-        default_limit: Optional[int] = None,
-    ) -> "PaginationConfig":
+    def from_request(cls, request, raise_invalid_params=True, default_limit=None):
         direction = parse_string(request, "dir", default="f", allowed_values=["f", "b"])
         from_tok = parse_string(request, "from")
         to_tok = parse_string(request, "to")
         try:
             if from_tok == "END":
                 from_tok = None  # For backwards compat.
             elif from_tok:
-                from_tok = await StreamToken.from_string(store, from_tok)
+                from_tok = StreamToken.from_string(from_tok)
         except Exception:
             raise SynapseError(400, "'from' parameter is invalid")
         try:
             if to_tok:
-                to_tok = await StreamToken.from_string(store, to_tok)
+                to_tok = StreamToken.from_string(to_tok)
         except Exception:
             raise SynapseError(400, "'to' parameter is invalid")
         limit = parse_integer(request, "limit", default=default_limit)
-        if limit:
-            if limit < 0:
-                raise SynapseError(400, "Limit must be 0 or above")
-            limit = min(int(limit), MAX_LIMIT)
+        if limit and limit < 0:
+            raise SynapseError(400, "Limit must be 0 or above")
         try:
             return PaginationConfig(from_tok, to_tok, direction, limit)
         except Exception:
             logger.exception("Failed to create pagination config")
             raise SynapseError(400, "Invalid request.")
-    def __repr__(self) -> str:
+    def __repr__(self):
         return ("PaginationConfig(from_tok=%r, to_tok=%r, direction=%r, limit=%r)") % (
             self.from_token,
             self.to_token,
             self.direction,
             self.limit,
         )
+    def get_source_config(self, source_name):
+        keyname = "%s_key" % source_name
+        return SourcePaginationConfig(
+            from_key=getattr(self.from_token, keyname),
+            to_key=getattr(self.to_token, keyname) if self.to_token else None,
+            direction=self.direction,
+            limit=self.limit,
+        )

--- a/synapse/types.py
+++ b/synapse/types.py
@@ -1,32 +1,20 @@
 import abc
 import re
 import string
 import sys
 from collections import namedtuple
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Dict,
-    Mapping,
-    MutableMapping,
-    Optional,
-    Tuple,
-    Type,
-    TypeVar,
-)
+from typing import Any, Dict, Mapping, MutableMapping, Tuple, Type, TypeVar
 import attr
 from signedjson.key import decode_verify_key_bytes
 from unpaddedbase64 import decode_base64
 from synapse.api.errors import Codes, SynapseError
-if TYPE_CHECKING:
-    from synapse.storage.databases.main import DataStore
 if sys.version_info[:3] >= (3, 6, 0):
     from typing import Collection
 else:
     from typing import Container, Iterable, Sized
     T_co = TypeVar("T_co", covariant=True)
     class Collection(Iterable[T_co], Container[T_co], Sized):  # type: ignore
         __slots__ = ()
 T = TypeVar("T")
 StateKey = Tuple[str, str]
 StateMap = Mapping[StateKey, T]
@@ -121,29 +109,28 @@
     idx = string.find(":")
     if idx == -1:
         raise SynapseError(400, "Invalid ID: %r" % (string,))
     return string[idx + 1 :]
 def get_localpart_from_id(string):
     idx = string.find(":")
     if idx == -1:
         raise SynapseError(400, "Invalid ID: %r" % (string,))
     return string[1:idx]
 DS = TypeVar("DS", bound="DomainSpecificString")
-class DomainSpecificString(
-    namedtuple("DomainSpecificString", ("localpart", "domain")), metaclass=abc.ABCMeta
-):
+class DomainSpecificString(namedtuple("DomainSpecificString", ("localpart", "domain"))):
     """Common base class among ID/name strings that have a local part and a
     domain name, prefixed with a sigil.
     Has the fields:
         'localpart' : The local part of the name (without the leading sigil)
         'domain' : The domain part of the name
     """
+    __metaclass__ = abc.ABCMeta
     SIGIL = abc.abstractproperty()  # type: str  # type: ignore
     def __iter__(self):
         raise ValueError("Attempted to iterate a %s" % (type(self).__name__,))
     def __copy__(self):
         return self
     def __deepcopy__(self, memo):
         return self
     @classmethod
     def from_string(cls: Type[DS], s: str) -> DS:
         """Parse the string given by 's' into a structure object."""
@@ -238,157 +225,128 @@
     else:
         username = username.lower()
     def f2(m):
         g = m.group()[0]
         if isinstance(g, str):
             g = ord(g)
         return b"=%02x" % (g,)
     username = NON_MXID_CHARACTER_PATTERN.sub(f2, username)
     username = re.sub(b"^_", b"=5f", username)
     return username.decode("ascii")
-@attr.s(frozen=True, slots=True)
-class RoomStreamToken:
+class StreamToken(
+    namedtuple(
+        "Token",
+        (
+            "room_key",
+            "presence_key",
+            "typing_key",
+            "receipt_key",
+            "account_data_key",
+            "push_rules_key",
+            "to_device_key",
+            "device_list_key",
+            "groups_key",
+        ),
+    )
+):
+    _SEPARATOR = "_"
+    START = None  # type: StreamToken
+    @classmethod
+    def from_string(cls, string):
+        try:
+            keys = string.split(cls._SEPARATOR)
+            while len(keys) < len(cls._fields):
+                keys.append("0")
+            return cls(*keys)
+        except Exception:
+            raise SynapseError(400, "Invalid Token")
+    def to_string(self):
+        return self._SEPARATOR.join([str(k) for k in self])
+    @property
+    def room_stream_id(self):
+        if type(self.room_key) is int:
+            return self.room_key
+        else:
+            return int(self.room_key[1:].split("-")[-1])
+    def is_after(self, other):
+        """Does this token contain events that the other doesn't?"""
+        return (
+            (other.room_stream_id < self.room_stream_id)
+            or (int(other.presence_key) < int(self.presence_key))
+            or (int(other.typing_key) < int(self.typing_key))
+            or (int(other.receipt_key) < int(self.receipt_key))
+            or (int(other.account_data_key) < int(self.account_data_key))
+            or (int(other.push_rules_key) < int(self.push_rules_key))
+            or (int(other.to_device_key) < int(self.to_device_key))
+            or (int(other.device_list_key) < int(self.device_list_key))
+            or (int(other.groups_key) < int(self.groups_key))
+        )
+    def copy_and_advance(self, key, new_value):
+        """Advance the given key in the token to a new value if and only if the
+        new value is after the old value.
+        """
+        new_token = self.copy_and_replace(key, new_value)
+        if key == "room_key":
+            new_id = new_token.room_stream_id
+            old_id = self.room_stream_id
+        else:
+            new_id = int(getattr(new_token, key))
+            old_id = int(getattr(self, key))
+        if old_id < new_id:
+            return new_token
+        else:
+            return self
+    def copy_and_replace(self, key, new_value):
+        return self._replace(**{key: new_value})
+StreamToken.START = StreamToken(*(["s0"] + ["0"] * (len(StreamToken._fields) - 1)))
+class RoomStreamToken(namedtuple("_StreamToken", "topological stream")):
     """Tokens are positions between events. The token "s1" comes after event 1.
             s0    s1
             |     |
         [0] V [1] V [2]
     Tokens can either be a point in the live event stream or a cursor going
     through historic events.
     When traversing the live event stream events are ordered by when they
     arrived at the homeserver.
     When traversing historic events the events are ordered by their depth in
     the event graph "topological_ordering" and then by when they arrived at the
     homeserver "stream_ordering".
     Live tokens start with an "s" followed by the "stream_ordering" id of the
     event it comes after. Historic tokens start with a "t" followed by the
     "topological_ordering" id of the event it comes after, followed by "-",
     followed by the "stream_ordering" id of the event it comes after.
     """
-    topological = attr.ib(
-        type=Optional[int],
-        validator=attr.validators.optional(attr.validators.instance_of(int)),
-    )
-    stream = attr.ib(type=int, validator=attr.validators.instance_of(int))
-    @classmethod
-    async def parse(cls, store: "DataStore", string: str) -> "RoomStreamToken":
+    __slots__ = []  # type: list
+    @classmethod
+    def parse(cls, string):
         try:
             if string[0] == "s":
                 return cls(topological=None, stream=int(string[1:]))
             if string[0] == "t":
                 parts = string[1:].split("-", 1)
                 return cls(topological=int(parts[0]), stream=int(parts[1]))
         except Exception:
             pass
         raise SynapseError(400, "Invalid token %r" % (string,))
     @classmethod
-    def parse_stream_token(cls, string: str) -> "RoomStreamToken":
+    def parse_stream_token(cls, string):
         try:
             if string[0] == "s":
                 return cls(topological=None, stream=int(string[1:]))
         except Exception:
             pass
         raise SynapseError(400, "Invalid token %r" % (string,))
-    def copy_and_advance(self, other: "RoomStreamToken") -> "RoomStreamToken":
-        """Return a new token such that if an event is after both this token and
-        the other token, then its after the returned token too.
-        """
-        if self.topological or other.topological:
-            raise Exception("Can't advance topological tokens")
-        max_stream = max(self.stream, other.stream)
-        return RoomStreamToken(None, max_stream)
-    def as_tuple(self) -> Tuple[Optional[int], int]:
-        return (self.topological, self.stream)
-    async def to_string(self, store: "DataStore") -> str:
+    def __str__(self):
         if self.topological is not None:
             return "t%d-%d" % (self.topological, self.stream)
         else:
             return "s%d" % (self.stream,)
-@attr.s(slots=True, frozen=True)
-class StreamToken:
-    room_key = attr.ib(
-        type=RoomStreamToken, validator=attr.validators.instance_of(RoomStreamToken)
-    )
-    presence_key = attr.ib(type=int)
-    typing_key = attr.ib(type=int)
-    receipt_key = attr.ib(type=int)
-    account_data_key = attr.ib(type=int)
-    push_rules_key = attr.ib(type=int)
-    to_device_key = attr.ib(type=int)
-    device_list_key = attr.ib(type=int)
-    groups_key = attr.ib(type=int)
-    _SEPARATOR = "_"
-    START = None  # type: StreamToken
-    @classmethod
-    async def from_string(cls, store: "DataStore", string: str) -> "StreamToken":
-        try:
-            keys = string.split(cls._SEPARATOR)
-            while len(keys) < len(attr.fields(cls)):
-                keys.append("0")
-            return cls(
-                await RoomStreamToken.parse(store, keys[0]), *(int(k) for k in keys[1:])
-            )
-        except Exception:
-            raise SynapseError(400, "Invalid Token")
-    async def to_string(self, store: "DataStore") -> str:
-        return self._SEPARATOR.join(
-            [
-                await self.room_key.to_string(store),
-                str(self.presence_key),
-                str(self.typing_key),
-                str(self.receipt_key),
-                str(self.account_data_key),
-                str(self.push_rules_key),
-                str(self.to_device_key),
-                str(self.device_list_key),
-                str(self.groups_key),
-            ]
-        )
-    @property
-    def room_stream_id(self):
-        return self.room_key.stream
-    def copy_and_advance(self, key, new_value) -> "StreamToken":
-        """Advance the given key in the token to a new value if and only if the
-        new value is after the old value.
-        """
-        if key == "room_key":
-            new_token = self.copy_and_replace(
-                "room_key", self.room_key.copy_and_advance(new_value)
-            )
-            return new_token
-        new_token = self.copy_and_replace(key, new_value)
-        new_id = int(getattr(new_token, key))
-        old_id = int(getattr(self, key))
-        if old_id < new_id:
-            return new_token
-        else:
-            return self
-    def copy_and_replace(self, key, new_value) -> "StreamToken":
-        return attr.evolve(self, **{key: new_value})
-StreamToken.START = StreamToken(RoomStreamToken(None, 0), 0, 0, 0, 0, 0, 0, 0, 0)
-@attr.s(slots=True, frozen=True)
-class PersistedEventPosition:
-    """Position of a newly persisted event with instance that persisted it.
-    This can be used to test whether the event is persisted before or after a
-    RoomStreamToken.
-    """
-    instance_name = attr.ib(type=str)
-    stream = attr.ib(type=int)
-    def persisted_after(self, token: RoomStreamToken) -> bool:
-        return token.stream < self.stream
-    def to_room_stream_token(self) -> RoomStreamToken:
-        """Converts the position to a room stream token such that events
-        persisted in the same room after this position will be after the
-        returned `RoomStreamToken`.
-        Note: no guarentees are made about ordering w.r.t. events in other
-        rooms.
-        """
-        return RoomStreamToken(None, self.stream)
 class ThirdPartyInstanceID(
     namedtuple("ThirdPartyInstanceID", ("appservice_id", "network_id"))
 ):
     def __iter__(self):
         raise ValueError("Attempted to iterate a %s" % (type(self).__name__,))
     def __copy__(self):
         return self
     def __deepcopy__(self, memo):
         return self
     @classmethod

--- a/synapse/util/__init__.py
+++ b/synapse/util/__init__.py
@@ -1,26 +1,26 @@
-import json
 import logging
 import re
 import attr
+from canonicaljson import json
 from twisted.internet import defer, task
 from synapse.logging import context
 logger = logging.getLogger(__name__)
 def _reject_invalid_json(val):
     """Do not allow Infinity, -Infinity, or NaN values in JSON."""
     raise ValueError("Invalid JSON value: '%s'" % val)
 json_encoder = json.JSONEncoder(allow_nan=False, separators=(",", ":"))
 json_decoder = json.JSONDecoder(parse_constant=_reject_invalid_json)
 def unwrapFirstError(failure):
     failure.trap(defer.FirstError)
     return failure.value.subFailure
-@attr.s(slots=True)
+@attr.s
 class Clock:
     """
     A Clock wraps a Twisted reactor and provides utilities on top of it.
     Args:
         reactor: The Twisted reactor to use.
     """
     _reactor = attr.ib()
     @defer.inlineCallbacks
     def sleep(self, seconds):
         d = defer.Deferred()

--- a/synapse/util/async_helpers.py
+++ b/synapse/util/async_helpers.py
@@ -1,30 +1,18 @@
 import collections
 import logging
 from contextlib import contextmanager
-from typing import (
-    Any,
-    Callable,
-    Dict,
-    Hashable,
-    Iterable,
-    List,
-    Optional,
-    Set,
-    TypeVar,
-    Union,
-)
+from typing import Dict, Sequence, Set, Union
 import attr
 from typing_extensions import ContextManager
 from twisted.internet import defer
 from twisted.internet.defer import CancelledError
-from twisted.internet.interfaces import IReactorTime
 from twisted.python import failure
 from synapse.logging.context import (
     PreserveLoggingContext,
     make_deferred_yieldable,
     run_in_background,
 )
 from synapse.util import Clock, unwrapFirstError
 logger = logging.getLogger(__name__)
 class ObservableDeferred:
     """Wraps a deferred object so that we can add observer deferreds. These
@@ -32,21 +20,21 @@
     deferred.
     If consumeErrors is true errors will be captured from the origin deferred.
     Cancelling or otherwise resolving an observer will not affect the original
     ObservableDeferred.
     NB that it does not attempt to do anything with logcontexts; in general
     you should probably make_deferred_yieldable the deferreds
     returned by `observe`, and ensure that the original deferred runs its
     callbacks in the sentinel logcontext.
     """
     __slots__ = ["_deferred", "_observers", "_result"]
-    def __init__(self, deferred: defer.Deferred, consumeErrors: bool = False):
+    def __init__(self, deferred, consumeErrors=False):
         object.__setattr__(self, "_deferred", deferred)
         object.__setattr__(self, "_result", None)
         object.__setattr__(self, "_observers", set())
         def callback(r):
             object.__setattr__(self, "_result", (True, r))
             while self._observers:
                 try:
                     self._observers.pop().callback(r)
                 except Exception:
                     pass
@@ -74,176 +62,162 @@
             d = defer.Deferred()
             def remove(r):
                 self._observers.discard(d)
                 return r
             d.addBoth(remove)
             self._observers.add(d)
             return d
         else:
             success, res = self._result
             return defer.succeed(res) if success else defer.fail(res)
-    def observers(self) -> List[defer.Deferred]:
+    def observers(self):
         return self._observers
-    def has_called(self) -> bool:
+    def has_called(self):
         return self._result is not None
-    def has_succeeded(self) -> bool:
+    def has_succeeded(self):
         return self._result is not None and self._result[0] is True
-    def get_result(self) -> Any:
+    def get_result(self):
         return self._result[1]
-    def __getattr__(self, name: str) -> Any:
+    def __getattr__(self, name):
         return getattr(self._deferred, name)
-    def __setattr__(self, name: str, value: Any) -> None:
+    def __setattr__(self, name, value):
         setattr(self._deferred, name, value)
-    def __repr__(self) -> str:
+    def __repr__(self):
         return "<ObservableDeferred object at %s, result=%r, _deferred=%r>" % (
             id(self),
             self._result,
             self._deferred,
         )
-def concurrently_execute(
-    func: Callable, args: Iterable[Any], limit: int
-) -> defer.Deferred:
-    """Executes the function with each argument concurrently while limiting
+def concurrently_execute(func, args, limit):
+    """Executes the function with each argument conncurrently while limiting
     the number of concurrent executions.
     Args:
-        func: Function to execute, should return a deferred or coroutine.
-        args: List of arguments to pass to func, each invocation of func
+        func (func): Function to execute, should return a deferred or coroutine.
+        args (Iterable): List of arguments to pass to func, each invocation of func
             gets a single argument.
-        limit: Maximum number of conccurent executions.
+        limit (int): Maximum number of conccurent executions.
     Returns:
-        Deferred[list]: Resolved when all function invocations have finished.
+        deferred: Resolved when all function invocations have finished.
     """
     it = iter(args)
     async def _concurrently_execute_inner():
         try:
             while True:
                 await maybe_awaitable(func(next(it)))
         except StopIteration:
             pass
     return make_deferred_yieldable(
         defer.gatherResults(
             [run_in_background(_concurrently_execute_inner) for _ in range(limit)],
             consumeErrors=True,
         )
     ).addErrback(unwrapFirstError)
-def yieldable_gather_results(
-    func: Callable, iter: Iterable, *args: Any, **kwargs: Any
-) -> defer.Deferred:
+def yieldable_gather_results(func, iter, *args, **kwargs):
     """Executes the function with each argument concurrently.
     Args:
-        func: Function to execute that returns a Deferred
-        iter: An iterable that yields items that get passed as the first
+        func (func): Function to execute that returns a Deferred
+        iter (iter): An iterable that yields items that get passed as the first
             argument to the function
         *args: Arguments to be passed to each call to func
-        **kwargs: Keyword arguments to be passed to each call to func
     Returns
         Deferred[list]: Resolved when all functions have been invoked, or errors if
         one of the function calls fails.
     """
     return make_deferred_yieldable(
         defer.gatherResults(
             [run_in_background(func, item, *args, **kwargs) for item in iter],
             consumeErrors=True,
         )
     ).addErrback(unwrapFirstError)
-@attr.s(slots=True)
-class _LinearizerEntry:
-    count = attr.ib(type=int)
-    deferreds = attr.ib(type=collections.OrderedDict)
 class Linearizer:
     """Limits concurrent access to resources based on a key. Useful to ensure
     only a few things happen at a time on a given resource.
     Example:
-        with await limiter.queue("test_key"):
-    """
-    def __init__(
-        self,
-        name: Optional[str] = None,
-        max_count: int = 1,
-        clock: Optional[Clock] = None,
-    ):
+        with (yield limiter.queue("test_key")):
+    """
+    def __init__(self, name=None, max_count=1, clock=None):
         """
         Args:
-            max_count: The maximum number of concurrent accesses
+            max_count(int): The maximum number of concurrent accesses
         """
         if name is None:
-            self.name = id(self)  # type: Union[str, int]
+            self.name = id(self)
         else:
             self.name = name
         if not clock:
             from twisted.internet import reactor
             clock = Clock(reactor)
         self._clock = clock
         self.max_count = max_count
-        self.key_to_defer = {}  # type: Dict[Hashable, _LinearizerEntry]
-    def is_queued(self, key: Hashable) -> bool:
+        self.key_to_defer = (
+            {}
+        )  # type: Dict[str, Sequence[Union[int, Dict[defer.Deferred, int]]]]
+    def is_queued(self, key) -> bool:
         """Checks whether there is a process queued up waiting
         """
         entry = self.key_to_defer.get(key)
         if not entry:
             return False
-        return bool(entry.deferreds)
-    def queue(self, key: Hashable) -> defer.Deferred:
-        entry = self.key_to_defer.setdefault(
-            key, _LinearizerEntry(0, collections.OrderedDict())
-        )
-        if entry.count >= self.max_count:
+        return bool(entry[1])
+    def queue(self, key):
+        entry = self.key_to_defer.setdefault(key, [0, collections.OrderedDict()])
+        if entry[0] >= self.max_count:
             res = self._await_lock(key)
         else:
             logger.debug(
                 "Acquired uncontended linearizer lock %r for key %r", self.name, key
             )
-            entry.count += 1
+            entry[0] += 1
             res = defer.succeed(None)
         @contextmanager
         def _ctx_manager(_):
             try:
                 yield
             finally:
                 logger.debug("Releasing linearizer lock %r for key %r", self.name, key)
-                entry.count -= 1
-                if entry.deferreds:
-                    (next_def, _) = entry.deferreds.popitem(last=False)
+                entry[0] -= 1
+                if entry[1]:
+                    (next_def, _) = entry[1].popitem(last=False)
                     with PreserveLoggingContext():
                         next_def.callback(None)
-                elif entry.count == 0:
+                elif entry[0] == 0:
                     del self.key_to_defer[key]
         res.addCallback(_ctx_manager)
         return res
-    def _await_lock(self, key: Hashable) -> defer.Deferred:
+    def _await_lock(self, key):
         """Helper for queue: adds a deferred to the queue
         Assumes that we've already checked that we've reached the limit of the number
         of lock-holders we allow. Creates a new deferred which is added to the list, and
         adds some management around cancellations.
         Returns the deferred, which will callback once we have secured the lock.
         """
         entry = self.key_to_defer[key]
         logger.debug("Waiting to acquire linearizer lock %r for key %r", self.name, key)
         new_defer = make_deferred_yieldable(defer.Deferred())
-        entry.deferreds[new_defer] = 1
+        entry[1][new_defer] = 1
         def cb(_r):
             logger.debug("Acquired linearizer lock %r for key %r", self.name, key)
-            entry.count += 1
+            entry[0] += 1
             return self._clock.sleep(0)
         def eb(e):
             logger.info("defer %r got err %r", new_defer, e)
             if isinstance(e, CancelledError):
                 logger.debug(
                     "Cancelling wait for linearizer lock %r for key %r", self.name, key
                 )
             else:
                 logger.warning(
                     "Unexpected exception waiting for linearizer lock %r for key %r",
                     self.name,
                     key,
                 )
-            del entry.deferreds[new_defer]
+            del entry[1][new_defer]
             return e
         new_defer.addCallbacks(cb, eb)
         return new_defer
 class ReadWriteLock:
     """An async read write lock.
     Example:
         with await read_write_lock.read("test_key"):
     """
     def __init__(self):
         self.key_to_current_readers = {}  # type: Dict[str, Set[defer.Deferred]]
@@ -275,56 +249,64 @@
         await make_deferred_yieldable(defer.gatherResults(to_wait_on))
         @contextmanager
         def _ctx_manager():
             try:
                 yield
             finally:
                 new_defer.callback(None)
                 if self.key_to_current_writer[key] == new_defer:
                     self.key_to_current_writer.pop(key)
         return _ctx_manager()
-R = TypeVar("R")
-def timeout_deferred(
-    deferred: defer.Deferred, timeout: float, reactor: IReactorTime,
-) -> defer.Deferred:
+def _cancelled_to_timed_out_error(value, timeout):
+    if isinstance(value, failure.Failure):
+        value.trap(CancelledError)
+        raise defer.TimeoutError(timeout, "Deferred")
+    return value
+def timeout_deferred(deferred, timeout, reactor, on_timeout_cancel=None):
     """The in built twisted `Deferred.addTimeout` fails to time out deferreds
     that have a canceller that throws exceptions. This method creates a new
     deferred that wraps and times out the given deferred, correctly handling
     the case where the given deferred's canceller throws.
     (See https://twistedmatrix.com/trac/ticket/9534)
-    NOTE: Unlike `Deferred.addTimeout`, this function returns a new deferred.
-    NOTE: the TimeoutError raised by the resultant deferred is
-    twisted.internet.defer.TimeoutError, which is *different* to the built-in
-    TimeoutError, as well as various other TimeoutErrors you might have imported.
+    NOTE: Unlike `Deferred.addTimeout`, this function returns a new deferred
     Args:
-        deferred: The Deferred to potentially timeout.
-        timeout: Timeout in seconds
-        reactor: The twisted reactor to use
+        deferred (Deferred)
+        timeout (float): Timeout in seconds
+        reactor (twisted.interfaces.IReactorTime): The twisted reactor to use
+        on_timeout_cancel (callable): A callable which is called immediately
+            after the deferred times out, and not if this deferred is
+            otherwise cancelled before the timeout.
+            It takes an arbitrary value, which is the value of the deferred at
+            that exact point in time (probably a CancelledError Failure), and
+            the timeout.
+            The default callable (if none is provided) will translate a
+            CancelledError Failure into a defer.TimeoutError.
     Returns:
-        A new Deferred, which will errback with defer.TimeoutError on timeout.
+        Deferred
     """
     new_d = defer.Deferred()
     timed_out = [False]
     def time_it_out():
         timed_out[0] = True
         try:
             deferred.cancel()
         except:  # noqa: E722, if we throw any exception it'll break time outs
             logger.exception("Canceller failed during timeout")
         if not new_d.called:
-            new_d.errback(defer.TimeoutError("Timed out after %gs" % (timeout,)))
+            new_d.errback(defer.TimeoutError(timeout, "Deferred"))
     delayed_call = reactor.callLater(timeout, time_it_out)
-    def convert_cancelled(value: failure.Failure):
-        if timed_out[0] and value.check(CancelledError):
-            raise defer.TimeoutError("Timed out after %gs" % (timeout,))
+    def convert_cancelled(value):
+        if timed_out[0]:
+            to_call = on_timeout_cancel or _cancelled_to_timed_out_error
+            return to_call(value, timeout)
         return value
-    deferred.addErrback(convert_cancelled)
+    deferred.addBoth(convert_cancelled)
     def cancel_timeout(result):
         if delayed_call.active():
             delayed_call.cancel()
         return result
     deferred.addBoth(cancel_timeout)
     def success_cb(val):
         if not new_d.called:
             new_d.callback(val)
     def failure_cb(val):
         if not new_d.called:

--- a/synapse/util/caches/__init__.py
+++ b/synapse/util/caches/__init__.py
@@ -11,21 +11,21 @@
 cache_hits = Gauge("synapse_util_caches_cache:hits", "", ["name"])
 cache_evicted = Gauge("synapse_util_caches_cache:evicted_size", "", ["name"])
 cache_total = Gauge("synapse_util_caches_cache:total", "", ["name"])
 cache_max_size = Gauge("synapse_util_caches_cache_max_size", "", ["name"])
 response_cache_size = Gauge("synapse_util_caches_response_cache:size", "", ["name"])
 response_cache_hits = Gauge("synapse_util_caches_response_cache:hits", "", ["name"])
 response_cache_evicted = Gauge(
     "synapse_util_caches_response_cache:evicted_size", "", ["name"]
 )
 response_cache_total = Gauge("synapse_util_caches_response_cache:total", "", ["name"])
-@attr.s(slots=True)
+@attr.s
 class CacheMetric:
     _cache = attr.ib()
     _cache_type = attr.ib(type=str)
     _cache_name = attr.ib(type=str)
     _collect_callback = attr.ib(type=Optional[Callable])
     hits = attr.ib(default=0)
     misses = attr.ib(default=0)
     evicted_size = attr.ib(default=0)
     def inc_hits(self):
         self.hits += 1

--- a/synapse/util/distributor.py
+++ b/synapse/util/distributor.py
@@ -1,18 +1,22 @@
 import inspect
 import logging
 from twisted.internet import defer
+from twisted.internet.defer import Deferred, fail, succeed
+from twisted.python import failure
 from synapse.logging.context import make_deferred_yieldable, run_in_background
 from synapse.metrics.background_process_metrics import run_as_background_process
 logger = logging.getLogger(__name__)
 def user_left_room(distributor, user, room_id):
     distributor.fire("user_left_room", user=user, room_id=room_id)
+def user_joined_room(distributor, user, room_id):
+    distributor.fire("user_joined_room", user=user, room_id=room_id)
 class Distributor:
     """A central dispatch point for loosely-connected pieces of code to
     register, observe, and fire signals.
     Signals are named simply by strings.
     TODO(paul): It would be nice to give signals stronger object identities,
       so we can attach metadata, docstrings, detect typos, etc... But this
       model will do for today.
     """
     def __init__(self):
         self.signals = {}
@@ -32,20 +36,37 @@
             if name not in self.pre_registration:
                 self.pre_registration[name] = []
             self.pre_registration[name].append(observer)
     def fire(self, name, *args, **kwargs):
         """Dispatches the given signal to the registered observers.
         Runs the observers as a background process. Does not return a deferred.
         """
         if name not in self.signals:
             raise KeyError("%r does not have a signal named %s" % (self, name))
         run_as_background_process(name, self.signals[name].fire, *args, **kwargs)
+def maybeAwaitableDeferred(f, *args, **kw):
+    """
+    Invoke a function that may or may not return a Deferred or an Awaitable.
+    This is a modified version of twisted.internet.defer.maybeDeferred.
+    """
+    try:
+        result = f(*args, **kw)
+    except Exception:
+        return fail(failure.Failure(captureVars=Deferred.debug))
+    if isinstance(result, Deferred):
+        return result
+    elif inspect.isawaitable(result):
+        return defer.ensureDeferred(result)
+    elif isinstance(result, failure.Failure):
+        return fail(result)
+    else:
+        return succeed(result)
 class Signal:
     """A Signal is a dispatch point that stores a list of callables as
     observers of it.
     Signals can be "fired", meaning that every callable observing it is
     invoked. Firing a signal does not change its state; it can be fired again
     at any later point. Firing a signal passes any arguments from the fire
     method into all of the observers.
     """
     def __init__(self, name):
         self.name = name
@@ -54,26 +75,30 @@
         """Adds a new callable to the observer list which will be invoked by
         the 'fire' method.
         Each observer callable may return a Deferred."""
         self.observers.append(observer)
     def fire(self, *args, **kwargs):
         """Invokes every callable in the observer list, passing in the args and
         kwargs. Exceptions thrown by observers are logged but ignored. It is
         not an error to fire a signal with no observers.
         Returns a Deferred that will complete when all the observers have
         completed."""
-        async def do(observer):
-            try:
-                result = observer(*args, **kwargs)
-                if inspect.isawaitable(result):
-                    result = await result
-                return result
-            except Exception as e:
+        def do(observer):
+            def eb(failure):
                 logger.warning(
-                    "%s signal observer %s failed: %r", self.name, observer, e,
+                    "%s signal observer %s failed: %r",
+                    self.name,
+                    observer,
+                    failure,
+                    exc_info=(
+                        failure.type,
+                        failure.value,
+                        failure.getTracebackObject(),
+                    ),
                 )
+            return maybeAwaitableDeferred(observer, *args, **kwargs).addErrback(eb)
         deferreds = [run_in_background(do, o) for o in self.observers]
         return make_deferred_yieldable(
             defer.gatherResults(deferreds, consumeErrors=True)
         )
     def __repr__(self):
         return "<Signal name=%r>" % (self.name,)

--- a/synapse/util/frozenutils.py
+++ b/synapse/util/frozenutils.py
@@ -1,11 +1,11 @@
-import json
+from canonicaljson import json
 from frozendict import frozendict
 def freeze(o):
     if isinstance(o, dict):
         return frozendict({k: freeze(v) for k, v in o.items()})
     if isinstance(o, frozendict):
         return o
     if isinstance(o, (bytes, str)):
         return o
     try:
         return tuple(freeze(i) for i in o)
@@ -25,12 +25,12 @@
 def _handle_frozendict(obj):
     """Helper for EventEncoder. Makes frozendicts serializable by returning
     the underlying dict
     """
     if type(obj) is frozendict:
         return obj._dict
     raise TypeError(
         "Object of type %s is not JSON serializable" % obj.__class__.__name__
     )
 frozendict_json_encoder = json.JSONEncoder(
-    allow_nan=False, separators=(",", ":"), default=_handle_frozendict,
+    default=_handle_frozendict, separators=(",", ":"),
 )

--- a/synapse/util/manhole.py
+++ b/synapse/util/manhole.py
@@ -59,21 +59,21 @@
     rlm.chainedProtocolFactory = lambda: insults.ServerProtocol(
         SynapseManhole, dict(globals, __name__="__console__")
     )
     factory = manhole_ssh.ConchFactory(portal.Portal(rlm, [checker]))
     factory.publicKeys[b"ssh-rsa"] = Key.fromString(PUBLIC_KEY)
     factory.privateKeys[b"ssh-rsa"] = Key.fromString(PRIVATE_KEY)
     return factory
 class SynapseManhole(ColoredManhole):
     """Overrides connectionMade to create our own ManholeInterpreter"""
     def connectionMade(self):
-        super().connectionMade()
+        super(SynapseManhole, self).connectionMade()
         self.interpreter = SynapseManholeInterpreter(self, self.namespace)
 class SynapseManholeInterpreter(ManholeInterpreter):
     def showsyntaxerror(self, filename=None):
         """Display the syntax error that just occurred.
         Overrides the base implementation, ignoring sys.excepthook. We always want
         any syntax errors to be sent to the terminal, rather than sentry.
         """
         type, value, tb = sys.exc_info()
         sys.last_type = type
         sys.last_value = value

--- a/synapse/util/metrics.py
+++ b/synapse/util/metrics.py
@@ -1,19 +1,15 @@
 import logging
 from functools import wraps
 from typing import Any, Callable, Optional, TypeVar, cast
 from prometheus_client import Counter
-from synapse.logging.context import (
-    ContextResourceUsage,
-    LoggingContext,
-    current_context,
-)
+from synapse.logging.context import LoggingContext, current_context
 from synapse.metrics import InFlightGauge
 logger = logging.getLogger(__name__)
 block_counter = Counter("synapse_util_metrics_block_count", "", ["block_name"])
 block_timer = Counter("synapse_util_metrics_block_time_seconds", "", ["block_name"])
 block_ru_utime = Counter(
     "synapse_util_metrics_block_ru_utime_seconds", "", ["block_name"]
 )
 block_ru_stime = Counter(
     "synapse_util_metrics_block_ru_stime_seconds", "", ["block_name"]
 )
@@ -57,50 +53,45 @@
 class Measure:
     __slots__ = [
         "clock",
         "name",
         "_logging_context",
         "start",
     ]
     def __init__(self, clock, name):
         self.clock = clock
         self.name = name
+        self._logging_context = None
+        self.start = None
+    def __enter__(self):
+        if self._logging_context:
+            raise RuntimeError("Measure() objects cannot be re-used")
+        self.start = self.clock.time()
         parent_context = current_context()
         self._logging_context = LoggingContext(
             "Measure[%s]" % (self.name,), parent_context
         )
-        self.start = None
-    def __enter__(self) -> "Measure":
-        if self.start is not None:
-            raise RuntimeError("Measure() objects cannot be re-used")
-        self.start = self.clock.time()
         self._logging_context.__enter__()
         in_flight.register((self.name,), self._update_in_flight)
-        return self
     def __exit__(self, exc_type, exc_val, exc_tb):
-        if self.start is None:
+        if not self._logging_context:
             raise RuntimeError("Measure() block exited without being entered")
         duration = self.clock.time() - self.start
-        usage = self.get_resource_usage()
+        usage = self._logging_context.get_resource_usage()
         in_flight.unregister((self.name,), self._update_in_flight)
         self._logging_context.__exit__(exc_type, exc_val, exc_tb)
         try:
             block_counter.labels(self.name).inc()
             block_timer.labels(self.name).inc(duration)
             block_ru_utime.labels(self.name).inc(usage.ru_utime)
             block_ru_stime.labels(self.name).inc(usage.ru_stime)
             block_db_txn_count.labels(self.name).inc(usage.db_txn_count)
             block_db_txn_duration.labels(self.name).inc(usage.db_txn_duration_sec)
             block_db_sched_duration.labels(self.name).inc(usage.db_sched_duration_sec)
         except ValueError:
             logger.warning("Failed to save metrics! Usage: %s", usage)
-    def get_resource_usage(self) -> ContextResourceUsage:
-        """Get the resources used within this Measure block
-        If the Measure block is still active, returns the resource usage so far.
-        """
-        return self._logging_context.get_resource_usage()
     def _update_in_flight(self, metrics):
         """Gets called when processing in flight metrics
         """
         duration = self.clock.time() - self.start
         metrics.real_time_max = max(metrics.real_time_max, duration)
         metrics.real_time_sum += duration

--- a/synapse/util/patch_inline_callbacks.py
+++ b/synapse/util/patch_inline_callbacks.py
@@ -1,10 +1,11 @@
+from __future__ import print_function
 import functools
 import sys
 from typing import Any, Callable, List
 from twisted.internet import defer
 from twisted.internet.defer import Deferred
 from twisted.python.failure import Failure
 _already_patched = False
 def do_patch():
     """
     Patch defer.inlineCallbacks so that it checks the state of the logcontext on exit

--- a/synapse/util/retryutils.py
+++ b/synapse/util/retryutils.py
@@ -12,21 +12,21 @@
         are deliberately not attempting to contact a given server.
         Args:
             retry_last_ts (int): the unix ts in milliseconds of our last attempt
                 to contact the server.  0 indicates that the last attempt was
                 successful or that we've never actually attempted to connect.
             retry_interval (int): the time in milliseconds to wait until the next
                 attempt.
             destination (str): the domain in question
         """
         msg = "Not retrying server %s." % (destination,)
-        super().__init__(msg)
+        super(NotRetryingDestination, self).__init__(msg)
         self.retry_last_ts = retry_last_ts
         self.retry_interval = retry_interval
         self.destination = destination
 async def get_retry_limiter(destination, clock, store, ignore_backoff=False, **kwargs):
     """For a given destination check if we have previously failed to
     send a request there and are waiting before retrying the destination.
     If we are not ready to retry the destination, this will raise a
     NotRetryingDestination exception. Otherwise, will return a Context Manager
     that will mark the destination as down if an exception is thrown (excluding
     CodeMessageException with code < 500)
