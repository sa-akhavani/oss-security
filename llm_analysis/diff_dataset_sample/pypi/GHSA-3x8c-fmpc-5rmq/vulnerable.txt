# ====================================================================
# FILE: contrib/cmdclient/console.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-22 ---
     1| """ Starts a synapse client console. """
     2| from __future__ import print_function
     3| import argparse
     4| import cmd
     5| import getpass
     6| import json
     7| import shlex
     8| import sys
     9| import time
    10| import urllib
    11| from http import TwistedHttpClient
    12| import nacl.encoding
    13| import nacl.signing
    14| import urlparse
    15| from signedjson.sign import SignatureVerifyException, verify_signed_json
    16| from twisted.internet import defer, reactor, threads
    17| CONFIG_JSON = "cmdclient_config.json"
    18| TRUSTED_ID_SERVERS = ["localhost:8001"]
    19| class SynapseCmd(cmd.Cmd):
    20|     """Basic synapse command-line processor.
    21|     This processes commands from the user and calls the relevant HTTP methods.
    22|     """


# ====================================================================
# FILE: contrib/cmdclient/http.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-21 ---
     1| from __future__ import print_function
     2| import json
     3| import urllib
     4| from pprint import pformat
     5| from twisted.internet import defer, reactor
     6| from twisted.web.client import Agent, readBody
     7| from twisted.web.http_headers import Headers
     8| class HttpClient:
     9|     """ Interface for talking json over http
    10|     """
    11|     def put_json(self, url, data):
    12|         """ Sends the specifed json data using PUT
    13|         Args:
    14|             url (str): The URL to PUT data to.
    15|             data (dict): A dict containing the data that will be used as
    16|                 the request body. This will be encoded as JSON.
    17|         Returns:
    18|             Deferred: Succeeds when we get a 2xx HTTP response. The result
    19|             will be the decoded JSON body.
    20|         """
    21|         pass


# ====================================================================
# FILE: contrib/graph/graph.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-21 ---
     1| from __future__ import print_function
     2| import argparse
     3| import cgi
     4| import datetime
     5| import json
     6| import pydot
     7| import urllib2
     8| def make_name(pdu_id, origin):
     9|     return "%s@%s" % (pdu_id, origin)
    10| def make_graph(pdus, room, filename_prefix):
    11|     pdu_map = {}
    12|     node_map = {}
    13|     origins = set()
    14|     colors = {"red", "green", "blue", "yellow", "purple"}
    15|     for pdu in pdus:
    16|         origins.add(pdu.get("origin"))
    17|     color_map = {color: color for color in colors if color in origins}
    18|     colors -= set(color_map.values())
    19|     color_map[None] = "black"
    20|     for o in origins:
    21|         if o in color_map:


# ====================================================================
# FILE: contrib/graph/graph3.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-21 ---
     1| from __future__ import print_function
     2| import argparse
     3| import cgi
     4| import datetime
     5| import pydot
     6| import simplejson as json
     7| from synapse.events import FrozenEvent
     8| from synapse.util.frozenutils import unfreeze
     9| def make_graph(file_name, room_id, file_prefix, limit):
    10|     print("Reading lines")
    11|     with open(file_name) as f:
    12|         lines = f.readlines()
    13|     print("Read lines")
    14|     events = [FrozenEvent(json.loads(line)) for line in lines]
    15|     print("Loaded events.")
    16|     events.sort(key=lambda e: e.depth)
    17|     print("Sorted events")
    18|     if limit:
    19|         events = events[-int(limit) :]
    20|     node_map = {}
    21|     graph = pydot.Dot(graph_name="Test")


# ====================================================================
# FILE: contrib/jitsimeetbridge/jitsimeetbridge.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-30 ---
     1| """
     2| This is an attempt at bridging matrix clients into a Jitis meet room via Matrix
     3| video call.  It uses hard-coded xml strings overg XMPP BOSH. It can display one
     4| of the streams from the Jitsi bridge until the second lot of SDP comes down and
     5| we set the remote SDP at which point the stream ends. Our video never gets to
     6| the bridge.
     7| Requires:
     8| npm install jquery jsdom
     9| """
    10| from __future__ import print_function
    11| import json
    12| import subprocess
    13| import time
    14| import gevent
    15| import grequests
    16| from BeautifulSoup import BeautifulSoup
    17| ACCESS_TOKEN = ""
    18| MATRIXBASE = "https://matrix.org/_matrix/client/api/v1/"
    19| MYUSERNAME = "@davetest:matrix.org"
    20| HTTPBIND = "https://meet.jit.si/http-bind"
    21| ROOMNAME = "pibble"
    22| HOST = "guest.jit.si"
    23| TURNSERVER = "turn.guest.jit.si"
    24| ROOMDOMAIN = "meet.jit.si"
    25| class TrivialMatrixClient:
    26|     def __init__(self, access_token):
    27|         self.token = None
    28|         self.access_token = access_token
    29|     def getEvent(self):
    30|         while True:


# ====================================================================
# FILE: contrib/scripts/kick_users.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-70 ---
     1| from __future__ import print_function
     2| import json
     3| import sys
     4| import urllib
     5| from argparse import ArgumentParser
     6| import requests
     7| try:
     8|     raw_input
     9| except NameError:  # Python 3
    10|     raw_input = input
    11| def _mkurl(template, kws):
    12|     for key in kws:
    13|         template = template.replace(key, kws[key])
    14|     return template
    15| def main(hs, room_id, access_token, user_id_prefix, why):
    16|     if not why:
    17|         why = "Automated kick."
    18|     print(
    19|         "Kicking members on %s in room %s matching %s" % (hs, room_id, user_id_prefix)
    20|     )
    21|     room_state_url = _mkurl(
    22|         "$HS/_matrix/client/api/v1/rooms/$ROOM/state?access_token=$TOKEN",
    23|         {"$HS": hs, "$ROOM": room_id, "$TOKEN": access_token},
    24|     )
    25|     print("Getting room state => %s" % room_state_url)
    26|     res = requests.get(room_state_url)
    27|     print("HTTP %s" % res.status_code)
    28|     state_events = res.json()
    29|     if "error" in state_events:
    30|         print("FATAL")
    31|         print(state_events)
    32|         return
    33|     kick_list = []
    34|     room_name = room_id
    35|     for event in state_events:
    36|         if not event["type"] == "m.room.member":
    37|             if event["type"] == "m.room.name":
    38|                 room_name = event["content"].get("name")
    39|             continue
    40|         if not event["content"].get("membership") == "join":
    41|             continue
    42|         if event["state_key"].startswith(user_id_prefix):
    43|             kick_list.append(event["state_key"])
    44|     if len(kick_list) == 0:
    45|         print("No user IDs match the prefix '%s'" % user_id_prefix)
    46|         return
    47|     print("The following user IDs will be kicked from %s" % room_name)
    48|     for uid in kick_list:
    49|         print(uid)
    50|     doit = raw_input("Continue? [Y]es\n")
    51|     if len(doit) > 0 and doit.lower() == "y":
    52|         print("Kicking members...")
    53|         kick_list = [urllib.quote(uid) for uid in kick_list]
    54|         for uid in kick_list:
    55|             kick_url = _mkurl(
    56|                 "$HS/_matrix/client/api/v1/rooms/$ROOM/state/m.room.member/$UID?access_token=$TOKEN",
    57|                 {"$HS": hs, "$UID": uid, "$ROOM": room_id, "$TOKEN": access_token},
    58|             )
    59|             kick_body = {"membership": "leave", "reason": why}
    60|             print("Kicking %s" % uid)
    61|             res = requests.put(kick_url, data=json.dumps(kick_body))
    62|             if res.status_code != 200:
    63|                 print("ERROR: HTTP %s" % res.status_code)
    64|             if res.json().get("error"):
    65|                 print("ERROR: JSON %s" % res.json())
    66| if __name__ == "__main__":
    67|     parser = ArgumentParser("Kick members in a room matching a certain user ID prefix.")
    68|     parser.add_argument("-u", "--user-id", help="The user ID prefix e.g. '@irc_'")
    69|     parser.add_argument("-t", "--token", help="Your access_token")
    70|     parser.add_argument("-r", "--room", help="The room ID to kick members in")


# ====================================================================
# FILE: scripts-dev/definitions.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-30 ---
     1| from __future__ import print_function
     2| import argparse
     3| import ast
     4| import os
     5| import re
     6| import sys
     7| import yaml
     8| class DefinitionVisitor(ast.NodeVisitor):
     9|     def __init__(self):
    10|         super(DefinitionVisitor, self).__init__()
    11|         self.functions = {}
    12|         self.classes = {}
    13|         self.names = {}
    14|         self.attrs = set()
    15|         self.definitions = {
    16|             "def": self.functions,
    17|             "class": self.classes,
    18|             "names": self.names,
    19|             "attrs": self.attrs,
    20|         }
    21|     def visit_Name(self, node):
    22|         self.names.setdefault(type(node.ctx).__name__, set()).add(node.id)
    23|     def visit_Attribute(self, node):
    24|         self.attrs.add(node.attr)
    25|         for child in ast.iter_child_nodes(node):
    26|             self.visit(child)
    27|     def visit_ClassDef(self, node):
    28|         visitor = DefinitionVisitor()
    29|         self.classes[node.name] = visitor.definitions
    30|         for child in ast.iter_child_nodes(node):


# ====================================================================
# FILE: scripts-dev/dump_macaroon.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-18 ---
     1| from __future__ import print_function
     2| import sys
     3| import pymacaroons
     4| if len(sys.argv) == 1:
     5|     sys.stderr.write("usage: %s macaroon [key]\n" % (sys.argv[0],))
     6|     sys.exit(1)
     7| macaroon_string = sys.argv[1]
     8| key = sys.argv[2] if len(sys.argv) > 2 else None
     9| macaroon = pymacaroons.Macaroon.deserialize(macaroon_string)
    10| print(macaroon.inspect())
    11| print("")
    12| verifier = pymacaroons.Verifier()
    13| verifier.satisfy_general(lambda c: True)
    14| try:
    15|     verifier.verify(macaroon, key)
    16|     print("Signature is correct")
    17| except Exception as e:
    18|     print(str(e))


# ====================================================================
# FILE: scripts-dev/federation_client.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-21 ---
     1| from __future__ import print_function
     2| import argparse
     3| import base64
     4| import json
     5| import sys
     6| from typing import Any, Optional
     7| from urllib import parse as urlparse
     8| import nacl.signing
     9| import requests
    10| import signedjson.types
    11| import srvlookup
    12| import yaml
    13| from requests.adapters import HTTPAdapter
    14| def encode_base64(input_bytes):
    15|     """Encode bytes as a base64 string without any padding."""
    16|     input_len = len(input_bytes)
    17|     output_len = 4 * ((input_len + 2) // 3) + (input_len + 2) % 3 - 2
    18|     output_bytes = base64.b64encode(input_bytes)
    19|     output_string = output_bytes[:output_len].decode("ascii")
    20|     return output_string
    21| def decode_base64(input_string):

# --- HUNK 2: Lines 215-237 ---
   215|                 return None
   216|             parsed_well_known = resp.json()
   217|             if not isinstance(parsed_well_known, dict):
   218|                 raise Exception("not a dict")
   219|             if "m.server" not in parsed_well_known:
   220|                 raise Exception("Missing key 'm.server'")
   221|             new_name = parsed_well_known["m.server"]
   222|             print("well-known lookup gave %s" % (new_name,), file=sys.stderr)
   223|             return new_name
   224|         except Exception as e:
   225|             print("Invalid response from %s: %s" % (uri, e), file=sys.stderr)
   226|         return None
   227|     def get_connection(self, url, proxies=None):
   228|         parsed = urlparse.urlparse(url)
   229|         (host, port) = self.lookup(parsed.netloc)
   230|         netloc = "%s:%d" % (host, port)
   231|         print("Connecting to %s" % (netloc,), file=sys.stderr)
   232|         url = urlparse.urlunparse(
   233|             ("https", netloc, parsed.path, parsed.params, parsed.query, parsed.fragment)
   234|         )
   235|         return super(MatrixConnectionAdapter, self).get_connection(url, proxies)
   236| if __name__ == "__main__":
   237|     main()


# ====================================================================
# FILE: scripts-dev/hash_history.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-21 ---
     1| from __future__ import print_function
     2| import sqlite3
     3| import sys
     4| from unpaddedbase64 import decode_base64, encode_base64
     5| from synapse.crypto.event_signing import (
     6|     add_event_pdu_content_hash,
     7|     compute_pdu_event_reference_hash,
     8| )
     9| from synapse.federation.units import Pdu
    10| from synapse.storage._base import SQLBaseStore
    11| from synapse.storage.pdu import PduStore
    12| from synapse.storage.signatures import SignatureStore
    13| class Store:
    14|     _get_pdu_tuples = PduStore.__dict__["_get_pdu_tuples"]
    15|     _get_pdu_content_hashes_txn = SignatureStore.__dict__["_get_pdu_content_hashes_txn"]
    16|     _get_prev_pdu_hashes_txn = SignatureStore.__dict__["_get_prev_pdu_hashes_txn"]
    17|     _get_pdu_origin_signatures_txn = SignatureStore.__dict__[
    18|         "_get_pdu_origin_signatures_txn"
    19|     ]
    20|     _store_pdu_content_hash_txn = SignatureStore.__dict__["_store_pdu_content_hash_txn"]
    21|     _store_pdu_reference_hash_txn = SignatureStore.__dict__[


# ====================================================================
# FILE: scripts/move_remote_media_to_new_store.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-32 ---
     1| """
     2| Moves a list of remote media from one media store to another.
     3| The input should be a list of media files to be moved, one per line. Each line
     4| should be formatted::
     5|     <origin server>|<file id>
     6| This can be extracted from postgres with::
     7|     psql --tuples-only -A -c "select media_origin, filesystem_id from
     8|         matrix.remote_media_cache where ..."
     9| To use, pipe the above into::
    10|     PYTHON_PATH=. ./scripts/move_remote_media_to_new_store.py <source repo> <dest repo>
    11| """
    12| from __future__ import print_function
    13| import argparse
    14| import logging
    15| import os
    16| import shutil
    17| import sys
    18| from synapse.rest.media.v1.filepath import MediaFilePaths
    19| logger = logging.getLogger()
    20| def main(src_repo, dest_repo):
    21|     src_paths = MediaFilePaths(src_repo)
    22|     dest_paths = MediaFilePaths(dest_repo)
    23|     for line in sys.stdin:
    24|         line = line.strip()
    25|         parts = line.split("|")
    26|         if len(parts) != 2:
    27|             print("Unable to parse input line %s" % line, file=sys.stderr)
    28|             exit(1)
    29|         move_media(parts[0], parts[1], src_paths, dest_paths)
    30| def move_media(origin_server, file_id, src_paths, dest_paths):
    31|     """Move the given file, and any thumbnails, to the dest repo
    32|     Args:


# ====================================================================
# FILE: setup.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 17-56 ---
    17|         )
    18| def read_file(path_segments):
    19|     """Read a file from the package. Takes a list of strings to join to
    20|     make the path"""
    21|     file_path = os.path.join(here, *path_segments)
    22|     with open(file_path) as f:
    23|         return f.read()
    24| def exec_file(path_segments):
    25|     """Execute a single python file to get the variables defined in it"""
    26|     result = {}
    27|     code = read_file(path_segments)
    28|     exec(code, result)
    29|     return result
    30| version = exec_file(("synapse", "__init__.py"))["__version__"]
    31| dependencies = exec_file(("synapse", "python_dependencies.py"))
    32| long_description = read_file(("README.rst",))
    33| REQUIREMENTS = dependencies["REQUIREMENTS"]
    34| CONDITIONAL_REQUIREMENTS = dependencies["CONDITIONAL_REQUIREMENTS"]
    35| ALL_OPTIONAL_REQUIREMENTS = dependencies["ALL_OPTIONAL_REQUIREMENTS"]
    36| CONDITIONAL_REQUIREMENTS["all"] = list(ALL_OPTIONAL_REQUIREMENTS)
    37| setup(
    38|     name="matrix-synapse",
    39|     version=version,
    40|     packages=find_packages(exclude=["tests", "tests.*"]),
    41|     description="Reference homeserver for the Matrix decentralised comms protocol",
    42|     install_requires=REQUIREMENTS,
    43|     extras_require=CONDITIONAL_REQUIREMENTS,
    44|     include_package_data=True,
    45|     zip_safe=False,
    46|     long_description=long_description,
    47|     python_requires="~=3.5",
    48|     classifiers=[
    49|         "Development Status :: 5 - Production/Stable",
    50|         "Topic :: Communications :: Chat",
    51|         "License :: OSI Approved :: Apache Software License",
    52|         "Programming Language :: Python :: 3 :: Only",
    53|         "Programming Language :: Python :: 3.5",
    54|         "Programming Language :: Python :: 3.6",
    55|         "Programming Language :: Python :: 3.7",
    56|         "Programming Language :: Python :: 3.8",


# ====================================================================
# FILE: synapse/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 3-26 ---
     3| import json
     4| import os
     5| import sys
     6| if sys.version_info < (3, 5):
     7|     print("Synapse requires Python 3.5 or above.")
     8|     sys.exit(1)
     9| try:
    10|     from twisted.internet import protocol
    11|     from twisted.internet.protocol import Factory
    12|     from twisted.names.dns import DNSDatagramProtocol
    13|     protocol.Factory.noisy = False
    14|     Factory.noisy = False
    15|     DNSDatagramProtocol.noisy = False
    16| except ImportError:
    17|     pass
    18| try:
    19|     from canonicaljson import set_json_library
    20|     set_json_library(json)
    21| except ImportError:
    22|     pass
    23| __version__ = "1.20.1"
    24| if bool(os.environ.get("SYNAPSE_TEST_PATCH_LOG_CONTEXTS", False)):
    25|     from synapse.util.patch_inline_callbacks import do_patch
    26|     do_patch()


# ====================================================================
# FILE: synapse/_scripts/register_new_matrix_user.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-21 ---
     1| from __future__ import print_function
     2| import argparse
     3| import getpass
     4| import hashlib
     5| import hmac
     6| import logging
     7| import sys
     8| import requests as _requests
     9| import yaml
    10| def request_registration(
    11|     user,
    12|     password,
    13|     server_location,
    14|     shared_secret,
    15|     admin=False,
    16|     user_type=None,
    17|     requests=_requests,
    18|     _print=print,
    19|     exit=sys.exit,
    20| ):
    21|     url = "%s/_matrix/client/r0/admin/register" % (server_location,)


# ====================================================================
# FILE: synapse/api/auth.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 144-188 ---
   144|                 opentracing.set_tag("authenticated_entity", user_id)
   145|                 opentracing.set_tag("appservice_id", app_service.id)
   146|                 if ip_addr and self._track_appservice_user_ips:
   147|                     await self.store.insert_client_ip(
   148|                         user_id=user_id,
   149|                         access_token=access_token,
   150|                         ip=ip_addr,
   151|                         user_agent=user_agent,
   152|                         device_id="dummy-device",  # stubbed
   153|                     )
   154|                 return synapse.types.create_requester(user_id, app_service=app_service)
   155|             user_info = await self.get_user_by_access_token(
   156|                 access_token, rights, allow_expired=allow_expired
   157|             )
   158|             user = user_info["user"]
   159|             token_id = user_info["token_id"]
   160|             is_guest = user_info["is_guest"]
   161|             shadow_banned = user_info["shadow_banned"]
   162|             if self._account_validity.enabled and not allow_expired:
   163|                 user_id = user.to_string()
   164|                 expiration_ts = await self.store.get_expiration_ts_for_user(user_id)
   165|                 if (
   166|                     expiration_ts is not None
   167|                     and self.clock.time_msec() >= expiration_ts
   168|                 ):
   169|                     raise AuthError(
   170|                         403, "User account has expired", errcode=Codes.EXPIRED_ACCOUNT
   171|                     )
   172|             device_id = user_info.get("device_id")
   173|             if user and access_token and ip_addr:
   174|                 await self.store.insert_client_ip(
   175|                     user_id=user.to_string(),
   176|                     access_token=access_token,
   177|                     ip=ip_addr,
   178|                     user_agent=user_agent,
   179|                     device_id=device_id,
   180|                 )
   181|             if is_guest and not allow_guest:
   182|                 raise AuthError(
   183|                     403,
   184|                     "Guest access not allowed",
   185|                     errcode=Codes.GUEST_ACCESS_FORBIDDEN,
   186|                 )
   187|             request.authenticated_entity = user.to_string()
   188|             opentracing.set_tag("authenticated_entity", user.to_string())


# ====================================================================
# FILE: synapse/api/errors.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 43-373 ---
    43|     INCOMPATIBLE_ROOM_VERSION = "M_INCOMPATIBLE_ROOM_VERSION"
    44|     WRONG_ROOM_KEYS_VERSION = "M_WRONG_ROOM_KEYS_VERSION"
    45|     EXPIRED_ACCOUNT = "ORG_MATRIX_EXPIRED_ACCOUNT"
    46|     PASSWORD_TOO_SHORT = "M_PASSWORD_TOO_SHORT"
    47|     PASSWORD_NO_DIGIT = "M_PASSWORD_NO_DIGIT"
    48|     PASSWORD_NO_UPPERCASE = "M_PASSWORD_NO_UPPERCASE"
    49|     PASSWORD_NO_LOWERCASE = "M_PASSWORD_NO_LOWERCASE"
    50|     PASSWORD_NO_SYMBOL = "M_PASSWORD_NO_SYMBOL"
    51|     PASSWORD_IN_DICTIONARY = "M_PASSWORD_IN_DICTIONARY"
    52|     WEAK_PASSWORD = "M_WEAK_PASSWORD"
    53|     INVALID_SIGNATURE = "M_INVALID_SIGNATURE"
    54|     USER_DEACTIVATED = "M_USER_DEACTIVATED"
    55|     BAD_ALIAS = "M_BAD_ALIAS"
    56| class CodeMessageException(RuntimeError):
    57|     """An exception with integer code and message string attributes.
    58|     Attributes:
    59|         code: HTTP error code
    60|         msg: string describing the error
    61|     """
    62|     def __init__(self, code: Union[int, HTTPStatus], msg: str):
    63|         super(CodeMessageException, self).__init__("%d: %s" % (code, msg))
    64|         self.code = int(code)
    65|         self.msg = msg
    66| class RedirectException(CodeMessageException):
    67|     """A pseudo-error indicating that we want to redirect the client to a different
    68|     location
    69|     Attributes:
    70|         cookies: a list of set-cookies values to add to the response. For example:
    71|            b"sessionId=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT"
    72|     """
    73|     def __init__(self, location: bytes, http_code: int = http.FOUND):
    74|         """
    75|         Args:
    76|             location: the URI to redirect to
    77|             http_code: the HTTP response code
    78|         """
    79|         msg = "Redirect to %s" % (location.decode("utf-8"),)
    80|         super().__init__(code=http_code, msg=msg)
    81|         self.location = location
    82|         self.cookies = []  # type: List[bytes]
    83| class SynapseError(CodeMessageException):
    84|     """A base exception type for matrix errors which have an errcode and error
    85|     message (as well as an HTTP status code).
    86|     Attributes:
    87|         errcode: Matrix error code e.g 'M_FORBIDDEN'
    88|     """
    89|     def __init__(self, code: int, msg: str, errcode: str = Codes.UNKNOWN):
    90|         """Constructs a synapse error.
    91|         Args:
    92|             code: The integer error code (an HTTP response code)
    93|             msg: The human-readable error message.
    94|             errcode: The matrix error code e.g 'M_FORBIDDEN'
    95|         """
    96|         super(SynapseError, self).__init__(code, msg)
    97|         self.errcode = errcode
    98|     def error_dict(self):
    99|         return cs_error(self.msg, self.errcode)
   100| class ProxiedRequestError(SynapseError):
   101|     """An error from a general matrix endpoint, eg. from a proxied Matrix API call.
   102|     Attributes:
   103|         errcode: Matrix error code e.g 'M_FORBIDDEN'
   104|     """
   105|     def __init__(
   106|         self,
   107|         code: int,
   108|         msg: str,
   109|         errcode: str = Codes.UNKNOWN,
   110|         additional_fields: Optional[Dict] = None,
   111|     ):
   112|         super(ProxiedRequestError, self).__init__(code, msg, errcode)
   113|         if additional_fields is None:
   114|             self._additional_fields = {}  # type: Dict
   115|         else:
   116|             self._additional_fields = dict(additional_fields)
   117|     def error_dict(self):
   118|         return cs_error(self.msg, self.errcode, **self._additional_fields)
   119| class ConsentNotGivenError(SynapseError):
   120|     """The error returned to the client when the user has not consented to the
   121|     privacy policy.
   122|     """
   123|     def __init__(self, msg: str, consent_uri: str):
   124|         """Constructs a ConsentNotGivenError
   125|         Args:
   126|             msg: The human-readable error message
   127|             consent_url: The URL where the user can give their consent
   128|         """
   129|         super(ConsentNotGivenError, self).__init__(
   130|             code=HTTPStatus.FORBIDDEN, msg=msg, errcode=Codes.CONSENT_NOT_GIVEN
   131|         )
   132|         self._consent_uri = consent_uri
   133|     def error_dict(self):
   134|         return cs_error(self.msg, self.errcode, consent_uri=self._consent_uri)
   135| class UserDeactivatedError(SynapseError):
   136|     """The error returned to the client when the user attempted to access an
   137|     authenticated endpoint, but the account has been deactivated.
   138|     """
   139|     def __init__(self, msg: str):
   140|         """Constructs a UserDeactivatedError
   141|         Args:
   142|             msg: The human-readable error message
   143|         """
   144|         super(UserDeactivatedError, self).__init__(
   145|             code=HTTPStatus.FORBIDDEN, msg=msg, errcode=Codes.USER_DEACTIVATED
   146|         )
   147| class FederationDeniedError(SynapseError):
   148|     """An error raised when the server tries to federate with a server which
   149|     is not on its federation whitelist.
   150|     Attributes:
   151|         destination: The destination which has been denied
   152|     """
   153|     def __init__(self, destination: Optional[str]):
   154|         """Raised by federation client or server to indicate that we are
   155|         are deliberately not attempting to contact a given server because it is
   156|         not on our federation whitelist.
   157|         Args:
   158|             destination: the domain in question
   159|         """
   160|         self.destination = destination
   161|         super(FederationDeniedError, self).__init__(
   162|             code=403,
   163|             msg="Federation denied with %s." % (self.destination,),
   164|             errcode=Codes.FORBIDDEN,
   165|         )
   166| class InteractiveAuthIncompleteError(Exception):
   167|     """An error raised when UI auth is not yet complete
   168|     (This indicates we should return a 401 with 'result' as the body)
   169|     Attributes:
   170|         session_id: The ID of the ongoing interactive auth session.
   171|         result: the server response to the request, which should be
   172|             passed back to the client
   173|     """
   174|     def __init__(self, session_id: str, result: "JsonDict"):
   175|         super(InteractiveAuthIncompleteError, self).__init__(
   176|             "Interactive auth not yet complete"
   177|         )
   178|         self.session_id = session_id
   179|         self.result = result
   180| class UnrecognizedRequestError(SynapseError):
   181|     """An error indicating we don't understand the request you're trying to make"""
   182|     def __init__(self, *args, **kwargs):
   183|         if "errcode" not in kwargs:
   184|             kwargs["errcode"] = Codes.UNRECOGNIZED
   185|         if len(args) == 0:
   186|             message = "Unrecognized request"
   187|         else:
   188|             message = args[0]
   189|         super(UnrecognizedRequestError, self).__init__(400, message, **kwargs)
   190| class NotFoundError(SynapseError):
   191|     """An error indicating we can't find the thing you asked for"""
   192|     def __init__(self, msg: str = "Not found", errcode: str = Codes.NOT_FOUND):
   193|         super(NotFoundError, self).__init__(404, msg, errcode=errcode)
   194| class AuthError(SynapseError):
   195|     """An error raised when there was a problem authorising an event, and at various
   196|     other poorly-defined times.
   197|     """
   198|     def __init__(self, *args, **kwargs):
   199|         if "errcode" not in kwargs:
   200|             kwargs["errcode"] = Codes.FORBIDDEN
   201|         super(AuthError, self).__init__(*args, **kwargs)
   202| class InvalidClientCredentialsError(SynapseError):
   203|     """An error raised when there was a problem with the authorisation credentials
   204|     in a client request.
   205|     https://matrix.org/docs/spec/client_server/r0.5.0#using-access-tokens:
   206|     When credentials are required but missing or invalid, the HTTP call will
   207|     return with a status of 401 and the error code, M_MISSING_TOKEN or
   208|     M_UNKNOWN_TOKEN respectively.
   209|     """
   210|     def __init__(self, msg: str, errcode: str):
   211|         super().__init__(code=401, msg=msg, errcode=errcode)
   212| class MissingClientTokenError(InvalidClientCredentialsError):
   213|     """Raised when we couldn't find the access token in a request"""
   214|     def __init__(self, msg: str = "Missing access token"):
   215|         super().__init__(msg=msg, errcode="M_MISSING_TOKEN")
   216| class InvalidClientTokenError(InvalidClientCredentialsError):
   217|     """Raised when we didn't understand the access token in a request"""
   218|     def __init__(
   219|         self, msg: str = "Unrecognised access token", soft_logout: bool = False
   220|     ):
   221|         super().__init__(msg=msg, errcode="M_UNKNOWN_TOKEN")
   222|         self._soft_logout = soft_logout
   223|     def error_dict(self):
   224|         d = super().error_dict()
   225|         d["soft_logout"] = self._soft_logout
   226|         return d
   227| class ResourceLimitError(SynapseError):
   228|     """
   229|     Any error raised when there is a problem with resource usage.
   230|     For instance, the monthly active user limit for the server has been exceeded
   231|     """
   232|     def __init__(
   233|         self,
   234|         code: int,
   235|         msg: str,
   236|         errcode: str = Codes.RESOURCE_LIMIT_EXCEEDED,
   237|         admin_contact: Optional[str] = None,
   238|         limit_type: Optional[str] = None,
   239|     ):
   240|         self.admin_contact = admin_contact
   241|         self.limit_type = limit_type
   242|         super(ResourceLimitError, self).__init__(code, msg, errcode=errcode)
   243|     def error_dict(self):
   244|         return cs_error(
   245|             self.msg,
   246|             self.errcode,
   247|             admin_contact=self.admin_contact,
   248|             limit_type=self.limit_type,
   249|         )
   250| class EventSizeError(SynapseError):
   251|     """An error raised when an event is too big."""
   252|     def __init__(self, *args, **kwargs):
   253|         if "errcode" not in kwargs:
   254|             kwargs["errcode"] = Codes.TOO_LARGE
   255|         super(EventSizeError, self).__init__(413, *args, **kwargs)
   256| class EventStreamError(SynapseError):
   257|     """An error raised when there a problem with the event stream."""
   258|     def __init__(self, *args, **kwargs):
   259|         if "errcode" not in kwargs:
   260|             kwargs["errcode"] = Codes.BAD_PAGINATION
   261|         super(EventStreamError, self).__init__(*args, **kwargs)
   262| class LoginError(SynapseError):
   263|     """An error raised when there was a problem logging in."""
   264|     pass
   265| class StoreError(SynapseError):
   266|     """An error raised when there was a problem storing some data."""
   267|     pass
   268| class InvalidCaptchaError(SynapseError):
   269|     def __init__(
   270|         self,
   271|         code: int = 400,
   272|         msg: str = "Invalid captcha.",
   273|         error_url: Optional[str] = None,
   274|         errcode: str = Codes.CAPTCHA_INVALID,
   275|     ):
   276|         super(InvalidCaptchaError, self).__init__(code, msg, errcode)
   277|         self.error_url = error_url
   278|     def error_dict(self):
   279|         return cs_error(self.msg, self.errcode, error_url=self.error_url)
   280| class LimitExceededError(SynapseError):
   281|     """A client has sent too many requests and is being throttled.
   282|     """
   283|     def __init__(
   284|         self,
   285|         code: int = 429,
   286|         msg: str = "Too Many Requests",
   287|         retry_after_ms: Optional[int] = None,
   288|         errcode: str = Codes.LIMIT_EXCEEDED,
   289|     ):
   290|         super(LimitExceededError, self).__init__(code, msg, errcode)
   291|         self.retry_after_ms = retry_after_ms
   292|     def error_dict(self):
   293|         return cs_error(self.msg, self.errcode, retry_after_ms=self.retry_after_ms)
   294| class RoomKeysVersionError(SynapseError):
   295|     """A client has tried to upload to a non-current version of the room_keys store
   296|     """
   297|     def __init__(self, current_version: str):
   298|         """
   299|         Args:
   300|             current_version: the current version of the store they should have used
   301|         """
   302|         super(RoomKeysVersionError, self).__init__(
   303|             403, "Wrong room_keys version", Codes.WRONG_ROOM_KEYS_VERSION
   304|         )
   305|         self.current_version = current_version
   306| class UnsupportedRoomVersionError(SynapseError):
   307|     """The client's request to create a room used a room version that the server does
   308|     not support."""
   309|     def __init__(self, msg: str = "Homeserver does not support this room version"):
   310|         super(UnsupportedRoomVersionError, self).__init__(
   311|             code=400, msg=msg, errcode=Codes.UNSUPPORTED_ROOM_VERSION,
   312|         )
   313| class ThreepidValidationError(SynapseError):
   314|     """An error raised when there was a problem authorising an event."""
   315|     def __init__(self, *args, **kwargs):
   316|         if "errcode" not in kwargs:
   317|             kwargs["errcode"] = Codes.FORBIDDEN
   318|         super(ThreepidValidationError, self).__init__(*args, **kwargs)
   319| class IncompatibleRoomVersionError(SynapseError):
   320|     """A server is trying to join a room whose version it does not support.
   321|     Unlike UnsupportedRoomVersionError, it is specific to the case of the make_join
   322|     failing.
   323|     """
   324|     def __init__(self, room_version: str):
   325|         super(IncompatibleRoomVersionError, self).__init__(
   326|             code=400,
   327|             msg="Your homeserver does not support the features required to "
   328|             "join this room",
   329|             errcode=Codes.INCOMPATIBLE_ROOM_VERSION,
   330|         )
   331|         self._room_version = room_version
   332|     def error_dict(self):
   333|         return cs_error(self.msg, self.errcode, room_version=self._room_version)
   334| class PasswordRefusedError(SynapseError):
   335|     """A password has been refused, either during password reset/change or registration.
   336|     """
   337|     def __init__(
   338|         self,
   339|         msg: str = "This password doesn't comply with the server's policy",
   340|         errcode: str = Codes.WEAK_PASSWORD,
   341|     ):
   342|         super(PasswordRefusedError, self).__init__(
   343|             code=400, msg=msg, errcode=errcode,
   344|         )
   345| class RequestSendFailed(RuntimeError):
   346|     """Sending a HTTP request over federation failed due to not being able to
   347|     talk to the remote server for some reason.
   348|     This exception is used to differentiate "expected" errors that arise due to
   349|     networking (e.g. DNS failures, connection timeouts etc), versus unexpected
   350|     errors (like programming errors).
   351|     """
   352|     def __init__(self, inner_exception, can_retry):
   353|         super(RequestSendFailed, self).__init__(
   354|             "Failed to send request: %s: %s"
   355|             % (type(inner_exception).__name__, inner_exception)
   356|         )
   357|         self.inner_exception = inner_exception
   358|         self.can_retry = can_retry
   359| def cs_error(msg: str, code: str = Codes.UNKNOWN, **kwargs):
   360|     """ Utility method for constructing an error response for client-server
   361|     interactions.
   362|     Args:
   363|         msg: The error message.
   364|         code: The error code.
   365|         kwargs: Additional keys to add to the response.
   366|     Returns:
   367|         A dict representing the error response JSON.
   368|     """
   369|     err = {"error": msg, "errcode": code}
   370|     for key, value in kwargs.items():
   371|         err[key] = value
   372|     return err
   373| class FederationError(RuntimeError):

# --- HUNK 2: Lines 379-441 ---
   379|         check (e.g. auth)
   380|     WARN: The remote server accepted the event, but believes some part of it
   381|         is wrong (e.g., it referred to an invalid event)
   382|     """
   383|     def __init__(
   384|         self,
   385|         level: str,
   386|         code: int,
   387|         reason: str,
   388|         affected: str,
   389|         source: Optional[str] = None,
   390|     ):
   391|         if level not in ["FATAL", "ERROR", "WARN"]:
   392|             raise ValueError("Level is not valid: %s" % (level,))
   393|         self.level = level
   394|         self.code = code
   395|         self.reason = reason
   396|         self.affected = affected
   397|         self.source = source
   398|         msg = "%s %s: %s" % (level, code, reason)
   399|         super(FederationError, self).__init__(msg)
   400|     def get_dict(self):
   401|         return {
   402|             "level": self.level,
   403|             "code": self.code,
   404|             "reason": self.reason,
   405|             "affected": self.affected,
   406|             "source": self.source if self.source else self.affected,
   407|         }
   408| class HttpResponseException(CodeMessageException):
   409|     """
   410|     Represents an HTTP-level failure of an outbound request
   411|     Attributes:
   412|         response: body of response
   413|     """
   414|     def __init__(self, code: int, msg: str, response: bytes):
   415|         """
   416|         Args:
   417|             code: HTTP status code
   418|             msg: reason phrase from HTTP response status line
   419|             response: body of response
   420|         """
   421|         super(HttpResponseException, self).__init__(code, msg)
   422|         self.response = response
   423|     def to_synapse_error(self):
   424|         """Make a SynapseError based on an HTTPResponseException
   425|         This is useful when a proxied request has failed, and we need to
   426|         decide how to map the failure onto a matrix error to send back to the
   427|         client.
   428|         An attempt is made to parse the body of the http response as a matrix
   429|         error. If that succeeds, the errcode and error message from the body
   430|         are used as the errcode and error message in the new synapse error.
   431|         Otherwise, the errcode is set to M_UNKNOWN, and the error message is
   432|         set to the reason code from the HTTP response.
   433|         Returns:
   434|             SynapseError:
   435|         """
   436|         try:
   437|             j = json_decoder.decode(self.response.decode("utf-8"))
   438|         except ValueError:
   439|             j = {}
   440|         if not isinstance(j, dict):
   441|             j = {}


# ====================================================================
# FILE: synapse/api/filtering.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-23 ---
     1| from typing import List
     2| import jsonschema
     3| from canonicaljson import json
     4| from jsonschema import FormatChecker
     5| from synapse.api.constants import EventContentFields
     6| from synapse.api.errors import SynapseError
     7| from synapse.api.presence import UserPresenceState
     8| from synapse.types import RoomID, UserID
     9| FILTER_SCHEMA = {
    10|     "additionalProperties": False,
    11|     "type": "object",
    12|     "properties": {
    13|         "limit": {"type": "number"},
    14|         "senders": {"$ref": "#/definitions/user_id_array"},
    15|         "not_senders": {"$ref": "#/definitions/user_id_array"},
    16|         "types": {"type": "array", "items": {"type": "string"}},
    17|         "not_types": {"type": "array", "items": {"type": "string"}},
    18|     },
    19| }
    20| ROOM_FILTER_SCHEMA = {
    21|     "additionalProperties": False,
    22|     "type": "object",
    23|     "properties": {

# --- HUNK 2: Lines 73-113 ---
    73|         "room": {"$ref": "#/definitions/room_filter"},
    74|         "event_format": {"type": "string", "enum": ["client", "federation"]},
    75|         "event_fields": {
    76|             "type": "array",
    77|             "items": {
    78|                 "type": "string",
    79|                 "pattern": r"^((?!\\\\).)*$",
    80|             },
    81|         },
    82|     },
    83|     "additionalProperties": False,
    84| }
    85| @FormatChecker.cls_checks("matrix_room_id")
    86| def matrix_room_id_validator(room_id_str):
    87|     return RoomID.from_string(room_id_str)
    88| @FormatChecker.cls_checks("matrix_user_id")
    89| def matrix_user_id_validator(user_id_str):
    90|     return UserID.from_string(user_id_str)
    91| class Filtering:
    92|     def __init__(self, hs):
    93|         super(Filtering, self).__init__()
    94|         self.store = hs.get_datastore()
    95|     async def get_user_filter(self, user_localpart, filter_id):
    96|         result = await self.store.get_user_filter(user_localpart, filter_id)
    97|         return FilterCollection(result)
    98|     def add_user_filter(self, user_localpart, user_filter):
    99|         self.check_valid_filter(user_filter)
   100|         return self.store.add_user_filter(user_localpart, user_filter)
   101|     def check_valid_filter(self, user_filter_json):
   102|         """Check if the provided filter is valid.
   103|         This inspects all definitions contained within the filter.
   104|         Args:
   105|             user_filter_json(dict): The filter
   106|         Raises:
   107|             SynapseError: If the filter is not valid.
   108|         """
   109|         try:
   110|             jsonschema.validate(
   111|                 user_filter_json, USER_FILTER_SCHEMA, format_checker=FormatChecker()
   112|             )
   113|         except jsonschema.ValidationError as e:


# ====================================================================
# FILE: synapse/api/urls.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| """Contains the URL paths to prefix various aspects of the server with. """
     2| import hmac
     3| from hashlib import sha256
     4| from urllib.parse import urlencode
     5| from synapse.config import ConfigError
     6| CLIENT_API_PREFIX = "/_matrix/client"
     7| FEDERATION_PREFIX = "/_matrix/federation"
     8| FEDERATION_V1_PREFIX = FEDERATION_PREFIX + "/v1"
     9| FEDERATION_V2_PREFIX = FEDERATION_PREFIX + "/v2"
    10| FEDERATION_UNSTABLE_PREFIX = FEDERATION_PREFIX + "/unstable"
    11| STATIC_PREFIX = "/_matrix/static"
    12| WEB_CLIENT_PREFIX = "/_matrix/client"
    13| SERVER_KEY_V2_PREFIX = "/_matrix/key/v2"
    14| MEDIA_PREFIX = "/_matrix/media/r0"
    15| LEGACY_MEDIA_PREFIX = "/_matrix/media/v1"
    16| class ConsentURIBuilder:
    17|     def __init__(self, hs_config):
    18|         """
    19|         Args:
    20|             hs_config (synapse.config.homeserver.HomeServerConfig):
    21|         """
    22|         if hs_config.form_secret is None:
    23|             raise ConfigError("form_secret not set in config")
    24|         if hs_config.public_baseurl is None:
    25|             raise ConfigError("public_baseurl not set in config")


# ====================================================================
# FILE: synapse/app/admin_cmd.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-26 ---
     1| import argparse
     2| import logging
     3| import os
     4| import sys
     5| import tempfile
     6| from canonicaljson import json
     7| from twisted.internet import defer, task
     8| import synapse
     9| from synapse.app import _base
    10| from synapse.config._base import ConfigError
    11| from synapse.config.homeserver import HomeServerConfig
    12| from synapse.config.logger import setup_logging
    13| from synapse.handlers.admin import ExfiltrationWriter
    14| from synapse.replication.slave.storage._base import BaseSlavedStore
    15| from synapse.replication.slave.storage.account_data import SlavedAccountDataStore
    16| from synapse.replication.slave.storage.appservice import SlavedApplicationServiceStore
    17| from synapse.replication.slave.storage.client_ips import SlavedClientIpStore
    18| from synapse.replication.slave.storage.deviceinbox import SlavedDeviceInboxStore
    19| from synapse.replication.slave.storage.devices import SlavedDeviceStore
    20| from synapse.replication.slave.storage.events import SlavedEventStore
    21| from synapse.replication.slave.storage.filtering import SlavedFilteringStore
    22| from synapse.replication.slave.storage.groups import SlavedGroupServerStore
    23| from synapse.replication.slave.storage.presence import SlavedPresenceStore
    24| from synapse.replication.slave.storage.push_rule import SlavedPushRuleStore
    25| from synapse.replication.slave.storage.receipts import SlavedReceiptsStore
    26| from synapse.replication.slave.storage.registration import SlavedRegistrationStore


# ====================================================================
# FILE: synapse/app/generic_worker.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 111-169 ---
   111| from synapse.storage.databases.main.media_repository import MediaRepositoryStore
   112| from synapse.storage.databases.main.monthly_active_users import (
   113|     MonthlyActiveUsersWorkerStore,
   114| )
   115| from synapse.storage.databases.main.presence import UserPresenceState
   116| from synapse.storage.databases.main.search import SearchWorkerStore
   117| from synapse.storage.databases.main.ui_auth import UIAuthWorkerStore
   118| from synapse.storage.databases.main.user_directory import UserDirectoryStore
   119| from synapse.types import ReadReceipt
   120| from synapse.util.async_helpers import Linearizer
   121| from synapse.util.httpresourcetree import create_resource_tree
   122| from synapse.util.manhole import manhole
   123| from synapse.util.versionstring import get_version_string
   124| logger = logging.getLogger("synapse.app.generic_worker")
   125| class PresenceStatusStubServlet(RestServlet):
   126|     """If presence is disabled this servlet can be used to stub out setting
   127|     presence status.
   128|     """
   129|     PATTERNS = client_patterns("/presence/(?P<user_id>[^/]*)/status")
   130|     def __init__(self, hs):
   131|         super(PresenceStatusStubServlet, self).__init__()
   132|         self.auth = hs.get_auth()
   133|     async def on_GET(self, request, user_id):
   134|         await self.auth.get_user_by_req(request)
   135|         return 200, {"presence": "offline"}
   136|     async def on_PUT(self, request, user_id):
   137|         await self.auth.get_user_by_req(request)
   138|         return 200, {}
   139| class KeyUploadServlet(RestServlet):
   140|     """An implementation of the `KeyUploadServlet` that responds to read only
   141|     requests, but otherwise proxies through to the master instance.
   142|     """
   143|     PATTERNS = client_patterns("/keys/upload(/(?P<device_id>[^/]+))?$")
   144|     def __init__(self, hs):
   145|         """
   146|         Args:
   147|             hs (synapse.server.HomeServer): server
   148|         """
   149|         super(KeyUploadServlet, self).__init__()
   150|         self.auth = hs.get_auth()
   151|         self.store = hs.get_datastore()
   152|         self.http_client = hs.get_simple_http_client()
   153|         self.main_uri = hs.config.worker_main_http_uri
   154|     async def on_POST(self, request, device_id):
   155|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   156|         user_id = requester.user.to_string()
   157|         body = parse_json_object_from_request(request)
   158|         if device_id is not None:
   159|             if requester.device_id is not None and device_id != requester.device_id:
   160|                 logger.warning(
   161|                     "Client uploading keys for a different device "
   162|                     "(logged in as %s, uploading for %s)",
   163|                     requester.device_id,
   164|                     device_id,
   165|                 )
   166|         else:
   167|             device_id = requester.device_id
   168|         if device_id is None:
   169|             raise SynapseError(

# --- HUNK 2: Lines 486-526 ---
   486|                         (
   487|                             "Metrics listener configured, but "
   488|                             "enable_metrics is not True!"
   489|                         )
   490|                     )
   491|                 else:
   492|                     _base.listen_metrics(listener.bind_addresses, listener.port)
   493|             else:
   494|                 logger.warning("Unsupported listener type: %s", listener.type)
   495|         self.get_tcp_replication().start_replication(self)
   496|     async def remove_pusher(self, app_id, push_key, user_id):
   497|         self.get_tcp_replication().send_remove_pusher(app_id, push_key, user_id)
   498|     @cache_in_self
   499|     def get_replication_data_handler(self):
   500|         return GenericWorkerReplicationHandler(self)
   501|     @cache_in_self
   502|     def get_presence_handler(self):
   503|         return GenericWorkerPresence(self)
   504| class GenericWorkerReplicationHandler(ReplicationDataHandler):
   505|     def __init__(self, hs):
   506|         super(GenericWorkerReplicationHandler, self).__init__(hs)
   507|         self.store = hs.get_datastore()
   508|         self.presence_handler = hs.get_presence_handler()  # type: GenericWorkerPresence
   509|         self.notifier = hs.get_notifier()
   510|         self.notify_pushers = hs.config.start_pushers
   511|         self.pusher_pool = hs.get_pusherpool()
   512|         self.send_handler = None  # type: Optional[FederationSenderHandler]
   513|         if hs.config.send_federation:
   514|             self.send_handler = FederationSenderHandler(hs)
   515|     async def on_rdata(self, stream_name, instance_name, token, rows):
   516|         await super().on_rdata(stream_name, instance_name, token, rows)
   517|         await self._process_and_notify(stream_name, instance_name, token, rows)
   518|     async def _process_and_notify(self, stream_name, instance_name, token, rows):
   519|         try:
   520|             if self.send_handler:
   521|                 await self.send_handler.process_replication_rows(
   522|                     stream_name, token, rows
   523|                 )
   524|             if stream_name == PushRulesStream.NAME:
   525|                 self.notifier.on_new_event(
   526|                     "push_rules_key", token, users=[row.user_id for row in rows]


# ====================================================================
# FILE: synapse/app/homeserver.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-49 ---
     1| from __future__ import print_function
     2| import gc
     3| import logging
     4| import math
     5| import os
     6| import resource
     7| import sys
     8| from typing import Iterable
     9| from prometheus_client import Gauge
    10| from twisted.application import service
    11| from twisted.internet import defer, reactor
    12| from twisted.python.failure import Failure
    13| from twisted.web.resource import EncodingResourceWrapper, IResource
    14| from twisted.web.server import GzipEncoderFactory
    15| from twisted.web.static import File
    16| import synapse
    17| import synapse.config.logger
    18| from synapse import events
    19| from synapse.api.urls import (
    20|     FEDERATION_PREFIX,
    21|     LEGACY_MEDIA_PREFIX,
    22|     MEDIA_PREFIX,
    23|     SERVER_KEY_V2_PREFIX,
    24|     STATIC_PREFIX,
    25|     WEB_CLIENT_PREFIX,
    26| )
    27| from synapse.app import _base
    28| from synapse.app._base import listen_ssl, listen_tcp, quit_with_error
    29| from synapse.config._base import ConfigError
    30| from synapse.config.homeserver import HomeServerConfig
    31| from synapse.config.server import ListenerConfig
    32| from synapse.federation.transport.server import TransportLayerServer
    33| from synapse.http.additional_resource import AdditionalResource
    34| from synapse.http.server import (
    35|     OptionsResource,
    36|     RootOptionsRedirectResource,
    37|     RootRedirect,
    38|     StaticResource,
    39| )
    40| from synapse.http.site import SynapseSite
    41| from synapse.logging.context import LoggingContext
    42| from synapse.metrics import METRICS_PREFIX, MetricsResource, RegistryProxy
    43| from synapse.metrics.background_process_metrics import run_as_background_process
    44| from synapse.module_api import ModuleApi
    45| from synapse.python_dependencies import check_requirements
    46| from synapse.replication.http import REPLICATION_PREFIX, ReplicationRestResource
    47| from synapse.replication.tcp.resource import ReplicationStreamProtocolFactory
    48| from synapse.rest import ClientRestResource
    49| from synapse.rest.admin import AdminRestResource

# --- HUNK 2: Lines 144-183 ---
   144|             client_resource = ClientRestResource(self)
   145|             if compress:
   146|                 client_resource = gz_wrap(client_resource)
   147|             resources.update(
   148|                 {
   149|                     "/_matrix/client/api/v1": client_resource,
   150|                     "/_matrix/client/r0": client_resource,
   151|                     "/_matrix/client/unstable": client_resource,
   152|                     "/_matrix/client/v2_alpha": client_resource,
   153|                     "/_matrix/client/versions": client_resource,
   154|                     "/.well-known/matrix/client": WellKnownResource(self),
   155|                     "/_synapse/admin": AdminRestResource(self),
   156|                 }
   157|             )
   158|             if self.get_config().oidc_enabled:
   159|                 from synapse.rest.oidc import OIDCResource
   160|                 resources["/_synapse/oidc"] = OIDCResource(self)
   161|             if self.get_config().saml2_enabled:
   162|                 from synapse.rest.saml2 import SAML2Resource
   163|                 resources["/_matrix/saml2"] = SAML2Resource(self)
   164|         if name == "consent":
   165|             from synapse.rest.consent.consent_resource import ConsentResource
   166|             consent_resource = ConsentResource(self)
   167|             if compress:
   168|                 consent_resource = gz_wrap(consent_resource)
   169|             resources.update({"/_matrix/consent": consent_resource})
   170|         if name == "federation":
   171|             resources.update({FEDERATION_PREFIX: TransportLayerServer(self)})
   172|         if name == "openid":
   173|             resources.update(
   174|                 {
   175|                     FEDERATION_PREFIX: TransportLayerServer(
   176|                         self, servlet_groups=["openid"]
   177|                     )
   178|                 }
   179|             )
   180|         if name in ["static", "client"]:
   181|             resources.update(
   182|                 {
   183|                     STATIC_PREFIX: StaticResource(


# ====================================================================
# FILE: synapse/appservice/api.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 37-77 ---
    37|         return False
    38|     for k in (field, "protocol"):
    39|         if k not in r:
    40|             return False
    41|         if not isinstance(r[k], str):
    42|             return False
    43|     if "fields" not in r:
    44|         return False
    45|     fields = r["fields"]
    46|     if not isinstance(fields, dict):
    47|         return False
    48|     for k in fields.keys():
    49|         if not isinstance(fields[k], str):
    50|             return False
    51|     return True
    52| class ApplicationServiceApi(SimpleHttpClient):
    53|     """This class manages HS -> AS communications, including querying and
    54|     pushing.
    55|     """
    56|     def __init__(self, hs):
    57|         super(ApplicationServiceApi, self).__init__(hs)
    58|         self.clock = hs.get_clock()
    59|         self.protocol_meta_cache = ResponseCache(
    60|             hs, "as_protocol_meta", timeout_ms=HOUR_IN_MS
    61|         )
    62|     async def query_user(self, service, user_id):
    63|         if service.url is None:
    64|             return False
    65|         uri = service.url + ("/users/%s" % urllib.parse.quote(user_id))
    66|         try:
    67|             response = await self.get_json(uri, {"access_token": service.hs_token})
    68|             if response is not None:  # just an empty json object
    69|                 return True
    70|         except CodeMessageException as e:
    71|             if e.code == 404:
    72|                 return False
    73|             logger.warning("query_user to %s received %s", uri, e.code)
    74|         except Exception as ex:
    75|             logger.warning("query_user to %s threw exception %s", uri, ex)
    76|         return False
    77|     async def query_alias(self, service, alias):

# --- HUNK 2: Lines 118-158 ---
   118|                 else:
   119|                     logger.warning(
   120|                         "query_3pe to %s returned an invalid result %r", uri, r
   121|                     )
   122|             return ret
   123|         except Exception as ex:
   124|             logger.warning("query_3pe to %s threw exception %s", uri, ex)
   125|             return []
   126|     async def get_3pe_protocol(
   127|         self, service: "ApplicationService", protocol: str
   128|     ) -> Optional[JsonDict]:
   129|         if service.url is None:
   130|             return {}
   131|         async def _get() -> Optional[JsonDict]:
   132|             uri = "%s%s/thirdparty/protocol/%s" % (
   133|                 service.url,
   134|                 APP_SERVICE_PREFIX,
   135|                 urllib.parse.quote(protocol),
   136|             )
   137|             try:
   138|                 info = await self.get_json(uri, {})
   139|                 if not _is_valid_3pe_metadata(info):
   140|                     logger.warning(
   141|                         "query_3pe_protocol to %s did not return a valid result", uri
   142|                     )
   143|                     return None
   144|                 for instance in info.get("instances", []):
   145|                     network_id = instance.get("network_id", None)
   146|                     if network_id is not None:
   147|                         instance["instance_id"] = ThirdPartyInstanceID(
   148|                             service.id, network_id
   149|                         ).to_string()
   150|                 return info
   151|             except Exception as ex:
   152|                 logger.warning("query_3pe_protocol to %s threw exception %s", uri, ex)
   153|                 return None
   154|         key = (service.id, protocol)
   155|         return await self.protocol_meta_cache.wrap(key, _get)
   156|     async def push_bulk(self, service, events, txn_id=None):
   157|         if service.url is None:
   158|             return True


# ====================================================================
# FILE: synapse/config/_base.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 150-195 ---
   150|             custom_template_directory: A directory to try to look for the templates
   151|                 before using the default Synapse template directory instead.
   152|             autoescape: Whether to autoescape variables before inserting them into the
   153|                 template.
   154|         Raises:
   155|             ConfigError: if the file's path is incorrect or otherwise cannot be read.
   156|         Returns:
   157|             A list of jinja2 templates.
   158|         """
   159|         templates = []
   160|         search_directories = [self.default_template_dir]
   161|         if custom_template_directory:
   162|             if not self.path_exists(custom_template_directory):
   163|                 raise ConfigError(
   164|                     "Configured template directory does not exist: %s"
   165|                     % (custom_template_directory,)
   166|                 )
   167|             search_directories.insert(0, custom_template_directory)
   168|         loader = jinja2.FileSystemLoader(search_directories)
   169|         env = jinja2.Environment(loader=loader, autoescape=autoescape)
   170|         env.filters.update(
   171|             {
   172|                 "format_ts": _format_ts_filter,
   173|                 "mxc_to_http": _create_mxc_to_http_filter(self.public_baseurl),
   174|             }
   175|         )
   176|         for filename in filenames:
   177|             template = env.get_template(filename)
   178|             templates.append(template)
   179|         return templates
   180| def _format_ts_filter(value: int, format: str):
   181|     return time.strftime(format, time.localtime(value / 1000))
   182| def _create_mxc_to_http_filter(public_baseurl: str) -> Callable:
   183|     """Create and return a jinja2 filter that converts MXC urls to HTTP
   184|     Args:
   185|         public_baseurl: The public, accessible base URL of the homeserver
   186|     """
   187|     def mxc_to_http_filter(value, width, height, resize_method="crop"):
   188|         if value[0:6] != "mxc://":
   189|             return ""
   190|         server_and_media_id = value[6:]
   191|         fragment = None
   192|         if "#" in server_and_media_id:
   193|             server_and_media_id, fragment = server_and_media_id.split("#", 1)
   194|             fragment = "#" + fragment
   195|         params = {"width": width, "height": height, "method": resize_method}

# --- HUNK 2: Lines 626-650 ---
   626|                         continue
   627|                     files.append(entry_path)
   628|                 config_files.extend(sorted(files))
   629|             else:
   630|                 config_files.append(config_path)
   631|     return config_files
   632| @attr.s
   633| class ShardedWorkerHandlingConfig:
   634|     """Algorithm for choosing which instance is responsible for handling some
   635|     sharded work.
   636|     For example, the federation senders use this to determine which instances
   637|     handles sending stuff to a given destination (which is used as the `key`
   638|     below).
   639|     """
   640|     instances = attr.ib(type=List[str])
   641|     def should_handle(self, instance_name: str, key: str) -> bool:
   642|         """Whether this instance is responsible for handling the given key.
   643|         """
   644|         if not self.instances or len(self.instances) == 1:
   645|             return True
   646|         dest_hash = sha256(key.encode("utf8")).digest()
   647|         dest_int = int.from_bytes(dest_hash, byteorder="little")
   648|         remainder = dest_int % (len(self.instances))
   649|         return self.instances[remainder] == instance_name
   650| __all__ = ["Config", "RootConfig", "ShardedWorkerHandlingConfig"]


# ====================================================================
# FILE: synapse/config/_util.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| from typing import Any, List
     2| import jsonschema
     3| from synapse.config._base import ConfigError
     4| from synapse.types import JsonDict
     5| def validate_config(json_schema: JsonDict, config: Any, config_path: List[str]) -> None:
     6|     """Validates a config setting against a JsonSchema definition
     7|     This can be used to validate a section of the config file against a schema
     8|     definition. If the validation fails, a ConfigError is raised with a textual
     9|     description of the problem.
    10|     Args:
    11|         json_schema: the schema to validate against
    12|         config: the configuration value to be validated
    13|         config_path: the path within the config file. This will be used as a basis
    14|            for the error message.
    15|     """
    16|     try:
    17|         jsonschema.validate(config, json_schema)
    18|     except jsonschema.ValidationError as e:
    19|         path = list(config_path)
    20|         for p in list(e.path):
    21|             if isinstance(p, int):
    22|                 path.append("<item %i>" % p)
    23|             else:
    24|                 path.append(str(p))
    25|         raise ConfigError(


# ====================================================================
# FILE: synapse/config/captcha.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-16 ---
     1| from ._base import Config
     2| class CaptchaConfig(Config):
     3|     section = "captcha"
     4|     def read_config(self, config, **kwargs):
     5|         self.recaptcha_private_key = config.get("recaptcha_private_key")
     6|         self.recaptcha_public_key = config.get("recaptcha_public_key")
     7|         self.enable_registration_captcha = config.get(
     8|             "enable_registration_captcha", False
     9|         )
    10|         self.recaptcha_siteverify_api = config.get(
    11|             "recaptcha_siteverify_api",
    12|             "https://www.recaptcha.net/recaptcha/api/siteverify",
    13|         )
    14|     def generate_config_section(self, **kwargs):
    15|         return """\
    16|         """


# ====================================================================
# FILE: synapse/config/consent_config.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-38 ---
     1| from os import path
     2| from synapse.config import ConfigError
     3| from ._base import Config
     4| DEFAULT_CONFIG = """\
     5| """
     6| class ConsentConfig(Config):
     7|     section = "consent"
     8|     def __init__(self, *args):
     9|         super(ConsentConfig, self).__init__(*args)
    10|         self.user_consent_version = None
    11|         self.user_consent_template_dir = None
    12|         self.user_consent_server_notice_content = None
    13|         self.user_consent_server_notice_to_guests = False
    14|         self.block_events_without_consent_error = None
    15|         self.user_consent_at_registration = False
    16|         self.user_consent_policy_name = "Privacy Policy"
    17|     def read_config(self, config, **kwargs):
    18|         consent_config = config.get("user_consent")
    19|         if consent_config is None:
    20|             return
    21|         self.user_consent_version = str(consent_config["version"])
    22|         self.user_consent_template_dir = self.abspath(consent_config["template_dir"])
    23|         if not path.isdir(self.user_consent_template_dir):
    24|             raise ConfigError(
    25|                 "Could not find template directory '%s'"
    26|                 % (self.user_consent_template_dir,)
    27|             )
    28|         self.user_consent_server_notice_content = consent_config.get(
    29|             "server_notice_content"
    30|         )
    31|         self.block_events_without_consent_error = consent_config.get(
    32|             "block_events_error"
    33|         )
    34|         self.user_consent_server_notice_to_guests = bool(
    35|             consent_config.get("send_server_notice_to_guests", False)
    36|         )
    37|         self.user_consent_at_registration = bool(
    38|             consent_config.get("require_at_registration", False)


# ====================================================================
# FILE: synapse/config/emailconfig.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-21 ---
     1| from __future__ import print_function
     2| import email.utils
     3| import os
     4| from enum import Enum
     5| from typing import Optional
     6| import attr
     7| from ._base import Config, ConfigError
     8| MISSING_PASSWORD_RESET_CONFIG_ERROR = """\
     9| Password reset emails are enabled on this homeserver due to a partial
    10| 'email' block. However, the following required keys are missing:
    11|     %s
    12| """
    13| DEFAULT_SUBJECTS = {
    14|     "message_from_person_in_room": "[%(app)s] You have a message on %(app)s from %(person)s in the %(room)s room...",
    15|     "message_from_person": "[%(app)s] You have a message on %(app)s from %(person)s...",
    16|     "messages_from_person": "[%(app)s] You have messages on %(app)s from %(person)s...",
    17|     "messages_in_room": "[%(app)s] You have messages on %(app)s in the %(room)s room...",
    18|     "messages_in_room_and_others": "[%(app)s] You have messages on %(app)s in the %(room)s room and others...",
    19|     "messages_from_person_and_others": "[%(app)s] You have messages on %(app)s from %(person)s and others...",
    20|     "invite_from_person": "[%(app)s] %(person)s has invited you to chat on %(app)s...",
    21|     "invite_from_person_to_room": "[%(app)s] %(person)s has invited you to join the %(room)s room on %(app)s...",

# --- HUNK 2: Lines 130-183 ---
   130|             )
   131|             add_threepid_template_failure_html = email_config.get(
   132|                 "add_threepid_template_failure_html", "add_threepid_failure.html"
   133|             )
   134|             password_reset_template_success_html = email_config.get(
   135|                 "password_reset_template_success_html", "password_reset_success.html"
   136|             )
   137|             registration_template_success_html = email_config.get(
   138|                 "registration_template_success_html", "registration_success.html"
   139|             )
   140|             add_threepid_template_success_html = email_config.get(
   141|                 "add_threepid_template_success_html", "add_threepid_success.html"
   142|             )
   143|             (
   144|                 self.email_password_reset_template_html,
   145|                 self.email_password_reset_template_text,
   146|                 self.email_registration_template_html,
   147|                 self.email_registration_template_text,
   148|                 self.email_add_threepid_template_html,
   149|                 self.email_add_threepid_template_text,
   150|                 self.email_password_reset_template_failure_html,
   151|                 self.email_registration_template_failure_html,
   152|                 self.email_add_threepid_template_failure_html,
   153|                 password_reset_template_success_html_template,
   154|                 registration_template_success_html_template,
   155|                 add_threepid_template_success_html_template,
   156|             ) = self.read_templates(
   157|                 [
   158|                     password_reset_template_html,
   159|                     password_reset_template_text,
   160|                     registration_template_html,
   161|                     registration_template_text,
   162|                     add_threepid_template_html,
   163|                     add_threepid_template_text,
   164|                     password_reset_template_failure_html,
   165|                     registration_template_failure_html,
   166|                     add_threepid_template_failure_html,
   167|                     password_reset_template_success_html,
   168|                     registration_template_success_html,
   169|                     add_threepid_template_success_html,
   170|                 ],
   171|                 template_dir,
   172|             )
   173|             self.email_password_reset_template_success_html_content = (
   174|                 password_reset_template_success_html_template.render()
   175|             )
   176|             self.email_registration_template_success_html_content = (
   177|                 registration_template_success_html_template.render()
   178|             )
   179|             self.email_add_threepid_template_success_html_content = (
   180|                 add_threepid_template_success_html_template.render()
   181|             )
   182|         if self.email_enable_notifs:
   183|             missing = []


# ====================================================================
# FILE: synapse/config/federation.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| from typing import Optional
     2| from netaddr import IPSet
     3| from ._base import Config, ConfigError
     4| class FederationConfig(Config):
     5|     section = "federation"
     6|     def read_config(self, config, **kwargs):
     7|         self.federation_domain_whitelist = None  # type: Optional[dict]
     8|         federation_domain_whitelist = config.get("federation_domain_whitelist", None)
     9|         if federation_domain_whitelist is not None:
    10|             self.federation_domain_whitelist = {}
    11|             for domain in federation_domain_whitelist:
    12|                 self.federation_domain_whitelist[domain] = True
    13|         self.federation_ip_range_blacklist = config.get(
    14|             "federation_ip_range_blacklist", []
    15|         )
    16|         try:
    17|             self.federation_ip_range_blacklist = IPSet(
    18|                 self.federation_ip_range_blacklist
    19|             )
    20|             self.federation_ip_range_blacklist.update(["0.0.0.0", "::"])
    21|         except Exception as e:
    22|             raise ConfigError(
    23|                 "Invalid range(s) provided in federation_ip_range_blacklist: %s" % e
    24|             )
    25|     def generate_config_section(self, config_dir_path, server_name, **kwargs):
    26|         return """\
    27|         federation_ip_range_blacklist:
    28|           - '127.0.0.0/8'
    29|           - '10.0.0.0/8'
    30|           - '172.16.0.0/12'
    31|           - '192.168.0.0/16'
    32|           - '100.64.0.0/10'
    33|           - '169.254.0.0/16'
    34|           - '::1/128'
    35|           - 'fe80::/64'
    36|           - 'fc00::/7'
    37|         """


# ====================================================================
# FILE: synapse/config/homeserver.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 56-77 ---
    56|         OIDCConfig,
    57|         CasConfig,
    58|         SSOConfig,
    59|         JWTConfig,
    60|         PasswordConfig,
    61|         EmailConfig,
    62|         PasswordAuthProviderConfig,
    63|         PushConfig,
    64|         SpamCheckerConfig,
    65|         RoomConfig,
    66|         GroupsConfig,
    67|         UserDirectoryConfig,
    68|         ConsentConfig,
    69|         StatsConfig,
    70|         ServerNoticesConfig,
    71|         RoomDirectoryConfig,
    72|         ThirdPartyRulesConfig,
    73|         TracerConfig,
    74|         WorkerConfig,
    75|         RedisConfig,
    76|         FederationConfig,
    77|     ]


# ====================================================================
# FILE: synapse/config/logger.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| import argparse
     2| import logging
     3| import logging.config
     4| import os
     5| import sys
     6| from string import Template
     7| import yaml
     8| from twisted.logger import (
     9|     ILogObserver,
    10|     LogBeginner,
    11|     STDLibLogObserver,
    12|     globalLogBeginner,
    13| )
    14| import synapse
    15| from synapse.app import _base as appbase
    16| from synapse.logging._structured import (
    17|     reload_structured_logging,
    18|     setup_structured_logging,
    19| )
    20| from synapse.logging.context import LoggingContextFilter
    21| from synapse.util.versionstring import get_version_string
    22| from ._base import Config, ConfigError
    23| DEFAULT_LOG_CONFIG = Template(
    24|     """\
    25| version: 1
    26| formatters:
    27|     precise:
    28|         format: '%(asctime)s - %(name)s - %(lineno)d - %(levelname)s - \
    29| %(request)s - %(message)s'
    30| handlers:
    31|     file:

# --- HUNK 2: Lines 111-159 ---
   111|             "%(asctime)s - %(name)s - %(lineno)d - %(levelname)s - %(request)s"
   112|             " - %(message)s"
   113|         )
   114|         logger = logging.getLogger("")
   115|         logger.setLevel(logging.INFO)
   116|         logging.getLogger("synapse.storage.SQL").setLevel(logging.INFO)
   117|         formatter = logging.Formatter(log_format)
   118|         handler = logging.StreamHandler()
   119|         handler.setFormatter(formatter)
   120|         logger.addHandler(handler)
   121|     else:
   122|         logging.config.dictConfig(log_config)
   123|     log_filter = LoggingContextFilter(request="")
   124|     old_factory = logging.getLogRecordFactory()
   125|     def factory(*args, **kwargs):
   126|         record = old_factory(*args, **kwargs)
   127|         log_filter.filter(record)
   128|         return record
   129|     logging.setLogRecordFactory(factory)
   130|     observer = STDLibLogObserver()
   131|     def _log(event):
   132|         if "log_text" in event:
   133|             if event["log_text"].startswith("DNSDatagramProtocol starting on "):
   134|                 return
   135|             if event["log_text"].startswith("(UDP Port "):
   136|                 return
   137|             if event["log_text"].startswith("Timing out client"):
   138|                 return
   139|         return observer(event)
   140|     logBeginner.beginLoggingTo([_log], redirectStandardIO=not config.no_redirect_stdio)
   141|     if not config.no_redirect_stdio:
   142|         print("Redirected stdout/stderr to logs")
   143|     return observer
   144| def _reload_stdlib_logging(*args, log_config=None):
   145|     logger = logging.getLogger("")
   146|     if not log_config:
   147|         logger.warning("Reloaded a blank config?")
   148|     logging.config.dictConfig(log_config)
   149| def setup_logging(
   150|     hs, config, use_worker_options=False, logBeginner: LogBeginner = globalLogBeginner
   151| ) -> ILogObserver:
   152|     """
   153|     Set up the logging subsystem.
   154|     Args:
   155|         config (LoggingConfig | synapse.config.worker.WorkerConfig):
   156|             configuration data
   157|         use_worker_options (bool): True to use the 'worker_log_config' option
   158|             instead of 'log_config'.
   159|         logBeginner: The Twisted logBeginner to use.


# ====================================================================
# FILE: synapse/config/oidc_config.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 14-53 ---
    14|         except DependencyException as e:
    15|             raise ConfigError(e.message)
    16|         public_baseurl = self.public_baseurl
    17|         if public_baseurl is None:
    18|             raise ConfigError("oidc_config requires a public_baseurl to be set")
    19|         self.oidc_callback_url = public_baseurl + "_synapse/oidc/callback"
    20|         self.oidc_enabled = True
    21|         self.oidc_discover = oidc_config.get("discover", True)
    22|         self.oidc_issuer = oidc_config["issuer"]
    23|         self.oidc_client_id = oidc_config["client_id"]
    24|         self.oidc_client_secret = oidc_config["client_secret"]
    25|         self.oidc_client_auth_method = oidc_config.get(
    26|             "client_auth_method", "client_secret_basic"
    27|         )
    28|         self.oidc_scopes = oidc_config.get("scopes", ["openid"])
    29|         self.oidc_authorization_endpoint = oidc_config.get("authorization_endpoint")
    30|         self.oidc_token_endpoint = oidc_config.get("token_endpoint")
    31|         self.oidc_userinfo_endpoint = oidc_config.get("userinfo_endpoint")
    32|         self.oidc_jwks_uri = oidc_config.get("jwks_uri")
    33|         self.oidc_skip_verification = oidc_config.get("skip_verification", False)
    34|         ump_config = oidc_config.get("user_mapping_provider", {})
    35|         ump_config.setdefault("module", DEFAULT_USER_MAPPING_PROVIDER)
    36|         ump_config.setdefault("config", {})
    37|         (
    38|             self.oidc_user_mapping_provider_class,
    39|             self.oidc_user_mapping_provider_config,
    40|         ) = load_module(ump_config)
    41|         required_methods = [
    42|             "get_remote_user_id",
    43|             "map_user_attributes",
    44|         ]
    45|         missing_methods = [
    46|             method
    47|             for method in required_methods
    48|             if not hasattr(self.oidc_user_mapping_provider_class, method)
    49|         ]
    50|         if missing_methods:
    51|             raise ConfigError(
    52|                 "Class specified by oidc_config."
    53|                 "user_mapping_provider.module is missing required "


# ====================================================================
# FILE: synapse/config/registration.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-33 ---
     1| import os
     2| from distutils.util import strtobool
     3| import pkg_resources
     4| from synapse.api.constants import RoomCreationPreset
     5| from synapse.config._base import Config, ConfigError
     6| from synapse.types import RoomAlias, UserID
     7| from synapse.util.stringutils import random_string_with_symbols
     8| class AccountValidityConfig(Config):
     9|     section = "accountvalidity"
    10|     def __init__(self, config, synapse_config):
    11|         if config is None:
    12|             return
    13|         super(AccountValidityConfig, self).__init__()
    14|         self.enabled = config.get("enabled", False)
    15|         self.renew_by_email_enabled = "renew_at" in config
    16|         if self.enabled:
    17|             if "period" in config:
    18|                 self.period = self.parse_duration(config["period"])
    19|             else:
    20|                 raise ConfigError("'period' is required when using account validity")
    21|             if "renew_at" in config:
    22|                 self.renew_at = self.parse_duration(config["renew_at"])
    23|             if "renew_email_subject" in config:
    24|                 self.renew_email_subject = config["renew_email_subject"]
    25|             else:
    26|                 self.renew_email_subject = "Renew your %(app)s account"
    27|             self.startup_job_max_delta = self.period * 10.0 / 100.0
    28|         if self.renew_by_email_enabled:
    29|             if "public_baseurl" not in synapse_config:
    30|                 raise ConfigError("Can't send renewal emails without 'public_baseurl'")
    31|         template_dir = config.get("template_dir")
    32|         if not template_dir:
    33|             template_dir = pkg_resources.resource_filename("synapse", "res/templates")

# --- HUNK 2: Lines 115-154 ---
   115|                 raise ConfigError("Invalid value for autocreate_auto_join_room_preset")
   116|             if self.auto_join_room_requires_invite:
   117|                 if not mxid_localpart:
   118|                     raise ConfigError(
   119|                         "The configuration option `auto_join_mxid_localpart` is required if "
   120|                         "`autocreate_auto_join_room_preset` is set to private_chat or trusted_private_chat, such that "
   121|                         "Synapse knows who to send invitations from. Please "
   122|                         "configure `auto_join_mxid_localpart`."
   123|                     )
   124|         self.auto_join_rooms_for_guests = config.get("auto_join_rooms_for_guests", True)
   125|         self.enable_set_displayname = config.get("enable_set_displayname", True)
   126|         self.enable_set_avatar_url = config.get("enable_set_avatar_url", True)
   127|         self.enable_3pid_changes = config.get("enable_3pid_changes", True)
   128|         self.disable_msisdn_registration = config.get(
   129|             "disable_msisdn_registration", False
   130|         )
   131|         session_lifetime = config.get("session_lifetime")
   132|         if session_lifetime is not None:
   133|             session_lifetime = self.parse_duration(session_lifetime)
   134|         self.session_lifetime = session_lifetime
   135|     def generate_config_section(self, generate_secrets=False, **kwargs):
   136|         if generate_secrets:
   137|             registration_shared_secret = 'registration_shared_secret: "%s"' % (
   138|                 random_string_with_symbols(50),
   139|             )
   140|         else:
   141|             registration_shared_secret = "#registration_shared_secret: <PRIVATE STRING>"
   142|         return (
   143|             """\
   144|         account_validity:
   145|         %(registration_shared_secret)s
   146|         account_threepid_delegates:
   147|         """
   148|             % locals()
   149|         )
   150|     @staticmethod
   151|     def add_arguments(parser):
   152|         reg_group = parser.add_argument_group("registration")
   153|         reg_group.add_argument(
   154|             "--enable-registration",


# ====================================================================
# FILE: synapse/config/saml2_config.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 90-132 ---
    90|                 "methods: %s" % (", ".join(missing_methods),)
    91|             )
    92|         saml2_config_dict = self._default_saml_config_dict(
    93|             *self.saml2_user_mapping_provider_class.get_saml_attributes(
    94|                 self.saml2_user_mapping_provider_config
    95|             )
    96|         )
    97|         _dict_merge(
    98|             merge_dict=saml2_config.get("sp_config", {}), into_dict=saml2_config_dict
    99|         )
   100|         config_path = saml2_config.get("config_path", None)
   101|         if config_path is not None:
   102|             mod = load_python_module(config_path)
   103|             _dict_merge(merge_dict=mod.CONFIG, into_dict=saml2_config_dict)
   104|         import saml2.config
   105|         self.saml2_sp_config = saml2.config.SPConfig()
   106|         self.saml2_sp_config.load(saml2_config_dict)
   107|         self.saml2_session_lifetime = self.parse_duration(
   108|             saml2_config.get("saml_session_lifetime", "15m")
   109|         )
   110|         self.saml2_error_html_template = self.read_templates(
   111|             ["saml_error.html"], saml2_config.get("template_dir"), autoescape=True
   112|         )[0]
   113|     def _default_saml_config_dict(
   114|         self, required_attributes: set, optional_attributes: set
   115|     ):
   116|         """Generate a configuration dictionary with required and optional attributes that
   117|         will be needed to process new user registration
   118|         Args:
   119|             required_attributes: SAML auth response attributes that are
   120|                 necessary to function
   121|             optional_attributes: SAML auth response attributes that can be used to add
   122|                 additional information to Synapse user accounts, but are not required
   123|         Returns:
   124|             dict: A SAML configuration dictionary
   125|         """
   126|         import saml2
   127|         public_baseurl = self.public_baseurl
   128|         if public_baseurl is None:
   129|             raise ConfigError("saml2_config requires a public_baseurl to be set")
   130|         if self.saml2_grandfathered_mxid_source_attribute:
   131|             optional_attributes.add(self.saml2_grandfathered_mxid_source_attribute)
   132|         optional_attributes -= required_attributes


# ====================================================================
# FILE: synapse/config/server.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| import logging
     2| import os.path
     3| import re
     4| from textwrap import indent
     5| from typing import Any, Dict, Iterable, List, Optional
     6| import attr
     7| import yaml
     8| from synapse.api.room_versions import KNOWN_ROOM_VERSIONS
     9| from synapse.http.endpoint import parse_and_validate_server_name
    10| from ._base import Config, ConfigError
    11| logger = logging.Logger(__name__)
    12| DEFAULT_BIND_ADDRESSES = ["::", "0.0.0.0"]
    13| DEFAULT_ROOM_VERSION = "5"
    14| ROOM_COMPLEXITY_TOO_GREAT = (
    15|     "Your homeserver is unable to join rooms this large or complex. "
    16|     "Please speak to your server administrator, or upgrade your instance "
    17|     "to join this room."
    18| )
    19| METRICS_PORT_WARNING = """\
    20| The metrics_port configuration option is deprecated in Synapse 0.31 in favour of
    21| a listener. Please see
    22| https://github.com/matrix-org/synapse/blob/master/docs/metrics-howto.md
    23| on how to configure the new listener.
    24| --------------------------------------------------------------------------------"""
    25| KNOWN_LISTENER_TYPES = {

# --- HUNK 2: Lines 354-393 ---
   354|                         resources=[HttpResourceConfig(names=["metrics"])]
   355|                     ),
   356|                 )
   357|             )
   358|         self.cleanup_extremities_with_dummy_events = config.get(
   359|             "cleanup_extremities_with_dummy_events", True
   360|         )
   361|         self.dummy_events_threshold = config.get("dummy_events_threshold", 10)
   362|         self.enable_ephemeral_messages = config.get("enable_ephemeral_messages", False)
   363|         self.request_token_inhibit_3pid_errors = config.get(
   364|             "request_token_inhibit_3pid_errors", False,
   365|         )
   366|         users_new_default_push_rules = (
   367|             config.get("users_new_default_push_rules") or []
   368|         )  # type: list
   369|         if not isinstance(users_new_default_push_rules, list):
   370|             raise ConfigError("'users_new_default_push_rules' must be a list")
   371|         self.users_new_default_push_rules = set(
   372|             users_new_default_push_rules
   373|         )  # type: set
   374|     def has_tls_listener(self) -> bool:
   375|         return any(listener.tls for listener in self.listeners)
   376|     def generate_config_section(
   377|         self, server_name, data_dir_path, open_private_ports, listeners, **kwargs
   378|     ):
   379|         _, bind_port = parse_and_validate_server_name(server_name)
   380|         if bind_port is not None:
   381|             unsecure_port = bind_port - 400
   382|         else:
   383|             bind_port = 8448
   384|             unsecure_port = 8008
   385|         pid_file = os.path.join(data_dir_path, "homeserver.pid")
   386|         default_room_version = DEFAULT_ROOM_VERSION
   387|         secure_listeners = []
   388|         unsecure_listeners = []
   389|         private_addresses = ["::1", "127.0.0.1"]
   390|         if listeners:
   391|             for listener in listeners:
   392|                 if listener["tls"]:
   393|                     secure_listeners.append(listener)


# ====================================================================
# FILE: synapse/config/server_notices_config.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 3-38 ---
     3| DEFAULT_CONFIG = """\
     4| """
     5| class ServerNoticesConfig(Config):
     6|     """Configuration for the server notices room.
     7|     Attributes:
     8|         server_notices_mxid (str|None):
     9|             The MXID to use for server notices.
    10|             None if server notices are not enabled.
    11|         server_notices_mxid_display_name (str|None):
    12|             The display name to use for the server notices user.
    13|             None if server notices are not enabled.
    14|         server_notices_mxid_avatar_url (str|None):
    15|             The MXC URL for the avatar of the server notices user.
    16|             None if server notices are not enabled.
    17|         server_notices_room_name (str|None):
    18|             The name to use for the server notices room.
    19|             None if server notices are not enabled.
    20|     """
    21|     section = "servernotices"
    22|     def __init__(self, *args):
    23|         super(ServerNoticesConfig, self).__init__(*args)
    24|         self.server_notices_mxid = None
    25|         self.server_notices_mxid_display_name = None
    26|         self.server_notices_mxid_avatar_url = None
    27|         self.server_notices_room_name = None
    28|     def read_config(self, config, **kwargs):
    29|         c = config.get("server_notices")
    30|         if c is None:
    31|             return
    32|         mxid_localpart = c["system_mxid_localpart"]
    33|         self.server_notices_mxid = UserID(mxid_localpart, self.server_name).to_string()
    34|         self.server_notices_mxid_display_name = c.get("system_mxid_display_name", None)
    35|         self.server_notices_mxid_avatar_url = c.get("system_mxid_avatar_url", None)
    36|         self.server_notices_room_name = c.get("room_name", "Server Notices")
    37|     def generate_config_section(self, **kwargs):
    38|         return DEFAULT_CONFIG


# ====================================================================
# FILE: synapse/config/stats.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-21 ---
     1| from __future__ import division
     2| import sys
     3| from ._base import Config
     4| class StatsConfig(Config):
     5|     """Stats Configuration
     6|     Configuration for the behaviour of synapse's stats engine
     7|     """
     8|     section = "stats"
     9|     def read_config(self, config, **kwargs):
    10|         self.stats_enabled = True
    11|         self.stats_bucket_size = 86400 * 1000
    12|         self.stats_retention = sys.maxsize
    13|         stats_config = config.get("stats", None)
    14|         if stats_config:
    15|             self.stats_enabled = stats_config.get("enabled", self.stats_enabled)
    16|             self.stats_bucket_size = self.parse_duration(
    17|                 stats_config.get("bucket_size", "1d")
    18|             )
    19|             self.stats_retention = self.parse_duration(
    20|                 stats_config.get("retention", "%ds" % (sys.maxsize,))
    21|             )


# ====================================================================
# FILE: synapse/config/workers.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-71 ---
     1| import attr
     2| from ._base import Config, ConfigError, ShardedWorkerHandlingConfig
     3| from .server import ListenerConfig, parse_listener_def
     4| @attr.s
     5| class InstanceLocationConfig:
     6|     """The host and port to talk to an instance via HTTP replication.
     7|     """
     8|     host = attr.ib(type=str)
     9|     port = attr.ib(type=int)
    10| @attr.s
    11| class WriterLocations:
    12|     """Specifies the instances that write various streams.
    13|     Attributes:
    14|         events: The instance that writes to the event and backfill streams.
    15|         events: The instance that writes to the typing stream.
    16|     """
    17|     events = attr.ib(default="master", type=str)
    18|     typing = attr.ib(default="master", type=str)
    19| class WorkerConfig(Config):
    20|     """The workers are processes run separately to the main synapse process.
    21|     They have their own pid_file and listener configuration. They use the
    22|     replication_url to talk to the main synapse process."""
    23|     section = "worker"
    24|     def read_config(self, config, **kwargs):
    25|         self.worker_app = config.get("worker_app")
    26|         if self.worker_app == "synapse.app.homeserver":
    27|             self.worker_app = None
    28|         self.worker_listeners = [
    29|             parse_listener_def(x) for x in config.get("worker_listeners", [])
    30|         ]
    31|         self.worker_daemonize = config.get("worker_daemonize")
    32|         self.worker_pid_file = config.get("worker_pid_file")
    33|         self.worker_log_config = config.get("worker_log_config")
    34|         self.worker_replication_host = config.get("worker_replication_host", None)
    35|         self.worker_replication_port = config.get("worker_replication_port", None)
    36|         self.worker_replication_http_port = config.get("worker_replication_http_port")
    37|         self.worker_name = config.get("worker_name", self.worker_app)
    38|         self.worker_main_http_uri = config.get("worker_main_http_uri", None)
    39|         manhole = config.get("worker_manhole")
    40|         if manhole:
    41|             self.worker_listeners.append(
    42|                 ListenerConfig(
    43|                     port=manhole, bind_addresses=["127.0.0.1"], type="manhole",
    44|                 )
    45|             )
    46|         self.send_federation = config.get("send_federation", True)
    47|         federation_sender_instances = config.get("federation_sender_instances") or []
    48|         self.federation_shard_config = ShardedWorkerHandlingConfig(
    49|             federation_sender_instances
    50|         )
    51|         instance_map = config.get("instance_map") or {}
    52|         self.instance_map = {
    53|             name: InstanceLocationConfig(**c) for name, c in instance_map.items()
    54|         }
    55|         writers = config.get("stream_writers") or {}
    56|         self.writers = WriterLocations(**writers)
    57|         for stream in ("events", "typing"):
    58|             instance = getattr(self.writers, stream)
    59|             if instance != "master" and instance not in self.instance_map:
    60|                 raise ConfigError(
    61|                     "Instance %r is configured to write %s but does not appear in `instance_map` config."
    62|                     % (instance, stream)
    63|                 )
    64|     def generate_config_section(self, config_dir_path, server_name, **kwargs):
    65|         return """\
    66|         """
    67|     def read_arguments(self, args):
    68|         if args.daemonize is not None:
    69|             self.worker_daemonize = args.daemonize
    70|         if args.manhole is not None:
    71|             self.worker_manhole = args.worker_manhole


# ====================================================================
# FILE: synapse/crypto/context_factory.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 6-46 ---
     6| from twisted.internet._sslverify import _defaultCurveName
     7| from twisted.internet.abstract import isIPAddress, isIPv6Address
     8| from twisted.internet.interfaces import IOpenSSLClientConnectionCreator
     9| from twisted.internet.ssl import (
    10|     CertificateOptions,
    11|     ContextFactory,
    12|     TLSVersion,
    13|     platformTrust,
    14| )
    15| from twisted.python.failure import Failure
    16| from twisted.web.iweb import IPolicyForHTTPS
    17| logger = logging.getLogger(__name__)
    18| _TLS_VERSION_MAP = {
    19|     "1": TLSVersion.TLSv1_0,
    20|     "1.1": TLSVersion.TLSv1_1,
    21|     "1.2": TLSVersion.TLSv1_2,
    22|     "1.3": TLSVersion.TLSv1_3,
    23| }
    24| class ServerContextFactory(ContextFactory):
    25|     """Factory for PyOpenSSL SSL contexts that are used to handle incoming
    26|     connections."""
    27|     def __init__(self, config):
    28|         self._context = SSL.Context(SSL.SSLv23_METHOD)
    29|         self.configure_context(self._context, config)
    30|     @staticmethod
    31|     def configure_context(context, config):
    32|         try:
    33|             _ecCurve = crypto.get_elliptic_curve(_defaultCurveName)
    34|             context.set_tmp_ecdh(_ecCurve)
    35|         except Exception:
    36|             logger.exception("Failed to enable elliptic curve for TLS")
    37|         context.set_options(
    38|             SSL.OP_NO_SSLv2 | SSL.OP_NO_SSLv3 | SSL.OP_NO_TLSv1 | SSL.OP_NO_TLSv1_1
    39|         )
    40|         context.use_certificate_chain_file(config.tls_certificate_file)
    41|         context.use_privatekey(config.tls_private_key)
    42|         context.set_cipher_list(
    43|             "ECDH+AESGCM:ECDH+CHACHA20:ECDH+AES256:ECDH+AES128:!aNULL:!SHA1:!AESCCM"
    44|         )
    45|     def getContext(self):
    46|         return self._context


# ====================================================================
# FILE: synapse/crypto/keyring.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 6-46 ---
     6|     decode_verify_key_bytes,
     7|     encode_verify_key_base64,
     8|     is_signing_algorithm_supported,
     9| )
    10| from signedjson.sign import (
    11|     SignatureVerifyException,
    12|     encode_canonical_json,
    13|     signature_ids,
    14|     verify_signed_json,
    15| )
    16| from unpaddedbase64 import decode_base64
    17| from twisted.internet import defer
    18| from synapse.api.errors import (
    19|     Codes,
    20|     HttpResponseException,
    21|     RequestSendFailed,
    22|     SynapseError,
    23| )
    24| from synapse.logging.context import (
    25|     PreserveLoggingContext,
    26|     current_context,
    27|     make_deferred_yieldable,
    28|     preserve_fn,
    29|     run_in_background,
    30| )
    31| from synapse.storage.keys import FetchKeyResult
    32| from synapse.util import unwrapFirstError
    33| from synapse.util.async_helpers import yieldable_gather_results
    34| from synapse.util.metrics import Measure
    35| from synapse.util.retryutils import NotRetryingDestination
    36| logger = logging.getLogger(__name__)
    37| @attr.s(slots=True, cmp=False)
    38| class VerifyJsonRequest:
    39|     """
    40|     A request to verify a JSON object.
    41|     Attributes:
    42|         server_name(str): The name of the server to verify against.
    43|         key_ids(set[str]): The set of key_ids to that could be used to verify the
    44|             JSON object
    45|         json_object(dict): The JSON object to verify.
    46|         minimum_valid_until_ts (int): time at which we require the signing key to

# --- HUNK 2: Lines 141-201 ---
   141|             logger.debug(
   142|                 "Verifying %s for %s with key_ids %s, min_validity %i",
   143|                 verify_request.request_name,
   144|                 verify_request.server_name,
   145|                 verify_request.key_ids,
   146|                 verify_request.minimum_valid_until_ts,
   147|             )
   148|             key_lookups.append(verify_request)
   149|             return handle(verify_request)
   150|         results = [process(r) for r in verify_requests]
   151|         if key_lookups:
   152|             run_in_background(self._start_key_lookups, key_lookups)
   153|         return results
   154|     async def _start_key_lookups(self, verify_requests):
   155|         """Sets off the key fetches for each verify request
   156|         Once each fetch completes, verify_request.key_ready will be resolved.
   157|         Args:
   158|             verify_requests (List[VerifyJsonRequest]):
   159|         """
   160|         try:
   161|             ctx = current_context()
   162|             server_to_request_ids = {}
   163|             for verify_request in verify_requests:
   164|                 server_name = verify_request.server_name
   165|                 request_id = id(verify_request)
   166|                 server_to_request_ids.setdefault(server_name, set()).add(request_id)
   167|             await self.wait_for_previous_lookups(server_to_request_ids.keys())
   168|             for server_name in server_to_request_ids.keys():
   169|                 self.key_downloads[server_name] = defer.Deferred()
   170|                 logger.debug("Got key lookup lock on %s", server_name)
   171|             def drop_server_lock(server_name):
   172|                 d = self.key_downloads.pop(server_name)
   173|                 d.callback(None)
   174|             def lookup_done(res, verify_request):
   175|                 server_name = verify_request.server_name
   176|                 server_requests = server_to_request_ids[server_name]
   177|                 server_requests.remove(id(verify_request))
   178|                 if not server_requests:
   179|                     with PreserveLoggingContext(ctx):
   180|                         logger.debug("Releasing key lookup lock on %s", server_name)
   181|                     self.clock.call_later(0, drop_server_lock, server_name)
   182|                 return res
   183|             for verify_request in verify_requests:
   184|                 verify_request.key_ready.addBoth(lookup_done, verify_request)
   185|             self._get_server_verify_keys(verify_requests)
   186|         except Exception:
   187|             logger.exception("Error starting key lookups")
   188|     async def wait_for_previous_lookups(self, server_names) -> None:
   189|         """Waits for any previous key lookups for the given servers to finish.
   190|         Args:
   191|             server_names (Iterable[str]): list of servers which we want to look up
   192|         Returns:
   193|             Resolves once all key lookups for the given servers have
   194|                 completed. Follows the synapse rules of logcontext preservation.
   195|         """
   196|         loop_count = 1
   197|         while True:
   198|             wait_on = [
   199|                 (server_name, self.key_downloads[server_name])
   200|                 for server_name in server_names
   201|                 if server_name in self.key_downloads

# --- HUNK 3: Lines 211-305 ---
   211|                 await defer.DeferredList((w[1] for w in wait_on))
   212|             loop_count += 1
   213|     def _get_server_verify_keys(self, verify_requests):
   214|         """Tries to find at least one key for each verify request
   215|         For each verify_request, verify_request.key_ready is called back with
   216|         params (server_name, key_id, VerifyKey) if a key is found, or errbacked
   217|         with a SynapseError if none of the keys are found.
   218|         Args:
   219|             verify_requests (list[VerifyJsonRequest]): list of verify requests
   220|         """
   221|         remaining_requests = {rq for rq in verify_requests if not rq.key_ready.called}
   222|         async def do_iterations():
   223|             try:
   224|                 with Measure(self.clock, "get_server_verify_keys"):
   225|                     for f in self._key_fetchers:
   226|                         if not remaining_requests:
   227|                             return
   228|                         await self._attempt_key_fetches_with_fetcher(
   229|                             f, remaining_requests
   230|                         )
   231|                     with PreserveLoggingContext():
   232|                         for verify_request in remaining_requests:
   233|                             verify_request.key_ready.errback(
   234|                                 SynapseError(
   235|                                     401,
   236|                                     "No key for %s with ids in %s (min_validity %i)"
   237|                                     % (
   238|                                         verify_request.server_name,
   239|                                         verify_request.key_ids,
   240|                                         verify_request.minimum_valid_until_ts,
   241|                                     ),
   242|                                     Codes.UNAUTHORIZED,
   243|                                 )
   244|                             )
   245|             except Exception as err:
   246|                 logger.error("Unexpected error in _get_server_verify_keys: %s", err)
   247|                 with PreserveLoggingContext():
   248|                     for verify_request in remaining_requests:
   249|                         if not verify_request.key_ready.called:
   250|                             verify_request.key_ready.errback(err)
   251|         run_in_background(do_iterations)
   252|     async def _attempt_key_fetches_with_fetcher(self, fetcher, remaining_requests):
   253|         """Use a key fetcher to attempt to satisfy some key requests
   254|         Args:
   255|             fetcher (KeyFetcher): fetcher to use to fetch the keys
   256|             remaining_requests (set[VerifyJsonRequest]): outstanding key requests.
   257|                 Any successfully-completed requests will be removed from the list.
   258|         """
   259|         missing_keys = defaultdict(dict)
   260|         for verify_request in remaining_requests:
   261|             assert not verify_request.key_ready.called
   262|             keys_for_server = missing_keys[verify_request.server_name]
   263|             for key_id in verify_request.key_ids:
   264|                 keys_for_server[key_id] = max(
   265|                     keys_for_server.get(key_id, -1),
   266|                     verify_request.minimum_valid_until_ts,
   267|                 )
   268|         results = await fetcher.get_keys(missing_keys)
   269|         completed = []
   270|         for verify_request in remaining_requests:
   271|             server_name = verify_request.server_name
   272|             result_keys = results.get(server_name, {})
   273|             for key_id in verify_request.key_ids:
   274|                 fetch_key_result = result_keys.get(key_id)
   275|                 if not fetch_key_result:
   276|                     continue
   277|                 if (
   278|                     fetch_key_result.valid_until_ts
   279|                     < verify_request.minimum_valid_until_ts
   280|                 ):
   281|                     continue
   282|                 with PreserveLoggingContext():
   283|                     verify_request.key_ready.callback(
   284|                         (server_name, key_id, fetch_key_result.verify_key)
   285|                     )
   286|                 completed.append(verify_request)
   287|                 break
   288|         remaining_requests.difference_update(completed)
   289| class KeyFetcher:
   290|     async def get_keys(self, keys_to_fetch):
   291|         """
   292|         Args:
   293|             keys_to_fetch (dict[str, dict[str, int]]):
   294|                 the keys to be fetched. server_name -> key_id -> min_valid_ts
   295|         Returns:
   296|             Deferred[dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]]:
   297|                 map from server_name -> key_id -> FetchKeyResult
   298|         """
   299|         raise NotImplementedError
   300| class StoreKeyFetcher(KeyFetcher):
   301|     """KeyFetcher impl which fetches keys from our data store"""
   302|     def __init__(self, hs):
   303|         self.store = hs.get_datastore()
   304|     async def get_keys(self, keys_to_fetch):
   305|         """see KeyFetcher.get_keys"""

# --- HUNK 4: Lines 372-412 ---
   372|             defer.gatherResults(
   373|                 [
   374|                     run_in_background(
   375|                         self.store.store_server_keys_json,
   376|                         server_name=server_name,
   377|                         key_id=key_id,
   378|                         from_server=from_server,
   379|                         ts_now_ms=time_added_ms,
   380|                         ts_expires_ms=ts_valid_until_ms,
   381|                         key_json_bytes=key_json_bytes,
   382|                     )
   383|                     for key_id in verify_keys
   384|                 ],
   385|                 consumeErrors=True,
   386|             ).addErrback(unwrapFirstError)
   387|         )
   388|         return verify_keys
   389| class PerspectivesKeyFetcher(BaseV2KeyFetcher):
   390|     """KeyFetcher impl which fetches keys from the "perspectives" servers"""
   391|     def __init__(self, hs):
   392|         super(PerspectivesKeyFetcher, self).__init__(hs)
   393|         self.clock = hs.get_clock()
   394|         self.client = hs.get_http_client()
   395|         self.key_servers = self.config.key_servers
   396|     async def get_keys(self, keys_to_fetch):
   397|         """see KeyFetcher.get_keys"""
   398|         async def get_key(key_server):
   399|             try:
   400|                 result = await self.get_server_verify_key_v2_indirect(
   401|                     keys_to_fetch, key_server
   402|                 )
   403|                 return result
   404|             except KeyLookupError as e:
   405|                 logger.warning(
   406|                     "Key lookup failed from %r: %s", key_server.server_name, e
   407|                 )
   408|             except Exception as e:
   409|                 logger.exception(
   410|                     "Unable to get key from %r: %s %s",
   411|                     key_server.server_name,
   412|                     type(e).__name__,

# --- HUNK 5: Lines 509-549 ---
   509|             "signatures" not in response
   510|             or perspective_name not in response["signatures"]
   511|         ):
   512|             raise KeyLookupError("Response not signed by the notary server")
   513|         verified = False
   514|         for key_id in response["signatures"][perspective_name]:
   515|             if key_id in perspective_keys:
   516|                 verify_signed_json(response, perspective_name, perspective_keys[key_id])
   517|                 verified = True
   518|         if not verified:
   519|             raise KeyLookupError(
   520|                 "Response not signed with a known key: signed with: %r, known keys: %r"
   521|                 % (
   522|                     list(response["signatures"][perspective_name].keys()),
   523|                     list(perspective_keys.keys()),
   524|                 )
   525|             )
   526| class ServerKeyFetcher(BaseV2KeyFetcher):
   527|     """KeyFetcher impl which fetches keys from the origin servers"""
   528|     def __init__(self, hs):
   529|         super(ServerKeyFetcher, self).__init__(hs)
   530|         self.clock = hs.get_clock()
   531|         self.client = hs.get_http_client()
   532|     async def get_keys(self, keys_to_fetch):
   533|         """
   534|         Args:
   535|             keys_to_fetch (dict[str, iterable[str]]):
   536|                 the keys to be fetched. server_name -> key_ids
   537|         Returns:
   538|             dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]:
   539|                 map from server_name -> key_id -> FetchKeyResult
   540|         """
   541|         results = {}
   542|         async def get_key(key_to_fetch_item):
   543|             server_name, key_ids = key_to_fetch_item
   544|             try:
   545|                 keys = await self.get_server_verify_key_v2_direct(server_name, key_ids)
   546|                 results[server_name] = keys
   547|             except KeyLookupError as e:
   548|                 logger.warning(
   549|                     "Error looking up keys %s from %s: %s", key_ids, server_name, e


# ====================================================================
# FILE: synapse/events/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-27 ---
     1| import abc
     2| import os
     3| from distutils.util import strtobool
     4| from typing import Dict, Optional, Tuple, Type
     5| from unpaddedbase64 import encode_base64
     6| from synapse.api.room_versions import EventFormatVersions, RoomVersion, RoomVersions
     7| from synapse.types import JsonDict
     8| from synapse.util.caches import intern_dict
     9| from synapse.util.frozenutils import freeze
    10| USE_FROZEN_DICTS = strtobool(os.environ.get("SYNAPSE_USE_FROZEN_DICTS", "0"))
    11| class DictProperty:
    12|     """An object property which delegates to the `_dict` within its parent object."""
    13|     __slots__ = ["key"]
    14|     def __init__(self, key: str):
    15|         self.key = key
    16|     def __get__(self, instance, owner=None):
    17|         if instance is None:
    18|             return self
    19|         try:
    20|             return instance._dict[self.key]
    21|         except KeyError as e1:
    22|             raise AttributeError(
    23|                 "'%s' has no '%s' property" % (type(instance), self.key)
    24|             ) from e1.__context__
    25|     def __set__(self, instance, v):
    26|         instance._dict[self.key] = v
    27|     def __delete__(self, instance):

# --- HUNK 2: Lines 41-82 ---
    41|         super().__init__(key)
    42|         self.default = default
    43|     def __get__(self, instance, owner=None):
    44|         if instance is None:
    45|             return self
    46|         return instance._dict.get(self.key, self.default)
    47| class _EventInternalMetadata:
    48|     __slots__ = ["_dict"]
    49|     def __init__(self, internal_metadata_dict: JsonDict):
    50|         self._dict = dict(internal_metadata_dict)
    51|     outlier = DictProperty("outlier")  # type: bool
    52|     out_of_band_membership = DictProperty("out_of_band_membership")  # type: bool
    53|     send_on_behalf_of = DictProperty("send_on_behalf_of")  # type: str
    54|     recheck_redaction = DictProperty("recheck_redaction")  # type: bool
    55|     soft_failed = DictProperty("soft_failed")  # type: bool
    56|     proactively_send = DictProperty("proactively_send")  # type: bool
    57|     redacted = DictProperty("redacted")  # type: bool
    58|     txn_id = DictProperty("txn_id")  # type: str
    59|     token_id = DictProperty("token_id")  # type: str
    60|     stream_ordering = DictProperty("stream_ordering")  # type: int
    61|     before = DictProperty("before")  # type: str
    62|     after = DictProperty("after")  # type: str
    63|     order = DictProperty("order")  # type: Tuple[int, int]
    64|     def get_dict(self) -> JsonDict:
    65|         return dict(self._dict)
    66|     def is_outlier(self) -> bool:
    67|         return self._dict.get("outlier", False)
    68|     def is_out_of_band_membership(self) -> bool:
    69|         """Whether this is an out of band membership, like an invite or an invite
    70|         rejection. This is needed as those events are marked as outliers, but
    71|         they still need to be processed as if they're new events (e.g. updating
    72|         invite state in the database, relaying to clients, etc).
    73|         (Added in synapse 0.99.0, so may be unreliable for events received before that)
    74|         """
    75|         return self._dict.get("out_of_band_membership", False)
    76|     def get_send_on_behalf_of(self) -> Optional[str]:
    77|         """Whether this server should send the event on behalf of another server.
    78|         This is used by the federation "send_join" API to forward the initial join
    79|         event for a server in the room.
    80|         returns a str with the name of the server this event is sent on behalf of.
    81|         """
    82|         return self._dict.get("send_on_behalf_of")


# ====================================================================
# FILE: synapse/federation/federation_client.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-73 ---
     1| import copy
     2| import itertools
     3| import logging
     4| from typing import (
     5|     Any,
     6|     Awaitable,
     7|     Callable,
     8|     Dict,
     9|     Iterable,
    10|     List,
    11|     Optional,
    12|     Sequence,
    13|     Tuple,
    14|     TypeVar,
    15| )
    16| from prometheus_client import Counter
    17| from twisted.internet import defer
    18| from twisted.internet.defer import Deferred
    19| from synapse.api.constants import EventTypes, Membership
    20| from synapse.api.errors import (
    21|     CodeMessageException,
    22|     Codes,
    23|     FederationDeniedError,
    24|     HttpResponseException,
    25|     SynapseError,
    26|     UnsupportedRoomVersionError,
    27| )
    28| from synapse.api.room_versions import (
    29|     KNOWN_ROOM_VERSIONS,
    30|     EventFormatVersions,
    31|     RoomVersion,
    32|     RoomVersions,
    33| )
    34| from synapse.events import EventBase, builder
    35| from synapse.federation.federation_base import FederationBase, event_from_pdu_json
    36| from synapse.logging.context import make_deferred_yieldable, preserve_fn
    37| from synapse.logging.utils import log_function
    38| from synapse.types import JsonDict, get_domain_from_id
    39| from synapse.util import unwrapFirstError
    40| from synapse.util.caches.expiringcache import ExpiringCache
    41| from synapse.util.retryutils import NotRetryingDestination
    42| logger = logging.getLogger(__name__)
    43| sent_queries_counter = Counter("synapse_federation_client_sent_queries", "", ["type"])
    44| PDU_RETRY_TIME_MS = 1 * 60 * 1000
    45| T = TypeVar("T")
    46| class InvalidResponseError(RuntimeError):
    47|     """Helper for _try_destination_list: indicates that the server returned a response
    48|     we couldn't parse
    49|     """
    50|     pass
    51| class FederationClient(FederationBase):
    52|     def __init__(self, hs):
    53|         super(FederationClient, self).__init__(hs)
    54|         self.pdu_destination_tried = {}
    55|         self._clock.looping_call(self._clear_tried_cache, 60 * 1000)
    56|         self.state = hs.get_state_handler()
    57|         self.transport_layer = hs.get_federation_transport_client()
    58|         self.hostname = hs.hostname
    59|         self.signing_key = hs.signing_key
    60|         self._get_pdu_cache = ExpiringCache(
    61|             cache_name="get_pdu_cache",
    62|             clock=self._clock,
    63|             max_len=1000,
    64|             expiry_ms=120 * 1000,
    65|             reset_expiry_on_get=False,
    66|         )
    67|     def _clear_tried_cache(self):
    68|         """Clear pdu_destination_tried cache"""
    69|         now = self._clock.time_msec()
    70|         old_dict = self.pdu_destination_tried
    71|         self.pdu_destination_tried = {}
    72|         for event_id, destination_dict in old_dict.items():
    73|             destination_dict = {

# --- HUNK 2: Lines 370-410 ---
   370|                 else:
   371|                     logger.warning(
   372|                         "Failed to %s via %s: %i %s",
   373|                         description,
   374|                         destination,
   375|                         e.code,
   376|                         e.args[0],
   377|                     )
   378|             except Exception:
   379|                 logger.warning(
   380|                     "Failed to %s via %s", description, destination, exc_info=True
   381|                 )
   382|         raise SynapseError(502, "Failed to %s via any server" % (description,))
   383|     async def make_membership_event(
   384|         self,
   385|         destinations: Iterable[str],
   386|         room_id: str,
   387|         user_id: str,
   388|         membership: str,
   389|         content: dict,
   390|         params: Dict[str, str],
   391|     ) -> Tuple[str, EventBase, RoomVersion]:
   392|         """
   393|         Creates an m.room.member event, with context, without participating in the room.
   394|         Does so by asking one of the already participating servers to create an
   395|         event with proper context.
   396|         Returns a fully signed and hashed event.
   397|         Note that this does not append any events to any graphs.
   398|         Args:
   399|             destinations: Candidate homeservers which are probably
   400|                 participating in the room.
   401|             room_id: The room in which the event will happen.
   402|             user_id: The user whose membership is being evented.
   403|             membership: The "membership" property of the event. Must be one of
   404|                 "join" or "leave".
   405|             content: Any additional data to put into the content field of the
   406|                 event.
   407|             params: Query parameters to include in the request.
   408|         Returns:
   409|             `(origin, event, room_version)` where origin is the remote
   410|             homeserver which generated the event, and room_version is the


# ====================================================================
# FILE: synapse/federation/federation_server.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 1-34 ---
     1| import logging
     2| from typing import (
     3|     TYPE_CHECKING,
     4|     Any,
     5|     Awaitable,
     6|     Callable,
     7|     Dict,
     8|     List,
     9|     Match,
    10|     Optional,
    11|     Tuple,
    12|     Union,
    13| )
    14| from prometheus_client import Counter, Histogram
    15| from twisted.internet import defer
    16| from twisted.internet.abstract import isIPAddress
    17| from twisted.python import failure
    18| from synapse.api.constants import EventTypes, Membership
    19| from synapse.api.errors import (
    20|     AuthError,
    21|     Codes,
    22|     FederationError,
    23|     IncompatibleRoomVersionError,
    24|     NotFoundError,
    25|     SynapseError,
    26|     UnsupportedRoomVersionError,
    27| )
    28| from synapse.api.room_versions import KNOWN_ROOM_VERSIONS
    29| from synapse.events import EventBase
    30| from synapse.federation.federation_base import FederationBase, event_from_pdu_json
    31| from synapse.federation.persistence import TransactionActions
    32| from synapse.federation.units import Edu, Transaction
    33| from synapse.http.endpoint import parse_server_name
    34| from synapse.logging.context import (

# --- HUNK 2: Lines 41-98 ---
    41| from synapse.replication.http.federation import (
    42|     ReplicationFederationSendEduRestServlet,
    43|     ReplicationGetQueryRestServlet,
    44| )
    45| from synapse.types import JsonDict, get_domain_from_id
    46| from synapse.util import glob_to_regex, json_decoder, unwrapFirstError
    47| from synapse.util.async_helpers import Linearizer, concurrently_execute
    48| from synapse.util.caches.response_cache import ResponseCache
    49| if TYPE_CHECKING:
    50|     from synapse.server import HomeServer
    51| TRANSACTION_CONCURRENCY_LIMIT = 10
    52| logger = logging.getLogger(__name__)
    53| received_pdus_counter = Counter("synapse_federation_server_received_pdus", "")
    54| received_edus_counter = Counter("synapse_federation_server_received_edus", "")
    55| received_queries_counter = Counter(
    56|     "synapse_federation_server_received_queries", "", ["type"]
    57| )
    58| pdu_process_time = Histogram(
    59|     "synapse_federation_server_pdu_process_time", "Time taken to process an event",
    60| )
    61| class FederationServer(FederationBase):
    62|     def __init__(self, hs):
    63|         super(FederationServer, self).__init__(hs)
    64|         self.auth = hs.get_auth()
    65|         self.handler = hs.get_handlers().federation_handler
    66|         self.state = hs.get_state_handler()
    67|         self.device_handler = hs.get_device_handler()
    68|         self._federation_ratelimiter = hs.get_federation_ratelimiter()
    69|         self._server_linearizer = Linearizer("fed_server")
    70|         self._transaction_linearizer = Linearizer("fed_txn_handler")
    71|         self._transaction_resp_cache = ResponseCache(
    72|             hs, "fed_txn_handler", timeout_ms=30000
    73|         )
    74|         self.transaction_actions = TransactionActions(self.store)
    75|         self.registry = hs.get_federation_registry()
    76|         self._state_resp_cache = ResponseCache(hs, "state_resp", timeout_ms=30000)
    77|         self._state_ids_resp_cache = ResponseCache(
    78|             hs, "state_ids_resp", timeout_ms=30000
    79|         )
    80|     async def on_backfill_request(
    81|         self, origin: str, room_id: str, versions: List[str], limit: int
    82|     ) -> Tuple[int, Dict[str, Any]]:
    83|         with (await self._server_linearizer.queue((origin, room_id))):
    84|             origin_host, _ = parse_server_name(origin)
    85|             await self.check_server_matches_acl(origin_host, room_id)
    86|             pdus = await self.handler.on_backfill_request(
    87|                 origin, room_id, versions, limit
    88|             )
    89|             res = self._transaction_from_pdus(pdus).get_dict()
    90|         return 200, res
    91|     async def on_incoming_transaction(
    92|         self, origin: str, transaction_data: JsonDict
    93|     ) -> Tuple[int, Dict[str, Any]]:
    94|         request_time = self._clock.time_msec()
    95|         transaction = Transaction(**transaction_data)
    96|         transaction_id = transaction.transaction_id  # type: ignore
    97|         if not transaction_id:
    98|             raise Exception("Transaction missing transaction_id")

# --- HUNK 3: Lines 155-252 ---
   155|         )
   156|         response = {"pdus": pdu_results}
   157|         logger.debug("Returning: %s", str(response))
   158|         await self.transaction_actions.set_response(origin, transaction, 200, response)
   159|         return 200, response
   160|     async def _handle_pdus_in_txn(
   161|         self, origin: str, transaction: Transaction, request_time: int
   162|     ) -> Dict[str, dict]:
   163|         """Process the PDUs in a received transaction.
   164|         Args:
   165|             origin: the server making the request
   166|             transaction: incoming transaction
   167|             request_time: timestamp that the HTTP request arrived at
   168|         Returns:
   169|             A map from event ID of a processed PDU to any errors we should
   170|             report back to the sending server.
   171|         """
   172|         received_pdus_counter.inc(len(transaction.pdus))  # type: ignore
   173|         origin_host, _ = parse_server_name(origin)
   174|         pdus_by_room = {}  # type: Dict[str, List[EventBase]]
   175|         for p in transaction.pdus:  # type: ignore
   176|             if "unsigned" in p:
   177|                 unsigned = p["unsigned"]
   178|                 if "age" in unsigned:
   179|                     p["age"] = unsigned["age"]
   180|             if "age" in p:
   181|                 p["age_ts"] = request_time - int(p["age"])
   182|                 del p["age"]
   183|             possible_event_id = p.get("event_id", "<Unknown>")
   184|             room_id = p.get("room_id")
   185|             if not room_id:
   186|                 logger.info(
   187|                     "Ignoring PDU as does not have a room_id. Event ID: %s",
   188|                     possible_event_id,
   189|                 )
   190|                 continue
   191|             try:
   192|                 room_version = await self.store.get_room_version(room_id)
   193|             except NotFoundError:
   194|                 logger.info("Ignoring PDU for unknown room_id: %s", room_id)
   195|                 continue
   196|             except UnsupportedRoomVersionError as e:
   197|                 logger.info("Ignoring PDU: %s", e)
   198|                 continue
   199|             event = event_from_pdu_json(p, room_version)
   200|             pdus_by_room.setdefault(room_id, []).append(event)
   201|         pdu_results = {}
   202|         async def process_pdus_for_room(room_id: str):
   203|             logger.debug("Processing PDUs for %s", room_id)
   204|             try:
   205|                 await self.check_server_matches_acl(origin_host, room_id)
   206|             except AuthError as e:
   207|                 logger.warning("Ignoring PDUs for room %s from banned server", room_id)
   208|                 for pdu in pdus_by_room[room_id]:
   209|                     event_id = pdu.event_id
   210|                     pdu_results[event_id] = e.error_dict()
   211|                 return
   212|             for pdu in pdus_by_room[room_id]:
   213|                 event_id = pdu.event_id
   214|                 with pdu_process_time.time():
   215|                     with nested_logging_context(event_id):
   216|                         try:
   217|                             await self._handle_received_pdu(origin, pdu)
   218|                             pdu_results[event_id] = {}
   219|                         except FederationError as e:
   220|                             logger.warning("Error handling PDU %s: %s", event_id, e)
   221|                             pdu_results[event_id] = {"error": str(e)}
   222|                         except Exception as e:
   223|                             f = failure.Failure()
   224|                             pdu_results[event_id] = {"error": str(e)}
   225|                             logger.error(
   226|                                 "Failed to handle PDU %s",
   227|                                 event_id,
   228|                                 exc_info=(f.type, f.value, f.getTracebackObject()),
   229|                             )
   230|         await concurrently_execute(
   231|             process_pdus_for_room, pdus_by_room.keys(), TRANSACTION_CONCURRENCY_LIMIT
   232|         )
   233|         return pdu_results
   234|     async def _handle_edus_in_txn(self, origin: str, transaction: Transaction):
   235|         """Process the EDUs in a received transaction.
   236|         """
   237|         async def _process_edu(edu_dict):
   238|             received_edus_counter.inc()
   239|             edu = Edu(
   240|                 origin=origin,
   241|                 destination=self.server_name,
   242|                 edu_type=edu_dict["edu_type"],
   243|                 content=edu_dict["content"],
   244|             )
   245|             await self.registry.on_edu(edu.edu_type, origin, edu.content)
   246|         await concurrently_execute(
   247|             _process_edu,
   248|             getattr(transaction, "edus", []),
   249|             TRANSACTION_CONCURRENCY_LIMIT,
   250|         )
   251|     async def on_context_state_request(
   252|         self, origin: str, room_id: str, event_id: str


# ====================================================================
# FILE: synapse/federation/sender/__init__.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 16-55 ---
    16|     run_in_background,
    17| )
    18| from synapse.metrics import (
    19|     LaterGauge,
    20|     event_processing_loop_counter,
    21|     event_processing_loop_room_count,
    22|     events_processed_counter,
    23| )
    24| from synapse.metrics.background_process_metrics import run_as_background_process
    25| from synapse.types import ReadReceipt
    26| from synapse.util.metrics import Measure, measure_func
    27| logger = logging.getLogger(__name__)
    28| sent_pdus_destination_dist_count = Counter(
    29|     "synapse_federation_client_sent_pdu_destinations:count",
    30|     "Number of PDUs queued for sending to one or more destinations",
    31| )
    32| sent_pdus_destination_dist_total = Counter(
    33|     "synapse_federation_client_sent_pdu_destinations:total",
    34|     "Total number of PDUs queued for sending across all destinations",
    35| )
    36| class FederationSender:
    37|     def __init__(self, hs: "synapse.server.HomeServer"):
    38|         self.hs = hs
    39|         self.server_name = hs.hostname
    40|         self.store = hs.get_datastore()
    41|         self.state = hs.get_state_handler()
    42|         self.clock = hs.get_clock()
    43|         self.is_mine_id = hs.is_mine_id
    44|         self._transaction_manager = TransactionManager(hs)
    45|         self._instance_name = hs.get_instance_name()
    46|         self._federation_shard_config = hs.config.worker.federation_shard_config
    47|         self._per_destination_queues = {}  # type: Dict[str, PerDestinationQueue]
    48|         LaterGauge(
    49|             "synapse_federation_transaction_queue_pending_destinations",
    50|             "",
    51|             [],
    52|             lambda: sum(
    53|                 1
    54|                 for d in self._per_destination_queues.values()
    55|                 if d.transmission_loop_running

# --- HUNK 2: Lines 63-102 ---
    63|             lambda: sum(
    64|                 d.pending_pdu_count() for d in self._per_destination_queues.values()
    65|             ),
    66|         )
    67|         LaterGauge(
    68|             "synapse_federation_transaction_queue_pending_edus",
    69|             "",
    70|             [],
    71|             lambda: sum(
    72|                 d.pending_edu_count() for d in self._per_destination_queues.values()
    73|             ),
    74|         )
    75|         self._is_processing = False
    76|         self._last_poked_id = -1
    77|         self._processing_pending_presence = False
    78|         self._queues_awaiting_rr_flush_by_room = (
    79|             {}
    80|         )  # type: Dict[str, Set[PerDestinationQueue]]
    81|         self._rr_txn_interval_per_room_ms = (
    82|             1000.0 / hs.config.federation_rr_transactions_per_room_per_second
    83|         )
    84|     def _get_per_destination_queue(self, destination: str) -> PerDestinationQueue:
    85|         """Get or create a PerDestinationQueue for the given destination
    86|         Args:
    87|             destination: server_name of remote server
    88|         """
    89|         queue = self._per_destination_queues.get(destination)
    90|         if not queue:
    91|             queue = PerDestinationQueue(self.hs, self._transaction_manager, destination)
    92|             self._per_destination_queues[destination] = queue
    93|         return queue
    94|     def notify_new_events(self, current_id: int) -> None:
    95|         """This gets called when we have some new events we might want to
    96|         send out to other servers.
    97|         """
    98|         self._last_poked_id = max(current_id, self._last_poked_id)
    99|         if self._is_processing:
   100|             return
   101|         run_as_background_process(
   102|             "process_event_queue_for_federation", self._process_event_queue_loop

# --- HUNK 3: Lines 123-163 ---
   123|                         destinations = await self.state.get_hosts_in_room_at_events(
   124|                             event.room_id, event_ids=event.prev_event_ids()
   125|                         )
   126|                     except Exception:
   127|                         logger.exception(
   128|                             "Failed to calculate hosts in room for event: %s",
   129|                             event.event_id,
   130|                         )
   131|                         return
   132|                     destinations = {
   133|                         d
   134|                         for d in destinations
   135|                         if self._federation_shard_config.should_handle(
   136|                             self._instance_name, d
   137|                         )
   138|                     }
   139|                     if send_on_behalf_of is not None:
   140|                         destinations.discard(send_on_behalf_of)
   141|                     logger.debug("Sending %s to %r", event, destinations)
   142|                     if destinations:
   143|                         self._send_pdu(event, destinations)
   144|                         now = self.clock.time_msec()
   145|                         ts = await self.store.get_received_ts(event.event_id)
   146|                         synapse.metrics.event_processing_lag_by_event.labels(
   147|                             "federation_sender"
   148|                         ).observe((now - ts) / 1000)
   149|                 async def handle_room_events(events: Iterable[EventBase]) -> None:
   150|                     with Measure(self.clock, "handle_room_events"):
   151|                         for event in events:
   152|                             await handle_event(event)
   153|                 events_by_room = {}  # type: Dict[str, List[EventBase]]
   154|                 for event in events:
   155|                     events_by_room.setdefault(event.room_id, []).append(event)
   156|                 await make_deferred_yieldable(
   157|                     defer.gatherResults(
   158|                         [
   159|                             run_in_background(handle_room_events, evs)
   160|                             for evs in events_by_room.values()
   161|                         ],
   162|                         consumeErrors=True,
   163|                     )

# --- HUNK 4: Lines 165-212 ---
   165|                 await self.store.update_federation_out_pos("events", next_token)
   166|                 if events:
   167|                     now = self.clock.time_msec()
   168|                     ts = await self.store.get_received_ts(events[-1].event_id)
   169|                     synapse.metrics.event_processing_lag.labels(
   170|                         "federation_sender"
   171|                     ).set(now - ts)
   172|                     synapse.metrics.event_processing_last_ts.labels(
   173|                         "federation_sender"
   174|                     ).set(ts)
   175|                     events_processed_counter.inc(len(events))
   176|                     event_processing_loop_room_count.labels("federation_sender").inc(
   177|                         len(events_by_room)
   178|                     )
   179|                 event_processing_loop_counter.labels("federation_sender").inc()
   180|                 synapse.metrics.event_processing_positions.labels(
   181|                     "federation_sender"
   182|                 ).set(next_token)
   183|         finally:
   184|             self._is_processing = False
   185|     def _send_pdu(self, pdu: EventBase, destinations: Iterable[str]) -> None:
   186|         destinations = set(destinations)
   187|         destinations.discard(self.server_name)
   188|         logger.debug("Sending to: %s", str(destinations))
   189|         if not destinations:
   190|             return
   191|         sent_pdus_destination_dist_total.inc(len(destinations))
   192|         sent_pdus_destination_dist_count.inc()
   193|         for destination in destinations:
   194|             self._get_per_destination_queue(destination).send_pdu(pdu)
   195|     async def send_read_receipt(self, receipt: ReadReceipt) -> None:
   196|         """Send a RR to any other servers in the room
   197|         Args:
   198|             receipt: receipt to be sent
   199|         """
   200|         room_id = receipt.room_id
   201|         domains_set = await self.state.get_current_hosts_in_room(room_id)
   202|         domains = [
   203|             d
   204|             for d in domains_set
   205|             if d != self.server_name
   206|             and self._federation_shard_config.should_handle(self._instance_name, d)
   207|         ]
   208|         if not domains:
   209|             return
   210|         queues_pending_flush = self._queues_awaiting_rr_flush_by_room.get(room_id)
   211|         if queues_pending_flush is not None:
   212|             logger.debug("Queuing receipt for: %r", domains)

# --- HUNK 5: Lines 345-364 ---
   345|         """Called when we want to retry sending transactions to a remote.
   346|         This is mainly useful if the remote server has been down and we think it
   347|         might have come back.
   348|         """
   349|         if destination == self.server_name:
   350|             logger.warning("Not waking up ourselves")
   351|             return
   352|         if not self._federation_shard_config.should_handle(
   353|             self._instance_name, destination
   354|         ):
   355|             return
   356|         self._get_per_destination_queue(destination).attempt_new_transaction()
   357|     @staticmethod
   358|     def get_current_token() -> int:
   359|         return 0
   360|     @staticmethod
   361|     async def get_replication_rows(
   362|         instance_name: str, from_token: int, to_token: int, target_row_count: int
   363|     ) -> Tuple[List[Tuple[int, Tuple]], int, bool]:
   364|         return [], 0, False


# ====================================================================
# FILE: synapse/federation/sender/per_destination_queue.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 1-23 ---
     1| import datetime
     2| import logging
     3| from typing import TYPE_CHECKING, Dict, Hashable, Iterable, List, Tuple
     4| from prometheus_client import Counter
     5| from synapse.api.errors import (
     6|     FederationDeniedError,
     7|     HttpResponseException,
     8|     RequestSendFailed,
     9| )
    10| from synapse.api.presence import UserPresenceState
    11| from synapse.events import EventBase
    12| from synapse.federation.units import Edu
    13| from synapse.handlers.presence import format_user_presence_state
    14| from synapse.metrics import sent_transactions_counter
    15| from synapse.metrics.background_process_metrics import run_as_background_process
    16| from synapse.types import ReadReceipt
    17| from synapse.util.retryutils import NotRetryingDestination, get_retry_limiter
    18| if TYPE_CHECKING:
    19|     import synapse.server
    20| MAX_EDUS_PER_TRANSACTION = 100
    21| logger = logging.getLogger(__name__)
    22| sent_edus_counter = Counter(
    23|     "synapse_federation_client_sent_edus", "Total number of EDUs successfully sent"

# --- HUNK 2: Lines 41-104 ---
    41|         hs: "synapse.server.HomeServer",
    42|         transaction_manager: "synapse.federation.sender.TransactionManager",
    43|         destination: str,
    44|     ):
    45|         self._server_name = hs.hostname
    46|         self._clock = hs.get_clock()
    47|         self._store = hs.get_datastore()
    48|         self._transaction_manager = transaction_manager
    49|         self._instance_name = hs.get_instance_name()
    50|         self._federation_shard_config = hs.config.worker.federation_shard_config
    51|         self._should_send_on_this_instance = True
    52|         if not self._federation_shard_config.should_handle(
    53|             self._instance_name, destination
    54|         ):
    55|             logger.error(
    56|                 "Create a per destination queue for %s on wrong worker", destination,
    57|             )
    58|             self._should_send_on_this_instance = False
    59|         self._destination = destination
    60|         self.transmission_loop_running = False
    61|         self._pending_pdus = []  # type: List[EventBase]
    62|         self._pending_edus = []  # type: List[Edu]
    63|         self._pending_edus_keyed = {}  # type: Dict[Tuple[str, Hashable], Edu]
    64|         self._pending_presence = {}  # type: Dict[str, UserPresenceState]
    65|         self._pending_rrs = {}  # type: Dict[str, Dict[str, Dict[str, dict]]]
    66|         self._rrs_pending_flush = False
    67|         self._last_device_stream_id = 0
    68|         self._last_device_list_stream_id = 0
    69|     def __str__(self) -> str:
    70|         return "PerDestinationQueue[%s]" % self._destination
    71|     def pending_pdu_count(self) -> int:
    72|         return len(self._pending_pdus)
    73|     def pending_edu_count(self) -> int:
    74|         return (
    75|             len(self._pending_edus)
    76|             + len(self._pending_presence)
    77|             + len(self._pending_edus_keyed)
    78|         )
    79|     def send_pdu(self, pdu: EventBase) -> None:
    80|         """Add a PDU to the queue, and start the transmission loop if necessary
    81|         Args:
    82|             pdu: pdu to send
    83|         """
    84|         self._pending_pdus.append(pdu)
    85|         self.attempt_new_transaction()
    86|     def send_presence(self, states: Iterable[UserPresenceState]) -> None:
    87|         """Add presence updates to the queue. Start the transmission loop if necessary.
    88|         Args:
    89|             states: presence to send
    90|         """
    91|         self._pending_presence.update({state.user_id: state for state in states})
    92|         self.attempt_new_transaction()
    93|     def queue_read_receipt(self, receipt: ReadReceipt) -> None:
    94|         """Add a RR to the list to be sent. Doesn't start the transmission loop yet
    95|         (see flush_read_receipts_for_room)
    96|         Args:
    97|             receipt: receipt to be queued
    98|         """
    99|         self._pending_rrs.setdefault(receipt.room_id, {}).setdefault(
   100|             receipt.receipt_type, {}
   101|         )[receipt.user_id] = {"event_ids": receipt.event_ids, "data": receipt.data}
   102|     def flush_read_receipts_for_room(self, room_id: str) -> None:
   103|         if room_id not in self._pending_rrs:
   104|             return

# --- HUNK 3: Lines 117-156 ---
   117|         transaction in the background.
   118|         """
   119|         if self.transmission_loop_running:
   120|             logger.debug("TX [%s] Transaction already in progress", self._destination)
   121|             return
   122|         if not self._should_send_on_this_instance:
   123|             logger.error(
   124|                 "Trying to start a transaction to %s on wrong worker", self._destination
   125|             )
   126|             return
   127|         logger.debug("TX [%s] Starting transaction loop", self._destination)
   128|         run_as_background_process(
   129|             "federation_transaction_transmission_loop",
   130|             self._transaction_transmission_loop,
   131|         )
   132|     async def _transaction_transmission_loop(self) -> None:
   133|         pending_pdus = []  # type: List[EventBase]
   134|         try:
   135|             self.transmission_loop_running = True
   136|             await get_retry_limiter(self._destination, self._clock, self._store)
   137|             pending_pdus = []
   138|             while True:
   139|                 limit = MAX_EDUS_PER_TRANSACTION - 2
   140|                 device_update_edus, dev_list_id = await self._get_device_update_edus(
   141|                     limit
   142|                 )
   143|                 limit -= len(device_update_edus)
   144|                 (
   145|                     to_device_edus,
   146|                     device_stream_id,
   147|                 ) = await self._get_to_device_message_edus(limit)
   148|                 pending_edus = device_update_edus + to_device_edus
   149|                 pending_pdus = self._pending_pdus
   150|                 pending_pdus, self._pending_pdus = pending_pdus[:50], pending_pdus[50:]
   151|                 pending_edus.extend(self._get_rr_edus(force_flush=False))
   152|                 pending_presence = self._pending_presence
   153|                 self._pending_presence = {}
   154|                 if pending_presence:
   155|                     pending_edus.append(
   156|                         Edu(

# --- HUNK 4: Lines 192-273 ---
   192|                     self._destination, pending_pdus, pending_edus
   193|                 )
   194|                 if success:
   195|                     sent_transactions_counter.inc()
   196|                     sent_edus_counter.inc(len(pending_edus))
   197|                     for edu in pending_edus:
   198|                         sent_edus_by_type.labels(edu.edu_type).inc()
   199|                     if to_device_edus:
   200|                         await self._store.delete_device_msgs_for_remote(
   201|                             self._destination, device_stream_id
   202|                         )
   203|                     if device_update_edus:
   204|                         logger.info(
   205|                             "Marking as sent %r %r", self._destination, dev_list_id
   206|                         )
   207|                         await self._store.mark_as_sent_devices_by_remote(
   208|                             self._destination, dev_list_id
   209|                         )
   210|                     self._last_device_stream_id = device_stream_id
   211|                     self._last_device_list_stream_id = dev_list_id
   212|                 else:
   213|                     break
   214|         except NotRetryingDestination as e:
   215|             logger.debug(
   216|                 "TX [%s] not ready for retry yet (next retry at %s) - "
   217|                 "dropping transaction for now",
   218|                 self._destination,
   219|                 datetime.datetime.fromtimestamp(
   220|                     (e.retry_last_ts + e.retry_interval) / 1000.0
   221|                 ),
   222|             )
   223|             if e.retry_interval > 60 * 60 * 1000:
   224|                 self._pending_pdus = []
   225|                 self._pending_edus = []
   226|                 self._pending_edus_keyed = {}
   227|                 self._pending_presence = {}
   228|                 self._pending_rrs = {}
   229|         except FederationDeniedError as e:
   230|             logger.info(e)
   231|         except HttpResponseException as e:
   232|             logger.warning(
   233|                 "TX [%s] Received %d response to transaction: %s",
   234|                 self._destination,
   235|                 e.code,
   236|                 e,
   237|             )
   238|         except RequestSendFailed as e:
   239|             logger.warning(
   240|                 "TX [%s] Failed to send transaction: %s", self._destination, e
   241|             )
   242|             for p in pending_pdus:
   243|                 logger.info(
   244|                     "Failed to send event %s to %s", p.event_id, self._destination
   245|                 )
   246|         except Exception:
   247|             logger.exception("TX [%s] Failed to send transaction", self._destination)
   248|             for p in pending_pdus:
   249|                 logger.info(
   250|                     "Failed to send event %s to %s", p.event_id, self._destination
   251|                 )
   252|         finally:
   253|             self.transmission_loop_running = False
   254|     def _get_rr_edus(self, force_flush: bool) -> Iterable[Edu]:
   255|         if not self._pending_rrs:
   256|             return
   257|         if not force_flush and not self._rrs_pending_flush:
   258|             return
   259|         edu = Edu(
   260|             origin=self._server_name,
   261|             destination=self._destination,
   262|             edu_type="m.receipt",
   263|             content=self._pending_rrs,
   264|         )
   265|         self._pending_rrs = {}
   266|         self._rrs_pending_flush = False
   267|         yield edu
   268|     def _pop_pending_edus(self, limit: int) -> List[Edu]:
   269|         pending_edus = self._pending_edus
   270|         pending_edus, self._pending_edus = pending_edus[:limit], pending_edus[limit:]
   271|         return pending_edus
   272|     async def _get_device_update_edus(self, limit: int) -> Tuple[List[Edu], int]:
   273|         last_device_list = self._last_device_list_stream_id

# --- HUNK 5: Lines 284-303 ---
   284|             for (edu_type, content) in results
   285|         ]
   286|         assert len(edus) <= limit, "get_device_updates_by_remote returned too many EDUs"
   287|         return (edus, now_stream_id)
   288|     async def _get_to_device_message_edus(self, limit: int) -> Tuple[List[Edu], int]:
   289|         last_device_stream_id = self._last_device_stream_id
   290|         to_device_stream_id = self._store.get_to_device_stream_token()
   291|         contents, stream_id = await self._store.get_new_device_msgs_for_remote(
   292|             self._destination, last_device_stream_id, to_device_stream_id, limit
   293|         )
   294|         edus = [
   295|             Edu(
   296|                 origin=self._server_name,
   297|                 destination=self._destination,
   298|                 edu_type="m.direct_to_device",
   299|                 content=content,
   300|             )
   301|             for content in contents
   302|         ]
   303|         return (edus, stream_id)


# ====================================================================
# FILE: synapse/federation/sender/transaction_manager.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-48 ---
     1| import logging
     2| from typing import TYPE_CHECKING, List
     3| from synapse.api.errors import HttpResponseException
     4| from synapse.events import EventBase
     5| from synapse.federation.persistence import TransactionActions
     6| from synapse.federation.units import Edu, Transaction
     7| from synapse.logging.opentracing import (
     8|     extract_text_map,
     9|     set_tag,
    10|     start_active_span_follows_from,
    11|     tags,
    12|     whitelisted_homeserver,
    13| )
    14| from synapse.util import json_decoder
    15| from synapse.util.metrics import measure_func
    16| if TYPE_CHECKING:
    17|     import synapse.server
    18| logger = logging.getLogger(__name__)
    19| class TransactionManager:
    20|     """Helper class which handles building and sending transactions
    21|     shared between PerDestinationQueue objects
    22|     """
    23|     def __init__(self, hs: "synapse.server.HomeServer"):
    24|         self._server_name = hs.hostname
    25|         self.clock = hs.get_clock()  # nb must be called this for @measure_func
    26|         self._store = hs.get_datastore()
    27|         self._transaction_actions = TransactionActions(self._store)
    28|         self._transport_layer = hs.get_federation_transport_client()
    29|         self._next_txn_id = int(self.clock.time_msec())
    30|     @measure_func("_send_new_transaction")
    31|     async def send_new_transaction(
    32|         self, destination: str, pdus: List[EventBase], edus: List[Edu],
    33|     ) -> bool:
    34|         """
    35|         Args:
    36|             destination: The destination to send to (e.g. 'example.org')
    37|             pdus: In-order list of PDUs to send
    38|             edus: List of EDUs to send
    39|         Returns:
    40|             True iff the transaction was successful
    41|         """
    42|         span_contexts = []
    43|         keep_destination = whitelisted_homeserver(destination)
    44|         for edu in edus:
    45|             context = edu.get_context()
    46|             if context:
    47|                 span_contexts.append(extract_text_map(json_decoder.decode(context)))
    48|             if keep_destination:

# --- HUNK 2: Lines 101-122 ---
   101|             logger.info("TX [%s] {%s} got %d response", destination, txn_id, code)
   102|             if code == 200:
   103|                 for e_id, r in response.get("pdus", {}).items():
   104|                     if "error" in r:
   105|                         logger.warning(
   106|                             "TX [%s] {%s} Remote returned error for %s: %s",
   107|                             destination,
   108|                             txn_id,
   109|                             e_id,
   110|                             r,
   111|                         )
   112|             else:
   113|                 for p in pdus:
   114|                     logger.warning(
   115|                         "TX [%s] {%s} Failed to send event %s",
   116|                         destination,
   117|                         txn_id,
   118|                         p.event_id,
   119|                     )
   120|                 success = False
   121|             set_tag(tags.ERROR, not success)
   122|             return success


# ====================================================================
# FILE: synapse/federation/transport/server.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 26-66 ---
    26|     whitelisted_homeserver,
    27| )
    28| from synapse.server import HomeServer
    29| from synapse.types import ThirdPartyInstanceID, get_domain_from_id
    30| from synapse.util.versionstring import get_version_string
    31| logger = logging.getLogger(__name__)
    32| class TransportLayerServer(JsonResource):
    33|     """Handles incoming federation HTTP requests"""
    34|     def __init__(self, hs, servlet_groups=None):
    35|         """Initialize the TransportLayerServer
    36|         Will by default register all servlets. For custom behaviour, pass in
    37|         a list of servlet_groups to register.
    38|         Args:
    39|             hs (synapse.server.HomeServer): homeserver
    40|             servlet_groups (list[str], optional): List of servlet groups to register.
    41|                 Defaults to ``DEFAULT_SERVLET_GROUPS``.
    42|         """
    43|         self.hs = hs
    44|         self.clock = hs.get_clock()
    45|         self.servlet_groups = servlet_groups
    46|         super(TransportLayerServer, self).__init__(hs, canonical_json=False)
    47|         self.authenticator = Authenticator(hs)
    48|         self.ratelimiter = hs.get_federation_ratelimiter()
    49|         self.register_servlets()
    50|     def register_servlets(self):
    51|         register_servlets(
    52|             self.hs,
    53|             resource=self,
    54|             ratelimiter=self.ratelimiter,
    55|             authenticator=self.authenticator,
    56|             servlet_groups=self.servlet_groups,
    57|         )
    58| class AuthenticationError(SynapseError):
    59|     """There was a problem authenticating the request"""
    60|     pass
    61| class NoAuthenticationError(AuthenticationError):
    62|     """The request had no authentication information"""
    63|     pass
    64| class Authenticator:
    65|     def __init__(self, hs: HomeServer):
    66|         self._clock = hs.get_clock()

# --- HUNK 2: Lines 248-290 ---
   248|                         )
   249|                 else:
   250|                     response = await func(
   251|                         origin, content, request.args, *args, **kwargs
   252|                     )
   253|             return response
   254|         return new_func
   255|     def register(self, server):
   256|         pattern = re.compile("^" + self.PREFIX + self.PATH + "$")
   257|         for method in ("GET", "PUT", "POST"):
   258|             code = getattr(self, "on_%s" % (method), None)
   259|             if code is None:
   260|                 continue
   261|             server.register_paths(
   262|                 method, (pattern,), self._wrap(code), self.__class__.__name__,
   263|             )
   264| class FederationSendServlet(BaseFederationServlet):
   265|     PATH = "/send/(?P<transaction_id>[^/]*)/?"
   266|     RATELIMIT = False
   267|     def __init__(self, handler, server_name, **kwargs):
   268|         super(FederationSendServlet, self).__init__(
   269|             handler, server_name=server_name, **kwargs
   270|         )
   271|         self.server_name = server_name
   272|     async def on_PUT(self, origin, content, query, transaction_id):
   273|         """ Called on PUT /send/<transaction_id>/
   274|         Args:
   275|             request (twisted.web.http.Request): The HTTP request.
   276|             transaction_id (str): The transaction_id associated with this
   277|                 request. This is *not* None.
   278|         Returns:
   279|             Tuple of `(code, response)`, where
   280|             `response` is a python dict to be converted into JSON that is
   281|             used as the response body.
   282|         """
   283|         try:
   284|             transaction_data = content
   285|             logger.debug("Decoded %s: %s", transaction_id, str(transaction_data))
   286|             logger.info(
   287|                 "Received txn %s from %s. (PDUs: %d, EDUs: %d)",
   288|                 transaction_id,
   289|                 origin,
   290|                 len(transaction_data.get("pdus", [])),

# --- HUNK 3: Lines 513-555 ---
   513|     Content-Type: application/json
   514|     {
   515|         "chunk": [
   516|             {
   517|                 "aliases": [
   518|                     "#test:localhost"
   519|                 ],
   520|                 "guest_can_join": false,
   521|                 "name": "test room",
   522|                 "num_joined_members": 3,
   523|                 "room_id": "!whkydVegtvatLfXmPN:localhost",
   524|                 "world_readable": false
   525|             }
   526|         ],
   527|         "end": "END",
   528|         "start": "START"
   529|     }
   530|     """
   531|     PATH = "/publicRooms"
   532|     def __init__(self, handler, authenticator, ratelimiter, server_name, allow_access):
   533|         super(PublicRoomList, self).__init__(
   534|             handler, authenticator, ratelimiter, server_name
   535|         )
   536|         self.allow_access = allow_access
   537|     async def on_GET(self, origin, content, query):
   538|         if not self.allow_access:
   539|             raise FederationDeniedError(origin)
   540|         limit = parse_integer_from_args(query, "limit", 0)
   541|         since_token = parse_string_from_args(query, "since", None)
   542|         include_all_networks = parse_boolean_from_args(
   543|             query, "include_all_networks", False
   544|         )
   545|         third_party_instance_id = parse_string_from_args(
   546|             query, "third_party_instance_id", None
   547|         )
   548|         if include_all_networks:
   549|             network_tuple = None
   550|         elif third_party_instance_id:
   551|             network_tuple = ThirdPartyInstanceID.from_string(third_party_instance_id)
   552|         else:
   553|             network_tuple = ThirdPartyInstanceID(None, None)
   554|         if limit == 0:
   555|             limit = None


# ====================================================================
# FILE: synapse/groups/groups_server.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 211-251 ---
   211|             requester_user_id, group_id
   212|         )
   213|         room_results = await self.store.get_rooms_in_group(
   214|             group_id, include_private=is_user_in_group
   215|         )
   216|         chunk = []
   217|         for room_result in room_results:
   218|             room_id = room_result["room_id"]
   219|             joined_users = await self.store.get_users_in_room(room_id)
   220|             entry = await self.room_list_handler.generate_room_entry(
   221|                 room_id, len(joined_users), with_alias=False, allow_private=True
   222|             )
   223|             if not entry:
   224|                 continue
   225|             entry["is_public"] = bool(room_result["is_public"])
   226|             chunk.append(entry)
   227|         chunk.sort(key=lambda e: -e["num_joined_members"])
   228|         return {"chunk": chunk, "total_room_count_estimate": len(room_results)}
   229| class GroupsServerHandler(GroupsServerWorkerHandler):
   230|     def __init__(self, hs):
   231|         super(GroupsServerHandler, self).__init__(hs)
   232|         hs.get_groups_attestation_renewer()
   233|     async def update_group_summary_room(
   234|         self, group_id, requester_user_id, room_id, category_id, content
   235|     ):
   236|         """Add/update a room to the group summary
   237|         """
   238|         await self.check_group_is_ours(
   239|             group_id, requester_user_id, and_exists=True, and_is_admin=requester_user_id
   240|         )
   241|         RoomID.from_string(room_id)  # Ensure valid room id
   242|         order = content.get("order", None)
   243|         is_public = _parse_visibility_from_contents(content)
   244|         await self.store.add_room_to_summary(
   245|             group_id=group_id,
   246|             room_id=room_id,
   247|             category_id=category_id,
   248|             order=order,
   249|             is_public=is_public,
   250|         )
   251|         return {}


# ====================================================================
# FILE: synapse/handlers/acme_issuing_service.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 30-70 ---
    30|             we will attach a child resource for "acme-challenge".
    31|     Returns:
    32|         AcmeIssuingService
    33|     """
    34|     responder = HTTP01Responder()
    35|     well_known_resource.putChild(b"acme-challenge", responder.resource)
    36|     store = ErsatzStore()
    37|     return AcmeIssuingService(
    38|         cert_store=store,
    39|         client_creator=(
    40|             lambda: Client.from_url(
    41|                 reactor=reactor,
    42|                 url=URL.from_text(acme_url),
    43|                 key=load_or_create_client_key(account_key_file),
    44|                 alg=RS256,
    45|             )
    46|         ),
    47|         clock=reactor,
    48|         responders=[responder],
    49|     )
    50| @attr.s
    51| @implementer(ICertificateStore)
    52| class ErsatzStore:
    53|     """
    54|     A store that only stores in memory.
    55|     """
    56|     certs = attr.ib(default=attr.Factory(dict))
    57|     def store(self, server_name, pem_objects):
    58|         self.certs[server_name] = [o.as_bytes() for o in pem_objects]
    59|         return defer.succeed(None)
    60| def load_or_create_client_key(key_file):
    61|     """Load the ACME account key from a file, creating it if it does not exist.
    62|     Args:
    63|         key_file (str): name of the file to use as the account key
    64|     """
    65|     acme_key_file = FilePath(key_file)
    66|     if acme_key_file.exists():
    67|         logger.info("Loading ACME account key from '%s'", acme_key_file)
    68|         key = serialization.load_pem_private_key(
    69|             acme_key_file.getContent(), password=None, backend=default_backend()
    70|         )


# ====================================================================
# FILE: synapse/handlers/admin.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| import logging
     2| from typing import List
     3| from synapse.api.constants import Membership
     4| from synapse.events import FrozenEvent
     5| from synapse.types import RoomStreamToken, StateMap
     6| from synapse.visibility import filter_events_for_client
     7| from ._base import BaseHandler
     8| logger = logging.getLogger(__name__)
     9| class AdminHandler(BaseHandler):
    10|     def __init__(self, hs):
    11|         super(AdminHandler, self).__init__(hs)
    12|         self.storage = hs.get_storage()
    13|         self.state_store = self.storage.state
    14|     async def get_whois(self, user):
    15|         connections = []
    16|         sessions = await self.store.get_user_ip_and_agents(user)
    17|         for session in sessions:
    18|             connections.append(
    19|                 {
    20|                     "ip": session["ip"],
    21|                     "last_seen": session["last_seen"],
    22|                     "user_agent": session["user_agent"],
    23|                 }
    24|             )
    25|         ret = {
    26|             "user_id": user.to_string(),
    27|             "devices": {"": {"sessions": [{"connections": connections}]}},
    28|         }
    29|         return ret
    30|     async def get_user(self, user):
    31|         """Function to get user details"""

# --- HUNK 2: Lines 60-101 ---
    60|             room_id = room.room_id
    61|             logger.info(
    62|                 "[%s] Handling room %s, %d/%d", user_id, room_id, index + 1, len(rooms)
    63|             )
    64|             forgotten = await self.store.did_forget(user_id, room_id)
    65|             if forgotten:
    66|                 logger.info("[%s] User forgot room %d, ignoring", user_id, room_id)
    67|                 continue
    68|             if room_id not in rooms_user_has_been_in:
    69|                 if room.membership == Membership.INVITE:
    70|                     event_id = room.event_id
    71|                     invite = await self.store.get_event(event_id, allow_none=True)
    72|                     if invite:
    73|                         invited_state = invite.unsigned["invite_room_state"]
    74|                         writer.write_invite(room_id, invite, invited_state)
    75|                 continue
    76|             if room.membership == Membership.JOIN:
    77|                 stream_ordering = self.store.get_room_max_stream_ordering()
    78|             else:
    79|                 stream_ordering = room.stream_ordering
    80|             from_key = str(RoomStreamToken(0, 0))
    81|             to_key = str(RoomStreamToken(None, stream_ordering))
    82|             written_events = set()  # Events that we've processed in this room
    83|             event_to_unseen_prevs = {}
    84|             unseen_to_child_events = {}
    85|             while True:
    86|                 events, _ = await self.store.paginate_room_events(
    87|                     room_id, from_key, to_key, limit=100, direction="f"
    88|                 )
    89|                 if not events:
    90|                     break
    91|                 from_key = events[-1].internal_metadata.after
    92|                 events = await filter_events_for_client(self.storage, user_id, events)
    93|                 writer.write_events(room_id, events)
    94|                 for event in events:
    95|                     unseen_events = set(event.prev_event_ids()) - written_events
    96|                     if unseen_events:
    97|                         event_to_unseen_prevs[event.event_id] = unseen_events
    98|                         for unseen in unseen_events:
    99|                             unseen_to_child_events.setdefault(unseen, set()).add(
   100|                                 event.event_id
   101|                             )


# ====================================================================
# FILE: synapse/handlers/auth.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 71-118 ---
    71|     Convert a phone login identifier type to a generic threepid identifier.
    72|     Args:
    73|         identifier: Login identifier dict of type 'm.id.phone'
    74|     Returns:
    75|         An equivalent m.id.thirdparty identifier dict
    76|     """
    77|     if "country" not in identifier or (
    78|         "phone" not in identifier
    79|         and "number" not in identifier
    80|     ):
    81|         raise SynapseError(
    82|             400, "Invalid phone-type identifier", errcode=Codes.INVALID_PARAM
    83|         )
    84|     phone_number = identifier.get("phone", identifier["number"])
    85|     msisdn = phone_number_to_msisdn(identifier["country"], phone_number)
    86|     return {
    87|         "type": "m.id.thirdparty",
    88|         "medium": "msisdn",
    89|         "address": msisdn,
    90|     }
    91| class AuthHandler(BaseHandler):
    92|     SESSION_EXPIRE_MS = 48 * 60 * 60 * 1000
    93|     def __init__(self, hs):
    94|         """
    95|         Args:
    96|             hs (synapse.server.HomeServer):
    97|         """
    98|         super(AuthHandler, self).__init__(hs)
    99|         self.checkers = {}  # type: Dict[str, UserInteractiveAuthChecker]
   100|         for auth_checker_class in INTERACTIVE_AUTH_CHECKERS:
   101|             inst = auth_checker_class(hs)
   102|             if inst.is_enabled():
   103|                 self.checkers[inst.AUTH_TYPE] = inst  # type: ignore
   104|         self.bcrypt_rounds = hs.config.bcrypt_rounds
   105|         account_handler = ModuleApi(hs, self)
   106|         self.password_providers = [
   107|             module(config=config, account_handler=account_handler)
   108|             for module, config in hs.config.password_providers
   109|         ]
   110|         logger.info("Extra password_providers: %r", self.password_providers)
   111|         self.hs = hs  # FIXME better possibility to access registrationHandler later?
   112|         self.macaroon_gen = hs.get_macaroon_generator()
   113|         self._password_enabled = hs.config.password_enabled
   114|         self._sso_enabled = (
   115|             hs.config.cas_enabled or hs.config.saml2_enabled or hs.config.oidc_enabled
   116|         )
   117|         login_types = []
   118|         if self._password_enabled:

# --- HUNK 2: Lines 131-170 ---
   131|             clock=self.clock,
   132|             rate_hz=self.hs.config.rc_login_failed_attempts.per_second,
   133|             burst_count=self.hs.config.rc_login_failed_attempts.burst_count,
   134|         )
   135|         self._clock = self.hs.get_clock()
   136|         if hs.config.worker_app is None:
   137|             self._clock.looping_call(
   138|                 run_as_background_process,
   139|                 5 * 60 * 1000,
   140|                 "expire_old_sessions",
   141|                 self._expire_old_sessions,
   142|             )
   143|         self._sso_redirect_confirm_template = hs.config.sso_redirect_confirm_template
   144|         self._sso_auth_confirm_template = hs.config.sso_auth_confirm_template
   145|         self._sso_auth_success_template = hs.config.sso_auth_success_template
   146|         self._sso_account_deactivated_template = (
   147|             hs.config.sso_account_deactivated_template
   148|         )
   149|         self._server_name = hs.config.server_name
   150|         self._whitelisted_sso_clients = tuple(hs.config.sso_client_whitelist)
   151|     async def validate_user_via_ui_auth(
   152|         self,
   153|         requester: Requester,
   154|         request: SynapseRequest,
   155|         request_body: Dict[str, Any],
   156|         clientip: str,
   157|         description: str,
   158|     ) -> Tuple[dict, str]:
   159|         """
   160|         Checks that the user is who they claim to be, via a UI auth.
   161|         This is used for things like device deletion and password reset where
   162|         the user already has a valid access token, but we want to double-check
   163|         that it isn't stolen by re-authenticating them.
   164|         Args:
   165|             requester: The user, as given by the access token
   166|             request: The request sent by the client.
   167|             request_body: The body of the request sent by the client
   168|             clientip: The IP address of the client.
   169|             description: A human readable string to be displayed to the user that
   170|                          describes the operation happening on their account.

# --- HUNK 3: Lines 798-885 ---
   798|     async def complete_sso_ui_auth(
   799|         self, registered_user_id: str, session_id: str, request: SynapseRequest,
   800|     ):
   801|         """Having figured out a mxid for this user, complete the HTTP request
   802|         Args:
   803|             registered_user_id: The registered user ID to complete SSO login for.
   804|             request: The request to complete.
   805|             client_redirect_url: The URL to which to redirect the user at the end of the
   806|                 process.
   807|         """
   808|         await self.store.mark_ui_auth_stage_complete(
   809|             session_id, LoginType.SSO, registered_user_id
   810|         )
   811|         html = self._sso_auth_success_template
   812|         respond_with_html(request, 200, html)
   813|     async def complete_sso_login(
   814|         self,
   815|         registered_user_id: str,
   816|         request: SynapseRequest,
   817|         client_redirect_url: str,
   818|     ):
   819|         """Having figured out a mxid for this user, complete the HTTP request
   820|         Args:
   821|             registered_user_id: The registered user ID to complete SSO login for.
   822|             request: The request to complete.
   823|             client_redirect_url: The URL to which to redirect the user at the end of the
   824|                 process.
   825|         """
   826|         deactivated = await self.store.get_user_deactivated_status(registered_user_id)
   827|         if deactivated:
   828|             respond_with_html(request, 403, self._sso_account_deactivated_template)
   829|             return
   830|         self._complete_sso_login(registered_user_id, request, client_redirect_url)
   831|     def _complete_sso_login(
   832|         self,
   833|         registered_user_id: str,
   834|         request: SynapseRequest,
   835|         client_redirect_url: str,
   836|     ):
   837|         """
   838|         The synchronous portion of complete_sso_login.
   839|         This exists purely for backwards compatibility of synapse.module_api.ModuleApi.
   840|         """
   841|         login_token = self.macaroon_gen.generate_short_term_login_token(
   842|             registered_user_id
   843|         )
   844|         redirect_url = self.add_query_param_to_url(
   845|             client_redirect_url, "loginToken", login_token
   846|         )
   847|         if client_redirect_url.startswith(self._whitelisted_sso_clients):
   848|             request.redirect(redirect_url)
   849|             finish_request(request)
   850|             return
   851|         redirect_url_no_params = client_redirect_url.split("?")[0]
   852|         html = self._sso_redirect_confirm_template.render(
   853|             display_url=redirect_url_no_params,
   854|             redirect_url=redirect_url,
   855|             server_name=self._server_name,
   856|         )
   857|         respond_with_html(request, 200, html)
   858|     @staticmethod
   859|     def add_query_param_to_url(url: str, param_name: str, param: Any):
   860|         url_parts = list(urllib.parse.urlparse(url))
   861|         query = dict(urllib.parse.parse_qsl(url_parts[4]))
   862|         query.update({param_name: param})
   863|         url_parts[4] = urllib.parse.urlencode(query)
   864|         return urllib.parse.urlunparse(url_parts)
   865| @attr.s
   866| class MacaroonGenerator:
   867|     hs = attr.ib()
   868|     def generate_access_token(
   869|         self, user_id: str, extra_caveats: Optional[List[str]] = None
   870|     ) -> str:
   871|         extra_caveats = extra_caveats or []
   872|         macaroon = self._generate_base_macaroon(user_id)
   873|         macaroon.add_first_party_caveat("type = access")
   874|         macaroon.add_first_party_caveat(
   875|             "nonce = %s" % (stringutils.random_string_with_symbols(16),)
   876|         )
   877|         for caveat in extra_caveats:
   878|             macaroon.add_first_party_caveat(caveat)
   879|         return macaroon.serialize()
   880|     def generate_short_term_login_token(
   881|         self, user_id: str, duration_in_ms: int = (2 * 60 * 1000)
   882|     ) -> str:
   883|         macaroon = self._generate_base_macaroon(user_id)
   884|         macaroon.add_first_party_caveat("type = login")
   885|         now = self.hs.get_clock().time_msec()


# ====================================================================
# FILE: synapse/handlers/deactivate_account.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| import logging
     2| from typing import Optional
     3| from synapse.api.errors import SynapseError
     4| from synapse.metrics.background_process_metrics import run_as_background_process
     5| from synapse.types import UserID, create_requester
     6| from ._base import BaseHandler
     7| logger = logging.getLogger(__name__)
     8| class DeactivateAccountHandler(BaseHandler):
     9|     """Handler which deals with deactivating user accounts."""
    10|     def __init__(self, hs):
    11|         super(DeactivateAccountHandler, self).__init__(hs)
    12|         self.hs = hs
    13|         self._auth_handler = hs.get_auth_handler()
    14|         self._device_handler = hs.get_device_handler()
    15|         self._room_member_handler = hs.get_room_member_handler()
    16|         self._identity_handler = hs.get_handlers().identity_handler
    17|         self.user_directory_handler = hs.get_user_directory_handler()
    18|         self._user_parter_running = False
    19|         if hs.config.worker_app is None:
    20|             hs.get_reactor().callWhenRunning(self._start_user_parting)
    21|         self._account_validity_enabled = hs.config.account_validity.enabled
    22|     async def deactivate_account(
    23|         self, user_id: str, erase_data: bool, id_server: Optional[str] = None
    24|     ) -> bool:
    25|         """Deactivate a user's account
    26|         Args:
    27|             user_id: ID of user to be deactivated
    28|             erase_data: whether to GDPR-erase the user's data
    29|             id_server: Use the given identity server when unbinding
    30|                 any threepids. If None then will attempt to unbind using the
    31|                 identity server specified when binding (if known).


# ====================================================================
# FILE: synapse/handlers/device.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-48 ---
     1| import logging
     2| from typing import Any, Dict, List, Optional
     3| from synapse.api import errors
     4| from synapse.api.constants import EventTypes
     5| from synapse.api.errors import (
     6|     FederationDeniedError,
     7|     HttpResponseException,
     8|     RequestSendFailed,
     9|     SynapseError,
    10| )
    11| from synapse.logging.opentracing import log_kv, set_tag, trace
    12| from synapse.metrics.background_process_metrics import run_as_background_process
    13| from synapse.types import (
    14|     RoomStreamToken,
    15|     get_domain_from_id,
    16|     get_verify_key_from_cross_signing_key,
    17| )
    18| from synapse.util import stringutils
    19| from synapse.util.async_helpers import Linearizer
    20| from synapse.util.caches.expiringcache import ExpiringCache
    21| from synapse.util.metrics import measure_func
    22| from synapse.util.retryutils import NotRetryingDestination
    23| from ._base import BaseHandler
    24| logger = logging.getLogger(__name__)
    25| MAX_DEVICE_DISPLAY_NAME_LEN = 100
    26| class DeviceWorkerHandler(BaseHandler):
    27|     def __init__(self, hs):
    28|         super(DeviceWorkerHandler, self).__init__(hs)
    29|         self.hs = hs
    30|         self.state = hs.get_state_handler()
    31|         self.state_store = hs.get_storage().state
    32|         self._auth_handler = hs.get_auth_handler()
    33|     @trace
    34|     async def get_devices_by_user(self, user_id: str) -> List[Dict[str, Any]]:
    35|         """
    36|         Retrieve the given user's devices
    37|         Args:
    38|             user_id: The user ID to query for devices.
    39|         Returns:
    40|             info on each device
    41|         """
    42|         set_tag("user_id", user_id)
    43|         device_map = await self.store.get_devices_by_user(user_id)
    44|         ips = await self.store.get_last_client_ip_by_device(user_id, device_id=None)
    45|         devices = list(device_map.values())
    46|         for device in devices:
    47|             _update_device_from_client_ips(device, ips)
    48|         log_kv(device_map)

# --- HUNK 2: Lines 52-116 ---
    52|         """ Retrieve the given device
    53|         Args:
    54|             user_id: The user to get the device from
    55|             device_id: The device to fetch.
    56|         Returns:
    57|             info on the device
    58|         Raises:
    59|             errors.NotFoundError: if the device was not found
    60|         """
    61|         try:
    62|             device = await self.store.get_device(user_id, device_id)
    63|         except errors.StoreError:
    64|             raise errors.NotFoundError
    65|         ips = await self.store.get_last_client_ip_by_device(user_id, device_id)
    66|         _update_device_from_client_ips(device, ips)
    67|         set_tag("device", device)
    68|         set_tag("ips", ips)
    69|         return device
    70|     @trace
    71|     @measure_func("device.get_user_ids_changed")
    72|     async def get_user_ids_changed(self, user_id, from_token):
    73|         """Get list of users that have had the devices updated, or have newly
    74|         joined a room, that `user_id` may be interested in.
    75|         Args:
    76|             user_id (str)
    77|             from_token (StreamToken)
    78|         """
    79|         set_tag("user_id", user_id)
    80|         set_tag("from_token", from_token)
    81|         now_room_key = await self.store.get_room_events_max_id()
    82|         room_ids = await self.store.get_rooms_for_user(user_id)
    83|         users_who_share_room = await self.store.get_users_who_share_room_with_user(
    84|             user_id
    85|         )
    86|         tracked_users = set(users_who_share_room)
    87|         tracked_users.add(user_id)
    88|         changed = await self.store.get_users_whose_devices_changed(
    89|             from_token.device_list_key, tracked_users
    90|         )
    91|         rooms_changed = self.store.get_rooms_that_changed(room_ids, from_token.room_key)
    92|         member_events = await self.store.get_membership_changes_for_user(
    93|             user_id, from_token.room_key, now_room_key
    94|         )
    95|         rooms_changed.update(event.room_id for event in member_events)
    96|         stream_ordering = RoomStreamToken.parse_stream_token(from_token.room_key).stream
    97|         possibly_changed = set(changed)
    98|         possibly_left = set()
    99|         for room_id in rooms_changed:
   100|             current_state_ids = await self.store.get_current_state_ids(room_id)
   101|             if room_id not in room_ids:
   102|                 for key, event_id in current_state_ids.items():
   103|                     etype, state_key = key
   104|                     if etype != EventTypes.Member:
   105|                         continue
   106|                     possibly_left.add(state_key)
   107|                 continue
   108|             try:
   109|                 event_ids = await self.store.get_forward_extremeties_for_room(
   110|                     room_id, stream_ordering=stream_ordering
   111|                 )
   112|             except errors.StoreError:
   113|                 event_ids = []
   114|             if not event_ids:
   115|                 log_kv(
   116|                     {"event": "encountered empty previous state", "room_id": room_id}

# --- HUNK 3: Lines 153-215 ---
   153|         result = {"changed": list(possibly_joined), "left": list(possibly_left)}
   154|         log_kv(result)
   155|         return result
   156|     async def on_federation_query_user_devices(self, user_id):
   157|         stream_id, devices = await self.store.get_e2e_device_keys_for_federation_query(
   158|             user_id
   159|         )
   160|         master_key = await self.store.get_e2e_cross_signing_key(user_id, "master")
   161|         self_signing_key = await self.store.get_e2e_cross_signing_key(
   162|             user_id, "self_signing"
   163|         )
   164|         return {
   165|             "user_id": user_id,
   166|             "stream_id": stream_id,
   167|             "devices": devices,
   168|             "master_key": master_key,
   169|             "self_signing_key": self_signing_key,
   170|         }
   171| class DeviceHandler(DeviceWorkerHandler):
   172|     def __init__(self, hs):
   173|         super(DeviceHandler, self).__init__(hs)
   174|         self.federation_sender = hs.get_federation_sender()
   175|         self.device_list_updater = DeviceListUpdater(hs, self)
   176|         federation_registry = hs.get_federation_registry()
   177|         federation_registry.register_edu_handler(
   178|             "m.device_list_update", self.device_list_updater.incoming_device_list_update
   179|         )
   180|         hs.get_distributor().observe("user_left_room", self.user_left_room)
   181|     async def check_device_registered(
   182|         self, user_id, device_id, initial_device_display_name=None
   183|     ):
   184|         """
   185|         If the given device has not been registered, register it with the
   186|         supplied display name.
   187|         If no device_id is supplied, we make one up.
   188|         Args:
   189|             user_id (str):  @user:id
   190|             device_id (str | None): device id supplied by client
   191|             initial_device_display_name (str | None): device display name from
   192|                  client
   193|         Returns:
   194|             str: device id (generated if none was supplied)
   195|         """
   196|         if device_id is not None:
   197|             new_device = await self.store.store_device(
   198|                 user_id=user_id,
   199|                 device_id=device_id,
   200|                 initial_device_display_name=initial_device_display_name,
   201|             )
   202|             if new_device:
   203|                 await self.notify_device_update(user_id, [device_id])
   204|             return device_id
   205|         attempts = 0
   206|         while attempts < 5:
   207|             device_id = stringutils.random_string(10).upper()
   208|             new_device = await self.store.store_device(
   209|                 user_id=user_id,
   210|                 device_id=device_id,
   211|                 initial_device_display_name=initial_device_display_name,
   212|             )
   213|             if new_device:
   214|                 await self.notify_device_update(user_id, [device_id])
   215|                 return device_id

# --- HUNK 4: Lines 266-311 ---
   266|                 set_tag("reason", "User doesn't have that device id.")
   267|                 pass
   268|             else:
   269|                 raise
   270|         for device_id in device_ids:
   271|             await self._auth_handler.delete_access_tokens_for_user(
   272|                 user_id, device_id=device_id
   273|             )
   274|             await self.store.delete_e2e_keys_by_device(
   275|                 user_id=user_id, device_id=device_id
   276|             )
   277|         await self.notify_device_update(user_id, device_ids)
   278|     async def update_device(self, user_id: str, device_id: str, content: dict) -> None:
   279|         """ Update the given device
   280|         Args:
   281|             user_id: The user to update devices of.
   282|             device_id: The device to update.
   283|             content: body of update request
   284|         """
   285|         new_display_name = content.get("display_name")
   286|         if new_display_name and len(new_display_name) > MAX_DEVICE_DISPLAY_NAME_LEN:
   287|             raise SynapseError(
   288|                 400,
   289|                 "Device display name is too long (max %i)"
   290|                 % (MAX_DEVICE_DISPLAY_NAME_LEN,),
   291|             )
   292|         try:
   293|             await self.store.update_device(
   294|                 user_id, device_id, new_display_name=new_display_name
   295|             )
   296|             await self.notify_device_update(user_id, [device_id])
   297|         except errors.StoreError as e:
   298|             if e.code == 404:
   299|                 raise errors.NotFoundError()
   300|             else:
   301|                 raise
   302|     @trace
   303|     @measure_func("notify_device_update")
   304|     async def notify_device_update(self, user_id, device_ids):
   305|         """Notify that a user's device(s) has changed. Pokes the notifier, and
   306|         remote servers if the user is local.
   307|         """
   308|         if not device_ids:
   309|             return
   310|         users_who_share_room = await self.store.get_users_who_share_room_with_user(
   311|             user_id


# ====================================================================
# FILE: synapse/handlers/directory.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-40 ---
     1| import logging
     2| import string
     3| from typing import Iterable, List, Optional
     4| from synapse.api.constants import MAX_ALIAS_LENGTH, EventTypes
     5| from synapse.api.errors import (
     6|     AuthError,
     7|     CodeMessageException,
     8|     Codes,
     9|     NotFoundError,
    10|     ShadowBanError,
    11|     StoreError,
    12|     SynapseError,
    13| )
    14| from synapse.appservice import ApplicationService
    15| from synapse.types import Requester, RoomAlias, UserID, get_domain_from_id
    16| from ._base import BaseHandler
    17| logger = logging.getLogger(__name__)
    18| class DirectoryHandler(BaseHandler):
    19|     def __init__(self, hs):
    20|         super(DirectoryHandler, self).__init__(hs)
    21|         self.state = hs.get_state_handler()
    22|         self.appservice_handler = hs.get_application_service_handler()
    23|         self.event_creation_handler = hs.get_event_creation_handler()
    24|         self.store = hs.get_datastore()
    25|         self.config = hs.config
    26|         self.enable_room_list_search = hs.config.enable_room_list_search
    27|         self.require_membership = hs.config.require_membership_for_aliases
    28|         self.federation = hs.get_federation_client()
    29|         hs.get_federation_registry().register_query_handler(
    30|             "directory", self.on_directory_query
    31|         )
    32|         self.spam_checker = hs.get_spam_checker()
    33|     async def _create_association(
    34|         self,
    35|         room_alias: RoomAlias,
    36|         room_id: str,
    37|         servers: Optional[Iterable[str]] = None,
    38|         creator: Optional[str] = None,
    39|     ):
    40|         for wchar in string.whitespace:


# ====================================================================
# FILE: synapse/handlers/e2e_keys.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 857-897 ---
   857|         verify_signed_json(signed_device, user_id, verify_key)
   858|     except SignatureVerifyException:
   859|         logger.debug("invalid signature on key")
   860|         raise SynapseError(400, "Invalid signature", Codes.INVALID_SIGNATURE)
   861| def _exception_to_failure(e):
   862|     if isinstance(e, SynapseError):
   863|         return {"status": e.code, "errcode": e.errcode, "message": str(e)}
   864|     if isinstance(e, CodeMessageException):
   865|         return {"status": e.code, "message": str(e)}
   866|     if isinstance(e, NotRetryingDestination):
   867|         return {"status": 503, "message": "Not ready for retry"}
   868|     return {"status": 503, "message": str(e)}
   869| def _one_time_keys_match(old_key_json, new_key):
   870|     old_key = json_decoder.decode(old_key_json)
   871|     if not isinstance(old_key, dict) or not isinstance(new_key, dict):
   872|         return old_key == new_key
   873|     old_key.pop("signatures", None)
   874|     new_key_copy = dict(new_key)
   875|     new_key_copy.pop("signatures", None)
   876|     return old_key == new_key_copy
   877| @attr.s
   878| class SignatureListItem:
   879|     """An item in the signature list as used by upload_signatures_for_device_keys.
   880|     """
   881|     signing_key_id = attr.ib()
   882|     target_user_id = attr.ib()
   883|     target_device_id = attr.ib()
   884|     signature = attr.ib()
   885| class SigningKeyEduUpdater:
   886|     """Handles incoming signing key updates from federation and updates the DB"""
   887|     def __init__(self, hs, e2e_keys_handler):
   888|         self.store = hs.get_datastore()
   889|         self.federation = hs.get_federation_client()
   890|         self.clock = hs.get_clock()
   891|         self.e2e_keys_handler = e2e_keys_handler
   892|         self._remote_edu_linearizer = Linearizer(name="remote_signing_key")
   893|         self._pending_updates = {}
   894|         self._seen_updates = ExpiringCache(
   895|             cache_name="signing_key_update_edu",
   896|             clock=self.clock,
   897|             max_len=10000,


# ====================================================================
# FILE: synapse/handlers/events.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-41 ---
     1| import logging
     2| import random
     3| from typing import TYPE_CHECKING, Iterable, List, Optional
     4| from synapse.api.constants import EventTypes, Membership
     5| from synapse.api.errors import AuthError, SynapseError
     6| from synapse.events import EventBase
     7| from synapse.handlers.presence import format_user_presence_state
     8| from synapse.logging.utils import log_function
     9| from synapse.streams.config import PaginationConfig
    10| from synapse.types import JsonDict, UserID
    11| from synapse.visibility import filter_events_for_client
    12| from ._base import BaseHandler
    13| if TYPE_CHECKING:
    14|     from synapse.server import HomeServer
    15| logger = logging.getLogger(__name__)
    16| class EventStreamHandler(BaseHandler):
    17|     def __init__(self, hs: "HomeServer"):
    18|         super(EventStreamHandler, self).__init__(hs)
    19|         self.distributor = hs.get_distributor()
    20|         self.distributor.declare("started_user_eventstream")
    21|         self.distributor.declare("stopped_user_eventstream")
    22|         self.clock = hs.get_clock()
    23|         self.notifier = hs.get_notifier()
    24|         self.state = hs.get_state_handler()
    25|         self._server_notices_sender = hs.get_server_notices_sender()
    26|         self._event_serializer = hs.get_event_client_serializer()
    27|     @log_function
    28|     async def get_stream(
    29|         self,
    30|         auth_user_id: str,
    31|         pagin_config: PaginationConfig,
    32|         timeout: int = 0,
    33|         as_client_event: bool = True,
    34|         affect_presence: bool = True,
    35|         room_id: Optional[str] = None,
    36|         is_guest: bool = False,
    37|     ) -> JsonDict:
    38|         """Fetches the events stream for a given user.
    39|         """
    40|         if room_id:
    41|             blocked = await self.store.is_room_blocked(room_id)

# --- HUNK 2: Lines 72-118 ---
    72|                         )  # type: Iterable[str]
    73|                     else:
    74|                         users = [event.state_key]
    75|                     states = await presence_handler.get_states(users)
    76|                     to_add.extend(
    77|                         {
    78|                             "type": EventTypes.Presence,
    79|                             "content": format_user_presence_state(state, time_now),
    80|                         }
    81|                         for state in states
    82|                     )
    83|             events.extend(to_add)
    84|             chunks = await self._event_serializer.serialize_events(
    85|                 events,
    86|                 time_now,
    87|                 as_client_event=as_client_event,
    88|                 bundle_aggregations=False,
    89|             )
    90|             chunk = {
    91|                 "chunk": chunks,
    92|                 "start": tokens[0].to_string(),
    93|                 "end": tokens[1].to_string(),
    94|             }
    95|             return chunk
    96| class EventHandler(BaseHandler):
    97|     def __init__(self, hs: "HomeServer"):
    98|         super(EventHandler, self).__init__(hs)
    99|         self.storage = hs.get_storage()
   100|     async def get_event(
   101|         self, user: UserID, room_id: Optional[str], event_id: str
   102|     ) -> Optional[EventBase]:
   103|         """Retrieve a single specified event.
   104|         Args:
   105|             user: The user requesting the event
   106|             room_id: The expected room id. We'll return None if the
   107|                 event's room does not match.
   108|             event_id: The event ID to obtain.
   109|         Returns:
   110|             An event, or None if there is no event matching this ID.
   111|         Raises:
   112|             SynapseError if there was a problem retrieving this event, or
   113|             AuthError if the user does not have the rights to inspect this
   114|             event.
   115|         """
   116|         event = await self.store.get_event(event_id, check_room_id=room_id)
   117|         if not event:
   118|             return None


# ====================================================================
# FILE: synapse/handlers/federation.py
# Total hunks: 16
# ====================================================================
# --- HUNK 1: Lines 1-26 ---
     1| """Contains handlers for federation events."""
     2| import itertools
     3| import logging
     4| from collections.abc import Container
     5| from http import HTTPStatus
     6| from typing import Dict, Iterable, List, Optional, Sequence, Tuple, Union
     7| import attr
     8| from signedjson.key import decode_verify_key_bytes
     9| from signedjson.sign import verify_signed_json
    10| from unpaddedbase64 import decode_base64
    11| from twisted.internet import defer
    12| from synapse import event_auth
    13| from synapse.api.constants import (
    14|     EventTypes,
    15|     Membership,
    16|     RejectedReason,
    17|     RoomEncryptionAlgorithms,
    18| )
    19| from synapse.api.errors import (
    20|     AuthError,
    21|     CodeMessageException,
    22|     Codes,
    23|     FederationDeniedError,
    24|     FederationError,
    25|     HttpResponseException,
    26|     NotFoundError,

# --- HUNK 2: Lines 31-146 ---
    31| from synapse.crypto.event_signing import compute_event_signature
    32| from synapse.event_auth import auth_types_for_event
    33| from synapse.events import EventBase
    34| from synapse.events.snapshot import EventContext
    35| from synapse.events.validator import EventValidator
    36| from synapse.handlers._base import BaseHandler
    37| from synapse.logging.context import (
    38|     make_deferred_yieldable,
    39|     nested_logging_context,
    40|     preserve_fn,
    41|     run_in_background,
    42| )
    43| from synapse.logging.utils import log_function
    44| from synapse.metrics.background_process_metrics import run_as_background_process
    45| from synapse.replication.http.devices import ReplicationUserDevicesResyncRestServlet
    46| from synapse.replication.http.federation import (
    47|     ReplicationCleanRoomRestServlet,
    48|     ReplicationFederationSendEventsRestServlet,
    49|     ReplicationStoreRoomOnInviteRestServlet,
    50| )
    51| from synapse.replication.http.membership import ReplicationUserJoinedLeftRoomRestServlet
    52| from synapse.state import StateResolutionStore, resolve_events_with_store
    53| from synapse.storage.databases.main.events_worker import EventRedactBehaviour
    54| from synapse.types import (
    55|     JsonDict,
    56|     MutableStateMap,
    57|     StateMap,
    58|     UserID,
    59|     get_domain_from_id,
    60| )
    61| from synapse.util.async_helpers import Linearizer, concurrently_execute
    62| from synapse.util.distributor import user_joined_room
    63| from synapse.util.retryutils import NotRetryingDestination
    64| from synapse.util.stringutils import shortstr
    65| from synapse.visibility import filter_events_for_server
    66| logger = logging.getLogger(__name__)
    67| @attr.s
    68| class _NewEventInfo:
    69|     """Holds information about a received event, ready for passing to _handle_new_events
    70|     Attributes:
    71|         event: the received event
    72|         state: the state at that event
    73|         auth_events: the auth_event map for that event
    74|     """
    75|     event = attr.ib(type=EventBase)
    76|     state = attr.ib(type=Optional[Sequence[EventBase]], default=None)
    77|     auth_events = attr.ib(type=Optional[MutableStateMap[EventBase]], default=None)
    78| class FederationHandler(BaseHandler):
    79|     """Handles events that originated from federation.
    80|         Responsible for:
    81|         a) handling received Pdus before handing them on as Events to the rest
    82|         of the homeserver (including auth and state conflict resoultion)
    83|         b) converting events that were produced by local clients that may need
    84|         to be sent to remote homeservers.
    85|         c) doing the necessary dances to invite remote users and join remote
    86|         rooms.
    87|     """
    88|     def __init__(self, hs):
    89|         super(FederationHandler, self).__init__(hs)
    90|         self.hs = hs
    91|         self.store = hs.get_datastore()
    92|         self.storage = hs.get_storage()
    93|         self.state_store = self.storage.state
    94|         self.federation_client = hs.get_federation_client()
    95|         self.state_handler = hs.get_state_handler()
    96|         self.server_name = hs.hostname
    97|         self.keyring = hs.get_keyring()
    98|         self.action_generator = hs.get_action_generator()
    99|         self.is_mine_id = hs.is_mine_id
   100|         self.pusher_pool = hs.get_pusherpool()
   101|         self.spam_checker = hs.get_spam_checker()
   102|         self.event_creation_handler = hs.get_event_creation_handler()
   103|         self._message_handler = hs.get_message_handler()
   104|         self._server_notices_mxid = hs.config.server_notices_mxid
   105|         self.config = hs.config
   106|         self.http_client = hs.get_simple_http_client()
   107|         self._instance_name = hs.get_instance_name()
   108|         self._replication = hs.get_replication_data_handler()
   109|         self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)
   110|         self._notify_user_membership_change = ReplicationUserJoinedLeftRoomRestServlet.make_client(
   111|             hs
   112|         )
   113|         self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(
   114|             hs
   115|         )
   116|         if hs.config.worker_app:
   117|             self._user_device_resync = ReplicationUserDevicesResyncRestServlet.make_client(
   118|                 hs
   119|             )
   120|             self._maybe_store_room_on_invite = ReplicationStoreRoomOnInviteRestServlet.make_client(
   121|                 hs
   122|             )
   123|         else:
   124|             self._device_list_updater = hs.get_device_handler().device_list_updater
   125|             self._maybe_store_room_on_invite = self.store.maybe_store_room_on_invite
   126|         self.room_queues = {}
   127|         self._room_pdu_linearizer = Linearizer("fed_room_pdu")
   128|         self.third_party_event_rules = hs.get_third_party_event_rules()
   129|         self._ephemeral_messages_enabled = hs.config.enable_ephemeral_messages
   130|     async def on_receive_pdu(self, origin, pdu, sent_to_us_directly=False) -> None:
   131|         """ Process a PDU received via a federation /send/ transaction, or
   132|         via backfill of missing prev_events
   133|         Args:
   134|             origin (str): server which initiated the /send/ transaction. Will
   135|                 be used to fetch missing events or state.
   136|             pdu (FrozenEvent): received PDU
   137|             sent_to_us_directly (bool): True if this event was pushed to us; False if
   138|                 we pulled it as the result of a missing prev_event.
   139|         """
   140|         room_id = pdu.room_id
   141|         event_id = pdu.event_id
   142|         logger.info("handling received PDU: %s", pdu)
   143|         existing = await self.store.get_event(
   144|             event_id, allow_none=True, allow_rejected=True
   145|         )
   146|         already_seen = existing and (

# --- HUNK 3: Lines 191-231 ---
   191|                         room_id,
   192|                         event_id,
   193|                         len(missing_prevs),
   194|                         shortstr(missing_prevs),
   195|                     )
   196|                     with (await self._room_pdu_linearizer.queue(pdu.room_id)):
   197|                         logger.info(
   198|                             "[%s %s] Acquired room lock to fetch %d missing prev_events",
   199|                             room_id,
   200|                             event_id,
   201|                             len(missing_prevs),
   202|                         )
   203|                         try:
   204|                             await self._get_missing_events_for_pdu(
   205|                                 origin, pdu, prevs, min_depth
   206|                             )
   207|                         except Exception as e:
   208|                             raise Exception(
   209|                                 "Error fetching missing prev_events for %s: %s"
   210|                                 % (event_id, e)
   211|                             )
   212|                         seen = await self.store.have_events_in_timeline(prevs)
   213|                         if not prevs - seen:
   214|                             logger.info(
   215|                                 "[%s %s] Found all missing prev_events",
   216|                                 room_id,
   217|                                 event_id,
   218|                             )
   219|             if prevs - seen:
   220|                 if sent_to_us_directly:
   221|                     logger.warning(
   222|                         "[%s %s] Rejecting: failed to fetch %d prev events: %s",
   223|                         room_id,
   224|                         event_id,
   225|                         len(prevs - seen),
   226|                         shortstr(prevs - seen),
   227|                     )
   228|                     raise FederationError(
   229|                         "ERROR",
   230|                         403,
   231|                         (

# --- HUNK 4: Lines 242-283 ---
   242|                 event_map = {event_id: pdu}
   243|                 try:
   244|                     ours = await self.state_store.get_state_groups_ids(room_id, seen)
   245|                     state_maps = list(ours.values())  # type: List[StateMap[str]]
   246|                     del ours
   247|                     for p in prevs - seen:
   248|                         logger.info(
   249|                             "Requesting state at missing prev_event %s", event_id,
   250|                         )
   251|                         with nested_logging_context(p):
   252|                             (remote_state, _,) = await self._get_state_for_room(
   253|                                 origin, room_id, p, include_event_in_state=True
   254|                             )
   255|                             remote_state_map = {
   256|                                 (x.type, x.state_key): x.event_id for x in remote_state
   257|                             }
   258|                             state_maps.append(remote_state_map)
   259|                             for x in remote_state:
   260|                                 event_map[x.event_id] = x
   261|                     room_version = await self.store.get_room_version_id(room_id)
   262|                     state_map = await resolve_events_with_store(
   263|                         self.clock,
   264|                         room_id,
   265|                         room_version,
   266|                         state_maps,
   267|                         event_map,
   268|                         state_res_store=StateResolutionStore(self.store),
   269|                     )
   270|                     evs = await self.store.get_events(
   271|                         list(state_map.values()),
   272|                         get_prev_content=False,
   273|                         redact_behaviour=EventRedactBehaviour.AS_IS,
   274|                     )
   275|                     event_map.update(evs)
   276|                     state = [event_map[e] for e in state_map.values()]
   277|                 except Exception:
   278|                     logger.warning(
   279|                         "[%s %s] Error attempting to resolve state at missing "
   280|                         "prev_events",
   281|                         room_id,
   282|                         event_id,
   283|                         exc_info=True,

# --- HUNK 5: Lines 448-504 ---
   448|                 room_id,
   449|             )
   450|             del fetched_events[bad_event_id]
   451|         return fetched_events
   452|     async def _process_received_pdu(
   453|         self, origin: str, event: EventBase, state: Optional[Iterable[EventBase]],
   454|     ):
   455|         """ Called when we have a new pdu. We need to do auth checks and put it
   456|         through the StateHandler.
   457|         Args:
   458|             origin: server sending the event
   459|             event: event to be persisted
   460|             state: Normally None, but if we are handling a gap in the graph
   461|                 (ie, we are missing one or more prev_events), the resolved state at the
   462|                 event
   463|         """
   464|         room_id = event.room_id
   465|         event_id = event.event_id
   466|         logger.debug("[%s %s] Processing event: %s", room_id, event_id, event)
   467|         try:
   468|             context = await self._handle_new_event(origin, event, state=state)
   469|         except AuthError as e:
   470|             raise FederationError("ERROR", e.code, e.msg, affected=event.event_id)
   471|         if event.type == EventTypes.Member:
   472|             if event.membership == Membership.JOIN:
   473|                 newly_joined = True
   474|                 prev_state_ids = await context.get_prev_state_ids()
   475|                 prev_state_id = prev_state_ids.get((event.type, event.state_key))
   476|                 if prev_state_id:
   477|                     prev_state = await self.store.get_event(
   478|                         prev_state_id, allow_none=True
   479|                     )
   480|                     if prev_state and prev_state.membership == Membership.JOIN:
   481|                         newly_joined = False
   482|                 if newly_joined:
   483|                     user = UserID.from_string(event.state_key)
   484|                     await self.user_joined_room(user, room_id)
   485|         if event.type == EventTypes.Encrypted:
   486|             device_id = event.content.get("device_id")
   487|             sender_key = event.content.get("sender_key")
   488|             cached_devices = await self.store.get_cached_devices_for_user(event.sender)
   489|             resync = False  # Whether we should resync device lists.
   490|             device = None
   491|             if device_id is not None:
   492|                 device = cached_devices.get(device_id)
   493|                 if device is None:
   494|                     logger.info(
   495|                         "Received event from remote device not in our cache: %s %s",
   496|                         event.sender,
   497|                         device_id,
   498|                     )
   499|                     resync = True
   500|             if sender_key is not None:
   501|                 current_keys = []  # type: Container[str]
   502|                 if device:
   503|                     keys = device.get("keys", {}).get("keys", {})
   504|                     if (

# --- HUNK 6: Lines 541-580 ---
   541|                 await self._device_list_updater.user_device_resync(sender)
   542|         except Exception:
   543|             logger.exception("Failed to resync device for %s", sender)
   544|     @log_function
   545|     async def backfill(self, dest, room_id, limit, extremities):
   546|         """ Trigger a backfill request to `dest` for the given `room_id`
   547|         This will attempt to get more events from the remote. If the other side
   548|         has no new events to offer, this will return an empty list.
   549|         As the events are received, we check their signatures, and also do some
   550|         sanity-checking on them. If any of the backfilled events are invalid,
   551|         this method throws a SynapseError.
   552|         TODO: make this more useful to distinguish failures of the remote
   553|         server from invalid events (there is probably no point in trying to
   554|         re-fetch invalid events from every other HS in the room.)
   555|         """
   556|         if dest == self.server_name:
   557|             raise SynapseError(400, "Can't backfill from self.")
   558|         events = await self.federation_client.backfill(
   559|             dest, room_id, limit=limit, extremities=extremities
   560|         )
   561|         seen_events = await self.store.have_events_in_timeline(
   562|             {e.event_id for e in events}
   563|         )
   564|         events = [e for e in events if e.event_id not in seen_events]
   565|         if not events:
   566|             return []
   567|         event_map = {e.event_id: e for e in events}
   568|         event_ids = {e.event_id for e in events}
   569|         edges = [ev.event_id for ev in events if set(ev.prev_event_ids()) - event_ids]
   570|         logger.info("backfill: Got %d events with %d edges", len(events), len(edges))
   571|         auth_events = {}
   572|         state_events = {}
   573|         events_to_state = {}
   574|         for e_id in edges:
   575|             state, auth = await self._get_state_for_room(
   576|                 destination=dest,
   577|                 room_id=room_id,
   578|                 event_id=e_id,
   579|                 include_event_in_state=False,
   580|             )

# --- HUNK 7: Lines 593-633 ---
   593|             {e_id: event_map[e_id] for e_id in required_auth if e_id in event_map}
   594|         )
   595|         ev_infos = []
   596|         for e_id in events_to_state:
   597|             ev = event_map[e_id]
   598|             assert not ev.internal_metadata.is_outlier()
   599|             ev_infos.append(
   600|                 _NewEventInfo(
   601|                     event=ev,
   602|                     state=events_to_state[e_id],
   603|                     auth_events={
   604|                         (
   605|                             auth_events[a_id].type,
   606|                             auth_events[a_id].state_key,
   607|                         ): auth_events[a_id]
   608|                         for a_id in ev.auth_event_ids()
   609|                         if a_id in auth_events
   610|                     },
   611|                 )
   612|             )
   613|         await self._handle_new_events(dest, ev_infos, backfilled=True)
   614|         events.sort(key=lambda e: e.depth)
   615|         for event in events:
   616|             if event in events_to_state:
   617|                 continue
   618|             assert not event.internal_metadata.is_outlier()
   619|             await self._handle_new_event(dest, event, backfilled=True)
   620|         return events
   621|     async def maybe_backfill(
   622|         self, room_id: str, current_depth: int, limit: int
   623|     ) -> bool:
   624|         """Checks the database to see if we should backfill before paginating,
   625|         and if so do.
   626|         Args:
   627|             room_id
   628|             current_depth: The depth from which we're paginating from. This is
   629|                 used to decide if we should backfill and what extremities to
   630|                 use.
   631|             limit: The number of events that the pagination request will
   632|                 return. This is used as part of the heuristic to decide if we
   633|                 should back paginate.

# --- HUNK 8: Lines 807-847 ---
   807|         auth_events = [
   808|             aid
   809|             for event in event_map.values()
   810|             for aid in event.auth_event_ids()
   811|             if aid not in event_map
   812|         ]
   813|         persisted_events = await self.store.get_events(
   814|             auth_events, allow_rejected=True,
   815|         )
   816|         event_infos = []
   817|         for event in event_map.values():
   818|             auth = {}
   819|             for auth_event_id in event.auth_event_ids():
   820|                 ae = persisted_events.get(auth_event_id) or event_map.get(auth_event_id)
   821|                 if ae:
   822|                     auth[(ae.type, ae.state_key)] = ae
   823|                 else:
   824|                     logger.info("Missing auth event %s", auth_event_id)
   825|             event_infos.append(_NewEventInfo(event, None, auth))
   826|         await self._handle_new_events(
   827|             destination, event_infos,
   828|         )
   829|     def _sanity_check_event(self, ev):
   830|         """
   831|         Do some early sanity checks of a received event
   832|         In particular, checks it doesn't have an excessive number of
   833|         prev_events or auth_events, which could cause a huge state resolution
   834|         or cascade of event fetches.
   835|         Args:
   836|             ev (synapse.events.EventBase): event to be checked
   837|         Returns: None
   838|         Raises:
   839|             SynapseError if the event does not pass muster
   840|         """
   841|         if len(ev.prev_event_ids()) > 20:
   842|             logger.warning(
   843|                 "Rejecting event %s which has %i prev_events",
   844|                 ev.event_id,
   845|                 len(ev.prev_event_ids()),
   846|             )
   847|             raise SynapseError(HTTPStatus.BAD_REQUEST, "Too many prev_events")

# --- HUNK 9: Lines 907-950 ---
   907|                 host_list.insert(0, origin)
   908|             except ValueError:
   909|                 pass
   910|             ret = await self.federation_client.send_join(
   911|                 host_list, event, room_version_obj
   912|             )
   913|             origin = ret["origin"]
   914|             state = ret["state"]
   915|             auth_chain = ret["auth_chain"]
   916|             auth_chain.sort(key=lambda e: e.depth)
   917|             handled_events.update([s.event_id for s in state])
   918|             handled_events.update([a.event_id for a in auth_chain])
   919|             handled_events.add(event.event_id)
   920|             logger.debug("do_invite_join auth_chain: %s", auth_chain)
   921|             logger.debug("do_invite_join state: %s", state)
   922|             logger.debug("do_invite_join event: %s", event)
   923|             await self.store.upsert_room_on_join(
   924|                 room_id=room_id, room_version=room_version_obj,
   925|             )
   926|             max_stream_id = await self._persist_auth_tree(
   927|                 origin, auth_chain, state, event, room_version_obj
   928|             )
   929|             await self._replication.wait_for_stream_position(
   930|                 self.config.worker.writers.events, "events", max_stream_id
   931|             )
   932|             predecessor = await self.store.get_room_predecessor(room_id)
   933|             if not predecessor or not isinstance(predecessor.get("room_id"), str):
   934|                 return event.event_id, max_stream_id
   935|             old_room_id = predecessor["room_id"]
   936|             logger.debug(
   937|                 "Found predecessor for %s during remote join: %s", room_id, old_room_id
   938|             )
   939|             member_handler = self.hs.get_room_member_handler()
   940|             await member_handler.transfer_room_state_on_room_upgrade(
   941|                 old_room_id, room_id
   942|             )
   943|             logger.debug("Finished joining %s to %s", joinee, room_id)
   944|             return event.event_id, max_stream_id
   945|         finally:
   946|             room_queue = self.room_queues[room_id]
   947|             del self.room_queues[room_id]
   948|             run_in_background(self._handle_queued_pdus, room_queue)
   949|     async def _handle_queued_pdus(self, room_queue):
   950|         """Process PDUs which got queued up while we were busy send_joining.

# --- HUNK 10: Lines 1038-1081 ---
  1038|                 event.sender,
  1039|                 origin,
  1040|             )
  1041|             raise SynapseError(403, "User not from origin", Codes.FORBIDDEN)
  1042|         event.internal_metadata.outlier = False
  1043|         event.internal_metadata.send_on_behalf_of = origin
  1044|         context = await self._handle_new_event(origin, event)
  1045|         event_allowed = await self.third_party_event_rules.check_event_allowed(
  1046|             event, context
  1047|         )
  1048|         if not event_allowed:
  1049|             logger.info("Sending of join %s forbidden by third-party rules", event)
  1050|             raise SynapseError(
  1051|                 403, "This event is not allowed in this context", Codes.FORBIDDEN
  1052|             )
  1053|         logger.debug(
  1054|             "on_send_join_request: After _handle_new_event: %s, sigs: %s",
  1055|             event.event_id,
  1056|             event.signatures,
  1057|         )
  1058|         if event.type == EventTypes.Member:
  1059|             if event.content["membership"] == Membership.JOIN:
  1060|                 user = UserID.from_string(event.state_key)
  1061|                 await self.user_joined_room(user, event.room_id)
  1062|         prev_state_ids = await context.get_prev_state_ids()
  1063|         state_ids = list(prev_state_ids.values())
  1064|         auth_chain = await self.store.get_auth_chain(state_ids)
  1065|         state = await self.store.get_events(list(prev_state_ids.values()))
  1066|         return {"state": list(state.values()), "auth_chain": auth_chain}
  1067|     async def on_invite_request(
  1068|         self, origin: str, event: EventBase, room_version: RoomVersion
  1069|     ):
  1070|         """ We've got an invite event. Process and persist it. Sign it.
  1071|         Respond with the now signed event.
  1072|         """
  1073|         if event.state_key is None:
  1074|             raise SynapseError(400, "The invite event did not have a state key")
  1075|         is_blocked = await self.store.is_room_blocked(event.room_id)
  1076|         if is_blocked:
  1077|             raise SynapseError(403, "This room has been blocked on this server")
  1078|         if self.hs.config.block_non_admin_invites:
  1079|             raise SynapseError(403, "This server does not accept room invites")
  1080|         if not self.spam_checker.user_may_invite(
  1081|             event.sender, event.state_key, event.room_id

# --- HUNK 11: Lines 1092-1150 ---
  1092|                 400, "The invite event was not from the server sending it"
  1093|             )
  1094|         if not self.is_mine_id(event.state_key):
  1095|             raise SynapseError(400, "The invite event must be for this server")
  1096|         if event.state_key == self._server_notices_mxid:
  1097|             raise SynapseError(HTTPStatus.FORBIDDEN, "Cannot invite this user")
  1098|         await self._maybe_store_room_on_invite(
  1099|             room_id=event.room_id, room_version=room_version
  1100|         )
  1101|         event.internal_metadata.outlier = True
  1102|         event.internal_metadata.out_of_band_membership = True
  1103|         event.signatures.update(
  1104|             compute_event_signature(
  1105|                 room_version,
  1106|                 event.get_pdu_json(),
  1107|                 self.hs.hostname,
  1108|                 self.hs.signing_key,
  1109|             )
  1110|         )
  1111|         context = await self.state_handler.compute_event_context(event)
  1112|         await self.persist_events_and_notify([(event, context)])
  1113|         return event
  1114|     async def do_remotely_reject_invite(
  1115|         self, target_hosts: Iterable[str], room_id: str, user_id: str, content: JsonDict
  1116|     ) -> Tuple[EventBase, int]:
  1117|         origin, event, room_version = await self._make_and_verify_event(
  1118|             target_hosts, room_id, user_id, "leave", content=content
  1119|         )
  1120|         event.internal_metadata.outlier = True
  1121|         event.internal_metadata.out_of_band_membership = True
  1122|         host_list = list(target_hosts)
  1123|         try:
  1124|             host_list.remove(origin)
  1125|             host_list.insert(0, origin)
  1126|         except ValueError:
  1127|             pass
  1128|         await self.federation_client.send_leave(host_list, event)
  1129|         context = await self.state_handler.compute_event_context(event)
  1130|         stream_id = await self.persist_events_and_notify([(event, context)])
  1131|         return event, stream_id
  1132|     async def _make_and_verify_event(
  1133|         self,
  1134|         target_hosts: Iterable[str],
  1135|         room_id: str,
  1136|         user_id: str,
  1137|         membership: str,
  1138|         content: JsonDict = {},
  1139|         params: Optional[Dict[str, Union[str, Iterable[str]]]] = None,
  1140|     ) -> Tuple[str, EventBase, RoomVersion]:
  1141|         (
  1142|             origin,
  1143|             event,
  1144|             room_version,
  1145|         ) = await self.federation_client.make_membership_event(
  1146|             target_hosts, room_id, user_id, membership, content, params=params
  1147|         )
  1148|         logger.debug("Got response to make_%s: %s", membership, event)
  1149|         assert event.type == EventTypes.Member
  1150|         assert event.user_id == user_id

# --- HUNK 12: Lines 1306-1404 ---
  1306|         else:
  1307|             return None
  1308|     async def get_min_depth_for_context(self, context):
  1309|         return await self.store.get_min_depth(context)
  1310|     async def _handle_new_event(
  1311|         self, origin, event, state=None, auth_events=None, backfilled=False
  1312|     ):
  1313|         context = await self._prep_event(
  1314|             origin, event, state=state, auth_events=auth_events, backfilled=backfilled
  1315|         )
  1316|         try:
  1317|             if (
  1318|                 not event.internal_metadata.is_outlier()
  1319|                 and not backfilled
  1320|                 and not context.rejected
  1321|             ):
  1322|                 await self.action_generator.handle_push_actions_for_event(
  1323|                     event, context
  1324|                 )
  1325|             await self.persist_events_and_notify(
  1326|                 [(event, context)], backfilled=backfilled
  1327|             )
  1328|         except Exception:
  1329|             run_in_background(
  1330|                 self.store.remove_push_actions_from_staging, event.event_id
  1331|             )
  1332|             raise
  1333|         return context
  1334|     async def _handle_new_events(
  1335|         self,
  1336|         origin: str,
  1337|         event_infos: Iterable[_NewEventInfo],
  1338|         backfilled: bool = False,
  1339|     ) -> None:
  1340|         """Creates the appropriate contexts and persists events. The events
  1341|         should not depend on one another, e.g. this should be used to persist
  1342|         a bunch of outliers, but not a chunk of individual events that depend
  1343|         on each other for state calculations.
  1344|         Notifies about the events where appropriate.
  1345|         """
  1346|         async def prep(ev_info: _NewEventInfo):
  1347|             event = ev_info.event
  1348|             with nested_logging_context(suffix=event.event_id):
  1349|                 res = await self._prep_event(
  1350|                     origin,
  1351|                     event,
  1352|                     state=ev_info.state,
  1353|                     auth_events=ev_info.auth_events,
  1354|                     backfilled=backfilled,
  1355|                 )
  1356|             return res
  1357|         contexts = await make_deferred_yieldable(
  1358|             defer.gatherResults(
  1359|                 [run_in_background(prep, ev_info) for ev_info in event_infos],
  1360|                 consumeErrors=True,
  1361|             )
  1362|         )
  1363|         await self.persist_events_and_notify(
  1364|             [
  1365|                 (ev_info.event, context)
  1366|                 for ev_info, context in zip(event_infos, contexts)
  1367|             ],
  1368|             backfilled=backfilled,
  1369|         )
  1370|     async def _persist_auth_tree(
  1371|         self,
  1372|         origin: str,
  1373|         auth_events: List[EventBase],
  1374|         state: List[EventBase],
  1375|         event: EventBase,
  1376|         room_version: RoomVersion,
  1377|     ) -> int:
  1378|         """Checks the auth chain is valid (and passes auth checks) for the
  1379|         state and event. Then persists the auth chain and state atomically.
  1380|         Persists the event separately. Notifies about the persisted events
  1381|         where appropriate.
  1382|         Will attempt to fetch missing auth events.
  1383|         Args:
  1384|             origin: Where the events came from
  1385|             auth_events
  1386|             state
  1387|             event
  1388|             room_version: The room version we expect this room to have, and
  1389|                 will raise if it doesn't match the version in the create event.
  1390|         """
  1391|         events_to_context = {}
  1392|         for e in itertools.chain(auth_events, state):
  1393|             e.internal_metadata.outlier = True
  1394|             ctx = await self.state_handler.compute_event_context(e)
  1395|             events_to_context[e.event_id] = ctx
  1396|         event_map = {
  1397|             e.event_id: e for e in itertools.chain(auth_events, state, [event])
  1398|         }
  1399|         create_event = None
  1400|         for e in auth_events:
  1401|             if (e.type, e.state_key) == (EventTypes.Create, ""):
  1402|                 create_event = e
  1403|                 break
  1404|         if create_event is None:

# --- HUNK 13: Lines 1420-1468 ---
  1420|             if m_ev and m_ev.event_id == e_id:
  1421|                 event_map[e_id] = m_ev
  1422|             else:
  1423|                 logger.info("Failed to find auth event %r", e_id)
  1424|         for e in itertools.chain(auth_events, state, [event]):
  1425|             auth_for_e = {
  1426|                 (event_map[e_id].type, event_map[e_id].state_key): event_map[e_id]
  1427|                 for e_id in e.auth_event_ids()
  1428|                 if e_id in event_map
  1429|             }
  1430|             if create_event:
  1431|                 auth_for_e[(EventTypes.Create, "")] = create_event
  1432|             try:
  1433|                 event_auth.check(room_version, e, auth_events=auth_for_e)
  1434|             except SynapseError as err:
  1435|                 logger.warning("Rejecting %s because %s", e.event_id, err.msg)
  1436|                 if e == event:
  1437|                     raise
  1438|                 events_to_context[e.event_id].rejected = RejectedReason.AUTH_ERROR
  1439|         await self.persist_events_and_notify(
  1440|             [
  1441|                 (e, events_to_context[e.event_id])
  1442|                 for e in itertools.chain(auth_events, state)
  1443|             ]
  1444|         )
  1445|         new_event_context = await self.state_handler.compute_event_context(
  1446|             event, old_state=state
  1447|         )
  1448|         return await self.persist_events_and_notify([(event, new_event_context)])
  1449|     async def _prep_event(
  1450|         self,
  1451|         origin: str,
  1452|         event: EventBase,
  1453|         state: Optional[Iterable[EventBase]],
  1454|         auth_events: Optional[MutableStateMap[EventBase]],
  1455|         backfilled: bool,
  1456|     ) -> EventContext:
  1457|         context = await self.state_handler.compute_event_context(event, old_state=state)
  1458|         if not auth_events:
  1459|             prev_state_ids = await context.get_prev_state_ids()
  1460|             auth_events_ids = self.auth.compute_auth_events(
  1461|                 event, prev_state_ids, for_verification=True
  1462|             )
  1463|             auth_events_x = await self.store.get_events(auth_events_ids)
  1464|             auth_events = {(e.type, e.state_key): e for e in auth_events_x.values()}
  1465|         if event.type == EventTypes.Member and not event.auth_event_ids():
  1466|             if len(event.prev_event_ids()) == 1 and event.depth < 5:
  1467|                 c = await self.store.get_event(
  1468|                     event.prev_event_ids()[0], allow_none=True

# --- HUNK 14: Lines 1478-1521 ---
  1478|     async def _check_for_soft_fail(
  1479|         self, event: EventBase, state: Optional[Iterable[EventBase]], backfilled: bool
  1480|     ) -> None:
  1481|         """Checks if we should soft fail the event; if so, marks the event as
  1482|         such.
  1483|         Args:
  1484|             event
  1485|             state: The state at the event if we don't have all the event's prev events
  1486|             backfilled: Whether the event is from backfill
  1487|         """
  1488|         if backfilled or event.internal_metadata.is_outlier():
  1489|             return
  1490|         extrem_ids_list = await self.store.get_latest_event_ids_in_room(event.room_id)
  1491|         extrem_ids = set(extrem_ids_list)
  1492|         prev_event_ids = set(event.prev_event_ids())
  1493|         if extrem_ids == prev_event_ids:
  1494|             return
  1495|         room_version = await self.store.get_room_version_id(event.room_id)
  1496|         room_version_obj = KNOWN_ROOM_VERSIONS[room_version]
  1497|         if state is not None:
  1498|             state_sets = await self.state_store.get_state_groups(
  1499|                 event.room_id, extrem_ids
  1500|             )
  1501|             state_sets = list(state_sets.values())
  1502|             state_sets.append(state)
  1503|             current_states = await self.state_handler.resolve_events(
  1504|                 room_version, state_sets, event
  1505|             )
  1506|             current_state_ids = {
  1507|                 k: e.event_id for k, e in current_states.items()
  1508|             }  # type: StateMap[str]
  1509|         else:
  1510|             current_state_ids = await self.state_handler.get_current_state_ids(
  1511|                 event.room_id, latest_event_ids=extrem_ids
  1512|             )
  1513|         logger.debug(
  1514|             "Doing soft-fail check for %s: state %s", event.event_id, current_state_ids,
  1515|         )
  1516|         auth_types = auth_types_for_event(event)
  1517|         current_state_ids_list = [
  1518|             e for k, e in current_state_ids.items() if k in auth_types
  1519|         ]
  1520|         auth_events_map = await self.store.get_events(current_state_ids_list)
  1521|         current_auth_events = {

# --- HUNK 15: Lines 2036-2145 ---
  2036|         raise last_exception
  2037|     async def _check_key_revocation(self, public_key, url):
  2038|         """
  2039|         Checks whether public_key has been revoked.
  2040|         Args:
  2041|             public_key (str): base-64 encoded public key.
  2042|             url (str): Key revocation URL.
  2043|         Raises:
  2044|             AuthError: if they key has been revoked.
  2045|             SynapseError: if a transient error meant a key couldn't be checked
  2046|                 for revocation.
  2047|         """
  2048|         try:
  2049|             response = await self.http_client.get_json(url, {"public_key": public_key})
  2050|         except Exception:
  2051|             raise SynapseError(502, "Third party certificate could not be checked")
  2052|         if "valid" not in response or not response["valid"]:
  2053|             raise AuthError(403, "Third party certificate was invalid")
  2054|     async def persist_events_and_notify(
  2055|         self,
  2056|         event_and_contexts: Sequence[Tuple[EventBase, EventContext]],
  2057|         backfilled: bool = False,
  2058|     ) -> int:
  2059|         """Persists events and tells the notifier/pushers about them, if
  2060|         necessary.
  2061|         Args:
  2062|             event_and_contexts:
  2063|             backfilled: Whether these events are a result of
  2064|                 backfilling or not
  2065|         """
  2066|         if self.config.worker.writers.events != self._instance_name:
  2067|             result = await self._send_events(
  2068|                 instance_name=self.config.worker.writers.events,
  2069|                 store=self.store,
  2070|                 event_and_contexts=event_and_contexts,
  2071|                 backfilled=backfilled,
  2072|             )
  2073|             return result["max_stream_id"]
  2074|         else:
  2075|             max_stream_id = await self.storage.persistence.persist_events(
  2076|                 event_and_contexts, backfilled=backfilled
  2077|             )
  2078|             if self._ephemeral_messages_enabled:
  2079|                 for (event, context) in event_and_contexts:
  2080|                     self._message_handler.maybe_schedule_expiry(event)
  2081|             if not backfilled:  # Never notify for backfilled events
  2082|                 for event, _ in event_and_contexts:
  2083|                     await self._notify_persisted_event(event, max_stream_id)
  2084|             return max_stream_id
  2085|     async def _notify_persisted_event(
  2086|         self, event: EventBase, max_stream_id: int
  2087|     ) -> None:
  2088|         """Checks to see if notifier/pushers should be notified about the
  2089|         event or not.
  2090|         Args:
  2091|             event:
  2092|             max_stream_id: The max_stream_id returned by persist_events
  2093|         """
  2094|         extra_users = []
  2095|         if event.type == EventTypes.Member:
  2096|             target_user_id = event.state_key
  2097|             if event.internal_metadata.is_outlier():
  2098|                 if event.membership != Membership.INVITE:
  2099|                     if not self.is_mine_id(target_user_id):
  2100|                         return
  2101|             target_user = UserID.from_string(target_user_id)
  2102|             extra_users.append(target_user)
  2103|         elif event.internal_metadata.is_outlier():
  2104|             return
  2105|         event_stream_id = event.internal_metadata.stream_ordering
  2106|         self.notifier.on_new_room_event(
  2107|             event, event_stream_id, max_stream_id, extra_users=extra_users
  2108|         )
  2109|         await self.pusher_pool.on_new_notifications(event_stream_id, max_stream_id)
  2110|     async def _clean_room_for_join(self, room_id: str) -> None:
  2111|         """Called to clean up any data in DB for a given room, ready for the
  2112|         server to join the room.
  2113|         Args:
  2114|             room_id
  2115|         """
  2116|         if self.config.worker_app:
  2117|             await self._clean_room_for_join_client(room_id)
  2118|         else:
  2119|             await self.store.clean_room_for_join(room_id)
  2120|     async def user_joined_room(self, user: UserID, room_id: str) -> None:
  2121|         """Called when a new user has joined the room
  2122|         """
  2123|         if self.config.worker_app:
  2124|             await self._notify_user_membership_change(
  2125|                 room_id=room_id, user_id=user.to_string(), change="joined"
  2126|             )
  2127|         else:
  2128|             user_joined_room(self.distributor, user, room_id)
  2129|     async def get_room_complexity(
  2130|         self, remote_room_hosts: List[str], room_id: str
  2131|     ) -> Optional[dict]:
  2132|         """
  2133|         Fetch the complexity of a remote room over federation.
  2134|         Args:
  2135|             remote_room_hosts (list[str]): The remote servers to ask.
  2136|             room_id (str): The room ID to ask about.
  2137|         Returns:
  2138|             Dict contains the complexity
  2139|             metric versions, while None means we could not fetch the complexity.
  2140|         """
  2141|         for host in remote_room_hosts:
  2142|             res = await self.federation_client.get_room_complexity(host, room_id)
  2143|             if res:
  2144|                 return res
  2145|         return None


# ====================================================================
# FILE: synapse/handlers/groups_local.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 151-191 ---
   151|                 destinations.setdefault(get_domain_from_id(user_id), set()).add(user_id)
   152|         if not proxy and destinations:
   153|             raise SynapseError(400, "Some user_ids are not local")
   154|         results = {}
   155|         failed_results = []
   156|         for destination, dest_user_ids in destinations.items():
   157|             try:
   158|                 r = await self.transport_client.bulk_get_publicised_groups(
   159|                     destination, list(dest_user_ids)
   160|                 )
   161|                 results.update(r["users"])
   162|             except Exception:
   163|                 failed_results.extend(dest_user_ids)
   164|         for uid in local_users:
   165|             results[uid] = await self.store.get_publicised_groups_for_user(uid)
   166|             for app_service in self.store.get_app_services():
   167|                 results[uid].extend(app_service.get_groups_for_user(uid))
   168|         return {"users": results}
   169| class GroupsLocalHandler(GroupsLocalWorkerHandler):
   170|     def __init__(self, hs):
   171|         super(GroupsLocalHandler, self).__init__(hs)
   172|         hs.get_groups_attestation_renewer()
   173|     update_group_profile = _create_rerouter("update_group_profile")
   174|     add_room_to_group = _create_rerouter("add_room_to_group")
   175|     update_room_in_group = _create_rerouter("update_room_in_group")
   176|     remove_room_from_group = _create_rerouter("remove_room_from_group")
   177|     update_group_summary_room = _create_rerouter("update_group_summary_room")
   178|     delete_group_summary_room = _create_rerouter("delete_group_summary_room")
   179|     update_group_category = _create_rerouter("update_group_category")
   180|     delete_group_category = _create_rerouter("delete_group_category")
   181|     update_group_summary_user = _create_rerouter("update_group_summary_user")
   182|     delete_group_summary_user = _create_rerouter("delete_group_summary_user")
   183|     update_group_role = _create_rerouter("update_group_role")
   184|     delete_group_role = _create_rerouter("delete_group_role")
   185|     set_group_join_policy = _create_rerouter("set_group_join_policy")
   186|     async def create_group(self, group_id, user_id, content):
   187|         """Create a group
   188|         """
   189|         logger.info("Asking to create group with ID: %r", group_id)
   190|         if self.is_mine_id(group_id):
   191|             res = await self.groups_server_handler.create_group(


# ====================================================================
# FILE: synapse/handlers/identity.py
# Total hunks: 11
# ====================================================================
# --- HUNK 1: Lines 1-81 ---
     1| """Utilities for interacting with Identity Servers"""
     2| import logging
     3| import urllib.parse
     4| from typing import Awaitable, Callable, Dict, List, Optional, Tuple
     5| from twisted.internet.error import TimeoutError
     6| from synapse.api.errors import (
     7|     CodeMessageException,
     8|     Codes,
     9|     HttpResponseException,
    10|     SynapseError,
    11| )
    12| from synapse.config.emailconfig import ThreepidBehaviour
    13| from synapse.http.client import SimpleHttpClient
    14| from synapse.types import JsonDict, Requester
    15| from synapse.util import json_decoder
    16| from synapse.util.hash import sha256_and_url_safe_base64
    17| from synapse.util.stringutils import assert_valid_client_secret, random_string
    18| from ._base import BaseHandler
    19| logger = logging.getLogger(__name__)
    20| id_server_scheme = "https://"
    21| class IdentityHandler(BaseHandler):
    22|     def __init__(self, hs):
    23|         super(IdentityHandler, self).__init__(hs)
    24|         self.http_client = SimpleHttpClient(hs)
    25|         self.blacklisting_http_client = SimpleHttpClient(
    26|             hs, ip_blacklist=hs.config.federation_ip_range_blacklist
    27|         )
    28|         self.federation_http_client = hs.get_http_client()
    29|         self.hs = hs
    30|     async def threepid_from_creds(
    31|         self, id_server: str, creds: Dict[str, str]
    32|     ) -> Optional[JsonDict]:
    33|         """
    34|         Retrieve and validate a threepid identifier from a "credentials" dictionary against a
    35|         given identity server
    36|         Args:
    37|             id_server: The identity server to validate 3PIDs against. Must be a
    38|                 complete URL including the protocol (http(s)://)
    39|             creds: Dictionary containing the following keys:
    40|                 * client_secret|clientSecret: A unique secret str provided by the client
    41|                 * sid: The ID of the validation session
    42|         Returns:
    43|             A dictionary consisting of response params to the /getValidated3pid
    44|             endpoint of the Identity Service API, or None if the threepid was not found
    45|         """
    46|         client_secret = creds.get("client_secret") or creds.get("clientSecret")
    47|         if not client_secret:
    48|             raise SynapseError(
    49|                 400, "Missing param client_secret in creds", errcode=Codes.MISSING_PARAM
    50|             )
    51|         assert_valid_client_secret(client_secret)
    52|         session_id = creds.get("sid")
    53|         if not session_id:
    54|             raise SynapseError(
    55|                 400, "Missing param session_id in creds", errcode=Codes.MISSING_PARAM
    56|             )
    57|         query_params = {"sid": session_id, "client_secret": client_secret}
    58|         url = id_server + "/_matrix/identity/api/v1/3pid/getValidated3pid"
    59|         try:
    60|             data = await self.http_client.get_json(url, query_params)
    61|         except TimeoutError:
    62|             raise SynapseError(500, "Timed out contacting identity server")
    63|         except HttpResponseException as e:
    64|             logger.info(
    65|                 "%s returned %i for threepid validation for: %s",
    66|                 id_server,
    67|                 e.code,
    68|                 creds,
    69|             )
    70|             return None
    71|         if "medium" in data:
    72|             return data
    73|         logger.info("%s reported non-validated threepid: %s", id_server, creds)
    74|         return None
    75|     async def bind_threepid(
    76|         self,
    77|         client_secret: str,
    78|         sid: str,
    79|         mxid: str,
    80|         id_server: str,
    81|         id_access_token: Optional[str] = None,

# --- HUNK 2: Lines 101-141 ---
   101|         if use_v2:
   102|             bind_url = "https://%s/_matrix/identity/v2/3pid/bind" % (id_server,)
   103|             headers["Authorization"] = create_id_access_token_header(id_access_token)  # type: ignore
   104|         else:
   105|             bind_url = "https://%s/_matrix/identity/api/v1/3pid/bind" % (id_server,)
   106|         try:
   107|             data = await self.blacklisting_http_client.post_json_get_json(
   108|                 bind_url, bind_data, headers=headers
   109|             )
   110|             await self.store.add_user_bound_threepid(
   111|                 user_id=mxid,
   112|                 medium=data["medium"],
   113|                 address=data["address"],
   114|                 id_server=id_server,
   115|             )
   116|             return data
   117|         except HttpResponseException as e:
   118|             if e.code != 404 or not use_v2:
   119|                 logger.error("3PID bind failed with Matrix error: %r", e)
   120|                 raise e.to_synapse_error()
   121|         except TimeoutError:
   122|             raise SynapseError(500, "Timed out contacting identity server")
   123|         except CodeMessageException as e:
   124|             data = json_decoder.decode(e.msg)  # XXX WAT?
   125|             return data
   126|         logger.info("Got 404 when POSTing JSON %s, falling back to v1 URL", bind_url)
   127|         res = await self.bind_threepid(
   128|             client_secret, sid, mxid, id_server, id_access_token, use_v2=False
   129|         )
   130|         return res
   131|     async def try_unbind_threepid(self, mxid: str, threepid: dict) -> bool:
   132|         """Attempt to remove a 3PID from an identity server, or if one is not provided, all
   133|         identity servers we're aware the binding is present on
   134|         Args:
   135|             mxid: Matrix user ID of binding to be removed
   136|             threepid: Dict with medium & address of binding to be
   137|                 removed, and an optional id_server.
   138|         Raises:
   139|             SynapseError: If we failed to contact the identity server
   140|         Returns:
   141|             True on success, otherwise False if the identity

# --- HUNK 3: Lines 179-219 ---
   179|         auth_headers = self.federation_http_client.build_auth_headers(
   180|             destination=None,
   181|             method=b"POST",
   182|             url_bytes=url_bytes,
   183|             content=content,
   184|             destination_is=id_server.encode("ascii"),
   185|         )
   186|         headers = {b"Authorization": auth_headers}
   187|         try:
   188|             await self.blacklisting_http_client.post_json_get_json(
   189|                 url, content, headers
   190|             )
   191|             changed = True
   192|         except HttpResponseException as e:
   193|             changed = False
   194|             if e.code in (400, 404, 501):
   195|                 logger.warning("Received %d response while unbinding threepid", e.code)
   196|             else:
   197|                 logger.error("Failed to unbind threepid on identity server: %s", e)
   198|                 raise SynapseError(500, "Failed to contact identity server")
   199|         except TimeoutError:
   200|             raise SynapseError(500, "Timed out contacting identity server")
   201|         await self.store.remove_user_bound_threepid(
   202|             user_id=mxid,
   203|             medium=threepid["medium"],
   204|             address=threepid["address"],
   205|             id_server=id_server,
   206|         )
   207|         return changed
   208|     async def send_threepid_validation(
   209|         self,
   210|         email_address: str,
   211|         client_secret: str,
   212|         send_attempt: int,
   213|         send_email_func: Callable[[str, str, str, str], Awaitable],
   214|         next_link: Optional[str] = None,
   215|     ) -> str:
   216|         """Send a threepid validation email for password reset or
   217|         registration purposes
   218|         Args:
   219|             email_address: The user's email address

# --- HUNK 4: Lines 292-332 ---
   292|             "send_attempt": send_attempt,
   293|         }
   294|         if next_link:
   295|             params["next_link"] = next_link
   296|         if self.hs.config.using_identity_server_from_trusted_list:
   297|             logger.warning(
   298|                 'The config option "trust_identity_server_for_password_resets" '
   299|                 'has been replaced by "account_threepid_delegate". '
   300|                 "Please consult the sample config at docs/sample_config.yaml for "
   301|                 "details and update your config file."
   302|             )
   303|         try:
   304|             data = await self.http_client.post_json_get_json(
   305|                 id_server + "/_matrix/identity/api/v1/validate/email/requestToken",
   306|                 params,
   307|             )
   308|             return data
   309|         except HttpResponseException as e:
   310|             logger.info("Proxied requestToken failed: %r", e)
   311|             raise e.to_synapse_error()
   312|         except TimeoutError:
   313|             raise SynapseError(500, "Timed out contacting identity server")
   314|     async def requestMsisdnToken(
   315|         self,
   316|         id_server: str,
   317|         country: str,
   318|         phone_number: str,
   319|         client_secret: str,
   320|         send_attempt: int,
   321|         next_link: Optional[str] = None,
   322|     ) -> JsonDict:
   323|         """
   324|         Request an external server send an SMS message on our behalf for the purposes of
   325|         threepid validation.
   326|         Args:
   327|             id_server: The identity server to proxy to
   328|             country: The country code of the phone number
   329|             phone_number: The number to send the message to
   330|             client_secret: The unique client_secret sends by the user
   331|             send_attempt: Which attempt this is
   332|             next_link: A link to redirect the user to once they submit the token

# --- HUNK 5: Lines 339-379 ---
   339|             "client_secret": client_secret,
   340|             "send_attempt": send_attempt,
   341|         }
   342|         if next_link:
   343|             params["next_link"] = next_link
   344|         if self.hs.config.using_identity_server_from_trusted_list:
   345|             logger.warning(
   346|                 'The config option "trust_identity_server_for_password_resets" '
   347|                 'has been replaced by "account_threepid_delegate". '
   348|                 "Please consult the sample config at docs/sample_config.yaml for "
   349|                 "details and update your config file."
   350|             )
   351|         try:
   352|             data = await self.http_client.post_json_get_json(
   353|                 id_server + "/_matrix/identity/api/v1/validate/msisdn/requestToken",
   354|                 params,
   355|             )
   356|         except HttpResponseException as e:
   357|             logger.info("Proxied requestToken failed: %r", e)
   358|             raise e.to_synapse_error()
   359|         except TimeoutError:
   360|             raise SynapseError(500, "Timed out contacting identity server")
   361|         assert self.hs.config.public_baseurl
   362|         data["submit_url"] = (
   363|             self.hs.config.public_baseurl
   364|             + "_matrix/client/unstable/add_threepid/msisdn/submit_token"
   365|         )
   366|         return data
   367|     async def validate_threepid_session(
   368|         self, client_secret: str, sid: str
   369|     ) -> Optional[JsonDict]:
   370|         """Validates a threepid session with only the client secret and session ID
   371|         Tries validating against any configured account_threepid_delegates as well as locally.
   372|         Args:
   373|             client_secret: A secret provided by the client
   374|             sid: The ID of the session
   375|         Returns:
   376|             The json response if validation was successful, otherwise None
   377|         """
   378|         threepid_creds = {"client_secret": client_secret, "sid": sid}
   379|         validation_session = None

# --- HUNK 6: Lines 395-435 ---
   395|     async def proxy_msisdn_submit_token(
   396|         self, id_server: str, client_secret: str, sid: str, token: str
   397|     ) -> JsonDict:
   398|         """Proxy a POST submitToken request to an identity server for verification purposes
   399|         Args:
   400|             id_server: The identity server URL to contact
   401|             client_secret: Secret provided by the client
   402|             sid: The ID of the session
   403|             token: The verification token
   404|         Raises:
   405|             SynapseError: If we failed to contact the identity server
   406|         Returns:
   407|             The response dict from the identity server
   408|         """
   409|         body = {"client_secret": client_secret, "sid": sid, "token": token}
   410|         try:
   411|             return await self.http_client.post_json_get_json(
   412|                 id_server + "/_matrix/identity/api/v1/validate/msisdn/submitToken",
   413|                 body,
   414|             )
   415|         except TimeoutError:
   416|             raise SynapseError(500, "Timed out contacting identity server")
   417|         except HttpResponseException as e:
   418|             logger.warning("Error contacting msisdn account_threepid_delegate: %s", e)
   419|             raise SynapseError(400, "Error contacting the identity server")
   420|     async def lookup_3pid(
   421|         self,
   422|         id_server: str,
   423|         medium: str,
   424|         address: str,
   425|         id_access_token: Optional[str] = None,
   426|     ) -> Optional[str]:
   427|         """Looks up a 3pid in the passed identity server.
   428|         Args:
   429|             id_server: The server name (including port, if required)
   430|                 of the identity server to use.
   431|             medium: The type of the third party identifier (e.g. "email").
   432|             address: The third party identifier (e.g. "foo@example.com").
   433|             id_access_token: The access token to authenticate to the identity
   434|                 server with
   435|         Returns:

# --- HUNK 7: Lines 454-517 ---
   454|         return await self._lookup_3pid_v1(id_server, medium, address)
   455|     async def _lookup_3pid_v1(
   456|         self, id_server: str, medium: str, address: str
   457|     ) -> Optional[str]:
   458|         """Looks up a 3pid in the passed identity server using v1 lookup.
   459|         Args:
   460|             id_server: The server name (including port, if required)
   461|                 of the identity server to use.
   462|             medium: The type of the third party identifier (e.g. "email").
   463|             address: The third party identifier (e.g. "foo@example.com").
   464|         Returns:
   465|             the matrix ID of the 3pid, or None if it is not recognized.
   466|         """
   467|         try:
   468|             data = await self.blacklisting_http_client.get_json(
   469|                 "%s%s/_matrix/identity/api/v1/lookup" % (id_server_scheme, id_server),
   470|                 {"medium": medium, "address": address},
   471|             )
   472|             if "mxid" in data:
   473|                 return data["mxid"]
   474|         except TimeoutError:
   475|             raise SynapseError(500, "Timed out contacting identity server")
   476|         except IOError as e:
   477|             logger.warning("Error from v1 identity server lookup: %s" % (e,))
   478|         return None
   479|     async def _lookup_3pid_v2(
   480|         self, id_server: str, id_access_token: str, medium: str, address: str
   481|     ) -> Optional[str]:
   482|         """Looks up a 3pid in the passed identity server using v2 lookup.
   483|         Args:
   484|             id_server: The server name (including port, if required)
   485|                 of the identity server to use.
   486|             id_access_token: The access token to authenticate to the identity server with
   487|             medium: The type of the third party identifier (e.g. "email").
   488|             address: The third party identifier (e.g. "foo@example.com").
   489|         Returns:
   490|             the matrix ID of the 3pid, or None if it is not recognised.
   491|         """
   492|         try:
   493|             hash_details = await self.blacklisting_http_client.get_json(
   494|                 "%s%s/_matrix/identity/v2/hash_details" % (id_server_scheme, id_server),
   495|                 {"access_token": id_access_token},
   496|             )
   497|         except TimeoutError:
   498|             raise SynapseError(500, "Timed out contacting identity server")
   499|         if not isinstance(hash_details, dict):
   500|             logger.warning(
   501|                 "Got non-dict object when checking hash details of %s%s: %s",
   502|                 id_server_scheme,
   503|                 id_server,
   504|                 hash_details,
   505|             )
   506|             raise SynapseError(
   507|                 400,
   508|                 "Non-dict object from %s%s during v2 hash_details request: %s"
   509|                 % (id_server_scheme, id_server, hash_details),
   510|             )
   511|         supported_lookup_algorithms = hash_details.get("algorithms")
   512|         lookup_pepper = hash_details.get("lookup_pepper")
   513|         if (
   514|             not supported_lookup_algorithms
   515|             or not isinstance(supported_lookup_algorithms, list)
   516|             or not lookup_pepper
   517|             or not isinstance(lookup_pepper, str)

# --- HUNK 8: Lines 533-573 ---
   533|                 "None of the provided lookup algorithms of %s are supported: %s",
   534|                 id_server,
   535|                 supported_lookup_algorithms,
   536|             )
   537|             raise SynapseError(
   538|                 400,
   539|                 "Provided identity server does not support any v2 lookup "
   540|                 "algorithms that this homeserver supports.",
   541|             )
   542|         headers = {"Authorization": create_id_access_token_header(id_access_token)}
   543|         try:
   544|             lookup_results = await self.blacklisting_http_client.post_json_get_json(
   545|                 "%s%s/_matrix/identity/v2/lookup" % (id_server_scheme, id_server),
   546|                 {
   547|                     "addresses": [lookup_value],
   548|                     "algorithm": lookup_algorithm,
   549|                     "pepper": lookup_pepper,
   550|                 },
   551|                 headers=headers,
   552|             )
   553|         except TimeoutError:
   554|             raise SynapseError(500, "Timed out contacting identity server")
   555|         except Exception as e:
   556|             logger.warning("Error when performing a v2 3pid lookup: %s", e)
   557|             raise SynapseError(
   558|                 500, "Unknown error occurred during identity server lookup"
   559|             )
   560|         if "mappings" not in lookup_results or not isinstance(
   561|             lookup_results["mappings"], dict
   562|         ):
   563|             logger.warning("No results from 3pid lookup")
   564|             return None
   565|         mxid = lookup_results["mappings"].get(lookup_value)
   566|         return mxid
   567|     async def ask_id_server_for_third_party_invite(
   568|         self,
   569|         requester: Requester,
   570|         id_server: str,
   571|         medium: str,
   572|         address: str,
   573|         room_id: str,

# --- HUNK 9: Lines 616-672 ---
   616|             "room_join_rules": room_join_rules,
   617|             "room_name": room_name,
   618|             "sender": inviter_user_id,
   619|             "sender_display_name": inviter_display_name,
   620|             "sender_avatar_url": inviter_avatar_url,
   621|         }
   622|         data = None
   623|         base_url = "%s%s/_matrix/identity" % (id_server_scheme, id_server)
   624|         if id_access_token:
   625|             key_validity_url = "%s%s/_matrix/identity/v2/pubkey/isvalid" % (
   626|                 id_server_scheme,
   627|                 id_server,
   628|             )
   629|             url = base_url + "/v2/store-invite"
   630|             try:
   631|                 data = await self.blacklisting_http_client.post_json_get_json(
   632|                     url,
   633|                     invite_config,
   634|                     {"Authorization": create_id_access_token_header(id_access_token)},
   635|                 )
   636|             except TimeoutError:
   637|                 raise SynapseError(500, "Timed out contacting identity server")
   638|             except HttpResponseException as e:
   639|                 if e.code != 404:
   640|                     logger.info("Failed to POST %s with JSON: %s", url, e)
   641|                     raise e
   642|         if data is None:
   643|             key_validity_url = "%s%s/_matrix/identity/api/v1/pubkey/isvalid" % (
   644|                 id_server_scheme,
   645|                 id_server,
   646|             )
   647|             url = base_url + "/api/v1/store-invite"
   648|             try:
   649|                 data = await self.blacklisting_http_client.post_json_get_json(
   650|                     url, invite_config
   651|                 )
   652|             except TimeoutError:
   653|                 raise SynapseError(500, "Timed out contacting identity server")
   654|             except HttpResponseException as e:
   655|                 logger.warning(
   656|                     "Error trying to call /store-invite on %s%s: %s",
   657|                     id_server_scheme,
   658|                     id_server,
   659|                     e,
   660|                 )
   661|             if data is None:
   662|                 try:
   663|                     data = await self.blacklisting_http_client.post_urlencoded_get_json(
   664|                         url, invite_config
   665|                     )
   666|                 except HttpResponseException as e:
   667|                     logger.warning(
   668|                         "Error calling /store-invite on %s%s with fallback "
   669|                         "encoding: %s",
   670|                         id_server_scheme,
   671|                         id_server,
   672|                         e,


# ====================================================================
# FILE: synapse/handlers/initial_sync.py
# Total hunks: 7
# ====================================================================
# --- HUNK 1: Lines 1-42 ---
     1| import logging
     2| from typing import TYPE_CHECKING
     3| from twisted.internet import defer
     4| from synapse.api.constants import EventTypes, Membership
     5| from synapse.api.errors import SynapseError
     6| from synapse.events.validator import EventValidator
     7| from synapse.handlers.presence import format_user_presence_state
     8| from synapse.logging.context import make_deferred_yieldable, run_in_background
     9| from synapse.storage.roommember import RoomsForUser
    10| from synapse.streams.config import PaginationConfig
    11| from synapse.types import JsonDict, Requester, StreamToken, UserID
    12| from synapse.util import unwrapFirstError
    13| from synapse.util.async_helpers import concurrently_execute
    14| from synapse.util.caches.response_cache import ResponseCache
    15| from synapse.visibility import filter_events_for_client
    16| from ._base import BaseHandler
    17| if TYPE_CHECKING:
    18|     from synapse.server import HomeServer
    19| logger = logging.getLogger(__name__)
    20| class InitialSyncHandler(BaseHandler):
    21|     def __init__(self, hs: "HomeServer"):
    22|         super(InitialSyncHandler, self).__init__(hs)
    23|         self.hs = hs
    24|         self.state = hs.get_state_handler()
    25|         self.clock = hs.get_clock()
    26|         self.validator = EventValidator()
    27|         self.snapshot_cache = ResponseCache(hs, "initial_sync_cache")
    28|         self._event_serializer = hs.get_event_client_serializer()
    29|         self.storage = hs.get_storage()
    30|         self.state_store = self.storage.state
    31|     def snapshot_all_rooms(
    32|         self,
    33|         user_id: str,
    34|         pagin_config: PaginationConfig,
    35|         as_client_event: bool = True,
    36|         include_archived: bool = False,
    37|     ) -> JsonDict:
    38|         """Retrieve a snapshot of all rooms the user is invited or has joined.
    39|         This snapshot may include messages for all rooms where the user is
    40|         joined, depending on the pagination config.
    41|         Args:
    42|             user_id: The ID of the user making the request.

# --- HUNK 2: Lines 65-216 ---
    65|             as_client_event,
    66|             include_archived,
    67|         )
    68|     async def _snapshot_all_rooms(
    69|         self,
    70|         user_id: str,
    71|         pagin_config: PaginationConfig,
    72|         as_client_event: bool = True,
    73|         include_archived: bool = False,
    74|     ) -> JsonDict:
    75|         memberships = [Membership.INVITE, Membership.JOIN]
    76|         if include_archived:
    77|             memberships.append(Membership.LEAVE)
    78|         room_list = await self.store.get_rooms_for_local_user_where_membership_is(
    79|             user_id=user_id, membership_list=memberships
    80|         )
    81|         user = UserID.from_string(user_id)
    82|         rooms_ret = []
    83|         now_token = self.hs.get_event_sources().get_current_token()
    84|         presence_stream = self.hs.get_event_sources().sources["presence"]
    85|         pagination_config = PaginationConfig(from_token=now_token)
    86|         presence, _ = await presence_stream.get_pagination_rows(
    87|             user, pagination_config.get_source_config("presence"), None
    88|         )
    89|         receipt_stream = self.hs.get_event_sources().sources["receipt"]
    90|         receipt, _ = await receipt_stream.get_pagination_rows(
    91|             user, pagination_config.get_source_config("receipt"), None
    92|         )
    93|         tags_by_room = await self.store.get_tags_for_user(user_id)
    94|         account_data, account_data_by_room = await self.store.get_account_data_for_user(
    95|             user_id
    96|         )
    97|         public_room_ids = await self.store.get_public_room_ids()
    98|         limit = pagin_config.limit
    99|         if limit is None:
   100|             limit = 10
   101|         async def handle_room(event: RoomsForUser):
   102|             d = {
   103|                 "room_id": event.room_id,
   104|                 "membership": event.membership,
   105|                 "visibility": (
   106|                     "public" if event.room_id in public_room_ids else "private"
   107|                 ),
   108|             }
   109|             if event.membership == Membership.INVITE:
   110|                 time_now = self.clock.time_msec()
   111|                 d["inviter"] = event.sender
   112|                 invite_event = await self.store.get_event(event.event_id)
   113|                 d["invite"] = await self._event_serializer.serialize_event(
   114|                     invite_event, time_now, as_client_event
   115|                 )
   116|             rooms_ret.append(d)
   117|             if event.membership not in (Membership.JOIN, Membership.LEAVE):
   118|                 return
   119|             try:
   120|                 if event.membership == Membership.JOIN:
   121|                     room_end_token = now_token.room_key
   122|                     deferred_room_state = run_in_background(
   123|                         self.state_handler.get_current_state, event.room_id
   124|                     )
   125|                 elif event.membership == Membership.LEAVE:
   126|                     room_end_token = "s%d" % (event.stream_ordering,)
   127|                     deferred_room_state = run_in_background(
   128|                         self.state_store.get_state_for_events, [event.event_id]
   129|                     )
   130|                     deferred_room_state.addCallback(
   131|                         lambda states: states[event.event_id]
   132|                     )
   133|                 (messages, token), current_state = await make_deferred_yieldable(
   134|                     defer.gatherResults(
   135|                         [
   136|                             run_in_background(
   137|                                 self.store.get_recent_events_for_room,
   138|                                 event.room_id,
   139|                                 limit=limit,
   140|                                 end_token=room_end_token,
   141|                             ),
   142|                             deferred_room_state,
   143|                         ]
   144|                     )
   145|                 ).addErrback(unwrapFirstError)
   146|                 messages = await filter_events_for_client(
   147|                     self.storage, user_id, messages
   148|                 )
   149|                 start_token = now_token.copy_and_replace("room_key", token)
   150|                 end_token = now_token.copy_and_replace("room_key", room_end_token)
   151|                 time_now = self.clock.time_msec()
   152|                 d["messages"] = {
   153|                     "chunk": (
   154|                         await self._event_serializer.serialize_events(
   155|                             messages, time_now=time_now, as_client_event=as_client_event
   156|                         )
   157|                     ),
   158|                     "start": start_token.to_string(),
   159|                     "end": end_token.to_string(),
   160|                 }
   161|                 d["state"] = await self._event_serializer.serialize_events(
   162|                     current_state.values(),
   163|                     time_now=time_now,
   164|                     as_client_event=as_client_event,
   165|                 )
   166|                 account_data_events = []
   167|                 tags = tags_by_room.get(event.room_id)
   168|                 if tags:
   169|                     account_data_events.append(
   170|                         {"type": "m.tag", "content": {"tags": tags}}
   171|                     )
   172|                 account_data = account_data_by_room.get(event.room_id, {})
   173|                 for account_data_type, content in account_data.items():
   174|                     account_data_events.append(
   175|                         {"type": account_data_type, "content": content}
   176|                     )
   177|                 d["account_data"] = account_data_events
   178|             except Exception:
   179|                 logger.exception("Failed to get snapshot")
   180|         await concurrently_execute(handle_room, room_list, 10)
   181|         account_data_events = []
   182|         for account_data_type, content in account_data.items():
   183|             account_data_events.append({"type": account_data_type, "content": content})
   184|         now = self.clock.time_msec()
   185|         ret = {
   186|             "rooms": rooms_ret,
   187|             "presence": [
   188|                 {
   189|                     "type": "m.presence",
   190|                     "content": format_user_presence_state(event, now),
   191|                 }
   192|                 for event in presence
   193|             ],
   194|             "account_data": account_data_events,
   195|             "receipts": receipt,
   196|             "end": now_token.to_string(),
   197|         }
   198|         return ret
   199|     async def room_initial_sync(
   200|         self, requester: Requester, room_id: str, pagin_config: PaginationConfig
   201|     ) -> JsonDict:
   202|         """Capture the a snapshot of a room. If user is currently a member of
   203|         the room this will be what is currently in the room. If the user left
   204|         the room this will be what was in the room when they left.
   205|         Args:
   206|             requester: The user to get a snapshot for.
   207|             room_id: The room to get a snapshot of.
   208|             pagin_config: The pagination config used to determine how many
   209|                 messages to return.
   210|         Raises:
   211|             AuthError if the user wasn't in the room.
   212|         Returns:
   213|             A JSON serialisable dict with the snapshot of the room.
   214|         """
   215|         blocked = await self.store.is_room_blocked(room_id)
   216|         if blocked:

# --- HUNK 3: Lines 237-295 ---
   237|             account_data_events.append({"type": "m.tag", "content": {"tags": tags}})
   238|         account_data = await self.store.get_account_data_for_room(user_id, room_id)
   239|         for account_data_type, content in account_data.items():
   240|             account_data_events.append({"type": account_data_type, "content": content})
   241|         result["account_data"] = account_data_events
   242|         return result
   243|     async def _room_initial_sync_parted(
   244|         self,
   245|         user_id: str,
   246|         room_id: str,
   247|         pagin_config: PaginationConfig,
   248|         membership: Membership,
   249|         member_event_id: str,
   250|         is_peeking: bool,
   251|     ) -> JsonDict:
   252|         room_state = await self.state_store.get_state_for_events([member_event_id])
   253|         room_state = room_state[member_event_id]
   254|         limit = pagin_config.limit if pagin_config else None
   255|         if limit is None:
   256|             limit = 10
   257|         stream_token = await self.store.get_stream_token_for_event(member_event_id)
   258|         messages, token = await self.store.get_recent_events_for_room(
   259|             room_id, limit=limit, end_token=stream_token
   260|         )
   261|         messages = await filter_events_for_client(
   262|             self.storage, user_id, messages, is_peeking=is_peeking
   263|         )
   264|         start_token = StreamToken.START.copy_and_replace("room_key", token)
   265|         end_token = StreamToken.START.copy_and_replace("room_key", stream_token)
   266|         time_now = self.clock.time_msec()
   267|         return {
   268|             "membership": membership,
   269|             "room_id": room_id,
   270|             "messages": {
   271|                 "chunk": (
   272|                     await self._event_serializer.serialize_events(messages, time_now)
   273|                 ),
   274|                 "start": start_token.to_string(),
   275|                 "end": end_token.to_string(),
   276|             },
   277|             "state": (
   278|                 await self._event_serializer.serialize_events(
   279|                     room_state.values(), time_now
   280|                 )
   281|             ),
   282|             "presence": [],
   283|             "receipts": [],
   284|         }
   285|     async def _room_initial_sync_joined(
   286|         self,
   287|         user_id: str,
   288|         room_id: str,
   289|         pagin_config: PaginationConfig,
   290|         membership: Membership,
   291|         is_peeking: bool,
   292|     ) -> JsonDict:
   293|         current_state = await self.state.get_current_state(room_id=room_id)
   294|         time_now = self.clock.time_msec()
   295|         state = await self._event_serializer.serialize_events(

# --- HUNK 4: Lines 336-365 ---
   336|                         room_id,
   337|                         limit=limit,
   338|                         end_token=now_token.room_key,
   339|                     ),
   340|                 ],
   341|                 consumeErrors=True,
   342|             ).addErrback(unwrapFirstError)
   343|         )
   344|         messages = await filter_events_for_client(
   345|             self.storage, user_id, messages, is_peeking=is_peeking
   346|         )
   347|         start_token = now_token.copy_and_replace("room_key", token)
   348|         end_token = now_token
   349|         time_now = self.clock.time_msec()
   350|         ret = {
   351|             "room_id": room_id,
   352|             "messages": {
   353|                 "chunk": (
   354|                     await self._event_serializer.serialize_events(messages, time_now)
   355|                 ),
   356|                 "start": start_token.to_string(),
   357|                 "end": end_token.to_string(),
   358|             },
   359|             "state": state,
   360|             "presence": presence,
   361|             "receipts": receipts,
   362|         }
   363|         if not is_peeking:
   364|             ret["membership"] = membership
   365|         return ret


# ====================================================================
# FILE: synapse/handlers/message.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 258-304 ---
   258|             await self.store.expire_event(event_id)
   259|         except Exception as e:
   260|             logger.error("Could not expire event %s: %r", event_id, e)
   261|         await self._schedule_next_expiry()
   262| _DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY = 7 * 24 * 60 * 60 * 1000
   263| class EventCreationHandler:
   264|     def __init__(self, hs: "HomeServer"):
   265|         self.hs = hs
   266|         self.auth = hs.get_auth()
   267|         self.store = hs.get_datastore()
   268|         self.storage = hs.get_storage()
   269|         self.state = hs.get_state_handler()
   270|         self.clock = hs.get_clock()
   271|         self.validator = EventValidator()
   272|         self.profile_handler = hs.get_profile_handler()
   273|         self.event_builder_factory = hs.get_event_builder_factory()
   274|         self.server_name = hs.hostname
   275|         self.notifier = hs.get_notifier()
   276|         self.config = hs.config
   277|         self.require_membership_for_aliases = hs.config.require_membership_for_aliases
   278|         self._is_event_writer = (
   279|             self.config.worker.writers.events == hs.get_instance_name()
   280|         )
   281|         self.room_invite_state_types = self.hs.config.room_invite_state_types
   282|         self.send_event = ReplicationSendEventRestServlet.make_client(hs)
   283|         self.base_handler = BaseHandler(hs)
   284|         self.pusher_pool = hs.get_pusherpool()
   285|         self.limiter = Linearizer(max_count=5, name="room_event_creation_limit")
   286|         self.action_generator = hs.get_action_generator()
   287|         self.spam_checker = hs.get_spam_checker()
   288|         self.third_party_event_rules = hs.get_third_party_event_rules()
   289|         self._block_events_without_consent_error = (
   290|             self.config.block_events_without_consent_error
   291|         )
   292|         self._rooms_to_exclude_from_dummy_event_insertion = {}  # type: Dict[str, int]
   293|         if self._block_events_without_consent_error:
   294|             self._consent_uri_builder = ConsentURIBuilder(self.config)
   295|         if (
   296|             not self.config.worker_app
   297|             and self.config.cleanup_extremities_with_dummy_events
   298|         ):
   299|             self.clock.looping_call(
   300|                 lambda: run_as_background_process(
   301|                     "send_dummy_events_to_fill_extremities",
   302|                     self._send_dummy_events_to_fill_extremities,
   303|                 ),
   304|                 5 * 60 * 1000,

# --- HUNK 2: Lines 649-691 ---
   649|             raise SynapseError(
   650|                 403, "This event is not allowed in this context", Codes.FORBIDDEN
   651|             )
   652|         if event.internal_metadata.is_out_of_band_membership():
   653|             assert event.type == EventTypes.Member
   654|             assert event.content["membership"] == Membership.LEAVE
   655|         else:
   656|             try:
   657|                 await self.auth.check_from_context(room_version, event, context)
   658|             except AuthError as err:
   659|                 logger.warning("Denying new event %r because %s", event, err)
   660|                 raise err
   661|         try:
   662|             dump = frozendict_json_encoder.encode(event.content)
   663|             json_decoder.decode(dump)
   664|         except Exception:
   665|             logger.exception("Failed to encode content: %r", event.content)
   666|             raise
   667|         await self.action_generator.handle_push_actions_for_event(event, context)
   668|         try:
   669|             if not self._is_event_writer:
   670|                 result = await self.send_event(
   671|                     instance_name=self.config.worker.writers.events,
   672|                     event_id=event.event_id,
   673|                     store=self.store,
   674|                     requester=requester,
   675|                     event=event,
   676|                     context=context,
   677|                     ratelimit=ratelimit,
   678|                     extra_users=extra_users,
   679|                 )
   680|                 stream_id = result["stream_id"]
   681|                 event.internal_metadata.stream_ordering = stream_id
   682|                 return stream_id
   683|             stream_id = await self.persist_and_notify_client_event(
   684|                 requester, event, context, ratelimit=ratelimit, extra_users=extra_users
   685|             )
   686|             return stream_id
   687|         except Exception:
   688|             await self.store.remove_push_actions_from_staging(event.event_id)
   689|             raise
   690|     async def _validate_canonical_alias(
   691|         self, directory_handler, room_alias_str: str, expected_room_id: str

# --- HUNK 3: Lines 709-749 ---
   709|                 )
   710|             raise
   711|         if mapping["room_id"] != expected_room_id:
   712|             raise SynapseError(
   713|                 400,
   714|                 "Room alias %s does not point to the room" % (room_alias_str,),
   715|                 Codes.BAD_ALIAS,
   716|             )
   717|     async def persist_and_notify_client_event(
   718|         self,
   719|         requester: Requester,
   720|         event: EventBase,
   721|         context: EventContext,
   722|         ratelimit: bool = True,
   723|         extra_users: List[UserID] = [],
   724|     ) -> int:
   725|         """Called when we have fully built the event, have already
   726|         calculated the push actions for the event, and checked auth.
   727|         This should only be run on the instance in charge of persisting events.
   728|         """
   729|         assert self._is_event_writer
   730|         if ratelimit:
   731|             is_admin_redaction = False
   732|             if event.type == EventTypes.Redaction:
   733|                 original_event = await self.store.get_event(
   734|                     event.redacts,
   735|                     redact_behaviour=EventRedactBehaviour.AS_IS,
   736|                     get_prev_content=False,
   737|                     allow_rejected=False,
   738|                     allow_none=True,
   739|                 )
   740|                 is_admin_redaction = bool(
   741|                     original_event and event.sender != original_event.sender
   742|                 )
   743|             await self.base_handler.ratelimit(
   744|                 requester, is_admin_redaction=is_admin_redaction
   745|             )
   746|         await self.base_handler.maybe_kick_guest_users(event, context)
   747|         if event.type == EventTypes.CanonicalAlias:
   748|             original_alias = None
   749|             original_alt_aliases = []  # type: List[str]

# --- HUNK 4: Lines 818-930 ---
   818|             prev_state_ids = await context.get_prev_state_ids()
   819|             auth_events_ids = self.auth.compute_auth_events(
   820|                 event, prev_state_ids, for_verification=True
   821|             )
   822|             auth_events_map = await self.store.get_events(auth_events_ids)
   823|             auth_events = {(e.type, e.state_key): e for e in auth_events_map.values()}
   824|             room_version = await self.store.get_room_version_id(event.room_id)
   825|             room_version_obj = KNOWN_ROOM_VERSIONS[room_version]
   826|             if event_auth.check_redaction(
   827|                 room_version_obj, event, auth_events=auth_events
   828|             ):
   829|                 if not original_event:
   830|                     raise NotFoundError("Could not find event %s" % (event.redacts,))
   831|                 if event.user_id != original_event.user_id:
   832|                     raise AuthError(403, "You don't have permission to redact events")
   833|                 event.internal_metadata.recheck_redaction = False
   834|         if event.type == EventTypes.Create:
   835|             prev_state_ids = await context.get_prev_state_ids()
   836|             if prev_state_ids:
   837|                 raise AuthError(403, "Changing the room create event is forbidden")
   838|         event_stream_id, max_stream_id = await self.storage.persistence.persist_event(
   839|             event, context=context
   840|         )
   841|         if self._ephemeral_events_enabled:
   842|             self._message_handler.maybe_schedule_expiry(event)
   843|         await self.pusher_pool.on_new_notifications(event_stream_id, max_stream_id)
   844|         def _notify():
   845|             try:
   846|                 self.notifier.on_new_room_event(
   847|                     event, event_stream_id, max_stream_id, extra_users=extra_users
   848|                 )
   849|             except Exception:
   850|                 logger.exception("Error notifying about new room event")
   851|         run_in_background(_notify)
   852|         if event.type == EventTypes.Message:
   853|             run_in_background(self._bump_active_time, requester.user)
   854|         return event_stream_id
   855|     async def _bump_active_time(self, user: UserID) -> None:
   856|         try:
   857|             presence = self.hs.get_presence_handler()
   858|             await presence.bump_presence_active_time(user)
   859|         except Exception:
   860|             logger.exception("Error bumping presence active time")
   861|     async def _send_dummy_events_to_fill_extremities(self):
   862|         """Background task to send dummy events into rooms that have a large
   863|         number of extremities
   864|         """
   865|         self._expire_rooms_to_exclude_from_dummy_event_insertion()
   866|         room_ids = await self.store.get_rooms_with_many_extremities(
   867|             min_count=self._dummy_events_threshold,
   868|             limit=5,
   869|             room_id_filter=self._rooms_to_exclude_from_dummy_event_insertion.keys(),
   870|         )
   871|         for room_id in room_ids:
   872|             latest_event_ids = await self.store.get_prev_events_for_room(room_id)
   873|             members = await self.state.get_current_users_in_room(
   874|                 room_id, latest_event_ids=latest_event_ids
   875|             )
   876|             dummy_event_sent = False
   877|             for user_id in members:
   878|                 if not self.hs.is_mine_id(user_id):
   879|                     continue
   880|                 requester = create_requester(user_id)
   881|                 try:
   882|                     event, context = await self.create_event(
   883|                         requester,
   884|                         {
   885|                             "type": "org.matrix.dummy_event",
   886|                             "content": {},
   887|                             "room_id": room_id,
   888|                             "sender": user_id,
   889|                         },
   890|                         prev_event_ids=latest_event_ids,
   891|                     )
   892|                     event.internal_metadata.proactively_send = False
   893|                     await self.send_nonmember_event(
   894|                         requester,
   895|                         event,
   896|                         context,
   897|                         ratelimit=False,
   898|                         ignore_shadow_ban=True,
   899|                     )
   900|                     dummy_event_sent = True
   901|                     break
   902|                 except ConsentNotGivenError:
   903|                     logger.info(
   904|                         "Failed to send dummy event into room %s for user %s due to "
   905|                         "lack of consent. Will try another user" % (room_id, user_id)
   906|                     )
   907|                 except AuthError:
   908|                     logger.info(
   909|                         "Failed to send dummy event into room %s for user %s due to "
   910|                         "lack of power. Will try another user" % (room_id, user_id)
   911|                     )
   912|             if not dummy_event_sent:
   913|                 logger.info(
   914|                     "Failed to send dummy event into room %s. Will exclude it from "
   915|                     "future attempts until cache expires" % (room_id,)
   916|                 )
   917|                 now = self.clock.time_msec()
   918|                 self._rooms_to_exclude_from_dummy_event_insertion[room_id] = now
   919|     def _expire_rooms_to_exclude_from_dummy_event_insertion(self):
   920|         expire_before = self.clock.time_msec() - _DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY
   921|         to_expire = set()
   922|         for room_id, time in self._rooms_to_exclude_from_dummy_event_insertion.items():
   923|             if time < expire_before:
   924|                 to_expire.add(room_id)
   925|         for room_id in to_expire:
   926|             logger.debug(
   927|                 "Expiring room id %s from dummy event insertion exclusion cache",
   928|                 room_id,
   929|             )
   930|             del self._rooms_to_exclude_from_dummy_event_insertion[room_id]


# ====================================================================
# FILE: synapse/handlers/oidc_handler.py
# Total hunks: 7
# ====================================================================
# --- HUNK 1: Lines 3-43 ---
     3| from urllib.parse import urlencode
     4| import attr
     5| import pymacaroons
     6| from authlib.common.security import generate_token
     7| from authlib.jose import JsonWebToken
     8| from authlib.oauth2.auth import ClientAuth
     9| from authlib.oauth2.rfc6749.parameters import prepare_grant_uri
    10| from authlib.oidc.core import CodeIDToken, ImplicitIDToken, UserInfo
    11| from authlib.oidc.discovery import OpenIDProviderMetadata, get_well_known_url
    12| from jinja2 import Environment, Template
    13| from pymacaroons.exceptions import (
    14|     MacaroonDeserializationException,
    15|     MacaroonInvalidSignatureException,
    16| )
    17| from typing_extensions import TypedDict
    18| from twisted.web.client import readBody
    19| from synapse.config import ConfigError
    20| from synapse.http.server import respond_with_html
    21| from synapse.http.site import SynapseRequest
    22| from synapse.logging.context import make_deferred_yieldable
    23| from synapse.types import UserID, map_username_to_mxid_localpart
    24| from synapse.util import json_decoder
    25| if TYPE_CHECKING:
    26|     from synapse.server import HomeServer
    27| logger = logging.getLogger(__name__)
    28| SESSION_COOKIE_NAME = b"oidc_session"
    29| Token = TypedDict(
    30|     "Token",
    31|     {
    32|         "access_token": str,
    33|         "token_type": str,
    34|         "id_token": Optional[str],
    35|         "refresh_token": Optional[str],
    36|         "expires_in": int,
    37|         "scope": Optional[str],
    38|     },
    39| )
    40| JWK = Dict[str, str]
    41| JWKS = TypedDict("JWKS", {"keys": List[JWK]})
    42| class OidcError(Exception):
    43|     """Used to catch errors when calling the token_endpoint

# --- HUNK 2: Lines 60-115 ---
    60|         self._callback_url = hs.config.oidc_callback_url  # type: str
    61|         self._scopes = hs.config.oidc_scopes  # type: List[str]
    62|         self._client_auth = ClientAuth(
    63|             hs.config.oidc_client_id,
    64|             hs.config.oidc_client_secret,
    65|             hs.config.oidc_client_auth_method,
    66|         )  # type: ClientAuth
    67|         self._client_auth_method = hs.config.oidc_client_auth_method  # type: str
    68|         self._provider_metadata = OpenIDProviderMetadata(
    69|             issuer=hs.config.oidc_issuer,
    70|             authorization_endpoint=hs.config.oidc_authorization_endpoint,
    71|             token_endpoint=hs.config.oidc_token_endpoint,
    72|             userinfo_endpoint=hs.config.oidc_userinfo_endpoint,
    73|             jwks_uri=hs.config.oidc_jwks_uri,
    74|         )  # type: OpenIDProviderMetadata
    75|         self._provider_needs_discovery = hs.config.oidc_discover  # type: bool
    76|         self._user_mapping_provider = hs.config.oidc_user_mapping_provider_class(
    77|             hs.config.oidc_user_mapping_provider_config
    78|         )  # type: OidcMappingProvider
    79|         self._skip_verification = hs.config.oidc_skip_verification  # type: bool
    80|         self._http_client = hs.get_proxied_http_client()
    81|         self._auth_handler = hs.get_auth_handler()
    82|         self._registration_handler = hs.get_registration_handler()
    83|         self._datastore = hs.get_datastore()
    84|         self._clock = hs.get_clock()
    85|         self._hostname = hs.hostname  # type: str
    86|         self._server_name = hs.config.server_name  # type: str
    87|         self._macaroon_secret_key = hs.config.macaroon_secret_key
    88|         self._error_template = hs.config.sso_error_template
    89|         self._auth_provider_id = "oidc"
    90|     def _render_error(
    91|         self, request, error: str, error_description: Optional[str] = None
    92|     ) -> None:
    93|         """Renders the error template and respond with it.
    94|         This is used to show errors to the user. The template of this page can
    95|         be found under ``synapse/res/templates/sso_error.html``.
    96|         Args:
    97|             request: The incoming request from the browser.
    98|                 We'll respond with an HTML page describing the error.
    99|             error: A technical identifier for this error. Those include
   100|                 well-known OAuth2/OIDC error types like invalid_request or
   101|                 access_denied.
   102|             error_description: A human-readable description of the error.
   103|         """
   104|         html = self._error_template.render(
   105|             error=error, error_description=error_description
   106|         )
   107|         respond_with_html(request, 400, html)
   108|     def _validate_metadata(self):
   109|         """Verifies the provider metadata.
   110|         This checks the validity of the currently loaded provider. Not
   111|         everything is checked, only:
   112|           - ``issuer``
   113|           - ``authorization_endpoint``
   114|           - ``token_endpoint``
   115|           - ``response_types_supported`` (checks if "code" is in it)

# --- HUNK 3: Lines 497-543 ---
   497|         else:
   498|             logger.debug("Extracting userinfo from id_token")
   499|             try:
   500|                 userinfo = await self._parse_id_token(token, nonce=nonce)
   501|             except Exception as e:
   502|                 logger.exception("Invalid id_token")
   503|                 self._render_error(request, "invalid_token", str(e))
   504|                 return
   505|         user_agent = request.requestHeaders.getRawHeaders(b"User-Agent", default=[b""])[
   506|             0
   507|         ].decode("ascii", "surrogateescape")
   508|         ip_address = self.hs.get_ip_from_request(request)
   509|         try:
   510|             user_id = await self._map_userinfo_to_user(
   511|                 userinfo, token, user_agent, ip_address
   512|             )
   513|         except MappingException as e:
   514|             logger.exception("Could not map user")
   515|             self._render_error(request, "mapping_error", str(e))
   516|             return
   517|         if ui_auth_session_id:
   518|             await self._auth_handler.complete_sso_ui_auth(
   519|                 user_id, ui_auth_session_id, request
   520|             )
   521|         else:
   522|             await self._auth_handler.complete_sso_login(
   523|                 user_id, request, client_redirect_url
   524|             )
   525|     def _generate_oidc_session_token(
   526|         self,
   527|         state: str,
   528|         nonce: str,
   529|         client_redirect_url: str,
   530|         ui_auth_session_id: Optional[str],
   531|         duration_in_ms: int = (60 * 60 * 1000),
   532|     ) -> str:
   533|         """Generates a signed token storing data about an OIDC session.
   534|         When Synapse initiates an authorization flow, it creates a random state
   535|         and a random nonce. Those parameters are given to the provider and
   536|         should be verified when the client comes back from the provider.
   537|         It is also used to store the client_redirect_url, which is used to
   538|         complete the SSO login flow.
   539|         Args:
   540|             state: The ``state`` parameter passed to the OIDC provider.
   541|             nonce: The ``nonce`` parameter passed to the OIDC provider.
   542|             client_redirect_url: The URL the client gave when it initiated the
   543|                 flow.

# --- HUNK 4: Lines 613-653 ---
   613|         for caveat in macaroon.caveats:
   614|             if caveat.caveat_id.startswith(prefix):
   615|                 return caveat.caveat_id[len(prefix) :]
   616|         raise ValueError("No %s caveat in macaroon" % (key,))
   617|     def _verify_expiry(self, caveat: str) -> bool:
   618|         prefix = "time < "
   619|         if not caveat.startswith(prefix):
   620|             return False
   621|         expiry = int(caveat[len(prefix) :])
   622|         now = self._clock.time_msec()
   623|         return now < expiry
   624|     async def _map_userinfo_to_user(
   625|         self, userinfo: UserInfo, token: Token, user_agent: str, ip_address: str
   626|     ) -> str:
   627|         """Maps a UserInfo object to a mxid.
   628|         UserInfo should have a claim that uniquely identifies users. This claim
   629|         is usually `sub`, but can be configured with `oidc_config.subject_claim`.
   630|         It is then used as an `external_id`.
   631|         If we don't find the user that way, we should register the user,
   632|         mapping the localpart and the display name from the UserInfo.
   633|         If a user already exists with the mxid we've mapped, raise an exception.
   634|         Args:
   635|             userinfo: an object representing the user
   636|             token: a dict with the tokens obtained from the provider
   637|             user_agent: The user agent of the client making the request.
   638|             ip_address: The IP address of the client making the request.
   639|         Raises:
   640|             MappingException: if there was an error while mapping some properties
   641|         Returns:
   642|             The mxid of the user
   643|         """
   644|         try:
   645|             remote_user_id = self._user_mapping_provider.get_remote_user_id(userinfo)
   646|         except Exception as e:
   647|             raise MappingException(
   648|                 "Failed to extract subject from OIDC response: %s" % (e,)
   649|             )
   650|         remote_user_id = str(remote_user_id)
   651|         logger.info(
   652|             "Looking for existing mapping for user %s:%s",
   653|             self._auth_provider_id,

# --- HUNK 5: Lines 656-787 ---
   656|         registered_user_id = await self._datastore.get_user_by_external_id(
   657|             self._auth_provider_id, remote_user_id,
   658|         )
   659|         if registered_user_id is not None:
   660|             logger.info("Found existing mapping %s", registered_user_id)
   661|             return registered_user_id
   662|         try:
   663|             attributes = await self._user_mapping_provider.map_user_attributes(
   664|                 userinfo, token
   665|             )
   666|         except Exception as e:
   667|             raise MappingException(
   668|                 "Could not extract user attributes from OIDC response: " + str(e)
   669|             )
   670|         logger.debug(
   671|             "Retrieved user attributes from user mapping provider: %r", attributes
   672|         )
   673|         if not attributes["localpart"]:
   674|             raise MappingException("localpart is empty")
   675|         localpart = map_username_to_mxid_localpart(attributes["localpart"])
   676|         user_id = UserID(localpart, self._hostname)
   677|         if await self._datastore.get_users_by_id_case_insensitive(user_id.to_string()):
   678|             raise MappingException(
   679|                 "mxid '{}' is already taken".format(user_id.to_string())
   680|             )
   681|         registered_user_id = await self._registration_handler.register_user(
   682|             localpart=localpart,
   683|             default_display_name=attributes["display_name"],
   684|             user_agent_ips=(user_agent, ip_address),
   685|         )
   686|         await self._datastore.record_user_external_id(
   687|             self._auth_provider_id, remote_user_id, registered_user_id,
   688|         )
   689|         return registered_user_id
   690| UserAttribute = TypedDict(
   691|     "UserAttribute", {"localpart": str, "display_name": Optional[str]}
   692| )
   693| C = TypeVar("C")
   694| class OidcMappingProvider(Generic[C]):
   695|     """A mapping provider maps a UserInfo object to user attributes.
   696|     It should provide the API described by this class.
   697|     """
   698|     def __init__(self, config: C):
   699|         """
   700|         Args:
   701|             config: A custom config object from this module, parsed by ``parse_config()``
   702|         """
   703|     @staticmethod
   704|     def parse_config(config: dict) -> C:
   705|         """Parse the dict provided by the homeserver's config
   706|         Args:
   707|             config: A dictionary containing configuration options for this provider
   708|         Returns:
   709|             A custom config object for this module
   710|         """
   711|         raise NotImplementedError()
   712|     def get_remote_user_id(self, userinfo: UserInfo) -> str:
   713|         """Get a unique user ID for this user.
   714|         Usually, in an OIDC-compliant scenario, it should be the ``sub`` claim from the UserInfo object.
   715|         Args:
   716|             userinfo: An object representing the user given by the OIDC provider
   717|         Returns:
   718|             A unique user ID
   719|         """
   720|         raise NotImplementedError()
   721|     async def map_user_attributes(
   722|         self, userinfo: UserInfo, token: Token
   723|     ) -> UserAttribute:
   724|         """Map a ``UserInfo`` objects into user attributes.
   725|         Args:
   726|             userinfo: An object representing the user given by the OIDC provider
   727|             token: A dict with the tokens returned by the provider
   728|         Returns:
   729|             A dict containing the ``localpart`` and (optionally) the ``display_name``
   730|         """
   731|         raise NotImplementedError()
   732| def jinja_finalize(thing):
   733|     return thing if thing is not None else ""
   734| env = Environment(finalize=jinja_finalize)
   735| @attr.s
   736| class JinjaOidcMappingConfig:
   737|     subject_claim = attr.ib()  # type: str
   738|     localpart_template = attr.ib()  # type: Template
   739|     display_name_template = attr.ib()  # type: Optional[Template]
   740| class JinjaOidcMappingProvider(OidcMappingProvider[JinjaOidcMappingConfig]):
   741|     """An implementation of a mapping provider based on Jinja templates.
   742|     This is the default mapping provider.
   743|     """
   744|     def __init__(self, config: JinjaOidcMappingConfig):
   745|         self._config = config
   746|     @staticmethod
   747|     def parse_config(config: dict) -> JinjaOidcMappingConfig:
   748|         subject_claim = config.get("subject_claim", "sub")
   749|         if "localpart_template" not in config:
   750|             raise ConfigError(
   751|                 "missing key: oidc_config.user_mapping_provider.config.localpart_template"
   752|             )
   753|         try:
   754|             localpart_template = env.from_string(config["localpart_template"])
   755|         except Exception as e:
   756|             raise ConfigError(
   757|                 "invalid jinja template for oidc_config.user_mapping_provider.config.localpart_template: %r"
   758|                 % (e,)
   759|             )
   760|         display_name_template = None  # type: Optional[Template]
   761|         if "display_name_template" in config:
   762|             try:
   763|                 display_name_template = env.from_string(config["display_name_template"])
   764|             except Exception as e:
   765|                 raise ConfigError(
   766|                     "invalid jinja template for oidc_config.user_mapping_provider.config.display_name_template: %r"
   767|                     % (e,)
   768|                 )
   769|         return JinjaOidcMappingConfig(
   770|             subject_claim=subject_claim,
   771|             localpart_template=localpart_template,
   772|             display_name_template=display_name_template,
   773|         )
   774|     def get_remote_user_id(self, userinfo: UserInfo) -> str:
   775|         return userinfo[self._config.subject_claim]
   776|     async def map_user_attributes(
   777|         self, userinfo: UserInfo, token: Token
   778|     ) -> UserAttribute:
   779|         localpart = self._config.localpart_template.render(user=userinfo).strip()
   780|         display_name = None  # type: Optional[str]
   781|         if self._config.display_name_template is not None:
   782|             display_name = self._config.display_name_template.render(
   783|                 user=userinfo
   784|             ).strip()
   785|             if display_name == "":
   786|                 display_name = None
   787|         return UserAttribute(localpart=localpart, display_name=display_name)


# ====================================================================
# FILE: synapse/handlers/pagination.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| import logging
     2| from typing import TYPE_CHECKING, Any, Dict, Optional, Set
     3| from twisted.python.failure import Failure
     4| from synapse.api.constants import EventTypes, Membership
     5| from synapse.api.errors import SynapseError
     6| from synapse.api.filtering import Filter
     7| from synapse.logging.context import run_in_background
     8| from synapse.metrics.background_process_metrics import run_as_background_process
     9| from synapse.storage.state import StateFilter
    10| from synapse.streams.config import PaginationConfig
    11| from synapse.types import Requester, RoomStreamToken
    12| from synapse.util.async_helpers import ReadWriteLock
    13| from synapse.util.stringutils import random_string
    14| from synapse.visibility import filter_events_for_client
    15| if TYPE_CHECKING:
    16|     from synapse.server import HomeServer
    17| logger = logging.getLogger(__name__)
    18| class PurgeStatus:
    19|     """Object tracking the status of a purge request
    20|     This class contains information on the progress of a purge request, for
    21|     return by get_purge_status.
    22|     Attributes:
    23|         status (int): Tracks whether this request has completed. One of
    24|             STATUS_{ACTIVE,COMPLETE,FAILED}
    25|     """
    26|     STATUS_ACTIVE = 0
    27|     STATUS_COMPLETE = 1
    28|     STATUS_FAILED = 2
    29|     STATUS_TEXT = {
    30|         STATUS_ACTIVE: "active",
    31|         STATUS_COMPLETE: "complete",

# --- HUNK 2: Lines 208-309 ---
   208|     async def get_messages(
   209|         self,
   210|         requester: Requester,
   211|         room_id: str,
   212|         pagin_config: PaginationConfig,
   213|         as_client_event: bool = True,
   214|         event_filter: Optional[Filter] = None,
   215|     ) -> Dict[str, Any]:
   216|         """Get messages in a room.
   217|         Args:
   218|             requester: The user requesting messages.
   219|             room_id: The room they want messages from.
   220|             pagin_config: The pagination config rules to apply, if any.
   221|             as_client_event: True to get events in client-server format.
   222|             event_filter: Filter to apply to results or None
   223|         Returns:
   224|             Pagination API results
   225|         """
   226|         user_id = requester.user.to_string()
   227|         if pagin_config.from_token:
   228|             room_token = pagin_config.from_token.room_key
   229|         else:
   230|             pagin_config.from_token = (
   231|                 self.hs.get_event_sources().get_current_token_for_pagination()
   232|             )
   233|             room_token = pagin_config.from_token.room_key
   234|         room_token = RoomStreamToken.parse(room_token)
   235|         pagin_config.from_token = pagin_config.from_token.copy_and_replace(
   236|             "room_key", str(room_token)
   237|         )
   238|         source_config = pagin_config.get_source_config("room")
   239|         with await self.pagination_lock.read(room_id):
   240|             (
   241|                 membership,
   242|                 member_event_id,
   243|             ) = await self.auth.check_user_in_room_or_world_readable(
   244|                 room_id, user_id, allow_departed_users=True
   245|             )
   246|             if source_config.direction == "b":
   247|                 if room_token.topological:
   248|                     curr_topo = room_token.topological
   249|                 else:
   250|                     curr_topo = await self.store.get_current_topological_token(
   251|                         room_id, room_token.stream
   252|                     )
   253|                 if membership == Membership.LEAVE:
   254|                     assert member_event_id
   255|                     leave_token = await self.store.get_topological_token_for_event(
   256|                         member_event_id
   257|                     )
   258|                     if RoomStreamToken.parse(leave_token).topological < curr_topo:
   259|                         source_config.from_key = str(leave_token)
   260|                 await self.hs.get_handlers().federation_handler.maybe_backfill(
   261|                     room_id, curr_topo, limit=source_config.limit,
   262|                 )
   263|             events, next_key = await self.store.paginate_room_events(
   264|                 room_id=room_id,
   265|                 from_key=source_config.from_key,
   266|                 to_key=source_config.to_key,
   267|                 direction=source_config.direction,
   268|                 limit=source_config.limit,
   269|                 event_filter=event_filter,
   270|             )
   271|             next_token = pagin_config.from_token.copy_and_replace("room_key", next_key)
   272|         if events:
   273|             if event_filter:
   274|                 events = event_filter.filter(events)
   275|             events = await filter_events_for_client(
   276|                 self.storage, user_id, events, is_peeking=(member_event_id is None)
   277|             )
   278|         if not events:
   279|             return {
   280|                 "chunk": [],
   281|                 "start": pagin_config.from_token.to_string(),
   282|                 "end": next_token.to_string(),
   283|             }
   284|         state = None
   285|         if event_filter and event_filter.lazy_load_members() and len(events) > 0:
   286|             state_filter = StateFilter.from_types(
   287|                 (EventTypes.Member, event.sender) for event in events
   288|             )
   289|             state_ids = await self.state_store.get_state_ids_for_event(
   290|                 events[0].event_id, state_filter=state_filter
   291|             )
   292|             if state_ids:
   293|                 state_dict = await self.store.get_events(list(state_ids.values()))
   294|                 state = state_dict.values()
   295|         time_now = self.clock.time_msec()
   296|         chunk = {
   297|             "chunk": (
   298|                 await self._event_serializer.serialize_events(
   299|                     events, time_now, as_client_event=as_client_event
   300|                 )
   301|             ),
   302|             "start": pagin_config.from_token.to_string(),
   303|             "end": next_token.to_string(),
   304|         }
   305|         if state:
   306|             chunk["state"] = await self._event_serializer.serialize_events(
   307|                 state, time_now, as_client_event=as_client_event
   308|             )
   309|         return chunk


# ====================================================================
# FILE: synapse/handlers/presence.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 755-796 ---
   755|                     if other_user_id in users_interested_in:
   756|                         user_ids_changed.add(other_user_id)
   757|             else:
   758|                 get_updates_counter.labels("full").inc()
   759|                 if from_key:
   760|                     user_ids_changed = stream_change_cache.get_entities_changed(
   761|                         users_interested_in, from_key
   762|                     )
   763|                 else:
   764|                     user_ids_changed = users_interested_in
   765|             updates = await presence.current_state_for_users(user_ids_changed)
   766|         if include_offline:
   767|             return (list(updates.values()), max_token)
   768|         else:
   769|             return (
   770|                 [s for s in updates.values() if s.state != PresenceState.OFFLINE],
   771|                 max_token,
   772|             )
   773|     def get_current_key(self):
   774|         return self.store.get_current_presence_token()
   775|     async def get_pagination_rows(self, user, pagination_config, key):
   776|         return await self.get_new_events(user, from_key=None, include_offline=False)
   777|     @cached(num_args=2, cache_context=True)
   778|     async def _get_interested_in(self, user, explicit_room_id, cache_context):
   779|         """Returns the set of users that the given user should see presence
   780|         updates for
   781|         """
   782|         user_id = user.to_string()
   783|         users_interested_in = set()
   784|         users_interested_in.add(user_id)  # So that we receive our own presence
   785|         users_who_share_room = await self.store.get_users_who_share_room_with_user(
   786|             user_id, on_invalidate=cache_context.invalidate
   787|         )
   788|         users_interested_in.update(users_who_share_room)
   789|         if explicit_room_id:
   790|             user_ids = await self.store.get_users_in_room(
   791|                 explicit_room_id, on_invalidate=cache_context.invalidate
   792|             )
   793|             users_interested_in.update(user_ids)
   794|         return users_interested_in
   795| def handle_timeouts(user_states, is_mine_fn, syncing_user_ids, now):
   796|     """Checks the presence of users that have timed out and updates as


# ====================================================================
# FILE: synapse/handlers/profile.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 4-44 ---
     4|     AuthError,
     5|     Codes,
     6|     HttpResponseException,
     7|     RequestSendFailed,
     8|     StoreError,
     9|     SynapseError,
    10| )
    11| from synapse.metrics.background_process_metrics import run_as_background_process
    12| from synapse.types import UserID, create_requester, get_domain_from_id
    13| from ._base import BaseHandler
    14| logger = logging.getLogger(__name__)
    15| MAX_DISPLAYNAME_LEN = 256
    16| MAX_AVATAR_URL_LEN = 1000
    17| class BaseProfileHandler(BaseHandler):
    18|     """Handles fetching and updating user profile information.
    19|     BaseProfileHandler can be instantiated directly on workers and will
    20|     delegate to master when necessary. The master process should use the
    21|     subclass MasterProfileHandler
    22|     """
    23|     def __init__(self, hs):
    24|         super(BaseProfileHandler, self).__init__(hs)
    25|         self.federation = hs.get_federation_client()
    26|         hs.get_federation_registry().register_query_handler(
    27|             "profile", self.on_profile_query
    28|         )
    29|         self.user_directory_handler = hs.get_user_directory_handler()
    30|     async def get_profile(self, user_id):
    31|         target_user = UserID.from_string(user_id)
    32|         if self.hs.is_mine(target_user):
    33|             try:
    34|                 displayname = await self.store.get_profile_displayname(
    35|                     target_user.localpart
    36|                 )
    37|                 avatar_url = await self.store.get_profile_avatar_url(
    38|                     target_user.localpart
    39|                 )
    40|             except StoreError as e:
    41|                 if e.code == 404:
    42|                     raise SynapseError(404, "Profile was not found", Codes.NOT_FOUND)
    43|                 raise
    44|             return {"displayname": displayname, "avatar_url": avatar_url}

# --- HUNK 2: Lines 259-299 ---
   259|             or not requester
   260|         ):
   261|             return
   262|         if target_user.to_string() == requester.to_string():
   263|             return
   264|         try:
   265|             requester_rooms = await self.store.get_rooms_for_user(requester.to_string())
   266|             target_user_rooms = await self.store.get_rooms_for_user(
   267|                 target_user.to_string()
   268|             )
   269|             if requester_rooms.isdisjoint(target_user_rooms):
   270|                 raise SynapseError(403, "Profile isn't available", Codes.FORBIDDEN)
   271|         except StoreError as e:
   272|             if e.code == 404:
   273|                 raise SynapseError(403, "Profile isn't available", Codes.FORBIDDEN)
   274|             raise
   275| class MasterProfileHandler(BaseProfileHandler):
   276|     PROFILE_UPDATE_MS = 60 * 1000
   277|     PROFILE_UPDATE_EVERY_MS = 24 * 60 * 60 * 1000
   278|     def __init__(self, hs):
   279|         super(MasterProfileHandler, self).__init__(hs)
   280|         assert hs.config.worker_app is None
   281|         self.clock.looping_call(
   282|             self._start_update_remote_profile_cache, self.PROFILE_UPDATE_MS
   283|         )
   284|     def _start_update_remote_profile_cache(self):
   285|         return run_as_background_process(
   286|             "Update remote profile", self._update_remote_profile_cache
   287|         )
   288|     async def _update_remote_profile_cache(self):
   289|         """Called periodically to check profiles of remote users we haven't
   290|         checked in a while.
   291|         """
   292|         entries = await self.store.get_remote_profile_cache_entries_that_expire(
   293|             last_checked=self.clock.time_msec() - self.PROFILE_UPDATE_EVERY_MS
   294|         )
   295|         for user_id, displayname, avatar_url in entries:
   296|             is_subscribed = await self.store.is_subscribed_remote_profile_for_user(
   297|                 user_id
   298|             )
   299|             if not is_subscribed:


# ====================================================================
# FILE: synapse/handlers/read_marker.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-27 ---
     1| import logging
     2| from synapse.util.async_helpers import Linearizer
     3| from ._base import BaseHandler
     4| logger = logging.getLogger(__name__)
     5| class ReadMarkerHandler(BaseHandler):
     6|     def __init__(self, hs):
     7|         super(ReadMarkerHandler, self).__init__(hs)
     8|         self.server_name = hs.config.server_name
     9|         self.store = hs.get_datastore()
    10|         self.read_marker_linearizer = Linearizer(name="read_marker")
    11|         self.notifier = hs.get_notifier()
    12|     async def received_client_read_marker(self, room_id, user_id, event_id):
    13|         """Updates the read marker for a given user in a given room if the event ID given
    14|         is ahead in the stream relative to the current read marker.
    15|         This uses a notifier to indicate that account data should be sent down /sync if
    16|         the read marker has changed.
    17|         """
    18|         with await self.read_marker_linearizer.queue((room_id, user_id)):
    19|             existing_read_marker = await self.store.get_account_data_for_room_and_type(
    20|                 user_id, room_id, "m.fully_read"
    21|             )
    22|             should_update = True
    23|             if existing_read_marker:
    24|                 should_update = await self.store.is_event_after(
    25|                     event_id, existing_read_marker["event_id"]
    26|                 )
    27|             if should_update:


# ====================================================================
# FILE: synapse/handlers/receipts.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| import logging
     2| from synapse.handlers._base import BaseHandler
     3| from synapse.types import ReadReceipt, get_domain_from_id
     4| from synapse.util.async_helpers import maybe_awaitable
     5| logger = logging.getLogger(__name__)
     6| class ReceiptsHandler(BaseHandler):
     7|     def __init__(self, hs):
     8|         super(ReceiptsHandler, self).__init__(hs)
     9|         self.server_name = hs.config.server_name
    10|         self.store = hs.get_datastore()
    11|         self.hs = hs
    12|         self.federation = hs.get_federation_sender()
    13|         hs.get_federation_registry().register_edu_handler(
    14|             "m.receipt", self._received_remote_receipt
    15|         )
    16|         self.clock = self.hs.get_clock()
    17|         self.state = hs.get_state_handler()
    18|     async def _received_remote_receipt(self, origin, content):
    19|         """Called when we receive an EDU of type m.receipt from a remote HS.
    20|         """
    21|         receipts = []
    22|         for room_id, room_values in content.items():
    23|             for receipt_type, users in room_values.items():
    24|                 for user_id, user_values in users.items():
    25|                     if get_domain_from_id(user_id) != origin:
    26|                         logger.info(
    27|                             "Received receipt for user %r from server %s, ignoring",
    28|                             user_id,

# --- HUNK 2: Lines 81-111 ---
    81|             data={"ts": int(self.clock.time_msec())},
    82|         )
    83|         is_new = await self._handle_new_receipts([receipt])
    84|         if not is_new:
    85|             return
    86|         await self.federation.send_read_receipt(receipt)
    87| class ReceiptEventSource:
    88|     def __init__(self, hs):
    89|         self.store = hs.get_datastore()
    90|     async def get_new_events(self, from_key, room_ids, **kwargs):
    91|         from_key = int(from_key)
    92|         to_key = self.get_current_key()
    93|         if from_key == to_key:
    94|             return [], to_key
    95|         events = await self.store.get_linearized_receipts_for_rooms(
    96|             room_ids, from_key=from_key, to_key=to_key
    97|         )
    98|         return (events, to_key)
    99|     def get_current_key(self, direction="f"):
   100|         return self.store.get_max_receipt_stream_id()
   101|     async def get_pagination_rows(self, user, config, key):
   102|         to_key = int(config.from_key)
   103|         if config.to_key:
   104|             from_key = int(config.to_key)
   105|         else:
   106|             from_key = None
   107|         room_ids = await self.store.get_rooms_for_user(user.to_string())
   108|         events = await self.store.get_linearized_receipts_for_rooms(
   109|             room_ids, from_key=from_key, to_key=to_key
   110|         )
   111|         return (events, to_key)


# ====================================================================
# FILE: synapse/handlers/register.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 4-44 ---
     4| from synapse.api.constants import MAX_USERID_LENGTH, EventTypes, JoinRules, LoginType
     5| from synapse.api.errors import AuthError, Codes, ConsentNotGivenError, SynapseError
     6| from synapse.config.server import is_threepid_reserved
     7| from synapse.http.servlet import assert_params_in_dict
     8| from synapse.replication.http.login import RegisterDeviceReplicationServlet
     9| from synapse.replication.http.register import (
    10|     ReplicationPostRegisterActionsServlet,
    11|     ReplicationRegisterServlet,
    12| )
    13| from synapse.spam_checker_api import RegistrationBehaviour
    14| from synapse.storage.state import StateFilter
    15| from synapse.types import RoomAlias, UserID, create_requester
    16| from ._base import BaseHandler
    17| logger = logging.getLogger(__name__)
    18| class RegistrationHandler(BaseHandler):
    19|     def __init__(self, hs):
    20|         """
    21|         Args:
    22|             hs (synapse.server.HomeServer):
    23|         """
    24|         super(RegistrationHandler, self).__init__(hs)
    25|         self.hs = hs
    26|         self.auth = hs.get_auth()
    27|         self._auth_handler = hs.get_auth_handler()
    28|         self.profile_handler = hs.get_profile_handler()
    29|         self.user_directory_handler = hs.get_user_directory_handler()
    30|         self.identity_handler = self.hs.get_handlers().identity_handler
    31|         self.ratelimiter = hs.get_registration_ratelimiter()
    32|         self.macaroon_gen = hs.get_macaroon_generator()
    33|         self._server_notices_mxid = hs.config.server_notices_mxid
    34|         self.spam_checker = hs.get_spam_checker()
    35|         if hs.config.worker_app:
    36|             self._register_client = ReplicationRegisterServlet.make_client(hs)
    37|             self._register_device_client = RegisterDeviceReplicationServlet.make_client(
    38|                 hs
    39|             )
    40|             self._post_registration_client = ReplicationPostRegisterActionsServlet.make_client(
    41|                 hs
    42|             )
    43|         else:
    44|             self.device_handler = hs.get_device_handler()


# ====================================================================
# FILE: synapse/handlers/room.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 27-67 ---
    27|     RoomAlias,
    28|     RoomID,
    29|     RoomStreamToken,
    30|     StateMap,
    31|     StreamToken,
    32|     UserID,
    33|     create_requester,
    34| )
    35| from synapse.util import stringutils
    36| from synapse.util.async_helpers import Linearizer
    37| from synapse.util.caches.response_cache import ResponseCache
    38| from synapse.visibility import filter_events_for_client
    39| from ._base import BaseHandler
    40| if TYPE_CHECKING:
    41|     from synapse.server import HomeServer
    42| logger = logging.getLogger(__name__)
    43| id_server_scheme = "https://"
    44| FIVE_MINUTES_IN_MS = 5 * 60 * 1000
    45| class RoomCreationHandler(BaseHandler):
    46|     def __init__(self, hs: "HomeServer"):
    47|         super(RoomCreationHandler, self).__init__(hs)
    48|         self.spam_checker = hs.get_spam_checker()
    49|         self.event_creation_handler = hs.get_event_creation_handler()
    50|         self.room_member_handler = hs.get_room_member_handler()
    51|         self.config = hs.config
    52|         self._presets_dict = {
    53|             RoomCreationPreset.PRIVATE_CHAT: {
    54|                 "join_rules": JoinRules.INVITE,
    55|                 "history_visibility": "shared",
    56|                 "original_invitees_have_ops": False,
    57|                 "guest_can_join": True,
    58|                 "power_level_content_override": {"invite": 0},
    59|             },
    60|             RoomCreationPreset.TRUSTED_PRIVATE_CHAT: {
    61|                 "join_rules": JoinRules.INVITE,
    62|                 "history_visibility": "shared",
    63|                 "original_invitees_have_ops": True,
    64|                 "guest_can_join": True,
    65|                 "power_level_content_override": {"invite": 0},
    66|             },
    67|             RoomCreationPreset.PUBLIC_CHAT: {

# --- HUNK 2: Lines 578-618 ---
   578|             )
   579|         for invite_3pid in invite_3pid_list:
   580|             id_server = invite_3pid["id_server"]
   581|             id_access_token = invite_3pid.get("id_access_token")  # optional
   582|             address = invite_3pid["address"]
   583|             medium = invite_3pid["medium"]
   584|             last_stream_id = await self.hs.get_room_member_handler().do_3pid_invite(
   585|                 room_id,
   586|                 requester.user,
   587|                 medium,
   588|                 address,
   589|                 id_server,
   590|                 requester,
   591|                 txn_id=None,
   592|                 id_access_token=id_access_token,
   593|             )
   594|         result = {"room_id": room_id}
   595|         if room_alias:
   596|             result["room_alias"] = room_alias.to_string()
   597|         await self._replication.wait_for_stream_position(
   598|             self.hs.config.worker.writers.events, "events", last_stream_id
   599|         )
   600|         return result, last_stream_id
   601|     async def _send_events_for_new_room(
   602|         self,
   603|         creator: Requester,
   604|         room_id: str,
   605|         preset_config: str,
   606|         invite_list: List[str],
   607|         initial_state: MutableStateMap,
   608|         creation_content: JsonDict,
   609|         room_alias: Optional[RoomAlias] = None,
   610|         power_level_content_override: Optional[JsonDict] = None,
   611|         creator_join_profile: Optional[JsonDict] = None,
   612|     ) -> int:
   613|         """Sends the initial events into a new room.
   614|         `power_level_content_override` doesn't apply when initial state has
   615|         power level state event content.
   616|         Returns:
   617|             The stream_id of the last event persisted.
   618|         """

# --- HUNK 3: Lines 788-875 ---
   788|             last_event_id = event_id
   789|         if event_filter and event_filter.lazy_load_members():
   790|             state_filter = StateFilter.from_lazy_load_member_list(
   791|                 ev.sender
   792|                 for ev in itertools.chain(
   793|                     results["events_before"],
   794|                     (results["event"],),
   795|                     results["events_after"],
   796|                 )
   797|             )
   798|         else:
   799|             state_filter = StateFilter.all()
   800|         state = await self.state_store.get_state_for_events(
   801|             [last_event_id], state_filter=state_filter
   802|         )
   803|         state_events = list(state[last_event_id].values())
   804|         if event_filter:
   805|             state_events = event_filter.filter(state_events)
   806|         results["state"] = await filter_evts(state_events)
   807|         token = StreamToken.START
   808|         results["start"] = token.copy_and_replace(
   809|             "room_key", results["start"]
   810|         ).to_string()
   811|         results["end"] = token.copy_and_replace("room_key", results["end"]).to_string()
   812|         return results
   813| class RoomEventSource:
   814|     def __init__(self, hs: "HomeServer"):
   815|         self.store = hs.get_datastore()
   816|     async def get_new_events(
   817|         self,
   818|         user: UserID,
   819|         from_key: str,
   820|         limit: int,
   821|         room_ids: List[str],
   822|         is_guest: bool,
   823|         explicit_room_id: Optional[str] = None,
   824|     ) -> Tuple[List[EventBase], str]:
   825|         to_key = self.get_current_key()
   826|         from_token = RoomStreamToken.parse(from_key)
   827|         if from_token.topological:
   828|             logger.warning("Stream has topological part!!!! %r", from_key)
   829|             from_key = "s%s" % (from_token.stream,)
   830|         app_service = self.store.get_app_service_by_user_id(user.to_string())
   831|         if app_service:
   832|             raise NotImplementedError()
   833|         else:
   834|             room_events = await self.store.get_membership_changes_for_user(
   835|                 user.to_string(), from_key, to_key
   836|             )
   837|             room_to_events = await self.store.get_room_events_stream_for_rooms(
   838|                 room_ids=room_ids,
   839|                 from_key=from_key,
   840|                 to_key=to_key,
   841|                 limit=limit or 10,
   842|                 order="ASC",
   843|             )
   844|             events = list(room_events)
   845|             events.extend(e for evs, _ in room_to_events.values() for e in evs)
   846|             events.sort(key=lambda e: e.internal_metadata.order)
   847|             if limit:
   848|                 events[:] = events[:limit]
   849|             if events:
   850|                 end_key = events[-1].internal_metadata.after
   851|             else:
   852|                 end_key = to_key
   853|         return (events, end_key)
   854|     def get_current_key(self) -> str:
   855|         return "s%d" % (self.store.get_room_max_stream_ordering(),)
   856|     def get_current_key_for_room(self, room_id: str) -> Awaitable[str]:
   857|         return self.store.get_room_events_max_id(room_id)
   858| class RoomShutdownHandler:
   859|     DEFAULT_MESSAGE = (
   860|         "Sharing illegal content on this server is not permitted and rooms in"
   861|         " violation will be blocked."
   862|     )
   863|     DEFAULT_ROOM_NAME = "Content Violation Notification"
   864|     def __init__(self, hs: "HomeServer"):
   865|         self.hs = hs
   866|         self.room_member_handler = hs.get_room_member_handler()
   867|         self._room_creation_handler = hs.get_room_creation_handler()
   868|         self._replication = hs.get_replication_data_handler()
   869|         self.event_creation_handler = hs.get_event_creation_handler()
   870|         self.state = hs.get_state_handler()
   871|         self.store = hs.get_datastore()
   872|     async def shutdown_room(
   873|         self,
   874|         room_id: str,
   875|         requester_user_id: str,

# --- HUNK 4: Lines 932-996 ---
   932|         if new_room_user_id is not None:
   933|             if not self.hs.is_mine_id(new_room_user_id):
   934|                 raise SynapseError(
   935|                     400, "User must be our own: %s" % (new_room_user_id,)
   936|                 )
   937|             room_creator_requester = create_requester(new_room_user_id)
   938|             info, stream_id = await self._room_creation_handler.create_room(
   939|                 room_creator_requester,
   940|                 config={
   941|                     "preset": RoomCreationPreset.PUBLIC_CHAT,
   942|                     "name": new_room_name,
   943|                     "power_level_content_override": {"users_default": -10},
   944|                 },
   945|                 ratelimit=False,
   946|             )
   947|             new_room_id = info["room_id"]
   948|             logger.info(
   949|                 "Shutting down room %r, joining to new room: %r", room_id, new_room_id
   950|             )
   951|             await self._replication.wait_for_stream_position(
   952|                 self.hs.config.worker.writers.events, "events", stream_id
   953|             )
   954|         else:
   955|             new_room_id = None
   956|             logger.info("Shutting down room %r", room_id)
   957|         users = await self.state.get_current_users_in_room(room_id)
   958|         kicked_users = []
   959|         failed_to_kick_users = []
   960|         for user_id in users:
   961|             if not self.hs.is_mine_id(user_id):
   962|                 continue
   963|             logger.info("Kicking %r from %r...", user_id, room_id)
   964|             try:
   965|                 target_requester = create_requester(user_id)
   966|                 _, stream_id = await self.room_member_handler.update_membership(
   967|                     requester=target_requester,
   968|                     target=target_requester.user,
   969|                     room_id=room_id,
   970|                     action=Membership.LEAVE,
   971|                     content={},
   972|                     ratelimit=False,
   973|                     require_consent=False,
   974|                 )
   975|                 await self._replication.wait_for_stream_position(
   976|                     self.hs.config.worker.writers.events, "events", stream_id
   977|                 )
   978|                 await self.room_member_handler.forget(target_requester.user, room_id)
   979|                 if new_room_user_id:
   980|                     await self.room_member_handler.update_membership(
   981|                         requester=target_requester,
   982|                         target=target_requester.user,
   983|                         room_id=new_room_id,
   984|                         action=Membership.JOIN,
   985|                         content={},
   986|                         ratelimit=False,
   987|                         require_consent=False,
   988|                     )
   989|                 kicked_users.append(user_id)
   990|             except Exception:
   991|                 logger.exception(
   992|                     "Failed to leave old room and join new room for %r", user_id
   993|                 )
   994|                 failed_to_kick_users.append(user_id)
   995|         if new_room_user_id:
   996|             await self.event_creation_handler.create_and_send_nonmember_event(


# ====================================================================
# FILE: synapse/handlers/room_list.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| import logging
     2| from collections import namedtuple
     3| from typing import Any, Dict, Optional
     4| import msgpack
     5| from unpaddedbase64 import decode_base64, encode_base64
     6| from synapse.api.constants import EventTypes, JoinRules
     7| from synapse.api.errors import Codes, HttpResponseException
     8| from synapse.types import ThirdPartyInstanceID
     9| from synapse.util.caches.descriptors import cached
    10| from synapse.util.caches.response_cache import ResponseCache
    11| from ._base import BaseHandler
    12| logger = logging.getLogger(__name__)
    13| REMOTE_ROOM_LIST_POLL_INTERVAL = 60 * 1000
    14| EMPTY_THIRD_PARTY_ID = ThirdPartyInstanceID(None, None)
    15| class RoomListHandler(BaseHandler):
    16|     def __init__(self, hs):
    17|         super(RoomListHandler, self).__init__(hs)
    18|         self.enable_room_list_search = hs.config.enable_room_list_search
    19|         self.response_cache = ResponseCache(hs, "room_list")
    20|         self.remote_response_cache = ResponseCache(
    21|             hs, "remote_room_list", timeout_ms=30 * 1000
    22|         )
    23|     async def get_local_public_room_list(
    24|         self,
    25|         limit=None,
    26|         since_token=None,
    27|         search_filter=None,
    28|         network_tuple=EMPTY_THIRD_PARTY_ID,
    29|         from_federation=False,
    30|     ):
    31|         """Generate a local public room list.
    32|         There are multiple different lists: the main one plus one per third
    33|         party network. A client can ask for a specific list or to return all.
    34|         Args:
    35|             limit (int|None)
    36|             since_token (str|None)
    37|             search_filter (dict|None)


# ====================================================================
# FILE: synapse/handlers/room_member.py
# Total hunks: 7
# ====================================================================
# --- HUNK 1: Lines 6-77 ---
     6| from unpaddedbase64 import encode_base64
     7| from synapse import types
     8| from synapse.api.constants import MAX_DEPTH, EventTypes, Membership
     9| from synapse.api.errors import (
    10|     AuthError,
    11|     Codes,
    12|     LimitExceededError,
    13|     ShadowBanError,
    14|     SynapseError,
    15| )
    16| from synapse.api.ratelimiting import Ratelimiter
    17| from synapse.api.room_versions import EventFormatVersions
    18| from synapse.crypto.event_signing import compute_event_reference_hash
    19| from synapse.events import EventBase
    20| from synapse.events.builder import create_local_event_from_event_dict
    21| from synapse.events.snapshot import EventContext
    22| from synapse.events.validator import EventValidator
    23| from synapse.storage.roommember import RoomsForUser
    24| from synapse.types import JsonDict, Requester, RoomAlias, RoomID, StateMap, UserID
    25| from synapse.util.async_helpers import Linearizer
    26| from synapse.util.distributor import user_joined_room, user_left_room
    27| from ._base import BaseHandler
    28| if TYPE_CHECKING:
    29|     from synapse.server import HomeServer
    30| logger = logging.getLogger(__name__)
    31| class RoomMemberHandler:
    32|     __metaclass__ = abc.ABCMeta
    33|     def __init__(self, hs: "HomeServer"):
    34|         self.hs = hs
    35|         self.store = hs.get_datastore()
    36|         self.auth = hs.get_auth()
    37|         self.state_handler = hs.get_state_handler()
    38|         self.config = hs.config
    39|         self.federation_handler = hs.get_handlers().federation_handler
    40|         self.directory_handler = hs.get_handlers().directory_handler
    41|         self.identity_handler = hs.get_handlers().identity_handler
    42|         self.registration_handler = hs.get_registration_handler()
    43|         self.profile_handler = hs.get_profile_handler()
    44|         self.event_creation_handler = hs.get_event_creation_handler()
    45|         self.member_linearizer = Linearizer(name="member")
    46|         self.clock = hs.get_clock()
    47|         self.spam_checker = hs.get_spam_checker()
    48|         self.third_party_event_rules = hs.get_third_party_event_rules()
    49|         self._server_notices_mxid = self.config.server_notices_mxid
    50|         self._enable_lookup = hs.config.enable_3pid_lookup
    51|         self.allow_per_room_profiles = self.config.allow_per_room_profiles
    52|         self._event_stream_writer_instance = hs.config.worker.writers.events
    53|         self._is_on_event_persistence_instance = (
    54|             self._event_stream_writer_instance == hs.get_instance_name()
    55|         )
    56|         if self._is_on_event_persistence_instance:
    57|             self.persist_event_storage = hs.get_storage().persistence
    58|         self._join_rate_limiter_local = Ratelimiter(
    59|             clock=self.clock,
    60|             rate_hz=hs.config.ratelimiting.rc_joins_local.per_second,
    61|             burst_count=hs.config.ratelimiting.rc_joins_local.burst_count,
    62|         )
    63|         self._join_rate_limiter_remote = Ratelimiter(
    64|             clock=self.clock,
    65|             rate_hz=hs.config.ratelimiting.rc_joins_remote.per_second,
    66|             burst_count=hs.config.ratelimiting.rc_joins_remote.burst_count,
    67|         )
    68|         self.base_handler = BaseHandler(hs)
    69|     @abc.abstractmethod
    70|     async def _remote_join(
    71|         self,
    72|         requester: Requester,
    73|         remote_room_hosts: List[str],
    74|         room_id: str,
    75|         user: UserID,
    76|         content: dict,
    77|     ) -> Tuple[str, int]:

# --- HUNK 2: Lines 85-133 ---
    85|         """
    86|         raise NotImplementedError()
    87|     @abc.abstractmethod
    88|     async def remote_reject_invite(
    89|         self,
    90|         invite_event_id: str,
    91|         txn_id: Optional[str],
    92|         requester: Requester,
    93|         content: JsonDict,
    94|     ) -> Tuple[str, int]:
    95|         """
    96|         Rejects an out-of-band invite we have received from a remote server
    97|         Args:
    98|             invite_event_id: ID of the invite to be rejected
    99|             txn_id: optional transaction ID supplied by the client
   100|             requester: user making the rejection request, according to the access token
   101|             content: additional content to include in the rejection event.
   102|                Normally an empty dict.
   103|         Returns:
   104|             event id, stream_id of the leave event
   105|         """
   106|         raise NotImplementedError()
   107|     @abc.abstractmethod
   108|     async def _user_joined_room(self, target: UserID, room_id: str) -> None:
   109|         """Notifies distributor on master process that the user has joined the
   110|         room.
   111|         Args:
   112|             target
   113|             room_id
   114|         """
   115|         raise NotImplementedError()
   116|     @abc.abstractmethod
   117|     async def _user_left_room(self, target: UserID, room_id: str) -> None:
   118|         """Notifies distributor on master process that the user has left the
   119|         room.
   120|         Args:
   121|             target
   122|             room_id
   123|         """
   124|         raise NotImplementedError()
   125|     async def _local_membership_update(
   126|         self,
   127|         requester: Requester,
   128|         target: UserID,
   129|         room_id: str,
   130|         membership: str,
   131|         prev_event_ids: List[str],
   132|         txn_id: Optional[str] = None,
   133|         ratelimit: bool = True,

# --- HUNK 3: Lines 146-207 ---
   146|                 "type": EventTypes.Member,
   147|                 "content": content,
   148|                 "room_id": room_id,
   149|                 "sender": requester.user.to_string(),
   150|                 "state_key": user_id,
   151|                 "membership": membership,
   152|             },
   153|             token_id=requester.access_token_id,
   154|             txn_id=txn_id,
   155|             prev_event_ids=prev_event_ids,
   156|             require_consent=require_consent,
   157|         )
   158|         duplicate = await self.event_creation_handler.deduplicate_state_event(
   159|             event, context
   160|         )
   161|         if duplicate is not None:
   162|             _, stream_id = await self.store.get_event_ordering(duplicate.event_id)
   163|             return duplicate.event_id, stream_id
   164|         prev_state_ids = await context.get_prev_state_ids()
   165|         prev_member_event_id = prev_state_ids.get((EventTypes.Member, user_id), None)
   166|         newly_joined = False
   167|         if event.membership == Membership.JOIN:
   168|             newly_joined = True
   169|             if prev_member_event_id:
   170|                 prev_member_event = await self.store.get_event(prev_member_event_id)
   171|                 newly_joined = prev_member_event.membership != Membership.JOIN
   172|             if newly_joined:
   173|                 time_now_s = self.clock.time()
   174|                 (
   175|                     allowed,
   176|                     time_allowed,
   177|                 ) = self._join_rate_limiter_local.can_requester_do_action(requester)
   178|                 if not allowed:
   179|                     raise LimitExceededError(
   180|                         retry_after_ms=int(1000 * (time_allowed - time_now_s))
   181|                     )
   182|         stream_id = await self.event_creation_handler.handle_new_client_event(
   183|             requester, event, context, extra_users=[target], ratelimit=ratelimit,
   184|         )
   185|         if event.membership == Membership.JOIN and newly_joined:
   186|             await self._user_joined_room(target, room_id)
   187|         elif event.membership == Membership.LEAVE:
   188|             if prev_member_event_id:
   189|                 prev_member_event = await self.store.get_event(prev_member_event_id)
   190|                 if prev_member_event.membership == Membership.JOIN:
   191|                     await self._user_left_room(target, room_id)
   192|         return event.event_id, stream_id
   193|     async def copy_room_tags_and_direct_to_room(
   194|         self, old_room_id, new_room_id, user_id
   195|     ) -> None:
   196|         """Copies the tags and direct room state from one room to another.
   197|         Args:
   198|             old_room_id: The room ID of the old room.
   199|             new_room_id: The room ID of the new room.
   200|             user_id: The user's ID.
   201|         """
   202|         user_account_data, _ = await self.store.get_account_data_for_user(user_id)
   203|         direct_rooms = user_account_data.get("m.direct", {})
   204|         if isinstance(direct_rooms, dict):
   205|             for key, room_id_list in direct_rooms.items():
   206|                 if old_room_id in room_id_list and new_room_id not in room_id_list:
   207|                     direct_rooms[key].append(new_room_id)

# --- HUNK 4: Lines 523-570 ---
   523|             event, context
   524|         )
   525|         if prev_event is not None:
   526|             return
   527|         prev_state_ids = await context.get_prev_state_ids()
   528|         if event.membership == Membership.JOIN:
   529|             if requester.is_guest:
   530|                 guest_can_join = await self._can_guest_join(prev_state_ids)
   531|                 if not guest_can_join:
   532|                     raise AuthError(403, "Guest access not allowed")
   533|         if event.membership not in (Membership.LEAVE, Membership.BAN):
   534|             is_blocked = await self.store.is_room_blocked(room_id)
   535|             if is_blocked:
   536|                 raise SynapseError(403, "This room has been blocked on this server")
   537|         await self.event_creation_handler.handle_new_client_event(
   538|             requester, event, context, extra_users=[target_user], ratelimit=ratelimit
   539|         )
   540|         prev_member_event_id = prev_state_ids.get(
   541|             (EventTypes.Member, event.state_key), None
   542|         )
   543|         if event.membership == Membership.JOIN:
   544|             newly_joined = True
   545|             if prev_member_event_id:
   546|                 prev_member_event = await self.store.get_event(prev_member_event_id)
   547|                 newly_joined = prev_member_event.membership != Membership.JOIN
   548|             if newly_joined:
   549|                 await self._user_joined_room(target_user, room_id)
   550|         elif event.membership == Membership.LEAVE:
   551|             if prev_member_event_id:
   552|                 prev_member_event = await self.store.get_event(prev_member_event_id)
   553|                 if prev_member_event.membership == Membership.JOIN:
   554|                     await self._user_left_room(target_user, room_id)
   555|     async def _can_guest_join(self, current_state_ids: StateMap[str]) -> bool:
   556|         """
   557|         Returns whether a guest can join a room based on its current state.
   558|         """
   559|         guest_access_id = current_state_ids.get((EventTypes.GuestAccess, ""), None)
   560|         if not guest_access_id:
   561|             return False
   562|         guest_access = await self.store.get_event(guest_access_id)
   563|         return bool(
   564|             guest_access
   565|             and guest_access.content
   566|             and "guest_access" in guest_access.content
   567|             and guest_access.content["guest_access"] == "can_join"
   568|         )
   569|     async def lookup_room_alias(
   570|         self, room_alias: RoomAlias

# --- HUNK 5: Lines 747-789 ---
   747|         create_event_id = current_state_ids.get(("m.room.create", ""))
   748|         if len(current_state_ids) == 1 and create_event_id:
   749|             return True
   750|         for etype, state_key in current_state_ids:
   751|             if etype != EventTypes.Member or not self.hs.is_mine_id(state_key):
   752|                 continue
   753|             event_id = current_state_ids[(etype, state_key)]
   754|             event = await self.store.get_event(event_id, allow_none=True)
   755|             if not event:
   756|                 continue
   757|             if event.membership == Membership.JOIN:
   758|                 return True
   759|         return False
   760|     async def _is_server_notice_room(self, room_id: str) -> bool:
   761|         if self._server_notices_mxid is None:
   762|             return False
   763|         user_ids = await self.store.get_users_in_room(room_id)
   764|         return self._server_notices_mxid in user_ids
   765| class RoomMemberMasterHandler(RoomMemberHandler):
   766|     def __init__(self, hs):
   767|         super(RoomMemberMasterHandler, self).__init__(hs)
   768|         self.distributor = hs.get_distributor()
   769|         self.distributor.declare("user_joined_room")
   770|         self.distributor.declare("user_left_room")
   771|     async def _is_remote_room_too_complex(
   772|         self, room_id: str, remote_room_hosts: List[str]
   773|     ) -> Optional[bool]:
   774|         """
   775|         Check if complexity of a remote room is too great.
   776|         Args:
   777|             room_id
   778|             remote_room_hosts
   779|         Returns: bool of whether the complexity is too great, or None
   780|             if unable to be fetched
   781|         """
   782|         max_complexity = self.hs.config.limit_remote_rooms.complexity
   783|         complexity = await self.federation_handler.get_room_complexity(
   784|             remote_room_hosts, room_id
   785|         )
   786|         if complexity:
   787|             return complexity["v1"] > max_complexity
   788|         return None
   789|     async def _is_local_room_too_complex(self, room_id: str) -> bool:

# --- HUNK 6: Lines 809-849 ---
   809|             host for host in remote_room_hosts if host != self.hs.hostname
   810|         ]
   811|         if len(remote_room_hosts) == 0:
   812|             raise SynapseError(404, "No known servers")
   813|         check_complexity = self.hs.config.limit_remote_rooms.enabled
   814|         if check_complexity and self.hs.config.limit_remote_rooms.admins_can_join:
   815|             check_complexity = not await self.auth.is_server_admin(user)
   816|         if check_complexity:
   817|             too_complex = await self._is_remote_room_too_complex(
   818|                 room_id, remote_room_hosts
   819|             )
   820|             if too_complex is True:
   821|                 raise SynapseError(
   822|                     code=400,
   823|                     msg=self.hs.config.limit_remote_rooms.complexity_error,
   824|                     errcode=Codes.RESOURCE_LIMIT_EXCEEDED,
   825|                 )
   826|         event_id, stream_id = await self.federation_handler.do_invite_join(
   827|             remote_room_hosts, room_id, user.to_string(), content
   828|         )
   829|         await self._user_joined_room(user, room_id)
   830|         if check_complexity:
   831|             if too_complex is False:
   832|                 return event_id, stream_id
   833|             too_complex = await self._is_local_room_too_complex(room_id)
   834|             if too_complex is False:
   835|                 return event_id, stream_id
   836|             requester = types.create_requester(user, None, False, False, None)
   837|             await self.update_membership(
   838|                 requester=requester, target=user, room_id=room_id, action="leave"
   839|             )
   840|             raise SynapseError(
   841|                 code=400,
   842|                 msg=self.hs.config.limit_remote_rooms.complexity_error,
   843|                 errcode=Codes.RESOURCE_LIMIT_EXCEEDED,
   844|             )
   845|         return event_id, stream_id
   846|     async def remote_reject_invite(
   847|         self,
   848|         invite_event_id: str,
   849|         txn_id: Optional[str],

# --- HUNK 7: Lines 910-950 ---
   910|         event = create_local_event_from_event_dict(
   911|             clock=self.clock,
   912|             hostname=self.hs.hostname,
   913|             signing_key=self.hs.signing_key,
   914|             room_version=room_version,
   915|             event_dict=event_dict,
   916|         )
   917|         event.internal_metadata.outlier = True
   918|         event.internal_metadata.out_of_band_membership = True
   919|         if txn_id is not None:
   920|             event.internal_metadata.txn_id = txn_id
   921|         if requester.access_token_id is not None:
   922|             event.internal_metadata.token_id = requester.access_token_id
   923|         EventValidator().validate_new(event, self.config)
   924|         context = await self.state_handler.compute_event_context(event)
   925|         context.app_service = requester.app_service
   926|         stream_id = await self.event_creation_handler.handle_new_client_event(
   927|             requester, event, context, extra_users=[UserID.from_string(target_user)],
   928|         )
   929|         return event.event_id, stream_id
   930|     async def _user_joined_room(self, target: UserID, room_id: str) -> None:
   931|         """Implements RoomMemberHandler._user_joined_room
   932|         """
   933|         user_joined_room(self.distributor, target, room_id)
   934|     async def _user_left_room(self, target: UserID, room_id: str) -> None:
   935|         """Implements RoomMemberHandler._user_left_room
   936|         """
   937|         user_left_room(self.distributor, target, room_id)
   938|     async def forget(self, user: UserID, room_id: str) -> None:
   939|         user_id = user.to_string()
   940|         member = await self.state_handler.get_current_state(
   941|             room_id=room_id, event_type=EventTypes.Member, state_key=user_id
   942|         )
   943|         membership = member.membership if member else None
   944|         if membership is not None and membership not in [
   945|             Membership.LEAVE,
   946|             Membership.BAN,
   947|         ]:
   948|             raise SynapseError(400, "User %s in room %s" % (user_id, room_id))
   949|         if membership:
   950|             await self.store.forget(user_id, room_id)


# ====================================================================
# FILE: synapse/handlers/room_member_worker.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-68 ---
     1| import logging
     2| from typing import List, Optional, Tuple
     3| from synapse.api.errors import SynapseError
     4| from synapse.handlers.room_member import RoomMemberHandler
     5| from synapse.replication.http.membership import (
     6|     ReplicationRemoteJoinRestServlet as ReplRemoteJoin,
     7|     ReplicationRemoteRejectInviteRestServlet as ReplRejectInvite,
     8|     ReplicationUserJoinedLeftRoomRestServlet as ReplJoinedLeft,
     9| )
    10| from synapse.types import Requester, UserID
    11| logger = logging.getLogger(__name__)
    12| class RoomMemberWorkerHandler(RoomMemberHandler):
    13|     def __init__(self, hs):
    14|         super(RoomMemberWorkerHandler, self).__init__(hs)
    15|         self._remote_join_client = ReplRemoteJoin.make_client(hs)
    16|         self._remote_reject_client = ReplRejectInvite.make_client(hs)
    17|         self._notify_change_client = ReplJoinedLeft.make_client(hs)
    18|     async def _remote_join(
    19|         self,
    20|         requester: Requester,
    21|         remote_room_hosts: List[str],
    22|         room_id: str,
    23|         user: UserID,
    24|         content: dict,
    25|     ) -> Tuple[str, int]:
    26|         """Implements RoomMemberHandler._remote_join
    27|         """
    28|         if len(remote_room_hosts) == 0:
    29|             raise SynapseError(404, "No known servers")
    30|         ret = await self._remote_join_client(
    31|             requester=requester,
    32|             remote_room_hosts=remote_room_hosts,
    33|             room_id=room_id,
    34|             user_id=user.to_string(),
    35|             content=content,
    36|         )
    37|         await self._user_joined_room(user, room_id)
    38|         return ret["event_id"], ret["stream_id"]
    39|     async def remote_reject_invite(
    40|         self,
    41|         invite_event_id: str,
    42|         txn_id: Optional[str],
    43|         requester: Requester,
    44|         content: dict,
    45|     ) -> Tuple[str, int]:
    46|         """
    47|         Rejects an out-of-band invite received from a remote user
    48|         Implements RoomMemberHandler.remote_reject_invite
    49|         """
    50|         ret = await self._remote_reject_client(
    51|             invite_event_id=invite_event_id,
    52|             txn_id=txn_id,
    53|             requester=requester,
    54|             content=content,
    55|         )
    56|         return ret["event_id"], ret["stream_id"]
    57|     async def _user_joined_room(self, target: UserID, room_id: str) -> None:
    58|         """Implements RoomMemberHandler._user_joined_room
    59|         """
    60|         await self._notify_change_client(
    61|             user_id=target.to_string(), room_id=room_id, change="joined"
    62|         )
    63|     async def _user_left_room(self, target: UserID, room_id: str) -> None:
    64|         """Implements RoomMemberHandler._user_left_room
    65|         """
    66|         await self._notify_change_client(
    67|             user_id=target.to_string(), room_id=room_id, change="left"
    68|         )


# ====================================================================
# FILE: synapse/handlers/saml_handler.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-301 ---
     1| import logging
     2| import re
     3| from typing import TYPE_CHECKING, Callable, Dict, Optional, Set, Tuple
     4| import attr
     5| import saml2
     6| import saml2.response
     7| from saml2.client import Saml2Client
     8| from synapse.api.errors import AuthError, SynapseError
     9| from synapse.config import ConfigError
    10| from synapse.config.saml2_config import SamlAttributeRequirement
    11| from synapse.http.servlet import parse_string
    12| from synapse.http.site import SynapseRequest
    13| from synapse.module_api import ModuleApi
    14| from synapse.types import (
    15|     UserID,
    16|     map_username_to_mxid_localpart,
    17|     mxid_localpart_allowed_characters,
    18| )
    19| from synapse.util.async_helpers import Linearizer
    20| from synapse.util.iterutils import chunk_seq
    21| if TYPE_CHECKING:
    22|     import synapse.server
    23| logger = logging.getLogger(__name__)
    24| @attr.s
    25| class Saml2SessionData:
    26|     """Data we track about SAML2 sessions"""
    27|     creation_time = attr.ib()
    28|     ui_auth_session_id = attr.ib(type=Optional[str], default=None)
    29| class SamlHandler:
    30|     def __init__(self, hs: "synapse.server.HomeServer"):
    31|         self.hs = hs
    32|         self._saml_client = Saml2Client(hs.config.saml2_sp_config)
    33|         self._auth = hs.get_auth()
    34|         self._auth_handler = hs.get_auth_handler()
    35|         self._registration_handler = hs.get_registration_handler()
    36|         self._clock = hs.get_clock()
    37|         self._datastore = hs.get_datastore()
    38|         self._hostname = hs.hostname
    39|         self._saml2_session_lifetime = hs.config.saml2_session_lifetime
    40|         self._grandfathered_mxid_source_attribute = (
    41|             hs.config.saml2_grandfathered_mxid_source_attribute
    42|         )
    43|         self._saml2_attribute_requirements = hs.config.saml2.attribute_requirements
    44|         self._user_mapping_provider = hs.config.saml2_user_mapping_provider_class(
    45|             hs.config.saml2_user_mapping_provider_config,
    46|             ModuleApi(hs, hs.get_auth_handler()),
    47|         )
    48|         self._auth_provider_id = "saml"
    49|         self._outstanding_requests_dict = {}  # type: Dict[str, Saml2SessionData]
    50|         self._mapping_lock = Linearizer(name="saml_mapping", clock=self._clock)
    51|     def handle_redirect_request(
    52|         self, client_redirect_url: bytes, ui_auth_session_id: Optional[str] = None
    53|     ) -> bytes:
    54|         """Handle an incoming request to /login/sso/redirect
    55|         Args:
    56|             client_redirect_url: the URL that we should redirect the
    57|                 client to when everything is done
    58|             ui_auth_session_id: The session ID of the ongoing UI Auth (or
    59|                 None if this is a login).
    60|         Returns:
    61|             URL to redirect to
    62|         """
    63|         reqid, info = self._saml_client.prepare_for_authenticate(
    64|             relay_state=client_redirect_url
    65|         )
    66|         logger.info("Initiating a new SAML session: %s" % (reqid,))
    67|         now = self._clock.time_msec()
    68|         self._outstanding_requests_dict[reqid] = Saml2SessionData(
    69|             creation_time=now, ui_auth_session_id=ui_auth_session_id,
    70|         )
    71|         for key, value in info["headers"]:
    72|             if key == "Location":
    73|                 return value
    74|         raise Exception("prepare_for_authenticate didn't return a Location header")
    75|     async def handle_saml_response(self, request: SynapseRequest) -> None:
    76|         """Handle an incoming request to /_matrix/saml2/authn_response
    77|         Args:
    78|             request: the incoming request from the browser. We'll
    79|                 respond to it with a redirect.
    80|         Returns:
    81|             Completes once we have handled the request.
    82|         """
    83|         resp_bytes = parse_string(request, "SAMLResponse", required=True)
    84|         relay_state = parse_string(request, "RelayState", required=True)
    85|         self.expire_sessions()
    86|         user_agent = request.requestHeaders.getRawHeaders(b"User-Agent", default=[b""])[
    87|             0
    88|         ].decode("ascii", "surrogateescape")
    89|         ip_address = self.hs.get_ip_from_request(request)
    90|         user_id, current_session = await self._map_saml_response_to_user(
    91|             resp_bytes, relay_state, user_agent, ip_address
    92|         )
    93|         if current_session and current_session.ui_auth_session_id:
    94|             await self._auth_handler.complete_sso_ui_auth(
    95|                 user_id, current_session.ui_auth_session_id, request
    96|             )
    97|         else:
    98|             await self._auth_handler.complete_sso_login(user_id, request, relay_state)
    99|     async def _map_saml_response_to_user(
   100|         self,
   101|         resp_bytes: str,
   102|         client_redirect_url: str,
   103|         user_agent: str,
   104|         ip_address: str,
   105|     ) -> Tuple[str, Optional[Saml2SessionData]]:
   106|         """
   107|         Given a sample response, retrieve the cached session and user for it.
   108|         Args:
   109|             resp_bytes: The SAML response.
   110|             client_redirect_url: The redirect URL passed in by the client.
   111|             user_agent: The user agent of the client making the request.
   112|             ip_address: The IP address of the client making the request.
   113|         Returns:
   114|              Tuple of the user ID and SAML session associated with this response.
   115|         Raises:
   116|             SynapseError if there was a problem with the response.
   117|             RedirectException: some mapping providers may raise this if they need
   118|                 to redirect to an interstitial page.
   119|         """
   120|         try:
   121|             saml2_auth = self._saml_client.parse_authn_request_response(
   122|                 resp_bytes,
   123|                 saml2.BINDING_HTTP_POST,
   124|                 outstanding=self._outstanding_requests_dict,
   125|             )
   126|         except saml2.response.UnsolicitedResponse as e:
   127|             logger.warning(str(e))
   128|             raise SynapseError(400, "Unexpected SAML2 login.")
   129|         except Exception as e:
   130|             raise SynapseError(400, "Unable to parse SAML2 response: %s." % (e,))
   131|         if saml2_auth.not_signed:
   132|             raise SynapseError(400, "SAML2 response was not signed.")
   133|         logger.debug("SAML2 response: %s", saml2_auth.origxml)
   134|         for assertion in saml2_auth.assertions:
   135|             count = 0
   136|             for part in chunk_seq(str(assertion), 10000):
   137|                 logger.info(
   138|                     "SAML2 assertion: %s%s", "(%i)..." % (count,) if count else "", part
   139|                 )
   140|                 count += 1
   141|         logger.info("SAML2 mapped attributes: %s", saml2_auth.ava)
   142|         current_session = self._outstanding_requests_dict.pop(
   143|             saml2_auth.in_response_to, None
   144|         )
   145|         for requirement in self._saml2_attribute_requirements:
   146|             _check_attribute_requirement(saml2_auth.ava, requirement)
   147|         remote_user_id = self._user_mapping_provider.get_remote_user_id(
   148|             saml2_auth, client_redirect_url
   149|         )
   150|         if not remote_user_id:
   151|             raise Exception("Failed to extract remote user id from SAML response")
   152|         with (await self._mapping_lock.queue(self._auth_provider_id)):
   153|             logger.info(
   154|                 "Looking for existing mapping for user %s:%s",
   155|                 self._auth_provider_id,
   156|                 remote_user_id,
   157|             )
   158|             registered_user_id = await self._datastore.get_user_by_external_id(
   159|                 self._auth_provider_id, remote_user_id
   160|             )
   161|             if registered_user_id is not None:
   162|                 logger.info("Found existing mapping %s", registered_user_id)
   163|                 return registered_user_id, current_session
   164|             if (
   165|                 self._grandfathered_mxid_source_attribute
   166|                 and self._grandfathered_mxid_source_attribute in saml2_auth.ava
   167|             ):
   168|                 attrval = saml2_auth.ava[self._grandfathered_mxid_source_attribute][0]
   169|                 user_id = UserID(
   170|                     map_username_to_mxid_localpart(attrval), self._hostname
   171|                 ).to_string()
   172|                 logger.info(
   173|                     "Looking for existing account based on mapped %s %s",
   174|                     self._grandfathered_mxid_source_attribute,
   175|                     user_id,
   176|                 )
   177|                 users = await self._datastore.get_users_by_id_case_insensitive(user_id)
   178|                 if users:
   179|                     registered_user_id = list(users.keys())[0]
   180|                     logger.info("Grandfathering mapping to %s", registered_user_id)
   181|                     await self._datastore.record_user_external_id(
   182|                         self._auth_provider_id, remote_user_id, registered_user_id
   183|                     )
   184|                     return registered_user_id, current_session
   185|             for i in range(1000):
   186|                 attribute_dict = self._user_mapping_provider.saml_response_to_user_attributes(
   187|                     saml2_auth, i, client_redirect_url=client_redirect_url,
   188|                 )
   189|                 logger.debug(
   190|                     "Retrieved SAML attributes from user mapping provider: %s "
   191|                     "(attempt %d)",
   192|                     attribute_dict,
   193|                     i,
   194|                 )
   195|                 localpart = attribute_dict.get("mxid_localpart")
   196|                 if not localpart:
   197|                     raise Exception(
   198|                         "Error parsing SAML2 response: SAML mapping provider plugin "
   199|                         "did not return a mxid_localpart value"
   200|                     )
   201|                 displayname = attribute_dict.get("displayname")
   202|                 emails = attribute_dict.get("emails", [])
   203|                 if not await self._datastore.get_users_by_id_case_insensitive(
   204|                     UserID(localpart, self._hostname).to_string()
   205|                 ):
   206|                     break
   207|             else:
   208|                 raise SynapseError(
   209|                     500, "Unable to generate a Matrix ID from the SAML response"
   210|                 )
   211|             logger.info("Mapped SAML user to local part %s", localpart)
   212|             registered_user_id = await self._registration_handler.register_user(
   213|                 localpart=localpart,
   214|                 default_display_name=displayname,
   215|                 bind_emails=emails,
   216|                 user_agent_ips=(user_agent, ip_address),
   217|             )
   218|             await self._datastore.record_user_external_id(
   219|                 self._auth_provider_id, remote_user_id, registered_user_id
   220|             )
   221|             return registered_user_id, current_session
   222|     def expire_sessions(self):
   223|         expire_before = self._clock.time_msec() - self._saml2_session_lifetime
   224|         to_expire = set()
   225|         for reqid, data in self._outstanding_requests_dict.items():
   226|             if data.creation_time < expire_before:
   227|                 to_expire.add(reqid)
   228|         for reqid in to_expire:
   229|             logger.debug("Expiring session id %s", reqid)
   230|             del self._outstanding_requests_dict[reqid]
   231| def _check_attribute_requirement(ava: dict, req: SamlAttributeRequirement):
   232|     values = ava.get(req.attribute, [])
   233|     for v in values:
   234|         if v == req.value:
   235|             return
   236|     logger.info(
   237|         "SAML2 attribute %s did not match required value '%s' (was '%s')",
   238|         req.attribute,
   239|         req.value,
   240|         values,
   241|     )
   242|     raise AuthError(403, "You are not authorized to log in here.")
   243| DOT_REPLACE_PATTERN = re.compile(
   244|     ("[^%s]" % (re.escape("".join(mxid_localpart_allowed_characters)),))
   245| )
   246| def dot_replace_for_mxid(username: str) -> str:
   247|     """Replace any characters which are not allowed in Matrix IDs with a dot."""
   248|     username = username.lower()
   249|     username = DOT_REPLACE_PATTERN.sub(".", username)
   250|     username = re.sub("^_", "", username)
   251|     return username
   252| MXID_MAPPER_MAP = {
   253|     "hexencode": map_username_to_mxid_localpart,
   254|     "dotreplace": dot_replace_for_mxid,
   255| }  # type: Dict[str, Callable[[str], str]]
   256| @attr.s
   257| class SamlConfig:
   258|     mxid_source_attribute = attr.ib()
   259|     mxid_mapper = attr.ib()
   260| class DefaultSamlMappingProvider:
   261|     __version__ = "0.0.1"
   262|     def __init__(self, parsed_config: SamlConfig, module_api: ModuleApi):
   263|         """The default SAML user mapping provider
   264|         Args:
   265|             parsed_config: Module configuration
   266|             module_api: module api proxy
   267|         """
   268|         self._mxid_source_attribute = parsed_config.mxid_source_attribute
   269|         self._mxid_mapper = parsed_config.mxid_mapper
   270|         self._grandfathered_mxid_source_attribute = (
   271|             module_api._hs.config.saml2_grandfathered_mxid_source_attribute
   272|         )
   273|     def get_remote_user_id(
   274|         self, saml_response: saml2.response.AuthnResponse, client_redirect_url: str
   275|     ) -> str:
   276|         """Extracts the remote user id from the SAML response"""
   277|         try:
   278|             return saml_response.ava["uid"][0]
   279|         except KeyError:
   280|             logger.warning("SAML2 response lacks a 'uid' attestation")
   281|             raise SynapseError(400, "'uid' not in SAML2 response")
   282|     def saml_response_to_user_attributes(
   283|         self,
   284|         saml_response: saml2.response.AuthnResponse,
   285|         failures: int,
   286|         client_redirect_url: str,
   287|     ) -> dict:
   288|         """Maps some text from a SAML response to attributes of a new user
   289|         Args:
   290|             saml_response: A SAML auth response object
   291|             failures: How many times a call to this function with this
   292|                 saml_response has resulted in a failure
   293|             client_redirect_url: where the client wants to redirect to
   294|         Returns:
   295|             dict: A dict containing new user attributes. Possible keys:
   296|                 * mxid_localpart (str): Required. The localpart of the user's mxid
   297|                 * displayname (str): The displayname of the user
   298|                 * emails (list[str]): Any emails for the user
   299|         """
   300|         try:
   301|             mxid_source = saml_response.ava[self._mxid_source_attribute][0]


# ====================================================================
# FILE: synapse/handlers/search.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-34 ---
     1| import itertools
     2| import logging
     3| from typing import Iterable
     4| from unpaddedbase64 import decode_base64, encode_base64
     5| from synapse.api.constants import EventTypes, Membership
     6| from synapse.api.errors import NotFoundError, SynapseError
     7| from synapse.api.filtering import Filter
     8| from synapse.storage.state import StateFilter
     9| from synapse.visibility import filter_events_for_client
    10| from ._base import BaseHandler
    11| logger = logging.getLogger(__name__)
    12| class SearchHandler(BaseHandler):
    13|     def __init__(self, hs):
    14|         super(SearchHandler, self).__init__(hs)
    15|         self._event_serializer = hs.get_event_client_serializer()
    16|         self.storage = hs.get_storage()
    17|         self.state_store = self.storage.state
    18|         self.auth = hs.get_auth()
    19|     async def get_old_rooms_from_upgraded_room(self, room_id: str) -> Iterable[str]:
    20|         """Retrieves room IDs of old rooms in the history of an upgraded room.
    21|         We do so by checking the m.room.create event of the room for a
    22|         `predecessor` key. If it exists, we add the room ID to our return
    23|         list and then check that room for a m.room.create event and so on
    24|         until we can no longer find any more previous rooms.
    25|         The full list of all found rooms in then returned.
    26|         Args:
    27|             room_id: id of the room to search through.
    28|         Returns:
    29|             Predecessor room ids
    30|         """
    31|         historical_room_ids = []
    32|         predecessor = await self.store.get_room_predecessor(room_id)
    33|         while True:
    34|             if not predecessor:

# --- HUNK 2: Lines 210-255 ---
   210|             raise NotImplementedError()
   211|         logger.info("Found %d events to return", len(allowed_events))
   212|         if event_context is not None:
   213|             now_token = self.hs.get_event_sources().get_current_token()
   214|             contexts = {}
   215|             for event in allowed_events:
   216|                 res = await self.store.get_events_around(
   217|                     event.room_id, event.event_id, before_limit, after_limit
   218|                 )
   219|                 logger.info(
   220|                     "Context for search returned %d and %d events",
   221|                     len(res["events_before"]),
   222|                     len(res["events_after"]),
   223|                 )
   224|                 res["events_before"] = await filter_events_for_client(
   225|                     self.storage, user.to_string(), res["events_before"]
   226|                 )
   227|                 res["events_after"] = await filter_events_for_client(
   228|                     self.storage, user.to_string(), res["events_after"]
   229|                 )
   230|                 res["start"] = now_token.copy_and_replace(
   231|                     "room_key", res["start"]
   232|                 ).to_string()
   233|                 res["end"] = now_token.copy_and_replace(
   234|                     "room_key", res["end"]
   235|                 ).to_string()
   236|                 if include_profile:
   237|                     senders = {
   238|                         ev.sender
   239|                         for ev in itertools.chain(
   240|                             res["events_before"], [event], res["events_after"]
   241|                         )
   242|                     }
   243|                     if res["events_after"]:
   244|                         last_event_id = res["events_after"][-1].event_id
   245|                     else:
   246|                         last_event_id = event.event_id
   247|                     state_filter = StateFilter.from_types(
   248|                         [(EventTypes.Member, sender) for sender in senders]
   249|                     )
   250|                     state = await self.state_store.get_state_for_event(
   251|                         last_event_id, state_filter
   252|                     )
   253|                     res["profile_info"] = {
   254|                         s.state_key: {
   255|                             "displayname": s.content.get("displayname", None),


# ====================================================================
# FILE: synapse/handlers/set_password.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-30 ---
     1| import logging
     2| from typing import Optional
     3| from synapse.api.errors import Codes, StoreError, SynapseError
     4| from synapse.types import Requester
     5| from ._base import BaseHandler
     6| logger = logging.getLogger(__name__)
     7| class SetPasswordHandler(BaseHandler):
     8|     """Handler which deals with changing user account passwords"""
     9|     def __init__(self, hs):
    10|         super(SetPasswordHandler, self).__init__(hs)
    11|         self._auth_handler = hs.get_auth_handler()
    12|         self._device_handler = hs.get_device_handler()
    13|         self._password_policy_handler = hs.get_password_policy_handler()
    14|     async def set_password(
    15|         self,
    16|         user_id: str,
    17|         password_hash: str,
    18|         logout_devices: bool,
    19|         requester: Optional[Requester] = None,
    20|     ):
    21|         if not self.hs.config.password_localdb_enabled:
    22|             raise SynapseError(403, "Password change disabled", errcode=Codes.FORBIDDEN)
    23|         try:
    24|             await self.store.user_set_password_hash(user_id, password_hash)
    25|         except StoreError as e:
    26|             if e.code == 404:
    27|                 raise SynapseError(404, "Unknown user", Codes.NOT_FOUND)
    28|             raise e
    29|         if logout_devices:
    30|             except_device_id = requester.device_id if requester else None


# ====================================================================
# FILE: synapse/handlers/sync.py
# Total hunks: 11
# ====================================================================
# --- HUNK 1: Lines 33-190 ---
    33|     "synapse_handlers_sync_nonempty_total",
    34|     "Count of non empty sync responses. type is initial_sync/full_state_sync"
    35|     "/incremental_sync. lazy_loaded indicates if lazy loaded members were "
    36|     "enabled for that request.",
    37|     ["type", "lazy_loaded"],
    38| )
    39| LAZY_LOADED_MEMBERS_CACHE_MAX_AGE = 30 * 60 * 1000
    40| LAZY_LOADED_MEMBERS_CACHE_MAX_SIZE = 100
    41| @attr.s(slots=True, frozen=True)
    42| class SyncConfig:
    43|     user = attr.ib(type=UserID)
    44|     filter_collection = attr.ib(type=FilterCollection)
    45|     is_guest = attr.ib(type=bool)
    46|     request_key = attr.ib(type=Tuple[Any, ...])
    47|     device_id = attr.ib(type=str)
    48| @attr.s(slots=True, frozen=True)
    49| class TimelineBatch:
    50|     prev_batch = attr.ib(type=StreamToken)
    51|     events = attr.ib(type=List[EventBase])
    52|     limited = attr.ib(bool)
    53|     def __nonzero__(self) -> bool:
    54|         """Make the result appear empty if there are no updates. This is used
    55|         to tell if room needs to be part of the sync result.
    56|         """
    57|         return bool(self.events)
    58|     __bool__ = __nonzero__  # python3
    59| @attr.s(slots=True)
    60| class JoinedSyncResult:
    61|     room_id = attr.ib(type=str)
    62|     timeline = attr.ib(type=TimelineBatch)
    63|     state = attr.ib(type=StateMap[EventBase])
    64|     ephemeral = attr.ib(type=List[JsonDict])
    65|     account_data = attr.ib(type=List[JsonDict])
    66|     unread_notifications = attr.ib(type=JsonDict)
    67|     summary = attr.ib(type=Optional[JsonDict])
    68|     unread_count = attr.ib(type=int)
    69|     def __nonzero__(self) -> bool:
    70|         """Make the result appear empty if there are no updates. This is used
    71|         to tell if room needs to be part of the sync result.
    72|         """
    73|         return bool(
    74|             self.timeline
    75|             or self.state
    76|             or self.ephemeral
    77|             or self.account_data
    78|         )
    79|     __bool__ = __nonzero__  # python3
    80| @attr.s(slots=True, frozen=True)
    81| class ArchivedSyncResult:
    82|     room_id = attr.ib(type=str)
    83|     timeline = attr.ib(type=TimelineBatch)
    84|     state = attr.ib(type=StateMap[EventBase])
    85|     account_data = attr.ib(type=List[JsonDict])
    86|     def __nonzero__(self) -> bool:
    87|         """Make the result appear empty if there are no updates. This is used
    88|         to tell if room needs to be part of the sync result.
    89|         """
    90|         return bool(self.timeline or self.state or self.account_data)
    91|     __bool__ = __nonzero__  # python3
    92| @attr.s(slots=True, frozen=True)
    93| class InvitedSyncResult:
    94|     room_id = attr.ib(type=str)
    95|     invite = attr.ib(type=EventBase)
    96|     def __nonzero__(self) -> bool:
    97|         """Invited rooms should always be reported to the client"""
    98|         return True
    99|     __bool__ = __nonzero__  # python3
   100| @attr.s(slots=True, frozen=True)
   101| class GroupsSyncResult:
   102|     join = attr.ib(type=JsonDict)
   103|     invite = attr.ib(type=JsonDict)
   104|     leave = attr.ib(type=JsonDict)
   105|     def __nonzero__(self) -> bool:
   106|         return bool(self.join or self.invite or self.leave)
   107|     __bool__ = __nonzero__  # python3
   108| @attr.s(slots=True, frozen=True)
   109| class DeviceLists:
   110|     """
   111|     Attributes:
   112|         changed: List of user_ids whose devices may have changed
   113|         left: List of user_ids whose devices we no longer track
   114|     """
   115|     changed = attr.ib(type=Collection[str])
   116|     left = attr.ib(type=Collection[str])
   117|     def __nonzero__(self) -> bool:
   118|         return bool(self.changed or self.left)
   119|     __bool__ = __nonzero__  # python3
   120| @attr.s
   121| class _RoomChanges:
   122|     """The set of room entries to include in the sync, plus the set of joined
   123|     and left room IDs since last sync.
   124|     """
   125|     room_entries = attr.ib(type=List["RoomSyncResultBuilder"])
   126|     invited = attr.ib(type=List[InvitedSyncResult])
   127|     newly_joined_rooms = attr.ib(type=List[str])
   128|     newly_left_rooms = attr.ib(type=List[str])
   129| @attr.s(slots=True, frozen=True)
   130| class SyncResult:
   131|     """
   132|     Attributes:
   133|         next_batch: Token for the next sync
   134|         presence: List of presence events for the user.
   135|         account_data: List of account_data events for the user.
   136|         joined: JoinedSyncResult for each joined room.
   137|         invited: InvitedSyncResult for each invited room.
   138|         archived: ArchivedSyncResult for each archived room.
   139|         to_device: List of direct messages for the device.
   140|         device_lists: List of user_ids whose devices have changed
   141|         device_one_time_keys_count: Dict of algorithm to count for one time keys
   142|             for this device
   143|         groups: Group updates, if any
   144|     """
   145|     next_batch = attr.ib(type=StreamToken)
   146|     presence = attr.ib(type=List[JsonDict])
   147|     account_data = attr.ib(type=List[JsonDict])
   148|     joined = attr.ib(type=List[JoinedSyncResult])
   149|     invited = attr.ib(type=List[InvitedSyncResult])
   150|     archived = attr.ib(type=List[ArchivedSyncResult])
   151|     to_device = attr.ib(type=List[JsonDict])
   152|     device_lists = attr.ib(type=DeviceLists)
   153|     device_one_time_keys_count = attr.ib(type=JsonDict)
   154|     groups = attr.ib(type=Optional[GroupsSyncResult])
   155|     def __nonzero__(self) -> bool:
   156|         """Make the result appear empty if there are no updates. This is used
   157|         to tell if the notifier needs to wait for more events when polling for
   158|         events.
   159|         """
   160|         return bool(
   161|             self.presence
   162|             or self.joined
   163|             or self.invited
   164|             or self.archived
   165|             or self.account_data
   166|             or self.to_device
   167|             or self.device_lists
   168|             or self.groups
   169|         )
   170|     __bool__ = __nonzero__  # python3
   171| class SyncHandler:
   172|     def __init__(self, hs: "HomeServer"):
   173|         self.hs_config = hs.config
   174|         self.store = hs.get_datastore()
   175|         self.notifier = hs.get_notifier()
   176|         self.presence_handler = hs.get_presence_handler()
   177|         self.event_sources = hs.get_event_sources()
   178|         self.clock = hs.get_clock()
   179|         self.response_cache = ResponseCache(hs, "sync")
   180|         self.state = hs.get_state_handler()
   181|         self.auth = hs.get_auth()
   182|         self.storage = hs.get_storage()
   183|         self.state_store = self.storage.state
   184|         self.lazy_loaded_members_cache = ExpiringCache(
   185|             "lazy_loaded_members_cache",
   186|             self.clock,
   187|             max_len=0,
   188|             expiry_ms=LAZY_LOADED_MEMBERS_CACHE_MAX_AGE,
   189|         )
   190|     async def wait_for_sync_for_user(

# --- HUNK 2: Lines 262-318 ---
   262|         return rules
   263|     async def ephemeral_by_room(
   264|         self,
   265|         sync_result_builder: "SyncResultBuilder",
   266|         now_token: StreamToken,
   267|         since_token: Optional[StreamToken] = None,
   268|     ) -> Tuple[StreamToken, Dict[str, List[JsonDict]]]:
   269|         """Get the ephemeral events for each room the user is in
   270|         Args:
   271|             sync_result_builder
   272|             now_token: Where the server is currently up to.
   273|             since_token: Where the server was when the client
   274|                 last synced.
   275|         Returns:
   276|             A tuple of the now StreamToken, updated to reflect the which typing
   277|             events are included, and a dict mapping from room_id to a list of
   278|             typing events for that room.
   279|         """
   280|         sync_config = sync_result_builder.sync_config
   281|         with Measure(self.clock, "ephemeral_by_room"):
   282|             typing_key = since_token.typing_key if since_token else "0"
   283|             room_ids = sync_result_builder.joined_room_ids
   284|             typing_source = self.event_sources.sources["typing"]
   285|             typing, typing_key = await typing_source.get_new_events(
   286|                 user=sync_config.user,
   287|                 from_key=typing_key,
   288|                 limit=sync_config.filter_collection.ephemeral_limit(),
   289|                 room_ids=room_ids,
   290|                 is_guest=sync_config.is_guest,
   291|             )
   292|             now_token = now_token.copy_and_replace("typing_key", typing_key)
   293|             ephemeral_by_room = {}  # type: JsonDict
   294|             for event in typing:
   295|                 room_id = event["room_id"]
   296|                 event_copy = {k: v for (k, v) in event.items() if k != "room_id"}
   297|                 ephemeral_by_room.setdefault(room_id, []).append(event_copy)
   298|             receipt_key = since_token.receipt_key if since_token else "0"
   299|             receipt_source = self.event_sources.sources["receipt"]
   300|             receipts, receipt_key = await receipt_source.get_new_events(
   301|                 user=sync_config.user,
   302|                 from_key=receipt_key,
   303|                 limit=sync_config.filter_collection.ephemeral_limit(),
   304|                 room_ids=room_ids,
   305|                 is_guest=sync_config.is_guest,
   306|             )
   307|             now_token = now_token.copy_and_replace("receipt_key", receipt_key)
   308|             for event in receipts:
   309|                 room_id = event["room_id"]
   310|                 event_copy = {k: v for (k, v) in event.items() if k != "room_id"}
   311|                 ephemeral_by_room.setdefault(room_id, []).append(event_copy)
   312|         return now_token, ephemeral_by_room
   313|     async def _load_filtered_recents(
   314|         self,
   315|         room_id: str,
   316|         sync_config: SyncConfig,
   317|         now_token: StreamToken,
   318|         since_token: Optional[StreamToken] = None,

# --- HUNK 3: Lines 695-735 ---
   695|         self,
   696|         sync_config: SyncConfig,
   697|         since_token: Optional[StreamToken] = None,
   698|         full_state: bool = False,
   699|     ) -> SyncResult:
   700|         """Generates a sync result.
   701|         """
   702|         now_token = self.event_sources.get_current_token()
   703|         logger.debug(
   704|             "Calculating sync response for %r between %s and %s",
   705|             sync_config.user,
   706|             since_token,
   707|             now_token,
   708|         )
   709|         user_id = sync_config.user.to_string()
   710|         app_service = self.store.get_app_service_by_user_id(user_id)
   711|         if app_service:
   712|             raise NotImplementedError()
   713|         else:
   714|             joined_room_ids = await self.get_rooms_for_user_at(
   715|                 user_id, now_token.room_stream_id
   716|             )
   717|         sync_result_builder = SyncResultBuilder(
   718|             sync_config,
   719|             full_state,
   720|             since_token=since_token,
   721|             now_token=now_token,
   722|             joined_room_ids=joined_room_ids,
   723|         )
   724|         logger.debug("Fetching account data")
   725|         account_data_by_room = await self._generate_sync_entry_for_account_data(
   726|             sync_result_builder
   727|         )
   728|         logger.debug("Fetching room data")
   729|         res = await self._generate_sync_entry_for_rooms(
   730|             sync_result_builder, account_data_by_room
   731|         )
   732|         newly_joined_rooms, newly_joined_or_invited_users, _, _ = res
   733|         _, _, newly_left_rooms, newly_left_users = res
   734|         block_all_presence_data = (
   735|             since_token is None and sync_config.filter_collection.blocks_all_presence()

# --- HUNK 4: Lines 946-997 ---
   946|         self,
   947|         sync_result_builder: "SyncResultBuilder",
   948|         newly_joined_rooms: Set[str],
   949|         newly_joined_or_invited_users: Set[str],
   950|     ) -> None:
   951|         """Generates the presence portion of the sync response. Populates the
   952|         `sync_result_builder` with the result.
   953|         Args:
   954|             sync_result_builder
   955|             newly_joined_rooms: Set of rooms that the user has joined since
   956|                 the last sync (or empty if an initial sync)
   957|             newly_joined_or_invited_users: Set of users that have joined or
   958|                 been invited to rooms since the last sync (or empty if an
   959|                 initial sync)
   960|         """
   961|         now_token = sync_result_builder.now_token
   962|         sync_config = sync_result_builder.sync_config
   963|         user = sync_result_builder.sync_config.user
   964|         presence_source = self.event_sources.sources["presence"]
   965|         since_token = sync_result_builder.since_token
   966|         if since_token and not sync_result_builder.full_state:
   967|             presence_key = since_token.presence_key
   968|             include_offline = True
   969|         else:
   970|             presence_key = None
   971|             include_offline = False
   972|         presence, presence_key = await presence_source.get_new_events(
   973|             user=user,
   974|             from_key=presence_key,
   975|             is_guest=sync_config.is_guest,
   976|             include_offline=include_offline,
   977|         )
   978|         sync_result_builder.now_token = now_token.copy_and_replace(
   979|             "presence_key", presence_key
   980|         )
   981|         extra_users_ids = set(newly_joined_or_invited_users)
   982|         for room_id in newly_joined_rooms:
   983|             users = await self.state.get_current_users_in_room(room_id)
   984|             extra_users_ids.update(users)
   985|         extra_users_ids.discard(user.to_string())
   986|         if extra_users_ids:
   987|             states = await self.presence_handler.get_states(extra_users_ids)
   988|             presence.extend(states)
   989|             presence = list({p.user_id: p for p in presence}.values())
   990|         presence = sync_config.filter_collection.filter_presence(presence)
   991|         sync_result_builder.presence = presence
   992|     async def _generate_sync_entry_for_rooms(
   993|         self,
   994|         sync_result_builder: "SyncResultBuilder",
   995|         account_data_by_room: Dict[str, Dict[str, JsonDict]],
   996|     ) -> Tuple[Set[str], Set[str], Set[str], Set[str]]:
   997|         """Generates the rooms portion of the sync response. Populates the

# --- HUNK 5: Lines 1089-1129 ---
  1089|             set(newly_joined_rooms),
  1090|             newly_joined_or_invited_users,
  1091|             set(newly_left_rooms),
  1092|             newly_left_users,
  1093|         )
  1094|     async def _have_rooms_changed(
  1095|         self, sync_result_builder: "SyncResultBuilder"
  1096|     ) -> bool:
  1097|         """Returns whether there may be any new events that should be sent down
  1098|         the sync. Returns True if there are.
  1099|         """
  1100|         user_id = sync_result_builder.sync_config.user.to_string()
  1101|         since_token = sync_result_builder.since_token
  1102|         now_token = sync_result_builder.now_token
  1103|         assert since_token
  1104|         rooms_changed = await self.store.get_membership_changes_for_user(
  1105|             user_id, since_token.room_key, now_token.room_key
  1106|         )
  1107|         if rooms_changed:
  1108|             return True
  1109|         stream_id = RoomStreamToken.parse_stream_token(since_token.room_key).stream
  1110|         for room_id in sync_result_builder.joined_room_ids:
  1111|             if self.store.has_room_changed_since(room_id, stream_id):
  1112|                 return True
  1113|         return False
  1114|     async def _get_rooms_changed(
  1115|         self, sync_result_builder: "SyncResultBuilder", ignored_users: Set[str]
  1116|     ) -> _RoomChanges:
  1117|         """Gets the the changes that have happened since the last sync.
  1118|         """
  1119|         user_id = sync_result_builder.sync_config.user.to_string()
  1120|         since_token = sync_result_builder.since_token
  1121|         now_token = sync_result_builder.now_token
  1122|         sync_config = sync_result_builder.sync_config
  1123|         assert since_token
  1124|         rooms_changed = await self.store.get_membership_changes_for_user(
  1125|             user_id, since_token.room_key, now_token.room_key
  1126|         )
  1127|         mem_change_events_by_room_id = {}  # type: Dict[str, List[EventBase]]
  1128|         for event in rooms_changed:
  1129|             mem_change_events_by_room_id.setdefault(event.room_id, []).append(event)

# --- HUNK 6: Lines 1179-1226 ---
  1179|                         old_mem_ev = None
  1180|                         if old_mem_ev_id:
  1181|                             old_mem_ev = await self.store.get_event(
  1182|                                 old_mem_ev_id, allow_none=True
  1183|                             )
  1184|                     if old_mem_ev and old_mem_ev.membership == Membership.JOIN:
  1185|                         newly_left_rooms.append(room_id)
  1186|             should_invite = non_joins[-1].membership == Membership.INVITE
  1187|             if should_invite:
  1188|                 if event.sender not in ignored_users:
  1189|                     room_sync = InvitedSyncResult(room_id, invite=non_joins[-1])
  1190|                     if room_sync:
  1191|                         invited.append(room_sync)
  1192|             leave_events = [
  1193|                 e
  1194|                 for e in non_joins
  1195|                 if e.membership in (Membership.LEAVE, Membership.BAN)
  1196|             ]
  1197|             if leave_events:
  1198|                 leave_event = leave_events[-1]
  1199|                 leave_stream_token = await self.store.get_stream_token_for_event(
  1200|                     leave_event.event_id
  1201|                 )
  1202|                 leave_token = since_token.copy_and_replace(
  1203|                     "room_key", leave_stream_token
  1204|                 )
  1205|                 if since_token and since_token.is_after(leave_token):
  1206|                     continue
  1207|                 if leave_event.internal_metadata.is_out_of_band_membership():
  1208|                     batch_events = [leave_event]  # type: Optional[List[EventBase]]
  1209|                 else:
  1210|                     batch_events = None
  1211|                 room_entries.append(
  1212|                     RoomSyncResultBuilder(
  1213|                         room_id=room_id,
  1214|                         rtype="archived",
  1215|                         events=batch_events,
  1216|                         newly_joined=room_id in newly_joined_rooms,
  1217|                         full_state=False,
  1218|                         since_token=since_token,
  1219|                         upto_token=leave_token,
  1220|                     )
  1221|                 )
  1222|         timeline_limit = sync_config.filter_collection.timeline_limit()
  1223|         room_to_events = await self.store.get_room_events_stream_for_rooms(
  1224|             room_ids=sync_result_builder.joined_room_ids,
  1225|             from_key=since_token.room_key,
  1226|             to_key=now_token.room_key,

# --- HUNK 7: Lines 1289-1329 ---
  1289|                         room_id=event.room_id,
  1290|                         rtype="joined",
  1291|                         events=None,
  1292|                         newly_joined=False,
  1293|                         full_state=True,
  1294|                         since_token=since_token,
  1295|                         upto_token=now_token,
  1296|                     )
  1297|                 )
  1298|             elif event.membership == Membership.INVITE:
  1299|                 if event.sender in ignored_users:
  1300|                     continue
  1301|                 invite = await self.store.get_event(event.event_id)
  1302|                 invited.append(InvitedSyncResult(room_id=event.room_id, invite=invite))
  1303|             elif event.membership in (Membership.LEAVE, Membership.BAN):
  1304|                 if not sync_config.filter_collection.include_leave:
  1305|                     if event.membership == Membership.LEAVE:
  1306|                         if user_id == event.sender:
  1307|                             continue
  1308|                 leave_token = now_token.copy_and_replace(
  1309|                     "room_key", "s%d" % (event.stream_ordering,)
  1310|                 )
  1311|                 room_entries.append(
  1312|                     RoomSyncResultBuilder(
  1313|                         room_id=event.room_id,
  1314|                         rtype="archived",
  1315|                         events=None,
  1316|                         newly_joined=False,
  1317|                         full_state=True,
  1318|                         since_token=since_token,
  1319|                         upto_token=leave_token,
  1320|                     )
  1321|                 )
  1322|         return _RoomChanges(room_entries, invited, [], [])
  1323|     async def _generate_room_entry(
  1324|         self,
  1325|         sync_result_builder: "SyncResultBuilder",
  1326|         ignored_users: Set[str],
  1327|         room_builder: "RoomSyncResultBuilder",
  1328|         ephemeral: List[JsonDict],
  1329|         tags: Optional[Dict[str, Dict[str, Any]]],

# --- HUNK 8: Lines 1422-1482 ---
  1422|                 room_sync.unread_count = notifs["unread_count"]
  1423|                 sync_result_builder.joined.append(room_sync)
  1424|             if batch.limited and since_token:
  1425|                 user_id = sync_result_builder.sync_config.user.to_string()
  1426|                 logger.debug(
  1427|                     "Incremental gappy sync of %s for user %s with %d state events"
  1428|                     % (room_id, user_id, len(state))
  1429|                 )
  1430|         elif room_builder.rtype == "archived":
  1431|             archived_room_sync = ArchivedSyncResult(
  1432|                 room_id=room_id,
  1433|                 timeline=batch,
  1434|                 state=state,
  1435|                 account_data=account_data_events,
  1436|             )
  1437|             if archived_room_sync or always_include:
  1438|                 sync_result_builder.archived.append(archived_room_sync)
  1439|         else:
  1440|             raise Exception("Unrecognized rtype: %r", room_builder.rtype)
  1441|     async def get_rooms_for_user_at(
  1442|         self, user_id: str, stream_ordering: int
  1443|     ) -> FrozenSet[str]:
  1444|         """Get set of joined rooms for a user at the given stream ordering.
  1445|         The stream ordering *must* be recent, otherwise this may throw an
  1446|         exception if older than a month. (This function is called with the
  1447|         current token, which should be perfectly fine).
  1448|         Args:
  1449|             user_id
  1450|             stream_ordering
  1451|         ReturnValue:
  1452|             Set of room_ids the user is in at given stream_ordering.
  1453|         """
  1454|         joined_rooms = await self.store.get_rooms_for_user_with_stream_ordering(user_id)
  1455|         joined_room_ids = set()
  1456|         for room_id, membership_stream_ordering in joined_rooms:
  1457|             if membership_stream_ordering <= stream_ordering:
  1458|                 joined_room_ids.add(room_id)
  1459|                 continue
  1460|             logger.info("User joined room after current token: %s", room_id)
  1461|             extrems = await self.store.get_forward_extremeties_for_room(
  1462|                 room_id, stream_ordering
  1463|             )
  1464|             users_in_room = await self.state.get_current_users_in_room(room_id, extrems)
  1465|             if user_id in users_in_room:
  1466|                 joined_room_ids.add(room_id)
  1467|         return frozenset(joined_room_ids)
  1468| def _action_has_highlight(actions: List[JsonDict]) -> bool:
  1469|     for action in actions:
  1470|         try:
  1471|             if action.get("set_tweak", None) == "highlight":
  1472|                 return action.get("value", True)
  1473|         except AttributeError:
  1474|             pass
  1475|     return False
  1476| def _calculate_state(
  1477|     timeline_contains: StateMap[str],
  1478|     timeline_start: StateMap[str],
  1479|     previous: StateMap[str],
  1480|     current: StateMap[str],
  1481|     lazy_load_members: bool,
  1482| ) -> StateMap[str]:

# --- HUNK 9: Lines 1493-1562 ---
  1493|     """
  1494|     event_id_to_key = {
  1495|         e: key
  1496|         for key, e in itertools.chain(
  1497|             timeline_contains.items(),
  1498|             previous.items(),
  1499|             timeline_start.items(),
  1500|             current.items(),
  1501|         )
  1502|     }
  1503|     c_ids = set(current.values())
  1504|     ts_ids = set(timeline_start.values())
  1505|     p_ids = set(previous.values())
  1506|     tc_ids = set(timeline_contains.values())
  1507|     if lazy_load_members:
  1508|         p_ids.difference_update(
  1509|             e for t, e in timeline_start.items() if t[0] == EventTypes.Member
  1510|         )
  1511|     state_ids = ((c_ids | ts_ids) - p_ids) - tc_ids
  1512|     return {event_id_to_key[e]: e for e in state_ids}
  1513| @attr.s
  1514| class SyncResultBuilder:
  1515|     """Used to help build up a new SyncResult for a user
  1516|     Attributes:
  1517|         sync_config
  1518|         full_state: The full_state flag as specified by user
  1519|         since_token: The token supplied by user, or None.
  1520|         now_token: The token to sync up to.
  1521|         joined_room_ids: List of rooms the user is joined to
  1522|         presence (list)
  1523|         account_data (list)
  1524|         joined (list[JoinedSyncResult])
  1525|         invited (list[InvitedSyncResult])
  1526|         archived (list[ArchivedSyncResult])
  1527|         groups (GroupsSyncResult|None)
  1528|         to_device (list)
  1529|     """
  1530|     sync_config = attr.ib(type=SyncConfig)
  1531|     full_state = attr.ib(type=bool)
  1532|     since_token = attr.ib(type=Optional[StreamToken])
  1533|     now_token = attr.ib(type=StreamToken)
  1534|     joined_room_ids = attr.ib(type=FrozenSet[str])
  1535|     presence = attr.ib(type=List[JsonDict], default=attr.Factory(list))
  1536|     account_data = attr.ib(type=List[JsonDict], default=attr.Factory(list))
  1537|     joined = attr.ib(type=List[JoinedSyncResult], default=attr.Factory(list))
  1538|     invited = attr.ib(type=List[InvitedSyncResult], default=attr.Factory(list))
  1539|     archived = attr.ib(type=List[ArchivedSyncResult], default=attr.Factory(list))
  1540|     groups = attr.ib(type=Optional[GroupsSyncResult], default=None)
  1541|     to_device = attr.ib(type=List[JsonDict], default=attr.Factory(list))
  1542| @attr.s
  1543| class RoomSyncResultBuilder:
  1544|     """Stores information needed to create either a `JoinedSyncResult` or
  1545|     `ArchivedSyncResult`.
  1546|     Attributes:
  1547|         room_id
  1548|         rtype: One of `"joined"` or `"archived"`
  1549|         events: List of events to include in the room (more events may be added
  1550|             when generating result).
  1551|         newly_joined: If the user has newly joined the room
  1552|         full_state: Whether the full state should be sent in result
  1553|         since_token: Earliest point to return events from, or None
  1554|         upto_token: Latest point to return events from.
  1555|     """
  1556|     room_id = attr.ib(type=str)
  1557|     rtype = attr.ib(type=str)
  1558|     events = attr.ib(type=Optional[List[EventBase]])
  1559|     newly_joined = attr.ib(type=bool)
  1560|     full_state = attr.ib(type=bool)
  1561|     since_token = attr.ib(type=Optional[StreamToken])
  1562|     upto_token = attr.ib(type=StreamToken)


# ====================================================================
# FILE: synapse/handlers/user_directory.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-38 ---
     1| import logging
     2| import synapse.metrics
     3| from synapse.api.constants import EventTypes, JoinRules, Membership
     4| from synapse.handlers.state_deltas import StateDeltasHandler
     5| from synapse.metrics.background_process_metrics import run_as_background_process
     6| from synapse.storage.roommember import ProfileInfo
     7| from synapse.util.metrics import Measure
     8| logger = logging.getLogger(__name__)
     9| class UserDirectoryHandler(StateDeltasHandler):
    10|     """Handles querying of and keeping updated the user_directory.
    11|     N.B.: ASSUMES IT IS THE ONLY THING THAT MODIFIES THE USER DIRECTORY
    12|     The user directory is filled with users who this server can see are joined to a
    13|     world_readable or publically joinable room. We keep a database table up to date
    14|     by streaming changes of the current state and recalculating whether users should
    15|     be in the directory or not when necessary.
    16|     """
    17|     def __init__(self, hs):
    18|         super(UserDirectoryHandler, self).__init__(hs)
    19|         self.store = hs.get_datastore()
    20|         self.state = hs.get_state_handler()
    21|         self.server_name = hs.hostname
    22|         self.clock = hs.get_clock()
    23|         self.notifier = hs.get_notifier()
    24|         self.is_mine_id = hs.is_mine_id
    25|         self.update_user_directory = hs.config.update_user_directory
    26|         self.search_all_users = hs.config.user_directory_search_all_users
    27|         self.spam_checker = hs.get_spam_checker()
    28|         self.pos = None
    29|         self._is_processing = False
    30|         if self.update_user_directory:
    31|             self.notifier.add_replication_callback(self.notify_new_event)
    32|             self.clock.call_later(0, self.notify_new_event)
    33|     async def search_users(self, user_id, search_term, limit):
    34|         """Searches for users in directory
    35|         Returns:
    36|             dict of the form::
    37|                 {
    38|                     "limited": <bool>,  # whether there were more results or not


# ====================================================================
# FILE: synapse/http/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-34 ---
     1| import re
     2| from twisted.internet import task
     3| from twisted.internet.defer import CancelledError
     4| from twisted.python import failure
     5| from twisted.web.client import FileBodyProducer
     6| from synapse.api.errors import SynapseError
     7| class RequestTimedOutError(SynapseError):
     8|     """Exception representing timeout of an outbound request"""
     9|     def __init__(self):
    10|         super(RequestTimedOutError, self).__init__(504, "Timed out")
    11| def cancelled_to_request_timed_out_error(value, timeout):
    12|     """Turns CancelledErrors into RequestTimedOutErrors.
    13|     For use with async.add_timeout_to_deferred
    14|     """
    15|     if isinstance(value, failure.Failure):
    16|         value.trap(CancelledError)
    17|         raise RequestTimedOutError()
    18|     return value
    19| ACCESS_TOKEN_RE = re.compile(r"(\?.*access(_|%5[Ff])token=)[^&]*(.*)$")
    20| CLIENT_SECRET_RE = re.compile(r"(\?.*client(_|%5[Ff])secret=)[^&]*(.*)$")
    21| def redact_uri(uri):
    22|     """Strips sensitive information from the uri replaces with <redacted>"""
    23|     uri = ACCESS_TOKEN_RE.sub(r"\1<redacted>\3", uri)
    24|     return CLIENT_SECRET_RE.sub(r"\1<redacted>\3", uri)
    25| class QuieterFileBodyProducer(FileBodyProducer):
    26|     """Wrapper for FileBodyProducer that avoids CRITICAL errors when the connection drops.
    27|     Workaround for https://github.com/matrix-org/synapse/issues/4003 /
    28|     https://twistedmatrix.com/trac/ticket/6528
    29|     """
    30|     def stopProducing(self):
    31|         try:
    32|             FileBodyProducer.stopProducing(self)
    33|         except task.TaskStopped:
    34|             pass


# ====================================================================
# FILE: synapse/http/client.py
# Total hunks: 8
# ====================================================================
# --- HUNK 1: Lines 1-57 ---
     1| import logging
     2| import urllib
     3| from io import BytesIO
     4| import treq
     5| from canonicaljson import encode_canonical_json
     6| from netaddr import IPAddress
     7| from prometheus_client import Counter
     8| from zope.interface import implementer, provider
     9| from OpenSSL import SSL
    10| from OpenSSL.SSL import VERIFY_NONE
    11| from twisted.internet import defer, protocol, ssl
    12| from twisted.internet.interfaces import (
    13|     IReactorPluggableNameResolver,
    14|     IResolutionReceiver,
    15| )
    16| from twisted.internet.task import Cooperator
    17| from twisted.python.failure import Failure
    18| from twisted.web._newclient import ResponseDone
    19| from twisted.web.client import Agent, HTTPConnectionPool, readBody
    20| from twisted.web.http import PotentialDataLoss
    21| from twisted.web.http_headers import Headers
    22| from synapse.api.errors import Codes, HttpResponseException, SynapseError
    23| from synapse.http import (
    24|     QuieterFileBodyProducer,
    25|     cancelled_to_request_timed_out_error,
    26|     redact_uri,
    27| )
    28| from synapse.http.proxyagent import ProxyAgent
    29| from synapse.logging.context import make_deferred_yieldable
    30| from synapse.logging.opentracing import set_tag, start_active_span, tags
    31| from synapse.util import json_decoder
    32| from synapse.util.async_helpers import timeout_deferred
    33| logger = logging.getLogger(__name__)
    34| outgoing_requests_counter = Counter("synapse_http_client_requests", "", ["method"])
    35| incoming_responses_counter = Counter(
    36|     "synapse_http_client_responses", "", ["method", "code"]
    37| )
    38| def check_against_blacklist(ip_address, ip_whitelist, ip_blacklist):
    39|     """
    40|     Args:
    41|         ip_address (netaddr.IPAddress)
    42|         ip_whitelist (netaddr.IPSet)
    43|         ip_blacklist (netaddr.IPSet)
    44|     """
    45|     if ip_address in ip_blacklist:
    46|         if ip_whitelist is None or ip_address not in ip_whitelist:
    47|             return True
    48|     return False
    49| _EPSILON = 0.00000001
    50| def _make_scheduler(reactor):
    51|     """Makes a schedular suitable for a Cooperator using the given reactor.
    52|     (This is effectively just a copy from `twisted.internet.task`)
    53|     """
    54|     def _scheduler(x):
    55|         return reactor.callLater(_EPSILON, x)
    56|     return _scheduler
    57| class IPBlacklistingResolver:

# --- HUNK 2: Lines 186-485 ---
   186|         else:
   187|             self.reactor = hs.get_reactor()
   188|         pool = HTTPConnectionPool(self.reactor)
   189|         pool.maxPersistentPerHost = max((100 * hs.config.caches.global_factor, 5))
   190|         pool.cachedConnectionTimeout = 2 * 60
   191|         self.agent = ProxyAgent(
   192|             self.reactor,
   193|             connectTimeout=15,
   194|             contextFactory=self.hs.get_http_client_context_factory(),
   195|             pool=pool,
   196|             http_proxy=http_proxy,
   197|             https_proxy=https_proxy,
   198|         )
   199|         if self._ip_blacklist:
   200|             self.agent = BlacklistingAgentWrapper(
   201|                 self.agent,
   202|                 self.reactor,
   203|                 ip_whitelist=self._ip_whitelist,
   204|                 ip_blacklist=self._ip_blacklist,
   205|             )
   206|     async def request(self, method, uri, data=None, headers=None):
   207|         """
   208|         Args:
   209|             method (str): HTTP method to use.
   210|             uri (str): URI to query.
   211|             data (bytes): Data to send in the request body, if applicable.
   212|             headers (t.w.http_headers.Headers): Request headers.
   213|         """
   214|         outgoing_requests_counter.labels(method).inc()
   215|         logger.debug("Sending request %s %s", method, redact_uri(uri))
   216|         with start_active_span(
   217|             "outgoing-client-request",
   218|             tags={
   219|                 tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,
   220|                 tags.HTTP_METHOD: method,
   221|                 tags.HTTP_URL: uri,
   222|             },
   223|             finish_on_close=True,
   224|         ):
   225|             try:
   226|                 body_producer = None
   227|                 if data is not None:
   228|                     body_producer = QuieterFileBodyProducer(
   229|                         BytesIO(data), cooperator=self._cooperator,
   230|                     )
   231|                 request_deferred = treq.request(
   232|                     method,
   233|                     uri,
   234|                     agent=self.agent,
   235|                     data=body_producer,
   236|                     headers=headers,
   237|                     **self._extra_treq_args
   238|                 )
   239|                 request_deferred = timeout_deferred(
   240|                     request_deferred,
   241|                     60,
   242|                     self.hs.get_reactor(),
   243|                     cancelled_to_request_timed_out_error,
   244|                 )
   245|                 response = await make_deferred_yieldable(request_deferred)
   246|                 incoming_responses_counter.labels(method, response.code).inc()
   247|                 logger.info(
   248|                     "Received response to %s %s: %s",
   249|                     method,
   250|                     redact_uri(uri),
   251|                     response.code,
   252|                 )
   253|                 return response
   254|             except Exception as e:
   255|                 incoming_responses_counter.labels(method, "ERR").inc()
   256|                 logger.info(
   257|                     "Error sending request to  %s %s: %s %s",
   258|                     method,
   259|                     redact_uri(uri),
   260|                     type(e).__name__,
   261|                     e.args[0],
   262|                 )
   263|                 set_tag(tags.ERROR, True)
   264|                 set_tag("error_reason", e.args[0])
   265|                 raise
   266|     async def post_urlencoded_get_json(self, uri, args={}, headers=None):
   267|         """
   268|         Args:
   269|             uri (str):
   270|             args (dict[str, str|List[str]]): query params
   271|             headers (dict[str|bytes, List[str|bytes]]|None): If not None, a map from
   272|                header name to a list of values for that header
   273|         Returns:
   274|             object: parsed json
   275|         Raises:
   276|             HttpResponseException: On a non-2xx HTTP response.
   277|             ValueError: if the response was not JSON
   278|         """
   279|         logger.debug("post_urlencoded_get_json args: %s", args)
   280|         query_bytes = urllib.parse.urlencode(encode_urlencode_args(args), True).encode(
   281|             "utf8"
   282|         )
   283|         actual_headers = {
   284|             b"Content-Type": [b"application/x-www-form-urlencoded"],
   285|             b"User-Agent": [self.user_agent],
   286|             b"Accept": [b"application/json"],
   287|         }
   288|         if headers:
   289|             actual_headers.update(headers)
   290|         response = await self.request(
   291|             "POST", uri, headers=Headers(actual_headers), data=query_bytes
   292|         )
   293|         body = await make_deferred_yieldable(readBody(response))
   294|         if 200 <= response.code < 300:
   295|             return json_decoder.decode(body.decode("utf-8"))
   296|         else:
   297|             raise HttpResponseException(
   298|                 response.code, response.phrase.decode("ascii", errors="replace"), body
   299|             )
   300|     async def post_json_get_json(self, uri, post_json, headers=None):
   301|         """
   302|         Args:
   303|             uri (str):
   304|             post_json (object):
   305|             headers (dict[str|bytes, List[str|bytes]]|None): If not None, a map from
   306|                header name to a list of values for that header
   307|         Returns:
   308|             object: parsed json
   309|         Raises:
   310|             HttpResponseException: On a non-2xx HTTP response.
   311|             ValueError: if the response was not JSON
   312|         """
   313|         json_str = encode_canonical_json(post_json)
   314|         logger.debug("HTTP POST %s -> %s", json_str, uri)
   315|         actual_headers = {
   316|             b"Content-Type": [b"application/json"],
   317|             b"User-Agent": [self.user_agent],
   318|             b"Accept": [b"application/json"],
   319|         }
   320|         if headers:
   321|             actual_headers.update(headers)
   322|         response = await self.request(
   323|             "POST", uri, headers=Headers(actual_headers), data=json_str
   324|         )
   325|         body = await make_deferred_yieldable(readBody(response))
   326|         if 200 <= response.code < 300:
   327|             return json_decoder.decode(body.decode("utf-8"))
   328|         else:
   329|             raise HttpResponseException(
   330|                 response.code, response.phrase.decode("ascii", errors="replace"), body
   331|             )
   332|     async def get_json(self, uri, args={}, headers=None):
   333|         """ Gets some json from the given URI.
   334|         Args:
   335|             uri (str): The URI to request, not including query parameters
   336|             args (dict): A dictionary used to create query strings, defaults to
   337|                 None.
   338|                 **Note**: The value of each key is assumed to be an iterable
   339|                 and *not* a string.
   340|             headers (dict[str|bytes, List[str|bytes]]|None): If not None, a map from
   341|                header name to a list of values for that header
   342|         Returns:
   343|             Succeeds when we get *any* 2xx HTTP response, with the
   344|             HTTP body as JSON.
   345|         Raises:
   346|             HttpResponseException On a non-2xx HTTP response.
   347|             ValueError: if the response was not JSON
   348|         """
   349|         actual_headers = {b"Accept": [b"application/json"]}
   350|         if headers:
   351|             actual_headers.update(headers)
   352|         body = await self.get_raw(uri, args, headers=headers)
   353|         return json_decoder.decode(body.decode("utf-8"))
   354|     async def put_json(self, uri, json_body, args={}, headers=None):
   355|         """ Puts some json to the given URI.
   356|         Args:
   357|             uri (str): The URI to request, not including query parameters
   358|             json_body (dict): The JSON to put in the HTTP body,
   359|             args (dict): A dictionary used to create query strings, defaults to
   360|                 None.
   361|                 **Note**: The value of each key is assumed to be an iterable
   362|                 and *not* a string.
   363|             headers (dict[str|bytes, List[str|bytes]]|None): If not None, a map from
   364|                header name to a list of values for that header
   365|         Returns:
   366|             Succeeds when we get *any* 2xx HTTP response, with the
   367|             HTTP body as JSON.
   368|         Raises:
   369|             HttpResponseException On a non-2xx HTTP response.
   370|             ValueError: if the response was not JSON
   371|         """
   372|         if len(args):
   373|             query_bytes = urllib.parse.urlencode(args, True)
   374|             uri = "%s?%s" % (uri, query_bytes)
   375|         json_str = encode_canonical_json(json_body)
   376|         actual_headers = {
   377|             b"Content-Type": [b"application/json"],
   378|             b"User-Agent": [self.user_agent],
   379|             b"Accept": [b"application/json"],
   380|         }
   381|         if headers:
   382|             actual_headers.update(headers)
   383|         response = await self.request(
   384|             "PUT", uri, headers=Headers(actual_headers), data=json_str
   385|         )
   386|         body = await make_deferred_yieldable(readBody(response))
   387|         if 200 <= response.code < 300:
   388|             return json_decoder.decode(body.decode("utf-8"))
   389|         else:
   390|             raise HttpResponseException(
   391|                 response.code, response.phrase.decode("ascii", errors="replace"), body
   392|             )
   393|     async def get_raw(self, uri, args={}, headers=None):
   394|         """ Gets raw text from the given URI.
   395|         Args:
   396|             uri (str): The URI to request, not including query parameters
   397|             args (dict): A dictionary used to create query strings, defaults to
   398|                 None.
   399|                 **Note**: The value of each key is assumed to be an iterable
   400|                 and *not* a string.
   401|             headers (dict[str|bytes, List[str|bytes]]|None): If not None, a map from
   402|                header name to a list of values for that header
   403|         Returns:
   404|             Succeeds when we get *any* 2xx HTTP response, with the
   405|             HTTP body as bytes.
   406|         Raises:
   407|             HttpResponseException on a non-2xx HTTP response.
   408|         """
   409|         if len(args):
   410|             query_bytes = urllib.parse.urlencode(args, True)
   411|             uri = "%s?%s" % (uri, query_bytes)
   412|         actual_headers = {b"User-Agent": [self.user_agent]}
   413|         if headers:
   414|             actual_headers.update(headers)
   415|         response = await self.request("GET", uri, headers=Headers(actual_headers))
   416|         body = await make_deferred_yieldable(readBody(response))
   417|         if 200 <= response.code < 300:
   418|             return body
   419|         else:
   420|             raise HttpResponseException(
   421|                 response.code, response.phrase.decode("ascii", errors="replace"), body
   422|             )
   423|     async def get_file(self, url, output_stream, max_size=None, headers=None):
   424|         """GETs a file from a given URL
   425|         Args:
   426|             url (str): The URL to GET
   427|             output_stream (file): File to write the response body to.
   428|             headers (dict[str|bytes, List[str|bytes]]|None): If not None, a map from
   429|                header name to a list of values for that header
   430|         Returns:
   431|             A (int,dict,string,int) tuple of the file length, dict of the response
   432|             headers, absolute URI of the response and HTTP response code.
   433|         """
   434|         actual_headers = {b"User-Agent": [self.user_agent]}
   435|         if headers:
   436|             actual_headers.update(headers)
   437|         response = await self.request("GET", url, headers=Headers(actual_headers))
   438|         resp_headers = dict(response.headers.getAllRawHeaders())
   439|         if (
   440|             b"Content-Length" in resp_headers
   441|             and int(resp_headers[b"Content-Length"][0]) > max_size
   442|         ):
   443|             logger.warning("Requested URL is too large > %r bytes" % (self.max_size,))
   444|             raise SynapseError(
   445|                 502,
   446|                 "Requested file is too large > %r bytes" % (self.max_size,),
   447|                 Codes.TOO_LARGE,
   448|             )
   449|         if response.code > 299:
   450|             logger.warning("Got %d when downloading %s" % (response.code, url))
   451|             raise SynapseError(502, "Got error %d" % (response.code,), Codes.UNKNOWN)
   452|         try:
   453|             length = await make_deferred_yieldable(
   454|                 _readBodyToFile(response, output_stream, max_size)
   455|             )
   456|         except SynapseError:
   457|             raise
   458|         except Exception as e:
   459|             raise SynapseError(502, ("Failed to download remote body: %s" % e)) from e
   460|         return (
   461|             length,
   462|             resp_headers,
   463|             response.request.absoluteURI.decode("ascii"),
   464|             response.code,
   465|         )
   466| class _ReadBodyToFileProtocol(protocol.Protocol):
   467|     def __init__(self, stream, deferred, max_size):
   468|         self.stream = stream
   469|         self.deferred = deferred
   470|         self.length = 0
   471|         self.max_size = max_size
   472|     def dataReceived(self, data):
   473|         self.stream.write(data)
   474|         self.length += len(data)
   475|         if self.max_size is not None and self.length >= self.max_size:
   476|             self.deferred.errback(
   477|                 SynapseError(
   478|                     502,
   479|                     "Requested file is too large > %r bytes" % (self.max_size,),
   480|                     Codes.TOO_LARGE,
   481|                 )
   482|             )
   483|             self.deferred = defer.Deferred()
   484|             self.transport.loseConnection()
   485|     def connectionLost(self, reason):


# ====================================================================
# FILE: synapse/http/federation/well_known_resolver.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 178-200 ---
   178|                 return int(max_age)
   179|             except ValueError:
   180|                 pass
   181|     expires = headers.getRawHeaders(b"expires")
   182|     if expires is not None:
   183|         try:
   184|             expires_date = stringToDatetime(expires[-1])
   185|             return expires_date - time_now()
   186|         except ValueError:
   187|             return 0
   188|     return None
   189| def _parse_cache_control(headers: Headers) -> Dict[bytes, Optional[bytes]]:
   190|     cache_controls = {}
   191|     for hdr in headers.getRawHeaders(b"cache-control", []):
   192|         for directive in hdr.split(b","):
   193|             splits = [x.strip() for x in directive.split(b"=", 1)]
   194|             k = splits[0].lower()
   195|             v = splits[1] if len(splits) > 1 else None
   196|             cache_controls[k] = v
   197|     return cache_controls
   198| @attr.s()
   199| class _FetchWellKnownFailure(Exception):
   200|     temporary = attr.ib()


# ====================================================================
# FILE: synapse/http/matrixfederationclient.py
# Total hunks: 9
# ====================================================================
# --- HUNK 1: Lines 33-73 ---
    33| from synapse.logging.opentracing import (
    34|     inject_active_span_byte_dict,
    35|     set_tag,
    36|     start_active_span,
    37|     tags,
    38| )
    39| from synapse.util import json_decoder
    40| from synapse.util.async_helpers import timeout_deferred
    41| from synapse.util.metrics import Measure
    42| logger = logging.getLogger(__name__)
    43| outgoing_requests_counter = Counter(
    44|     "synapse_http_matrixfederationclient_requests", "", ["method"]
    45| )
    46| incoming_responses_counter = Counter(
    47|     "synapse_http_matrixfederationclient_responses", "", ["method", "code"]
    48| )
    49| MAX_LONG_RETRIES = 10
    50| MAX_SHORT_RETRIES = 3
    51| MAXINT = sys.maxsize
    52| _next_id = 1
    53| @attr.s(frozen=True)
    54| class MatrixFederationRequest:
    55|     method = attr.ib()
    56|     """HTTP method
    57|     :type: str
    58|     """
    59|     path = attr.ib()
    60|     """HTTP path
    61|     :type: str
    62|     """
    63|     destination = attr.ib()
    64|     """The remote server to send the HTTP request to.
    65|     :type: str"""
    66|     json = attr.ib(default=None)
    67|     """JSON to send in the body.
    68|     :type: dict|None
    69|     """
    70|     json_callback = attr.ib(default=None)
    71|     """A callback to generate the JSON.
    72|     :type: func|None
    73|     """

# --- HUNK 2: Lines 108-148 ---
   108|     response: IResponse,
   109|     start_ms: int,
   110| ):
   111|     """
   112|     Reads the JSON body of a response, with a timeout
   113|     Args:
   114|         reactor: twisted reactor, for the timeout
   115|         timeout_sec: number of seconds to wait for response to complete
   116|         request: the request that triggered the response
   117|         response: response to the request
   118|         start_ms: Timestamp when request was made
   119|     Returns:
   120|         dict: parsed JSON response
   121|     """
   122|     try:
   123|         check_content_type_is_json(response.headers)
   124|         d = treq.text_content(response, encoding="utf-8")
   125|         d.addCallback(json_decoder.decode)
   126|         d = timeout_deferred(d, timeout=timeout_sec, reactor=reactor)
   127|         body = await make_deferred_yieldable(d)
   128|     except TimeoutError as e:
   129|         logger.warning(
   130|             "{%s} [%s] Timed out reading response - %s %s",
   131|             request.txn_id,
   132|             request.destination,
   133|             request.method,
   134|             request.uri.decode("ascii"),
   135|         )
   136|         raise RequestSendFailed(e, can_retry=True) from e
   137|     except Exception as e:
   138|         logger.warning(
   139|             "{%s} [%s] Error reading response %s %s: %s",
   140|             request.txn_id,
   141|             request.destination,
   142|             request.method,
   143|             request.uri.decode("ascii"),
   144|             e,
   145|         )
   146|         raise
   147|     time_taken_secs = reactor.seconds() - start_ms / 1000
   148|     logger.info(

# --- HUNK 3: Lines 341-382 ---
   341|                         request.destination,
   342|                         request.method,
   343|                         url_str,
   344|                         _sec_timeout,
   345|                     )
   346|                     outgoing_requests_counter.labels(request.method).inc()
   347|                     try:
   348|                         with Measure(self.clock, "outbound_request"):
   349|                             request_deferred = self.agent.request(
   350|                                 method_bytes,
   351|                                 url_bytes,
   352|                                 headers=Headers(headers_dict),
   353|                                 bodyProducer=producer,
   354|                             )
   355|                             request_deferred = timeout_deferred(
   356|                                 request_deferred,
   357|                                 timeout=_sec_timeout,
   358|                                 reactor=self.reactor,
   359|                             )
   360|                             response = await request_deferred
   361|                     except TimeoutError as e:
   362|                         raise RequestSendFailed(e, can_retry=True) from e
   363|                     except DNSLookupError as e:
   364|                         raise RequestSendFailed(e, can_retry=retry_on_dns_fail) from e
   365|                     except Exception as e:
   366|                         raise RequestSendFailed(e, can_retry=True) from e
   367|                     incoming_responses_counter.labels(
   368|                         request.method, response.code
   369|                     ).inc()
   370|                     set_tag(tags.HTTP_STATUS_CODE, response.code)
   371|                     response_phrase = response.phrase.decode("ascii", errors="replace")
   372|                     if 200 <= response.code < 300:
   373|                         logger.debug(
   374|                             "{%s} [%s] Got response headers: %d %s",
   375|                             request.txn_id,
   376|                             request.destination,
   377|                             response.code,
   378|                             response_phrase,
   379|                         )
   380|                         pass
   381|                     else:
   382|                         logger.info(

# --- HUNK 4: Lines 495-603 ---
   495|         data={},
   496|         json_data_callback=None,
   497|         long_retries=False,
   498|         timeout=None,
   499|         ignore_backoff=False,
   500|         backoff_on_404=False,
   501|         try_trailing_slash_on_400=False,
   502|     ):
   503|         """ Sends the specifed json data using PUT
   504|         Args:
   505|             destination (str): The remote server to send the HTTP request
   506|                 to.
   507|             path (str): The HTTP path.
   508|             args (dict): query params
   509|             data (dict): A dict containing the data that will be used as
   510|                 the request body. This will be encoded as JSON.
   511|             json_data_callback (callable): A callable returning the dict to
   512|                 use as the request body.
   513|             long_retries (bool): whether to use the long retry algorithm. See
   514|                 docs on _send_request for details.
   515|             timeout (int|None): number of milliseconds to wait for the response headers
   516|                 (including connecting to the server), *for each attempt*.
   517|                 self._default_timeout (60s) by default.
   518|             ignore_backoff (bool): true to ignore the historical backoff data
   519|                 and try the request anyway.
   520|             backoff_on_404 (bool): True if we should count a 404 response as
   521|                 a failure of the server (and should therefore back off future
   522|                 requests).
   523|             try_trailing_slash_on_400 (bool): True if on a 400 M_UNRECOGNIZED
   524|                 response we should try appending a trailing slash to the end
   525|                 of the request. Workaround for #3622 in Synapse <= v0.99.3. This
   526|                 will be attempted before backing off if backing off has been
   527|                 enabled.
   528|         Returns:
   529|             dict|list: Succeeds when we get a 2xx HTTP response. The
   530|             result will be the decoded JSON body.
   531|         Raises:
   532|             HttpResponseException: If we get an HTTP response code >= 300
   533|                 (except 429).
   534|             NotRetryingDestination: If we are not yet ready to retry this
   535|                 server.
   536|             FederationDeniedError: If this destination  is not on our
   537|                 federation whitelist
   538|             RequestSendFailed: If there were problems connecting to the
   539|                 remote, due to e.g. DNS failures, connection timeouts etc.
   540|         """
   541|         request = MatrixFederationRequest(
   542|             method="PUT",
   543|             destination=destination,
   544|             path=path,
   545|             query=args,
   546|             json_callback=json_data_callback,
   547|             json=data,
   548|         )
   549|         start_ms = self.clock.time_msec()
   550|         response = await self._send_request_with_optional_trailing_slash(
   551|             request,
   552|             try_trailing_slash_on_400,
   553|             backoff_on_404=backoff_on_404,
   554|             ignore_backoff=ignore_backoff,
   555|             long_retries=long_retries,
   556|             timeout=timeout,
   557|         )
   558|         body = await _handle_json_response(
   559|             self.reactor, self.default_timeout, request, response, start_ms
   560|         )
   561|         return body
   562|     async def post_json(
   563|         self,
   564|         destination,
   565|         path,
   566|         data={},
   567|         long_retries=False,
   568|         timeout=None,
   569|         ignore_backoff=False,
   570|         args={},
   571|     ):
   572|         """ Sends the specifed json data using POST
   573|         Args:
   574|             destination (str): The remote server to send the HTTP request
   575|                 to.
   576|             path (str): The HTTP path.
   577|             data (dict): A dict containing the data that will be used as
   578|                 the request body. This will be encoded as JSON.
   579|             long_retries (bool): whether to use the long retry algorithm. See
   580|                 docs on _send_request for details.
   581|             timeout (int|None): number of milliseconds to wait for the response headers
   582|                 (including connecting to the server), *for each attempt*.
   583|                 self._default_timeout (60s) by default.
   584|             ignore_backoff (bool): true to ignore the historical backoff data and
   585|                 try the request anyway.
   586|             args (dict): query params
   587|         Returns:
   588|             dict|list: Succeeds when we get a 2xx HTTP response. The
   589|             result will be the decoded JSON body.
   590|         Raises:
   591|             HttpResponseException: If we get an HTTP response code >= 300
   592|                 (except 429).
   593|             NotRetryingDestination: If we are not yet ready to retry this
   594|                 server.
   595|             FederationDeniedError: If this destination  is not on our
   596|                 federation whitelist
   597|             RequestSendFailed: If there were problems connecting to the
   598|                 remote, due to e.g. DNS failures, connection timeouts etc.
   599|         """
   600|         request = MatrixFederationRequest(
   601|             method="POST", destination=destination, path=path, query=args, json=data
   602|         )
   603|         start_ms = self.clock.time_msec()

# --- HUNK 5: Lines 615-738 ---
   615|             self.reactor, _sec_timeout, request, response, start_ms,
   616|         )
   617|         return body
   618|     async def get_json(
   619|         self,
   620|         destination,
   621|         path,
   622|         args=None,
   623|         retry_on_dns_fail=True,
   624|         timeout=None,
   625|         ignore_backoff=False,
   626|         try_trailing_slash_on_400=False,
   627|     ):
   628|         """ GETs some json from the given host homeserver and path
   629|         Args:
   630|             destination (str): The remote server to send the HTTP request
   631|                 to.
   632|             path (str): The HTTP path.
   633|             args (dict|None): A dictionary used to create query strings, defaults to
   634|                 None.
   635|             timeout (int|None): number of milliseconds to wait for the response headers
   636|                 (including connecting to the server), *for each attempt*.
   637|                 self._default_timeout (60s) by default.
   638|             ignore_backoff (bool): true to ignore the historical backoff data
   639|                 and try the request anyway.
   640|             try_trailing_slash_on_400 (bool): True if on a 400 M_UNRECOGNIZED
   641|                 response we should try appending a trailing slash to the end of
   642|                 the request. Workaround for #3622 in Synapse <= v0.99.3.
   643|         Returns:
   644|             dict|list: Succeeds when we get a 2xx HTTP response. The
   645|             result will be the decoded JSON body.
   646|         Raises:
   647|             HttpResponseException: If we get an HTTP response code >= 300
   648|                 (except 429).
   649|             NotRetryingDestination: If we are not yet ready to retry this
   650|                 server.
   651|             FederationDeniedError: If this destination  is not on our
   652|                 federation whitelist
   653|             RequestSendFailed: If there were problems connecting to the
   654|                 remote, due to e.g. DNS failures, connection timeouts etc.
   655|         """
   656|         request = MatrixFederationRequest(
   657|             method="GET", destination=destination, path=path, query=args
   658|         )
   659|         start_ms = self.clock.time_msec()
   660|         response = await self._send_request_with_optional_trailing_slash(
   661|             request,
   662|             try_trailing_slash_on_400,
   663|             backoff_on_404=False,
   664|             ignore_backoff=ignore_backoff,
   665|             retry_on_dns_fail=retry_on_dns_fail,
   666|             timeout=timeout,
   667|         )
   668|         body = await _handle_json_response(
   669|             self.reactor, self.default_timeout, request, response, start_ms
   670|         )
   671|         return body
   672|     async def delete_json(
   673|         self,
   674|         destination,
   675|         path,
   676|         long_retries=False,
   677|         timeout=None,
   678|         ignore_backoff=False,
   679|         args={},
   680|     ):
   681|         """Send a DELETE request to the remote expecting some json response
   682|         Args:
   683|             destination (str): The remote server to send the HTTP request
   684|                 to.
   685|             path (str): The HTTP path.
   686|             long_retries (bool): whether to use the long retry algorithm. See
   687|                 docs on _send_request for details.
   688|             timeout (int|None): number of milliseconds to wait for the response headers
   689|                 (including connecting to the server), *for each attempt*.
   690|                 self._default_timeout (60s) by default.
   691|             ignore_backoff (bool): true to ignore the historical backoff data and
   692|                 try the request anyway.
   693|             args (dict): query params
   694|         Returns:
   695|             dict|list: Succeeds when we get a 2xx HTTP response. The
   696|             result will be the decoded JSON body.
   697|         Raises:
   698|             HttpResponseException: If we get an HTTP response code >= 300
   699|                 (except 429).
   700|             NotRetryingDestination: If we are not yet ready to retry this
   701|                 server.
   702|             FederationDeniedError: If this destination  is not on our
   703|                 federation whitelist
   704|             RequestSendFailed: If there were problems connecting to the
   705|                 remote, due to e.g. DNS failures, connection timeouts etc.
   706|         """
   707|         request = MatrixFederationRequest(
   708|             method="DELETE", destination=destination, path=path, query=args
   709|         )
   710|         start_ms = self.clock.time_msec()
   711|         response = await self._send_request(
   712|             request,
   713|             long_retries=long_retries,
   714|             timeout=timeout,
   715|             ignore_backoff=ignore_backoff,
   716|         )
   717|         body = await _handle_json_response(
   718|             self.reactor, self.default_timeout, request, response, start_ms
   719|         )
   720|         return body
   721|     async def get_file(
   722|         self,
   723|         destination,
   724|         path,
   725|         output_stream,
   726|         args={},
   727|         retry_on_dns_fail=True,
   728|         max_size=None,
   729|         ignore_backoff=False,
   730|     ):
   731|         """GETs a file from a given homeserver
   732|         Args:
   733|             destination (str): The remote server to send the HTTP request to.
   734|             path (str): The HTTP path to GET.
   735|             output_stream (file): File to write the response body to.
   736|             args (dict): Optional dictionary used to create the query string.
   737|             ignore_backoff (bool): true to ignore the historical backoff data
   738|                 and try the request anyway.


# ====================================================================
# FILE: synapse/http/proxyagent.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 3-44 ---
     3| from zope.interface import implementer
     4| from twisted.internet import defer
     5| from twisted.internet.endpoints import HostnameEndpoint, wrapClientTLS
     6| from twisted.python.failure import Failure
     7| from twisted.web.client import URI, BrowserLikePolicyForHTTPS, _AgentBase
     8| from twisted.web.error import SchemeNotSupported
     9| from twisted.web.iweb import IAgent
    10| from synapse.http.connectproxyclient import HTTPConnectProxyEndpoint
    11| logger = logging.getLogger(__name__)
    12| _VALID_URI = re.compile(br"\A[\x21-\x7e]+\Z")
    13| @implementer(IAgent)
    14| class ProxyAgent(_AgentBase):
    15|     """An Agent implementation which will use an HTTP proxy if one was requested
    16|     Args:
    17|         reactor: twisted reactor to place outgoing
    18|             connections.
    19|         contextFactory (IPolicyForHTTPS): A factory for TLS contexts, to control the
    20|             verification parameters of OpenSSL.  The default is to use a
    21|             `BrowserLikePolicyForHTTPS`, so unless you have special
    22|             requirements you can leave this as-is.
    23|         connectTimeout (float): The amount of time that this Agent will wait
    24|             for the peer to accept a connection.
    25|         bindAddress (bytes): The local address for client sockets to bind to.
    26|         pool (HTTPConnectionPool|None): connection pool to be used. If None, a
    27|             non-persistent pool instance will be created.
    28|     """
    29|     def __init__(
    30|         self,
    31|         reactor,
    32|         contextFactory=BrowserLikePolicyForHTTPS(),
    33|         connectTimeout=None,
    34|         bindAddress=None,
    35|         pool=None,
    36|         http_proxy=None,
    37|         https_proxy=None,
    38|     ):
    39|         _AgentBase.__init__(self, reactor, pool)
    40|         self._endpoint_kwargs = {}
    41|         if connectTimeout is not None:
    42|             self._endpoint_kwargs["timeout"] = connectTimeout
    43|         if bindAddress is not None:
    44|             self._endpoint_kwargs["bindAddress"] = bindAddress

# --- HUNK 2: Lines 51-90 ---
    51|         self._policy_for_https = contextFactory
    52|         self._reactor = reactor
    53|     def request(self, method, uri, headers=None, bodyProducer=None):
    54|         """
    55|         Issue a request to the server indicated by the given uri.
    56|         Supports `http` and `https` schemes.
    57|         An existing connection from the connection pool may be used or a new one may be
    58|         created.
    59|         See also: twisted.web.iweb.IAgent.request
    60|         Args:
    61|             method (bytes): The request method to use, such as `GET`, `POST`, etc
    62|             uri (bytes): The location of the resource to request.
    63|             headers (Headers|None): Extra headers to send with the request
    64|             bodyProducer (IBodyProducer|None): An object which can generate bytes to
    65|                 make up the body of this request (for example, the properly encoded
    66|                 contents of a file for a file upload). Or, None if the request is to
    67|                 have no body.
    68|         Returns:
    69|             Deferred[IResponse]: completes when the header of the response has
    70|                  been received (regardless of the response status code).
    71|         """
    72|         uri = uri.strip()
    73|         if not _VALID_URI.match(uri):
    74|             raise ValueError("Invalid URI {!r}".format(uri))
    75|         parsed_uri = URI.fromBytes(uri)
    76|         pool_key = (parsed_uri.scheme, parsed_uri.host, parsed_uri.port)
    77|         request_path = parsed_uri.originForm
    78|         if parsed_uri.scheme == b"http" and self.http_proxy_endpoint:
    79|             pool_key = ("http-proxy", self.http_proxy_endpoint)
    80|             endpoint = self.http_proxy_endpoint
    81|             request_path = uri
    82|         elif parsed_uri.scheme == b"https" and self.https_proxy_endpoint:
    83|             endpoint = HTTPConnectProxyEndpoint(
    84|                 self._reactor,
    85|                 self.https_proxy_endpoint,
    86|                 parsed_uri.host,
    87|                 parsed_uri.port,
    88|             )
    89|         else:
    90|             endpoint = HostnameEndpoint(


# ====================================================================
# FILE: synapse/logging/context.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 10-49 ---
    10| import threading
    11| import types
    12| import warnings
    13| from typing import TYPE_CHECKING, Optional, Tuple, TypeVar, Union
    14| from typing_extensions import Literal
    15| from twisted.internet import defer, threads
    16| if TYPE_CHECKING:
    17|     from synapse.logging.scopecontextmanager import _LogContextScope
    18| logger = logging.getLogger(__name__)
    19| try:
    20|     import resource
    21|     RUSAGE_THREAD = 1
    22|     resource.getrusage(RUSAGE_THREAD)
    23|     is_thread_resource_usage_supported = True
    24|     def get_thread_resource_usage() -> "Optional[resource._RUsage]":
    25|         return resource.getrusage(RUSAGE_THREAD)
    26| except Exception:
    27|     is_thread_resource_usage_supported = False
    28|     def get_thread_resource_usage() -> "Optional[resource._RUsage]":
    29|         return None
    30| get_thread_id = threading.get_ident
    31| class ContextResourceUsage:
    32|     """Object for tracking the resources used by a log context
    33|     Attributes:
    34|         ru_utime (float): user CPU time (in seconds)
    35|         ru_stime (float): system CPU time (in seconds)
    36|         db_txn_count (int): number of database transactions done
    37|         db_sched_duration_sec (float): amount of time spent waiting for a
    38|             database connection
    39|         db_txn_duration_sec (float): amount of time spent doing database
    40|             transactions (excluding scheduling time)
    41|         evt_db_fetch_count (int): number of events requested from the database
    42|     """
    43|     __slots__ = [
    44|         "ru_stime",
    45|         "ru_utime",
    46|         "db_txn_count",
    47|         "db_txn_duration_sec",
    48|         "db_sched_duration_sec",
    49|         "evt_db_fetch_count",

# --- HUNK 2: Lines 123-165 ---
   123|         self.request = None
   124|         self.scope = None
   125|         self.tag = None
   126|     def __str__(self):
   127|         return "sentinel"
   128|     def copy_to(self, record):
   129|         pass
   130|     def copy_to_twisted_log_entry(self, record):
   131|         record["request"] = None
   132|         record["scope"] = None
   133|     def start(self, rusage: "Optional[resource._RUsage]"):
   134|         pass
   135|     def stop(self, rusage: "Optional[resource._RUsage]"):
   136|         pass
   137|     def add_database_transaction(self, duration_sec):
   138|         pass
   139|     def add_database_scheduled(self, sched_sec):
   140|         pass
   141|     def record_event_fetch(self, event_count):
   142|         pass
   143|     def __nonzero__(self):
   144|         return False
   145|     __bool__ = __nonzero__  # python3
   146| SENTINEL_CONTEXT = _Sentinel()
   147| class LoggingContext:
   148|     """Additional context for log formatting. Contexts are scoped within a
   149|     "with" block.
   150|     If a parent is given when creating a new context, then:
   151|         - logging fields are copied from the parent to the new context on entry
   152|         - when the new context exits, the cpu usage stats are copied from the
   153|           child to the parent
   154|     Args:
   155|         name (str): Name for the context for debugging.
   156|         parent_context (LoggingContext|None): The parent of the new context
   157|     """
   158|     __slots__ = [
   159|         "previous_context",
   160|         "name",
   161|         "parent_context",
   162|         "_resource_usage",
   163|         "usage_start",
   164|         "main_thread",
   165|         "finished",

# --- HUNK 3: Lines 207-316 ---
   207|     ) -> LoggingContextOrSentinel:
   208|         """Set the current logging context in thread local storage
   209|         This exists for backwards compatibility. ``set_current_context()`` should be
   210|         called directly.
   211|         Args:
   212|             context(LoggingContext): The context to activate.
   213|         Returns:
   214|             The context that was previously active
   215|         """
   216|         warnings.warn(
   217|             "synapse.logging.context.LoggingContext.set_current_context() is deprecated "
   218|             "in favor of synapse.logging.context.set_current_context().",
   219|             DeprecationWarning,
   220|             stacklevel=2,
   221|         )
   222|         return set_current_context(context)
   223|     def __enter__(self) -> "LoggingContext":
   224|         """Enters this logging context into thread local storage"""
   225|         old_context = set_current_context(self)
   226|         if self.previous_context != old_context:
   227|             logger.warning(
   228|                 "Expected previous context %r, found %r",
   229|                 self.previous_context,
   230|                 old_context,
   231|             )
   232|         return self
   233|     def __exit__(self, type, value, traceback) -> None:
   234|         """Restore the logging context in thread local storage to the state it
   235|         was before this context was entered.
   236|         Returns:
   237|             None to avoid suppressing any exceptions that were thrown.
   238|         """
   239|         current = set_current_context(self.previous_context)
   240|         if current is not self:
   241|             if current is SENTINEL_CONTEXT:
   242|                 logger.warning("Expected logging context %s was lost", self)
   243|             else:
   244|                 logger.warning(
   245|                     "Expected logging context %s but found %s", self, current
   246|                 )
   247|         self.finished = True
   248|     def copy_to(self, record) -> None:
   249|         """Copy logging fields from this context to a log record or
   250|         another LoggingContext
   251|         """
   252|         record.request = self.request
   253|         record.scope = self.scope
   254|     def copy_to_twisted_log_entry(self, record) -> None:
   255|         """
   256|         Copy logging fields from this context to a Twisted log record.
   257|         """
   258|         record["request"] = self.request
   259|         record["scope"] = self.scope
   260|     def start(self, rusage: "Optional[resource._RUsage]") -> None:
   261|         """
   262|         Record that this logcontext is currently running.
   263|         This should not be called directly: use set_current_context
   264|         Args:
   265|             rusage: the resources used by the current thread, at the point of
   266|                 switching to this logcontext. May be None if this platform doesn't
   267|                 support getrusuage.
   268|         """
   269|         if get_thread_id() != self.main_thread:
   270|             logger.warning("Started logcontext %s on different thread", self)
   271|             return
   272|         if self.finished:
   273|             logger.warning("Re-starting finished log context %s", self)
   274|         if self.usage_start:
   275|             logger.warning("Re-starting already-active log context %s", self)
   276|         else:
   277|             self.usage_start = rusage
   278|     def stop(self, rusage: "Optional[resource._RUsage]") -> None:
   279|         """
   280|         Record that this logcontext is no longer running.
   281|         This should not be called directly: use set_current_context
   282|         Args:
   283|             rusage: the resources used by the current thread, at the point of
   284|                 switching away from this logcontext. May be None if this platform
   285|                 doesn't support getrusuage.
   286|         """
   287|         try:
   288|             if get_thread_id() != self.main_thread:
   289|                 logger.warning("Stopped logcontext %s on different thread", self)
   290|                 return
   291|             if not rusage:
   292|                 return
   293|             if not self.usage_start:
   294|                 logger.warning(
   295|                     "Called stop on logcontext %s without recording a start rusage",
   296|                     self,
   297|                 )
   298|                 return
   299|             utime_delta, stime_delta = self._get_cputime(rusage)
   300|             self.add_cputime(utime_delta, stime_delta)
   301|         finally:
   302|             self.usage_start = None
   303|     def get_resource_usage(self) -> ContextResourceUsage:
   304|         """Get resources used by this logcontext so far.
   305|         Returns:
   306|             ContextResourceUsage: a *copy* of the object tracking resource
   307|                 usage so far
   308|         """
   309|         res = self._resource_usage.copy()
   310|         is_main_thread = get_thread_id() == self.main_thread
   311|         if self.usage_start and is_main_thread:
   312|             rusage = get_thread_resource_usage()
   313|             assert rusage is not None
   314|             utime_delta, stime_delta = self._get_cputime(rusage)
   315|             res.ru_utime += utime_delta
   316|             res.ru_stime += stime_delta

# --- HUNK 4: Lines 395-442 ---
   395|         context = current_context()
   396|         for key, value in self.defaults.items():
   397|             setattr(record, key, value)
   398|         if context is not None:
   399|             context.copy_to(record)
   400|         return True
   401| class PreserveLoggingContext:
   402|     """Context manager which replaces the logging context
   403|      The previous logging context is restored on exit."""
   404|     __slots__ = ["_old_context", "_new_context"]
   405|     def __init__(
   406|         self, new_context: LoggingContextOrSentinel = SENTINEL_CONTEXT
   407|     ) -> None:
   408|         self._new_context = new_context
   409|     def __enter__(self) -> None:
   410|         self._old_context = set_current_context(self._new_context)
   411|     def __exit__(self, type, value, traceback) -> None:
   412|         context = set_current_context(self._old_context)
   413|         if context != self._new_context:
   414|             if not context:
   415|                 logger.warning(
   416|                     "Expected logging context %s was lost", self._new_context
   417|                 )
   418|             else:
   419|                 logger.warning(
   420|                     "Expected logging context %s but found %s",
   421|                     self._new_context,
   422|                     context,
   423|                 )
   424| _thread_local = threading.local()
   425| _thread_local.current_context = SENTINEL_CONTEXT
   426| def current_context() -> LoggingContextOrSentinel:
   427|     """Get the current logging context from thread local storage"""
   428|     return getattr(_thread_local, "current_context", SENTINEL_CONTEXT)
   429| def set_current_context(context: LoggingContextOrSentinel) -> LoggingContextOrSentinel:
   430|     """Set the current logging context in thread local storage
   431|     Args:
   432|         context(LoggingContext): The context to activate.
   433|     Returns:
   434|         The context that was previously active
   435|     """
   436|     if context is None:
   437|         raise TypeError("'context' argument may not be None")
   438|     current = current_context()
   439|     if current is not context:
   440|         rusage = get_thread_resource_usage()
   441|         current.stop(rusage)
   442|         _thread_local.current_context = context


# ====================================================================
# FILE: synapse/logging/formatter.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| import logging
     2| import traceback
     3| from io import StringIO
     4| class LogFormatter(logging.Formatter):
     5|     """Log formatter which gives more detail for exceptions
     6|     This is the same as the standard log formatter, except that when logging
     7|     exceptions [typically via log.foo("msg", exc_info=1)], it prints the
     8|     sequence that led up to the point at which the exception was caught.
     9|     (Normally only stack frames between the point the exception was raised and
    10|     where it was caught are logged).
    11|     """
    12|     def __init__(self, *args, **kwargs):
    13|         super(LogFormatter, self).__init__(*args, **kwargs)
    14|     def formatException(self, ei):
    15|         sio = StringIO()
    16|         (typ, val, tb) = ei
    17|         if tb and hasattr(tb.tb_frame, "f_back"):
    18|             sio.write("Capture point (most recent call last):\n")
    19|             traceback.print_stack(tb.tb_frame.f_back, None, sio)
    20|         traceback.print_exception(typ, val, tb, None, sio)
    21|         s = sio.getvalue()
    22|         sio.close()
    23|         if s[-1:] == "\n":
    24|             s = s[:-1]
    25|         return s


# ====================================================================
# FILE: synapse/logging/scopecontextmanager.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 55-97 ---
    55|     A custom opentracing scope. The only significant difference is that it will
    56|     close the log context it's related to if the logcontext was created specifically
    57|     for this scope.
    58|     """
    59|     def __init__(self, manager, span, logcontext, enter_logcontext, finish_on_close):
    60|         """
    61|         Args:
    62|             manager (LogContextScopeManager):
    63|                 the manager that is responsible for this scope.
    64|             span (Span):
    65|                 the opentracing span which this scope represents the local
    66|                 lifetime for.
    67|             logcontext (LogContext):
    68|                 the logcontext to which this scope is attached.
    69|             enter_logcontext (Boolean):
    70|                 if True the logcontext will be entered and exited when the scope
    71|                 is entered and exited respectively
    72|             finish_on_close (Boolean):
    73|                 if True finish the span when the scope is closed
    74|         """
    75|         super(_LogContextScope, self).__init__(manager, span)
    76|         self.logcontext = logcontext
    77|         self._finish_on_close = finish_on_close
    78|         self._enter_logcontext = enter_logcontext
    79|     def __enter__(self):
    80|         if self._enter_logcontext:
    81|             self.logcontext.__enter__()
    82|         return self
    83|     def __exit__(self, type, value, traceback):
    84|         if type == twisted.internet.defer._DefGen_Return:
    85|             super(_LogContextScope, self).__exit__(None, None, None)
    86|         else:
    87|             super(_LogContextScope, self).__exit__(type, value, traceback)
    88|         if self._enter_logcontext:
    89|             self.logcontext.__exit__(type, value, traceback)
    90|         else:  # the logcontext existed before the creation of the scope
    91|             self.logcontext.scope = None
    92|     def close(self):
    93|         if self.manager.active is not self:
    94|             logger.error("Tried to close a non-active scope!")
    95|             return
    96|         if self._finish_on_close:
    97|             self.span.finish()


# ====================================================================
# FILE: synapse/logging/utils.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-35 ---
     1| import logging
     2| from functools import wraps
     3| from inspect import getcallargs
     4| _TIME_FUNC_ID = 0
     5| def _log_debug_as_f(f, msg, msg_args):
     6|     name = f.__module__
     7|     logger = logging.getLogger(name)
     8|     if logger.isEnabledFor(logging.DEBUG):
     9|         lineno = f.__code__.co_firstlineno
    10|         pathname = f.__code__.co_filename
    11|         record = logging.LogRecord(
    12|             name=name,
    13|             level=logging.DEBUG,
    14|             pathname=pathname,
    15|             lineno=lineno,
    16|             msg=msg,
    17|             args=msg_args,
    18|             exc_info=None,
    19|         )
    20|         logger.handle(record)
    21| def log_function(f):
    22|     """ Function decorator that logs every call to that function.
    23|     """
    24|     func_name = f.__name__
    25|     @wraps(f)
    26|     def wrapped(*args, **kwargs):
    27|         name = f.__module__
    28|         logger = logging.getLogger(name)
    29|         level = logging.DEBUG
    30|         if logger.isEnabledFor(level):
    31|             bound_args = getcallargs(f, *args, **kwargs)
    32|             def format(value):
    33|                 r = str(value)
    34|                 if len(r) > 50:
    35|                     r = r[:50] + "..."


# ====================================================================
# FILE: synapse/metrics/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-56 ---
     1| import functools
     2| import gc
     3| import logging
     4| import os
     5| import platform
     6| import threading
     7| import time
     8| from typing import Callable, Dict, Iterable, Optional, Tuple, Union
     9| import attr
    10| from prometheus_client import Counter, Gauge, Histogram
    11| from prometheus_client.core import (
    12|     REGISTRY,
    13|     CounterMetricFamily,
    14|     GaugeMetricFamily,
    15|     HistogramMetricFamily,
    16| )
    17| from twisted.internet import reactor
    18| import synapse
    19| from synapse.metrics._exposition import (
    20|     MetricsResource,
    21|     generate_latest,
    22|     start_http_server,
    23| )
    24| from synapse.util.versionstring import get_version_string
    25| logger = logging.getLogger(__name__)
    26| METRICS_PREFIX = "/_synapse/metrics"
    27| running_on_pypy = platform.python_implementation() == "PyPy"
    28| all_gauges = {}  # type: Dict[str, Union[LaterGauge, InFlightGauge, BucketCollector]]
    29| HAVE_PROC_SELF_STAT = os.path.exists("/proc/self/stat")
    30| class RegistryProxy:
    31|     @staticmethod
    32|     def collect():
    33|         for metric in REGISTRY.collect():
    34|             if not metric.name.startswith("__"):
    35|                 yield metric
    36| @attr.s(hash=True)
    37| class LaterGauge:
    38|     name = attr.ib(type=str)
    39|     desc = attr.ib(type=str)
    40|     labels = attr.ib(hash=False, type=Optional[Iterable[str]])
    41|     caller = attr.ib(type=Callable[[], Union[Dict[Tuple[str, ...], float], float]])
    42|     def collect(self):
    43|         g = GaugeMetricFamily(self.name, self.desc, labels=self.labels)
    44|         try:
    45|             calls = self.caller()
    46|         except Exception:
    47|             logger.exception("Exception running callback for LaterGauge(%s)", self.name)
    48|             yield g
    49|             return
    50|         if isinstance(calls, dict):
    51|             for k, v in calls.items():
    52|                 g.add_metric(k, v)
    53|         else:
    54|             g.add_metric([], calls)
    55|         yield g
    56|     def __attrs_post_init__(self):

# --- HUNK 2: Lines 118-199 ---
   118|                 callbacks = set(self._registrations[key])
   119|             in_flight.add_metric(key, len(callbacks))
   120|             metrics = self._metrics_class()
   121|             metrics_by_key[key] = metrics
   122|             for callback in callbacks:
   123|                 callback(metrics)
   124|         yield in_flight
   125|         for name in self.sub_metrics:
   126|             gauge = GaugeMetricFamily(
   127|                 "_".join([self.name, name]), "", labels=self.labels
   128|             )
   129|             for key, metrics in metrics_by_key.items():
   130|                 gauge.add_metric(key, getattr(metrics, name))
   131|             yield gauge
   132|     def _register_with_collector(self):
   133|         if self.name in all_gauges.keys():
   134|             logger.warning("%s already registered, reregistering" % (self.name,))
   135|             REGISTRY.unregister(all_gauges.pop(self.name))
   136|         REGISTRY.register(self)
   137|         all_gauges[self.name] = self
   138| @attr.s(hash=True)
   139| class BucketCollector:
   140|     """
   141|     Like a Histogram, but allows buckets to be point-in-time instead of
   142|     incrementally added to.
   143|     Args:
   144|         name (str): Base name of metric to be exported to Prometheus.
   145|         data_collector (callable -> dict): A synchronous callable that
   146|             returns a dict mapping bucket to number of items in the
   147|             bucket. If these buckets are not the same as the buckets
   148|             given to this class, they will be remapped into them.
   149|         buckets (list[float]): List of floats/ints of the buckets to
   150|             give to Prometheus. +Inf is ignored, if given.
   151|     """
   152|     name = attr.ib()
   153|     data_collector = attr.ib()
   154|     buckets = attr.ib()
   155|     def collect(self):
   156|         data = self.data_collector()
   157|         buckets = {}  # type: Dict[float, int]
   158|         res = []
   159|         for x in data.keys():
   160|             for i, bound in enumerate(self.buckets):
   161|                 if x <= bound:
   162|                     buckets[bound] = buckets.get(bound, 0) + data[x]
   163|         for i in self.buckets:
   164|             res.append([str(i), buckets.get(i, 0)])
   165|         res.append(["+Inf", sum(data.values())])
   166|         metric = HistogramMetricFamily(
   167|             self.name, "", buckets=res, sum_value=sum(x * y for x, y in data.items())
   168|         )
   169|         yield metric
   170|     def __attrs_post_init__(self):
   171|         self.buckets = [float(x) for x in self.buckets if x != "+Inf"]
   172|         if self.buckets != sorted(self.buckets):
   173|             raise ValueError("Buckets not sorted")
   174|         self.buckets = tuple(self.buckets)
   175|         if self.name in all_gauges.keys():
   176|             logger.warning("%s already registered, reregistering" % (self.name,))
   177|             REGISTRY.unregister(all_gauges.pop(self.name))
   178|         REGISTRY.register(self)
   179|         all_gauges[self.name] = self
   180| class CPUMetrics:
   181|     def __init__(self):
   182|         ticks_per_sec = 100
   183|         try:
   184|             ticks_per_sec = os.sysconf("SC_CLK_TCK")
   185|         except (ValueError, TypeError, AttributeError):
   186|             pass
   187|         self.ticks_per_sec = ticks_per_sec
   188|     def collect(self):
   189|         if not HAVE_PROC_SELF_STAT:
   190|             return
   191|         with open("/proc/self/stat") as s:
   192|             line = s.read()
   193|             raw_stats = line.split(") ", 1)[1].split(" ")
   194|             user = GaugeMetricFamily("process_cpu_user_seconds_total", "")
   195|             user.add_metric([], float(raw_stats[11]) / self.ticks_per_sec)
   196|             yield user
   197|             sys = GaugeMetricFamily("process_cpu_system_seconds_total", "")
   198|             sys.add_metric([], float(raw_stats[12]) / self.ticks_per_sec)
   199|             yield sys


# ====================================================================
# FILE: synapse/metrics/_exposition.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-148 ---
     1| """
     2| This code is based off `prometheus_client/exposition.py` from version 0.7.1.
     3| Due to the renaming of metrics in prometheus_client 0.4.0, this customised
     4| vendoring of the code will emit both the old versions that Synapse dashboards
     5| expect, and the newer "best practice" version of the up-to-date official client.
     6| """
     7| import math
     8| import threading
     9| from collections import namedtuple
    10| from http.server import BaseHTTPRequestHandler, HTTPServer
    11| from socketserver import ThreadingMixIn
    12| from urllib.parse import parse_qs, urlparse
    13| from prometheus_client import REGISTRY
    14| from twisted.web.resource import Resource
    15| from synapse.util import caches
    16| try:
    17|     from prometheus_client.samples import Sample
    18| except ImportError:
    19|     Sample = namedtuple(  # type: ignore[no-redef] # noqa
    20|         "Sample", ["name", "labels", "value", "timestamp", "exemplar"]
    21|     )
    22| CONTENT_TYPE_LATEST = str("text/plain; version=0.0.4; charset=utf-8")
    23| INF = float("inf")
    24| MINUS_INF = float("-inf")
    25| def floatToGoString(d):
    26|     d = float(d)
    27|     if d == INF:
    28|         return "+Inf"
    29|     elif d == MINUS_INF:
    30|         return "-Inf"
    31|     elif math.isnan(d):
    32|         return "NaN"
    33|     else:
    34|         s = repr(d)
    35|         dot = s.find(".")
    36|         if d > 0 and dot > 6:
    37|             mantissa = "{0}.{1}{2}".format(s[0], s[1:dot], s[dot + 1 :]).rstrip("0.")
    38|             return "{0}e+0{1}".format(mantissa, dot - 1)
    39|         return s
    40| def sample_line(line, name):
    41|     if line.labels:
    42|         labelstr = "{{{0}}}".format(
    43|             ",".join(
    44|                 [
    45|                     '{0}="{1}"'.format(
    46|                         k,
    47|                         v.replace("\\", r"\\").replace("\n", r"\n").replace('"', r"\""),
    48|                     )
    49|                     for k, v in sorted(line.labels.items())
    50|                 ]
    51|             )
    52|         )
    53|     else:
    54|         labelstr = ""
    55|     timestamp = ""
    56|     if line.timestamp is not None:
    57|         timestamp = " {0:d}".format(int(float(line.timestamp) * 1000))
    58|     return "{0}{1} {2}{3}\n".format(
    59|         name, labelstr, floatToGoString(line.value), timestamp
    60|     )
    61| def nameify_sample(sample):
    62|     """
    63|     If we get a prometheus_client<0.4.0 sample as a tuple, transform it into a
    64|     namedtuple which has the names we expect.
    65|     """
    66|     if not isinstance(sample, Sample):
    67|         sample = Sample(*sample, None, None)
    68|     return sample
    69| def generate_latest(registry, emit_help=False):
    70|     for collector in caches.collectors_by_name.values():
    71|         collector.collect()
    72|     output = []
    73|     for metric in registry.collect():
    74|         if not metric.samples:
    75|             continue
    76|         mname = metric.name
    77|         mnewname = metric.name
    78|         mtype = metric.type
    79|         if mtype == "counter":
    80|             mnewname = mnewname + "_total"
    81|         elif mtype == "info":
    82|             mtype = "gauge"
    83|             mnewname = mnewname + "_info"
    84|         elif mtype == "stateset":
    85|             mtype = "gauge"
    86|         elif mtype == "gaugehistogram":
    87|             mtype = "histogram"
    88|         elif mtype == "unknown":
    89|             mtype = "untyped"
    90|         if emit_help:
    91|             output.append(
    92|                 "# HELP {0} {1}\n".format(
    93|                     mname,
    94|                     metric.documentation.replace("\\", r"\\").replace("\n", r"\n"),
    95|                 )
    96|             )
    97|         output.append("# TYPE {0} {1}\n".format(mname, mtype))
    98|         for sample in map(nameify_sample, metric.samples):
    99|             for suffix in ["_created", "_gsum", "_gcount"]:
   100|                 if sample.name.endswith(suffix):
   101|                     break
   102|             else:
   103|                 newname = sample.name.replace(mnewname, mname)
   104|                 if ":" in newname and newname.endswith("_total"):
   105|                     newname = newname[: -len("_total")]
   106|                 output.append(sample_line(sample, newname))
   107|         if mtype == "counter":
   108|             mnewname = mnewname.replace(":total", "")
   109|         mnewname = mnewname.replace(":", "_")
   110|         if mname == mnewname:
   111|             continue
   112|         if emit_help:
   113|             output.append(
   114|                 "# HELP {0} {1}\n".format(
   115|                     mnewname,
   116|                     metric.documentation.replace("\\", r"\\").replace("\n", r"\n"),
   117|                 )
   118|             )
   119|         output.append("# TYPE {0} {1}\n".format(mnewname, mtype))
   120|         for sample in map(nameify_sample, metric.samples):
   121|             for suffix in ["_created", "_gsum", "_gcount"]:
   122|                 if sample.name.endswith(suffix):
   123|                     break
   124|             else:
   125|                 output.append(
   126|                     sample_line(
   127|                         sample, sample.name.replace(":total", "").replace(":", "_")
   128|                     )
   129|                 )
   130|     return "".join(output).encode("utf-8")
   131| class MetricsHandler(BaseHTTPRequestHandler):
   132|     """HTTP handler that gives metrics from ``REGISTRY``."""
   133|     registry = REGISTRY
   134|     def do_GET(self):
   135|         registry = self.registry
   136|         params = parse_qs(urlparse(self.path).query)
   137|         if "help" in params:
   138|             emit_help = True
   139|         else:
   140|             emit_help = False
   141|         try:
   142|             output = generate_latest(registry, emit_help=emit_help)
   143|         except Exception:
   144|             self.send_error(500, "error generating metric output")
   145|             raise
   146|         self.send_response(200)
   147|         self.send_header("Content-Type", CONTENT_TYPE_LATEST)
   148|         self.send_header("Content-Length", str(len(output)))


# ====================================================================
# FILE: synapse/notifier.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 7-47 ---
     7|     Iterable,
     8|     List,
     9|     Optional,
    10|     Set,
    11|     Tuple,
    12|     TypeVar,
    13|     Union,
    14| )
    15| from prometheus_client import Counter
    16| from twisted.internet import defer
    17| import synapse.server
    18| from synapse.api.constants import EventTypes, Membership
    19| from synapse.api.errors import AuthError
    20| from synapse.events import EventBase
    21| from synapse.handlers.presence import format_user_presence_state
    22| from synapse.logging.context import PreserveLoggingContext
    23| from synapse.logging.utils import log_function
    24| from synapse.metrics import LaterGauge
    25| from synapse.metrics.background_process_metrics import run_as_background_process
    26| from synapse.streams.config import PaginationConfig
    27| from synapse.types import Collection, StreamToken, UserID
    28| from synapse.util.async_helpers import ObservableDeferred, timeout_deferred
    29| from synapse.util.metrics import Measure
    30| from synapse.visibility import filter_events_for_client
    31| logger = logging.getLogger(__name__)
    32| notified_events_counter = Counter("synapse_notifier_notified_events", "")
    33| users_woken_by_stream_counter = Counter(
    34|     "synapse_notifier_users_woken_by_stream", "", ["stream"]
    35| )
    36| T = TypeVar("T")
    37| def count(func: Callable[[T], bool], it: Iterable[T]) -> int:
    38|     """Return the number of items in it for which func returns true."""
    39|     n = 0
    40|     for x in it:
    41|         if func(x):
    42|             n += 1
    43|     return n
    44| class _NotificationListener:
    45|     """ This represents a single client connection to the events stream.
    46|     The events stream handler will have yielded to the deferred, so to
    47|     notify the handler it is sufficient to resolve the deferred.

# --- HUNK 2: Lines 54-247 ---
    54|     It tracks the most recent stream token for that user.
    55|     At a given point a user may have a number of streams listening for
    56|     events.
    57|     This listener will also keep track of which rooms it is listening in
    58|     so that it can remove itself from the indexes in the Notifier class.
    59|     """
    60|     def __init__(
    61|         self,
    62|         user_id: str,
    63|         rooms: Collection[str],
    64|         current_token: StreamToken,
    65|         time_now_ms: int,
    66|     ):
    67|         self.user_id = user_id
    68|         self.rooms = set(rooms)
    69|         self.current_token = current_token
    70|         self.last_notified_token = current_token
    71|         self.last_notified_ms = time_now_ms
    72|         with PreserveLoggingContext():
    73|             self.notify_deferred = ObservableDeferred(defer.Deferred())
    74|     def notify(self, stream_key: str, stream_id: int, time_now_ms: int):
    75|         """Notify any listeners for this user of a new event from an
    76|         event source.
    77|         Args:
    78|             stream_key: The stream the event came from.
    79|             stream_id: The new id for the stream the event came from.
    80|             time_now_ms: The current time in milliseconds.
    81|         """
    82|         self.current_token = self.current_token.copy_and_advance(stream_key, stream_id)
    83|         self.last_notified_token = self.current_token
    84|         self.last_notified_ms = time_now_ms
    85|         noify_deferred = self.notify_deferred
    86|         users_woken_by_stream_counter.labels(stream_key).inc()
    87|         with PreserveLoggingContext():
    88|             self.notify_deferred = ObservableDeferred(defer.Deferred())
    89|             noify_deferred.callback(self.current_token)
    90|     def remove(self, notifier: "Notifier"):
    91|         """ Remove this listener from all the indexes in the Notifier
    92|         it knows about.
    93|         """
    94|         for room in self.rooms:
    95|             lst = notifier.room_to_user_streams.get(room, set())
    96|             lst.discard(self)
    97|         notifier.user_to_user_stream.pop(self.user_id)
    98|     def count_listeners(self) -> int:
    99|         return len(self.notify_deferred.observers())
   100|     def new_listener(self, token: StreamToken) -> _NotificationListener:
   101|         """Returns a deferred that is resolved when there is a new token
   102|         greater than the given token.
   103|         Args:
   104|             token: The token from which we are streaming from, i.e. we shouldn't
   105|                 notify for things that happened before this.
   106|         """
   107|         if self.last_notified_token.is_after(token):
   108|             return _NotificationListener(defer.succeed(self.current_token))
   109|         else:
   110|             return _NotificationListener(self.notify_deferred.observe())
   111| class EventStreamResult(namedtuple("EventStreamResult", ("events", "tokens"))):
   112|     def __nonzero__(self):
   113|         return bool(self.events)
   114|     __bool__ = __nonzero__  # python3
   115| class Notifier:
   116|     """ This class is responsible for notifying any listeners when there are
   117|     new events available for it.
   118|     Primarily used from the /events stream.
   119|     """
   120|     UNUSED_STREAM_EXPIRY_MS = 10 * 60 * 1000
   121|     def __init__(self, hs: "synapse.server.HomeServer"):
   122|         self.user_to_user_stream = {}  # type: Dict[str, _NotifierUserStream]
   123|         self.room_to_user_streams = {}  # type: Dict[str, Set[_NotifierUserStream]]
   124|         self.hs = hs
   125|         self.storage = hs.get_storage()
   126|         self.event_sources = hs.get_event_sources()
   127|         self.store = hs.get_datastore()
   128|         self.pending_new_room_events = (
   129|             []
   130|         )  # type: List[Tuple[int, EventBase, Collection[Union[str, UserID]]]]
   131|         self.replication_callbacks = []  # type: List[Callable[[], None]]
   132|         self.remote_server_up_callbacks = []  # type: List[Callable[[str], None]]
   133|         self.clock = hs.get_clock()
   134|         self.appservice_handler = hs.get_application_service_handler()
   135|         self.federation_sender = None
   136|         if hs.should_send_federation():
   137|             self.federation_sender = hs.get_federation_sender()
   138|         self.state_handler = hs.get_state_handler()
   139|         self.clock.looping_call(
   140|             self.remove_expired_streams, self.UNUSED_STREAM_EXPIRY_MS
   141|         )
   142|         def count_listeners():
   143|             all_user_streams = set()  # type: Set[_NotifierUserStream]
   144|             for streams in list(self.room_to_user_streams.values()):
   145|                 all_user_streams |= streams
   146|             for stream in list(self.user_to_user_stream.values()):
   147|                 all_user_streams.add(stream)
   148|             return sum(stream.count_listeners() for stream in all_user_streams)
   149|         LaterGauge("synapse_notifier_listeners", "", [], count_listeners)
   150|         LaterGauge(
   151|             "synapse_notifier_rooms",
   152|             "",
   153|             [],
   154|             lambda: count(bool, list(self.room_to_user_streams.values())),
   155|         )
   156|         LaterGauge(
   157|             "synapse_notifier_users", "", [], lambda: len(self.user_to_user_stream)
   158|         )
   159|     def add_replication_callback(self, cb: Callable[[], None]):
   160|         """Add a callback that will be called when some new data is available.
   161|         Callback is not given any arguments. It should *not* return a Deferred - if
   162|         it needs to do any asynchronous work, a background thread should be started and
   163|         wrapped with run_as_background_process.
   164|         """
   165|         self.replication_callbacks.append(cb)
   166|     def on_new_room_event(
   167|         self,
   168|         event: EventBase,
   169|         room_stream_id: int,
   170|         max_room_stream_id: int,
   171|         extra_users: Collection[Union[str, UserID]] = [],
   172|     ):
   173|         """ Used by handlers to inform the notifier something has happened
   174|         in the room, room event wise.
   175|         This triggers the notifier to wake up any listeners that are
   176|         listening to the room, and any listeners for the users in the
   177|         `extra_users` param.
   178|         The events can be peristed out of order. The notifier will wait
   179|         until all previous events have been persisted before notifying
   180|         the client streams.
   181|         """
   182|         self.pending_new_room_events.append((room_stream_id, event, extra_users))
   183|         self._notify_pending_new_room_events(max_room_stream_id)
   184|         self.notify_replication()
   185|     def _notify_pending_new_room_events(self, max_room_stream_id: int):
   186|         """Notify for the room events that were queued waiting for a previous
   187|         event to be persisted.
   188|         Args:
   189|             max_room_stream_id: The highest stream_id below which all
   190|                 events have been persisted.
   191|         """
   192|         pending = self.pending_new_room_events
   193|         self.pending_new_room_events = []
   194|         for room_stream_id, event, extra_users in pending:
   195|             if room_stream_id > max_room_stream_id:
   196|                 self.pending_new_room_events.append(
   197|                     (room_stream_id, event, extra_users)
   198|                 )
   199|             else:
   200|                 self._on_new_room_event(event, room_stream_id, extra_users)
   201|     def _on_new_room_event(
   202|         self,
   203|         event: EventBase,
   204|         room_stream_id: int,
   205|         extra_users: Collection[Union[str, UserID]] = [],
   206|     ):
   207|         """Notify any user streams that are interested in this room event"""
   208|         run_as_background_process(
   209|             "notify_app_services", self._notify_app_services, room_stream_id
   210|         )
   211|         if self.federation_sender:
   212|             self.federation_sender.notify_new_events(room_stream_id)
   213|         if event.type == EventTypes.Member and event.membership == Membership.JOIN:
   214|             self._user_joined_room(event.state_key, event.room_id)
   215|         self.on_new_event(
   216|             "room_key", room_stream_id, users=extra_users, rooms=[event.room_id]
   217|         )
   218|     async def _notify_app_services(self, room_stream_id: int):
   219|         try:
   220|             await self.appservice_handler.notify_interested_services(room_stream_id)
   221|         except Exception:
   222|             logger.exception("Error notifying application services of event")
   223|     def on_new_event(
   224|         self,
   225|         stream_key: str,
   226|         new_token: int,
   227|         users: Collection[Union[str, UserID]] = [],
   228|         rooms: Collection[str] = [],
   229|     ):
   230|         """ Used to inform listeners that something has happened event wise.
   231|         Will wake up all listeners for the given users and rooms.
   232|         """
   233|         with PreserveLoggingContext():
   234|             with Measure(self.clock, "on_new_event"):
   235|                 user_streams = set()
   236|                 for user in users:
   237|                     user_stream = self.user_to_user_stream.get(str(user))
   238|                     if user_stream is not None:
   239|                         user_streams.add(user_stream)
   240|                 for room in rooms:
   241|                     user_streams |= self.room_to_user_streams.get(room, set())
   242|                 time_now_ms = self.clock.time_msec()
   243|                 for user_stream in user_streams:
   244|                     try:
   245|                         user_stream.notify(stream_key, new_token, time_now_ms)
   246|                     except Exception:
   247|                         logger.exception("Failed to notify listener")

# --- HUNK 3: Lines 302-351 ---
   302|         if result is None:
   303|             current_token = user_stream.current_token
   304|             result = await callback(prev_token, current_token)
   305|         return result
   306|     async def get_events_for(
   307|         self,
   308|         user: UserID,
   309|         pagination_config: PaginationConfig,
   310|         timeout: int,
   311|         is_guest: bool = False,
   312|         explicit_room_id: str = None,
   313|     ) -> EventStreamResult:
   314|         """ For the given user and rooms, return any new events for them. If
   315|         there are no new events wait for up to `timeout` milliseconds for any
   316|         new events to happen before returning.
   317|         If explicit_room_id is not set, the user's joined rooms will be polled
   318|         for events.
   319|         If explicit_room_id is set, that room will be polled for events only if
   320|         it is world readable or the user has joined the room.
   321|         """
   322|         from_token = pagination_config.from_token
   323|         if not from_token:
   324|             from_token = self.event_sources.get_current_token()
   325|         limit = pagination_config.limit
   326|         room_ids, is_joined = await self._get_room_ids(user, explicit_room_id)
   327|         is_peeking = not is_joined
   328|         async def check_for_updates(
   329|             before_token: StreamToken, after_token: StreamToken
   330|         ) -> EventStreamResult:
   331|             if not after_token.is_after(before_token):
   332|                 return EventStreamResult([], (from_token, from_token))
   333|             events = []  # type: List[EventBase]
   334|             end_token = from_token
   335|             for name, source in self.event_sources.sources.items():
   336|                 keyname = "%s_key" % name
   337|                 before_id = getattr(before_token, keyname)
   338|                 after_id = getattr(after_token, keyname)
   339|                 if before_id == after_id:
   340|                     continue
   341|                 new_events, new_key = await source.get_new_events(
   342|                     user=user,
   343|                     from_key=getattr(from_token, keyname),
   344|                     limit=limit,
   345|                     is_guest=is_peeking,
   346|                     room_ids=room_ids,
   347|                     explicit_room_id=explicit_room_id,
   348|                 )
   349|                 if name == "room":
   350|                     new_events = await filter_events_for_client(
   351|                         self.storage,


# ====================================================================
# FILE: synapse/push/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-3 ---
     1| class PusherConfigException(Exception):
     2|     def __init__(self, msg):
     3|         super(PusherConfigException, self).__init__(msg)


# ====================================================================
# FILE: synapse/push/emailpusher.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 28-68 ---
    28|         self.timed_call = None
    29|         self.throttle_params = None
    30|         self.max_stream_ordering = None
    31|         self._is_processing = False
    32|     def on_started(self, should_check_for_notifs):
    33|         """Called when this pusher has been started.
    34|         Args:
    35|             should_check_for_notifs (bool): Whether we should immediately
    36|                 check for push to send. Set to False only if it's known there
    37|                 is nothing to send
    38|         """
    39|         if should_check_for_notifs and self.mailer is not None:
    40|             self._start_processing()
    41|     def on_stop(self):
    42|         if self.timed_call:
    43|             try:
    44|                 self.timed_call.cancel()
    45|             except (AlreadyCalled, AlreadyCancelled):
    46|                 pass
    47|             self.timed_call = None
    48|     def on_new_notifications(self, min_stream_ordering, max_stream_ordering):
    49|         if self.max_stream_ordering:
    50|             self.max_stream_ordering = max(
    51|                 max_stream_ordering, self.max_stream_ordering
    52|             )
    53|         else:
    54|             self.max_stream_ordering = max_stream_ordering
    55|         self._start_processing()
    56|     def on_new_receipts(self, min_stream_id, max_stream_id):
    57|         pass
    58|     def on_timer(self):
    59|         self.timed_call = None
    60|         self._start_processing()
    61|     def _start_processing(self):
    62|         if self._is_processing:
    63|             return
    64|         run_as_background_process("emailpush.process", self._process)
    65|     def _pause_processing(self):
    66|         """Used by tests to temporarily pause processing of events.
    67|         Asserts that its not currently processing.
    68|         """


# ====================================================================
# FILE: synapse/push/httppusher.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 55-95 ---
    55|             pusherdict["pushkey"],
    56|         )
    57|         if self.data is None:
    58|             raise PusherConfigException("data can not be null for HTTP pusher")
    59|         if "url" not in self.data:
    60|             raise PusherConfigException("'url' required in data for HTTP pusher")
    61|         self.url = self.data["url"]
    62|         self.http_client = hs.get_proxied_http_client()
    63|         self.data_minus_url = {}
    64|         self.data_minus_url.update(self.data)
    65|         del self.data_minus_url["url"]
    66|     def on_started(self, should_check_for_notifs):
    67|         """Called when this pusher has been started.
    68|         Args:
    69|             should_check_for_notifs (bool): Whether we should immediately
    70|                 check for push to send. Set to False only if it's known there
    71|                 is nothing to send
    72|         """
    73|         if should_check_for_notifs:
    74|             self._start_processing()
    75|     def on_new_notifications(self, min_stream_ordering, max_stream_ordering):
    76|         self.max_stream_ordering = max(
    77|             max_stream_ordering, self.max_stream_ordering or 0
    78|         )
    79|         self._start_processing()
    80|     def on_new_receipts(self, min_stream_id, max_stream_id):
    81|         run_as_background_process("http_pusher.on_new_receipts", self._update_badge)
    82|     async def _update_badge(self):
    83|         badge = await push_tools.get_badge_count(self.hs.get_datastore(), self.user_id)
    84|         await self._send_badge(badge)
    85|     def on_timer(self):
    86|         self._start_processing()
    87|     def on_stop(self):
    88|         if self.timed_call:
    89|             try:
    90|                 self.timed_call.cancel()
    91|             except (AlreadyCalled, AlreadyCancelled):
    92|                 pass
    93|             self.timed_call = None
    94|     def _start_processing(self):
    95|         if self._is_processing:


# ====================================================================
# FILE: synapse/push/mailer.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 71-111 ---
    71|         self.macaroon_gen = self.hs.get_macaroon_generator()
    72|         self.state_handler = self.hs.get_state_handler()
    73|         self.storage = hs.get_storage()
    74|         self.app_name = app_name
    75|         self.email_subjects = hs.config.email_subjects  # type: EmailSubjectConfig
    76|         logger.info("Created Mailer for app_name %s" % app_name)
    77|     async def send_password_reset_mail(self, email_address, token, client_secret, sid):
    78|         """Send an email with a password reset link to a user
    79|         Args:
    80|             email_address (str): Email address we're sending the password
    81|                 reset to
    82|             token (str): Unique token generated by the server to verify
    83|                 the email was received
    84|             client_secret (str): Unique token generated by the client to
    85|                 group together multiple email sending attempts
    86|             sid (str): The generated session ID
    87|         """
    88|         params = {"token": token, "client_secret": client_secret, "sid": sid}
    89|         link = (
    90|             self.hs.config.public_baseurl
    91|             + "_matrix/client/unstable/password_reset/email/submit_token?%s"
    92|             % urllib.parse.urlencode(params)
    93|         )
    94|         template_vars = {"link": link}
    95|         await self.send_email(
    96|             email_address,
    97|             self.email_subjects.password_reset
    98|             % {"server_name": self.hs.config.server_name},
    99|             template_vars,
   100|         )
   101|     async def send_registration_mail(self, email_address, token, client_secret, sid):
   102|         """Send an email with a registration confirmation link to a user
   103|         Args:
   104|             email_address (str): Email address we're sending the registration
   105|                 link to
   106|             token (str): Unique token generated by the server to verify
   107|                 the email was received
   108|             client_secret (str): Unique token generated by the client to
   109|                 group together multiple email sending attempts
   110|             sid (str): The generated session ID
   111|         """


# ====================================================================
# FILE: synapse/push/pusherpool.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 14-55 ---
    14|     "synapse_pushers", "Number of active synapse pushers", ["kind", "app_id"]
    15| )
    16| class PusherPool:
    17|     """
    18|     The pusher pool. This is responsible for dispatching notifications of new events to
    19|     the http and email pushers.
    20|     It provides three methods which are designed to be called by the rest of the
    21|     application: `start`, `on_new_notifications`, and `on_new_receipts`: each of these
    22|     delegates to each of the relevant pushers.
    23|     Note that it is expected that each pusher will have its own 'processing' loop which
    24|     will send out the notifications in the background, rather than blocking until the
    25|     notifications are sent; accordingly Pusher.on_started, Pusher.on_new_notifications and
    26|     Pusher.on_new_receipts are not expected to return awaitables.
    27|     """
    28|     def __init__(self, hs: "HomeServer"):
    29|         self.hs = hs
    30|         self.pusher_factory = PusherFactory(hs)
    31|         self._should_start_pushers = hs.config.start_pushers
    32|         self.store = self.hs.get_datastore()
    33|         self.clock = self.hs.get_clock()
    34|         self._pusher_shard_config = hs.config.push.pusher_shard_config
    35|         self._instance_name = hs.get_instance_name()
    36|         self.pushers = {}  # type: Dict[str, Dict[str, Union[HttpPusher, EmailPusher]]]
    37|     def start(self):
    38|         """Starts the pushers off in a background process.
    39|         """
    40|         if not self._should_start_pushers:
    41|             logger.info("Not starting pushers because they are disabled in the config")
    42|             return
    43|         run_as_background_process("start_pushers", self._start_pushers)
    44|     async def add_pusher(
    45|         self,
    46|         user_id,
    47|         access_token,
    48|         kind,
    49|         app_id,
    50|         app_display_name,
    51|         device_display_name,
    52|         pushkey,
    53|         lang,
    54|         data,
    55|         profile_tag="",

# --- HUNK 2: Lines 109-169 ---
   109|     async def remove_pushers_by_access_token(self, user_id, access_tokens):
   110|         """Remove the pushers for a given user corresponding to a set of
   111|         access_tokens.
   112|         Args:
   113|             user_id (str): user to remove pushers for
   114|             access_tokens (Iterable[int]): access token *ids* to remove pushers
   115|                 for
   116|         """
   117|         if not self._pusher_shard_config.should_handle(self._instance_name, user_id):
   118|             return
   119|         tokens = set(access_tokens)
   120|         for p in await self.store.get_pushers_by_user_id(user_id):
   121|             if p["access_token"] in tokens:
   122|                 logger.info(
   123|                     "Removing pusher for app id %s, pushkey %s, user %s",
   124|                     p["app_id"],
   125|                     p["pushkey"],
   126|                     p["user_name"],
   127|                 )
   128|                 await self.remove_pusher(p["app_id"], p["pushkey"], p["user_name"])
   129|     async def on_new_notifications(self, min_stream_id, max_stream_id):
   130|         if not self.pushers:
   131|             return
   132|         try:
   133|             users_affected = await self.store.get_push_action_users_in_range(
   134|                 min_stream_id, max_stream_id
   135|             )
   136|             for u in users_affected:
   137|                 if u in self.pushers:
   138|                     for p in self.pushers[u].values():
   139|                         p.on_new_notifications(min_stream_id, max_stream_id)
   140|         except Exception:
   141|             logger.exception("Exception in pusher on_new_notifications")
   142|     async def on_new_receipts(self, min_stream_id, max_stream_id, affected_room_ids):
   143|         if not self.pushers:
   144|             return
   145|         try:
   146|             users_affected = await self.store.get_users_sent_receipts_between(
   147|                 min_stream_id - 1, max_stream_id
   148|             )
   149|             for u in users_affected:
   150|                 if u in self.pushers:
   151|                     for p in self.pushers[u].values():
   152|                         p.on_new_receipts(min_stream_id, max_stream_id)
   153|         except Exception:
   154|             logger.exception("Exception in pusher on_new_receipts")
   155|     async def start_pusher_by_id(self, app_id, pushkey, user_id):
   156|         """Look up the details for the given pusher, and start it
   157|         Returns:
   158|             EmailPusher|HttpPusher|None: The pusher started, if any
   159|         """
   160|         if not self._should_start_pushers:
   161|             return
   162|         if not self._pusher_shard_config.should_handle(self._instance_name, user_id):
   163|             return
   164|         resultlist = await self.store.get_pushers_by_app_id_and_pushkey(app_id, pushkey)
   165|         pusher_dict = None
   166|         for r in resultlist:
   167|             if r["user_name"] == user_id:
   168|                 pusher_dict = r
   169|         pusher = None


# ====================================================================
# FILE: synapse/python_dependencies.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-69 ---
     1| import logging
     2| from typing import List, Set
     3| from pkg_resources import (
     4|     DistributionNotFound,
     5|     Requirement,
     6|     VersionConflict,
     7|     get_provider,
     8| )
     9| logger = logging.getLogger(__name__)
    10| REQUIREMENTS = [
    11|     "jsonschema>=2.5.1",
    12|     "frozendict>=1",
    13|     "unpaddedbase64>=1.1.0",
    14|     "canonicaljson>=1.3.0",
    15|     "signedjson>=1.1.0",
    16|     "pynacl>=1.2.1",
    17|     "idna>=2.5",
    18|     "service_identity>=18.1.0",
    19|     "Twisted>=18.9.0",
    20|     "treq>=15.1",
    21|     "pyopenssl>=16.0.0",
    22|     "pyyaml>=3.11",
    23|     "pyasn1>=0.1.9",
    24|     "pyasn1-modules>=0.0.7",
    25|     "bcrypt>=3.1.0",
    26|     "pillow>=4.3.0",
    27|     "sortedcontainers>=1.4.4",
    28|     "pymacaroons>=0.13.0",
    29|     "msgpack>=0.5.2",
    30|     "phonenumbers>=8.2.0",
    31|     "prometheus_client>=0.0.18,<0.9.0",
    32|     "attrs>=19.1.0",
    33|     "netaddr>=0.7.18",
    34|     "Jinja2>=2.9",
    35|     "bleach>=1.4.3",
    36|     "typing-extensions>=3.7.4",
    37| ]
    38| CONDITIONAL_REQUIREMENTS = {
    39|     "matrix-synapse-ldap3": ["matrix-synapse-ldap3>=0.1"],
    40|     "postgres": ["psycopg2>=2.7"],
    41|     "acme": [
    42|         "txacme>=0.9.2",
    43|         'eliot<1.8.0;python_version<"3.5.3"',
    44|     ],
    45|     "saml2": ["pysaml2>=4.5.0"],
    46|     "oidc": ["authlib>=0.14.0"],
    47|     "systemd": ["systemd-python>=231"],
    48|     "url_preview": ["lxml>=3.5.0"],
    49|     "test": ["mock>=2.0", "parameterized>=0.7.0"],
    50|     "sentry": ["sentry-sdk>=0.7.2"],
    51|     "opentracing": ["jaeger-client>=4.0.0", "opentracing>=2.2.0"],
    52|     "jwt": ["pyjwt>=1.6.4"],
    53|     "redis": ["txredisapi>=1.4.7", "hiredis"],
    54| }
    55| ALL_OPTIONAL_REQUIREMENTS = set()  # type: Set[str]
    56| for name, optional_deps in CONDITIONAL_REQUIREMENTS.items():
    57|     if name not in ["systemd"]:
    58|         ALL_OPTIONAL_REQUIREMENTS = set(optional_deps) | ALL_OPTIONAL_REQUIREMENTS
    59| def list_requirements():
    60|     return list(set(REQUIREMENTS) | ALL_OPTIONAL_REQUIREMENTS)
    61| class DependencyException(Exception):
    62|     @property
    63|     def message(self):
    64|         return "\n".join(
    65|             [
    66|                 "Missing Requirements: %s" % (", ".join(self.dependencies),),
    67|                 "To install run:",
    68|                 "    pip install --upgrade --force %s" % (" ".join(self.dependencies),),
    69|                 "",


# ====================================================================
# FILE: synapse/replication/http/_base.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-68 ---
     1| import abc
     2| import logging
     3| import re
     4| import urllib
     5| from inspect import signature
     6| from typing import Dict, List, Tuple
     7| from synapse.api.errors import (
     8|     CodeMessageException,
     9|     HttpResponseException,
    10|     RequestSendFailed,
    11|     SynapseError,
    12| )
    13| from synapse.logging.opentracing import inject_active_span_byte_dict, trace
    14| from synapse.util.caches.response_cache import ResponseCache
    15| from synapse.util.stringutils import random_string
    16| logger = logging.getLogger(__name__)
    17| class ReplicationEndpoint:
    18|     """Helper base class for defining new replication HTTP endpoints.
    19|     This creates an endpoint under `/_synapse/replication/:NAME/:PATH_ARGS..`
    20|     (with a `/:txn_id` suffix for cached requests), where NAME is a name,
    21|     PATH_ARGS are a tuple of parameters to be encoded in the URL.
    22|     For example, if `NAME` is "send_event" and `PATH_ARGS` is `("event_id",)`,
    23|     with `CACHE` set to true then this generates an endpoint:
    24|         /_synapse/replication/send_event/:event_id/:txn_id
    25|     For POST/PUT requests the payload is serialized to json and sent as the
    26|     body, while for GET requests the payload is added as query parameters. See
    27|     `_serialize_payload` for details.
    28|     Incoming requests are handled by overriding `_handle_request`. Servers
    29|     must call `register` to register the path with the HTTP server.
    30|     Requests can be sent by calling the client returned by `make_client`.
    31|     Requests are sent to master process by default, but can be sent to other
    32|     named processes by specifying an `instance_name` keyword argument.
    33|     Attributes:
    34|         NAME (str): A name for the endpoint, added to the path as well as used
    35|             in logging and metrics.
    36|         PATH_ARGS (tuple[str]): A list of parameters to be added to the path.
    37|             Adding parameters to the path (rather than payload) can make it
    38|             easier to follow along in the log files.
    39|         METHOD (str): The method of the HTTP request, defaults to POST. Can be
    40|             one of POST, PUT or GET. If GET then the payload is sent as query
    41|             parameters rather than a JSON body.
    42|         CACHE (bool): Whether server should cache the result of the request/
    43|             If true then transparently adds a txn_id to all requests, and
    44|             `_handle_request` must return a Deferred.
    45|         RETRY_ON_TIMEOUT(bool): Whether or not to retry the request when a 504
    46|             is received.
    47|     """
    48|     __metaclass__ = abc.ABCMeta
    49|     NAME = abc.abstractproperty()  # type: str  # type: ignore
    50|     PATH_ARGS = abc.abstractproperty()  # type: Tuple[str, ...]  # type: ignore
    51|     METHOD = "POST"
    52|     CACHE = True
    53|     RETRY_ON_TIMEOUT = True
    54|     def __init__(self, hs):
    55|         if self.CACHE:
    56|             self.response_cache = ResponseCache(
    57|                 hs, "repl." + self.NAME, timeout_ms=30 * 60 * 1000
    58|             )
    59|         assert (
    60|             "instance_name" not in self.PATH_ARGS
    61|         ), "`instance_name` is a reserved parameter name"
    62|         assert (
    63|             "instance_name"
    64|             not in signature(self.__class__._serialize_payload).parameters
    65|         ), "`instance_name` is a reserved parameter name"
    66|         assert self.METHOD in ("PUT", "POST", "GET")
    67|     @abc.abstractmethod
    68|     async def _serialize_payload(**kwargs):

# --- HUNK 2: Lines 79-119 ---
    79|     @abc.abstractmethod
    80|     async def _handle_request(self, request, **kwargs):
    81|         """Handle incoming request.
    82|         This is called with the request object and PATH_ARGS.
    83|         Returns:
    84|             tuple[int, dict]: HTTP status code and a JSON serialisable dict
    85|             to be used as response body of request.
    86|         """
    87|         pass
    88|     @classmethod
    89|     def make_client(cls, hs):
    90|         """Create a client that makes requests.
    91|         Returns a callable that accepts the same parameters as `_serialize_payload`.
    92|         """
    93|         clock = hs.get_clock()
    94|         client = hs.get_simple_http_client()
    95|         local_instance_name = hs.get_instance_name()
    96|         master_host = hs.config.worker_replication_host
    97|         master_port = hs.config.worker_replication_http_port
    98|         instance_map = hs.config.worker.instance_map
    99|         @trace(opname="outgoing_replication_request")
   100|         async def send_request(instance_name="master", **kwargs):
   101|             if instance_name == local_instance_name:
   102|                 raise Exception("Trying to send HTTP request to self")
   103|             if instance_name == "master":
   104|                 host = master_host
   105|                 port = master_port
   106|             elif instance_name in instance_map:
   107|                 host = instance_map[instance_name].host
   108|                 port = instance_map[instance_name].port
   109|             else:
   110|                 raise Exception(
   111|                     "Instance %r not in 'instance_map' config" % (instance_name,)
   112|                 )
   113|             data = await cls._serialize_payload(**kwargs)
   114|             url_args = [
   115|                 urllib.parse.quote(kwargs[name], safe="") for name in cls.PATH_ARGS
   116|             ]
   117|             if cls.CACHE:
   118|                 txn_id = random_string(10)
   119|                 url_args.append(txn_id)

# --- HUNK 3: Lines 123-171 ---
   123|                 request_func = client.put_json
   124|             elif cls.METHOD == "GET":
   125|                 request_func = client.get_json
   126|             else:
   127|                 raise Exception(
   128|                     "Unknown METHOD on %s replication endpoint" % (cls.NAME,)
   129|                 )
   130|             uri = "http://%s:%s/_synapse/replication/%s/%s" % (
   131|                 host,
   132|                 port,
   133|                 cls.NAME,
   134|                 "/".join(url_args),
   135|             )
   136|             try:
   137|                 while True:
   138|                     headers = {}  # type: Dict[bytes, List[bytes]]
   139|                     inject_active_span_byte_dict(headers, None, check_destination=False)
   140|                     try:
   141|                         result = await request_func(uri, data, headers=headers)
   142|                         break
   143|                     except CodeMessageException as e:
   144|                         if e.code != 504 or not cls.RETRY_ON_TIMEOUT:
   145|                             raise
   146|                     logger.warning("%s request timed out", cls.NAME)
   147|                     await clock.sleep(1)
   148|             except HttpResponseException as e:
   149|                 raise e.to_synapse_error()
   150|             except RequestSendFailed as e:
   151|                 raise SynapseError(502, "Failed to talk to master") from e
   152|             return result
   153|         return send_request
   154|     def register(self, http_server):
   155|         """Called by the server to register this as a handler to the
   156|         appropriate path.
   157|         """
   158|         url_args = list(self.PATH_ARGS)
   159|         handler = self._handle_request
   160|         method = self.METHOD
   161|         if self.CACHE:
   162|             handler = self._cached_handler  # type: ignore
   163|             url_args.append("txn_id")
   164|         args = "/".join("(?P<%s>[^/]+)" % (arg,) for arg in url_args)
   165|         pattern = re.compile("^/_synapse/replication/%s/%s$" % (self.NAME, args))
   166|         http_server.register_paths(
   167|             method, [pattern], handler, self.__class__.__name__,
   168|         )
   169|     def _cached_handler(self, request, txn_id, **kwargs):
   170|         """Called on new incoming requests when caching is enabled. Checks
   171|         if there is a cached response for the request and returns that,


# ====================================================================
# FILE: synapse/replication/http/devices.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 9-40 ---
     9|     Request format:
    10|         POST /_synapse/replication/user_device_resync/:user_id
    11|         {}
    12|     Response is equivalent to ` /_matrix/federation/v1/user/devices/:user_id`
    13|     response, e.g.:
    14|         {
    15|             "user_id": "@alice:example.org",
    16|             "devices": [
    17|                 {
    18|                     "device_id": "JLAFKJWSCS",
    19|                     "keys": { ... },
    20|                     "device_display_name": "Alice's Mobile Phone"
    21|                 }
    22|             ]
    23|         }
    24|     """
    25|     NAME = "user_device_resync"
    26|     PATH_ARGS = ("user_id",)
    27|     CACHE = False
    28|     def __init__(self, hs):
    29|         super(ReplicationUserDevicesResyncRestServlet, self).__init__(hs)
    30|         self.device_list_updater = hs.get_device_handler().device_list_updater
    31|         self.store = hs.get_datastore()
    32|         self.clock = hs.get_clock()
    33|     @staticmethod
    34|     async def _serialize_payload(user_id):
    35|         return {}
    36|     async def _handle_request(self, request, user_id):
    37|         user_devices = await self.device_list_updater.user_device_resync(user_id)
    38|         return 200, user_devices
    39| def register_servlets(hs, http_server):
    40|     ReplicationUserDevicesResyncRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/replication/http/federation.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 14-175 ---
    14|         {
    15|             "events": [{
    16|                 "event": { .. serialized event .. },
    17|                 "room_version": .., // "1", "2", "3", etc: the version of the room
    18|                                     // containing the event
    19|                 "event_format_version": .., // 1,2,3 etc: the event format version
    20|                 "internal_metadata": { .. serialized internal_metadata .. },
    21|                 "rejected_reason": ..,   // The event.rejected_reason field
    22|                 "context": { .. serialized event context .. },
    23|             }],
    24|             "backfilled": false
    25|         }
    26|         200 OK
    27|         {
    28|             "max_stream_id": 32443,
    29|         }
    30|     """
    31|     NAME = "fed_send_events"
    32|     PATH_ARGS = ()
    33|     def __init__(self, hs):
    34|         super(ReplicationFederationSendEventsRestServlet, self).__init__(hs)
    35|         self.store = hs.get_datastore()
    36|         self.storage = hs.get_storage()
    37|         self.clock = hs.get_clock()
    38|         self.federation_handler = hs.get_handlers().federation_handler
    39|     @staticmethod
    40|     async def _serialize_payload(store, event_and_contexts, backfilled):
    41|         """
    42|         Args:
    43|             store
    44|             event_and_contexts (list[tuple[FrozenEvent, EventContext]])
    45|             backfilled (bool): Whether or not the events are the result of
    46|                 backfilling
    47|         """
    48|         event_payloads = []
    49|         for event, context in event_and_contexts:
    50|             serialized_context = await context.serialize(event, store)
    51|             event_payloads.append(
    52|                 {
    53|                     "event": event.get_pdu_json(),
    54|                     "room_version": event.room_version.identifier,
    55|                     "event_format_version": event.format_version,
    56|                     "internal_metadata": event.internal_metadata.get_dict(),
    57|                     "rejected_reason": event.rejected_reason,
    58|                     "context": serialized_context,
    59|                 }
    60|             )
    61|         payload = {"events": event_payloads, "backfilled": backfilled}
    62|         return payload
    63|     async def _handle_request(self, request):
    64|         with Measure(self.clock, "repl_fed_send_events_parse"):
    65|             content = parse_json_object_from_request(request)
    66|             backfilled = content["backfilled"]
    67|             event_payloads = content["events"]
    68|             event_and_contexts = []
    69|             for event_payload in event_payloads:
    70|                 event_dict = event_payload["event"]
    71|                 room_ver = KNOWN_ROOM_VERSIONS[event_payload["room_version"]]
    72|                 internal_metadata = event_payload["internal_metadata"]
    73|                 rejected_reason = event_payload["rejected_reason"]
    74|                 event = make_event_from_dict(
    75|                     event_dict, room_ver, internal_metadata, rejected_reason
    76|                 )
    77|                 context = EventContext.deserialize(
    78|                     self.storage, event_payload["context"]
    79|                 )
    80|                 event_and_contexts.append((event, context))
    81|         logger.info("Got %d events from federation", len(event_and_contexts))
    82|         max_stream_id = await self.federation_handler.persist_events_and_notify(
    83|             event_and_contexts, backfilled
    84|         )
    85|         return 200, {"max_stream_id": max_stream_id}
    86| class ReplicationFederationSendEduRestServlet(ReplicationEndpoint):
    87|     """Handles EDUs newly received from federation, including persisting and
    88|     notifying.
    89|     Request format:
    90|         POST /_synapse/replication/fed_send_edu/:edu_type/:txn_id
    91|         {
    92|             "origin": ...,
    93|             "content: { ... }
    94|         }
    95|     """
    96|     NAME = "fed_send_edu"
    97|     PATH_ARGS = ("edu_type",)
    98|     def __init__(self, hs):
    99|         super(ReplicationFederationSendEduRestServlet, self).__init__(hs)
   100|         self.store = hs.get_datastore()
   101|         self.clock = hs.get_clock()
   102|         self.registry = hs.get_federation_registry()
   103|     @staticmethod
   104|     async def _serialize_payload(edu_type, origin, content):
   105|         return {"origin": origin, "content": content}
   106|     async def _handle_request(self, request, edu_type):
   107|         with Measure(self.clock, "repl_fed_send_edu_parse"):
   108|             content = parse_json_object_from_request(request)
   109|             origin = content["origin"]
   110|             edu_content = content["content"]
   111|         logger.info("Got %r edu from %s", edu_type, origin)
   112|         result = await self.registry.on_edu(edu_type, origin, edu_content)
   113|         return 200, result
   114| class ReplicationGetQueryRestServlet(ReplicationEndpoint):
   115|     """Handle responding to queries from federation.
   116|     Request format:
   117|         POST /_synapse/replication/fed_query/:query_type
   118|         {
   119|             "args": { ... }
   120|         }
   121|     """
   122|     NAME = "fed_query"
   123|     PATH_ARGS = ("query_type",)
   124|     CACHE = False
   125|     def __init__(self, hs):
   126|         super(ReplicationGetQueryRestServlet, self).__init__(hs)
   127|         self.store = hs.get_datastore()
   128|         self.clock = hs.get_clock()
   129|         self.registry = hs.get_federation_registry()
   130|     @staticmethod
   131|     async def _serialize_payload(query_type, args):
   132|         """
   133|         Args:
   134|             query_type (str)
   135|             args (dict): The arguments received for the given query type
   136|         """
   137|         return {"args": args}
   138|     async def _handle_request(self, request, query_type):
   139|         with Measure(self.clock, "repl_fed_query_parse"):
   140|             content = parse_json_object_from_request(request)
   141|             args = content["args"]
   142|         logger.info("Got %r query", query_type)
   143|         result = await self.registry.on_query(query_type, args)
   144|         return 200, result
   145| class ReplicationCleanRoomRestServlet(ReplicationEndpoint):
   146|     """Called to clean up any data in DB for a given room, ready for the
   147|     server to join the room.
   148|     Request format:
   149|         POST /_synapse/replication/fed_cleanup_room/:room_id/:txn_id
   150|         {}
   151|     """
   152|     NAME = "fed_cleanup_room"
   153|     PATH_ARGS = ("room_id",)
   154|     def __init__(self, hs):
   155|         super(ReplicationCleanRoomRestServlet, self).__init__(hs)
   156|         self.store = hs.get_datastore()
   157|     @staticmethod
   158|     async def _serialize_payload(room_id, args):
   159|         """
   160|         Args:
   161|             room_id (str)
   162|         """
   163|         return {}
   164|     async def _handle_request(self, request, room_id):
   165|         await self.store.clean_room_for_join(room_id)
   166|         return 200, {}
   167| class ReplicationStoreRoomOnInviteRestServlet(ReplicationEndpoint):
   168|     """Called to clean up any data in DB for a given room, ready for the
   169|     server to join the room.
   170|     Request format:
   171|         POST /_synapse/replication/store_room_on_invite/:room_id/:txn_id
   172|         {
   173|             "room_version": "1",
   174|         }
   175|     """


# ====================================================================
# FILE: synapse/replication/http/login.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-33 ---
     1| import logging
     2| from synapse.http.servlet import parse_json_object_from_request
     3| from synapse.replication.http._base import ReplicationEndpoint
     4| logger = logging.getLogger(__name__)
     5| class RegisterDeviceReplicationServlet(ReplicationEndpoint):
     6|     """Ensure a device is registered, generating a new access token for the
     7|     device.
     8|     Used during registration and login.
     9|     """
    10|     NAME = "device_check_registered"
    11|     PATH_ARGS = ("user_id",)
    12|     def __init__(self, hs):
    13|         super(RegisterDeviceReplicationServlet, self).__init__(hs)
    14|         self.registration_handler = hs.get_registration_handler()
    15|     @staticmethod
    16|     async def _serialize_payload(user_id, device_id, initial_display_name, is_guest):
    17|         """
    18|         Args:
    19|             device_id (str|None): Device ID to use, if None a new one is
    20|                 generated.
    21|             initial_display_name (str|None)
    22|             is_guest (bool)
    23|         """
    24|         return {
    25|             "device_id": device_id,
    26|             "initial_display_name": initial_display_name,
    27|             "is_guest": is_guest,
    28|         }
    29|     async def _handle_request(self, request, user_id):
    30|         content = parse_json_object_from_request(request)
    31|         device_id = content["device_id"]
    32|         initial_display_name = content["initial_display_name"]
    33|         is_guest = content["is_guest"]


# ====================================================================
# FILE: synapse/replication/http/membership.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-43 ---
     1| import logging
     2| from typing import TYPE_CHECKING, Optional
     3| from synapse.http.servlet import parse_json_object_from_request
     4| from synapse.replication.http._base import ReplicationEndpoint
     5| from synapse.types import JsonDict, Requester, UserID
     6| from synapse.util.distributor import user_joined_room, user_left_room
     7| if TYPE_CHECKING:
     8|     from synapse.server import HomeServer
     9| logger = logging.getLogger(__name__)
    10| class ReplicationRemoteJoinRestServlet(ReplicationEndpoint):
    11|     """Does a remote join for the given user to the given room
    12|     Request format:
    13|         POST /_synapse/replication/remote_join/:room_id/:user_id
    14|         {
    15|             "requester": ...,
    16|             "remote_room_hosts": [...],
    17|             "content": { ... }
    18|         }
    19|     """
    20|     NAME = "remote_join"
    21|     PATH_ARGS = ("room_id", "user_id")
    22|     def __init__(self, hs):
    23|         super(ReplicationRemoteJoinRestServlet, self).__init__(hs)
    24|         self.federation_handler = hs.get_handlers().federation_handler
    25|         self.store = hs.get_datastore()
    26|         self.clock = hs.get_clock()
    27|     @staticmethod
    28|     async def _serialize_payload(
    29|         requester, room_id, user_id, remote_room_hosts, content
    30|     ):
    31|         """
    32|         Args:
    33|             requester(Requester)
    34|             room_id (str)
    35|             user_id (str)
    36|             remote_room_hosts (list[str]): Servers to try and join via
    37|             content(dict): The event content to use for the join event
    38|         """
    39|         return {
    40|             "requester": requester.serialize(),
    41|             "remote_room_hosts": remote_room_hosts,
    42|             "content": content,
    43|         }

# --- HUNK 2: Lines 49-89 ---
    49|         if requester.user:
    50|             request.authenticated_entity = requester.user.to_string()
    51|         logger.info("remote_join: %s into room: %s", user_id, room_id)
    52|         event_id, stream_id = await self.federation_handler.do_invite_join(
    53|             remote_room_hosts, room_id, user_id, event_content
    54|         )
    55|         return 200, {"event_id": event_id, "stream_id": stream_id}
    56| class ReplicationRemoteRejectInviteRestServlet(ReplicationEndpoint):
    57|     """Rejects an out-of-band invite we have received from a remote server
    58|     Request format:
    59|         POST /_synapse/replication/remote_reject_invite/:event_id
    60|         {
    61|             "txn_id": ...,
    62|             "requester": ...,
    63|             "content": { ... }
    64|         }
    65|     """
    66|     NAME = "remote_reject_invite"
    67|     PATH_ARGS = ("invite_event_id",)
    68|     def __init__(self, hs: "HomeServer"):
    69|         super(ReplicationRemoteRejectInviteRestServlet, self).__init__(hs)
    70|         self.store = hs.get_datastore()
    71|         self.clock = hs.get_clock()
    72|         self.member_handler = hs.get_room_member_handler()
    73|     @staticmethod
    74|     async def _serialize_payload(  # type: ignore
    75|         invite_event_id: str,
    76|         txn_id: Optional[str],
    77|         requester: Requester,
    78|         content: JsonDict,
    79|     ):
    80|         """
    81|         Args:
    82|             invite_event_id: ID of the invite to be rejected
    83|             txn_id: optional transaction ID supplied by the client
    84|             requester: user making the rejection request, according to the access token
    85|             content: additional content to include in the rejection event.
    86|                Normally an empty dict.
    87|         """
    88|         return {
    89|             "txn_id": txn_id,

# --- HUNK 3: Lines 94-142 ---
    94|         content = parse_json_object_from_request(request)
    95|         txn_id = content["txn_id"]
    96|         event_content = content["content"]
    97|         requester = Requester.deserialize(self.store, content["requester"])
    98|         if requester.user:
    99|             request.authenticated_entity = requester.user.to_string()
   100|         event_id, stream_id = await self.member_handler.remote_reject_invite(
   101|             invite_event_id, txn_id, requester, event_content,
   102|         )
   103|         return 200, {"event_id": event_id, "stream_id": stream_id}
   104| class ReplicationUserJoinedLeftRoomRestServlet(ReplicationEndpoint):
   105|     """Notifies that a user has joined or left the room
   106|     Request format:
   107|         POST /_synapse/replication/membership_change/:room_id/:user_id/:change
   108|         {}
   109|     """
   110|     NAME = "membership_change"
   111|     PATH_ARGS = ("room_id", "user_id", "change")
   112|     CACHE = False  # No point caching as should return instantly.
   113|     def __init__(self, hs):
   114|         super(ReplicationUserJoinedLeftRoomRestServlet, self).__init__(hs)
   115|         self.registeration_handler = hs.get_registration_handler()
   116|         self.store = hs.get_datastore()
   117|         self.clock = hs.get_clock()
   118|         self.distributor = hs.get_distributor()
   119|     @staticmethod
   120|     async def _serialize_payload(room_id, user_id, change):
   121|         """
   122|         Args:
   123|             room_id (str)
   124|             user_id (str)
   125|             change (str): Either "joined" or "left"
   126|         """
   127|         assert change in ("joined", "left")
   128|         return {}
   129|     def _handle_request(self, request, room_id, user_id, change):
   130|         logger.info("user membership change: %s in %s", user_id, room_id)
   131|         user = UserID.from_string(user_id)
   132|         if change == "joined":
   133|             user_joined_room(self.distributor, user, room_id)
   134|         elif change == "left":
   135|             user_left_room(self.distributor, user, room_id)
   136|         else:
   137|             raise Exception("Unrecognized change: %r", change)
   138|         return 200, {}
   139| def register_servlets(hs, http_server):
   140|     ReplicationRemoteJoinRestServlet(hs).register(http_server)
   141|     ReplicationRemoteRejectInviteRestServlet(hs).register(http_server)
   142|     ReplicationUserJoinedLeftRoomRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/replication/http/register.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| import logging
     2| from synapse.http.servlet import parse_json_object_from_request
     3| from synapse.replication.http._base import ReplicationEndpoint
     4| logger = logging.getLogger(__name__)
     5| class ReplicationRegisterServlet(ReplicationEndpoint):
     6|     """Register a new user
     7|     """
     8|     NAME = "register_user"
     9|     PATH_ARGS = ("user_id",)
    10|     def __init__(self, hs):
    11|         super(ReplicationRegisterServlet, self).__init__(hs)
    12|         self.store = hs.get_datastore()
    13|         self.registration_handler = hs.get_registration_handler()
    14|     @staticmethod
    15|     async def _serialize_payload(
    16|         user_id,
    17|         password_hash,
    18|         was_guest,
    19|         make_guest,
    20|         appservice_id,
    21|         create_profile_with_displayname,
    22|         admin,
    23|         user_type,
    24|         address,
    25|         shadow_banned,
    26|     ):
    27|         """
    28|         Args:
    29|             user_id (str): The desired user ID to register.
    30|             password_hash (str|None): Optional. The password hash for this user.
    31|             was_guest (bool): Optional. Whether this is a guest account being

# --- HUNK 2: Lines 57-97 ---
    57|         self.registration_handler.check_registration_ratelimit(content["address"])
    58|         await self.registration_handler.register_with_store(
    59|             user_id=user_id,
    60|             password_hash=content["password_hash"],
    61|             was_guest=content["was_guest"],
    62|             make_guest=content["make_guest"],
    63|             appservice_id=content["appservice_id"],
    64|             create_profile_with_displayname=content["create_profile_with_displayname"],
    65|             admin=content["admin"],
    66|             user_type=content["user_type"],
    67|             address=content["address"],
    68|             shadow_banned=content["shadow_banned"],
    69|         )
    70|         return 200, {}
    71| class ReplicationPostRegisterActionsServlet(ReplicationEndpoint):
    72|     """Run any post registration actions
    73|     """
    74|     NAME = "post_register"
    75|     PATH_ARGS = ("user_id",)
    76|     def __init__(self, hs):
    77|         super(ReplicationPostRegisterActionsServlet, self).__init__(hs)
    78|         self.store = hs.get_datastore()
    79|         self.registration_handler = hs.get_registration_handler()
    80|     @staticmethod
    81|     async def _serialize_payload(user_id, auth_result, access_token):
    82|         """
    83|         Args:
    84|             user_id (str): The user ID that consented
    85|             auth_result (dict): The authenticated credentials of the newly
    86|                 registered user.
    87|             access_token (str|None): The access token of the newly logged in
    88|                 device, or None if `inhibit_login` enabled.
    89|         """
    90|         return {"auth_result": auth_result, "access_token": access_token}
    91|     async def _handle_request(self, request, user_id):
    92|         content = parse_json_object_from_request(request)
    93|         auth_result = content["auth_result"]
    94|         access_token = content["access_token"]
    95|         await self.registration_handler.post_registration_actions(
    96|             user_id=user_id, auth_result=auth_result, access_token=access_token
    97|         )


# ====================================================================
# FILE: synapse/replication/http/send_event.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 11-51 ---
    11|     """Handles events newly created on workers, including persisting and
    12|     notifying.
    13|     The API looks like:
    14|         POST /_synapse/replication/send_event/:event_id/:txn_id
    15|         {
    16|             "event": { .. serialized event .. },
    17|             "room_version": .., // "1", "2", "3", etc: the version of the room
    18|                                 // containing the event
    19|             "event_format_version": .., // 1,2,3 etc: the event format version
    20|             "internal_metadata": { .. serialized internal_metadata .. },
    21|             "rejected_reason": ..,   // The event.rejected_reason field
    22|             "context": { .. serialized event context .. },
    23|             "requester": { .. serialized requester .. },
    24|             "ratelimit": true,
    25|             "extra_users": [],
    26|         }
    27|     """
    28|     NAME = "send_event"
    29|     PATH_ARGS = ("event_id",)
    30|     def __init__(self, hs):
    31|         super(ReplicationSendEventRestServlet, self).__init__(hs)
    32|         self.event_creation_handler = hs.get_event_creation_handler()
    33|         self.store = hs.get_datastore()
    34|         self.storage = hs.get_storage()
    35|         self.clock = hs.get_clock()
    36|     @staticmethod
    37|     async def _serialize_payload(
    38|         event_id, store, event, context, requester, ratelimit, extra_users
    39|     ):
    40|         """
    41|         Args:
    42|             event_id (str)
    43|             store (DataStore)
    44|             requester (Requester)
    45|             event (FrozenEvent)
    46|             context (EventContext)
    47|             ratelimit (bool)
    48|             extra_users (list(UserID)): Any extra users to notify about event
    49|         """
    50|         serialized_context = await context.serialize(event, store)
    51|         payload = {


# ====================================================================
# FILE: synapse/replication/slave/storage/_base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-23 ---
     1| import logging
     2| from typing import Optional
     3| from synapse.storage.database import DatabasePool
     4| from synapse.storage.databases.main.cache import CacheInvalidationWorkerStore
     5| from synapse.storage.engines import PostgresEngine
     6| from synapse.storage.util.id_generators import MultiWriterIdGenerator
     7| logger = logging.getLogger(__name__)
     8| class BaseSlavedStore(CacheInvalidationWorkerStore):
     9|     def __init__(self, database: DatabasePool, db_conn, hs):
    10|         super(BaseSlavedStore, self).__init__(database, db_conn, hs)
    11|         if isinstance(self.database_engine, PostgresEngine):
    12|             self._cache_id_gen = MultiWriterIdGenerator(
    13|                 db_conn,
    14|                 database,
    15|                 instance_name=hs.get_instance_name(),
    16|                 table="cache_invalidation_stream_by_instance",
    17|                 instance_column="instance_name",
    18|                 id_column="stream_id",
    19|                 sequence_name="cache_invalidation_stream_seq",
    20|             )  # type: Optional[MultiWriterIdGenerator]
    21|         else:
    22|             self._cache_id_gen = None
    23|         self.hs = hs


# ====================================================================
# FILE: synapse/replication/slave/storage/account_data.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-38 ---
     1| from synapse.replication.slave.storage._base import BaseSlavedStore
     2| from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
     3| from synapse.replication.tcp.streams import AccountDataStream, TagAccountDataStream
     4| from synapse.storage.database import DatabasePool
     5| from synapse.storage.databases.main.account_data import AccountDataWorkerStore
     6| from synapse.storage.databases.main.tags import TagsWorkerStore
     7| class SlavedAccountDataStore(TagsWorkerStore, AccountDataWorkerStore, BaseSlavedStore):
     8|     def __init__(self, database: DatabasePool, db_conn, hs):
     9|         self._account_data_id_gen = SlavedIdTracker(
    10|             db_conn,
    11|             "account_data",
    12|             "stream_id",
    13|             extra_tables=[
    14|                 ("room_account_data", "stream_id"),
    15|                 ("room_tags_revisions", "stream_id"),
    16|             ],
    17|         )
    18|         super(SlavedAccountDataStore, self).__init__(database, db_conn, hs)
    19|     def get_max_account_data_stream_id(self):
    20|         return self._account_data_id_gen.get_current_token()
    21|     def process_replication_rows(self, stream_name, instance_name, token, rows):
    22|         if stream_name == TagAccountDataStream.NAME:
    23|             self._account_data_id_gen.advance(instance_name, token)
    24|             for row in rows:
    25|                 self.get_tags_for_user.invalidate((row.user_id,))
    26|                 self._account_data_stream_cache.entity_has_changed(row.user_id, token)
    27|         elif stream_name == AccountDataStream.NAME:
    28|             self._account_data_id_gen.advance(instance_name, token)
    29|             for row in rows:
    30|                 if not row.room_id:
    31|                     self.get_global_account_data_by_type_for_user.invalidate(
    32|                         (row.data_type, row.user_id)
    33|                     )
    34|                 self.get_account_data_for_user.invalidate((row.user_id,))
    35|                 self.get_account_data_for_room.invalidate((row.user_id, row.room_id))
    36|                 self.get_account_data_for_room_and_type.invalidate(
    37|                     (row.user_id, row.room_id, row.data_type)
    38|                 )


# ====================================================================
# FILE: synapse/replication/slave/storage/client_ips.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-23 ---
     1| from synapse.storage.database import DatabasePool
     2| from synapse.storage.databases.main.client_ips import LAST_SEEN_GRANULARITY
     3| from synapse.util.caches.descriptors import Cache
     4| from ._base import BaseSlavedStore
     5| class SlavedClientIpStore(BaseSlavedStore):
     6|     def __init__(self, database: DatabasePool, db_conn, hs):
     7|         super(SlavedClientIpStore, self).__init__(database, db_conn, hs)
     8|         self.client_ip_last_seen = Cache(
     9|             name="client_ip_last_seen", keylen=4, max_entries=50000
    10|         )
    11|     async def insert_client_ip(self, user_id, access_token, ip, user_agent, device_id):
    12|         now = int(self._clock.time_msec())
    13|         key = (user_id, access_token, ip)
    14|         try:
    15|             last_seen = self.client_ip_last_seen.get(key)
    16|         except KeyError:
    17|             last_seen = None
    18|         if last_seen is not None and (now - last_seen) < LAST_SEEN_GRANULARITY:
    19|             return
    20|         self.client_ip_last_seen.prefill(key, now)
    21|         self.hs.get_tcp_replication().send_user_ip(
    22|             user_id, access_token, ip, user_agent, device_id, now
    23|         )


# ====================================================================
# FILE: synapse/replication/slave/storage/deviceinbox.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-30 ---
     1| from synapse.replication.slave.storage._base import BaseSlavedStore
     2| from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
     3| from synapse.replication.tcp.streams import ToDeviceStream
     4| from synapse.storage.database import DatabasePool
     5| from synapse.storage.databases.main.deviceinbox import DeviceInboxWorkerStore
     6| from synapse.util.caches.expiringcache import ExpiringCache
     7| from synapse.util.caches.stream_change_cache import StreamChangeCache
     8| class SlavedDeviceInboxStore(DeviceInboxWorkerStore, BaseSlavedStore):
     9|     def __init__(self, database: DatabasePool, db_conn, hs):
    10|         super(SlavedDeviceInboxStore, self).__init__(database, db_conn, hs)
    11|         self._device_inbox_id_gen = SlavedIdTracker(
    12|             db_conn, "device_inbox", "stream_id"
    13|         )
    14|         self._device_inbox_stream_cache = StreamChangeCache(
    15|             "DeviceInboxStreamChangeCache",
    16|             self._device_inbox_id_gen.get_current_token(),
    17|         )
    18|         self._device_federation_outbox_stream_cache = StreamChangeCache(
    19|             "DeviceFederationOutboxStreamChangeCache",
    20|             self._device_inbox_id_gen.get_current_token(),
    21|         )
    22|         self._last_device_delete_cache = ExpiringCache(
    23|             cache_name="last_device_delete_cache",
    24|             clock=self._clock,
    25|             max_len=10000,
    26|             expiry_ms=30 * 60 * 1000,
    27|         )
    28|     def process_replication_rows(self, stream_name, instance_name, token, rows):
    29|         if stream_name == ToDeviceStream.NAME:
    30|             self._device_inbox_id_gen.advance(instance_name, token)


# ====================================================================
# FILE: synapse/replication/slave/storage/devices.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-30 ---
     1| from synapse.replication.slave.storage._base import BaseSlavedStore
     2| from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
     3| from synapse.replication.tcp.streams._base import DeviceListsStream, UserSignatureStream
     4| from synapse.storage.database import DatabasePool
     5| from synapse.storage.databases.main.devices import DeviceWorkerStore
     6| from synapse.storage.databases.main.end_to_end_keys import EndToEndKeyWorkerStore
     7| from synapse.util.caches.stream_change_cache import StreamChangeCache
     8| class SlavedDeviceStore(EndToEndKeyWorkerStore, DeviceWorkerStore, BaseSlavedStore):
     9|     def __init__(self, database: DatabasePool, db_conn, hs):
    10|         super(SlavedDeviceStore, self).__init__(database, db_conn, hs)
    11|         self.hs = hs
    12|         self._device_list_id_gen = SlavedIdTracker(
    13|             db_conn,
    14|             "device_lists_stream",
    15|             "stream_id",
    16|             extra_tables=[
    17|                 ("user_signature_stream", "stream_id"),
    18|                 ("device_lists_outbound_pokes", "stream_id"),
    19|             ],
    20|         )
    21|         device_list_max = self._device_list_id_gen.get_current_token()
    22|         self._device_list_stream_cache = StreamChangeCache(
    23|             "DeviceListStreamChangeCache", device_list_max
    24|         )
    25|         self._user_signature_stream_cache = StreamChangeCache(
    26|             "UserSignatureStreamChangeCache", device_list_max
    27|         )
    28|         self._device_list_federation_stream_cache = StreamChangeCache(
    29|             "DeviceListFederationStreamChangeCache", device_list_max
    30|         )


# ====================================================================
# FILE: synapse/replication/slave/storage/events.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 10-48 ---
    10| from synapse.storage.databases.main.signatures import SignatureWorkerStore
    11| from synapse.storage.databases.main.state import StateGroupWorkerStore
    12| from synapse.storage.databases.main.stream import StreamWorkerStore
    13| from synapse.storage.databases.main.user_erasure_store import UserErasureWorkerStore
    14| from synapse.util.caches.stream_change_cache import StreamChangeCache
    15| from ._base import BaseSlavedStore
    16| logger = logging.getLogger(__name__)
    17| class SlavedEventStore(
    18|     EventFederationWorkerStore,
    19|     RoomMemberWorkerStore,
    20|     EventPushActionsWorkerStore,
    21|     StreamWorkerStore,
    22|     StateGroupWorkerStore,
    23|     EventsWorkerStore,
    24|     SignatureWorkerStore,
    25|     UserErasureWorkerStore,
    26|     RelationsWorkerStore,
    27|     BaseSlavedStore,
    28| ):
    29|     def __init__(self, database: DatabasePool, db_conn, hs):
    30|         super(SlavedEventStore, self).__init__(database, db_conn, hs)
    31|         events_max = self._stream_id_gen.get_current_token()
    32|         curr_state_delta_prefill, min_curr_state_delta_id = self.db_pool.get_cache_dict(
    33|             db_conn,
    34|             "current_state_delta_stream",
    35|             entity_column="room_id",
    36|             stream_column="stream_id",
    37|             max_value=events_max,  # As we share the stream id with events token
    38|             limit=1000,
    39|         )
    40|         self._curr_state_delta_stream_cache = StreamChangeCache(
    41|             "_curr_state_delta_stream_cache",
    42|             min_curr_state_delta_id,
    43|             prefilled_cache=curr_state_delta_prefill,
    44|         )
    45|     def get_room_max_stream_ordering(self):
    46|         return self._stream_id_gen.get_current_token()
    47|     def get_room_min_stream_ordering(self):
    48|         return self._backfill_id_gen.get_current_token()


# ====================================================================
# FILE: synapse/replication/slave/storage/filtering.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-7 ---
     1| from synapse.storage.database import DatabasePool
     2| from synapse.storage.databases.main.filtering import FilteringStore
     3| from ._base import BaseSlavedStore
     4| class SlavedFilteringStore(BaseSlavedStore):
     5|     def __init__(self, database: DatabasePool, db_conn, hs):
     6|         super(SlavedFilteringStore, self).__init__(database, db_conn, hs)
     7|     get_user_filter = FilteringStore.__dict__["get_user_filter"]


# ====================================================================
# FILE: synapse/replication/slave/storage/groups.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| from synapse.replication.slave.storage._base import BaseSlavedStore
     2| from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
     3| from synapse.replication.tcp.streams import GroupServerStream
     4| from synapse.storage.database import DatabasePool
     5| from synapse.storage.databases.main.group_server import GroupServerWorkerStore
     6| from synapse.util.caches.stream_change_cache import StreamChangeCache
     7| class SlavedGroupServerStore(GroupServerWorkerStore, BaseSlavedStore):
     8|     def __init__(self, database: DatabasePool, db_conn, hs):
     9|         super(SlavedGroupServerStore, self).__init__(database, db_conn, hs)
    10|         self.hs = hs
    11|         self._group_updates_id_gen = SlavedIdTracker(
    12|             db_conn, "local_group_updates", "stream_id"
    13|         )
    14|         self._group_updates_stream_cache = StreamChangeCache(
    15|             "_group_updates_stream_cache",
    16|             self._group_updates_id_gen.get_current_token(),
    17|         )
    18|     def get_group_stream_token(self):
    19|         return self._group_updates_id_gen.get_current_token()
    20|     def process_replication_rows(self, stream_name, instance_name, token, rows):
    21|         if stream_name == GroupServerStream.NAME:
    22|             self._group_updates_id_gen.advance(instance_name, token)
    23|             for row in rows:
    24|                 self._group_updates_stream_cache.entity_has_changed(row.user_id, token)
    25|         return super().process_replication_rows(stream_name, instance_name, token, rows)


# ====================================================================
# FILE: synapse/replication/slave/storage/presence.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| from synapse.replication.tcp.streams import PresenceStream
     2| from synapse.storage import DataStore
     3| from synapse.storage.database import DatabasePool
     4| from synapse.storage.databases.main.presence import PresenceStore
     5| from synapse.util.caches.stream_change_cache import StreamChangeCache
     6| from ._base import BaseSlavedStore
     7| from ._slaved_id_tracker import SlavedIdTracker
     8| class SlavedPresenceStore(BaseSlavedStore):
     9|     def __init__(self, database: DatabasePool, db_conn, hs):
    10|         super(SlavedPresenceStore, self).__init__(database, db_conn, hs)
    11|         self._presence_id_gen = SlavedIdTracker(db_conn, "presence_stream", "stream_id")
    12|         self._presence_on_startup = self._get_active_presence(db_conn)  # type: ignore
    13|         self.presence_stream_cache = StreamChangeCache(
    14|             "PresenceStreamChangeCache", self._presence_id_gen.get_current_token()
    15|         )
    16|     _get_active_presence = DataStore._get_active_presence
    17|     take_presence_startup_info = DataStore.take_presence_startup_info
    18|     _get_presence_for_user = PresenceStore.__dict__["_get_presence_for_user"]
    19|     get_presence_for_users = PresenceStore.__dict__["get_presence_for_users"]
    20|     def get_current_presence_token(self):
    21|         return self._presence_id_gen.get_current_token()
    22|     def process_replication_rows(self, stream_name, instance_name, token, rows):
    23|         if stream_name == PresenceStream.NAME:
    24|             self._presence_id_gen.advance(instance_name, token)
    25|             for row in rows:
    26|                 self.presence_stream_cache.entity_has_changed(row.user_id, token)
    27|                 self._get_presence_for_user.invalidate((row.user_id,))
    28|         return super().process_replication_rows(stream_name, instance_name, token, rows)


# ====================================================================
# FILE: synapse/replication/slave/storage/pushers.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-17 ---
     1| from synapse.replication.tcp.streams import PushersStream
     2| from synapse.storage.database import DatabasePool
     3| from synapse.storage.databases.main.pusher import PusherWorkerStore
     4| from ._base import BaseSlavedStore
     5| from ._slaved_id_tracker import SlavedIdTracker
     6| class SlavedPusherStore(PusherWorkerStore, BaseSlavedStore):
     7|     def __init__(self, database: DatabasePool, db_conn, hs):
     8|         super(SlavedPusherStore, self).__init__(database, db_conn, hs)
     9|         self._pushers_id_gen = SlavedIdTracker(
    10|             db_conn, "pushers", "id", extra_tables=[("deleted_pushers", "stream_id")]
    11|         )
    12|     def get_pushers_stream_token(self):
    13|         return self._pushers_id_gen.get_current_token()
    14|     def process_replication_rows(self, stream_name, instance_name, token, rows):
    15|         if stream_name == PushersStream.NAME:
    16|             self._pushers_id_gen.advance(instance_name, token)
    17|         return super().process_replication_rows(stream_name, instance_name, token, rows)


# ====================================================================
# FILE: synapse/replication/slave/storage/receipts.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-30 ---
     1| from synapse.replication.tcp.streams import ReceiptsStream
     2| from synapse.storage.database import DatabasePool
     3| from synapse.storage.databases.main.receipts import ReceiptsWorkerStore
     4| from ._base import BaseSlavedStore
     5| from ._slaved_id_tracker import SlavedIdTracker
     6| class SlavedReceiptsStore(ReceiptsWorkerStore, BaseSlavedStore):
     7|     def __init__(self, database: DatabasePool, db_conn, hs):
     8|         self._receipts_id_gen = SlavedIdTracker(
     9|             db_conn, "receipts_linearized", "stream_id"
    10|         )
    11|         super(SlavedReceiptsStore, self).__init__(database, db_conn, hs)
    12|     def get_max_receipt_stream_id(self):
    13|         return self._receipts_id_gen.get_current_token()
    14|     def invalidate_caches_for_receipt(self, room_id, receipt_type, user_id):
    15|         self.get_receipts_for_user.invalidate((user_id, receipt_type))
    16|         self._get_linearized_receipts_for_room.invalidate_many((room_id,))
    17|         self.get_last_receipt_event_id_for_user.invalidate(
    18|             (user_id, room_id, receipt_type)
    19|         )
    20|         self._invalidate_get_users_with_receipts_in_room(room_id, receipt_type, user_id)
    21|         self.get_receipts_for_room.invalidate((room_id, receipt_type))
    22|     def process_replication_rows(self, stream_name, instance_name, token, rows):
    23|         if stream_name == ReceiptsStream.NAME:
    24|             self._receipts_id_gen.advance(instance_name, token)
    25|             for row in rows:
    26|                 self.invalidate_caches_for_receipt(
    27|                     row.room_id, row.receipt_type, row.user_id
    28|                 )
    29|                 self._receipts_stream_cache.entity_has_changed(row.room_id, token)
    30|         return super().process_replication_rows(stream_name, instance_name, token, rows)


# ====================================================================
# FILE: synapse/replication/slave/storage/room.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-17 ---
     1| from synapse.replication.tcp.streams import PublicRoomsStream
     2| from synapse.storage.database import DatabasePool
     3| from synapse.storage.databases.main.room import RoomWorkerStore
     4| from ._base import BaseSlavedStore
     5| from ._slaved_id_tracker import SlavedIdTracker
     6| class RoomStore(RoomWorkerStore, BaseSlavedStore):
     7|     def __init__(self, database: DatabasePool, db_conn, hs):
     8|         super(RoomStore, self).__init__(database, db_conn, hs)
     9|         self._public_room_id_gen = SlavedIdTracker(
    10|             db_conn, "public_room_list_stream", "stream_id"
    11|         )
    12|     def get_current_public_room_stream_id(self):
    13|         return self._public_room_id_gen.get_current_token()
    14|     def process_replication_rows(self, stream_name, instance_name, token, rows):
    15|         if stream_name == PublicRoomsStream.NAME:
    16|             self._public_room_id_gen.advance(instance_name, token)
    17|         return super().process_replication_rows(stream_name, instance_name, token, rows)


# ====================================================================
# FILE: synapse/replication/tcp/client.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-35 ---
     1| """A replication client for use by synapse workers.
     2| """
     3| import logging
     4| from typing import TYPE_CHECKING, Dict, List, Tuple
     5| from twisted.internet.defer import Deferred
     6| from twisted.internet.protocol import ReconnectingClientFactory
     7| from synapse.api.constants import EventTypes
     8| from synapse.logging.context import PreserveLoggingContext, make_deferred_yieldable
     9| from synapse.replication.tcp.protocol import ClientReplicationStreamProtocol
    10| from synapse.replication.tcp.streams import TypingStream
    11| from synapse.replication.tcp.streams.events import (
    12|     EventsStream,
    13|     EventsStreamEventRow,
    14|     EventsStreamRow,
    15| )
    16| from synapse.util.async_helpers import timeout_deferred
    17| from synapse.util.metrics import Measure
    18| if TYPE_CHECKING:
    19|     from synapse.replication.tcp.handler import ReplicationCommandHandler
    20|     from synapse.server import HomeServer
    21| logger = logging.getLogger(__name__)
    22| _WAIT_FOR_REPLICATION_TIMEOUT_SECONDS = 30
    23| class DirectTcpReplicationClientFactory(ReconnectingClientFactory):
    24|     """Factory for building connections to the master. Will reconnect if the
    25|     connection is lost.
    26|     Accepts a handler that is passed to `ClientReplicationStreamProtocol`.
    27|     """
    28|     initialDelay = 0.1
    29|     maxDelay = 1  # Try at least once every N seconds
    30|     def __init__(
    31|         self,
    32|         hs: "HomeServer",
    33|         client_name: str,
    34|         command_handler: "ReplicationCommandHandler",
    35|     ):

# --- HUNK 2: Lines 46-129 ---
    46|         return ClientReplicationStreamProtocol(
    47|             self.hs,
    48|             self.client_name,
    49|             self.server_name,
    50|             self._clock,
    51|             self.command_handler,
    52|         )
    53|     def clientConnectionLost(self, connector, reason):
    54|         logger.error("Lost replication conn: %r", reason)
    55|         ReconnectingClientFactory.clientConnectionLost(self, connector, reason)
    56|     def clientConnectionFailed(self, connector, reason):
    57|         logger.error("Failed to connect to replication: %r", reason)
    58|         ReconnectingClientFactory.clientConnectionFailed(self, connector, reason)
    59| class ReplicationDataHandler:
    60|     """Handles incoming stream updates from replication.
    61|     This instance notifies the slave data store about updates. Can be subclassed
    62|     to handle updates in additional ways.
    63|     """
    64|     def __init__(self, hs: "HomeServer"):
    65|         self.store = hs.get_datastore()
    66|         self.pusher_pool = hs.get_pusherpool()
    67|         self.notifier = hs.get_notifier()
    68|         self._reactor = hs.get_reactor()
    69|         self._clock = hs.get_clock()
    70|         self._streams = hs.get_replication_streams()
    71|         self._instance_name = hs.get_instance_name()
    72|         self._typing_handler = hs.get_typing_handler()
    73|         self._streams_to_waiters = (
    74|             {}
    75|         )  # type: Dict[str, List[Tuple[int, Deferred[None]]]]
    76|     async def on_rdata(
    77|         self, stream_name: str, instance_name: str, token: int, rows: list
    78|     ):
    79|         """Called to handle a batch of replication data with a given stream token.
    80|         By default this just pokes the slave store. Can be overridden in subclasses to
    81|         handle more.
    82|         Args:
    83|             stream_name: name of the replication stream for this batch of rows
    84|             instance_name: the instance that wrote the rows.
    85|             token: stream token for this batch of rows
    86|             rows: a list of Stream.ROW_TYPE objects as returned by Stream.parse_row.
    87|         """
    88|         self.store.process_replication_rows(stream_name, instance_name, token, rows)
    89|         if stream_name == TypingStream.NAME:
    90|             self._typing_handler.process_replication_rows(token, rows)
    91|             self.notifier.on_new_event(
    92|                 "typing_key", token, rooms=[row.room_id for row in rows]
    93|             )
    94|         if stream_name == EventsStream.NAME:
    95|             for row in rows:
    96|                 if row.type != EventsStreamEventRow.TypeId:
    97|                     continue
    98|                 assert isinstance(row, EventsStreamRow)
    99|                 event = await self.store.get_event(
   100|                     row.data.event_id, allow_rejected=True
   101|                 )
   102|                 if event.rejected_reason:
   103|                     continue
   104|                 extra_users = ()  # type: Tuple[str, ...]
   105|                 if event.type == EventTypes.Member:
   106|                     extra_users = (event.state_key,)
   107|                 max_token = self.store.get_room_max_stream_ordering()
   108|                 self.notifier.on_new_room_event(event, token, max_token, extra_users)
   109|             await self.pusher_pool.on_new_notifications(token, token)
   110|         waiting_list = self._streams_to_waiters.get(stream_name, [])
   111|         index_of_first_deferred_not_called = len(waiting_list)
   112|         for idx, (position, deferred) in enumerate(waiting_list):
   113|             if position <= token:
   114|                 try:
   115|                     with PreserveLoggingContext():
   116|                         deferred.callback(None)
   117|                 except Exception:
   118|                     pass
   119|             else:
   120|                 index_of_first_deferred_not_called = idx
   121|                 break
   122|         waiting_list[:] = waiting_list[index_of_first_deferred_not_called:]
   123|     async def on_position(self, stream_name: str, instance_name: str, token: int):
   124|         self.store.process_replication_rows(stream_name, instance_name, token, [])
   125|     def on_remote_server_up(self, server: str):
   126|         """Called when get a new REMOTE_SERVER_UP command."""
   127|     async def wait_for_stream_position(
   128|         self, instance_name: str, stream_name: str, position: int
   129|     ):


# ====================================================================
# FILE: synapse/replication/tcp/handler.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 55-95 ---
    55|     """Handles incoming commands from replication as well as sending commands
    56|     back out to connections.
    57|     """
    58|     def __init__(self, hs):
    59|         self._replication_data_handler = hs.get_replication_data_handler()
    60|         self._presence_handler = hs.get_presence_handler()
    61|         self._store = hs.get_datastore()
    62|         self._notifier = hs.get_notifier()
    63|         self._clock = hs.get_clock()
    64|         self._instance_id = hs.get_instance_id()
    65|         self._instance_name = hs.get_instance_name()
    66|         self._streams = {
    67|             stream.NAME: stream(hs) for stream in STREAMS_MAP.values()
    68|         }  # type: Dict[str, Stream]
    69|         self._streams_to_replicate = []  # type: List[Stream]
    70|         for stream in self._streams.values():
    71|             if stream.NAME == CachesStream.NAME:
    72|                 self._streams_to_replicate.append(stream)
    73|                 continue
    74|             if isinstance(stream, (EventsStream, BackfillStream)):
    75|                 if hs.config.worker.writers.events == hs.get_instance_name():
    76|                     self._streams_to_replicate.append(stream)
    77|                 continue
    78|             if isinstance(stream, TypingStream):
    79|                 if hs.config.worker.writers.typing == hs.get_instance_name():
    80|                     self._streams_to_replicate.append(stream)
    81|                 continue
    82|             if hs.config.worker_app is not None:
    83|                 continue
    84|             if stream.NAME == FederationStream.NAME and hs.config.send_federation:
    85|                 continue
    86|             self._streams_to_replicate.append(stream)
    87|         self._pending_batches = {}  # type: Dict[str, List[Any]]
    88|         self._factory = None  # type: Optional[ReconnectingClientFactory]
    89|         self._connections = []  # type: List[AbstractConnection]
    90|         LaterGauge(
    91|             "synapse_replication_tcp_resource_total_connections",
    92|             "",
    93|             [],
    94|             lambda: len(self._connections),
    95|         )


# ====================================================================
# FILE: synapse/replication/tcp/streams/_base.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 209-277 ---
   209|             "data",  # dict
   210|         ),
   211|     )
   212|     NAME = "receipts"
   213|     ROW_TYPE = ReceiptsStreamRow
   214|     def __init__(self, hs):
   215|         store = hs.get_datastore()
   216|         super().__init__(
   217|             hs.get_instance_name(),
   218|             current_token_without_instance(store.get_max_receipt_stream_id),
   219|             store.get_all_updated_receipts,
   220|         )
   221| class PushRulesStream(Stream):
   222|     """A user has changed their push rules
   223|     """
   224|     PushRulesStreamRow = namedtuple("PushRulesStreamRow", ("user_id",))  # str
   225|     NAME = "push_rules"
   226|     ROW_TYPE = PushRulesStreamRow
   227|     def __init__(self, hs):
   228|         self.store = hs.get_datastore()
   229|         super(PushRulesStream, self).__init__(
   230|             hs.get_instance_name(),
   231|             self._current_token,
   232|             self.store.get_all_push_rule_updates,
   233|         )
   234|     def _current_token(self, instance_name: str) -> int:
   235|         push_rules_token = self.store.get_max_push_rules_stream_id()
   236|         return push_rules_token
   237| class PushersStream(Stream):
   238|     """A user has added/changed/removed a pusher
   239|     """
   240|     PushersStreamRow = namedtuple(
   241|         "PushersStreamRow",
   242|         ("user_id", "app_id", "pushkey", "deleted"),  # str  # str  # str  # bool
   243|     )
   244|     NAME = "pushers"
   245|     ROW_TYPE = PushersStreamRow
   246|     def __init__(self, hs):
   247|         store = hs.get_datastore()
   248|         super().__init__(
   249|             hs.get_instance_name(),
   250|             current_token_without_instance(store.get_pushers_stream_token),
   251|             store.get_all_updated_pushers_rows,
   252|         )
   253| class CachesStream(Stream):
   254|     """A cache was invalidated on the master and no other stream would invalidate
   255|     the cache on the workers
   256|     """
   257|     @attr.s
   258|     class CachesStreamRow:
   259|         """Stream to inform workers they should invalidate their cache.
   260|         Attributes:
   261|             cache_func: Name of the cached function.
   262|             keys: The entry in the cache to invalidate. If None then will
   263|                 invalidate all.
   264|             invalidation_ts: Timestamp of when the invalidation took place.
   265|         """
   266|         cache_func = attr.ib(type=str)
   267|         keys = attr.ib(type=Optional[List[Any]])
   268|         invalidation_ts = attr.ib(type=int)
   269|     NAME = "caches"
   270|     ROW_TYPE = CachesStreamRow
   271|     def __init__(self, hs):
   272|         store = hs.get_datastore()
   273|         super().__init__(
   274|             hs.get_instance_name(),
   275|             store.get_cache_stream_token_for_writer,
   276|             store.get_all_updated_caches,
   277|         )

# --- HUNK 2: Lines 283-323 ---
   283|         (
   284|             "room_id",  # str
   285|             "visibility",  # str
   286|             "appservice_id",  # str, optional
   287|             "network_id",  # str, optional
   288|         ),
   289|     )
   290|     NAME = "public_rooms"
   291|     ROW_TYPE = PublicRoomsStreamRow
   292|     def __init__(self, hs):
   293|         store = hs.get_datastore()
   294|         super().__init__(
   295|             hs.get_instance_name(),
   296|             current_token_without_instance(store.get_current_public_room_stream_id),
   297|             store.get_all_new_public_rooms,
   298|         )
   299| class DeviceListsStream(Stream):
   300|     """Either a user has updated their devices or a remote server needs to be
   301|     told about a device update.
   302|     """
   303|     @attr.s
   304|     class DeviceListsStreamRow:
   305|         entity = attr.ib(type=str)
   306|     NAME = "device_lists"
   307|     ROW_TYPE = DeviceListsStreamRow
   308|     def __init__(self, hs):
   309|         store = hs.get_datastore()
   310|         super().__init__(
   311|             hs.get_instance_name(),
   312|             current_token_without_instance(store.get_device_stream_token),
   313|             store.get_all_device_list_changes_for_remotes,
   314|         )
   315| class ToDeviceStream(Stream):
   316|     """New to_device messages for a client
   317|     """
   318|     ToDeviceStreamRow = namedtuple("ToDeviceStreamRow", ("entity",))  # str
   319|     NAME = "to_device"
   320|     ROW_TYPE = ToDeviceStreamRow
   321|     def __init__(self, hs):
   322|         store = hs.get_datastore()
   323|         super().__init__(


# ====================================================================
# FILE: synapse/replication/tcp/streams/events.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| import heapq
     2| from collections.abc import Iterable
     3| from typing import List, Tuple, Type
     4| import attr
     5| from ._base import Stream, StreamUpdateResult, Token, current_token_without_instance
     6| """Handling of the 'events' replication stream
     7| This stream contains rows of various types. Each row therefore contains a 'type'
     8| identifier before the real data. For example::
     9|     RDATA events batch ["state", ["!room:id", "m.type", "", "$event:id"]]
    10|     RDATA events 12345 ["ev", ["$event:id", "!room:id", "m.type", null, null]]
    11| An "ev" row is sent for each new event. The fields in the data part are:
    12|  * The new event id
    13|  * The room id for the event
    14|  * The type of the new event
    15|  * The state key of the event, for state events
    16|  * The event id of an event which is redacted by this event.
    17| A "state" row is sent whenever the "current state" in a room changes. The fields in the
    18| data part are:
    19|  * The room id for the state change
    20|  * The event type of the state which has changed
    21|  * The state_key of the state which has changed
    22|  * The event id of the new state
    23| """
    24| @attr.s(slots=True, frozen=True)
    25| class EventsStreamRow:

# --- HUNK 2: Lines 51-91 ---
    51| @attr.s(slots=True, frozen=True)
    52| class EventsStreamCurrentStateRow(BaseEventsStreamRow):
    53|     TypeId = "state"
    54|     room_id = attr.ib()  # str
    55|     type = attr.ib()  # str
    56|     state_key = attr.ib()  # str
    57|     event_id = attr.ib()  # str, optional
    58| _EventRows = (
    59|     EventsStreamEventRow,
    60|     EventsStreamCurrentStateRow,
    61| )  # type: Tuple[Type[BaseEventsStreamRow], ...]
    62| TypeToRow = {Row.TypeId: Row for Row in _EventRows}
    63| class EventsStream(Stream):
    64|     """We received a new event, or an event went from being an outlier to not
    65|     """
    66|     NAME = "events"
    67|     def __init__(self, hs):
    68|         self._store = hs.get_datastore()
    69|         super().__init__(
    70|             hs.get_instance_name(),
    71|             current_token_without_instance(self._store.get_current_events_token),
    72|             self._update_function,
    73|         )
    74|     async def _update_function(
    75|         self,
    76|         instance_name: str,
    77|         from_token: Token,
    78|         current_token: Token,
    79|         target_row_count: int,
    80|     ) -> StreamUpdateResult:
    81|         target_row_count //= 2
    82|         event_rows = await self._store.get_all_new_forward_event_rows(
    83|             from_token, current_token, target_row_count
    84|         )  # type: List[Tuple]
    85|         assert (
    86|             len(event_rows) <= target_row_count
    87|         ), "get_all_new_forward_event_rows did not honour row limit"
    88|         if len(event_rows) == target_row_count:
    89|             limited = True
    90|             upper_limit = event_rows[-1][0]  # type: int
    91|         else:


# ====================================================================
# FILE: synapse/rest/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-22 ---
     1| import synapse.rest.admin
     2| from synapse.http.server import JsonResource
     3| from synapse.rest.client import versions
     4| from synapse.rest.client.v1 import (
     5|     directory,
     6|     events,
     7|     initial_sync,
     8|     login as v1_login,
     9|     logout,
    10|     presence,
    11|     profile,
    12|     push_rule,
    13|     pusher,
    14|     room,
    15|     voip,
    16| )
    17| from synapse.rest.client.v2_alpha import (
    18|     account,
    19|     account_data,
    20|     account_validity,
    21|     auth,
    22|     capabilities,

# --- HUNK 2: Lines 76-99 ---
    76|         receipts.register_servlets(hs, client_resource)
    77|         read_marker.register_servlets(hs, client_resource)
    78|         room_keys.register_servlets(hs, client_resource)
    79|         keys.register_servlets(hs, client_resource)
    80|         tokenrefresh.register_servlets(hs, client_resource)
    81|         tags.register_servlets(hs, client_resource)
    82|         account_data.register_servlets(hs, client_resource)
    83|         report_event.register_servlets(hs, client_resource)
    84|         openid.register_servlets(hs, client_resource)
    85|         notifications.register_servlets(hs, client_resource)
    86|         devices.register_servlets(hs, client_resource)
    87|         thirdparty.register_servlets(hs, client_resource)
    88|         sendtodevice.register_servlets(hs, client_resource)
    89|         user_directory.register_servlets(hs, client_resource)
    90|         groups.register_servlets(hs, client_resource)
    91|         room_upgrade_rest_servlet.register_servlets(hs, client_resource)
    92|         capabilities.register_servlets(hs, client_resource)
    93|         account_validity.register_servlets(hs, client_resource)
    94|         relations.register_servlets(hs, client_resource)
    95|         password_policy.register_servlets(hs, client_resource)
    96|         synapse.rest.admin.register_servlets_for_client_rest_resource(
    97|             hs, client_resource
    98|         )
    99|         shared_rooms.register_servlets(hs, client_resource)


# ====================================================================
# FILE: synapse/rest/admin/__init__.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-94 ---
     1| import logging
     2| import platform
     3| import re
     4| import synapse
     5| from synapse.api.errors import Codes, NotFoundError, SynapseError
     6| from synapse.http.server import JsonResource
     7| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     8| from synapse.rest.admin._base import (
     9|     assert_requester_is_admin,
    10|     historical_admin_path_patterns,
    11| )
    12| from synapse.rest.admin.devices import (
    13|     DeleteDevicesRestServlet,
    14|     DeviceRestServlet,
    15|     DevicesRestServlet,
    16| )
    17| from synapse.rest.admin.groups import DeleteGroupAdminRestServlet
    18| from synapse.rest.admin.media import ListMediaInRoom, register_servlets_for_media_repo
    19| from synapse.rest.admin.purge_room_servlet import PurgeRoomServlet
    20| from synapse.rest.admin.rooms import (
    21|     DeleteRoomRestServlet,
    22|     JoinRoomAliasServlet,
    23|     ListRoomRestServlet,
    24|     RoomMembersRestServlet,
    25|     RoomRestServlet,
    26|     ShutdownRoomRestServlet,
    27| )
    28| from synapse.rest.admin.server_notice_servlet import SendServerNoticeServlet
    29| from synapse.rest.admin.users import (
    30|     AccountValidityRenewServlet,
    31|     DeactivateAccountRestServlet,
    32|     ResetPasswordRestServlet,
    33|     SearchUsersRestServlet,
    34|     UserAdminServlet,
    35|     UserRegisterServlet,
    36|     UserRestServletV2,
    37|     UsersRestServlet,
    38|     UsersRestServletV2,
    39|     WhoisRestServlet,
    40| )
    41| from synapse.util.versionstring import get_version_string
    42| logger = logging.getLogger(__name__)
    43| class VersionServlet(RestServlet):
    44|     PATTERNS = (re.compile("^/_synapse/admin/v1/server_version$"),)
    45|     def __init__(self, hs):
    46|         self.res = {
    47|             "server_version": get_version_string(synapse),
    48|             "python_version": platform.python_version(),
    49|         }
    50|     def on_GET(self, request):
    51|         return 200, self.res
    52| class PurgeHistoryRestServlet(RestServlet):
    53|     PATTERNS = historical_admin_path_patterns(
    54|         "/purge_history/(?P<room_id>[^/]*)(/(?P<event_id>[^/]+))?"
    55|     )
    56|     def __init__(self, hs):
    57|         """
    58|         Args:
    59|             hs (synapse.server.HomeServer)
    60|         """
    61|         self.pagination_handler = hs.get_pagination_handler()
    62|         self.store = hs.get_datastore()
    63|         self.auth = hs.get_auth()
    64|     async def on_POST(self, request, room_id, event_id):
    65|         await assert_requester_is_admin(self.auth, request)
    66|         body = parse_json_object_from_request(request, allow_empty_body=True)
    67|         delete_local_events = bool(body.get("delete_local_events", False))
    68|         if event_id is None:
    69|             event_id = body.get("purge_up_to_event_id")
    70|         if event_id is not None:
    71|             event = await self.store.get_event(event_id)
    72|             if event.room_id != room_id:
    73|                 raise SynapseError(400, "Event is for wrong room.")
    74|             token = await self.store.get_topological_token_for_event(event_id)
    75|             logger.info("[purge] purging up to token %s (event_id %s)", token, event_id)
    76|         elif "purge_up_to_ts" in body:
    77|             ts = body["purge_up_to_ts"]
    78|             if not isinstance(ts, int):
    79|                 raise SynapseError(
    80|                     400, "purge_up_to_ts must be an int", errcode=Codes.BAD_JSON
    81|                 )
    82|             stream_ordering = await self.store.find_first_stream_ordering_after_ts(ts)
    83|             r = await self.store.get_room_event_before_stream_ordering(
    84|                 room_id, stream_ordering
    85|             )
    86|             if not r:
    87|                 logger.warning(
    88|                     "[purge] purging events not possible: No event found "
    89|                     "(received_ts %i => stream_ordering %i)",
    90|                     ts,
    91|                     stream_ordering,
    92|                 )
    93|                 raise SynapseError(
    94|                     404, "there is no event to be purged", errcode=Codes.NOT_FOUND

# --- HUNK 2: Lines 131-172 ---
   131|         return 200, purge_status.asdict()
   132| class AdminRestResource(JsonResource):
   133|     """The REST resource which gets mounted at /_synapse/admin"""
   134|     def __init__(self, hs):
   135|         JsonResource.__init__(self, hs, canonical_json=False)
   136|         register_servlets(hs, self)
   137| def register_servlets(hs, http_server):
   138|     """
   139|     Register all the admin servlets.
   140|     """
   141|     register_servlets_for_client_rest_resource(hs, http_server)
   142|     ListRoomRestServlet(hs).register(http_server)
   143|     RoomRestServlet(hs).register(http_server)
   144|     RoomMembersRestServlet(hs).register(http_server)
   145|     DeleteRoomRestServlet(hs).register(http_server)
   146|     JoinRoomAliasServlet(hs).register(http_server)
   147|     PurgeRoomServlet(hs).register(http_server)
   148|     SendServerNoticeServlet(hs).register(http_server)
   149|     VersionServlet(hs).register(http_server)
   150|     UserAdminServlet(hs).register(http_server)
   151|     UserRestServletV2(hs).register(http_server)
   152|     UsersRestServletV2(hs).register(http_server)
   153|     DeviceRestServlet(hs).register(http_server)
   154|     DevicesRestServlet(hs).register(http_server)
   155|     DeleteDevicesRestServlet(hs).register(http_server)
   156| def register_servlets_for_client_rest_resource(hs, http_server):
   157|     """Register only the servlets which need to be exposed on /_matrix/client/xxx"""
   158|     WhoisRestServlet(hs).register(http_server)
   159|     PurgeHistoryStatusRestServlet(hs).register(http_server)
   160|     DeactivateAccountRestServlet(hs).register(http_server)
   161|     PurgeHistoryRestServlet(hs).register(http_server)
   162|     UsersRestServlet(hs).register(http_server)
   163|     ResetPasswordRestServlet(hs).register(http_server)
   164|     SearchUsersRestServlet(hs).register(http_server)
   165|     ShutdownRoomRestServlet(hs).register(http_server)
   166|     UserRegisterServlet(hs).register(http_server)
   167|     DeleteGroupAdminRestServlet(hs).register(http_server)
   168|     AccountValidityRenewServlet(hs).register(http_server)
   169|     if hs.config.can_load_media_repo:
   170|         register_servlets_for_media_repo(hs, http_server)
   171|     else:
   172|         ListMediaInRoom(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/admin/_base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 4-52 ---
     4| from synapse.api.errors import AuthError
     5| from synapse.types import UserID
     6| def historical_admin_path_patterns(path_regex):
     7|     """Returns the list of patterns for an admin endpoint, including historical ones
     8|     This is a backwards-compatibility hack. Previously, the Admin API was exposed at
     9|     various paths under /_matrix/client. This function returns a list of patterns
    10|     matching those paths (as well as the new one), so that existing scripts which rely
    11|     on the endpoints being available there are not broken.
    12|     Note that this should only be used for existing endpoints: new ones should just
    13|     register for the /_synapse/admin path.
    14|     """
    15|     return [
    16|         re.compile(prefix + path_regex)
    17|         for prefix in (
    18|             "^/_synapse/admin/v1",
    19|             "^/_matrix/client/api/v1/admin",
    20|             "^/_matrix/client/unstable/admin",
    21|             "^/_matrix/client/r0/admin",
    22|         )
    23|     ]
    24| def admin_patterns(path_regex: str):
    25|     """Returns the list of patterns for an admin endpoint
    26|     Args:
    27|         path_regex: The regex string to match. This should NOT have a ^
    28|             as this will be prefixed.
    29|     Returns:
    30|         A list of regex patterns.
    31|     """
    32|     admin_prefix = "^/_synapse/admin/v1"
    33|     patterns = [re.compile(admin_prefix + path_regex)]
    34|     return patterns
    35| async def assert_requester_is_admin(
    36|     auth: synapse.api.auth.Auth, request: twisted.web.server.Request
    37| ) -> None:
    38|     """Verify that the requester is an admin user
    39|     Args:
    40|         auth: api.auth.Auth singleton
    41|         request: incoming request
    42|     Raises:
    43|         AuthError if the requester is not a server admin
    44|     """
    45|     requester = await auth.get_user_by_req(request)
    46|     await assert_user_is_admin(auth, requester.user)
    47| async def assert_user_is_admin(auth: synapse.api.auth.Auth, user_id: UserID) -> None:
    48|     """Verify that the given user is an admin user
    49|     Args:
    50|         auth: api.auth.Auth singleton
    51|         user_id: user to check
    52|     Raises:


# ====================================================================
# FILE: synapse/rest/admin/devices.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-42 ---
     1| import logging
     2| import re
     3| from synapse.api.errors import NotFoundError, SynapseError
     4| from synapse.http.servlet import (
     5|     RestServlet,
     6|     assert_params_in_dict,
     7|     parse_json_object_from_request,
     8| )
     9| from synapse.rest.admin._base import assert_requester_is_admin
    10| from synapse.types import UserID
    11| logger = logging.getLogger(__name__)
    12| class DeviceRestServlet(RestServlet):
    13|     """
    14|     Get, update or delete the given user's device
    15|     """
    16|     PATTERNS = (
    17|         re.compile(
    18|             "^/_synapse/admin/v2/users/(?P<user_id>[^/]*)/devices/(?P<device_id>[^/]*)$"
    19|         ),
    20|     )
    21|     def __init__(self, hs):
    22|         super(DeviceRestServlet, self).__init__()
    23|         self.hs = hs
    24|         self.auth = hs.get_auth()
    25|         self.device_handler = hs.get_device_handler()
    26|         self.store = hs.get_datastore()
    27|     async def on_GET(self, request, user_id, device_id):
    28|         await assert_requester_is_admin(self.auth, request)
    29|         target_user = UserID.from_string(user_id)
    30|         if not self.hs.is_mine(target_user):
    31|             raise SynapseError(400, "Can only lookup local users")
    32|         u = await self.store.get_user_by_id(target_user.to_string())
    33|         if u is None:
    34|             raise NotFoundError("Unknown user")
    35|         device = await self.device_handler.get_device(
    36|             target_user.to_string(), device_id
    37|         )
    38|         return 200, device
    39|     async def on_DELETE(self, request, user_id, device_id):
    40|         await assert_requester_is_admin(self.auth, request)
    41|         target_user = UserID.from_string(user_id)
    42|         if not self.hs.is_mine(target_user):

# --- HUNK 2: Lines 46-112 ---
    46|             raise NotFoundError("Unknown user")
    47|         await self.device_handler.delete_device(target_user.to_string(), device_id)
    48|         return 200, {}
    49|     async def on_PUT(self, request, user_id, device_id):
    50|         await assert_requester_is_admin(self.auth, request)
    51|         target_user = UserID.from_string(user_id)
    52|         if not self.hs.is_mine(target_user):
    53|             raise SynapseError(400, "Can only lookup local users")
    54|         u = await self.store.get_user_by_id(target_user.to_string())
    55|         if u is None:
    56|             raise NotFoundError("Unknown user")
    57|         body = parse_json_object_from_request(request, allow_empty_body=True)
    58|         await self.device_handler.update_device(
    59|             target_user.to_string(), device_id, body
    60|         )
    61|         return 200, {}
    62| class DevicesRestServlet(RestServlet):
    63|     """
    64|     Retrieve the given user's devices
    65|     """
    66|     PATTERNS = (re.compile("^/_synapse/admin/v2/users/(?P<user_id>[^/]*)/devices$"),)
    67|     def __init__(self, hs):
    68|         """
    69|         Args:
    70|             hs (synapse.server.HomeServer): server
    71|         """
    72|         self.hs = hs
    73|         self.auth = hs.get_auth()
    74|         self.device_handler = hs.get_device_handler()
    75|         self.store = hs.get_datastore()
    76|     async def on_GET(self, request, user_id):
    77|         await assert_requester_is_admin(self.auth, request)
    78|         target_user = UserID.from_string(user_id)
    79|         if not self.hs.is_mine(target_user):
    80|             raise SynapseError(400, "Can only lookup local users")
    81|         u = await self.store.get_user_by_id(target_user.to_string())
    82|         if u is None:
    83|             raise NotFoundError("Unknown user")
    84|         devices = await self.device_handler.get_devices_by_user(target_user.to_string())
    85|         return 200, {"devices": devices}
    86| class DeleteDevicesRestServlet(RestServlet):
    87|     """
    88|     API for bulk deletion of devices. Accepts a JSON object with a devices
    89|     key which lists the device_ids to delete.
    90|     """
    91|     PATTERNS = (
    92|         re.compile("^/_synapse/admin/v2/users/(?P<user_id>[^/]*)/delete_devices$"),
    93|     )
    94|     def __init__(self, hs):
    95|         self.hs = hs
    96|         self.auth = hs.get_auth()
    97|         self.device_handler = hs.get_device_handler()
    98|         self.store = hs.get_datastore()
    99|     async def on_POST(self, request, user_id):
   100|         await assert_requester_is_admin(self.auth, request)
   101|         target_user = UserID.from_string(user_id)
   102|         if not self.hs.is_mine(target_user):
   103|             raise SynapseError(400, "Can only lookup local users")
   104|         u = await self.store.get_user_by_id(target_user.to_string())
   105|         if u is None:
   106|             raise NotFoundError("Unknown user")
   107|         body = parse_json_object_from_request(request, allow_empty_body=False)
   108|         assert_params_in_dict(body, ["devices"])
   109|         await self.device_handler.delete_devices(
   110|             target_user.to_string(), body["devices"]
   111|         )
   112|         return 200, {}


# ====================================================================
# FILE: synapse/rest/admin/purge_room_servlet.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| import re
     2| from synapse.http.servlet import (
     3|     RestServlet,
     4|     assert_params_in_dict,
     5|     parse_json_object_from_request,
     6| )
     7| from synapse.rest.admin import assert_requester_is_admin
     8| class PurgeRoomServlet(RestServlet):
     9|     """Servlet which will remove all trace of a room from the database
    10|     POST /_synapse/admin/v1/purge_room
    11|     {
    12|         "room_id": "!room:id"
    13|     }
    14|     returns:
    15|     {}
    16|     """
    17|     PATTERNS = (re.compile("^/_synapse/admin/v1/purge_room$"),)
    18|     def __init__(self, hs):
    19|         """
    20|         Args:
    21|             hs (synapse.server.HomeServer): server
    22|         """
    23|         self.hs = hs
    24|         self.auth = hs.get_auth()
    25|         self.pagination_handler = hs.get_pagination_handler()
    26|     async def on_POST(self, request):
    27|         await assert_requester_is_admin(self.auth, request)
    28|         body = parse_json_object_from_request(request)
    29|         assert_params_in_dict(body, ("room_id",))
    30|         await self.pagination_handler.purge_room(body["room_id"])
    31|         return 200, {}


# ====================================================================
# FILE: synapse/rest/admin/server_notice_servlet.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-63 ---
     1| import re
     2| from synapse.api.constants import EventTypes
     3| from synapse.api.errors import SynapseError
     4| from synapse.http.servlet import (
     5|     RestServlet,
     6|     assert_params_in_dict,
     7|     parse_json_object_from_request,
     8| )
     9| from synapse.rest.admin import assert_requester_is_admin
    10| from synapse.rest.client.transactions import HttpTransactionCache
    11| from synapse.types import UserID
    12| class SendServerNoticeServlet(RestServlet):
    13|     """Servlet which will send a server notice to a given user
    14|     POST /_synapse/admin/v1/send_server_notice
    15|     {
    16|         "user_id": "@target_user:server_name",
    17|         "content": {
    18|             "msgtype": "m.text",
    19|             "body": "This is my message"
    20|         }
    21|     }
    22|     returns:
    23|     {
    24|         "event_id": "$1895723857jgskldgujpious"
    25|     }
    26|     """
    27|     def __init__(self, hs):
    28|         """
    29|         Args:
    30|             hs (synapse.server.HomeServer): server
    31|         """
    32|         self.hs = hs
    33|         self.auth = hs.get_auth()
    34|         self.txns = HttpTransactionCache(hs)
    35|         self.snm = hs.get_server_notices_manager()
    36|     def register(self, json_resource):
    37|         PATTERN = "^/_synapse/admin/v1/send_server_notice"
    38|         json_resource.register_paths(
    39|             "POST", (re.compile(PATTERN + "$"),), self.on_POST, self.__class__.__name__
    40|         )
    41|         json_resource.register_paths(
    42|             "PUT",
    43|             (re.compile(PATTERN + "/(?P<txn_id>[^/]*)$"),),
    44|             self.on_PUT,
    45|             self.__class__.__name__,
    46|         )
    47|     async def on_POST(self, request, txn_id=None):
    48|         await assert_requester_is_admin(self.auth, request)
    49|         body = parse_json_object_from_request(request)
    50|         assert_params_in_dict(body, ("user_id", "content"))
    51|         event_type = body.get("type", EventTypes.Message)
    52|         state_key = body.get("state_key")
    53|         if not self.snm.is_enabled():
    54|             raise SynapseError(400, "Server notices are not enabled on this server")
    55|         user_id = body["user_id"]
    56|         UserID.from_string(user_id)
    57|         if not self.hs.is_mine_id(user_id):
    58|             raise SynapseError(400, "Server notices can only be sent to local users")
    59|         event = await self.snm.send_notice(
    60|             user_id=body["user_id"],
    61|             type=event_type,
    62|             state_key=state_key,
    63|             event_content=body["content"],


# ====================================================================
# FILE: synapse/rest/admin/users.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 1-92 ---
     1| import hashlib
     2| import hmac
     3| import logging
     4| import re
     5| from http import HTTPStatus
     6| from synapse.api.constants import UserTypes
     7| from synapse.api.errors import Codes, NotFoundError, SynapseError
     8| from synapse.http.servlet import (
     9|     RestServlet,
    10|     assert_params_in_dict,
    11|     parse_boolean,
    12|     parse_integer,
    13|     parse_json_object_from_request,
    14|     parse_string,
    15| )
    16| from synapse.rest.admin._base import (
    17|     assert_requester_is_admin,
    18|     assert_user_is_admin,
    19|     historical_admin_path_patterns,
    20| )
    21| from synapse.types import UserID
    22| logger = logging.getLogger(__name__)
    23| class UsersRestServlet(RestServlet):
    24|     PATTERNS = historical_admin_path_patterns("/users/(?P<user_id>[^/]*)$")
    25|     def __init__(self, hs):
    26|         self.hs = hs
    27|         self.store = hs.get_datastore()
    28|         self.auth = hs.get_auth()
    29|         self.admin_handler = hs.get_handlers().admin_handler
    30|     async def on_GET(self, request, user_id):
    31|         target_user = UserID.from_string(user_id)
    32|         await assert_requester_is_admin(self.auth, request)
    33|         if not self.hs.is_mine(target_user):
    34|             raise SynapseError(400, "Can only users a local user")
    35|         ret = await self.store.get_users()
    36|         return 200, ret
    37| class UsersRestServletV2(RestServlet):
    38|     PATTERNS = (re.compile("^/_synapse/admin/v2/users$"),)
    39|     """Get request to list all local users.
    40|     This needs user to have administrator access in Synapse.
    41|     GET /_synapse/admin/v2/users?from=0&limit=10&guests=false
    42|     returns:
    43|         200 OK with list of users if success otherwise an error.
    44|     The parameters `from` and `limit` are required only for pagination.
    45|     By default, a `limit` of 100 is used.
    46|     The parameter `user_id` can be used to filter by user id.
    47|     The parameter `name` can be used to filter by user id or display name.
    48|     The parameter `guests` can be used to exclude guest users.
    49|     The parameter `deactivated` can be used to include deactivated users.
    50|     """
    51|     def __init__(self, hs):
    52|         self.hs = hs
    53|         self.store = hs.get_datastore()
    54|         self.auth = hs.get_auth()
    55|         self.admin_handler = hs.get_handlers().admin_handler
    56|     async def on_GET(self, request):
    57|         await assert_requester_is_admin(self.auth, request)
    58|         start = parse_integer(request, "from", default=0)
    59|         limit = parse_integer(request, "limit", default=100)
    60|         user_id = parse_string(request, "user_id", default=None)
    61|         name = parse_string(request, "name", default=None)
    62|         guests = parse_boolean(request, "guests", default=True)
    63|         deactivated = parse_boolean(request, "deactivated", default=False)
    64|         users, total = await self.store.get_users_paginate(
    65|             start, limit, user_id, name, guests, deactivated
    66|         )
    67|         ret = {"users": users, "total": total}
    68|         if len(users) >= limit:
    69|             ret["next_token"] = str(start + len(users))
    70|         return 200, ret
    71| class UserRestServletV2(RestServlet):
    72|     PATTERNS = (re.compile("^/_synapse/admin/v2/users/(?P<user_id>[^/]+)$"),)
    73|     """Get request to list user details.
    74|     This needs user to have administrator access in Synapse.
    75|     GET /_synapse/admin/v2/users/<user_id>
    76|     returns:
    77|         200 OK with user details if success otherwise an error.
    78|     Put request to allow an administrator to add or modify a user.
    79|     This needs user to have administrator access in Synapse.
    80|     We use PUT instead of POST since we already know the id of the user
    81|     object to create. POST could be used to create guests.
    82|     PUT /_synapse/admin/v2/users/<user_id>
    83|     {
    84|         "password": "secret",
    85|         "displayname": "User"
    86|     }
    87|     returns:
    88|         201 OK with new user object if user was created or
    89|         200 OK with modified user object if user was modified
    90|         otherwise an error.
    91|     """
    92|     def __init__(self, hs):

# --- HUNK 2: Lines 463-508 ---
   463|     Get or set whether or not a user is a server administrator.
   464|     Note that only local users can be server administrators, and that an
   465|     administrator may not demote themselves.
   466|     Only server administrators can use this API.
   467|     Examples:
   468|         * Get
   469|             GET /_synapse/admin/v1/users/@nonadmin:example.com/admin
   470|             response on success:
   471|                 {
   472|                     "admin": false
   473|                 }
   474|         * Set
   475|             PUT /_synapse/admin/v1/users/@reivilibre:librepush.net/admin
   476|             request body:
   477|                 {
   478|                     "admin": true
   479|                 }
   480|             response on success:
   481|                 {}
   482|     """
   483|     PATTERNS = (re.compile("^/_synapse/admin/v1/users/(?P<user_id>[^/]*)/admin$"),)
   484|     def __init__(self, hs):
   485|         self.hs = hs
   486|         self.store = hs.get_datastore()
   487|         self.auth = hs.get_auth()
   488|     async def on_GET(self, request, user_id):
   489|         await assert_requester_is_admin(self.auth, request)
   490|         target_user = UserID.from_string(user_id)
   491|         if not self.hs.is_mine(target_user):
   492|             raise SynapseError(400, "Only local users can be admins of this homeserver")
   493|         is_admin = await self.store.is_server_admin(target_user)
   494|         return 200, {"admin": is_admin}
   495|     async def on_PUT(self, request, user_id):
   496|         requester = await self.auth.get_user_by_req(request)
   497|         await assert_user_is_admin(self.auth, requester.user)
   498|         auth_user = requester.user
   499|         target_user = UserID.from_string(user_id)
   500|         body = parse_json_object_from_request(request)
   501|         assert_params_in_dict(body, ["admin"])
   502|         if not self.hs.is_mine(target_user):
   503|             raise SynapseError(400, "Only local users can be admins of this homeserver")
   504|         set_admin_to = bool(body["admin"])
   505|         if target_user == auth_user and not set_admin_to:
   506|             raise SynapseError(400, "You may not demote yourself.")
   507|         await self.store.set_server_admin(target_user, set_admin_to)
   508|         return 200, {}


# ====================================================================
# FILE: synapse/rest/client/v1/directory.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-40 ---
     1| import logging
     2| from synapse.api.errors import (
     3|     AuthError,
     4|     Codes,
     5|     InvalidClientCredentialsError,
     6|     NotFoundError,
     7|     SynapseError,
     8| )
     9| from synapse.http.servlet import RestServlet, parse_json_object_from_request
    10| from synapse.rest.client.v2_alpha._base import client_patterns
    11| from synapse.types import RoomAlias
    12| logger = logging.getLogger(__name__)
    13| def register_servlets(hs, http_server):
    14|     ClientDirectoryServer(hs).register(http_server)
    15|     ClientDirectoryListServer(hs).register(http_server)
    16|     ClientAppserviceDirectoryListServer(hs).register(http_server)
    17| class ClientDirectoryServer(RestServlet):
    18|     PATTERNS = client_patterns("/directory/room/(?P<room_alias>[^/]*)$", v1=True)
    19|     def __init__(self, hs):
    20|         super(ClientDirectoryServer, self).__init__()
    21|         self.store = hs.get_datastore()
    22|         self.handlers = hs.get_handlers()
    23|         self.auth = hs.get_auth()
    24|     async def on_GET(self, request, room_alias):
    25|         room_alias = RoomAlias.from_string(room_alias)
    26|         dir_handler = self.handlers.directory_handler
    27|         res = await dir_handler.get_association(room_alias)
    28|         return 200, res
    29|     async def on_PUT(self, request, room_alias):
    30|         room_alias = RoomAlias.from_string(room_alias)
    31|         content = parse_json_object_from_request(request)
    32|         if "room_id" not in content:
    33|             raise SynapseError(
    34|                 400, 'Missing params: ["room_id"]', errcode=Codes.BAD_JSON
    35|             )
    36|         logger.debug("Got content: %s", content)
    37|         logger.debug("Got room name: %s", room_alias.to_string())
    38|         room_id = content["room_id"]
    39|         servers = content["servers"] if "servers" in content else None
    40|         logger.debug("Got room_id: %s", room_id)

# --- HUNK 2: Lines 55-122 ---
    55|             await dir_handler.delete_appservice_association(service, room_alias)
    56|             logger.info(
    57|                 "Application service at %s deleted alias %s",
    58|                 service.url,
    59|                 room_alias.to_string(),
    60|             )
    61|             return 200, {}
    62|         except InvalidClientCredentialsError:
    63|             pass
    64|         requester = await self.auth.get_user_by_req(request)
    65|         user = requester.user
    66|         room_alias = RoomAlias.from_string(room_alias)
    67|         await dir_handler.delete_association(requester, room_alias)
    68|         logger.info(
    69|             "User %s deleted alias %s", user.to_string(), room_alias.to_string()
    70|         )
    71|         return 200, {}
    72| class ClientDirectoryListServer(RestServlet):
    73|     PATTERNS = client_patterns("/directory/list/room/(?P<room_id>[^/]*)$", v1=True)
    74|     def __init__(self, hs):
    75|         super(ClientDirectoryListServer, self).__init__()
    76|         self.store = hs.get_datastore()
    77|         self.handlers = hs.get_handlers()
    78|         self.auth = hs.get_auth()
    79|     async def on_GET(self, request, room_id):
    80|         room = await self.store.get_room(room_id)
    81|         if room is None:
    82|             raise NotFoundError("Unknown room")
    83|         return 200, {"visibility": "public" if room["is_public"] else "private"}
    84|     async def on_PUT(self, request, room_id):
    85|         requester = await self.auth.get_user_by_req(request)
    86|         content = parse_json_object_from_request(request)
    87|         visibility = content.get("visibility", "public")
    88|         await self.handlers.directory_handler.edit_published_room_list(
    89|             requester, room_id, visibility
    90|         )
    91|         return 200, {}
    92|     async def on_DELETE(self, request, room_id):
    93|         requester = await self.auth.get_user_by_req(request)
    94|         await self.handlers.directory_handler.edit_published_room_list(
    95|             requester, room_id, "private"
    96|         )
    97|         return 200, {}
    98| class ClientAppserviceDirectoryListServer(RestServlet):
    99|     PATTERNS = client_patterns(
   100|         "/directory/list/appservice/(?P<network_id>[^/]*)/(?P<room_id>[^/]*)$", v1=True
   101|     )
   102|     def __init__(self, hs):
   103|         super(ClientAppserviceDirectoryListServer, self).__init__()
   104|         self.store = hs.get_datastore()
   105|         self.handlers = hs.get_handlers()
   106|         self.auth = hs.get_auth()
   107|     def on_PUT(self, request, network_id, room_id):
   108|         content = parse_json_object_from_request(request)
   109|         visibility = content.get("visibility", "public")
   110|         return self._edit(request, network_id, room_id, visibility)
   111|     def on_DELETE(self, request, network_id, room_id):
   112|         return self._edit(request, network_id, room_id, "private")
   113|     async def _edit(self, request, network_id, room_id, visibility):
   114|         requester = await self.auth.get_user_by_req(request)
   115|         if not requester.app_service:
   116|             raise AuthError(
   117|                 403, "Only appservices can edit the appservice published room list"
   118|             )
   119|         await self.handlers.directory_handler.edit_published_appservice_room_list(
   120|             requester.app_service.id, network_id, room_id, visibility
   121|         )
   122|         return 200, {}


# ====================================================================
# FILE: synapse/rest/client/v1/events.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-63 ---
     1| """This module contains REST servlets to do with event streaming, /events."""
     2| import logging
     3| from synapse.api.errors import SynapseError
     4| from synapse.http.servlet import RestServlet
     5| from synapse.rest.client.v2_alpha._base import client_patterns
     6| from synapse.streams.config import PaginationConfig
     7| logger = logging.getLogger(__name__)
     8| class EventStreamRestServlet(RestServlet):
     9|     PATTERNS = client_patterns("/events$", v1=True)
    10|     DEFAULT_LONGPOLL_TIME_MS = 30000
    11|     def __init__(self, hs):
    12|         super(EventStreamRestServlet, self).__init__()
    13|         self.event_stream_handler = hs.get_event_stream_handler()
    14|         self.auth = hs.get_auth()
    15|     async def on_GET(self, request):
    16|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    17|         is_guest = requester.is_guest
    18|         room_id = None
    19|         if is_guest:
    20|             if b"room_id" not in request.args:
    21|                 raise SynapseError(400, "Guest users must specify room_id param")
    22|         if b"room_id" in request.args:
    23|             room_id = request.args[b"room_id"][0].decode("ascii")
    24|         pagin_config = PaginationConfig.from_request(request)
    25|         timeout = EventStreamRestServlet.DEFAULT_LONGPOLL_TIME_MS
    26|         if b"timeout" in request.args:
    27|             try:
    28|                 timeout = int(request.args[b"timeout"][0])
    29|             except ValueError:
    30|                 raise SynapseError(400, "timeout must be in milliseconds.")
    31|         as_client_event = b"raw" not in request.args
    32|         chunk = await self.event_stream_handler.get_stream(
    33|             requester.user.to_string(),
    34|             pagin_config,
    35|             timeout=timeout,
    36|             as_client_event=as_client_event,
    37|             affect_presence=(not is_guest),
    38|             room_id=room_id,
    39|             is_guest=is_guest,
    40|         )
    41|         return 200, chunk
    42|     def on_OPTIONS(self, request):
    43|         return 200, {}
    44| class EventRestServlet(RestServlet):
    45|     PATTERNS = client_patterns("/events/(?P<event_id>[^/]*)$", v1=True)
    46|     def __init__(self, hs):
    47|         super(EventRestServlet, self).__init__()
    48|         self.clock = hs.get_clock()
    49|         self.event_handler = hs.get_event_handler()
    50|         self.auth = hs.get_auth()
    51|         self._event_serializer = hs.get_event_client_serializer()
    52|     async def on_GET(self, request, event_id):
    53|         requester = await self.auth.get_user_by_req(request)
    54|         event = await self.event_handler.get_event(requester.user, None, event_id)
    55|         time_now = self.clock.time_msec()
    56|         if event:
    57|             event = await self._event_serializer.serialize_event(event, time_now)
    58|             return 200, event
    59|         else:
    60|             return 404, "Event not found."
    61| def register_servlets(hs, http_server):
    62|     EventStreamRestServlet(hs).register(http_server)
    63|     EventRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v1/initial_sync.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-23 ---
     1| from synapse.http.servlet import RestServlet, parse_boolean
     2| from synapse.rest.client.v2_alpha._base import client_patterns
     3| from synapse.streams.config import PaginationConfig
     4| class InitialSyncRestServlet(RestServlet):
     5|     PATTERNS = client_patterns("/initialSync$", v1=True)
     6|     def __init__(self, hs):
     7|         super(InitialSyncRestServlet, self).__init__()
     8|         self.initial_sync_handler = hs.get_initial_sync_handler()
     9|         self.auth = hs.get_auth()
    10|     async def on_GET(self, request):
    11|         requester = await self.auth.get_user_by_req(request)
    12|         as_client_event = b"raw" not in request.args
    13|         pagination_config = PaginationConfig.from_request(request)
    14|         include_archived = parse_boolean(request, "archived", default=False)
    15|         content = await self.initial_sync_handler.snapshot_all_rooms(
    16|             user_id=requester.user.to_string(),
    17|             pagin_config=pagination_config,
    18|             as_client_event=as_client_event,
    19|             include_archived=include_archived,
    20|         )
    21|         return 200, content
    22| def register_servlets(hs, http_server):
    23|     InitialSyncRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v1/login.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 1-112 ---
     1| import logging
     2| from typing import Awaitable, Callable, Dict, Optional
     3| from synapse.api.errors import Codes, LoginError, SynapseError
     4| from synapse.api.ratelimiting import Ratelimiter
     5| from synapse.handlers.auth import (
     6|     convert_client_dict_legacy_fields_to_identifier,
     7|     login_id_phone_to_thirdparty,
     8| )
     9| from synapse.http.server import finish_request
    10| from synapse.http.servlet import (
    11|     RestServlet,
    12|     parse_json_object_from_request,
    13|     parse_string,
    14| )
    15| from synapse.http.site import SynapseRequest
    16| from synapse.rest.client.v2_alpha._base import client_patterns
    17| from synapse.rest.well_known import WellKnownBuilder
    18| from synapse.types import JsonDict, UserID
    19| from synapse.util.threepids import canonicalise_email
    20| logger = logging.getLogger(__name__)
    21| class LoginRestServlet(RestServlet):
    22|     PATTERNS = client_patterns("/login$", v1=True)
    23|     CAS_TYPE = "m.login.cas"
    24|     SSO_TYPE = "m.login.sso"
    25|     TOKEN_TYPE = "m.login.token"
    26|     JWT_TYPE = "org.matrix.login.jwt"
    27|     JWT_TYPE_DEPRECATED = "m.login.jwt"
    28|     def __init__(self, hs):
    29|         super(LoginRestServlet, self).__init__()
    30|         self.hs = hs
    31|         self.jwt_enabled = hs.config.jwt_enabled
    32|         self.jwt_secret = hs.config.jwt_secret
    33|         self.jwt_algorithm = hs.config.jwt_algorithm
    34|         self.jwt_issuer = hs.config.jwt_issuer
    35|         self.jwt_audiences = hs.config.jwt_audiences
    36|         self.saml2_enabled = hs.config.saml2_enabled
    37|         self.cas_enabled = hs.config.cas_enabled
    38|         self.oidc_enabled = hs.config.oidc_enabled
    39|         self.auth_handler = self.hs.get_auth_handler()
    40|         self.registration_handler = hs.get_registration_handler()
    41|         self.handlers = hs.get_handlers()
    42|         self._well_known_builder = WellKnownBuilder(hs)
    43|         self._address_ratelimiter = Ratelimiter(
    44|             clock=hs.get_clock(),
    45|             rate_hz=self.hs.config.rc_login_address.per_second,
    46|             burst_count=self.hs.config.rc_login_address.burst_count,
    47|         )
    48|         self._account_ratelimiter = Ratelimiter(
    49|             clock=hs.get_clock(),
    50|             rate_hz=self.hs.config.rc_login_account.per_second,
    51|             burst_count=self.hs.config.rc_login_account.burst_count,
    52|         )
    53|         self._failed_attempts_ratelimiter = Ratelimiter(
    54|             clock=hs.get_clock(),
    55|             rate_hz=self.hs.config.rc_login_failed_attempts.per_second,
    56|             burst_count=self.hs.config.rc_login_failed_attempts.burst_count,
    57|         )
    58|     def on_GET(self, request: SynapseRequest):
    59|         flows = []
    60|         if self.jwt_enabled:
    61|             flows.append({"type": LoginRestServlet.JWT_TYPE})
    62|             flows.append({"type": LoginRestServlet.JWT_TYPE_DEPRECATED})
    63|         if self.cas_enabled:
    64|             flows.append({"type": LoginRestServlet.CAS_TYPE})
    65|         if self.cas_enabled or self.saml2_enabled or self.oidc_enabled:
    66|             flows.append({"type": LoginRestServlet.SSO_TYPE})
    67|             flows.append({"type": LoginRestServlet.TOKEN_TYPE})
    68|         flows.extend(
    69|             ({"type": t} for t in self.auth_handler.get_supported_login_types())
    70|         )
    71|         return 200, {"flows": flows}
    72|     def on_OPTIONS(self, request: SynapseRequest):
    73|         return 200, {}
    74|     async def on_POST(self, request: SynapseRequest):
    75|         self._address_ratelimiter.ratelimit(request.getClientIP())
    76|         login_submission = parse_json_object_from_request(request)
    77|         try:
    78|             if self.jwt_enabled and (
    79|                 login_submission["type"] == LoginRestServlet.JWT_TYPE
    80|                 or login_submission["type"] == LoginRestServlet.JWT_TYPE_DEPRECATED
    81|             ):
    82|                 result = await self._do_jwt_login(login_submission)
    83|             elif login_submission["type"] == LoginRestServlet.TOKEN_TYPE:
    84|                 result = await self._do_token_login(login_submission)
    85|             else:
    86|                 result = await self._do_other_login(login_submission)
    87|         except KeyError:
    88|             raise SynapseError(400, "Missing JSON keys.")
    89|         well_known_data = self._well_known_builder.get_well_known()
    90|         if well_known_data:
    91|             result["well_known"] = well_known_data
    92|         return 200, result
    93|     async def _do_other_login(self, login_submission: JsonDict) -> Dict[str, str]:
    94|         """Handle non-token/saml/jwt logins
    95|         Args:
    96|             login_submission:
    97|         Returns:
    98|             HTTP response
    99|         """
   100|         logger.info(
   101|             "Got login request with identifier: %r, medium: %r, address: %r, user: %r",
   102|             login_submission.get("identifier"),
   103|             login_submission.get("medium"),
   104|             login_submission.get("address"),
   105|             login_submission.get("user"),
   106|         )
   107|         identifier = convert_client_dict_legacy_fields_to_identifier(login_submission)
   108|         if identifier["type"] == "m.id.phone":
   109|             identifier = login_id_phone_to_thirdparty(identifier)
   110|         if identifier["type"] == "m.id.thirdparty":
   111|             address = identifier.get("address")
   112|             medium = identifier.get("medium")

# --- HUNK 2: Lines 122-236 ---
   122|                 canonical_user_id,
   123|                 callback_3pid,
   124|             ) = await self.auth_handler.check_password_provider_3pid(
   125|                 medium, address, login_submission["password"]
   126|             )
   127|             if canonical_user_id:
   128|                 result = await self._complete_login(
   129|                     canonical_user_id, login_submission, callback_3pid
   130|                 )
   131|                 return result
   132|             user_id = await self.hs.get_datastore().get_user_id_by_threepid(
   133|                 medium, address
   134|             )
   135|             if not user_id:
   136|                 logger.warning(
   137|                     "unknown 3pid identifier medium %s, address %r", medium, address
   138|                 )
   139|                 self._failed_attempts_ratelimiter.can_do_action((medium, address))
   140|                 raise LoginError(403, "", errcode=Codes.FORBIDDEN)
   141|             identifier = {"type": "m.id.user", "user": user_id}
   142|         if identifier["type"] != "m.id.user":
   143|             raise SynapseError(400, "Unknown login identifier type")
   144|         if "user" not in identifier:
   145|             raise SynapseError(400, "User identifier is missing 'user' key")
   146|         if identifier["user"].startswith("@"):
   147|             qualified_user_id = identifier["user"]
   148|         else:
   149|             qualified_user_id = UserID(identifier["user"], self.hs.hostname).to_string()
   150|         self._failed_attempts_ratelimiter.ratelimit(
   151|             qualified_user_id.lower(), update=False
   152|         )
   153|         try:
   154|             canonical_user_id, callback = await self.auth_handler.validate_login(
   155|                 identifier["user"], login_submission
   156|             )
   157|         except LoginError:
   158|             self._failed_attempts_ratelimiter.can_do_action(qualified_user_id.lower())
   159|             raise
   160|         result = await self._complete_login(
   161|             canonical_user_id, login_submission, callback
   162|         )
   163|         return result
   164|     async def _complete_login(
   165|         self,
   166|         user_id: str,
   167|         login_submission: JsonDict,
   168|         callback: Optional[
   169|             Callable[[Dict[str, str]], Awaitable[Dict[str, str]]]
   170|         ] = None,
   171|         create_non_existent_users: bool = False,
   172|     ) -> Dict[str, str]:
   173|         """Called when we've successfully authed the user and now need to
   174|         actually login them in (e.g. create devices). This gets called on
   175|         all successful logins.
   176|         Applies the ratelimiting for successful login attempts against an
   177|         account.
   178|         Args:
   179|             user_id: ID of the user to register.
   180|             login_submission: Dictionary of login information.
   181|             callback: Callback function to run after registration.
   182|             create_non_existent_users: Whether to create the user if they don't
   183|                 exist. Defaults to False.
   184|         Returns:
   185|             result: Dictionary of account information after successful registration.
   186|         """
   187|         self._account_ratelimiter.ratelimit(user_id.lower())
   188|         if create_non_existent_users:
   189|             canonical_uid = await self.auth_handler.check_user_exists(user_id)
   190|             if not canonical_uid:
   191|                 canonical_uid = await self.registration_handler.register_user(
   192|                     localpart=UserID.from_string(user_id).localpart
   193|                 )
   194|             user_id = canonical_uid
   195|         device_id = login_submission.get("device_id")
   196|         initial_display_name = login_submission.get("initial_device_display_name")
   197|         device_id, access_token = await self.registration_handler.register_device(
   198|             user_id, device_id, initial_display_name
   199|         )
   200|         result = {
   201|             "user_id": user_id,
   202|             "access_token": access_token,
   203|             "home_server": self.hs.hostname,
   204|             "device_id": device_id,
   205|         }
   206|         if callback is not None:
   207|             await callback(result)
   208|         return result
   209|     async def _do_token_login(self, login_submission: JsonDict) -> Dict[str, str]:
   210|         token = login_submission["token"]
   211|         auth_handler = self.auth_handler
   212|         user_id = await auth_handler.validate_short_term_login_token_and_get_user_id(
   213|             token
   214|         )
   215|         result = await self._complete_login(user_id, login_submission)
   216|         return result
   217|     async def _do_jwt_login(self, login_submission: JsonDict) -> Dict[str, str]:
   218|         token = login_submission.get("token", None)
   219|         if token is None:
   220|             raise LoginError(
   221|                 403, "Token field for JWT is missing", errcode=Codes.FORBIDDEN
   222|             )
   223|         import jwt
   224|         try:
   225|             payload = jwt.decode(
   226|                 token,
   227|                 self.jwt_secret,
   228|                 algorithms=[self.jwt_algorithm],
   229|                 issuer=self.jwt_issuer,
   230|                 audience=self.jwt_audiences,
   231|             )
   232|         except jwt.PyJWTError as e:
   233|             raise LoginError(
   234|                 403, "JWT validation failed: %s" % (str(e),), errcode=Codes.FORBIDDEN,
   235|             )
   236|         user = payload.get("sub", None)

# --- HUNK 3: Lines 259-299 ---
   259|         Args:
   260|             request: The client request to redirect.
   261|             client_redirect_url: the URL that we should redirect the
   262|                 client to when everything is done
   263|         Returns:
   264|             URL to redirect to
   265|         """
   266|         raise NotImplementedError()
   267| class CasRedirectServlet(BaseSSORedirectServlet):
   268|     def __init__(self, hs):
   269|         self._cas_handler = hs.get_cas_handler()
   270|     async def get_sso_url(
   271|         self, request: SynapseRequest, client_redirect_url: bytes
   272|     ) -> bytes:
   273|         return self._cas_handler.get_redirect_url(
   274|             {"redirectUrl": client_redirect_url}
   275|         ).encode("ascii")
   276| class CasTicketServlet(RestServlet):
   277|     PATTERNS = client_patterns("/login/cas/ticket", v1=True)
   278|     def __init__(self, hs):
   279|         super(CasTicketServlet, self).__init__()
   280|         self._cas_handler = hs.get_cas_handler()
   281|     async def on_GET(self, request: SynapseRequest) -> None:
   282|         client_redirect_url = parse_string(request, "redirectUrl")
   283|         ticket = parse_string(request, "ticket", required=True)
   284|         session = parse_string(request, "session")
   285|         if not client_redirect_url and not session:
   286|             message = "Missing string query parameter redirectUrl or session"
   287|             raise SynapseError(400, message, errcode=Codes.MISSING_PARAM)
   288|         await self._cas_handler.handle_ticket(
   289|             request, ticket, client_redirect_url, session
   290|         )
   291| class SAMLRedirectServlet(BaseSSORedirectServlet):
   292|     PATTERNS = client_patterns("/login/sso/redirect", v1=True)
   293|     def __init__(self, hs):
   294|         self._saml_handler = hs.get_saml_handler()
   295|     async def get_sso_url(
   296|         self, request: SynapseRequest, client_redirect_url: bytes
   297|     ) -> bytes:
   298|         return self._saml_handler.handle_redirect_request(client_redirect_url)
   299| class OIDCRedirectServlet(BaseSSORedirectServlet):


# ====================================================================
# FILE: synapse/rest/client/v1/logout.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-41 ---
     1| import logging
     2| from synapse.http.servlet import RestServlet
     3| from synapse.rest.client.v2_alpha._base import client_patterns
     4| logger = logging.getLogger(__name__)
     5| class LogoutRestServlet(RestServlet):
     6|     PATTERNS = client_patterns("/logout$", v1=True)
     7|     def __init__(self, hs):
     8|         super(LogoutRestServlet, self).__init__()
     9|         self.auth = hs.get_auth()
    10|         self._auth_handler = hs.get_auth_handler()
    11|         self._device_handler = hs.get_device_handler()
    12|     def on_OPTIONS(self, request):
    13|         return 200, {}
    14|     async def on_POST(self, request):
    15|         requester = await self.auth.get_user_by_req(request, allow_expired=True)
    16|         if requester.device_id is None:
    17|             access_token = self.auth.get_access_token_from_request(request)
    18|             await self._auth_handler.delete_access_token(access_token)
    19|         else:
    20|             await self._device_handler.delete_device(
    21|                 requester.user.to_string(), requester.device_id
    22|             )
    23|         return 200, {}
    24| class LogoutAllRestServlet(RestServlet):
    25|     PATTERNS = client_patterns("/logout/all$", v1=True)
    26|     def __init__(self, hs):
    27|         super(LogoutAllRestServlet, self).__init__()
    28|         self.auth = hs.get_auth()
    29|         self._auth_handler = hs.get_auth_handler()
    30|         self._device_handler = hs.get_device_handler()
    31|     def on_OPTIONS(self, request):
    32|         return 200, {}
    33|     async def on_POST(self, request):
    34|         requester = await self.auth.get_user_by_req(request, allow_expired=True)
    35|         user_id = requester.user.to_string()
    36|         await self._device_handler.delete_all_devices_for_user(user_id)
    37|         await self._auth_handler.delete_access_tokens_for_user(user_id)
    38|         return 200, {}
    39| def register_servlets(hs, http_server):
    40|     LogoutRestServlet(hs).register(http_server)
    41|     LogoutAllRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v1/presence.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-33 ---
     1| """ This module contains REST servlets to do with presence: /presence/<paths>
     2| """
     3| import logging
     4| from synapse.api.errors import AuthError, SynapseError
     5| from synapse.handlers.presence import format_user_presence_state
     6| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     7| from synapse.rest.client.v2_alpha._base import client_patterns
     8| from synapse.types import UserID
     9| logger = logging.getLogger(__name__)
    10| class PresenceStatusRestServlet(RestServlet):
    11|     PATTERNS = client_patterns("/presence/(?P<user_id>[^/]*)/status", v1=True)
    12|     def __init__(self, hs):
    13|         super(PresenceStatusRestServlet, self).__init__()
    14|         self.hs = hs
    15|         self.presence_handler = hs.get_presence_handler()
    16|         self.clock = hs.get_clock()
    17|         self.auth = hs.get_auth()
    18|     async def on_GET(self, request, user_id):
    19|         requester = await self.auth.get_user_by_req(request)
    20|         user = UserID.from_string(user_id)
    21|         if requester.user != user:
    22|             allowed = await self.presence_handler.is_visible(
    23|                 observed_user=user, observer_user=requester.user
    24|             )
    25|             if not allowed:
    26|                 raise AuthError(403, "You are not allowed to see their presence.")
    27|         state = await self.presence_handler.get_state(target_user=user)
    28|         state = format_user_presence_state(
    29|             state, self.clock.time_msec(), include_user_id=False
    30|         )
    31|         return 200, state
    32|     async def on_PUT(self, request, user_id):
    33|         requester = await self.auth.get_user_by_req(request)


# ====================================================================
# FILE: synapse/rest/client/v1/profile.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-97 ---
     1| """ This module contains REST servlets to do with profile: /profile/<paths> """
     2| from synapse.api.errors import Codes, SynapseError
     3| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     4| from synapse.rest.client.v2_alpha._base import client_patterns
     5| from synapse.types import UserID
     6| class ProfileDisplaynameRestServlet(RestServlet):
     7|     PATTERNS = client_patterns("/profile/(?P<user_id>[^/]*)/displayname", v1=True)
     8|     def __init__(self, hs):
     9|         super(ProfileDisplaynameRestServlet, self).__init__()
    10|         self.hs = hs
    11|         self.profile_handler = hs.get_profile_handler()
    12|         self.auth = hs.get_auth()
    13|     async def on_GET(self, request, user_id):
    14|         requester_user = None
    15|         if self.hs.config.require_auth_for_profile_requests:
    16|             requester = await self.auth.get_user_by_req(request)
    17|             requester_user = requester.user
    18|         user = UserID.from_string(user_id)
    19|         await self.profile_handler.check_profile_query_allowed(user, requester_user)
    20|         displayname = await self.profile_handler.get_displayname(user)
    21|         ret = {}
    22|         if displayname is not None:
    23|             ret["displayname"] = displayname
    24|         return 200, ret
    25|     async def on_PUT(self, request, user_id):
    26|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    27|         user = UserID.from_string(user_id)
    28|         is_admin = await self.auth.is_server_admin(requester.user)
    29|         content = parse_json_object_from_request(request)
    30|         try:
    31|             new_name = content["displayname"]
    32|         except Exception:
    33|             return 400, "Unable to parse name"
    34|         await self.profile_handler.set_displayname(user, requester, new_name, is_admin)
    35|         return 200, {}
    36|     def on_OPTIONS(self, request, user_id):
    37|         return 200, {}
    38| class ProfileAvatarURLRestServlet(RestServlet):
    39|     PATTERNS = client_patterns("/profile/(?P<user_id>[^/]*)/avatar_url", v1=True)
    40|     def __init__(self, hs):
    41|         super(ProfileAvatarURLRestServlet, self).__init__()
    42|         self.hs = hs
    43|         self.profile_handler = hs.get_profile_handler()
    44|         self.auth = hs.get_auth()
    45|     async def on_GET(self, request, user_id):
    46|         requester_user = None
    47|         if self.hs.config.require_auth_for_profile_requests:
    48|             requester = await self.auth.get_user_by_req(request)
    49|             requester_user = requester.user
    50|         user = UserID.from_string(user_id)
    51|         await self.profile_handler.check_profile_query_allowed(user, requester_user)
    52|         avatar_url = await self.profile_handler.get_avatar_url(user)
    53|         ret = {}
    54|         if avatar_url is not None:
    55|             ret["avatar_url"] = avatar_url
    56|         return 200, ret
    57|     async def on_PUT(self, request, user_id):
    58|         requester = await self.auth.get_user_by_req(request)
    59|         user = UserID.from_string(user_id)
    60|         is_admin = await self.auth.is_server_admin(requester.user)
    61|         content = parse_json_object_from_request(request)
    62|         try:
    63|             new_avatar_url = content["avatar_url"]
    64|         except KeyError:
    65|             raise SynapseError(
    66|                 400, "Missing key 'avatar_url'", errcode=Codes.MISSING_PARAM
    67|             )
    68|         await self.profile_handler.set_avatar_url(
    69|             user, requester, new_avatar_url, is_admin
    70|         )
    71|         return 200, {}
    72|     def on_OPTIONS(self, request, user_id):
    73|         return 200, {}
    74| class ProfileRestServlet(RestServlet):
    75|     PATTERNS = client_patterns("/profile/(?P<user_id>[^/]*)", v1=True)
    76|     def __init__(self, hs):
    77|         super(ProfileRestServlet, self).__init__()
    78|         self.hs = hs
    79|         self.profile_handler = hs.get_profile_handler()
    80|         self.auth = hs.get_auth()
    81|     async def on_GET(self, request, user_id):
    82|         requester_user = None
    83|         if self.hs.config.require_auth_for_profile_requests:
    84|             requester = await self.auth.get_user_by_req(request)
    85|             requester_user = requester.user
    86|         user = UserID.from_string(user_id)
    87|         await self.profile_handler.check_profile_query_allowed(user, requester_user)
    88|         displayname = await self.profile_handler.get_displayname(user)
    89|         avatar_url = await self.profile_handler.get_avatar_url(user)
    90|         ret = {}
    91|         if displayname is not None:
    92|             ret["displayname"] = displayname
    93|         if avatar_url is not None:
    94|             ret["avatar_url"] = avatar_url
    95|         return 200, ret
    96| def register_servlets(hs, http_server):
    97|     ProfileDisplaynameRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v1/push_rule.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 3-43 ---
     3|     StoreError,
     4|     SynapseError,
     5|     UnrecognizedRequestError,
     6| )
     7| from synapse.http.servlet import (
     8|     RestServlet,
     9|     parse_json_value_from_request,
    10|     parse_string,
    11| )
    12| from synapse.push.baserules import BASE_RULE_IDS, NEW_RULE_IDS
    13| from synapse.push.clientformat import format_push_rules_for_user
    14| from synapse.push.rulekinds import PRIORITY_CLASS_MAP
    15| from synapse.rest.client.v2_alpha._base import client_patterns
    16| from synapse.storage.push_rule import InconsistentRuleException, RuleNotFoundException
    17| class PushRuleRestServlet(RestServlet):
    18|     PATTERNS = client_patterns("/(?P<path>pushrules/.*)$", v1=True)
    19|     SLIGHTLY_PEDANTIC_TRAILING_SLASH_ERROR = (
    20|         "Unrecognised request: You probably wanted a trailing slash"
    21|     )
    22|     def __init__(self, hs):
    23|         super(PushRuleRestServlet, self).__init__()
    24|         self.auth = hs.get_auth()
    25|         self.store = hs.get_datastore()
    26|         self.notifier = hs.get_notifier()
    27|         self._is_worker = hs.config.worker_app is not None
    28|         self._users_new_default_push_rules = hs.config.users_new_default_push_rules
    29|     async def on_PUT(self, request, path):
    30|         if self._is_worker:
    31|             raise Exception("Cannot handle PUT /push_rules on worker")
    32|         spec = _rule_spec_from_path(path.split("/"))
    33|         try:
    34|             priority_class = _priority_class_from_spec(spec)
    35|         except InvalidRuleException as e:
    36|             raise SynapseError(400, str(e))
    37|         requester = await self.auth.get_user_by_req(request)
    38|         if "/" in spec["rule_id"] or "\\" in spec["rule_id"]:
    39|             raise SynapseError(400, "rule_id may not contain slashes")
    40|         content = parse_json_value_from_request(request)
    41|         user_id = requester.user.to_string()
    42|         if "attr" in spec:
    43|             await self.set_rule_attr(user_id, spec, content)

# --- HUNK 2: Lines 95-142 ---
    95|         rules = await self.store.get_push_rules_for_user(user_id)
    96|         rules = format_push_rules_for_user(requester.user, rules)
    97|         path = path.split("/")[1:]
    98|         if path == []:
    99|             raise UnrecognizedRequestError(
   100|                 PushRuleRestServlet.SLIGHTLY_PEDANTIC_TRAILING_SLASH_ERROR
   101|             )
   102|         if path[0] == "":
   103|             return 200, rules
   104|         elif path[0] == "global":
   105|             result = _filter_ruleset_with_path(rules["global"], path[1:])
   106|             return 200, result
   107|         else:
   108|             raise UnrecognizedRequestError()
   109|     def on_OPTIONS(self, request, path):
   110|         return 200, {}
   111|     def notify_user(self, user_id):
   112|         stream_id = self.store.get_max_push_rules_stream_id()
   113|         self.notifier.on_new_event("push_rules_key", stream_id, users=[user_id])
   114|     async def set_rule_attr(self, user_id, spec, val):
   115|         if spec["attr"] == "enabled":
   116|             if isinstance(val, dict) and "enabled" in val:
   117|                 val = val["enabled"]
   118|             if not isinstance(val, bool):
   119|                 raise SynapseError(400, "Value for 'enabled' must be boolean")
   120|             namespaced_rule_id = _namespaced_rule_id_from_spec(spec)
   121|             return await self.store.set_push_rule_enabled(
   122|                 user_id, namespaced_rule_id, val
   123|             )
   124|         elif spec["attr"] == "actions":
   125|             actions = val.get("actions")
   126|             _check_actions(actions)
   127|             namespaced_rule_id = _namespaced_rule_id_from_spec(spec)
   128|             rule_id = spec["rule_id"]
   129|             is_default_rule = rule_id.startswith(".")
   130|             if is_default_rule:
   131|                 if user_id in self._users_new_default_push_rules:
   132|                     rule_ids = NEW_RULE_IDS
   133|                 else:
   134|                     rule_ids = BASE_RULE_IDS
   135|                 if namespaced_rule_id not in rule_ids:
   136|                     raise SynapseError(404, "Unknown rule %r" % (namespaced_rule_id,))
   137|             return await self.store.set_push_rule_actions(
   138|                 user_id, namespaced_rule_id, actions, is_default_rule
   139|             )
   140|         else:
   141|             raise UnrecognizedRequestError()
   142| def _rule_spec_from_path(path):


# ====================================================================
# FILE: synapse/rest/client/v1/pusher.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 6-62 ---
     6|     assert_params_in_dict,
     7|     parse_json_object_from_request,
     8|     parse_string,
     9| )
    10| from synapse.push import PusherConfigException
    11| from synapse.rest.client.v2_alpha._base import client_patterns
    12| logger = logging.getLogger(__name__)
    13| ALLOWED_KEYS = {
    14|     "app_display_name",
    15|     "app_id",
    16|     "data",
    17|     "device_display_name",
    18|     "kind",
    19|     "lang",
    20|     "profile_tag",
    21|     "pushkey",
    22| }
    23| class PushersRestServlet(RestServlet):
    24|     PATTERNS = client_patterns("/pushers$", v1=True)
    25|     def __init__(self, hs):
    26|         super(PushersRestServlet, self).__init__()
    27|         self.hs = hs
    28|         self.auth = hs.get_auth()
    29|     async def on_GET(self, request):
    30|         requester = await self.auth.get_user_by_req(request)
    31|         user = requester.user
    32|         pushers = await self.hs.get_datastore().get_pushers_by_user_id(user.to_string())
    33|         filtered_pushers = [
    34|             {k: v for k, v in p.items() if k in ALLOWED_KEYS} for p in pushers
    35|         ]
    36|         return 200, {"pushers": filtered_pushers}
    37|     def on_OPTIONS(self, _):
    38|         return 200, {}
    39| class PushersSetRestServlet(RestServlet):
    40|     PATTERNS = client_patterns("/pushers/set$", v1=True)
    41|     def __init__(self, hs):
    42|         super(PushersSetRestServlet, self).__init__()
    43|         self.hs = hs
    44|         self.auth = hs.get_auth()
    45|         self.notifier = hs.get_notifier()
    46|         self.pusher_pool = self.hs.get_pusherpool()
    47|     async def on_POST(self, request):
    48|         requester = await self.auth.get_user_by_req(request)
    49|         user = requester.user
    50|         content = parse_json_object_from_request(request)
    51|         if (
    52|             "pushkey" in content
    53|             and "app_id" in content
    54|             and "kind" in content
    55|             and content["kind"] is None
    56|         ):
    57|             await self.pusher_pool.remove_pusher(
    58|                 content["app_id"], content["pushkey"], user_id=user.to_string()
    59|             )
    60|             return 200, {}
    61|         assert_params_in_dict(
    62|             content,

# --- HUNK 2: Lines 92-132 ---
    92|                 pushkey=content["pushkey"],
    93|                 lang=content["lang"],
    94|                 data=content["data"],
    95|                 profile_tag=content.get("profile_tag", ""),
    96|             )
    97|         except PusherConfigException as pce:
    98|             raise SynapseError(
    99|                 400, "Config Error: " + str(pce), errcode=Codes.MISSING_PARAM
   100|             )
   101|         self.notifier.on_new_replication_data()
   102|         return 200, {}
   103|     def on_OPTIONS(self, _):
   104|         return 200, {}
   105| class PushersRemoveRestServlet(RestServlet):
   106|     """
   107|     To allow pusher to be delete by clicking a link (ie. GET request)
   108|     """
   109|     PATTERNS = client_patterns("/pushers/remove$", v1=True)
   110|     SUCCESS_HTML = b"<html><body>You have been unsubscribed</body><html>"
   111|     def __init__(self, hs):
   112|         super(PushersRemoveRestServlet, self).__init__()
   113|         self.hs = hs
   114|         self.notifier = hs.get_notifier()
   115|         self.auth = hs.get_auth()
   116|         self.pusher_pool = self.hs.get_pusherpool()
   117|     async def on_GET(self, request):
   118|         requester = await self.auth.get_user_by_req(request, rights="delete_pusher")
   119|         user = requester.user
   120|         app_id = parse_string(request, "app_id", required=True)
   121|         pushkey = parse_string(request, "pushkey", required=True)
   122|         try:
   123|             await self.pusher_pool.remove_pusher(
   124|                 app_id=app_id, pushkey=pushkey, user_id=user.to_string()
   125|             )
   126|         except StoreError as se:
   127|             if se.code != 404:
   128|                 raise
   129|         self.notifier.on_new_replication_data()
   130|         respond_with_html_bytes(
   131|             request, 200, PushersRemoveRestServlet.SUCCESS_HTML,
   132|         )


# ====================================================================
# FILE: synapse/rest/client/v1/room.py
# Total hunks: 12
# ====================================================================
# --- HUNK 1: Lines 18-96 ---
    18|     RestServlet,
    19|     assert_params_in_dict,
    20|     parse_integer,
    21|     parse_json_object_from_request,
    22|     parse_string,
    23| )
    24| from synapse.logging.opentracing import set_tag
    25| from synapse.rest.client.transactions import HttpTransactionCache
    26| from synapse.rest.client.v2_alpha._base import client_patterns
    27| from synapse.storage.state import StateFilter
    28| from synapse.streams.config import PaginationConfig
    29| from synapse.types import RoomAlias, RoomID, StreamToken, ThirdPartyInstanceID, UserID
    30| from synapse.util import json_decoder
    31| from synapse.util.stringutils import random_string
    32| MYPY = False
    33| if MYPY:
    34|     import synapse.server
    35| logger = logging.getLogger(__name__)
    36| class TransactionRestServlet(RestServlet):
    37|     def __init__(self, hs):
    38|         super(TransactionRestServlet, self).__init__()
    39|         self.txns = HttpTransactionCache(hs)
    40| class RoomCreateRestServlet(TransactionRestServlet):
    41|     def __init__(self, hs):
    42|         super(RoomCreateRestServlet, self).__init__(hs)
    43|         self._room_creation_handler = hs.get_room_creation_handler()
    44|         self.auth = hs.get_auth()
    45|     def register(self, http_server):
    46|         PATTERNS = "/createRoom"
    47|         register_txn_path(self, PATTERNS, http_server)
    48|         http_server.register_paths(
    49|             "OPTIONS",
    50|             client_patterns("/rooms(?:/.*)?$", v1=True),
    51|             self.on_OPTIONS,
    52|             self.__class__.__name__,
    53|         )
    54|         http_server.register_paths(
    55|             "OPTIONS",
    56|             client_patterns("/createRoom(?:/.*)?$", v1=True),
    57|             self.on_OPTIONS,
    58|             self.__class__.__name__,
    59|         )
    60|     def on_PUT(self, request, txn_id):
    61|         set_tag("txn_id", txn_id)
    62|         return self.txns.fetch_or_execute_request(request, self.on_POST, request)
    63|     async def on_POST(self, request):
    64|         requester = await self.auth.get_user_by_req(request)
    65|         info, _ = await self._room_creation_handler.create_room(
    66|             requester, self.get_room_config(request)
    67|         )
    68|         return 200, info
    69|     def get_room_config(self, request):
    70|         user_supplied_config = parse_json_object_from_request(request)
    71|         return user_supplied_config
    72|     def on_OPTIONS(self, request):
    73|         return 200, {}
    74| class RoomStateEventRestServlet(TransactionRestServlet):
    75|     def __init__(self, hs):
    76|         super(RoomStateEventRestServlet, self).__init__(hs)
    77|         self.handlers = hs.get_handlers()
    78|         self.event_creation_handler = hs.get_event_creation_handler()
    79|         self.room_member_handler = hs.get_room_member_handler()
    80|         self.message_handler = hs.get_message_handler()
    81|         self.auth = hs.get_auth()
    82|     def register(self, http_server):
    83|         no_state_key = "/rooms/(?P<room_id>[^/]*)/state/(?P<event_type>[^/]*)$"
    84|         state_key = (
    85|             "/rooms/(?P<room_id>[^/]*)/state/"
    86|             "(?P<event_type>[^/]*)/(?P<state_key>[^/]*)$"
    87|         )
    88|         http_server.register_paths(
    89|             "GET",
    90|             client_patterns(state_key, v1=True),
    91|             self.on_GET,
    92|             self.__class__.__name__,
    93|         )
    94|         http_server.register_paths(
    95|             "PUT",
    96|             client_patterns(state_key, v1=True),

# --- HUNK 2: Lines 153-231 ---
   153|                     target=UserID.from_string(state_key),
   154|                     room_id=room_id,
   155|                     action=membership,
   156|                     content=content,
   157|                 )
   158|             else:
   159|                 (
   160|                     event,
   161|                     _,
   162|                 ) = await self.event_creation_handler.create_and_send_nonmember_event(
   163|                     requester, event_dict, txn_id=txn_id
   164|                 )
   165|                 event_id = event.event_id
   166|         except ShadowBanError:
   167|             event_id = "$" + random_string(43)
   168|         set_tag("event_id", event_id)
   169|         ret = {"event_id": event_id}
   170|         return 200, ret
   171| class RoomSendEventRestServlet(TransactionRestServlet):
   172|     def __init__(self, hs):
   173|         super(RoomSendEventRestServlet, self).__init__(hs)
   174|         self.event_creation_handler = hs.get_event_creation_handler()
   175|         self.auth = hs.get_auth()
   176|     def register(self, http_server):
   177|         PATTERNS = "/rooms/(?P<room_id>[^/]*)/send/(?P<event_type>[^/]*)"
   178|         register_txn_path(self, PATTERNS, http_server, with_get=True)
   179|     async def on_POST(self, request, room_id, event_type, txn_id=None):
   180|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   181|         content = parse_json_object_from_request(request)
   182|         event_dict = {
   183|             "type": event_type,
   184|             "content": content,
   185|             "room_id": room_id,
   186|             "sender": requester.user.to_string(),
   187|         }
   188|         if b"ts" in request.args and requester.app_service:
   189|             event_dict["origin_server_ts"] = parse_integer(request, "ts", 0)
   190|         try:
   191|             (
   192|                 event,
   193|                 _,
   194|             ) = await self.event_creation_handler.create_and_send_nonmember_event(
   195|                 requester, event_dict, txn_id=txn_id
   196|             )
   197|             event_id = event.event_id
   198|         except ShadowBanError:
   199|             event_id = "$" + random_string(43)
   200|         set_tag("event_id", event_id)
   201|         return 200, {"event_id": event_id}
   202|     def on_GET(self, request, room_id, event_type, txn_id):
   203|         return 200, "Not implemented"
   204|     def on_PUT(self, request, room_id, event_type, txn_id):
   205|         set_tag("txn_id", txn_id)
   206|         return self.txns.fetch_or_execute_request(
   207|             request, self.on_POST, request, room_id, event_type, txn_id
   208|         )
   209| class JoinRoomAliasServlet(TransactionRestServlet):
   210|     def __init__(self, hs):
   211|         super(JoinRoomAliasServlet, self).__init__(hs)
   212|         self.room_member_handler = hs.get_room_member_handler()
   213|         self.auth = hs.get_auth()
   214|     def register(self, http_server):
   215|         PATTERNS = "/join/(?P<room_identifier>[^/]*)"
   216|         register_txn_path(self, PATTERNS, http_server)
   217|     async def on_POST(self, request, room_identifier, txn_id=None):
   218|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   219|         try:
   220|             content = parse_json_object_from_request(request)
   221|         except Exception:
   222|             content = {}
   223|         if RoomID.is_valid(room_identifier):
   224|             room_id = room_identifier
   225|             try:
   226|                 remote_room_hosts = [
   227|                     x.decode("ascii") for x in request.args[b"server_name"]
   228|                 ]  # type: Optional[List[str]]
   229|             except Exception:
   230|                 remote_room_hosts = None
   231|         elif RoomAlias.is_valid(room_identifier):

# --- HUNK 3: Lines 239-279 ---
   239|             )
   240|         await self.room_member_handler.update_membership(
   241|             requester=requester,
   242|             target=requester.user,
   243|             room_id=room_id,
   244|             action="join",
   245|             txn_id=txn_id,
   246|             remote_room_hosts=remote_room_hosts,
   247|             content=content,
   248|             third_party_signed=content.get("third_party_signed", None),
   249|         )
   250|         return 200, {"room_id": room_id}
   251|     def on_PUT(self, request, room_identifier, txn_id):
   252|         set_tag("txn_id", txn_id)
   253|         return self.txns.fetch_or_execute_request(
   254|             request, self.on_POST, request, room_identifier, txn_id
   255|         )
   256| class PublicRoomListRestServlet(TransactionRestServlet):
   257|     PATTERNS = client_patterns("/publicRooms$", v1=True)
   258|     def __init__(self, hs):
   259|         super(PublicRoomListRestServlet, self).__init__(hs)
   260|         self.hs = hs
   261|         self.auth = hs.get_auth()
   262|     async def on_GET(self, request):
   263|         server = parse_string(request, "server", default=None)
   264|         try:
   265|             await self.auth.get_user_by_req(request, allow_guest=True)
   266|         except InvalidClientCredentialsError as e:
   267|             if not self.hs.config.allow_public_rooms_without_auth:
   268|                 raise
   269|             if server:
   270|                 raise e
   271|             else:
   272|                 pass
   273|         limit = parse_integer(request, "limit", 0)
   274|         since_token = parse_string(request, "since", None)
   275|         if limit == 0:
   276|             limit = None
   277|         handler = self.hs.get_room_list_handler()
   278|         if server and server != self.hs.config.server_name:
   279|             try:

# --- HUNK 4: Lines 315-535 ---
   315|                     server,
   316|                     limit=limit,
   317|                     since_token=since_token,
   318|                     search_filter=search_filter,
   319|                     include_all_networks=include_all_networks,
   320|                     third_party_instance_id=third_party_instance_id,
   321|                 )
   322|             except HttpResponseException as e:
   323|                 raise e.to_synapse_error()
   324|         else:
   325|             data = await handler.get_local_public_room_list(
   326|                 limit=limit,
   327|                 since_token=since_token,
   328|                 search_filter=search_filter,
   329|                 network_tuple=network_tuple,
   330|             )
   331|         return 200, data
   332| class RoomMemberListRestServlet(RestServlet):
   333|     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/members$", v1=True)
   334|     def __init__(self, hs):
   335|         super(RoomMemberListRestServlet, self).__init__()
   336|         self.message_handler = hs.get_message_handler()
   337|         self.auth = hs.get_auth()
   338|     async def on_GET(self, request, room_id):
   339|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   340|         handler = self.message_handler
   341|         at_token_string = parse_string(request, "at")
   342|         if at_token_string is None:
   343|             at_token = None
   344|         else:
   345|             at_token = StreamToken.from_string(at_token_string)
   346|         membership = parse_string(request, "membership")
   347|         not_membership = parse_string(request, "not_membership")
   348|         events = await handler.get_state_events(
   349|             room_id=room_id,
   350|             user_id=requester.user.to_string(),
   351|             at_token=at_token,
   352|             state_filter=StateFilter.from_types([(EventTypes.Member, None)]),
   353|         )
   354|         chunk = []
   355|         for event in events:
   356|             if (membership and event["content"].get("membership") != membership) or (
   357|                 not_membership and event["content"].get("membership") == not_membership
   358|             ):
   359|                 continue
   360|             chunk.append(event)
   361|         return 200, {"chunk": chunk}
   362| class JoinedRoomMemberListRestServlet(RestServlet):
   363|     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/joined_members$", v1=True)
   364|     def __init__(self, hs):
   365|         super(JoinedRoomMemberListRestServlet, self).__init__()
   366|         self.message_handler = hs.get_message_handler()
   367|         self.auth = hs.get_auth()
   368|     async def on_GET(self, request, room_id):
   369|         requester = await self.auth.get_user_by_req(request)
   370|         users_with_profile = await self.message_handler.get_joined_members(
   371|             requester, room_id
   372|         )
   373|         return 200, {"joined": users_with_profile}
   374| class RoomMessageListRestServlet(RestServlet):
   375|     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/messages$", v1=True)
   376|     def __init__(self, hs):
   377|         super(RoomMessageListRestServlet, self).__init__()
   378|         self.pagination_handler = hs.get_pagination_handler()
   379|         self.auth = hs.get_auth()
   380|     async def on_GET(self, request, room_id):
   381|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   382|         pagination_config = PaginationConfig.from_request(request, default_limit=10)
   383|         as_client_event = b"raw" not in request.args
   384|         filter_str = parse_string(request, b"filter", encoding="utf-8")
   385|         if filter_str:
   386|             filter_json = urlparse.unquote(filter_str)
   387|             event_filter = Filter(
   388|                 json_decoder.decode(filter_json)
   389|             )  # type: Optional[Filter]
   390|             if (
   391|                 event_filter
   392|                 and event_filter.filter_json.get("event_format", "client")
   393|                 == "federation"
   394|             ):
   395|                 as_client_event = False
   396|         else:
   397|             event_filter = None
   398|         msgs = await self.pagination_handler.get_messages(
   399|             room_id=room_id,
   400|             requester=requester,
   401|             pagin_config=pagination_config,
   402|             as_client_event=as_client_event,
   403|             event_filter=event_filter,
   404|         )
   405|         return 200, msgs
   406| class RoomStateRestServlet(RestServlet):
   407|     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/state$", v1=True)
   408|     def __init__(self, hs):
   409|         super(RoomStateRestServlet, self).__init__()
   410|         self.message_handler = hs.get_message_handler()
   411|         self.auth = hs.get_auth()
   412|     async def on_GET(self, request, room_id):
   413|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   414|         events = await self.message_handler.get_state_events(
   415|             room_id=room_id,
   416|             user_id=requester.user.to_string(),
   417|             is_guest=requester.is_guest,
   418|         )
   419|         return 200, events
   420| class RoomInitialSyncRestServlet(RestServlet):
   421|     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/initialSync$", v1=True)
   422|     def __init__(self, hs):
   423|         super(RoomInitialSyncRestServlet, self).__init__()
   424|         self.initial_sync_handler = hs.get_initial_sync_handler()
   425|         self.auth = hs.get_auth()
   426|     async def on_GET(self, request, room_id):
   427|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   428|         pagination_config = PaginationConfig.from_request(request)
   429|         content = await self.initial_sync_handler.room_initial_sync(
   430|             room_id=room_id, requester=requester, pagin_config=pagination_config
   431|         )
   432|         return 200, content
   433| class RoomEventServlet(RestServlet):
   434|     PATTERNS = client_patterns(
   435|         "/rooms/(?P<room_id>[^/]*)/event/(?P<event_id>[^/]*)$", v1=True
   436|     )
   437|     def __init__(self, hs):
   438|         super(RoomEventServlet, self).__init__()
   439|         self.clock = hs.get_clock()
   440|         self.event_handler = hs.get_event_handler()
   441|         self._event_serializer = hs.get_event_client_serializer()
   442|         self.auth = hs.get_auth()
   443|     async def on_GET(self, request, room_id, event_id):
   444|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   445|         try:
   446|             event = await self.event_handler.get_event(
   447|                 requester.user, room_id, event_id
   448|             )
   449|         except AuthError:
   450|             raise SynapseError(404, "Event not found.", errcode=Codes.NOT_FOUND)
   451|         time_now = self.clock.time_msec()
   452|         if event:
   453|             event = await self._event_serializer.serialize_event(event, time_now)
   454|             return 200, event
   455|         return SynapseError(404, "Event not found.", errcode=Codes.NOT_FOUND)
   456| class RoomEventContextServlet(RestServlet):
   457|     PATTERNS = client_patterns(
   458|         "/rooms/(?P<room_id>[^/]*)/context/(?P<event_id>[^/]*)$", v1=True
   459|     )
   460|     def __init__(self, hs):
   461|         super(RoomEventContextServlet, self).__init__()
   462|         self.clock = hs.get_clock()
   463|         self.room_context_handler = hs.get_room_context_handler()
   464|         self._event_serializer = hs.get_event_client_serializer()
   465|         self.auth = hs.get_auth()
   466|     async def on_GET(self, request, room_id, event_id):
   467|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   468|         limit = parse_integer(request, "limit", default=10)
   469|         filter_str = parse_string(request, b"filter", encoding="utf-8")
   470|         if filter_str:
   471|             filter_json = urlparse.unquote(filter_str)
   472|             event_filter = Filter(
   473|                 json_decoder.decode(filter_json)
   474|             )  # type: Optional[Filter]
   475|         else:
   476|             event_filter = None
   477|         results = await self.room_context_handler.get_event_context(
   478|             requester.user, room_id, event_id, limit, event_filter
   479|         )
   480|         if not results:
   481|             raise SynapseError(404, "Event not found.", errcode=Codes.NOT_FOUND)
   482|         time_now = self.clock.time_msec()
   483|         results["events_before"] = await self._event_serializer.serialize_events(
   484|             results["events_before"], time_now
   485|         )
   486|         results["event"] = await self._event_serializer.serialize_event(
   487|             results["event"], time_now
   488|         )
   489|         results["events_after"] = await self._event_serializer.serialize_events(
   490|             results["events_after"], time_now
   491|         )
   492|         results["state"] = await self._event_serializer.serialize_events(
   493|             results["state"], time_now
   494|         )
   495|         return 200, results
   496| class RoomForgetRestServlet(TransactionRestServlet):
   497|     def __init__(self, hs):
   498|         super(RoomForgetRestServlet, self).__init__(hs)
   499|         self.room_member_handler = hs.get_room_member_handler()
   500|         self.auth = hs.get_auth()
   501|     def register(self, http_server):
   502|         PATTERNS = "/rooms/(?P<room_id>[^/]*)/forget"
   503|         register_txn_path(self, PATTERNS, http_server)
   504|     async def on_POST(self, request, room_id, txn_id=None):
   505|         requester = await self.auth.get_user_by_req(request, allow_guest=False)
   506|         await self.room_member_handler.forget(user=requester.user, room_id=room_id)
   507|         return 200, {}
   508|     def on_PUT(self, request, room_id, txn_id):
   509|         set_tag("txn_id", txn_id)
   510|         return self.txns.fetch_or_execute_request(
   511|             request, self.on_POST, request, room_id, txn_id
   512|         )
   513| class RoomMembershipRestServlet(TransactionRestServlet):
   514|     def __init__(self, hs):
   515|         super(RoomMembershipRestServlet, self).__init__(hs)
   516|         self.room_member_handler = hs.get_room_member_handler()
   517|         self.auth = hs.get_auth()
   518|     def register(self, http_server):
   519|         PATTERNS = (
   520|             "/rooms/(?P<room_id>[^/]*)/"
   521|             "(?P<membership_action>join|invite|leave|ban|unban|kick)"
   522|         )
   523|         register_txn_path(self, PATTERNS, http_server)
   524|     async def on_POST(self, request, room_id, membership_action, txn_id=None):
   525|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   526|         if requester.is_guest and membership_action not in {
   527|             Membership.JOIN,
   528|             Membership.LEAVE,
   529|         }:
   530|             raise AuthError(403, "Guest access not allowed")
   531|         try:
   532|             content = parse_json_object_from_request(request)
   533|         except Exception:
   534|             content = {}
   535|         if membership_action == "invite" and self._has_3pid_invite_keys(content):

# --- HUNK 5: Lines 565-645 ---
   565|                 content=event_content,
   566|             )
   567|         except ShadowBanError:
   568|             pass
   569|         return_value = {}
   570|         if membership_action == "join":
   571|             return_value["room_id"] = room_id
   572|         return 200, return_value
   573|     def _has_3pid_invite_keys(self, content):
   574|         for key in {"id_server", "medium", "address"}:
   575|             if key not in content:
   576|                 return False
   577|         return True
   578|     def on_PUT(self, request, room_id, membership_action, txn_id):
   579|         set_tag("txn_id", txn_id)
   580|         return self.txns.fetch_or_execute_request(
   581|             request, self.on_POST, request, room_id, membership_action, txn_id
   582|         )
   583| class RoomRedactEventRestServlet(TransactionRestServlet):
   584|     def __init__(self, hs):
   585|         super(RoomRedactEventRestServlet, self).__init__(hs)
   586|         self.handlers = hs.get_handlers()
   587|         self.event_creation_handler = hs.get_event_creation_handler()
   588|         self.auth = hs.get_auth()
   589|     def register(self, http_server):
   590|         PATTERNS = "/rooms/(?P<room_id>[^/]*)/redact/(?P<event_id>[^/]*)"
   591|         register_txn_path(self, PATTERNS, http_server)
   592|     async def on_POST(self, request, room_id, event_id, txn_id=None):
   593|         requester = await self.auth.get_user_by_req(request)
   594|         content = parse_json_object_from_request(request)
   595|         try:
   596|             (
   597|                 event,
   598|                 _,
   599|             ) = await self.event_creation_handler.create_and_send_nonmember_event(
   600|                 requester,
   601|                 {
   602|                     "type": EventTypes.Redaction,
   603|                     "content": content,
   604|                     "room_id": room_id,
   605|                     "sender": requester.user.to_string(),
   606|                     "redacts": event_id,
   607|                 },
   608|                 txn_id=txn_id,
   609|             )
   610|             event_id = event.event_id
   611|         except ShadowBanError:
   612|             event_id = "$" + random_string(43)
   613|         set_tag("event_id", event_id)
   614|         return 200, {"event_id": event_id}
   615|     def on_PUT(self, request, room_id, event_id, txn_id):
   616|         set_tag("txn_id", txn_id)
   617|         return self.txns.fetch_or_execute_request(
   618|             request, self.on_POST, request, room_id, event_id, txn_id
   619|         )
   620| class RoomTypingRestServlet(RestServlet):
   621|     PATTERNS = client_patterns(
   622|         "/rooms/(?P<room_id>[^/]*)/typing/(?P<user_id>[^/]*)$", v1=True
   623|     )
   624|     def __init__(self, hs):
   625|         super(RoomTypingRestServlet, self).__init__()
   626|         self.presence_handler = hs.get_presence_handler()
   627|         self.typing_handler = hs.get_typing_handler()
   628|         self.auth = hs.get_auth()
   629|         self._is_typing_writer = (
   630|             hs.config.worker.writers.typing == hs.get_instance_name()
   631|         )
   632|     async def on_PUT(self, request, room_id, user_id):
   633|         requester = await self.auth.get_user_by_req(request)
   634|         if not self._is_typing_writer:
   635|             raise Exception("Got /typing request on instance that is not typing writer")
   636|         room_id = urlparse.unquote(room_id)
   637|         target_user = UserID.from_string(urlparse.unquote(user_id))
   638|         content = parse_json_object_from_request(request)
   639|         await self.presence_handler.bump_presence_active_time(requester.user)
   640|         timeout = min(content.get("timeout", 30000), 120000)
   641|         try:
   642|             if content["typing"]:
   643|                 await self.typing_handler.started_typing(
   644|                     target_user=target_user,
   645|                     requester=requester,

# --- HUNK 6: Lines 656-710 ---
   656| class RoomAliasListServlet(RestServlet):
   657|     PATTERNS = [
   658|         re.compile(
   659|             r"^/_matrix/client/unstable/org\.matrix\.msc2432"
   660|             r"/rooms/(?P<room_id>[^/]*)/aliases"
   661|         ),
   662|     ]
   663|     def __init__(self, hs: "synapse.server.HomeServer"):
   664|         super().__init__()
   665|         self.auth = hs.get_auth()
   666|         self.directory_handler = hs.get_handlers().directory_handler
   667|     async def on_GET(self, request, room_id):
   668|         requester = await self.auth.get_user_by_req(request)
   669|         alias_list = await self.directory_handler.get_aliases_for_room(
   670|             requester, room_id
   671|         )
   672|         return 200, {"aliases": alias_list}
   673| class SearchRestServlet(RestServlet):
   674|     PATTERNS = client_patterns("/search$", v1=True)
   675|     def __init__(self, hs):
   676|         super(SearchRestServlet, self).__init__()
   677|         self.handlers = hs.get_handlers()
   678|         self.auth = hs.get_auth()
   679|     async def on_POST(self, request):
   680|         requester = await self.auth.get_user_by_req(request)
   681|         content = parse_json_object_from_request(request)
   682|         batch = parse_string(request, "next_batch")
   683|         results = await self.handlers.search_handler.search(
   684|             requester.user, content, batch
   685|         )
   686|         return 200, results
   687| class JoinedRoomsRestServlet(RestServlet):
   688|     PATTERNS = client_patterns("/joined_rooms$", v1=True)
   689|     def __init__(self, hs):
   690|         super(JoinedRoomsRestServlet, self).__init__()
   691|         self.store = hs.get_datastore()
   692|         self.auth = hs.get_auth()
   693|     async def on_GET(self, request):
   694|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   695|         room_ids = await self.store.get_rooms_for_user(requester.user.to_string())
   696|         return 200, {"joined_rooms": list(room_ids)}
   697| def register_txn_path(servlet, regex_string, http_server, with_get=False):
   698|     """Registers a transaction-based path.
   699|     This registers two paths:
   700|         PUT regex_string/$txnid
   701|         POST regex_string
   702|     Args:
   703|         regex_string (str): The regex string to register. Must NOT have a
   704|         trailing $ as this string will be appended to.
   705|         http_server : The http_server to register paths with.
   706|         with_get: True to also register respective GET paths for the PUTs.
   707|     """
   708|     http_server.register_paths(
   709|         "POST",
   710|         client_patterns(regex_string + "$", v1=True),


# ====================================================================
# FILE: synapse/rest/client/v1/voip.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-29 ---
     1| import base64
     2| import hashlib
     3| import hmac
     4| from synapse.http.servlet import RestServlet
     5| from synapse.rest.client.v2_alpha._base import client_patterns
     6| class VoipRestServlet(RestServlet):
     7|     PATTERNS = client_patterns("/voip/turnServer$", v1=True)
     8|     def __init__(self, hs):
     9|         super(VoipRestServlet, self).__init__()
    10|         self.hs = hs
    11|         self.auth = hs.get_auth()
    12|     async def on_GET(self, request):
    13|         requester = await self.auth.get_user_by_req(
    14|             request, self.hs.config.turn_allow_guests
    15|         )
    16|         turnUris = self.hs.config.turn_uris
    17|         turnSecret = self.hs.config.turn_shared_secret
    18|         turnUsername = self.hs.config.turn_username
    19|         turnPassword = self.hs.config.turn_password
    20|         userLifetime = self.hs.config.turn_user_lifetime
    21|         if turnUris and turnSecret and userLifetime:
    22|             expiry = (self.hs.get_clock().time_msec() + userLifetime) / 1000
    23|             username = "%d:%s" % (expiry, requester.user.to_string())
    24|             mac = hmac.new(
    25|                 turnSecret.encode(), msg=username.encode(), digestmod=hashlib.sha1
    26|             )
    27|             password = base64.b64encode(mac.digest()).decode("ascii")
    28|         elif turnUris and turnUsername and turnPassword and userLifetime:
    29|             username = turnUsername


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/account.py
# Total hunks: 14
# ====================================================================
# --- HUNK 1: Lines 1-173 ---
     1| import logging
     2| import random
     3| from http import HTTPStatus
     4| from synapse.api.constants import LoginType
     5| from synapse.api.errors import (
     6|     Codes,
     7|     InteractiveAuthIncompleteError,
     8|     SynapseError,
     9|     ThreepidValidationError,
    10| )
    11| from synapse.config.emailconfig import ThreepidBehaviour
    12| from synapse.http.server import finish_request, respond_with_html
    13| from synapse.http.servlet import (
    14|     RestServlet,
    15|     assert_params_in_dict,
    16|     parse_json_object_from_request,
    17|     parse_string,
    18| )
    19| from synapse.push.mailer import Mailer
    20| from synapse.util.msisdn import phone_number_to_msisdn
    21| from synapse.util.stringutils import assert_valid_client_secret, random_string
    22| from synapse.util.threepids import canonicalise_email, check_3pid_allowed
    23| from ._base import client_patterns, interactive_auth_handler
    24| logger = logging.getLogger(__name__)
    25| class EmailPasswordRequestTokenRestServlet(RestServlet):
    26|     PATTERNS = client_patterns("/account/password/email/requestToken$")
    27|     def __init__(self, hs):
    28|         super(EmailPasswordRequestTokenRestServlet, self).__init__()
    29|         self.hs = hs
    30|         self.datastore = hs.get_datastore()
    31|         self.config = hs.config
    32|         self.identity_handler = hs.get_handlers().identity_handler
    33|         if self.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:
    34|             self.mailer = Mailer(
    35|                 hs=self.hs,
    36|                 app_name=self.config.email_app_name,
    37|                 template_html=self.config.email_password_reset_template_html,
    38|                 template_text=self.config.email_password_reset_template_text,
    39|             )
    40|     async def on_POST(self, request):
    41|         if self.config.threepid_behaviour_email == ThreepidBehaviour.OFF:
    42|             if self.config.local_threepid_handling_disabled_due_to_email_config:
    43|                 logger.warning(
    44|                     "User password resets have been disabled due to lack of email config"
    45|                 )
    46|             raise SynapseError(
    47|                 400, "Email-based password resets have been disabled on this server"
    48|             )
    49|         body = parse_json_object_from_request(request)
    50|         assert_params_in_dict(body, ["client_secret", "email", "send_attempt"])
    51|         client_secret = body["client_secret"]
    52|         assert_valid_client_secret(client_secret)
    53|         try:
    54|             email = canonicalise_email(body["email"])
    55|         except ValueError as e:
    56|             raise SynapseError(400, str(e))
    57|         send_attempt = body["send_attempt"]
    58|         next_link = body.get("next_link")  # Optional param
    59|         if not check_3pid_allowed(self.hs, "email", email):
    60|             raise SynapseError(
    61|                 403,
    62|                 "Your email domain is not authorized on this server",
    63|                 Codes.THREEPID_DENIED,
    64|             )
    65|         existing_user_id = await self.hs.get_datastore().get_user_id_by_threepid(
    66|             "email", email
    67|         )
    68|         if existing_user_id is None:
    69|             if self.config.request_token_inhibit_3pid_errors:
    70|                 await self.hs.clock.sleep(random.randint(1, 10) / 10)
    71|                 return 200, {"sid": random_string(16)}
    72|             raise SynapseError(400, "Email not found", Codes.THREEPID_NOT_FOUND)
    73|         if self.config.threepid_behaviour_email == ThreepidBehaviour.REMOTE:
    74|             assert self.hs.config.account_threepid_delegate_email
    75|             ret = await self.identity_handler.requestEmailToken(
    76|                 self.hs.config.account_threepid_delegate_email,
    77|                 email,
    78|                 client_secret,
    79|                 send_attempt,
    80|                 next_link,
    81|             )
    82|         else:
    83|             sid = await self.identity_handler.send_threepid_validation(
    84|                 email,
    85|                 client_secret,
    86|                 send_attempt,
    87|                 self.mailer.send_password_reset_mail,
    88|                 next_link,
    89|             )
    90|             ret = {"sid": sid}
    91|         return 200, ret
    92| class PasswordResetSubmitTokenServlet(RestServlet):
    93|     """Handles 3PID validation token submission"""
    94|     PATTERNS = client_patterns(
    95|         "/password_reset/(?P<medium>[^/]*)/submit_token$", releases=(), unstable=True
    96|     )
    97|     def __init__(self, hs):
    98|         """
    99|         Args:
   100|             hs (synapse.server.HomeServer): server
   101|         """
   102|         super(PasswordResetSubmitTokenServlet, self).__init__()
   103|         self.hs = hs
   104|         self.auth = hs.get_auth()
   105|         self.config = hs.config
   106|         self.clock = hs.get_clock()
   107|         self.store = hs.get_datastore()
   108|         if self.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:
   109|             self._failure_email_template = (
   110|                 self.config.email_password_reset_template_failure_html
   111|             )
   112|     async def on_GET(self, request, medium):
   113|         if medium != "email":
   114|             raise SynapseError(
   115|                 400, "This medium is currently not supported for password resets"
   116|             )
   117|         if self.config.threepid_behaviour_email == ThreepidBehaviour.OFF:
   118|             if self.config.local_threepid_handling_disabled_due_to_email_config:
   119|                 logger.warning(
   120|                     "Password reset emails have been disabled due to lack of an email config"
   121|                 )
   122|             raise SynapseError(
   123|                 400, "Email-based password resets are disabled on this server"
   124|             )
   125|         sid = parse_string(request, "sid", required=True)
   126|         token = parse_string(request, "token", required=True)
   127|         client_secret = parse_string(request, "client_secret", required=True)
   128|         assert_valid_client_secret(client_secret)
   129|         try:
   130|             next_link = await self.store.validate_threepid_session(
   131|                 sid, client_secret, token, self.clock.time_msec()
   132|             )
   133|             if next_link:
   134|                 if next_link.startswith("file:///"):
   135|                     logger.warning(
   136|                         "Not redirecting to next_link as it is a local file: address"
   137|                     )
   138|                 else:
   139|                     request.setResponseCode(302)
   140|                     request.setHeader("Location", next_link)
   141|                     finish_request(request)
   142|                     return None
   143|             html = self.config.email_password_reset_template_success_html_content
   144|             status_code = 200
   145|         except ThreepidValidationError as e:
   146|             status_code = e.code
   147|             template_vars = {"failure_reason": e.msg}
   148|             html = self._failure_email_template.render(**template_vars)
   149|         respond_with_html(request, status_code, html)
   150| class PasswordRestServlet(RestServlet):
   151|     PATTERNS = client_patterns("/account/password$")
   152|     def __init__(self, hs):
   153|         super(PasswordRestServlet, self).__init__()
   154|         self.hs = hs
   155|         self.auth = hs.get_auth()
   156|         self.auth_handler = hs.get_auth_handler()
   157|         self.datastore = self.hs.get_datastore()
   158|         self.password_policy_handler = hs.get_password_policy_handler()
   159|         self._set_password_handler = hs.get_set_password_handler()
   160|     @interactive_auth_handler
   161|     async def on_POST(self, request):
   162|         body = parse_json_object_from_request(request)
   163|         new_password = body.pop("new_password", None)
   164|         if new_password is not None:
   165|             if not isinstance(new_password, str) or len(new_password) > 512:
   166|                 raise SynapseError(400, "Invalid password")
   167|             self.password_policy_handler.validate_password(new_password)
   168|         if self.auth.has_access_token(request):
   169|             requester = await self.auth.get_user_by_req(request)
   170|             try:
   171|                 params, session_id = await self.auth_handler.validate_user_via_ui_auth(
   172|                     requester,
   173|                     request,

# --- HUNK 2: Lines 219-384 ---
   219|                 logger.error("Auth succeeded but no known type! %r", result.keys())
   220|                 raise SynapseError(500, "", Codes.UNKNOWN)
   221|         if new_password:
   222|             password_hash = await self.auth_handler.hash(new_password)
   223|         else:
   224|             password_hash = await self.auth_handler.get_session_data(
   225|                 session_id, "password_hash", None
   226|             )
   227|         if not password_hash:
   228|             raise SynapseError(400, "Missing params: password", Codes.MISSING_PARAM)
   229|         logout_devices = params.get("logout_devices", True)
   230|         await self._set_password_handler.set_password(
   231|             user_id, password_hash, logout_devices, requester
   232|         )
   233|         return 200, {}
   234|     def on_OPTIONS(self, _):
   235|         return 200, {}
   236| class DeactivateAccountRestServlet(RestServlet):
   237|     PATTERNS = client_patterns("/account/deactivate$")
   238|     def __init__(self, hs):
   239|         super(DeactivateAccountRestServlet, self).__init__()
   240|         self.hs = hs
   241|         self.auth = hs.get_auth()
   242|         self.auth_handler = hs.get_auth_handler()
   243|         self._deactivate_account_handler = hs.get_deactivate_account_handler()
   244|     @interactive_auth_handler
   245|     async def on_POST(self, request):
   246|         body = parse_json_object_from_request(request)
   247|         erase = body.get("erase", False)
   248|         if not isinstance(erase, bool):
   249|             raise SynapseError(
   250|                 HTTPStatus.BAD_REQUEST,
   251|                 "Param 'erase' must be a boolean, if given",
   252|                 Codes.BAD_JSON,
   253|             )
   254|         requester = await self.auth.get_user_by_req(request)
   255|         if requester.app_service:
   256|             await self._deactivate_account_handler.deactivate_account(
   257|                 requester.user.to_string(), erase
   258|             )
   259|             return 200, {}
   260|         await self.auth_handler.validate_user_via_ui_auth(
   261|             requester,
   262|             request,
   263|             body,
   264|             self.hs.get_ip_from_request(request),
   265|             "deactivate your account",
   266|         )
   267|         result = await self._deactivate_account_handler.deactivate_account(
   268|             requester.user.to_string(), erase, id_server=body.get("id_server")
   269|         )
   270|         if result:
   271|             id_server_unbind_result = "success"
   272|         else:
   273|             id_server_unbind_result = "no-support"
   274|         return 200, {"id_server_unbind_result": id_server_unbind_result}
   275| class EmailThreepidRequestTokenRestServlet(RestServlet):
   276|     PATTERNS = client_patterns("/account/3pid/email/requestToken$")
   277|     def __init__(self, hs):
   278|         super(EmailThreepidRequestTokenRestServlet, self).__init__()
   279|         self.hs = hs
   280|         self.config = hs.config
   281|         self.identity_handler = hs.get_handlers().identity_handler
   282|         self.store = self.hs.get_datastore()
   283|         if self.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:
   284|             self.mailer = Mailer(
   285|                 hs=self.hs,
   286|                 app_name=self.config.email_app_name,
   287|                 template_html=self.config.email_add_threepid_template_html,
   288|                 template_text=self.config.email_add_threepid_template_text,
   289|             )
   290|     async def on_POST(self, request):
   291|         if self.config.threepid_behaviour_email == ThreepidBehaviour.OFF:
   292|             if self.config.local_threepid_handling_disabled_due_to_email_config:
   293|                 logger.warning(
   294|                     "Adding emails have been disabled due to lack of an email config"
   295|                 )
   296|             raise SynapseError(
   297|                 400, "Adding an email to your account is disabled on this server"
   298|             )
   299|         body = parse_json_object_from_request(request)
   300|         assert_params_in_dict(body, ["client_secret", "email", "send_attempt"])
   301|         client_secret = body["client_secret"]
   302|         assert_valid_client_secret(client_secret)
   303|         try:
   304|             email = canonicalise_email(body["email"])
   305|         except ValueError as e:
   306|             raise SynapseError(400, str(e))
   307|         send_attempt = body["send_attempt"]
   308|         next_link = body.get("next_link")  # Optional param
   309|         if not check_3pid_allowed(self.hs, "email", email):
   310|             raise SynapseError(
   311|                 403,
   312|                 "Your email domain is not authorized on this server",
   313|                 Codes.THREEPID_DENIED,
   314|             )
   315|         existing_user_id = await self.store.get_user_id_by_threepid("email", email)
   316|         if existing_user_id is not None:
   317|             if self.config.request_token_inhibit_3pid_errors:
   318|                 await self.hs.clock.sleep(random.randint(1, 10) / 10)
   319|                 return 200, {"sid": random_string(16)}
   320|             raise SynapseError(400, "Email is already in use", Codes.THREEPID_IN_USE)
   321|         if self.config.threepid_behaviour_email == ThreepidBehaviour.REMOTE:
   322|             assert self.hs.config.account_threepid_delegate_email
   323|             ret = await self.identity_handler.requestEmailToken(
   324|                 self.hs.config.account_threepid_delegate_email,
   325|                 email,
   326|                 client_secret,
   327|                 send_attempt,
   328|                 next_link,
   329|             )
   330|         else:
   331|             sid = await self.identity_handler.send_threepid_validation(
   332|                 email,
   333|                 client_secret,
   334|                 send_attempt,
   335|                 self.mailer.send_add_threepid_mail,
   336|                 next_link,
   337|             )
   338|             ret = {"sid": sid}
   339|         return 200, ret
   340| class MsisdnThreepidRequestTokenRestServlet(RestServlet):
   341|     PATTERNS = client_patterns("/account/3pid/msisdn/requestToken$")
   342|     def __init__(self, hs):
   343|         self.hs = hs
   344|         super(MsisdnThreepidRequestTokenRestServlet, self).__init__()
   345|         self.store = self.hs.get_datastore()
   346|         self.identity_handler = hs.get_handlers().identity_handler
   347|     async def on_POST(self, request):
   348|         body = parse_json_object_from_request(request)
   349|         assert_params_in_dict(
   350|             body, ["client_secret", "country", "phone_number", "send_attempt"]
   351|         )
   352|         client_secret = body["client_secret"]
   353|         assert_valid_client_secret(client_secret)
   354|         country = body["country"]
   355|         phone_number = body["phone_number"]
   356|         send_attempt = body["send_attempt"]
   357|         next_link = body.get("next_link")  # Optional param
   358|         msisdn = phone_number_to_msisdn(country, phone_number)
   359|         if not check_3pid_allowed(self.hs, "msisdn", msisdn):
   360|             raise SynapseError(
   361|                 403,
   362|                 "Account phone numbers are not authorized on this server",
   363|                 Codes.THREEPID_DENIED,
   364|             )
   365|         existing_user_id = await self.store.get_user_id_by_threepid("msisdn", msisdn)
   366|         if existing_user_id is not None:
   367|             if self.hs.config.request_token_inhibit_3pid_errors:
   368|                 await self.hs.clock.sleep(random.randint(1, 10) / 10)
   369|                 return 200, {"sid": random_string(16)}
   370|             raise SynapseError(400, "MSISDN is already in use", Codes.THREEPID_IN_USE)
   371|         if not self.hs.config.account_threepid_delegate_msisdn:
   372|             logger.warning(
   373|                 "No upstream msisdn account_threepid_delegate configured on the server to "
   374|                 "handle this request"
   375|             )
   376|             raise SynapseError(
   377|                 400,
   378|                 "Adding phone numbers to user account is not supported by this homeserver",
   379|             )
   380|         ret = await self.identity_handler.requestMsisdnToken(
   381|             self.hs.config.account_threepid_delegate_msisdn,
   382|             country,
   383|             phone_number,
   384|             client_secret,

# --- HUNK 3: Lines 411-459 ---
   411|                     "Adding emails have been disabled due to lack of an email config"
   412|                 )
   413|             raise SynapseError(
   414|                 400, "Adding an email to your account is disabled on this server"
   415|             )
   416|         elif self.config.threepid_behaviour_email == ThreepidBehaviour.REMOTE:
   417|             raise SynapseError(
   418|                 400,
   419|                 "This homeserver is not validating threepids. Use an identity server "
   420|                 "instead.",
   421|             )
   422|         sid = parse_string(request, "sid", required=True)
   423|         token = parse_string(request, "token", required=True)
   424|         client_secret = parse_string(request, "client_secret", required=True)
   425|         assert_valid_client_secret(client_secret)
   426|         try:
   427|             next_link = await self.store.validate_threepid_session(
   428|                 sid, client_secret, token, self.clock.time_msec()
   429|             )
   430|             if next_link:
   431|                 if next_link.startswith("file:///"):
   432|                     logger.warning(
   433|                         "Not redirecting to next_link as it is a local file: address"
   434|                     )
   435|                 else:
   436|                     request.setResponseCode(302)
   437|                     request.setHeader("Location", next_link)
   438|                     finish_request(request)
   439|                     return None
   440|             html = self.config.email_add_threepid_template_success_html_content
   441|             status_code = 200
   442|         except ThreepidValidationError as e:
   443|             status_code = e.code
   444|             template_vars = {"failure_reason": e.msg}
   445|             html = self._failure_email_template.render(**template_vars)
   446|         respond_with_html(request, status_code, html)
   447| class AddThreepidMsisdnSubmitTokenServlet(RestServlet):
   448|     """Handles 3PID validation token submission for adding a phone number to a user's
   449|     account
   450|     """
   451|     PATTERNS = client_patterns(
   452|         "/add_threepid/msisdn/submit_token$", releases=(), unstable=True
   453|     )
   454|     def __init__(self, hs):
   455|         """
   456|         Args:
   457|             hs (synapse.server.HomeServer): server
   458|         """
   459|         super().__init__()

# --- HUNK 4: Lines 464-504 ---
   464|     async def on_POST(self, request):
   465|         if not self.config.account_threepid_delegate_msisdn:
   466|             raise SynapseError(
   467|                 400,
   468|                 "This homeserver is not validating phone numbers. Use an identity server "
   469|                 "instead.",
   470|             )
   471|         body = parse_json_object_from_request(request)
   472|         assert_params_in_dict(body, ["client_secret", "sid", "token"])
   473|         assert_valid_client_secret(body["client_secret"])
   474|         response = await self.identity_handler.proxy_msisdn_submit_token(
   475|             self.config.account_threepid_delegate_msisdn,
   476|             body["client_secret"],
   477|             body["sid"],
   478|             body["token"],
   479|         )
   480|         return 200, response
   481| class ThreepidRestServlet(RestServlet):
   482|     PATTERNS = client_patterns("/account/3pid$")
   483|     def __init__(self, hs):
   484|         super(ThreepidRestServlet, self).__init__()
   485|         self.hs = hs
   486|         self.identity_handler = hs.get_handlers().identity_handler
   487|         self.auth = hs.get_auth()
   488|         self.auth_handler = hs.get_auth_handler()
   489|         self.datastore = self.hs.get_datastore()
   490|     async def on_GET(self, request):
   491|         requester = await self.auth.get_user_by_req(request)
   492|         threepids = await self.datastore.user_get_threepids(requester.user.to_string())
   493|         return 200, {"threepids": threepids}
   494|     async def on_POST(self, request):
   495|         if not self.hs.config.enable_3pid_changes:
   496|             raise SynapseError(
   497|                 400, "3PID changes are disabled on this server", Codes.FORBIDDEN
   498|             )
   499|         requester = await self.auth.get_user_by_req(request)
   500|         user_id = requester.user.to_string()
   501|         body = parse_json_object_from_request(request)
   502|         threepid_creds = body.get("threePidCreds") or body.get("three_pid_creds")
   503|         if threepid_creds is None:
   504|             raise SynapseError(

# --- HUNK 5: Lines 508-548 ---
   508|         sid = threepid_creds["sid"]
   509|         client_secret = threepid_creds["client_secret"]
   510|         assert_valid_client_secret(client_secret)
   511|         validation_session = await self.identity_handler.validate_threepid_session(
   512|             client_secret, sid
   513|         )
   514|         if validation_session:
   515|             await self.auth_handler.add_threepid(
   516|                 user_id,
   517|                 validation_session["medium"],
   518|                 validation_session["address"],
   519|                 validation_session["validated_at"],
   520|             )
   521|             return 200, {}
   522|         raise SynapseError(
   523|             400, "No validated 3pid session found", Codes.THREEPID_AUTH_FAILED
   524|         )
   525| class ThreepidAddRestServlet(RestServlet):
   526|     PATTERNS = client_patterns("/account/3pid/add$")
   527|     def __init__(self, hs):
   528|         super(ThreepidAddRestServlet, self).__init__()
   529|         self.hs = hs
   530|         self.identity_handler = hs.get_handlers().identity_handler
   531|         self.auth = hs.get_auth()
   532|         self.auth_handler = hs.get_auth_handler()
   533|     @interactive_auth_handler
   534|     async def on_POST(self, request):
   535|         if not self.hs.config.enable_3pid_changes:
   536|             raise SynapseError(
   537|                 400, "3PID changes are disabled on this server", Codes.FORBIDDEN
   538|             )
   539|         requester = await self.auth.get_user_by_req(request)
   540|         user_id = requester.user.to_string()
   541|         body = parse_json_object_from_request(request)
   542|         assert_params_in_dict(body, ["client_secret", "sid"])
   543|         sid = body["sid"]
   544|         client_secret = body["client_secret"]
   545|         assert_valid_client_secret(client_secret)
   546|         await self.auth_handler.validate_user_via_ui_auth(
   547|             requester,
   548|             request,

# --- HUNK 6: Lines 550-661 ---
   550|             self.hs.get_ip_from_request(request),
   551|             "add a third-party identifier to your account",
   552|         )
   553|         validation_session = await self.identity_handler.validate_threepid_session(
   554|             client_secret, sid
   555|         )
   556|         if validation_session:
   557|             await self.auth_handler.add_threepid(
   558|                 user_id,
   559|                 validation_session["medium"],
   560|                 validation_session["address"],
   561|                 validation_session["validated_at"],
   562|             )
   563|             return 200, {}
   564|         raise SynapseError(
   565|             400, "No validated 3pid session found", Codes.THREEPID_AUTH_FAILED
   566|         )
   567| class ThreepidBindRestServlet(RestServlet):
   568|     PATTERNS = client_patterns("/account/3pid/bind$")
   569|     def __init__(self, hs):
   570|         super(ThreepidBindRestServlet, self).__init__()
   571|         self.hs = hs
   572|         self.identity_handler = hs.get_handlers().identity_handler
   573|         self.auth = hs.get_auth()
   574|     async def on_POST(self, request):
   575|         body = parse_json_object_from_request(request)
   576|         assert_params_in_dict(body, ["id_server", "sid", "client_secret"])
   577|         id_server = body["id_server"]
   578|         sid = body["sid"]
   579|         id_access_token = body.get("id_access_token")  # optional
   580|         client_secret = body["client_secret"]
   581|         assert_valid_client_secret(client_secret)
   582|         requester = await self.auth.get_user_by_req(request)
   583|         user_id = requester.user.to_string()
   584|         await self.identity_handler.bind_threepid(
   585|             client_secret, sid, user_id, id_server, id_access_token
   586|         )
   587|         return 200, {}
   588| class ThreepidUnbindRestServlet(RestServlet):
   589|     PATTERNS = client_patterns("/account/3pid/unbind$")
   590|     def __init__(self, hs):
   591|         super(ThreepidUnbindRestServlet, self).__init__()
   592|         self.hs = hs
   593|         self.identity_handler = hs.get_handlers().identity_handler
   594|         self.auth = hs.get_auth()
   595|         self.datastore = self.hs.get_datastore()
   596|     async def on_POST(self, request):
   597|         """Unbind the given 3pid from a specific identity server, or identity servers that are
   598|         known to have this 3pid bound
   599|         """
   600|         requester = await self.auth.get_user_by_req(request)
   601|         body = parse_json_object_from_request(request)
   602|         assert_params_in_dict(body, ["medium", "address"])
   603|         medium = body.get("medium")
   604|         address = body.get("address")
   605|         id_server = body.get("id_server")
   606|         result = await self.identity_handler.try_unbind_threepid(
   607|             requester.user.to_string(),
   608|             {"address": address, "medium": medium, "id_server": id_server},
   609|         )
   610|         return 200, {"id_server_unbind_result": "success" if result else "no-support"}
   611| class ThreepidDeleteRestServlet(RestServlet):
   612|     PATTERNS = client_patterns("/account/3pid/delete$")
   613|     def __init__(self, hs):
   614|         super(ThreepidDeleteRestServlet, self).__init__()
   615|         self.hs = hs
   616|         self.auth = hs.get_auth()
   617|         self.auth_handler = hs.get_auth_handler()
   618|     async def on_POST(self, request):
   619|         if not self.hs.config.enable_3pid_changes:
   620|             raise SynapseError(
   621|                 400, "3PID changes are disabled on this server", Codes.FORBIDDEN
   622|             )
   623|         body = parse_json_object_from_request(request)
   624|         assert_params_in_dict(body, ["medium", "address"])
   625|         requester = await self.auth.get_user_by_req(request)
   626|         user_id = requester.user.to_string()
   627|         try:
   628|             ret = await self.auth_handler.delete_threepid(
   629|                 user_id, body["medium"], body["address"], body.get("id_server")
   630|             )
   631|         except Exception:
   632|             logger.exception("Failed to remove threepid")
   633|             raise SynapseError(500, "Failed to remove threepid")
   634|         if ret:
   635|             id_server_unbind_result = "success"
   636|         else:
   637|             id_server_unbind_result = "no-support"
   638|         return 200, {"id_server_unbind_result": id_server_unbind_result}
   639| class WhoamiRestServlet(RestServlet):
   640|     PATTERNS = client_patterns("/account/whoami$")
   641|     def __init__(self, hs):
   642|         super(WhoamiRestServlet, self).__init__()
   643|         self.auth = hs.get_auth()
   644|     async def on_GET(self, request):
   645|         requester = await self.auth.get_user_by_req(request)
   646|         return 200, {"user_id": requester.user.to_string()}
   647| def register_servlets(hs, http_server):
   648|     EmailPasswordRequestTokenRestServlet(hs).register(http_server)
   649|     PasswordResetSubmitTokenServlet(hs).register(http_server)
   650|     PasswordRestServlet(hs).register(http_server)
   651|     DeactivateAccountRestServlet(hs).register(http_server)
   652|     EmailThreepidRequestTokenRestServlet(hs).register(http_server)
   653|     MsisdnThreepidRequestTokenRestServlet(hs).register(http_server)
   654|     AddThreepidEmailSubmitTokenServlet(hs).register(http_server)
   655|     AddThreepidMsisdnSubmitTokenServlet(hs).register(http_server)
   656|     ThreepidRestServlet(hs).register(http_server)
   657|     ThreepidAddRestServlet(hs).register(http_server)
   658|     ThreepidBindRestServlet(hs).register(http_server)
   659|     ThreepidUnbindRestServlet(hs).register(http_server)
   660|     ThreepidDeleteRestServlet(hs).register(http_server)
   661|     WhoamiRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/account_data.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-73 ---
     1| import logging
     2| from synapse.api.errors import AuthError, NotFoundError, SynapseError
     3| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     4| from ._base import client_patterns
     5| logger = logging.getLogger(__name__)
     6| class AccountDataServlet(RestServlet):
     7|     """
     8|     PUT /user/{user_id}/account_data/{account_dataType} HTTP/1.1
     9|     GET /user/{user_id}/account_data/{account_dataType} HTTP/1.1
    10|     """
    11|     PATTERNS = client_patterns(
    12|         "/user/(?P<user_id>[^/]*)/account_data/(?P<account_data_type>[^/]*)"
    13|     )
    14|     def __init__(self, hs):
    15|         super(AccountDataServlet, self).__init__()
    16|         self.auth = hs.get_auth()
    17|         self.store = hs.get_datastore()
    18|         self.notifier = hs.get_notifier()
    19|         self._is_worker = hs.config.worker_app is not None
    20|     async def on_PUT(self, request, user_id, account_data_type):
    21|         if self._is_worker:
    22|             raise Exception("Cannot handle PUT /account_data on worker")
    23|         requester = await self.auth.get_user_by_req(request)
    24|         if user_id != requester.user.to_string():
    25|             raise AuthError(403, "Cannot add account data for other users.")
    26|         body = parse_json_object_from_request(request)
    27|         max_id = await self.store.add_account_data_for_user(
    28|             user_id, account_data_type, body
    29|         )
    30|         self.notifier.on_new_event("account_data_key", max_id, users=[user_id])
    31|         return 200, {}
    32|     async def on_GET(self, request, user_id, account_data_type):
    33|         requester = await self.auth.get_user_by_req(request)
    34|         if user_id != requester.user.to_string():
    35|             raise AuthError(403, "Cannot get account data for other users.")
    36|         event = await self.store.get_global_account_data_by_type_for_user(
    37|             account_data_type, user_id
    38|         )
    39|         if event is None:
    40|             raise NotFoundError("Account data not found")
    41|         return 200, event
    42| class RoomAccountDataServlet(RestServlet):
    43|     """
    44|     PUT /user/{user_id}/rooms/{room_id}/account_data/{account_dataType} HTTP/1.1
    45|     GET /user/{user_id}/rooms/{room_id}/account_data/{account_dataType} HTTP/1.1
    46|     """
    47|     PATTERNS = client_patterns(
    48|         "/user/(?P<user_id>[^/]*)"
    49|         "/rooms/(?P<room_id>[^/]*)"
    50|         "/account_data/(?P<account_data_type>[^/]*)"
    51|     )
    52|     def __init__(self, hs):
    53|         super(RoomAccountDataServlet, self).__init__()
    54|         self.auth = hs.get_auth()
    55|         self.store = hs.get_datastore()
    56|         self.notifier = hs.get_notifier()
    57|         self._is_worker = hs.config.worker_app is not None
    58|     async def on_PUT(self, request, user_id, room_id, account_data_type):
    59|         if self._is_worker:
    60|             raise Exception("Cannot handle PUT /account_data on worker")
    61|         requester = await self.auth.get_user_by_req(request)
    62|         if user_id != requester.user.to_string():
    63|             raise AuthError(403, "Cannot add account data for other users.")
    64|         body = parse_json_object_from_request(request)
    65|         if account_data_type == "m.fully_read":
    66|             raise SynapseError(
    67|                 405,
    68|                 "Cannot set m.fully_read through this API."
    69|                 " Use /rooms/!roomId:server.name/read_markers",
    70|             )
    71|         max_id = await self.store.add_account_data_to_room(
    72|             user_id, room_id, account_data_type, body
    73|         )


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/account_validity.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-57 ---
     1| import logging
     2| from synapse.api.errors import AuthError, SynapseError
     3| from synapse.http.server import respond_with_html
     4| from synapse.http.servlet import RestServlet
     5| from ._base import client_patterns
     6| logger = logging.getLogger(__name__)
     7| class AccountValidityRenewServlet(RestServlet):
     8|     PATTERNS = client_patterns("/account_validity/renew$")
     9|     def __init__(self, hs):
    10|         """
    11|         Args:
    12|             hs (synapse.server.HomeServer): server
    13|         """
    14|         super(AccountValidityRenewServlet, self).__init__()
    15|         self.hs = hs
    16|         self.account_activity_handler = hs.get_account_validity_handler()
    17|         self.auth = hs.get_auth()
    18|         self.success_html = hs.config.account_validity.account_renewed_html_content
    19|         self.failure_html = hs.config.account_validity.invalid_token_html_content
    20|     async def on_GET(self, request):
    21|         if b"token" not in request.args:
    22|             raise SynapseError(400, "Missing renewal token")
    23|         renewal_token = request.args[b"token"][0]
    24|         token_valid = await self.account_activity_handler.renew_account(
    25|             renewal_token.decode("utf8")
    26|         )
    27|         if token_valid:
    28|             status_code = 200
    29|             response = self.success_html
    30|         else:
    31|             status_code = 404
    32|             response = self.failure_html
    33|         respond_with_html(request, status_code, response)
    34| class AccountValiditySendMailServlet(RestServlet):
    35|     PATTERNS = client_patterns("/account_validity/send_mail$")
    36|     def __init__(self, hs):
    37|         """
    38|         Args:
    39|             hs (synapse.server.HomeServer): server
    40|         """
    41|         super(AccountValiditySendMailServlet, self).__init__()
    42|         self.hs = hs
    43|         self.account_activity_handler = hs.get_account_validity_handler()
    44|         self.auth = hs.get_auth()
    45|         self.account_validity = self.hs.config.account_validity
    46|     async def on_POST(self, request):
    47|         if not self.account_validity.renew_by_email_enabled:
    48|             raise AuthError(
    49|                 403, "Account renewal via email is disabled on this server."
    50|             )
    51|         requester = await self.auth.get_user_by_req(request, allow_expired=True)
    52|         user_id = requester.user.to_string()
    53|         await self.account_activity_handler.send_renewal_email_to_user(user_id)
    54|         return 200, {}
    55| def register_servlets(hs, http_server):
    56|     AccountValidityRenewServlet(hs).register(http_server)
    57|     AccountValiditySendMailServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/auth.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-209 ---
     1| import logging
     2| from synapse.api.constants import LoginType
     3| from synapse.api.errors import SynapseError
     4| from synapse.api.urls import CLIENT_API_PREFIX
     5| from synapse.http.server import respond_with_html
     6| from synapse.http.servlet import RestServlet, parse_string
     7| from ._base import client_patterns
     8| logger = logging.getLogger(__name__)
     9| RECAPTCHA_TEMPLATE = """
    10| <html>
    11| <head>
    12| <title>Authentication</title>
    13| <meta name='viewport' content='width=device-width, initial-scale=1,
    14|     user-scalable=no, minimum-scale=1.0, maximum-scale=1.0'>
    15| <script src="https://www.recaptcha.net/recaptcha/api.js"
    16|     async defer></script>
    17| <script src="//code.jquery.com/jquery-1.11.2.min.js"></script>
    18| <link rel="stylesheet" href="/_matrix/static/client/register/style.css">
    19| <script>
    20| function captchaDone() {
    21|     $('#registrationForm').submit();
    22| }
    23| </script>
    24| </head>
    25| <body>
    26| <form id="registrationForm" method="post" action="%(myurl)s">
    27|     <div>
    28|         <p>
    29|         Hello! We need to prevent computer programs and other automated
    30|         things from creating accounts on this server.
    31|         </p>
    32|         <p>
    33|         Please verify that you're not a robot.
    34|         </p>
    35|         <input type="hidden" name="session" value="%(session)s" />
    36|         <div class="g-recaptcha"
    37|             data-sitekey="%(sitekey)s"
    38|             data-callback="captchaDone">
    39|         </div>
    40|         <noscript>
    41|         <input type="submit" value="All Done" />
    42|         </noscript>
    43|         </div>
    44|     </div>
    45| </form>
    46| </body>
    47| </html>
    48| """
    49| TERMS_TEMPLATE = """
    50| <html>
    51| <head>
    52| <title>Authentication</title>
    53| <meta name='viewport' content='width=device-width, initial-scale=1,
    54|     user-scalable=no, minimum-scale=1.0, maximum-scale=1.0'>
    55| <link rel="stylesheet" href="/_matrix/static/client/register/style.css">
    56| </head>
    57| <body>
    58| <form id="registrationForm" method="post" action="%(myurl)s">
    59|     <div>
    60|         <p>
    61|             Please click the button below if you agree to the
    62|             <a href="%(terms_url)s">privacy policy of this homeserver.</a>
    63|         </p>
    64|         <input type="hidden" name="session" value="%(session)s" />
    65|         <input type="submit" value="Agree" />
    66|     </div>
    67| </form>
    68| </body>
    69| </html>
    70| """
    71| SUCCESS_TEMPLATE = """
    72| <html>
    73| <head>
    74| <title>Success!</title>
    75| <meta name='viewport' content='width=device-width, initial-scale=1,
    76|     user-scalable=no, minimum-scale=1.0, maximum-scale=1.0'>
    77| <link rel="stylesheet" href="/_matrix/static/client/register/style.css">
    78| <script>
    79| if (window.onAuthDone) {
    80|     window.onAuthDone();
    81| } else if (window.opener && window.opener.postMessage) {
    82|      window.opener.postMessage("authDone", "*");
    83| }
    84| </script>
    85| </head>
    86| <body>
    87|     <div>
    88|         <p>Thank you</p>
    89|         <p>You may now close this window and return to the application</p>
    90|     </div>
    91| </body>
    92| </html>
    93| """
    94| class AuthRestServlet(RestServlet):
    95|     """
    96|     Handles Client / Server API authentication in any situations where it
    97|     cannot be handled in the normal flow (with requests to the same endpoint).
    98|     Current use is for web fallback auth.
    99|     """
   100|     PATTERNS = client_patterns(r"/auth/(?P<stagetype>[\w\.]*)/fallback/web")
   101|     def __init__(self, hs):
   102|         super(AuthRestServlet, self).__init__()
   103|         self.hs = hs
   104|         self.auth = hs.get_auth()
   105|         self.auth_handler = hs.get_auth_handler()
   106|         self.registration_handler = hs.get_registration_handler()
   107|         self._cas_enabled = hs.config.cas_enabled
   108|         if self._cas_enabled:
   109|             self._cas_handler = hs.get_cas_handler()
   110|             self._cas_server_url = hs.config.cas_server_url
   111|             self._cas_service_url = hs.config.cas_service_url
   112|         self._saml_enabled = hs.config.saml2_enabled
   113|         if self._saml_enabled:
   114|             self._saml_handler = hs.get_saml_handler()
   115|         self._oidc_enabled = hs.config.oidc_enabled
   116|         if self._oidc_enabled:
   117|             self._oidc_handler = hs.get_oidc_handler()
   118|             self._cas_server_url = hs.config.cas_server_url
   119|             self._cas_service_url = hs.config.cas_service_url
   120|     async def on_GET(self, request, stagetype):
   121|         session = parse_string(request, "session")
   122|         if not session:
   123|             raise SynapseError(400, "No session supplied")
   124|         if stagetype == LoginType.RECAPTCHA:
   125|             html = RECAPTCHA_TEMPLATE % {
   126|                 "session": session,
   127|                 "myurl": "%s/r0/auth/%s/fallback/web"
   128|                 % (CLIENT_API_PREFIX, LoginType.RECAPTCHA),
   129|                 "sitekey": self.hs.config.recaptcha_public_key,
   130|             }
   131|         elif stagetype == LoginType.TERMS:
   132|             html = TERMS_TEMPLATE % {
   133|                 "session": session,
   134|                 "terms_url": "%s_matrix/consent?v=%s"
   135|                 % (self.hs.config.public_baseurl, self.hs.config.user_consent_version),
   136|                 "myurl": "%s/r0/auth/%s/fallback/web"
   137|                 % (CLIENT_API_PREFIX, LoginType.TERMS),
   138|             }
   139|         elif stagetype == LoginType.SSO:
   140|             if self._cas_enabled:
   141|                 sso_redirect_url = self._cas_handler.get_redirect_url(
   142|                     {"session": session},
   143|                 )
   144|             elif self._saml_enabled:
   145|                 client_redirect_url = b"unused"
   146|                 sso_redirect_url = self._saml_handler.handle_redirect_request(
   147|                     client_redirect_url, session
   148|                 )
   149|             elif self._oidc_enabled:
   150|                 client_redirect_url = b""
   151|                 sso_redirect_url = await self._oidc_handler.handle_redirect_request(
   152|                     request, client_redirect_url, session
   153|                 )
   154|             else:
   155|                 raise SynapseError(400, "Homeserver not configured for SSO.")
   156|             html = await self.auth_handler.start_sso_ui_auth(sso_redirect_url, session)
   157|         else:
   158|             raise SynapseError(404, "Unknown auth stage type")
   159|         respond_with_html(request, 200, html)
   160|         return None
   161|     async def on_POST(self, request, stagetype):
   162|         session = parse_string(request, "session")
   163|         if not session:
   164|             raise SynapseError(400, "No session supplied")
   165|         if stagetype == LoginType.RECAPTCHA:
   166|             response = parse_string(request, "g-recaptcha-response")
   167|             if not response:
   168|                 raise SynapseError(400, "No captcha response supplied")
   169|             authdict = {"response": response, "session": session}
   170|             success = await self.auth_handler.add_oob_auth(
   171|                 LoginType.RECAPTCHA, authdict, self.hs.get_ip_from_request(request)
   172|             )
   173|             if success:
   174|                 html = SUCCESS_TEMPLATE
   175|             else:
   176|                 html = RECAPTCHA_TEMPLATE % {
   177|                     "session": session,
   178|                     "myurl": "%s/r0/auth/%s/fallback/web"
   179|                     % (CLIENT_API_PREFIX, LoginType.RECAPTCHA),
   180|                     "sitekey": self.hs.config.recaptcha_public_key,
   181|                 }
   182|         elif stagetype == LoginType.TERMS:
   183|             authdict = {"session": session}
   184|             success = await self.auth_handler.add_oob_auth(
   185|                 LoginType.TERMS, authdict, self.hs.get_ip_from_request(request)
   186|             )
   187|             if success:
   188|                 html = SUCCESS_TEMPLATE
   189|             else:
   190|                 html = TERMS_TEMPLATE % {
   191|                     "session": session,
   192|                     "terms_url": "%s_matrix/consent?v=%s"
   193|                     % (
   194|                         self.hs.config.public_baseurl,
   195|                         self.hs.config.user_consent_version,
   196|                     ),
   197|                     "myurl": "%s/r0/auth/%s/fallback/web"
   198|                     % (CLIENT_API_PREFIX, LoginType.TERMS),
   199|                 }
   200|         elif stagetype == LoginType.SSO:
   201|             raise SynapseError(404, "Fallback SSO auth does not support POST requests.")
   202|         else:
   203|             raise SynapseError(404, "Unknown auth stage type")
   204|         respond_with_html(request, 200, html)
   205|         return None
   206|     def on_OPTIONS(self, _):
   207|         return 200, {}
   208| def register_servlets(hs, http_server):
   209|     AuthRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/capabilities.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-34 ---
     1| import logging
     2| from synapse.api.room_versions import KNOWN_ROOM_VERSIONS
     3| from synapse.http.servlet import RestServlet
     4| from ._base import client_patterns
     5| logger = logging.getLogger(__name__)
     6| class CapabilitiesRestServlet(RestServlet):
     7|     """End point to expose the capabilities of the server."""
     8|     PATTERNS = client_patterns("/capabilities$")
     9|     def __init__(self, hs):
    10|         """
    11|         Args:
    12|             hs (synapse.server.HomeServer): server
    13|         """
    14|         super(CapabilitiesRestServlet, self).__init__()
    15|         self.hs = hs
    16|         self.config = hs.config
    17|         self.auth = hs.get_auth()
    18|         self.store = hs.get_datastore()
    19|     async def on_GET(self, request):
    20|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    21|         user = await self.store.get_user_by_id(requester.user.to_string())
    22|         change_password = bool(user["password_hash"])
    23|         response = {
    24|             "capabilities": {
    25|                 "m.room_versions": {
    26|                     "default": self.config.default_room_version.identifier,
    27|                     "available": {
    28|                         v.identifier: v.disposition
    29|                         for v in KNOWN_ROOM_VERSIONS.values()
    30|                     },
    31|                 },
    32|                 "m.change_password": {"enabled": change_password},
    33|             }
    34|         }


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/devices.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-88 ---
     1| import logging
     2| from synapse.api import errors
     3| from synapse.http.servlet import (
     4|     RestServlet,
     5|     assert_params_in_dict,
     6|     parse_json_object_from_request,
     7| )
     8| from ._base import client_patterns, interactive_auth_handler
     9| logger = logging.getLogger(__name__)
    10| class DevicesRestServlet(RestServlet):
    11|     PATTERNS = client_patterns("/devices$")
    12|     def __init__(self, hs):
    13|         """
    14|         Args:
    15|             hs (synapse.server.HomeServer): server
    16|         """
    17|         super(DevicesRestServlet, self).__init__()
    18|         self.hs = hs
    19|         self.auth = hs.get_auth()
    20|         self.device_handler = hs.get_device_handler()
    21|     async def on_GET(self, request):
    22|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    23|         devices = await self.device_handler.get_devices_by_user(
    24|             requester.user.to_string()
    25|         )
    26|         return 200, {"devices": devices}
    27| class DeleteDevicesRestServlet(RestServlet):
    28|     """
    29|     API for bulk deletion of devices. Accepts a JSON object with a devices
    30|     key which lists the device_ids to delete. Requires user interactive auth.
    31|     """
    32|     PATTERNS = client_patterns("/delete_devices")
    33|     def __init__(self, hs):
    34|         super(DeleteDevicesRestServlet, self).__init__()
    35|         self.hs = hs
    36|         self.auth = hs.get_auth()
    37|         self.device_handler = hs.get_device_handler()
    38|         self.auth_handler = hs.get_auth_handler()
    39|     @interactive_auth_handler
    40|     async def on_POST(self, request):
    41|         requester = await self.auth.get_user_by_req(request)
    42|         try:
    43|             body = parse_json_object_from_request(request)
    44|         except errors.SynapseError as e:
    45|             if e.errcode == errors.Codes.NOT_JSON:
    46|                 body = {}
    47|             else:
    48|                 raise e
    49|         assert_params_in_dict(body, ["devices"])
    50|         await self.auth_handler.validate_user_via_ui_auth(
    51|             requester,
    52|             request,
    53|             body,
    54|             self.hs.get_ip_from_request(request),
    55|             "remove device(s) from your account",
    56|         )
    57|         await self.device_handler.delete_devices(
    58|             requester.user.to_string(), body["devices"]
    59|         )
    60|         return 200, {}
    61| class DeviceRestServlet(RestServlet):
    62|     PATTERNS = client_patterns("/devices/(?P<device_id>[^/]*)$")
    63|     def __init__(self, hs):
    64|         """
    65|         Args:
    66|             hs (synapse.server.HomeServer): server
    67|         """
    68|         super(DeviceRestServlet, self).__init__()
    69|         self.hs = hs
    70|         self.auth = hs.get_auth()
    71|         self.device_handler = hs.get_device_handler()
    72|         self.auth_handler = hs.get_auth_handler()
    73|     async def on_GET(self, request, device_id):
    74|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    75|         device = await self.device_handler.get_device(
    76|             requester.user.to_string(), device_id
    77|         )
    78|         return 200, device
    79|     @interactive_auth_handler
    80|     async def on_DELETE(self, request, device_id):
    81|         requester = await self.auth.get_user_by_req(request)
    82|         try:
    83|             body = parse_json_object_from_request(request)
    84|         except errors.SynapseError as e:
    85|             if e.errcode == errors.Codes.NOT_JSON:
    86|                 body = {}
    87|             else:
    88|                 raise


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/filter.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-56 ---
     1| import logging
     2| from synapse.api.errors import AuthError, NotFoundError, StoreError, SynapseError
     3| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     4| from synapse.types import UserID
     5| from ._base import client_patterns, set_timeline_upper_limit
     6| logger = logging.getLogger(__name__)
     7| class GetFilterRestServlet(RestServlet):
     8|     PATTERNS = client_patterns("/user/(?P<user_id>[^/]*)/filter/(?P<filter_id>[^/]*)")
     9|     def __init__(self, hs):
    10|         super(GetFilterRestServlet, self).__init__()
    11|         self.hs = hs
    12|         self.auth = hs.get_auth()
    13|         self.filtering = hs.get_filtering()
    14|     async def on_GET(self, request, user_id, filter_id):
    15|         target_user = UserID.from_string(user_id)
    16|         requester = await self.auth.get_user_by_req(request)
    17|         if target_user != requester.user:
    18|             raise AuthError(403, "Cannot get filters for other users")
    19|         if not self.hs.is_mine(target_user):
    20|             raise AuthError(403, "Can only get filters for local users")
    21|         try:
    22|             filter_id = int(filter_id)
    23|         except Exception:
    24|             raise SynapseError(400, "Invalid filter_id")
    25|         try:
    26|             filter_collection = await self.filtering.get_user_filter(
    27|                 user_localpart=target_user.localpart, filter_id=filter_id
    28|             )
    29|         except StoreError as e:
    30|             if e.code != 404:
    31|                 raise
    32|             raise NotFoundError("No such filter")
    33|         return 200, filter_collection.get_filter_json()
    34| class CreateFilterRestServlet(RestServlet):
    35|     PATTERNS = client_patterns("/user/(?P<user_id>[^/]*)/filter")
    36|     def __init__(self, hs):
    37|         super(CreateFilterRestServlet, self).__init__()
    38|         self.hs = hs
    39|         self.auth = hs.get_auth()
    40|         self.filtering = hs.get_filtering()
    41|     async def on_POST(self, request, user_id):
    42|         target_user = UserID.from_string(user_id)
    43|         requester = await self.auth.get_user_by_req(request)
    44|         if target_user != requester.user:
    45|             raise AuthError(403, "Cannot create filters for other users")
    46|         if not self.hs.is_mine(target_user):
    47|             raise AuthError(403, "Can only create filters for local users")
    48|         content = parse_json_object_from_request(request)
    49|         set_timeline_upper_limit(content, self.hs.config.filter_timeline_limit)
    50|         filter_id = await self.filtering.add_user_filter(
    51|             user_localpart=target_user.localpart, user_filter=content
    52|         )
    53|         return 200, {"filter_id": str(filter_id)}
    54| def register_servlets(hs, http_server):
    55|     GetFilterRestServlet(hs).register(http_server)
    56|     CreateFilterRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/groups.py
# Total hunks: 11
# ====================================================================
# --- HUNK 1: Lines 1-508 ---
     1| import logging
     2| from synapse.api.errors import SynapseError
     3| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     4| from synapse.types import GroupID
     5| from ._base import client_patterns
     6| logger = logging.getLogger(__name__)
     7| class GroupServlet(RestServlet):
     8|     """Get the group profile
     9|     """
    10|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/profile$")
    11|     def __init__(self, hs):
    12|         super(GroupServlet, self).__init__()
    13|         self.auth = hs.get_auth()
    14|         self.clock = hs.get_clock()
    15|         self.groups_handler = hs.get_groups_local_handler()
    16|     async def on_GET(self, request, group_id):
    17|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    18|         requester_user_id = requester.user.to_string()
    19|         group_description = await self.groups_handler.get_group_profile(
    20|             group_id, requester_user_id
    21|         )
    22|         return 200, group_description
    23|     async def on_POST(self, request, group_id):
    24|         requester = await self.auth.get_user_by_req(request)
    25|         requester_user_id = requester.user.to_string()
    26|         content = parse_json_object_from_request(request)
    27|         await self.groups_handler.update_group_profile(
    28|             group_id, requester_user_id, content
    29|         )
    30|         return 200, {}
    31| class GroupSummaryServlet(RestServlet):
    32|     """Get the full group summary
    33|     """
    34|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/summary$")
    35|     def __init__(self, hs):
    36|         super(GroupSummaryServlet, self).__init__()
    37|         self.auth = hs.get_auth()
    38|         self.clock = hs.get_clock()
    39|         self.groups_handler = hs.get_groups_local_handler()
    40|     async def on_GET(self, request, group_id):
    41|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    42|         requester_user_id = requester.user.to_string()
    43|         get_group_summary = await self.groups_handler.get_group_summary(
    44|             group_id, requester_user_id
    45|         )
    46|         return 200, get_group_summary
    47| class GroupSummaryRoomsCatServlet(RestServlet):
    48|     """Update/delete a rooms entry in the summary.
    49|     Matches both:
    50|         - /groups/:group/summary/rooms/:room_id
    51|         - /groups/:group/summary/categories/:category/rooms/:room_id
    52|     """
    53|     PATTERNS = client_patterns(
    54|         "/groups/(?P<group_id>[^/]*)/summary"
    55|         "(/categories/(?P<category_id>[^/]+))?"
    56|         "/rooms/(?P<room_id>[^/]*)$"
    57|     )
    58|     def __init__(self, hs):
    59|         super(GroupSummaryRoomsCatServlet, self).__init__()
    60|         self.auth = hs.get_auth()
    61|         self.clock = hs.get_clock()
    62|         self.groups_handler = hs.get_groups_local_handler()
    63|     async def on_PUT(self, request, group_id, category_id, room_id):
    64|         requester = await self.auth.get_user_by_req(request)
    65|         requester_user_id = requester.user.to_string()
    66|         content = parse_json_object_from_request(request)
    67|         resp = await self.groups_handler.update_group_summary_room(
    68|             group_id,
    69|             requester_user_id,
    70|             room_id=room_id,
    71|             category_id=category_id,
    72|             content=content,
    73|         )
    74|         return 200, resp
    75|     async def on_DELETE(self, request, group_id, category_id, room_id):
    76|         requester = await self.auth.get_user_by_req(request)
    77|         requester_user_id = requester.user.to_string()
    78|         resp = await self.groups_handler.delete_group_summary_room(
    79|             group_id, requester_user_id, room_id=room_id, category_id=category_id
    80|         )
    81|         return 200, resp
    82| class GroupCategoryServlet(RestServlet):
    83|     """Get/add/update/delete a group category
    84|     """
    85|     PATTERNS = client_patterns(
    86|         "/groups/(?P<group_id>[^/]*)/categories/(?P<category_id>[^/]+)$"
    87|     )
    88|     def __init__(self, hs):
    89|         super(GroupCategoryServlet, self).__init__()
    90|         self.auth = hs.get_auth()
    91|         self.clock = hs.get_clock()
    92|         self.groups_handler = hs.get_groups_local_handler()
    93|     async def on_GET(self, request, group_id, category_id):
    94|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    95|         requester_user_id = requester.user.to_string()
    96|         category = await self.groups_handler.get_group_category(
    97|             group_id, requester_user_id, category_id=category_id
    98|         )
    99|         return 200, category
   100|     async def on_PUT(self, request, group_id, category_id):
   101|         requester = await self.auth.get_user_by_req(request)
   102|         requester_user_id = requester.user.to_string()
   103|         content = parse_json_object_from_request(request)
   104|         resp = await self.groups_handler.update_group_category(
   105|             group_id, requester_user_id, category_id=category_id, content=content
   106|         )
   107|         return 200, resp
   108|     async def on_DELETE(self, request, group_id, category_id):
   109|         requester = await self.auth.get_user_by_req(request)
   110|         requester_user_id = requester.user.to_string()
   111|         resp = await self.groups_handler.delete_group_category(
   112|             group_id, requester_user_id, category_id=category_id
   113|         )
   114|         return 200, resp
   115| class GroupCategoriesServlet(RestServlet):
   116|     """Get all group categories
   117|     """
   118|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/categories/$")
   119|     def __init__(self, hs):
   120|         super(GroupCategoriesServlet, self).__init__()
   121|         self.auth = hs.get_auth()
   122|         self.clock = hs.get_clock()
   123|         self.groups_handler = hs.get_groups_local_handler()
   124|     async def on_GET(self, request, group_id):
   125|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   126|         requester_user_id = requester.user.to_string()
   127|         category = await self.groups_handler.get_group_categories(
   128|             group_id, requester_user_id
   129|         )
   130|         return 200, category
   131| class GroupRoleServlet(RestServlet):
   132|     """Get/add/update/delete a group role
   133|     """
   134|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/roles/(?P<role_id>[^/]+)$")
   135|     def __init__(self, hs):
   136|         super(GroupRoleServlet, self).__init__()
   137|         self.auth = hs.get_auth()
   138|         self.clock = hs.get_clock()
   139|         self.groups_handler = hs.get_groups_local_handler()
   140|     async def on_GET(self, request, group_id, role_id):
   141|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   142|         requester_user_id = requester.user.to_string()
   143|         category = await self.groups_handler.get_group_role(
   144|             group_id, requester_user_id, role_id=role_id
   145|         )
   146|         return 200, category
   147|     async def on_PUT(self, request, group_id, role_id):
   148|         requester = await self.auth.get_user_by_req(request)
   149|         requester_user_id = requester.user.to_string()
   150|         content = parse_json_object_from_request(request)
   151|         resp = await self.groups_handler.update_group_role(
   152|             group_id, requester_user_id, role_id=role_id, content=content
   153|         )
   154|         return 200, resp
   155|     async def on_DELETE(self, request, group_id, role_id):
   156|         requester = await self.auth.get_user_by_req(request)
   157|         requester_user_id = requester.user.to_string()
   158|         resp = await self.groups_handler.delete_group_role(
   159|             group_id, requester_user_id, role_id=role_id
   160|         )
   161|         return 200, resp
   162| class GroupRolesServlet(RestServlet):
   163|     """Get all group roles
   164|     """
   165|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/roles/$")
   166|     def __init__(self, hs):
   167|         super(GroupRolesServlet, self).__init__()
   168|         self.auth = hs.get_auth()
   169|         self.clock = hs.get_clock()
   170|         self.groups_handler = hs.get_groups_local_handler()
   171|     async def on_GET(self, request, group_id):
   172|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   173|         requester_user_id = requester.user.to_string()
   174|         category = await self.groups_handler.get_group_roles(
   175|             group_id, requester_user_id
   176|         )
   177|         return 200, category
   178| class GroupSummaryUsersRoleServlet(RestServlet):
   179|     """Update/delete a user's entry in the summary.
   180|     Matches both:
   181|         - /groups/:group/summary/users/:room_id
   182|         - /groups/:group/summary/roles/:role/users/:user_id
   183|     """
   184|     PATTERNS = client_patterns(
   185|         "/groups/(?P<group_id>[^/]*)/summary"
   186|         "(/roles/(?P<role_id>[^/]+))?"
   187|         "/users/(?P<user_id>[^/]*)$"
   188|     )
   189|     def __init__(self, hs):
   190|         super(GroupSummaryUsersRoleServlet, self).__init__()
   191|         self.auth = hs.get_auth()
   192|         self.clock = hs.get_clock()
   193|         self.groups_handler = hs.get_groups_local_handler()
   194|     async def on_PUT(self, request, group_id, role_id, user_id):
   195|         requester = await self.auth.get_user_by_req(request)
   196|         requester_user_id = requester.user.to_string()
   197|         content = parse_json_object_from_request(request)
   198|         resp = await self.groups_handler.update_group_summary_user(
   199|             group_id,
   200|             requester_user_id,
   201|             user_id=user_id,
   202|             role_id=role_id,
   203|             content=content,
   204|         )
   205|         return 200, resp
   206|     async def on_DELETE(self, request, group_id, role_id, user_id):
   207|         requester = await self.auth.get_user_by_req(request)
   208|         requester_user_id = requester.user.to_string()
   209|         resp = await self.groups_handler.delete_group_summary_user(
   210|             group_id, requester_user_id, user_id=user_id, role_id=role_id
   211|         )
   212|         return 200, resp
   213| class GroupRoomServlet(RestServlet):
   214|     """Get all rooms in a group
   215|     """
   216|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/rooms$")
   217|     def __init__(self, hs):
   218|         super(GroupRoomServlet, self).__init__()
   219|         self.auth = hs.get_auth()
   220|         self.clock = hs.get_clock()
   221|         self.groups_handler = hs.get_groups_local_handler()
   222|     async def on_GET(self, request, group_id):
   223|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   224|         requester_user_id = requester.user.to_string()
   225|         if not GroupID.is_valid(group_id):
   226|             raise SynapseError(400, "%s was not legal group ID" % (group_id,))
   227|         result = await self.groups_handler.get_rooms_in_group(
   228|             group_id, requester_user_id
   229|         )
   230|         return 200, result
   231| class GroupUsersServlet(RestServlet):
   232|     """Get all users in a group
   233|     """
   234|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/users$")
   235|     def __init__(self, hs):
   236|         super(GroupUsersServlet, self).__init__()
   237|         self.auth = hs.get_auth()
   238|         self.clock = hs.get_clock()
   239|         self.groups_handler = hs.get_groups_local_handler()
   240|     async def on_GET(self, request, group_id):
   241|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   242|         requester_user_id = requester.user.to_string()
   243|         result = await self.groups_handler.get_users_in_group(
   244|             group_id, requester_user_id
   245|         )
   246|         return 200, result
   247| class GroupInvitedUsersServlet(RestServlet):
   248|     """Get users invited to a group
   249|     """
   250|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/invited_users$")
   251|     def __init__(self, hs):
   252|         super(GroupInvitedUsersServlet, self).__init__()
   253|         self.auth = hs.get_auth()
   254|         self.clock = hs.get_clock()
   255|         self.groups_handler = hs.get_groups_local_handler()
   256|     async def on_GET(self, request, group_id):
   257|         requester = await self.auth.get_user_by_req(request)
   258|         requester_user_id = requester.user.to_string()
   259|         result = await self.groups_handler.get_invited_users_in_group(
   260|             group_id, requester_user_id
   261|         )
   262|         return 200, result
   263| class GroupSettingJoinPolicyServlet(RestServlet):
   264|     """Set group join policy
   265|     """
   266|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/settings/m.join_policy$")
   267|     def __init__(self, hs):
   268|         super(GroupSettingJoinPolicyServlet, self).__init__()
   269|         self.auth = hs.get_auth()
   270|         self.groups_handler = hs.get_groups_local_handler()
   271|     async def on_PUT(self, request, group_id):
   272|         requester = await self.auth.get_user_by_req(request)
   273|         requester_user_id = requester.user.to_string()
   274|         content = parse_json_object_from_request(request)
   275|         result = await self.groups_handler.set_group_join_policy(
   276|             group_id, requester_user_id, content
   277|         )
   278|         return 200, result
   279| class GroupCreateServlet(RestServlet):
   280|     """Create a group
   281|     """
   282|     PATTERNS = client_patterns("/create_group$")
   283|     def __init__(self, hs):
   284|         super(GroupCreateServlet, self).__init__()
   285|         self.auth = hs.get_auth()
   286|         self.clock = hs.get_clock()
   287|         self.groups_handler = hs.get_groups_local_handler()
   288|         self.server_name = hs.hostname
   289|     async def on_POST(self, request):
   290|         requester = await self.auth.get_user_by_req(request)
   291|         requester_user_id = requester.user.to_string()
   292|         content = parse_json_object_from_request(request)
   293|         localpart = content.pop("localpart")
   294|         group_id = GroupID(localpart, self.server_name).to_string()
   295|         result = await self.groups_handler.create_group(
   296|             group_id, requester_user_id, content
   297|         )
   298|         return 200, result
   299| class GroupAdminRoomsServlet(RestServlet):
   300|     """Add a room to the group
   301|     """
   302|     PATTERNS = client_patterns(
   303|         "/groups/(?P<group_id>[^/]*)/admin/rooms/(?P<room_id>[^/]*)$"
   304|     )
   305|     def __init__(self, hs):
   306|         super(GroupAdminRoomsServlet, self).__init__()
   307|         self.auth = hs.get_auth()
   308|         self.clock = hs.get_clock()
   309|         self.groups_handler = hs.get_groups_local_handler()
   310|     async def on_PUT(self, request, group_id, room_id):
   311|         requester = await self.auth.get_user_by_req(request)
   312|         requester_user_id = requester.user.to_string()
   313|         content = parse_json_object_from_request(request)
   314|         result = await self.groups_handler.add_room_to_group(
   315|             group_id, requester_user_id, room_id, content
   316|         )
   317|         return 200, result
   318|     async def on_DELETE(self, request, group_id, room_id):
   319|         requester = await self.auth.get_user_by_req(request)
   320|         requester_user_id = requester.user.to_string()
   321|         result = await self.groups_handler.remove_room_from_group(
   322|             group_id, requester_user_id, room_id
   323|         )
   324|         return 200, result
   325| class GroupAdminRoomsConfigServlet(RestServlet):
   326|     """Update the config of a room in a group
   327|     """
   328|     PATTERNS = client_patterns(
   329|         "/groups/(?P<group_id>[^/]*)/admin/rooms/(?P<room_id>[^/]*)"
   330|         "/config/(?P<config_key>[^/]*)$"
   331|     )
   332|     def __init__(self, hs):
   333|         super(GroupAdminRoomsConfigServlet, self).__init__()
   334|         self.auth = hs.get_auth()
   335|         self.clock = hs.get_clock()
   336|         self.groups_handler = hs.get_groups_local_handler()
   337|     async def on_PUT(self, request, group_id, room_id, config_key):
   338|         requester = await self.auth.get_user_by_req(request)
   339|         requester_user_id = requester.user.to_string()
   340|         content = parse_json_object_from_request(request)
   341|         result = await self.groups_handler.update_room_in_group(
   342|             group_id, requester_user_id, room_id, config_key, content
   343|         )
   344|         return 200, result
   345| class GroupAdminUsersInviteServlet(RestServlet):
   346|     """Invite a user to the group
   347|     """
   348|     PATTERNS = client_patterns(
   349|         "/groups/(?P<group_id>[^/]*)/admin/users/invite/(?P<user_id>[^/]*)$"
   350|     )
   351|     def __init__(self, hs):
   352|         super(GroupAdminUsersInviteServlet, self).__init__()
   353|         self.auth = hs.get_auth()
   354|         self.clock = hs.get_clock()
   355|         self.groups_handler = hs.get_groups_local_handler()
   356|         self.store = hs.get_datastore()
   357|         self.is_mine_id = hs.is_mine_id
   358|     async def on_PUT(self, request, group_id, user_id):
   359|         requester = await self.auth.get_user_by_req(request)
   360|         requester_user_id = requester.user.to_string()
   361|         content = parse_json_object_from_request(request)
   362|         config = content.get("config", {})
   363|         result = await self.groups_handler.invite(
   364|             group_id, user_id, requester_user_id, config
   365|         )
   366|         return 200, result
   367| class GroupAdminUsersKickServlet(RestServlet):
   368|     """Kick a user from the group
   369|     """
   370|     PATTERNS = client_patterns(
   371|         "/groups/(?P<group_id>[^/]*)/admin/users/remove/(?P<user_id>[^/]*)$"
   372|     )
   373|     def __init__(self, hs):
   374|         super(GroupAdminUsersKickServlet, self).__init__()
   375|         self.auth = hs.get_auth()
   376|         self.clock = hs.get_clock()
   377|         self.groups_handler = hs.get_groups_local_handler()
   378|     async def on_PUT(self, request, group_id, user_id):
   379|         requester = await self.auth.get_user_by_req(request)
   380|         requester_user_id = requester.user.to_string()
   381|         content = parse_json_object_from_request(request)
   382|         result = await self.groups_handler.remove_user_from_group(
   383|             group_id, user_id, requester_user_id, content
   384|         )
   385|         return 200, result
   386| class GroupSelfLeaveServlet(RestServlet):
   387|     """Leave a joined group
   388|     """
   389|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/self/leave$")
   390|     def __init__(self, hs):
   391|         super(GroupSelfLeaveServlet, self).__init__()
   392|         self.auth = hs.get_auth()
   393|         self.clock = hs.get_clock()
   394|         self.groups_handler = hs.get_groups_local_handler()
   395|     async def on_PUT(self, request, group_id):
   396|         requester = await self.auth.get_user_by_req(request)
   397|         requester_user_id = requester.user.to_string()
   398|         content = parse_json_object_from_request(request)
   399|         result = await self.groups_handler.remove_user_from_group(
   400|             group_id, requester_user_id, requester_user_id, content
   401|         )
   402|         return 200, result
   403| class GroupSelfJoinServlet(RestServlet):
   404|     """Attempt to join a group, or knock
   405|     """
   406|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/self/join$")
   407|     def __init__(self, hs):
   408|         super(GroupSelfJoinServlet, self).__init__()
   409|         self.auth = hs.get_auth()
   410|         self.clock = hs.get_clock()
   411|         self.groups_handler = hs.get_groups_local_handler()
   412|     async def on_PUT(self, request, group_id):
   413|         requester = await self.auth.get_user_by_req(request)
   414|         requester_user_id = requester.user.to_string()
   415|         content = parse_json_object_from_request(request)
   416|         result = await self.groups_handler.join_group(
   417|             group_id, requester_user_id, content
   418|         )
   419|         return 200, result
   420| class GroupSelfAcceptInviteServlet(RestServlet):
   421|     """Accept a group invite
   422|     """
   423|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/self/accept_invite$")
   424|     def __init__(self, hs):
   425|         super(GroupSelfAcceptInviteServlet, self).__init__()
   426|         self.auth = hs.get_auth()
   427|         self.clock = hs.get_clock()
   428|         self.groups_handler = hs.get_groups_local_handler()
   429|     async def on_PUT(self, request, group_id):
   430|         requester = await self.auth.get_user_by_req(request)
   431|         requester_user_id = requester.user.to_string()
   432|         content = parse_json_object_from_request(request)
   433|         result = await self.groups_handler.accept_invite(
   434|             group_id, requester_user_id, content
   435|         )
   436|         return 200, result
   437| class GroupSelfUpdatePublicityServlet(RestServlet):
   438|     """Update whether we publicise a users membership of a group
   439|     """
   440|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/self/update_publicity$")
   441|     def __init__(self, hs):
   442|         super(GroupSelfUpdatePublicityServlet, self).__init__()
   443|         self.auth = hs.get_auth()
   444|         self.clock = hs.get_clock()
   445|         self.store = hs.get_datastore()
   446|     async def on_PUT(self, request, group_id):
   447|         requester = await self.auth.get_user_by_req(request)
   448|         requester_user_id = requester.user.to_string()
   449|         content = parse_json_object_from_request(request)
   450|         publicise = content["publicise"]
   451|         await self.store.update_group_publicity(group_id, requester_user_id, publicise)
   452|         return 200, {}
   453| class PublicisedGroupsForUserServlet(RestServlet):
   454|     """Get the list of groups a user is advertising
   455|     """
   456|     PATTERNS = client_patterns("/publicised_groups/(?P<user_id>[^/]*)$")
   457|     def __init__(self, hs):
   458|         super(PublicisedGroupsForUserServlet, self).__init__()
   459|         self.auth = hs.get_auth()
   460|         self.clock = hs.get_clock()
   461|         self.store = hs.get_datastore()
   462|         self.groups_handler = hs.get_groups_local_handler()
   463|     async def on_GET(self, request, user_id):
   464|         await self.auth.get_user_by_req(request, allow_guest=True)
   465|         result = await self.groups_handler.get_publicised_groups_for_user(user_id)
   466|         return 200, result
   467| class PublicisedGroupsForUsersServlet(RestServlet):
   468|     """Get the list of groups a user is advertising
   469|     """
   470|     PATTERNS = client_patterns("/publicised_groups$")
   471|     def __init__(self, hs):
   472|         super(PublicisedGroupsForUsersServlet, self).__init__()
   473|         self.auth = hs.get_auth()
   474|         self.clock = hs.get_clock()
   475|         self.store = hs.get_datastore()
   476|         self.groups_handler = hs.get_groups_local_handler()
   477|     async def on_POST(self, request):
   478|         await self.auth.get_user_by_req(request, allow_guest=True)
   479|         content = parse_json_object_from_request(request)
   480|         user_ids = content["user_ids"]
   481|         result = await self.groups_handler.bulk_get_publicised_groups(user_ids)
   482|         return 200, result
   483| class GroupsForUserServlet(RestServlet):
   484|     """Get all groups the logged in user is joined to
   485|     """
   486|     PATTERNS = client_patterns("/joined_groups$")
   487|     def __init__(self, hs):
   488|         super(GroupsForUserServlet, self).__init__()
   489|         self.auth = hs.get_auth()
   490|         self.clock = hs.get_clock()
   491|         self.groups_handler = hs.get_groups_local_handler()
   492|     async def on_GET(self, request):
   493|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   494|         requester_user_id = requester.user.to_string()
   495|         result = await self.groups_handler.get_joined_groups(requester_user_id)
   496|         return 200, result
   497| def register_servlets(hs, http_server):
   498|     GroupServlet(hs).register(http_server)
   499|     GroupSummaryServlet(hs).register(http_server)
   500|     GroupInvitedUsersServlet(hs).register(http_server)
   501|     GroupUsersServlet(hs).register(http_server)
   502|     GroupRoomServlet(hs).register(http_server)
   503|     GroupSettingJoinPolicyServlet(hs).register(http_server)
   504|     GroupCreateServlet(hs).register(http_server)
   505|     GroupAdminRoomsServlet(hs).register(http_server)
   506|     GroupAdminRoomsConfigServlet(hs).register(http_server)
   507|     GroupAdminUsersInviteServlet(hs).register(http_server)
   508|     GroupAdminUsersKickServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/keys.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 23-63 ---
    23|           "m.olm.curve25519-aes-sha2",
    24|         ]
    25|         "keys": {
    26|           "<algorithm>:<device_id>": "<key_base64>",
    27|         },
    28|         "signatures:" {
    29|           "<user_id>" {
    30|             "<algorithm>:<device_id>": "<signature_base64>"
    31|       } } },
    32|       "one_time_keys": {
    33|         "<algorithm>:<key_id>": "<key_base64>"
    34|       },
    35|     }
    36|     """
    37|     PATTERNS = client_patterns("/keys/upload(/(?P<device_id>[^/]+))?$")
    38|     def __init__(self, hs):
    39|         """
    40|         Args:
    41|             hs (synapse.server.HomeServer): server
    42|         """
    43|         super(KeyUploadServlet, self).__init__()
    44|         self.auth = hs.get_auth()
    45|         self.e2e_keys_handler = hs.get_e2e_keys_handler()
    46|     @trace(opname="upload_keys")
    47|     async def on_POST(self, request, device_id):
    48|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    49|         user_id = requester.user.to_string()
    50|         body = parse_json_object_from_request(request)
    51|         if device_id is not None:
    52|             if requester.device_id is not None and device_id != requester.device_id:
    53|                 set_tag("error", True)
    54|                 log_kv(
    55|                     {
    56|                         "message": "Client uploading keys for a different device",
    57|                         "logged_in_id": requester.device_id,
    58|                         "key_being_uploaded": device_id,
    59|                     }
    60|                 )
    61|                 logger.warning(
    62|                     "Client uploading keys for a different device "
    63|                     "(logged in as %s, uploading for %s)",

# --- HUNK 2: Lines 95-210 ---
    95|             ],
    96|             "keys": { // Must include a ed25519 signing key
    97|               "<algorithm>:<key_id>": "<key_base64>",
    98|             },
    99|             "signatures:" {
   100|               // Must be signed with device's ed25519 key
   101|               "<user_id>/<device_id>": {
   102|                 "<algorithm>:<key_id>": "<signature_base64>"
   103|               }
   104|               // Must be signed by this server.
   105|               "<server_name>": {
   106|                 "<algorithm>:<key_id>": "<signature_base64>"
   107|     } } } } } }
   108|     """
   109|     PATTERNS = client_patterns("/keys/query$")
   110|     def __init__(self, hs):
   111|         """
   112|         Args:
   113|             hs (synapse.server.HomeServer):
   114|         """
   115|         super(KeyQueryServlet, self).__init__()
   116|         self.auth = hs.get_auth()
   117|         self.e2e_keys_handler = hs.get_e2e_keys_handler()
   118|     async def on_POST(self, request):
   119|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   120|         user_id = requester.user.to_string()
   121|         timeout = parse_integer(request, "timeout", 10 * 1000)
   122|         body = parse_json_object_from_request(request)
   123|         result = await self.e2e_keys_handler.query_devices(body, timeout, user_id)
   124|         return 200, result
   125| class KeyChangesServlet(RestServlet):
   126|     """Returns the list of changes of keys between two stream tokens (may return
   127|     spurious extra results, since we currently ignore the `to` param).
   128|         GET /keys/changes?from=...&to=...
   129|         200 OK
   130|         { "changed": ["@foo:example.com"] }
   131|     """
   132|     PATTERNS = client_patterns("/keys/changes$")
   133|     def __init__(self, hs):
   134|         """
   135|         Args:
   136|             hs (synapse.server.HomeServer):
   137|         """
   138|         super(KeyChangesServlet, self).__init__()
   139|         self.auth = hs.get_auth()
   140|         self.device_handler = hs.get_device_handler()
   141|     async def on_GET(self, request):
   142|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   143|         from_token_string = parse_string(request, "from")
   144|         set_tag("from", from_token_string)
   145|         set_tag("to", parse_string(request, "to"))
   146|         from_token = StreamToken.from_string(from_token_string)
   147|         user_id = requester.user.to_string()
   148|         results = await self.device_handler.get_user_ids_changed(user_id, from_token)
   149|         return 200, results
   150| class OneTimeKeyServlet(RestServlet):
   151|     """
   152|     POST /keys/claim HTTP/1.1
   153|     {
   154|       "one_time_keys": {
   155|         "<user_id>": {
   156|           "<device_id>": "<algorithm>"
   157|     } } }
   158|     HTTP/1.1 200 OK
   159|     {
   160|       "one_time_keys": {
   161|         "<user_id>": {
   162|           "<device_id>": {
   163|             "<algorithm>:<key_id>": "<key_base64>"
   164|     } } } }
   165|     """
   166|     PATTERNS = client_patterns("/keys/claim$")
   167|     def __init__(self, hs):
   168|         super(OneTimeKeyServlet, self).__init__()
   169|         self.auth = hs.get_auth()
   170|         self.e2e_keys_handler = hs.get_e2e_keys_handler()
   171|     async def on_POST(self, request):
   172|         await self.auth.get_user_by_req(request, allow_guest=True)
   173|         timeout = parse_integer(request, "timeout", 10 * 1000)
   174|         body = parse_json_object_from_request(request)
   175|         result = await self.e2e_keys_handler.claim_one_time_keys(body, timeout)
   176|         return 200, result
   177| class SigningKeyUploadServlet(RestServlet):
   178|     """
   179|     POST /keys/device_signing/upload HTTP/1.1
   180|     Content-Type: application/json
   181|     {
   182|     }
   183|     """
   184|     PATTERNS = client_patterns("/keys/device_signing/upload$", releases=())
   185|     def __init__(self, hs):
   186|         """
   187|         Args:
   188|             hs (synapse.server.HomeServer): server
   189|         """
   190|         super(SigningKeyUploadServlet, self).__init__()
   191|         self.hs = hs
   192|         self.auth = hs.get_auth()
   193|         self.e2e_keys_handler = hs.get_e2e_keys_handler()
   194|         self.auth_handler = hs.get_auth_handler()
   195|     @interactive_auth_handler
   196|     async def on_POST(self, request):
   197|         requester = await self.auth.get_user_by_req(request)
   198|         user_id = requester.user.to_string()
   199|         body = parse_json_object_from_request(request)
   200|         await self.auth_handler.validate_user_via_ui_auth(
   201|             requester,
   202|             request,
   203|             body,
   204|             self.hs.get_ip_from_request(request),
   205|             "add a device signing key to your account",
   206|         )
   207|         result = await self.e2e_keys_handler.upload_signing_keys_for_user(user_id, body)
   208|         return 200, result
   209| class SignaturesUploadServlet(RestServlet):
   210|     """

# --- HUNK 3: Lines 220-257 ---
   220|             "m.megolm.v1.aes-sha2"
   221|           ],
   222|           "keys": {
   223|             "<algorithm>:<device_id>": "<key_base64>",
   224|           },
   225|           "signatures": {
   226|             "<signing_user_id>": {
   227|               "<algorithm>:<signing_key_base64>": "<signature_base64>>"
   228|             }
   229|           }
   230|         }
   231|       }
   232|     }
   233|     """
   234|     PATTERNS = client_patterns("/keys/signatures/upload$")
   235|     def __init__(self, hs):
   236|         """
   237|         Args:
   238|             hs (synapse.server.HomeServer): server
   239|         """
   240|         super(SignaturesUploadServlet, self).__init__()
   241|         self.auth = hs.get_auth()
   242|         self.e2e_keys_handler = hs.get_e2e_keys_handler()
   243|     async def on_POST(self, request):
   244|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   245|         user_id = requester.user.to_string()
   246|         body = parse_json_object_from_request(request)
   247|         result = await self.e2e_keys_handler.upload_signatures_for_device_keys(
   248|             user_id, body
   249|         )
   250|         return 200, result
   251| def register_servlets(hs, http_server):
   252|     KeyUploadServlet(hs).register(http_server)
   253|     KeyQueryServlet(hs).register(http_server)
   254|     KeyChangesServlet(hs).register(http_server)
   255|     OneTimeKeyServlet(hs).register(http_server)
   256|     SigningKeyUploadServlet(hs).register(http_server)
   257|     SignaturesUploadServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/notifications.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-29 ---
     1| import logging
     2| from synapse.events.utils import format_event_for_client_v2_without_room_id
     3| from synapse.http.servlet import RestServlet, parse_integer, parse_string
     4| from ._base import client_patterns
     5| logger = logging.getLogger(__name__)
     6| class NotificationsServlet(RestServlet):
     7|     PATTERNS = client_patterns("/notifications$")
     8|     def __init__(self, hs):
     9|         super(NotificationsServlet, self).__init__()
    10|         self.store = hs.get_datastore()
    11|         self.auth = hs.get_auth()
    12|         self.clock = hs.get_clock()
    13|         self._event_serializer = hs.get_event_client_serializer()
    14|     async def on_GET(self, request):
    15|         requester = await self.auth.get_user_by_req(request)
    16|         user_id = requester.user.to_string()
    17|         from_token = parse_string(request, "from", required=False)
    18|         limit = parse_integer(request, "limit", default=50)
    19|         only = parse_string(request, "only", required=False)
    20|         limit = min(limit, 500)
    21|         push_actions = await self.store.get_push_actions_for_user(
    22|             user_id, from_token, limit, only_highlight=(only == "highlight")
    23|         )
    24|         receipts_by_room = await self.store.get_receipts_for_user_with_orderings(
    25|             user_id, "m.read"
    26|         )
    27|         notif_event_ids = [pa["event_id"] for pa in push_actions]
    28|         notif_events = await self.store.get_events(notif_event_ids)
    29|         returned_push_actions = []


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/openid.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 12-52 ---
    12|     in http://openid.net/specs/openid-connect-core-1_0.html#TokenResponse
    13|     But instead of returning a signed "id_token" the response contains the
    14|     name of the issuing matrix homeserver. This means that for now the third
    15|     party will need to check the validity of the "id_token" against the
    16|     federation /openid/userinfo endpoint of the homeserver.
    17|     Request:
    18|     POST /user/{user_id}/openid/request_token?access_token=... HTTP/1.1
    19|     {}
    20|     Response:
    21|     HTTP/1.1 200 OK
    22|     {
    23|         "access_token": "ABDEFGH",
    24|         "token_type": "Bearer",
    25|         "matrix_server_name": "example.com",
    26|         "expires_in": 3600,
    27|     }
    28|     """
    29|     PATTERNS = client_patterns("/user/(?P<user_id>[^/]*)/openid/request_token")
    30|     EXPIRES_MS = 3600 * 1000
    31|     def __init__(self, hs):
    32|         super(IdTokenServlet, self).__init__()
    33|         self.auth = hs.get_auth()
    34|         self.store = hs.get_datastore()
    35|         self.clock = hs.get_clock()
    36|         self.server_name = hs.config.server_name
    37|     async def on_POST(self, request, user_id):
    38|         requester = await self.auth.get_user_by_req(request)
    39|         if user_id != requester.user.to_string():
    40|             raise AuthError(403, "Cannot request tokens for other users.")
    41|         parse_json_object_from_request(request)
    42|         token = random_string(24)
    43|         ts_valid_until_ms = self.clock.time_msec() + self.EXPIRES_MS
    44|         await self.store.insert_open_id_token(token, ts_valid_until_ms, user_id)
    45|         return (
    46|             200,
    47|             {
    48|                 "access_token": token,
    49|                 "token_type": "Bearer",
    50|                 "matrix_server_name": self.server_name,
    51|                 "expires_in": self.EXPIRES_MS / 1000,
    52|             },


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/password_policy.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-30 ---
     1| import logging
     2| from synapse.http.servlet import RestServlet
     3| from ._base import client_patterns
     4| logger = logging.getLogger(__name__)
     5| class PasswordPolicyServlet(RestServlet):
     6|     PATTERNS = client_patterns("/password_policy$")
     7|     def __init__(self, hs):
     8|         """
     9|         Args:
    10|             hs (synapse.server.HomeServer): server
    11|         """
    12|         super(PasswordPolicyServlet, self).__init__()
    13|         self.policy = hs.config.password_policy
    14|         self.enabled = hs.config.password_policy_enabled
    15|     def on_GET(self, request):
    16|         if not self.enabled or not self.policy:
    17|             return (200, {})
    18|         policy = {}
    19|         for param in [
    20|             "minimum_length",
    21|             "require_digit",
    22|             "require_symbol",
    23|             "require_lowercase",
    24|             "require_uppercase",
    25|         ]:
    26|             if param in self.policy:
    27|                 policy["m.%s" % param] = self.policy[param]
    28|         return (200, policy)
    29| def register_servlets(hs, http_server):
    30|     PasswordPolicyServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/read_marker.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| import logging
     2| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     3| from ._base import client_patterns
     4| logger = logging.getLogger(__name__)
     5| class ReadMarkerRestServlet(RestServlet):
     6|     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/read_markers$")
     7|     def __init__(self, hs):
     8|         super(ReadMarkerRestServlet, self).__init__()
     9|         self.auth = hs.get_auth()
    10|         self.receipts_handler = hs.get_receipts_handler()
    11|         self.read_marker_handler = hs.get_read_marker_handler()
    12|         self.presence_handler = hs.get_presence_handler()
    13|     async def on_POST(self, request, room_id):
    14|         requester = await self.auth.get_user_by_req(request)
    15|         await self.presence_handler.bump_presence_active_time(requester.user)
    16|         body = parse_json_object_from_request(request)
    17|         read_event_id = body.get("m.read", None)
    18|         if read_event_id:
    19|             await self.receipts_handler.received_client_receipt(
    20|                 room_id,
    21|                 "m.read",
    22|                 user_id=requester.user.to_string(),
    23|                 event_id=read_event_id,
    24|             )
    25|         read_marker_event_id = body.get("m.fully_read", None)
    26|         if read_marker_event_id:
    27|             await self.read_marker_handler.received_client_read_marker(
    28|                 room_id,


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/receipts.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| import logging
     2| from synapse.api.errors import SynapseError
     3| from synapse.http.servlet import RestServlet
     4| from ._base import client_patterns
     5| logger = logging.getLogger(__name__)
     6| class ReceiptRestServlet(RestServlet):
     7|     PATTERNS = client_patterns(
     8|         "/rooms/(?P<room_id>[^/]*)"
     9|         "/receipt/(?P<receipt_type>[^/]*)"
    10|         "/(?P<event_id>[^/]*)$"
    11|     )
    12|     def __init__(self, hs):
    13|         super(ReceiptRestServlet, self).__init__()
    14|         self.hs = hs
    15|         self.auth = hs.get_auth()
    16|         self.receipts_handler = hs.get_receipts_handler()
    17|         self.presence_handler = hs.get_presence_handler()
    18|     async def on_POST(self, request, room_id, receipt_type, event_id):
    19|         requester = await self.auth.get_user_by_req(request)
    20|         if receipt_type != "m.read":
    21|             raise SynapseError(400, "Receipt type must be 'm.read'")
    22|         await self.presence_handler.bump_presence_active_time(requester.user)
    23|         await self.receipts_handler.received_client_receipt(
    24|             room_id, receipt_type, user_id=requester.user.to_string(), event_id=event_id
    25|         )
    26|         return 200, {}
    27| def register_servlets(hs, http_server):
    28|     ReceiptRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/register.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 30-70 ---
    30| )
    31| from synapse.push.mailer import Mailer
    32| from synapse.util.msisdn import phone_number_to_msisdn
    33| from synapse.util.ratelimitutils import FederationRateLimiter
    34| from synapse.util.stringutils import assert_valid_client_secret, random_string
    35| from synapse.util.threepids import canonicalise_email, check_3pid_allowed
    36| from ._base import client_patterns, interactive_auth_handler
    37| if hasattr(hmac, "compare_digest"):
    38|     compare_digest = hmac.compare_digest
    39| else:
    40|     def compare_digest(a, b):
    41|         return a == b
    42| logger = logging.getLogger(__name__)
    43| class EmailRegisterRequestTokenRestServlet(RestServlet):
    44|     PATTERNS = client_patterns("/register/email/requestToken$")
    45|     def __init__(self, hs):
    46|         """
    47|         Args:
    48|             hs (synapse.server.HomeServer): server
    49|         """
    50|         super(EmailRegisterRequestTokenRestServlet, self).__init__()
    51|         self.hs = hs
    52|         self.identity_handler = hs.get_handlers().identity_handler
    53|         self.config = hs.config
    54|         if self.hs.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:
    55|             self.mailer = Mailer(
    56|                 hs=self.hs,
    57|                 app_name=self.config.email_app_name,
    58|                 template_html=self.config.email_registration_template_html,
    59|                 template_text=self.config.email_registration_template_text,
    60|             )
    61|     async def on_POST(self, request):
    62|         if self.hs.config.threepid_behaviour_email == ThreepidBehaviour.OFF:
    63|             if self.hs.config.local_threepid_handling_disabled_due_to_email_config:
    64|                 logger.warning(
    65|                     "Email registration has been disabled due to lack of email config"
    66|                 )
    67|             raise SynapseError(
    68|                 400, "Email-based registration has been disabled on this server"
    69|             )
    70|         body = parse_json_object_from_request(request)

# --- HUNK 2: Lines 100-140 ---
   100|                 send_attempt,
   101|                 next_link,
   102|             )
   103|         else:
   104|             sid = await self.identity_handler.send_threepid_validation(
   105|                 email,
   106|                 client_secret,
   107|                 send_attempt,
   108|                 self.mailer.send_registration_mail,
   109|                 next_link,
   110|             )
   111|             ret = {"sid": sid}
   112|         return 200, ret
   113| class MsisdnRegisterRequestTokenRestServlet(RestServlet):
   114|     PATTERNS = client_patterns("/register/msisdn/requestToken$")
   115|     def __init__(self, hs):
   116|         """
   117|         Args:
   118|             hs (synapse.server.HomeServer): server
   119|         """
   120|         super(MsisdnRegisterRequestTokenRestServlet, self).__init__()
   121|         self.hs = hs
   122|         self.identity_handler = hs.get_handlers().identity_handler
   123|     async def on_POST(self, request):
   124|         body = parse_json_object_from_request(request)
   125|         assert_params_in_dict(
   126|             body, ["client_secret", "country", "phone_number", "send_attempt"]
   127|         )
   128|         client_secret = body["client_secret"]
   129|         country = body["country"]
   130|         phone_number = body["phone_number"]
   131|         send_attempt = body["send_attempt"]
   132|         next_link = body.get("next_link")  # Optional param
   133|         msisdn = phone_number_to_msisdn(country, phone_number)
   134|         if not check_3pid_allowed(self.hs, "msisdn", msisdn):
   135|             raise SynapseError(
   136|                 403,
   137|                 "Phone numbers are not authorized to register on this server",
   138|                 Codes.THREEPID_DENIED,
   139|             )
   140|         existing_user_id = await self.hs.get_datastore().get_user_id_by_threepid(

# --- HUNK 3: Lines 157-197 ---
   157|             )
   158|         ret = await self.identity_handler.requestMsisdnToken(
   159|             self.hs.config.account_threepid_delegate_msisdn,
   160|             country,
   161|             phone_number,
   162|             client_secret,
   163|             send_attempt,
   164|             next_link,
   165|         )
   166|         return 200, ret
   167| class RegistrationSubmitTokenServlet(RestServlet):
   168|     """Handles registration 3PID validation token submission"""
   169|     PATTERNS = client_patterns(
   170|         "/registration/(?P<medium>[^/]*)/submit_token$", releases=(), unstable=True
   171|     )
   172|     def __init__(self, hs):
   173|         """
   174|         Args:
   175|             hs (synapse.server.HomeServer): server
   176|         """
   177|         super(RegistrationSubmitTokenServlet, self).__init__()
   178|         self.hs = hs
   179|         self.auth = hs.get_auth()
   180|         self.config = hs.config
   181|         self.clock = hs.get_clock()
   182|         self.store = hs.get_datastore()
   183|         if self.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:
   184|             self._failure_email_template = (
   185|                 self.config.email_registration_template_failure_html
   186|             )
   187|     async def on_GET(self, request, medium):
   188|         if medium != "email":
   189|             raise SynapseError(
   190|                 400, "This medium is currently not supported for registration"
   191|             )
   192|         if self.config.threepid_behaviour_email == ThreepidBehaviour.OFF:
   193|             if self.config.local_threepid_handling_disabled_due_to_email_config:
   194|                 logger.warning(
   195|                     "User registration via email has been disabled due to lack of email config"
   196|                 )
   197|             raise SynapseError(

# --- HUNK 4: Lines 211-282 ---
   211|                     )
   212|                 else:
   213|                     request.setResponseCode(302)
   214|                     request.setHeader("Location", next_link)
   215|                     finish_request(request)
   216|                     return None
   217|             html = self.config.email_registration_template_success_html_content
   218|             status_code = 200
   219|         except ThreepidValidationError as e:
   220|             status_code = e.code
   221|             template_vars = {"failure_reason": e.msg}
   222|             html = self._failure_email_template.render(**template_vars)
   223|         respond_with_html(request, status_code, html)
   224| class UsernameAvailabilityRestServlet(RestServlet):
   225|     PATTERNS = client_patterns("/register/available")
   226|     def __init__(self, hs):
   227|         """
   228|         Args:
   229|             hs (synapse.server.HomeServer): server
   230|         """
   231|         super(UsernameAvailabilityRestServlet, self).__init__()
   232|         self.hs = hs
   233|         self.registration_handler = hs.get_registration_handler()
   234|         self.ratelimiter = FederationRateLimiter(
   235|             hs.get_clock(),
   236|             FederationRateLimitConfig(
   237|                 window_size=2000,
   238|                 sleep_limit=1,
   239|                 sleep_msec=1000,
   240|                 reject_limit=1,
   241|                 concurrent_requests=1,
   242|             ),
   243|         )
   244|     async def on_GET(self, request):
   245|         if not self.hs.config.enable_registration:
   246|             raise SynapseError(
   247|                 403, "Registration has been disabled", errcode=Codes.FORBIDDEN
   248|             )
   249|         ip = self.hs.get_ip_from_request(request)
   250|         with self.ratelimiter.ratelimit(ip) as wait_deferred:
   251|             await wait_deferred
   252|             username = parse_string(request, "username", required=True)
   253|             await self.registration_handler.check_username(username)
   254|             return 200, {"available": True}
   255| class RegisterRestServlet(RestServlet):
   256|     PATTERNS = client_patterns("/register$")
   257|     def __init__(self, hs):
   258|         """
   259|         Args:
   260|             hs (synapse.server.HomeServer): server
   261|         """
   262|         super(RegisterRestServlet, self).__init__()
   263|         self.hs = hs
   264|         self.auth = hs.get_auth()
   265|         self.store = hs.get_datastore()
   266|         self.auth_handler = hs.get_auth_handler()
   267|         self.registration_handler = hs.get_registration_handler()
   268|         self.identity_handler = hs.get_handlers().identity_handler
   269|         self.room_member_handler = hs.get_room_member_handler()
   270|         self.macaroon_gen = hs.get_macaroon_generator()
   271|         self.ratelimiter = hs.get_registration_ratelimiter()
   272|         self.password_policy_handler = hs.get_password_policy_handler()
   273|         self.clock = hs.get_clock()
   274|         self._registration_enabled = self.hs.config.enable_registration
   275|         self._registration_flows = _calculate_registration_flows(
   276|             hs.config, self.auth_handler
   277|         )
   278|     @interactive_auth_handler
   279|     async def on_POST(self, request):
   280|         body = parse_json_object_from_request(request)
   281|         client_addr = request.getClientIP()
   282|         self.ratelimiter.ratelimit(client_addr, update=False)

# --- HUNK 5: Lines 284-328 ---
   284|         if b"kind" in request.args:
   285|             kind = request.args[b"kind"][0]
   286|         if kind == b"guest":
   287|             ret = await self._do_guest_registration(body, address=client_addr)
   288|             return ret
   289|         elif kind != b"user":
   290|             raise UnrecognizedRequestError(
   291|                 "Do not understand membership kind: %s" % (kind.decode("utf8"),)
   292|             )
   293|         desired_username = None
   294|         if "username" in body:
   295|             if not isinstance(body["username"], str) or len(body["username"]) > 512:
   296|                 raise SynapseError(400, "Invalid username")
   297|             desired_username = body["username"]
   298|         appservice = None
   299|         if self.auth.has_access_token(request):
   300|             appservice = self.auth.get_appservice_by_req(request)
   301|         if appservice:
   302|             desired_username = body.get("user", desired_username)
   303|             access_token = self.auth.get_access_token_from_request(request)
   304|             if isinstance(desired_username, str):
   305|                 result = await self._do_appservice_registration(
   306|                     desired_username, access_token, body
   307|                 )
   308|             return 200, result  # we throw for non 200 responses
   309|         if not self._registration_enabled:
   310|             raise SynapseError(403, "Registration has been disabled")
   311|         if desired_username is not None:
   312|             desired_username = desired_username.lower()
   313|         guest_access_token = body.get("guest_access_token", None)
   314|         password = body.pop("password", None)
   315|         if password is not None:
   316|             if not isinstance(password, str) or len(password) > 512:
   317|                 raise SynapseError(400, "Invalid password")
   318|             self.password_policy_handler.validate_password(password)
   319|         if "initial_device_display_name" in body and password is None:
   320|             logger.warning("Ignoring initial_device_display_name without password")
   321|             del body["initial_device_display_name"]
   322|         session_id = self.auth_handler.get_session_id(body)
   323|         registered_user_id = None
   324|         password_hash = None
   325|         if session_id:
   326|             registered_user_id = await self.auth_handler.get_session_data(
   327|                 session_id, "registered_user_id", None
   328|             )


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/relations.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 17-57 ---
    17|     PaginationChunk,
    18|     RelationPaginationToken,
    19| )
    20| from synapse.util.stringutils import random_string
    21| from ._base import client_patterns
    22| logger = logging.getLogger(__name__)
    23| class RelationSendServlet(RestServlet):
    24|     """Helper API for sending events that have relation data.
    25|     Example API shape to send a  reaction to a room:
    26|         POST /rooms/!foo/send_relation/$bar/m.annotation/m.reaction?key=%F0%9F%91%8D
    27|         {}
    28|         {
    29|             "event_id": "$foobar"
    30|         }
    31|     """
    32|     PATTERN = (
    33|         "/rooms/(?P<room_id>[^/]*)/send_relation"
    34|         "/(?P<parent_id>[^/]*)/(?P<relation_type>[^/]*)/(?P<event_type>[^/]*)"
    35|     )
    36|     def __init__(self, hs):
    37|         super(RelationSendServlet, self).__init__()
    38|         self.auth = hs.get_auth()
    39|         self.event_creation_handler = hs.get_event_creation_handler()
    40|         self.txns = HttpTransactionCache(hs)
    41|     def register(self, http_server):
    42|         http_server.register_paths(
    43|             "POST",
    44|             client_patterns(self.PATTERN + "$", releases=()),
    45|             self.on_PUT_or_POST,
    46|             self.__class__.__name__,
    47|         )
    48|         http_server.register_paths(
    49|             "PUT",
    50|             client_patterns(self.PATTERN + "/(?P<txn_id>[^/]*)$", releases=()),
    51|             self.on_PUT,
    52|             self.__class__.__name__,
    53|         )
    54|     def on_PUT(self, request, *args, **kwargs):
    55|         return self.txns.fetch_or_execute_request(
    56|             request, self.on_PUT_or_POST, request, *args, **kwargs
    57|         )

# --- HUNK 2: Lines 78-118 ---
    78|             (
    79|                 event,
    80|                 _,
    81|             ) = await self.event_creation_handler.create_and_send_nonmember_event(
    82|                 requester, event_dict=event_dict, txn_id=txn_id
    83|             )
    84|             event_id = event.event_id
    85|         except ShadowBanError:
    86|             event_id = "$" + random_string(43)
    87|         return 200, {"event_id": event_id}
    88| class RelationPaginationServlet(RestServlet):
    89|     """API to paginate relations on an event by topological ordering, optionally
    90|     filtered by relation type and event type.
    91|     """
    92|     PATTERNS = client_patterns(
    93|         "/rooms/(?P<room_id>[^/]*)/relations/(?P<parent_id>[^/]*)"
    94|         "(/(?P<relation_type>[^/]*)(/(?P<event_type>[^/]*))?)?$",
    95|         releases=(),
    96|     )
    97|     def __init__(self, hs):
    98|         super(RelationPaginationServlet, self).__init__()
    99|         self.auth = hs.get_auth()
   100|         self.store = hs.get_datastore()
   101|         self.clock = hs.get_clock()
   102|         self._event_serializer = hs.get_event_client_serializer()
   103|         self.event_handler = hs.get_event_handler()
   104|     async def on_GET(
   105|         self, request, room_id, parent_id, relation_type=None, event_type=None
   106|     ):
   107|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   108|         await self.auth.check_user_in_room_or_world_readable(
   109|             room_id, requester.user.to_string(), allow_departed_users=True
   110|         )
   111|         event = await self.event_handler.get_event(requester.user, room_id, parent_id)
   112|         limit = parse_integer(request, "limit", default=5)
   113|         from_token = parse_string(request, "from")
   114|         to_token = parse_string(request, "to")
   115|         if event.internal_metadata.is_redacted():
   116|             pagination_chunk = PaginationChunk(chunk=[])
   117|         else:
   118|             if from_token:

# --- HUNK 3: Lines 145-185 ---
   145|     """API to paginate aggregation groups of relations, e.g. paginate the
   146|     types and counts of the reactions on the events.
   147|     Example request and response:
   148|         GET /rooms/{room_id}/aggregations/{parent_id}
   149|         {
   150|             chunk: [
   151|                 {
   152|                     "type": "m.reaction",
   153|                     "key": "",
   154|                     "count": 3
   155|                 }
   156|             ]
   157|         }
   158|     """
   159|     PATTERNS = client_patterns(
   160|         "/rooms/(?P<room_id>[^/]*)/aggregations/(?P<parent_id>[^/]*)"
   161|         "(/(?P<relation_type>[^/]*)(/(?P<event_type>[^/]*))?)?$",
   162|         releases=(),
   163|     )
   164|     def __init__(self, hs):
   165|         super(RelationAggregationPaginationServlet, self).__init__()
   166|         self.auth = hs.get_auth()
   167|         self.store = hs.get_datastore()
   168|         self.event_handler = hs.get_event_handler()
   169|     async def on_GET(
   170|         self, request, room_id, parent_id, relation_type=None, event_type=None
   171|     ):
   172|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   173|         await self.auth.check_user_in_room_or_world_readable(
   174|             room_id, requester.user.to_string(), allow_departed_users=True,
   175|         )
   176|         event = await self.event_handler.get_event(requester.user, room_id, parent_id)
   177|         if relation_type not in (RelationTypes.ANNOTATION, None):
   178|             raise SynapseError(400, "Relation type must be 'annotation'")
   179|         limit = parse_integer(request, "limit", default=5)
   180|         from_token = parse_string(request, "from")
   181|         to_token = parse_string(request, "to")
   182|         if event.internal_metadata.is_redacted():
   183|             pagination_chunk = PaginationChunk(chunk=[])
   184|         else:
   185|             if from_token:

# --- HUNK 4: Lines 203-243 ---
   203|             chunk: [
   204|                 {
   205|                     "type": "m.reaction",
   206|                     "content": {
   207|                         "m.relates_to": {
   208|                             "rel_type": "m.annotation",
   209|                             "key": ""
   210|                         }
   211|                     }
   212|                 },
   213|                 ...
   214|             ]
   215|         }
   216|     """
   217|     PATTERNS = client_patterns(
   218|         "/rooms/(?P<room_id>[^/]*)/aggregations/(?P<parent_id>[^/]*)"
   219|         "/(?P<relation_type>[^/]*)/(?P<event_type>[^/]*)/(?P<key>[^/]*)$",
   220|         releases=(),
   221|     )
   222|     def __init__(self, hs):
   223|         super(RelationAggregationGroupPaginationServlet, self).__init__()
   224|         self.auth = hs.get_auth()
   225|         self.store = hs.get_datastore()
   226|         self.clock = hs.get_clock()
   227|         self._event_serializer = hs.get_event_client_serializer()
   228|         self.event_handler = hs.get_event_handler()
   229|     async def on_GET(self, request, room_id, parent_id, relation_type, event_type, key):
   230|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   231|         await self.auth.check_user_in_room_or_world_readable(
   232|             room_id, requester.user.to_string(), allow_departed_users=True,
   233|         )
   234|         await self.event_handler.get_event(requester.user, room_id, parent_id)
   235|         if relation_type != RelationTypes.ANNOTATION:
   236|             raise SynapseError(400, "Relation type must be 'annotation'")
   237|         limit = parse_integer(request, "limit", default=5)
   238|         from_token = parse_string(request, "from")
   239|         to_token = parse_string(request, "to")
   240|         if from_token:
   241|             from_token = RelationPaginationToken.from_string(from_token)
   242|         if to_token:
   243|             to_token = RelationPaginationToken.from_string(to_token)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/report_event.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-34 ---
     1| import logging
     2| from http import HTTPStatus
     3| from synapse.api.errors import Codes, SynapseError
     4| from synapse.http.servlet import (
     5|     RestServlet,
     6|     assert_params_in_dict,
     7|     parse_json_object_from_request,
     8| )
     9| from ._base import client_patterns
    10| logger = logging.getLogger(__name__)
    11| class ReportEventRestServlet(RestServlet):
    12|     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/report/(?P<event_id>[^/]*)$")
    13|     def __init__(self, hs):
    14|         super(ReportEventRestServlet, self).__init__()
    15|         self.hs = hs
    16|         self.auth = hs.get_auth()
    17|         self.clock = hs.get_clock()
    18|         self.store = hs.get_datastore()
    19|     async def on_POST(self, request, room_id, event_id):
    20|         requester = await self.auth.get_user_by_req(request)
    21|         user_id = requester.user.to_string()
    22|         body = parse_json_object_from_request(request)
    23|         assert_params_in_dict(body, ("reason", "score"))
    24|         if not isinstance(body["reason"], str):
    25|             raise SynapseError(
    26|                 HTTPStatus.BAD_REQUEST,
    27|                 "Param 'reason' must be a string",
    28|                 Codes.BAD_JSON,
    29|             )
    30|         if not isinstance(body["score"], int):
    31|             raise SynapseError(
    32|                 HTTPStatus.BAD_REQUEST,
    33|                 "Param 'score' must be an integer",
    34|                 Codes.BAD_JSON,


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/room_keys.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-39 ---
     1| import logging
     2| from synapse.api.errors import Codes, NotFoundError, SynapseError
     3| from synapse.http.servlet import (
     4|     RestServlet,
     5|     parse_json_object_from_request,
     6|     parse_string,
     7| )
     8| from ._base import client_patterns
     9| logger = logging.getLogger(__name__)
    10| class RoomKeysServlet(RestServlet):
    11|     PATTERNS = client_patterns(
    12|         "/room_keys/keys(/(?P<room_id>[^/]+))?(/(?P<session_id>[^/]+))?$"
    13|     )
    14|     def __init__(self, hs):
    15|         """
    16|         Args:
    17|             hs (synapse.server.HomeServer): server
    18|         """
    19|         super(RoomKeysServlet, self).__init__()
    20|         self.auth = hs.get_auth()
    21|         self.e2e_room_keys_handler = hs.get_e2e_room_keys_handler()
    22|     async def on_PUT(self, request, room_id, session_id):
    23|         """
    24|         Uploads one or more encrypted E2E room keys for backup purposes.
    25|         room_id: the ID of the room the keys are for (optional)
    26|         session_id: the ID for the E2E room keys for the room (optional)
    27|         version: the version of the user's backup which this data is for.
    28|         the version must already have been created via the /room_keys/version API.
    29|         Each session has:
    30|          * first_message_index: a numeric index indicating the oldest message
    31|            encrypted by this session.
    32|          * forwarded_count: how many times the uploading client claims this key
    33|            has been shared (forwarded)
    34|          * is_verified: whether the client that uploaded the keys claims they
    35|            were sent by a device which they've verified
    36|          * session_data: base64-encrypted data describing the session.
    37|         Returns 200 OK on success with body {}
    38|         Returns 403 Forbidden if the version in question is not the most recently
    39|         created version (i.e. if this is an old client trying to write to a stale backup)

# --- HUNK 2: Lines 166-246 ---
   166|         {}
   167|         room_id: the ID of the room whose keys to delete (optional)
   168|         session_id: the ID for the E2E session to delete (optional)
   169|         version: the version of the user's backup which this data is for.
   170|         the version must already have been created via the /change_secret API.
   171|         """
   172|         requester = await self.auth.get_user_by_req(request, allow_guest=False)
   173|         user_id = requester.user.to_string()
   174|         version = parse_string(request, "version")
   175|         ret = await self.e2e_room_keys_handler.delete_room_keys(
   176|             user_id, version, room_id, session_id
   177|         )
   178|         return 200, ret
   179| class RoomKeysNewVersionServlet(RestServlet):
   180|     PATTERNS = client_patterns("/room_keys/version$")
   181|     def __init__(self, hs):
   182|         """
   183|         Args:
   184|             hs (synapse.server.HomeServer): server
   185|         """
   186|         super(RoomKeysNewVersionServlet, self).__init__()
   187|         self.auth = hs.get_auth()
   188|         self.e2e_room_keys_handler = hs.get_e2e_room_keys_handler()
   189|     async def on_POST(self, request):
   190|         """
   191|         Create a new backup version for this user's room_keys with the given
   192|         info.  The version is allocated by the server and returned to the user
   193|         in the response.  This API is intended to be used whenever the user
   194|         changes the encryption key for their backups, ensuring that backups
   195|         encrypted with different keys don't collide.
   196|         It takes out an exclusive lock on this user's room_key backups, to ensure
   197|         clients only upload to the current backup.
   198|         The algorithm passed in the version info is a reverse-DNS namespaced
   199|         identifier to describe the format of the encrypted backupped keys.
   200|         The auth_data is { user_id: "user_id", nonce: <random string> }
   201|         encrypted using the algorithm and current encryption key described above.
   202|         POST /room_keys/version
   203|         Content-Type: application/json
   204|         {
   205|             "algorithm": "m.megolm_backup.v1",
   206|             "auth_data": "dGhpcyBzaG91bGQgYWN0dWFsbHkgYmUgZW5jcnlwdGVkIGpzb24K"
   207|         }
   208|         HTTP/1.1 200 OK
   209|         Content-Type: application/json
   210|         {
   211|             "version": 12345
   212|         }
   213|         """
   214|         requester = await self.auth.get_user_by_req(request, allow_guest=False)
   215|         user_id = requester.user.to_string()
   216|         info = parse_json_object_from_request(request)
   217|         new_version = await self.e2e_room_keys_handler.create_version(user_id, info)
   218|         return 200, {"version": new_version}
   219| class RoomKeysVersionServlet(RestServlet):
   220|     PATTERNS = client_patterns("/room_keys/version(/(?P<version>[^/]+))?$")
   221|     def __init__(self, hs):
   222|         """
   223|         Args:
   224|             hs (synapse.server.HomeServer): server
   225|         """
   226|         super(RoomKeysVersionServlet, self).__init__()
   227|         self.auth = hs.get_auth()
   228|         self.e2e_room_keys_handler = hs.get_e2e_room_keys_handler()
   229|     async def on_GET(self, request, version):
   230|         """
   231|         Retrieve the version information about a given version of the user's
   232|         room_keys backup.  If the version part is missing, returns info about the
   233|         most current backup version (if any)
   234|         It takes out an exclusive lock on this user's room_key backups, to ensure
   235|         clients only upload to the current backup.
   236|         Returns 404 if the given version does not exist.
   237|         GET /room_keys/version/12345 HTTP/1.1
   238|         {
   239|             "version": "12345",
   240|             "algorithm": "m.megolm_backup.v1",
   241|             "auth_data": "dGhpcyBzaG91bGQgYWN0dWFsbHkgYmUgZW5jcnlwdGVkIGpzb24K"
   242|         }
   243|         """
   244|         requester = await self.auth.get_user_by_req(request, allow_guest=False)
   245|         user_id = requester.user.to_string()
   246|         try:


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/room_upgrade_rest_servlet.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 8-48 ---
     8| )
     9| from synapse.util import stringutils
    10| from ._base import client_patterns
    11| logger = logging.getLogger(__name__)
    12| class RoomUpgradeRestServlet(RestServlet):
    13|     """Handler for room uprade requests.
    14|     Handles requests of the form:
    15|         POST /_matrix/client/r0/rooms/$roomid/upgrade HTTP/1.1
    16|         Content-Type: application/json
    17|         {
    18|             "new_version": "2",
    19|         }
    20|     Creates a new room and shuts down the old one. Returns the ID of the new room.
    21|     Args:
    22|         hs (synapse.server.HomeServer):
    23|     """
    24|     PATTERNS = client_patterns(
    25|         "/rooms/(?P<room_id>[^/]*)/upgrade$"
    26|     )
    27|     def __init__(self, hs):
    28|         super(RoomUpgradeRestServlet, self).__init__()
    29|         self._hs = hs
    30|         self._room_creation_handler = hs.get_room_creation_handler()
    31|         self._auth = hs.get_auth()
    32|     async def on_POST(self, request, room_id):
    33|         requester = await self._auth.get_user_by_req(request)
    34|         content = parse_json_object_from_request(request)
    35|         assert_params_in_dict(content, ("new_version",))
    36|         new_version = KNOWN_ROOM_VERSIONS.get(content["new_version"])
    37|         if new_version is None:
    38|             raise SynapseError(
    39|                 400,
    40|                 "Your homeserver does not support this room version",
    41|                 Codes.UNSUPPORTED_ROOM_VERSION,
    42|             )
    43|         try:
    44|             new_room_id = await self._room_creation_handler.upgrade_room(
    45|                 requester, room_id, new_version
    46|             )
    47|         except ShadowBanError:
    48|             new_room_id = stringutils.random_string(18)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/sendtodevice.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-38 ---
     1| import logging
     2| from typing import Tuple
     3| from synapse.http import servlet
     4| from synapse.http.servlet import parse_json_object_from_request
     5| from synapse.logging.opentracing import set_tag, trace
     6| from synapse.rest.client.transactions import HttpTransactionCache
     7| from ._base import client_patterns
     8| logger = logging.getLogger(__name__)
     9| class SendToDeviceRestServlet(servlet.RestServlet):
    10|     PATTERNS = client_patterns(
    11|         "/sendToDevice/(?P<message_type>[^/]*)/(?P<txn_id>[^/]*)$"
    12|     )
    13|     def __init__(self, hs):
    14|         """
    15|         Args:
    16|             hs (synapse.server.HomeServer): server
    17|         """
    18|         super(SendToDeviceRestServlet, self).__init__()
    19|         self.hs = hs
    20|         self.auth = hs.get_auth()
    21|         self.txns = HttpTransactionCache(hs)
    22|         self.device_message_handler = hs.get_device_message_handler()
    23|     @trace(opname="sendToDevice")
    24|     def on_PUT(self, request, message_type, txn_id):
    25|         set_tag("message_type", message_type)
    26|         set_tag("txn_id", txn_id)
    27|         return self.txns.fetch_or_execute_request(
    28|             request, self._put, request, message_type, txn_id
    29|         )
    30|     async def _put(self, request, message_type, txn_id):
    31|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    32|         content = parse_json_object_from_request(request)
    33|         sender_user_id = requester.user.to_string()
    34|         await self.device_message_handler.send_device_message(
    35|             sender_user_id, message_type, content["messages"]
    36|         )
    37|         response = (200, {})  # type: Tuple[int, dict]
    38|         return response


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/shared_rooms.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-36 ---
     1| import logging
     2| from synapse.api.errors import Codes, SynapseError
     3| from synapse.http.servlet import RestServlet
     4| from synapse.types import UserID
     5| from ._base import client_patterns
     6| logger = logging.getLogger(__name__)
     7| class UserSharedRoomsServlet(RestServlet):
     8|     """
     9|     GET /uk.half-shot.msc2666/user/shared_rooms/{user_id} HTTP/1.1
    10|     """
    11|     PATTERNS = client_patterns(
    12|         "/uk.half-shot.msc2666/user/shared_rooms/(?P<user_id>[^/]*)",
    13|         releases=(),  # This is an unstable feature
    14|     )
    15|     def __init__(self, hs):
    16|         super(UserSharedRoomsServlet, self).__init__()
    17|         self.auth = hs.get_auth()
    18|         self.store = hs.get_datastore()
    19|         self.user_directory_active = hs.config.update_user_directory
    20|     async def on_GET(self, request, user_id):
    21|         if not self.user_directory_active:
    22|             raise SynapseError(
    23|                 code=400,
    24|                 msg="The user directory is disabled on this server. Cannot determine shared rooms.",
    25|                 errcode=Codes.FORBIDDEN,
    26|             )
    27|         UserID.from_string(user_id)
    28|         requester = await self.auth.get_user_by_req(request)
    29|         if user_id == requester.user.to_string():
    30|             raise SynapseError(
    31|                 code=400,
    32|                 msg="You cannot request a list of shared rooms with yourself",
    33|                 errcode=Codes.FORBIDDEN,
    34|             )
    35|         rooms = await self.store.get_shared_rooms_for_users(
    36|             requester.user.to_string(), user_id


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/sync.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 33-75 ---
    33|                 "timeline": { // The recent events in the room if gap is "true"
    34|                   "limited": // Was the per-room event limit exceeded?
    35|                              // otherwise the next events in the room.
    36|                   "events": [] // list of EventIDs in the "event_map".
    37|                   "prev_batch": // back token for getting previous events.
    38|                 }
    39|                 "state": {"events": []} // list of EventIDs updating the
    40|                                         // current state to be what it should
    41|                                         // be at the end of the batch.
    42|                 "ephemeral": {"events": []} // list of event objects
    43|               }
    44|             },
    45|             "invite": {}, // Invited rooms being updated.
    46|             "leave": {} // Archived rooms being updated.
    47|           }
    48|         }
    49|     """
    50|     PATTERNS = client_patterns("/sync$")
    51|     ALLOWED_PRESENCE = {"online", "offline", "unavailable"}
    52|     def __init__(self, hs):
    53|         super(SyncRestServlet, self).__init__()
    54|         self.hs = hs
    55|         self.auth = hs.get_auth()
    56|         self.sync_handler = hs.get_sync_handler()
    57|         self.clock = hs.get_clock()
    58|         self.filtering = hs.get_filtering()
    59|         self.presence_handler = hs.get_presence_handler()
    60|         self._server_notices_sender = hs.get_server_notices_sender()
    61|         self._event_serializer = hs.get_event_client_serializer()
    62|     async def on_GET(self, request):
    63|         if b"from" in request.args:
    64|             raise SynapseError(
    65|                 400, "'from' is not a valid query parameter. Did you mean 'since'?"
    66|             )
    67|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    68|         user = requester.user
    69|         device_id = requester.device_id
    70|         timeout = parse_integer(request, "timeout", default=0)
    71|         since = parse_string(request, "since")
    72|         set_presence = parse_string(
    73|             request,
    74|             "set_presence",
    75|             default="online",

# --- HUNK 2: Lines 99-142 ---
    99|             except Exception:
   100|                 raise SynapseError(400, "Invalid filter JSON")
   101|             self.filtering.check_valid_filter(filter_object)
   102|             filter_collection = FilterCollection(filter_object)
   103|         else:
   104|             try:
   105|                 filter_collection = await self.filtering.get_user_filter(
   106|                     user.localpart, filter_id
   107|                 )
   108|             except StoreError as err:
   109|                 if err.code != 404:
   110|                     raise
   111|                 raise SynapseError(400, "No such filter", errcode=Codes.INVALID_PARAM)
   112|         sync_config = SyncConfig(
   113|             user=user,
   114|             filter_collection=filter_collection,
   115|             is_guest=requester.is_guest,
   116|             request_key=request_key,
   117|             device_id=device_id,
   118|         )
   119|         if since is not None:
   120|             since_token = StreamToken.from_string(since)
   121|         else:
   122|             since_token = None
   123|         await self._server_notices_sender.on_user_syncing(user.to_string())
   124|         affect_presence = set_presence != PresenceState.OFFLINE
   125|         if affect_presence:
   126|             await self.presence_handler.set_state(
   127|                 user, {"presence": set_presence}, True
   128|             )
   129|         context = await self.presence_handler.user_syncing(
   130|             user.to_string(), affect_presence=affect_presence
   131|         )
   132|         with context:
   133|             sync_result = await self.sync_handler.wait_for_sync_for_user(
   134|                 sync_config,
   135|                 since_token=since_token,
   136|                 timeout=timeout,
   137|                 full_state=full_state,
   138|             )
   139|         if request._disconnected:
   140|             logger.info("Client has disconnected; not serializing response.")
   141|             return 200, {}
   142|         time_now = self.clock.time_msec()

# --- HUNK 3: Lines 169-209 ---
   169|             access_token_id,
   170|             filter.event_fields,
   171|             event_formatter,
   172|         )
   173|         logger.debug("building sync response dict")
   174|         return {
   175|             "account_data": {"events": sync_result.account_data},
   176|             "to_device": {"events": sync_result.to_device},
   177|             "device_lists": {
   178|                 "changed": list(sync_result.device_lists.changed),
   179|                 "left": list(sync_result.device_lists.left),
   180|             },
   181|             "presence": SyncRestServlet.encode_presence(sync_result.presence, time_now),
   182|             "rooms": {"join": joined, "invite": invited, "leave": archived},
   183|             "groups": {
   184|                 "join": sync_result.groups.join,
   185|                 "invite": sync_result.groups.invite,
   186|                 "leave": sync_result.groups.leave,
   187|             },
   188|             "device_one_time_keys_count": sync_result.device_one_time_keys_count,
   189|             "next_batch": sync_result.next_batch.to_string(),
   190|         }
   191|     @staticmethod
   192|     def encode_presence(events, time_now):
   193|         return {
   194|             "events": [
   195|                 {
   196|                     "type": "m.presence",
   197|                     "sender": event.user_id,
   198|                     "content": format_user_presence_state(
   199|                         event, time_now, include_user_id=False
   200|                     ),
   201|                 }
   202|                 for event in events
   203|             ]
   204|         }
   205|     async def encode_joined(
   206|         self, rooms, time_now, token_id, event_fields, event_formatter
   207|     ):
   208|         """
   209|         Encode the joined rooms in a sync result

# --- HUNK 4: Lines 323-357 ---
   323|                 event_format=event_formatter,
   324|                 only_event_fields=only_fields,
   325|             )
   326|         state_dict = room.state
   327|         timeline_events = room.timeline.events
   328|         state_events = state_dict.values()
   329|         for event in itertools.chain(state_events, timeline_events):
   330|             if event.room_id != room.room_id:
   331|                 logger.warning(
   332|                     "Event %r is under room %r instead of %r",
   333|                     event.event_id,
   334|                     room.room_id,
   335|                     event.room_id,
   336|                 )
   337|         serialized_state = await serialize(state_events)
   338|         serialized_timeline = await serialize(timeline_events)
   339|         account_data = room.account_data
   340|         result = {
   341|             "timeline": {
   342|                 "events": serialized_timeline,
   343|                 "prev_batch": room.timeline.prev_batch.to_string(),
   344|                 "limited": room.timeline.limited,
   345|             },
   346|             "state": {"events": serialized_state},
   347|             "account_data": {"events": account_data},
   348|         }
   349|         if joined:
   350|             ephemeral_events = room.ephemeral
   351|             result["ephemeral"] = {"events": ephemeral_events}
   352|             result["unread_notifications"] = room.unread_notifications
   353|             result["summary"] = room.summary
   354|             result["org.matrix.msc2654.unread_count"] = room.unread_count
   355|         return result
   356| def register_servlets(hs, http_server):
   357|     SyncRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/tags.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-50 ---
     1| import logging
     2| from synapse.api.errors import AuthError
     3| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     4| from ._base import client_patterns
     5| logger = logging.getLogger(__name__)
     6| class TagListServlet(RestServlet):
     7|     """
     8|     GET /user/{user_id}/rooms/{room_id}/tags HTTP/1.1
     9|     """
    10|     PATTERNS = client_patterns("/user/(?P<user_id>[^/]*)/rooms/(?P<room_id>[^/]*)/tags")
    11|     def __init__(self, hs):
    12|         super(TagListServlet, self).__init__()
    13|         self.auth = hs.get_auth()
    14|         self.store = hs.get_datastore()
    15|     async def on_GET(self, request, user_id, room_id):
    16|         requester = await self.auth.get_user_by_req(request)
    17|         if user_id != requester.user.to_string():
    18|             raise AuthError(403, "Cannot get tags for other users.")
    19|         tags = await self.store.get_tags_for_room(user_id, room_id)
    20|         return 200, {"tags": tags}
    21| class TagServlet(RestServlet):
    22|     """
    23|     PUT /user/{user_id}/rooms/{room_id}/tags/{tag} HTTP/1.1
    24|     DELETE /user/{user_id}/rooms/{room_id}/tags/{tag} HTTP/1.1
    25|     """
    26|     PATTERNS = client_patterns(
    27|         "/user/(?P<user_id>[^/]*)/rooms/(?P<room_id>[^/]*)/tags/(?P<tag>[^/]*)"
    28|     )
    29|     def __init__(self, hs):
    30|         super(TagServlet, self).__init__()
    31|         self.auth = hs.get_auth()
    32|         self.store = hs.get_datastore()
    33|         self.notifier = hs.get_notifier()
    34|     async def on_PUT(self, request, user_id, room_id, tag):
    35|         requester = await self.auth.get_user_by_req(request)
    36|         if user_id != requester.user.to_string():
    37|             raise AuthError(403, "Cannot add tags for other users.")
    38|         body = parse_json_object_from_request(request)
    39|         max_id = await self.store.add_tag_to_room(user_id, room_id, tag, body)
    40|         self.notifier.on_new_event("account_data_key", max_id, users=[user_id])
    41|         return 200, {}
    42|     async def on_DELETE(self, request, user_id, room_id, tag):
    43|         requester = await self.auth.get_user_by_req(request)
    44|         if user_id != requester.user.to_string():
    45|             raise AuthError(403, "Cannot add tags for other users.")
    46|         max_id = await self.store.remove_tag_from_room(user_id, room_id, tag)
    47|         self.notifier.on_new_event("account_data_key", max_id, users=[user_id])
    48|         return 200, {}
    49| def register_servlets(hs, http_server):
    50|     TagListServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/thirdparty.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-63 ---
     1| import logging
     2| from synapse.api.constants import ThirdPartyEntityKind
     3| from synapse.http.servlet import RestServlet
     4| from ._base import client_patterns
     5| logger = logging.getLogger(__name__)
     6| class ThirdPartyProtocolsServlet(RestServlet):
     7|     PATTERNS = client_patterns("/thirdparty/protocols")
     8|     def __init__(self, hs):
     9|         super(ThirdPartyProtocolsServlet, self).__init__()
    10|         self.auth = hs.get_auth()
    11|         self.appservice_handler = hs.get_application_service_handler()
    12|     async def on_GET(self, request):
    13|         await self.auth.get_user_by_req(request, allow_guest=True)
    14|         protocols = await self.appservice_handler.get_3pe_protocols()
    15|         return 200, protocols
    16| class ThirdPartyProtocolServlet(RestServlet):
    17|     PATTERNS = client_patterns("/thirdparty/protocol/(?P<protocol>[^/]+)$")
    18|     def __init__(self, hs):
    19|         super(ThirdPartyProtocolServlet, self).__init__()
    20|         self.auth = hs.get_auth()
    21|         self.appservice_handler = hs.get_application_service_handler()
    22|     async def on_GET(self, request, protocol):
    23|         await self.auth.get_user_by_req(request, allow_guest=True)
    24|         protocols = await self.appservice_handler.get_3pe_protocols(
    25|             only_protocol=protocol
    26|         )
    27|         if protocol in protocols:
    28|             return 200, protocols[protocol]
    29|         else:
    30|             return 404, {"error": "Unknown protocol"}
    31| class ThirdPartyUserServlet(RestServlet):
    32|     PATTERNS = client_patterns("/thirdparty/user(/(?P<protocol>[^/]+))?$")
    33|     def __init__(self, hs):
    34|         super(ThirdPartyUserServlet, self).__init__()
    35|         self.auth = hs.get_auth()
    36|         self.appservice_handler = hs.get_application_service_handler()
    37|     async def on_GET(self, request, protocol):
    38|         await self.auth.get_user_by_req(request, allow_guest=True)
    39|         fields = request.args
    40|         fields.pop(b"access_token", None)
    41|         results = await self.appservice_handler.query_3pe(
    42|             ThirdPartyEntityKind.USER, protocol, fields
    43|         )
    44|         return 200, results
    45| class ThirdPartyLocationServlet(RestServlet):
    46|     PATTERNS = client_patterns("/thirdparty/location(/(?P<protocol>[^/]+))?$")
    47|     def __init__(self, hs):
    48|         super(ThirdPartyLocationServlet, self).__init__()
    49|         self.auth = hs.get_auth()
    50|         self.appservice_handler = hs.get_application_service_handler()
    51|     async def on_GET(self, request, protocol):
    52|         await self.auth.get_user_by_req(request, allow_guest=True)
    53|         fields = request.args
    54|         fields.pop(b"access_token", None)
    55|         results = await self.appservice_handler.query_3pe(
    56|             ThirdPartyEntityKind.LOCATION, protocol, fields
    57|         )
    58|         return 200, results
    59| def register_servlets(hs, http_server):
    60|     ThirdPartyProtocolsServlet(hs).register(http_server)
    61|     ThirdPartyProtocolServlet(hs).register(http_server)
    62|     ThirdPartyUserServlet(hs).register(http_server)
    63|     ThirdPartyLocationServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/tokenrefresh.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-15 ---
     1| from synapse.api.errors import AuthError
     2| from synapse.http.servlet import RestServlet
     3| from ._base import client_patterns
     4| class TokenRefreshRestServlet(RestServlet):
     5|     """
     6|     Exchanges refresh tokens for a pair of an access token and a new refresh
     7|     token.
     8|     """
     9|     PATTERNS = client_patterns("/tokenrefresh")
    10|     def __init__(self, hs):
    11|         super(TokenRefreshRestServlet, self).__init__()
    12|     async def on_POST(self, request):
    13|         raise AuthError(403, "tokenrefresh is no longer supported.")
    14| def register_servlets(hs, http_server):
    15|     TokenRefreshRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/user_directory.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-33 ---
     1| import logging
     2| from synapse.api.errors import SynapseError
     3| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     4| from ._base import client_patterns
     5| logger = logging.getLogger(__name__)
     6| class UserDirectorySearchRestServlet(RestServlet):
     7|     PATTERNS = client_patterns("/user_directory/search$")
     8|     def __init__(self, hs):
     9|         """
    10|         Args:
    11|             hs (synapse.server.HomeServer): server
    12|         """
    13|         super(UserDirectorySearchRestServlet, self).__init__()
    14|         self.hs = hs
    15|         self.auth = hs.get_auth()
    16|         self.user_directory_handler = hs.get_user_directory_handler()
    17|     async def on_POST(self, request):
    18|         """Searches for users in directory
    19|         Returns:
    20|             dict of the form::
    21|                 {
    22|                     "limited": <bool>,  # whether there were more results or not
    23|                     "results": [  # Ordered by best match first
    24|                         {
    25|                             "user_id": <user_id>,
    26|                             "display_name": <display_name>,
    27|                             "avatar_url": <avatar_url>
    28|                         }
    29|                     ]
    30|                 }
    31|         """
    32|         requester = await self.auth.get_user_by_req(request, allow_guest=False)
    33|         user_id = requester.user.to_string()


# ====================================================================
# FILE: synapse/rest/client/versions.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-29 ---
     1| import logging
     2| import re
     3| from synapse.api.constants import RoomCreationPreset
     4| from synapse.http.servlet import RestServlet
     5| logger = logging.getLogger(__name__)
     6| class VersionsRestServlet(RestServlet):
     7|     PATTERNS = [re.compile("^/_matrix/client/versions$")]
     8|     def __init__(self, hs):
     9|         super(VersionsRestServlet, self).__init__()
    10|         self.config = hs.config
    11|         self.e2ee_forced_public = (
    12|             RoomCreationPreset.PUBLIC_CHAT
    13|             in self.config.encryption_enabled_by_default_for_room_presets
    14|         )
    15|         self.e2ee_forced_private = (
    16|             RoomCreationPreset.PRIVATE_CHAT
    17|             in self.config.encryption_enabled_by_default_for_room_presets
    18|         )
    19|         self.e2ee_forced_trusted_private = (
    20|             RoomCreationPreset.TRUSTED_PRIVATE_CHAT
    21|             in self.config.encryption_enabled_by_default_for_room_presets
    22|         )
    23|     def on_GET(self, request):
    24|         return (
    25|             200,
    26|             {
    27|                 "versions": [
    28|                     "r0.0.1",
    29|                     "r0.1.0",


# ====================================================================
# FILE: synapse/rest/key/v2/remote_key_resource.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| import logging
     2| from typing import Dict, Set
     3| from signedjson.sign import sign_json
     4| from synapse.api.errors import Codes, SynapseError
     5| from synapse.crypto.keyring import ServerKeyFetcher
     6| from synapse.http.server import DirectServeJsonResource, respond_with_json
     7| from synapse.http.servlet import parse_integer, parse_json_object_from_request
     8| from synapse.util import json_decoder
     9| logger = logging.getLogger(__name__)
    10| class RemoteKey(DirectServeJsonResource):
    11|     """HTTP resource for retrieving the TLS certificate and NACL signature
    12|     verification keys for a collection of servers. Checks that the reported
    13|     X.509 TLS certificate matches the one used in the HTTPS connection. Checks
    14|     that the NACL signature for the remote server is valid. Returns a dict of
    15|     JSON signed by both the remote server and by this server.
    16|     Supports individual GET APIs and a bulk query POST API.
    17|     Requsts:
    18|     GET /_matrix/key/v2/query/remote.server.example.com HTTP/1.1
    19|     GET /_matrix/key/v2/query/remote.server.example.com/a.key.id HTTP/1.1
    20|     POST /_matrix/v2/query HTTP/1.1
    21|     Content-Type: application/json
    22|     {
    23|         "server_keys": {
    24|             "remote.server.example.com": {
    25|                 "a.key.id": {
    26|                     "minimum_valid_until_ts": 1234567890123
    27|                 }
    28|             }
    29|         }
    30|     }
    31|     Response:
    32|     HTTP/1.1 200 OK
    33|     Content-Type: application/json
    34|     {
    35|         "server_keys": [
    36|             {
    37|                 "server_name": "remote.server.example.com"


# ====================================================================
# FILE: synapse/rest/media/v1/filepath.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 30-79 ---
    30|     default_thumbnail = _wrap_in_base_path(default_thumbnail_rel)
    31|     def local_media_filepath_rel(self, media_id):
    32|         return os.path.join("local_content", media_id[0:2], media_id[2:4], media_id[4:])
    33|     local_media_filepath = _wrap_in_base_path(local_media_filepath_rel)
    34|     def local_media_thumbnail_rel(self, media_id, width, height, content_type, method):
    35|         top_level_type, sub_type = content_type.split("/")
    36|         file_name = "%i-%i-%s-%s-%s" % (width, height, top_level_type, sub_type, method)
    37|         return os.path.join(
    38|             "local_thumbnails", media_id[0:2], media_id[2:4], media_id[4:], file_name
    39|         )
    40|     local_media_thumbnail = _wrap_in_base_path(local_media_thumbnail_rel)
    41|     def remote_media_filepath_rel(self, server_name, file_id):
    42|         return os.path.join(
    43|             "remote_content", server_name, file_id[0:2], file_id[2:4], file_id[4:]
    44|         )
    45|     remote_media_filepath = _wrap_in_base_path(remote_media_filepath_rel)
    46|     def remote_media_thumbnail_rel(
    47|         self, server_name, file_id, width, height, content_type, method
    48|     ):
    49|         top_level_type, sub_type = content_type.split("/")
    50|         file_name = "%i-%i-%s-%s" % (width, height, top_level_type, sub_type)
    51|         return os.path.join(
    52|             "remote_thumbnail",
    53|             server_name,
    54|             file_id[0:2],
    55|             file_id[2:4],
    56|             file_id[4:],
    57|             file_name,
    58|         )
    59|     remote_media_thumbnail = _wrap_in_base_path(remote_media_thumbnail_rel)
    60|     def remote_media_thumbnail_dir(self, server_name, file_id):
    61|         return os.path.join(
    62|             self.base_path,
    63|             "remote_thumbnail",
    64|             server_name,
    65|             file_id[0:2],
    66|             file_id[2:4],
    67|             file_id[4:],
    68|         )
    69|     def url_cache_filepath_rel(self, media_id):
    70|         if NEW_FORMAT_ID_RE.match(media_id):
    71|             return os.path.join("url_cache", media_id[:10], media_id[11:])
    72|         else:
    73|             return os.path.join("url_cache", media_id[0:2], media_id[2:4], media_id[4:])
    74|     url_cache_filepath = _wrap_in_base_path(url_cache_filepath_rel)
    75|     def url_cache_filepath_dirs_to_delete(self, media_id):
    76|         "The dirs to try and remove if we delete the media_id file"
    77|         if NEW_FORMAT_ID_RE.match(media_id):
    78|             return [os.path.join(self.base_path, "url_cache", media_id[:10])]
    79|         else:


# ====================================================================
# FILE: synapse/rest/media/v1/media_repository.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 17-57 ---
    17| from synapse.config._base import ConfigError
    18| from synapse.logging.context import defer_to_thread
    19| from synapse.metrics.background_process_metrics import run_as_background_process
    20| from synapse.util.async_helpers import Linearizer
    21| from synapse.util.retryutils import NotRetryingDestination
    22| from synapse.util.stringutils import random_string
    23| from ._base import (
    24|     FileInfo,
    25|     Responder,
    26|     get_filename_from_headers,
    27|     respond_404,
    28|     respond_with_responder,
    29| )
    30| from .config_resource import MediaConfigResource
    31| from .download_resource import DownloadResource
    32| from .filepath import MediaFilePaths
    33| from .media_storage import MediaStorage
    34| from .preview_url_resource import PreviewUrlResource
    35| from .storage_provider import StorageProviderWrapper
    36| from .thumbnail_resource import ThumbnailResource
    37| from .thumbnailer import Thumbnailer
    38| from .upload_resource import UploadResource
    39| logger = logging.getLogger(__name__)
    40| UPDATE_RECENTLY_ACCESSED_TS = 60 * 1000
    41| class MediaRepository:
    42|     def __init__(self, hs):
    43|         self.hs = hs
    44|         self.auth = hs.get_auth()
    45|         self.client = hs.get_http_client()
    46|         self.clock = hs.get_clock()
    47|         self.server_name = hs.hostname
    48|         self.store = hs.get_datastore()
    49|         self.max_upload_size = hs.config.max_upload_size
    50|         self.max_image_pixels = hs.config.max_image_pixels
    51|         self.primary_base_path = hs.config.media_store_path
    52|         self.filepaths = MediaFilePaths(self.primary_base_path)
    53|         self.dynamic_thumbnails = hs.config.dynamic_thumbnails
    54|         self.thumbnail_requirements = hs.config.thumbnail_requirements
    55|         self.remote_media_linearizer = Linearizer(name="media_remote")
    56|         self.recently_accessed_remotes = set()
    57|         self.recently_accessed_locals = set()

# --- HUNK 2: Lines 80-128 ---
    80|         remote_media = self.recently_accessed_remotes
    81|         self.recently_accessed_remotes = set()
    82|         local_media = self.recently_accessed_locals
    83|         self.recently_accessed_locals = set()
    84|         await self.store.update_cached_last_access_time(
    85|             local_media, remote_media, self.clock.time_msec()
    86|         )
    87|     def mark_recently_accessed(self, server_name, media_id):
    88|         """Mark the given media as recently accessed.
    89|         Args:
    90|             server_name (str|None): Origin server of media, or None if local
    91|             media_id (str): The media ID of the content
    92|         """
    93|         if server_name:
    94|             self.recently_accessed_remotes.add((server_name, media_id))
    95|         else:
    96|             self.recently_accessed_locals.add(media_id)
    97|     async def create_content(
    98|         self,
    99|         media_type: str,
   100|         upload_name: str,
   101|         content: IO,
   102|         content_length: int,
   103|         auth_user: str,
   104|     ) -> str:
   105|         """Store uploaded content for a local user and return the mxc URL
   106|         Args:
   107|             media_type: The content type of the file
   108|             upload_name: The name of the file
   109|             content: A file like object that is the content to store
   110|             content_length: The length of the content
   111|             auth_user: The user_id of the uploader
   112|         Returns:
   113|             The mxc url of the stored content
   114|         """
   115|         media_id = random_string(24)
   116|         file_info = FileInfo(server_name=None, file_id=media_id)
   117|         fname = await self.media_storage.store_file(content, file_info)
   118|         logger.info("Stored local media in file %r", fname)
   119|         await self.store.store_local_media(
   120|             media_id=media_id,
   121|             media_type=media_type,
   122|             time_now_ms=self.clock.time_msec(),
   123|             upload_name=upload_name,
   124|             media_length=content_length,
   125|             user_id=auth_user,
   126|         )
   127|         await self._generate_thumbnails(None, media_id, media_id, media_type)
   128|         return "mxc://%s/%s" % (self.server_name, media_id)

# --- HUNK 3: Lines 329-479 ---
   329|             logger.info(
   330|                 "Image too large to thumbnail %r x %r > %r",
   331|                 m_width,
   332|                 m_height,
   333|                 self.max_image_pixels,
   334|             )
   335|             return
   336|         if thumbnailer.transpose_method is not None:
   337|             m_width, m_height = thumbnailer.transpose()
   338|         if t_method == "crop":
   339|             t_byte_source = thumbnailer.crop(t_width, t_height, t_type)
   340|         elif t_method == "scale":
   341|             t_width, t_height = thumbnailer.aspect(t_width, t_height)
   342|             t_width = min(m_width, t_width)
   343|             t_height = min(m_height, t_height)
   344|             t_byte_source = thumbnailer.scale(t_width, t_height, t_type)
   345|         else:
   346|             t_byte_source = None
   347|         return t_byte_source
   348|     async def generate_local_exact_thumbnail(
   349|         self, media_id, t_width, t_height, t_method, t_type, url_cache
   350|     ):
   351|         input_path = await self.media_storage.ensure_media_is_in_local_cache(
   352|             FileInfo(None, media_id, url_cache=url_cache)
   353|         )
   354|         thumbnailer = Thumbnailer(input_path)
   355|         t_byte_source = await defer_to_thread(
   356|             self.hs.get_reactor(),
   357|             self._generate_thumbnail,
   358|             thumbnailer,
   359|             t_width,
   360|             t_height,
   361|             t_method,
   362|             t_type,
   363|         )
   364|         if t_byte_source:
   365|             try:
   366|                 file_info = FileInfo(
   367|                     server_name=None,
   368|                     file_id=media_id,
   369|                     url_cache=url_cache,
   370|                     thumbnail=True,
   371|                     thumbnail_width=t_width,
   372|                     thumbnail_height=t_height,
   373|                     thumbnail_method=t_method,
   374|                     thumbnail_type=t_type,
   375|                 )
   376|                 output_path = await self.media_storage.store_file(
   377|                     t_byte_source, file_info
   378|                 )
   379|             finally:
   380|                 t_byte_source.close()
   381|             logger.info("Stored thumbnail in file %r", output_path)
   382|             t_len = os.path.getsize(output_path)
   383|             await self.store.store_local_thumbnail(
   384|                 media_id, t_width, t_height, t_type, t_method, t_len
   385|             )
   386|             return output_path
   387|     async def generate_remote_exact_thumbnail(
   388|         self, server_name, file_id, media_id, t_width, t_height, t_method, t_type
   389|     ):
   390|         input_path = await self.media_storage.ensure_media_is_in_local_cache(
   391|             FileInfo(server_name, file_id, url_cache=False)
   392|         )
   393|         thumbnailer = Thumbnailer(input_path)
   394|         t_byte_source = await defer_to_thread(
   395|             self.hs.get_reactor(),
   396|             self._generate_thumbnail,
   397|             thumbnailer,
   398|             t_width,
   399|             t_height,
   400|             t_method,
   401|             t_type,
   402|         )
   403|         if t_byte_source:
   404|             try:
   405|                 file_info = FileInfo(
   406|                     server_name=server_name,
   407|                     file_id=file_id,
   408|                     thumbnail=True,
   409|                     thumbnail_width=t_width,
   410|                     thumbnail_height=t_height,
   411|                     thumbnail_method=t_method,
   412|                     thumbnail_type=t_type,
   413|                 )
   414|                 output_path = await self.media_storage.store_file(
   415|                     t_byte_source, file_info
   416|                 )
   417|             finally:
   418|                 t_byte_source.close()
   419|             logger.info("Stored thumbnail in file %r", output_path)
   420|             t_len = os.path.getsize(output_path)
   421|             await self.store.store_remote_media_thumbnail(
   422|                 server_name,
   423|                 media_id,
   424|                 file_id,
   425|                 t_width,
   426|                 t_height,
   427|                 t_type,
   428|                 t_method,
   429|                 t_len,
   430|             )
   431|             return output_path
   432|     async def _generate_thumbnails(
   433|         self,
   434|         server_name: Optional[str],
   435|         media_id: str,
   436|         file_id: str,
   437|         media_type: str,
   438|         url_cache: bool = False,
   439|     ) -> Optional[dict]:
   440|         """Generate and store thumbnails for an image.
   441|         Args:
   442|             server_name: The server name if remote media, else None if local
   443|             media_id: The media ID of the content. (This is the same as
   444|                 the file_id for local content)
   445|             file_id: Local file ID
   446|             media_type: The content type of the file
   447|             url_cache: If we are thumbnailing images downloaded for the URL cache,
   448|                 used exclusively by the url previewer
   449|         Returns:
   450|             Dict with "width" and "height" keys of original image or None if the
   451|             media cannot be thumbnailed.
   452|         """
   453|         requirements = self._get_thumbnail_requirements(media_type)
   454|         if not requirements:
   455|             return None
   456|         input_path = await self.media_storage.ensure_media_is_in_local_cache(
   457|             FileInfo(server_name, file_id, url_cache=url_cache)
   458|         )
   459|         thumbnailer = Thumbnailer(input_path)
   460|         m_width = thumbnailer.width
   461|         m_height = thumbnailer.height
   462|         if m_width * m_height >= self.max_image_pixels:
   463|             logger.info(
   464|                 "Image too large to thumbnail %r x %r > %r",
   465|                 m_width,
   466|                 m_height,
   467|                 self.max_image_pixels,
   468|             )
   469|             return None
   470|         if thumbnailer.transpose_method is not None:
   471|             m_width, m_height = await defer_to_thread(
   472|                 self.hs.get_reactor(), thumbnailer.transpose
   473|             )
   474|         thumbnails = {}  # type: Dict[Tuple[int, int, str], str]
   475|         for r_width, r_height, r_method, r_type in requirements:
   476|             if r_method == "crop":
   477|                 thumbnails.setdefault((r_width, r_height, r_type), r_method)
   478|             elif r_method == "scale":
   479|                 t_width, t_height = thumbnailer.aspect(r_width, r_height)


# ====================================================================
# FILE: synapse/rest/media/v1/media_storage.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 76-137 ---
    76|             finished_called[0] = True
    77|         try:
    78|             with open(fname, "wb") as f:
    79|                 yield f, fname, finish
    80|         except Exception:
    81|             try:
    82|                 os.remove(fname)
    83|             except Exception:
    84|                 pass
    85|             raise
    86|         if not finished_called:
    87|             raise Exception("Finished callback not called")
    88|     async def fetch_media(self, file_info: FileInfo) -> Optional[Responder]:
    89|         """Attempts to fetch media described by file_info from the local cache
    90|         and configured storage providers.
    91|         Args:
    92|             file_info
    93|         Returns:
    94|             Returns a Responder if the file was found, otherwise None.
    95|         """
    96|         path = self._file_info_to_path(file_info)
    97|         local_path = os.path.join(self.local_media_directory, path)
    98|         if os.path.exists(local_path):
    99|             return FileResponder(open(local_path, "rb"))
   100|         for provider in self.storage_providers:
   101|             res = await provider.fetch(path, file_info)  # type: Any
   102|             if res:
   103|                 logger.debug("Streaming %s from %s", path, provider)
   104|                 return res
   105|         return None
   106|     async def ensure_media_is_in_local_cache(self, file_info: FileInfo) -> str:
   107|         """Ensures that the given file is in the local cache. Attempts to
   108|         download it from storage providers if it isn't.
   109|         Args:
   110|             file_info
   111|         Returns:
   112|             Full path to local file
   113|         """
   114|         path = self._file_info_to_path(file_info)
   115|         local_path = os.path.join(self.local_media_directory, path)
   116|         if os.path.exists(local_path):
   117|             return local_path
   118|         dirname = os.path.dirname(local_path)
   119|         if not os.path.exists(dirname):
   120|             os.makedirs(dirname)
   121|         for provider in self.storage_providers:
   122|             res = await provider.fetch(path, file_info)  # type: Any
   123|             if res:
   124|                 with res:
   125|                     consumer = BackgroundFileConsumer(
   126|                         open(local_path, "wb"), self.hs.get_reactor()
   127|                     )
   128|                     await res.write_to_consumer(consumer)
   129|                     await consumer.wait()
   130|                 return local_path
   131|         raise Exception("file could not be found")
   132|     def _file_info_to_path(self, file_info: FileInfo) -> str:
   133|         """Converts file_info into a relative path.
   134|         The path is suitable for storing files under a directory, e.g. used to
   135|         store files on local FS under the base media repository directory.
   136|         """
   137|         if file_info.url_cache:


# ====================================================================
# FILE: synapse/rest/media/v1/preview_url_resource.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 43-83 ---
    43|         "http://twitter.com/*/status/*",
    44|         "http://*.twitter.com/*/status/*",
    45|         "http://twitter.com/*/moments/*",
    46|         "http://*.twitter.com/*/moments/*",
    47|     ],
    48| }
    49| _oembed_patterns = {}
    50| for endpoint, globs in _oembed_globs.items():
    51|     for glob in globs:
    52|         results = urlparse.urlparse(glob)
    53|         if results.scheme not in {"http", "https"}:
    54|             raise ValueError("Insecure oEmbed glob scheme: %s" % (results.scheme,))
    55|         pattern = urlparse.urlunparse(
    56|             [
    57|                 results.scheme,
    58|                 re.escape(results.netloc).replace("\\*", "[a-zA-Z0-9_-]+"),
    59|             ]
    60|             + [re.escape(part).replace("\\*", ".+") for part in results[2:]]
    61|         )
    62|         _oembed_patterns[re.compile(pattern)] = endpoint
    63| @attr.s
    64| class OEmbedResult:
    65|     html = attr.ib(type=Optional[str])
    66|     url = attr.ib(type=Optional[str])
    67|     title = attr.ib(type=Optional[str])
    68|     cache_age = attr.ib(type=int)
    69| class OEmbedError(Exception):
    70|     """An error occurred processing the oEmbed object."""
    71| class PreviewUrlResource(DirectServeJsonResource):
    72|     isLeaf = True
    73|     def __init__(self, hs, media_repo, media_storage):
    74|         super().__init__()
    75|         self.auth = hs.get_auth()
    76|         self.clock = hs.get_clock()
    77|         self.filepaths = media_repo.filepaths
    78|         self.max_spider_size = hs.config.max_spider_size
    79|         self.server_name = hs.hostname
    80|         self.store = hs.get_datastore()
    81|         self.client = SimpleHttpClient(
    82|             hs,
    83|             treq_args={"browser_like_redirects": True},

# --- HUNK 2: Lines 282-325 ---
   282|             cache_age = result.get("cache_age")
   283|             if cache_age:
   284|                 cache_age = int(cache_age)
   285|             oembed_result = OEmbedResult(None, None, result.get("title"), cache_age)
   286|             if oembed_type == "rich":
   287|                 oembed_result.html = result.get("html")
   288|                 return oembed_result
   289|             if oembed_type == "photo":
   290|                 oembed_result.url = result.get("url")
   291|                 return oembed_result
   292|             if "thumbnail_url" in result:
   293|                 oembed_result.url = result.get("thumbnail_url")
   294|                 return oembed_result
   295|             raise OEmbedError("Incompatible oEmbed information.")
   296|         except OEmbedError as e:
   297|             logger.warning("Error parsing oEmbed metadata from %s: %r", url, e)
   298|             raise
   299|         except Exception as e:
   300|             logger.warning("Error downloading oEmbed metadata from %s: %r", url, e)
   301|             raise OEmbedError() from e
   302|     async def _download_url(self, url, user):
   303|         file_id = datetime.date.today().isoformat() + "_" + random_string(16)
   304|         file_info = FileInfo(server_name=None, file_id=file_id, url_cache=True)
   305|         url_to_download = url
   306|         oembed_url = self._get_oembed_url(url)
   307|         if oembed_url:
   308|             try:
   309|                 oembed_result = await self._get_oembed_content(oembed_url, url)
   310|                 if oembed_result.url:
   311|                     url_to_download = oembed_result.url
   312|                 elif oembed_result.html:
   313|                     url_to_download = None
   314|             except OEmbedError:
   315|                 pass
   316|         if url_to_download:
   317|             with self.media_storage.store_into_file(file_info) as (f, fname, finish):
   318|                 try:
   319|                     logger.debug("Trying to get preview for url '%s'", url_to_download)
   320|                     length, headers, uri, code = await self.client.get_file(
   321|                         url_to_download,
   322|                         output_stream=f,
   323|                         max_size=self.max_spider_size,
   324|                         headers={"Accept-Language": self.url_preview_accept_language},
   325|                     )

# --- HUNK 3: Lines 329-371 ---
   329|                     raise SynapseError(
   330|                         502,
   331|                         "DNS resolution failure during URL preview generation",
   332|                         Codes.UNKNOWN,
   333|                     )
   334|                 except Exception as e:
   335|                     logger.warning("Error downloading %s: %r", url_to_download, e)
   336|                     raise SynapseError(
   337|                         500,
   338|                         "Failed to download content: %s"
   339|                         % (traceback.format_exception_only(sys.exc_info()[0], e),),
   340|                         Codes.UNKNOWN,
   341|                     )
   342|                 await finish()
   343|                 if b"Content-Type" in headers:
   344|                     media_type = headers[b"Content-Type"][0].decode("ascii")
   345|                 else:
   346|                     media_type = "application/octet-stream"
   347|                 download_name = get_filename_from_headers(headers)
   348|                 expires = ONE_HOUR
   349|                 etag = headers["ETag"][0] if "ETag" in headers else None
   350|         else:
   351|             html_bytes = oembed_result.html.encode("utf-8")  # type: ignore
   352|             with self.media_storage.store_into_file(file_info) as (f, fname, finish):
   353|                 f.write(html_bytes)
   354|                 await finish()
   355|             media_type = "text/html"
   356|             download_name = oembed_result.title
   357|             length = len(html_bytes)
   358|             expires = oembed_result.cache_age or ONE_HOUR
   359|             uri = oembed_url
   360|             code = 200
   361|             etag = None
   362|         try:
   363|             time_now_ms = self.clock.time_msec()
   364|             await self.store.store_local_media(
   365|                 media_id=file_id,
   366|                 media_type=media_type,
   367|                 time_now_ms=time_now_ms,
   368|                 upload_name=download_name,
   369|                 media_length=length,
   370|                 user_id=user,
   371|                 url_cache=url,


# ====================================================================
# FILE: synapse/rest/media/v1/thumbnail_resource.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-21 ---
     1| import logging
     2| from synapse.http.server import DirectServeJsonResource, set_cors_headers
     3| from synapse.http.servlet import parse_integer, parse_string
     4| from ._base import (
     5|     FileInfo,
     6|     parse_media_id,
     7|     respond_404,
     8|     respond_with_file,
     9|     respond_with_responder,
    10| )
    11| logger = logging.getLogger(__name__)
    12| class ThumbnailResource(DirectServeJsonResource):
    13|     isLeaf = True
    14|     def __init__(self, hs, media_repo, media_storage):
    15|         super().__init__()
    16|         self.store = hs.get_datastore()
    17|         self.media_repo = media_repo
    18|         self.media_storage = media_storage
    19|         self.dynamic_thumbnails = hs.config.dynamic_thumbnails
    20|         self.server_name = hs.hostname
    21|     async def _async_render_GET(self, request):

# --- HUNK 2: Lines 114-154 ---
   114|                 )
   115|                 t_type = file_info.thumbnail_type
   116|                 t_length = info["thumbnail_length"]
   117|                 responder = await self.media_storage.fetch_media(file_info)
   118|                 if responder:
   119|                     await respond_with_responder(request, responder, t_type, t_length)
   120|                     return
   121|         logger.debug("We don't have a thumbnail of that size. Generating")
   122|         file_path = await self.media_repo.generate_local_exact_thumbnail(
   123|             media_id,
   124|             desired_width,
   125|             desired_height,
   126|             desired_method,
   127|             desired_type,
   128|             url_cache=media_info["url_cache"],
   129|         )
   130|         if file_path:
   131|             await respond_with_file(request, desired_type, file_path)
   132|         else:
   133|             logger.warning("Failed to generate thumbnail")
   134|             respond_404(request)
   135|     async def _select_or_generate_remote_thumbnail(
   136|         self,
   137|         request,
   138|         server_name,
   139|         media_id,
   140|         desired_width,
   141|         desired_height,
   142|         desired_method,
   143|         desired_type,
   144|     ):
   145|         media_info = await self.media_repo.get_remote_media_info(server_name, media_id)
   146|         thumbnail_infos = await self.store.get_remote_media_thumbnails(
   147|             server_name, media_id
   148|         )
   149|         file_id = media_info["filesystem_id"]
   150|         for info in thumbnail_infos:
   151|             t_w = info["thumbnail_width"] == desired_width
   152|             t_h = info["thumbnail_height"] == desired_height
   153|             t_method = info["thumbnail_method"] == desired_method
   154|             t_type = info["thumbnail_type"] == desired_type

# --- HUNK 3: Lines 165-205 ---
   165|                 t_type = file_info.thumbnail_type
   166|                 t_length = info["thumbnail_length"]
   167|                 responder = await self.media_storage.fetch_media(file_info)
   168|                 if responder:
   169|                     await respond_with_responder(request, responder, t_type, t_length)
   170|                     return
   171|         logger.debug("We don't have a thumbnail of that size. Generating")
   172|         file_path = await self.media_repo.generate_remote_exact_thumbnail(
   173|             server_name,
   174|             file_id,
   175|             media_id,
   176|             desired_width,
   177|             desired_height,
   178|             desired_method,
   179|             desired_type,
   180|         )
   181|         if file_path:
   182|             await respond_with_file(request, desired_type, file_path)
   183|         else:
   184|             logger.warning("Failed to generate thumbnail")
   185|             respond_404(request)
   186|     async def _respond_remote_thumbnail(
   187|         self, request, server_name, media_id, width, height, method, m_type
   188|     ):
   189|         media_info = await self.media_repo.get_remote_media_info(server_name, media_id)
   190|         thumbnail_infos = await self.store.get_remote_media_thumbnails(
   191|             server_name, media_id
   192|         )
   193|         if thumbnail_infos:
   194|             thumbnail_info = self._select_thumbnail(
   195|                 width, height, method, m_type, thumbnail_infos
   196|             )
   197|             file_info = FileInfo(
   198|                 server_name=server_name,
   199|                 file_id=media_info["filesystem_id"],
   200|                 thumbnail=True,
   201|                 thumbnail_width=thumbnail_info["thumbnail_width"],
   202|                 thumbnail_height=thumbnail_info["thumbnail_height"],
   203|                 thumbnail_type=thumbnail_info["thumbnail_type"],
   204|                 thumbnail_method=thumbnail_info["thumbnail_method"],
   205|             )


# ====================================================================
# FILE: synapse/rest/media/v1/thumbnailer.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-92 ---
     1| import logging
     2| from io import BytesIO
     3| from PIL import Image as Image
     4| logger = logging.getLogger(__name__)
     5| EXIF_ORIENTATION_TAG = 0x0112
     6| EXIF_TRANSPOSE_MAPPINGS = {
     7|     2: Image.FLIP_LEFT_RIGHT,
     8|     3: Image.ROTATE_180,
     9|     4: Image.FLIP_TOP_BOTTOM,
    10|     5: Image.TRANSPOSE,
    11|     6: Image.ROTATE_270,
    12|     7: Image.TRANSVERSE,
    13|     8: Image.ROTATE_90,
    14| }
    15| class Thumbnailer:
    16|     FORMATS = {"image/jpeg": "JPEG", "image/png": "PNG"}
    17|     def __init__(self, input_path):
    18|         self.image = Image.open(input_path)
    19|         self.width, self.height = self.image.size
    20|         self.transpose_method = None
    21|         try:
    22|             image_exif = self.image._getexif()
    23|             if image_exif is not None:
    24|                 image_orientation = image_exif.get(EXIF_ORIENTATION_TAG)
    25|                 self.transpose_method = EXIF_TRANSPOSE_MAPPINGS.get(image_orientation)
    26|         except Exception as e:
    27|             logger.info("Error parsing image EXIF information: %s", e)
    28|     def transpose(self):
    29|         """Transpose the image using its EXIF Orientation tag
    30|         Returns:
    31|             Tuple[int, int]: (width, height) containing the new image size in pixels.
    32|         """
    33|         if self.transpose_method is not None:
    34|             self.image = self.image.transpose(self.transpose_method)
    35|             self.width, self.height = self.image.size
    36|             self.transpose_method = None
    37|             self.image.info["exif"] = None
    38|         return self.image.size
    39|     def aspect(self, max_width, max_height):
    40|         """Calculate the largest size that preserves aspect ratio which
    41|         fits within the given rectangle::
    42|             (w_in / h_in) = (w_out / h_out)
    43|             w_out = min(w_max, h_max * (w_in / h_in))
    44|             h_out = min(h_max, w_max * (h_in / w_in))
    45|         Args:
    46|             max_width: The largest possible width.
    47|             max_height: The larget possible height.
    48|         """
    49|         if max_width * self.height < max_height * self.width:
    50|             return max_width, (max_width * self.height) // self.width
    51|         else:
    52|             return (max_height * self.width) // self.height, max_height
    53|     def _resize(self, width, height):
    54|         if self.image.mode in ["1", "P"]:
    55|             self.image = self.image.convert("RGB")
    56|         return self.image.resize((width, height), Image.ANTIALIAS)
    57|     def scale(self, width, height, output_type):
    58|         """Rescales the image to the given dimensions.
    59|         Returns:
    60|             BytesIO: the bytes of the encoded image ready to be written to disk
    61|         """
    62|         scaled = self._resize(width, height)
    63|         return self._encode_image(scaled, output_type)
    64|     def crop(self, width, height, output_type):
    65|         """Rescales and crops the image to the given dimensions preserving
    66|         aspect::
    67|             (w_in / h_in) = (w_scaled / h_scaled)
    68|             w_scaled = max(w_out, h_out * (w_in / h_in))
    69|             h_scaled = max(h_out, w_out * (h_in / w_in))
    70|         Args:
    71|             max_width: The largest possible width.
    72|             max_height: The larget possible height.
    73|         Returns:
    74|             BytesIO: the bytes of the encoded image ready to be written to disk
    75|         """
    76|         if width * self.height > height * self.width:
    77|             scaled_height = (width * self.height) // self.width
    78|             scaled_image = self._resize(width, scaled_height)
    79|             crop_top = (scaled_height - height) // 2
    80|             crop_bottom = height + crop_top
    81|             cropped = scaled_image.crop((0, crop_top, width, crop_bottom))
    82|         else:
    83|             scaled_width = (height * self.width) // self.height
    84|             scaled_image = self._resize(scaled_width, height)
    85|             crop_left = (scaled_width - width) // 2
    86|             crop_right = width + crop_left
    87|             cropped = scaled_image.crop((crop_left, 0, crop_right, height))
    88|         return self._encode_image(cropped, output_type)
    89|     def _encode_image(self, output_image, output_type):
    90|         output_bytes_io = BytesIO()
    91|         fmt = self.FORMATS[output_type]
    92|         if fmt == "JPEG":


# ====================================================================
# FILE: synapse/rest/media/v1/upload_resource.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 19-48 ---
    19|         respond_with_json(request, 200, {}, send_cors=True)
    20|     async def _async_render_POST(self, request):
    21|         requester = await self.auth.get_user_by_req(request)
    22|         content_length = request.getHeader(b"Content-Length").decode("ascii")
    23|         if content_length is None:
    24|             raise SynapseError(msg="Request must specify a Content-Length", code=400)
    25|         if int(content_length) > self.max_upload_size:
    26|             raise SynapseError(
    27|                 msg="Upload request body is too large",
    28|                 code=413,
    29|                 errcode=Codes.TOO_LARGE,
    30|             )
    31|         upload_name = parse_string(request, b"filename", encoding=None)
    32|         if upload_name:
    33|             try:
    34|                 upload_name = upload_name.decode("utf8")
    35|             except UnicodeDecodeError:
    36|                 raise SynapseError(
    37|                     msg="Invalid UTF-8 filename parameter: %r" % (upload_name), code=400
    38|                 )
    39|         headers = request.requestHeaders
    40|         if headers.hasHeader(b"Content-Type"):
    41|             media_type = headers.getRawHeaders(b"Content-Type")[0].decode("ascii")
    42|         else:
    43|             raise SynapseError(msg="Upload request missing 'Content-Type'", code=400)
    44|         content_uri = await self.media_repo.create_content(
    45|             media_type, upload_name, request.content, content_length, requester.user
    46|         )
    47|         logger.info("Uploaded content with URI %r", content_uri)
    48|         respond_with_json(request, 200, {"content_uri": content_uri}, send_cors=True)


# ====================================================================
# FILE: synapse/rest/saml2/response_resource.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-21 ---
     1| from twisted.python import failure
     2| from synapse.api.errors import SynapseError
     3| from synapse.http.server import DirectServeHtmlResource, return_html_error
     4| class SAML2ResponseResource(DirectServeHtmlResource):
     5|     """A Twisted web resource which handles the SAML response"""
     6|     isLeaf = 1
     7|     def __init__(self, hs):
     8|         super().__init__()
     9|         self._saml_handler = hs.get_saml_handler()
    10|         self._error_html_template = hs.config.saml2.saml2_error_html_template
    11|     async def _async_render_GET(self, request):
    12|         f = failure.Failure(
    13|             SynapseError(400, "Unexpected GET request on /saml2/authn_response")
    14|         )
    15|         return_html_error(f, request, self._error_html_template)
    16|     async def _async_render_POST(self, request):
    17|         try:
    18|             await self._saml_handler.handle_saml_response(request)
    19|         except Exception:
    20|             f = failure.Failure()
    21|             return_html_error(f, request, self._error_html_template)


# ====================================================================
# FILE: synapse/state/__init__.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-52 ---
     1| import logging
     2| from collections import namedtuple
     3| from typing import (
     4|     Awaitable,
     5|     Dict,
     6|     Iterable,
     7|     List,
     8|     Optional,
     9|     Sequence,
    10|     Set,
    11|     Union,
    12|     cast,
    13|     overload,
    14| )
    15| import attr
    16| from frozendict import frozendict
    17| from prometheus_client import Histogram
    18| from typing_extensions import Literal
    19| from synapse.api.constants import EventTypes
    20| from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, StateResolutionVersions
    21| from synapse.events import EventBase
    22| from synapse.events.snapshot import EventContext
    23| from synapse.logging.utils import log_function
    24| from synapse.state import v1, v2
    25| from synapse.storage.databases.main.events_worker import EventRedactBehaviour
    26| from synapse.storage.roommember import ProfileInfo
    27| from synapse.types import Collection, MutableStateMap, StateMap
    28| from synapse.util import Clock
    29| from synapse.util.async_helpers import Linearizer
    30| from synapse.util.caches.expiringcache import ExpiringCache
    31| from synapse.util.metrics import Measure, measure_func
    32| logger = logging.getLogger(__name__)
    33| state_groups_histogram = Histogram(
    34|     "synapse_state_number_state_groups_in_resolution",
    35|     "Number of state groups used when performing a state resolution",
    36|     buckets=(1, 2, 3, 5, 7, 10, 15, 20, 50, 100, 200, 500, "+Inf"),
    37| )
    38| KeyStateTuple = namedtuple("KeyStateTuple", ("context", "type", "state_key"))
    39| EVICTION_TIMEOUT_SECONDS = 60 * 60
    40| _NEXT_STATE_ID = 1
    41| POWER_KEY = (EventTypes.PowerLevels, "")
    42| def _gen_state_id():
    43|     global _NEXT_STATE_ID
    44|     s = "X%d" % (_NEXT_STATE_ID,)
    45|     _NEXT_STATE_ID += 1
    46|     return s
    47| class _StateCacheEntry:
    48|     __slots__ = ["state", "state_group", "state_id", "prev_group", "delta_ids"]
    49|     def __init__(
    50|         self,
    51|         state: StateMap[str],
    52|         state_group: Optional[int],

# --- HUNK 2: Lines 297-496 ---
   297|             room_id,
   298|             room_version,
   299|             state_groups_ids,
   300|             None,
   301|             state_res_store=StateResolutionStore(self.store),
   302|         )
   303|         return result
   304|     async def resolve_events(
   305|         self,
   306|         room_version: str,
   307|         state_sets: Collection[Iterable[EventBase]],
   308|         event: EventBase,
   309|     ) -> StateMap[EventBase]:
   310|         logger.info(
   311|             "Resolving state for %s with %d groups", event.room_id, len(state_sets)
   312|         )
   313|         state_set_ids = [
   314|             {(ev.type, ev.state_key): ev.event_id for ev in st} for st in state_sets
   315|         ]
   316|         state_map = {ev.event_id: ev for st in state_sets for ev in st}
   317|         with Measure(self.clock, "state._resolve_events"):
   318|             new_state = await resolve_events_with_store(
   319|                 self.clock,
   320|                 event.room_id,
   321|                 room_version,
   322|                 state_set_ids,
   323|                 event_map=state_map,
   324|                 state_res_store=StateResolutionStore(self.store),
   325|             )
   326|         return {key: state_map[ev_id] for key, ev_id in new_state.items()}
   327| class StateResolutionHandler:
   328|     """Responsible for doing state conflict resolution.
   329|     Note that the storage layer depends on this handler, so all functions must
   330|     be storage-independent.
   331|     """
   332|     def __init__(self, hs):
   333|         self.clock = hs.get_clock()
   334|         self._state_cache = None
   335|         self.resolve_linearizer = Linearizer(name="state_resolve_lock")
   336|         self._state_cache = ExpiringCache(
   337|             cache_name="state_cache",
   338|             clock=self.clock,
   339|             max_len=100000,
   340|             expiry_ms=EVICTION_TIMEOUT_SECONDS * 1000,
   341|             iterable=True,
   342|             reset_expiry_on_get=True,
   343|         )
   344|     @log_function
   345|     async def resolve_state_groups(
   346|         self,
   347|         room_id: str,
   348|         room_version: str,
   349|         state_groups_ids: Dict[int, StateMap[str]],
   350|         event_map: Optional[Dict[str, EventBase]],
   351|         state_res_store: "StateResolutionStore",
   352|     ):
   353|         """Resolves conflicts between a set of state groups
   354|         Always generates a new state group (unless we hit the cache), so should
   355|         not be called for a single state group
   356|         Args:
   357|             room_id: room we are resolving for (used for logging and sanity checks)
   358|             room_version: version of the room
   359|             state_groups_ids:
   360|                 A map from state group id to the state in that state group
   361|                 (where 'state' is a map from state key to event id)
   362|             event_map:
   363|                 a dict from event_id to event, for any events that we happen to
   364|                 have in flight (eg, those currently being persisted). This will be
   365|                 used as a starting point fof finding the state we need; any missing
   366|                 events will be requested via state_res_store.
   367|                 If None, all events will be fetched via state_res_store.
   368|             state_res_store
   369|         Returns:
   370|             The resolved state
   371|         """
   372|         logger.debug("resolve_state_groups state_groups %s", state_groups_ids.keys())
   373|         group_names = frozenset(state_groups_ids.keys())
   374|         with (await self.resolve_linearizer.queue(group_names)):
   375|             if self._state_cache is not None:
   376|                 cache = self._state_cache.get(group_names, None)
   377|                 if cache:
   378|                     return cache
   379|             logger.info(
   380|                 "Resolving state for %s with %d groups", room_id, len(state_groups_ids)
   381|             )
   382|             state_groups_histogram.observe(len(state_groups_ids))
   383|             new_state = {}  # type: MutableStateMap[str]
   384|             conflicted_state = False
   385|             for st in state_groups_ids.values():
   386|                 for key, e_id in st.items():
   387|                     if key in new_state:
   388|                         conflicted_state = True
   389|                         break
   390|                     new_state[key] = e_id
   391|                 if conflicted_state:
   392|                     break
   393|             if conflicted_state:
   394|                 logger.info("Resolving conflicted state for %r", room_id)
   395|                 with Measure(self.clock, "state._resolve_events"):
   396|                     new_state = cast(
   397|                         MutableStateMap,
   398|                         await resolve_events_with_store(
   399|                             self.clock,
   400|                             room_id,
   401|                             room_version,
   402|                             list(state_groups_ids.values()),
   403|                             event_map=event_map,
   404|                             state_res_store=state_res_store,
   405|                         ),
   406|                     )
   407|             with Measure(self.clock, "state.create_group_ids"):
   408|                 cache = _make_state_cache_entry(new_state, state_groups_ids)
   409|             if self._state_cache is not None:
   410|                 self._state_cache[group_names] = cache
   411|             return cache
   412| def _make_state_cache_entry(
   413|     new_state: StateMap[str], state_groups_ids: Dict[int, StateMap[str]]
   414| ) -> _StateCacheEntry:
   415|     """Given a resolved state, and a set of input state groups, pick one to base
   416|     a new state group on (if any), and return an appropriately-constructed
   417|     _StateCacheEntry.
   418|     Args:
   419|         new_state: resolved state map (mapping from (type, state_key) to event_id)
   420|         state_groups_ids:
   421|             map from state group id to the state in that state group (where
   422|             'state' is a map from state key to event id)
   423|     Returns:
   424|         The cache entry.
   425|     """
   426|     new_state_event_ids = set(new_state.values())
   427|     for sg, state in state_groups_ids.items():
   428|         if len(new_state_event_ids) != len(state):
   429|             continue
   430|         old_state_event_ids = set(state.values())
   431|         if new_state_event_ids == old_state_event_ids:
   432|             return _StateCacheEntry(state=new_state, state_group=sg)
   433|     prev_group = None
   434|     delta_ids = None
   435|     for old_group, old_state in state_groups_ids.items():
   436|         n_delta_ids = {k: v for k, v in new_state.items() if old_state.get(k) != v}
   437|         if not delta_ids or len(n_delta_ids) < len(delta_ids):
   438|             prev_group = old_group
   439|             delta_ids = n_delta_ids
   440|     return _StateCacheEntry(
   441|         state=new_state, state_group=None, prev_group=prev_group, delta_ids=delta_ids
   442|     )
   443| def resolve_events_with_store(
   444|     clock: Clock,
   445|     room_id: str,
   446|     room_version: str,
   447|     state_sets: Sequence[StateMap[str]],
   448|     event_map: Optional[Dict[str, EventBase]],
   449|     state_res_store: "StateResolutionStore",
   450| ) -> Awaitable[StateMap[str]]:
   451|     """
   452|     Args:
   453|         room_id: the room we are working in
   454|         room_version: Version of the room
   455|         state_sets: List of dicts of (type, state_key) -> event_id,
   456|             which are the different state groups to resolve.
   457|         event_map:
   458|             a dict from event_id to event, for any events that we happen to
   459|             have in flight (eg, those currently being persisted). This will be
   460|             used as a starting point fof finding the state we need; any missing
   461|             events will be requested via state_map_factory.
   462|             If None, all events will be fetched via state_res_store.
   463|         state_res_store: a place to fetch events from
   464|     Returns:
   465|         a map from (type, state_key) to event_id.
   466|     """
   467|     v = KNOWN_ROOM_VERSIONS[room_version]
   468|     if v.state_res == StateResolutionVersions.V1:
   469|         return v1.resolve_events_with_store(
   470|             room_id, state_sets, event_map, state_res_store.get_events
   471|         )
   472|     else:
   473|         return v2.resolve_events_with_store(
   474|             clock, room_id, room_version, state_sets, event_map, state_res_store
   475|         )
   476| @attr.s
   477| class StateResolutionStore:
   478|     """Interface that allows state resolution algorithms to access the database
   479|     in well defined way.
   480|     Args:
   481|         store (DataStore)
   482|     """
   483|     store = attr.ib()
   484|     def get_events(
   485|         self, event_ids: Iterable[str], allow_rejected: bool = False
   486|     ) -> Awaitable[Dict[str, EventBase]]:
   487|         """Get events from the database
   488|         Args:
   489|             event_ids: The event_ids of the events to fetch
   490|             allow_rejected: If True return rejected events.
   491|         Returns:
   492|             An awaitable which resolves to a dict from event_id to event.
   493|         """
   494|         return self.store.get_events(
   495|             event_ids,
   496|             redact_behaviour=EventRedactBehaviour.AS_IS,


# ====================================================================
# FILE: synapse/storage/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 4-26 ---
     4| databases). The `DatabasePool` class represents connections to a single physical
     5| database. The `databases` are classes that talk directly to a `DatabasePool`
     6| instance and have associated schemas, background updates, etc. On top of those
     7| there are classes that provide high level interfaces that combine calls to
     8| multiple `databases`.
     9| There are also schemas that get applied to every database, regardless of the
    10| data stores associated with them (e.g. the schema version tables), which are
    11| stored in `synapse.storage.schema`.
    12| """
    13| from synapse.storage.databases import Databases
    14| from synapse.storage.databases.main import DataStore
    15| from synapse.storage.persist_events import EventsPersistenceStorage
    16| from synapse.storage.purge_events import PurgeEventsStorage
    17| from synapse.storage.state import StateGroupStorage
    18| __all__ = ["DataStores", "DataStore"]
    19| class Storage:
    20|     """The high level interfaces for talking to various storage layers.
    21|     """
    22|     def __init__(self, hs, stores: Databases):
    23|         self.main = stores.main
    24|         self.persistence = EventsPersistenceStorage(hs, stores)
    25|         self.purge_events = PurgeEventsStorage(hs, stores)
    26|         self.state = StateGroupStorage(hs, stores)


# ====================================================================
# FILE: synapse/storage/database.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 262-301 ---
   262|             time_now = monotonic_time()
   263|             time_then = self._previous_loop_ts
   264|             self._previous_loop_ts = time_now
   265|             duration = time_now - time_then
   266|             ratio = (curr - prev) / duration
   267|             top_three_counters = self._txn_perf_counters.interval(duration, limit=3)
   268|             perf_logger.debug(
   269|                 "Total database time: %.3f%% {%s}", ratio * 100, top_three_counters
   270|             )
   271|         self._clock.looping_call(loop, 10000)
   272|     def new_transaction(
   273|         self,
   274|         conn: Connection,
   275|         desc: str,
   276|         after_callbacks: List[_CallbackListEntry],
   277|         exception_callbacks: List[_CallbackListEntry],
   278|         func: "Callable[..., R]",
   279|         *args: Any,
   280|         **kwargs: Any
   281|     ) -> R:
   282|         start = monotonic_time()
   283|         txn_id = self._TXN_ID
   284|         self._TXN_ID = (self._TXN_ID + 1) % (MAX_TXN_ID)
   285|         name = "%s-%x" % (desc, txn_id)
   286|         transaction_logger.debug("[TXN START] {%s}", name)
   287|         try:
   288|             i = 0
   289|             N = 5
   290|             while True:
   291|                 cursor = LoggingTransaction(
   292|                     conn.cursor(),
   293|                     name,
   294|                     self.engine,
   295|                     after_callbacks,
   296|                     exception_callbacks,
   297|                 )
   298|                 try:
   299|                     r = func(cursor, *args, **kwargs)
   300|                     conn.commit()
   301|                     return r

# --- HUNK 2: Lines 323-425 ---
   323|                             except self.engine.module.Error as e1:
   324|                                 transaction_logger.warning(
   325|                                     "[TXN EROLL] {%s} %s", name, e1,
   326|                                 )
   327|                             continue
   328|                     raise
   329|                 finally:
   330|                     cursor.close()
   331|         except Exception as e:
   332|             transaction_logger.debug("[TXN FAIL] {%s} %s", name, e)
   333|             raise
   334|         finally:
   335|             end = monotonic_time()
   336|             duration = end - start
   337|             current_context().add_database_transaction(duration)
   338|             transaction_logger.debug("[TXN END] {%s} %f sec", name, duration)
   339|             self._current_txn_total_time += duration
   340|             self._txn_perf_counters.update(desc, duration)
   341|             sql_txn_timer.labels(desc).observe(duration)
   342|     async def runInteraction(
   343|         self, desc: str, func: "Callable[..., R]", *args: Any, **kwargs: Any
   344|     ) -> R:
   345|         """Starts a transaction on the database and runs a given function
   346|         Arguments:
   347|             desc: description of the transaction, for logging and metrics
   348|             func: callback function, which will be called with a
   349|                 database transaction (twisted.enterprise.adbapi.Transaction) as
   350|                 its first argument, followed by `args` and `kwargs`.
   351|             args: positional args to pass to `func`
   352|             kwargs: named args to pass to `func`
   353|         Returns:
   354|             The result of func
   355|         """
   356|         after_callbacks = []  # type: List[_CallbackListEntry]
   357|         exception_callbacks = []  # type: List[_CallbackListEntry]
   358|         if not current_context():
   359|             logger.warning("Starting db txn '%s' from sentinel context", desc)
   360|         try:
   361|             result = await self.runWithConnection(
   362|                 self.new_transaction,
   363|                 desc,
   364|                 after_callbacks,
   365|                 exception_callbacks,
   366|                 func,
   367|                 *args,
   368|                 **kwargs
   369|             )
   370|             for after_callback, after_args, after_kwargs in after_callbacks:
   371|                 after_callback(*after_args, **after_kwargs)
   372|         except:  # noqa: E722, as we reraise the exception this is fine.
   373|             for after_callback, after_args, after_kwargs in exception_callbacks:
   374|                 after_callback(*after_args, **after_kwargs)
   375|             raise
   376|         return cast(R, result)
   377|     async def runWithConnection(
   378|         self, func: "Callable[..., R]", *args: Any, **kwargs: Any
   379|     ) -> R:
   380|         """Wraps the .runWithConnection() method on the underlying db_pool.
   381|         Arguments:
   382|             func: callback function, which will be called with a
   383|                 database connection (twisted.enterprise.adbapi.Connection) as
   384|                 its first argument, followed by `args` and `kwargs`.
   385|             args: positional args to pass to `func`
   386|             kwargs: named args to pass to `func`
   387|         Returns:
   388|             The result of func
   389|         """
   390|         parent_context = current_context()  # type: Optional[LoggingContextOrSentinel]
   391|         if not parent_context:
   392|             logger.warning(
   393|                 "Starting db connection from sentinel context: metrics will be lost"
   394|             )
   395|             parent_context = None
   396|         start_time = monotonic_time()
   397|         def inner_func(conn, *args, **kwargs):
   398|             with LoggingContext("runWithConnection", parent_context) as context:
   399|                 sched_duration_sec = monotonic_time() - start_time
   400|                 sql_scheduling_timer.observe(sched_duration_sec)
   401|                 context.add_database_scheduled(sched_duration_sec)
   402|                 if self.engine.is_connection_closed(conn):
   403|                     logger.debug("Reconnecting closed database connection")
   404|                     conn.reconnect()
   405|                 return func(conn, *args, **kwargs)
   406|         return await make_deferred_yieldable(
   407|             self._db_pool.runWithConnection(inner_func, *args, **kwargs)
   408|         )
   409|     @staticmethod
   410|     def cursor_to_dict(cursor: Cursor) -> List[Dict[str, Any]]:
   411|         """Converts a SQL cursor into an list of dicts.
   412|         Args:
   413|             cursor: The DBAPI cursor which has executed a query.
   414|         Returns:
   415|             A list of dicts where the key is the column header.
   416|         """
   417|         col_headers = [intern(str(column[0])) for column in cursor.description]
   418|         results = [dict(zip(col_headers, row)) for row in cursor]
   419|         return results
   420|     @overload
   421|     async def execute(
   422|         self, desc: str, decoder: Literal[None], query: str, *args: Any
   423|     ) -> List[Tuple[Any, ...]]:
   424|         ...
   425|     @overload

# --- HUNK 3: Lines 684-751 ---
   684|         if not values:
   685|             latter = "NOTHING"
   686|         else:
   687|             allvalues.update(values)
   688|             latter = "UPDATE SET " + ", ".join(k + "=EXCLUDED." + k for k in values)
   689|         sql = ("INSERT INTO %s (%s) VALUES (%s) ON CONFLICT (%s) DO %s") % (
   690|             table,
   691|             ", ".join(k for k in allvalues),
   692|             ", ".join("?" for _ in allvalues),
   693|             ", ".join(k for k in keyvalues),
   694|             latter,
   695|         )
   696|         txn.execute(sql, list(allvalues.values()))
   697|     def simple_upsert_many_txn(
   698|         self,
   699|         txn: LoggingTransaction,
   700|         table: str,
   701|         key_names: Collection[str],
   702|         key_values: Collection[Iterable[Any]],
   703|         value_names: Collection[str],
   704|         value_values: Iterable[Iterable[str]],
   705|     ) -> None:
   706|         """
   707|         Upsert, many times.
   708|         Args:
   709|             table: The table to upsert into
   710|             key_names: The key column names.
   711|             key_values: A list of each row's key column values.
   712|             value_names: The value column names
   713|             value_values: A list of each row's value column values.
   714|                 Ignored if value_names is empty.
   715|         """
   716|         if self.engine.can_native_upsert and table not in self._unsafe_to_upsert_tables:
   717|             return self.simple_upsert_many_txn_native_upsert(
   718|                 txn, table, key_names, key_values, value_names, value_values
   719|             )
   720|         else:
   721|             return self.simple_upsert_many_txn_emulated(
   722|                 txn, table, key_names, key_values, value_names, value_values
   723|             )
   724|     def simple_upsert_many_txn_emulated(
   725|         self,
   726|         txn: LoggingTransaction,
   727|         table: str,
   728|         key_names: Iterable[str],
   729|         key_values: Collection[Iterable[Any]],
   730|         value_names: Collection[str],
   731|         value_values: Iterable[Iterable[str]],
   732|     ) -> None:
   733|         """
   734|         Upsert, many times, but without native UPSERT support or batching.
   735|         Args:
   736|             table: The table to upsert into
   737|             key_names: The key column names.
   738|             key_values: A list of each row's key column values.
   739|             value_names: The value column names
   740|             value_values: A list of each row's value column values.
   741|                 Ignored if value_names is empty.
   742|         """
   743|         if not value_names:
   744|             value_values = [() for x in range(len(key_values))]
   745|         for keyv, valv in zip(key_values, value_values):
   746|             _keys = {x: y for x, y in zip(key_names, keyv)}
   747|             _vals = {x: y for x, y in zip(value_names, valv)}
   748|             self.simple_upsert_txn_emulated(txn, table, _keys, _vals)
   749|     def simple_upsert_many_txn_native_upsert(
   750|         self,
   751|         txn: LoggingTransaction,


# ====================================================================
# FILE: synapse/storage/databases/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 21-60 ---
    21|             engine = create_engine(database_config.config)
    22|             with make_conn(database_config, engine) as db_conn:
    23|                 logger.info("[database config %r]: Checking database server", db_name)
    24|                 engine.check_database(db_conn)
    25|                 logger.info(
    26|                     "[database config %r]: Preparing for databases %r",
    27|                     db_name,
    28|                     database_config.databases,
    29|                 )
    30|                 prepare_database(
    31|                     db_conn, engine, hs.config, databases=database_config.databases,
    32|                 )
    33|                 database = DatabasePool(hs, database_config, engine)
    34|                 if "main" in database_config.databases:
    35|                     logger.info(
    36|                         "[database config %r]: Starting 'main' database", db_name
    37|                     )
    38|                     if main:
    39|                         raise Exception("'main' data store already configured")
    40|                     main = main_store_class(database, db_conn, hs)
    41|                     if hs.config.worker.writers.events == hs.get_instance_name():
    42|                         persist_events = PersistEventsStore(hs, database, main)
    43|                 if "state" in database_config.databases:
    44|                     logger.info(
    45|                         "[database config %r]: Starting 'state' database", db_name
    46|                     )
    47|                     if state:
    48|                         raise Exception("'state' data store already configured")
    49|                     state = StateGroupDataStore(database, db_conn, hs)
    50|                 db_conn.commit()
    51|                 self.databases.append(database)
    52|                 logger.info("[database config %r]: prepared", db_name)
    53|             db_conn.close()
    54|         if not main:
    55|             raise Exception("No 'main' database configured")
    56|         if not state:
    57|             raise Exception("No 'state' database configured")
    58|         self.main = main
    59|         self.state = state
    60|         self.persist_events = persist_events


# ====================================================================
# FILE: synapse/storage/databases/main/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 121-169 ---
   121|                 ("device_lists_outbound_pokes", "stream_id"),
   122|             ],
   123|         )
   124|         self._cross_signing_id_gen = StreamIdGenerator(
   125|             db_conn, "e2e_cross_signing_keys", "stream_id"
   126|         )
   127|         self._access_tokens_id_gen = IdGenerator(db_conn, "access_tokens", "id")
   128|         self._event_reports_id_gen = IdGenerator(db_conn, "event_reports", "id")
   129|         self._push_rule_id_gen = IdGenerator(db_conn, "push_rules", "id")
   130|         self._push_rules_enable_id_gen = IdGenerator(db_conn, "push_rules_enable", "id")
   131|         self._pushers_id_gen = StreamIdGenerator(
   132|             db_conn, "pushers", "id", extra_tables=[("deleted_pushers", "stream_id")]
   133|         )
   134|         self._group_updates_id_gen = StreamIdGenerator(
   135|             db_conn, "local_group_updates", "stream_id"
   136|         )
   137|         if isinstance(self.database_engine, PostgresEngine):
   138|             self._cache_id_gen = MultiWriterIdGenerator(
   139|                 db_conn,
   140|                 database,
   141|                 instance_name="master",
   142|                 table="cache_invalidation_stream_by_instance",
   143|                 instance_column="instance_name",
   144|                 id_column="stream_id",
   145|                 sequence_name="cache_invalidation_stream_seq",
   146|             )
   147|         else:
   148|             self._cache_id_gen = None
   149|         super(DataStore, self).__init__(database, db_conn, hs)
   150|         self._presence_on_startup = self._get_active_presence(db_conn)
   151|         presence_cache_prefill, min_presence_val = self.db_pool.get_cache_dict(
   152|             db_conn,
   153|             "presence_stream",
   154|             entity_column="user_id",
   155|             stream_column="stream_id",
   156|             max_value=self._presence_id_gen.get_current_token(),
   157|         )
   158|         self.presence_stream_cache = StreamChangeCache(
   159|             "PresenceStreamChangeCache",
   160|             min_presence_val,
   161|             prefilled_cache=presence_cache_prefill,
   162|         )
   163|         max_device_inbox_id = self._device_inbox_id_gen.get_current_token()
   164|         device_inbox_prefill, min_device_inbox_id = self.db_pool.get_cache_dict(
   165|             db_conn,
   166|             "device_inbox",
   167|             entity_column="user_id",
   168|             stream_column="stream_id",
   169|             max_value=max_device_inbox_id,


# ====================================================================
# FILE: synapse/storage/databases/main/account_data.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-42 ---
     1| import abc
     2| import logging
     3| from typing import Dict, List, Optional, Tuple
     4| from synapse.storage._base import SQLBaseStore, db_to_json
     5| from synapse.storage.database import DatabasePool
     6| from synapse.storage.util.id_generators import StreamIdGenerator
     7| from synapse.types import JsonDict
     8| from synapse.util import json_encoder
     9| from synapse.util.caches.descriptors import _CacheContext, cached
    10| from synapse.util.caches.stream_change_cache import StreamChangeCache
    11| logger = logging.getLogger(__name__)
    12| class AccountDataWorkerStore(SQLBaseStore):
    13|     """This is an abstract base class where subclasses must implement
    14|     `get_max_account_data_stream_id` which can be called in the initializer.
    15|     """
    16|     __metaclass__ = abc.ABCMeta
    17|     def __init__(self, database: DatabasePool, db_conn, hs):
    18|         account_max = self.get_max_account_data_stream_id()
    19|         self._account_data_stream_cache = StreamChangeCache(
    20|             "AccountDataAndTagsChangeCache", account_max
    21|         )
    22|         super(AccountDataWorkerStore, self).__init__(database, db_conn, hs)
    23|     @abc.abstractmethod
    24|     def get_max_account_data_stream_id(self):
    25|         """Get the current max stream ID for account data stream
    26|         Returns:
    27|             int
    28|         """
    29|         raise NotImplementedError()
    30|     @cached()
    31|     async def get_account_data_for_user(
    32|         self, user_id: str
    33|     ) -> Tuple[Dict[str, JsonDict], Dict[str, Dict[str, JsonDict]]]:
    34|         """Get all the client account_data for a user.
    35|         Args:
    36|             user_id: The user to get the account_data for.
    37|         Returns:
    38|             A 2-tuple of a dict of global account_data and a dict mapping from
    39|             room_id string to per room account_data dicts.
    40|         """
    41|         def get_account_data_for_user_txn(txn):
    42|             rows = self.db_pool.simple_select_list_txn(

# --- HUNK 2: Lines 225-317 ---
   225|     ) -> bool:
   226|         ignored_account_data = await self.get_global_account_data_by_type_for_user(
   227|             "m.ignored_user_list",
   228|             ignorer_user_id,
   229|             on_invalidate=cache_context.invalidate,
   230|         )
   231|         if not ignored_account_data:
   232|             return False
   233|         return ignored_user_id in ignored_account_data.get("ignored_users", {})
   234| class AccountDataStore(AccountDataWorkerStore):
   235|     def __init__(self, database: DatabasePool, db_conn, hs):
   236|         self._account_data_id_gen = StreamIdGenerator(
   237|             db_conn,
   238|             "account_data_max_stream_id",
   239|             "stream_id",
   240|             extra_tables=[
   241|                 ("room_account_data", "stream_id"),
   242|                 ("room_tags_revisions", "stream_id"),
   243|             ],
   244|         )
   245|         super(AccountDataStore, self).__init__(database, db_conn, hs)
   246|     def get_max_account_data_stream_id(self) -> int:
   247|         """Get the current max stream id for the private user data stream
   248|         Returns:
   249|             The maximum stream ID.
   250|         """
   251|         return self._account_data_id_gen.get_current_token()
   252|     async def add_account_data_to_room(
   253|         self, user_id: str, room_id: str, account_data_type: str, content: JsonDict
   254|     ) -> int:
   255|         """Add some account_data to a room for a user.
   256|         Args:
   257|             user_id: The user to add a tag for.
   258|             room_id: The room to add a tag for.
   259|             account_data_type: The type of account_data to add.
   260|             content: A json object to associate with the tag.
   261|         Returns:
   262|             The maximum stream ID.
   263|         """
   264|         content_json = json_encoder.encode(content)
   265|         with await self._account_data_id_gen.get_next() as next_id:
   266|             await self.db_pool.simple_upsert(
   267|                 desc="add_room_account_data",
   268|                 table="room_account_data",
   269|                 keyvalues={
   270|                     "user_id": user_id,
   271|                     "room_id": room_id,
   272|                     "account_data_type": account_data_type,
   273|                 },
   274|                 values={"stream_id": next_id, "content": content_json},
   275|                 lock=False,
   276|             )
   277|             await self._update_max_stream_id(next_id)
   278|             self._account_data_stream_cache.entity_has_changed(user_id, next_id)
   279|             self.get_account_data_for_user.invalidate((user_id,))
   280|             self.get_account_data_for_room.invalidate((user_id, room_id))
   281|             self.get_account_data_for_room_and_type.prefill(
   282|                 (user_id, room_id, account_data_type), content
   283|             )
   284|         return self._account_data_id_gen.get_current_token()
   285|     async def add_account_data_for_user(
   286|         self, user_id: str, account_data_type: str, content: JsonDict
   287|     ) -> int:
   288|         """Add some account_data to a room for a user.
   289|         Args:
   290|             user_id: The user to add a tag for.
   291|             account_data_type: The type of account_data to add.
   292|             content: A json object to associate with the tag.
   293|         Returns:
   294|             The maximum stream ID.
   295|         """
   296|         content_json = json_encoder.encode(content)
   297|         with await self._account_data_id_gen.get_next() as next_id:
   298|             await self.db_pool.simple_upsert(
   299|                 desc="add_user_account_data",
   300|                 table="account_data",
   301|                 keyvalues={"user_id": user_id, "account_data_type": account_data_type},
   302|                 values={"stream_id": next_id, "content": content_json},
   303|                 lock=False,
   304|             )
   305|             await self._update_max_stream_id(next_id)
   306|             self._account_data_stream_cache.entity_has_changed(user_id, next_id)
   307|             self.get_account_data_for_user.invalidate((user_id,))
   308|             self.get_global_account_data_by_type_for_user.invalidate(
   309|                 (account_data_type, user_id)
   310|             )
   311|         return self._account_data_id_gen.get_current_token()
   312|     async def _update_max_stream_id(self, next_id: int) -> None:
   313|         """Update the max stream_id
   314|         Args:
   315|             next_id: The the revision to advance to.
   316|         """
   317|         def _update(txn):


# ====================================================================
# FILE: synapse/storage/databases/main/appservice.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 8-48 ---
     8| from synapse.util import json_encoder
     9| logger = logging.getLogger(__name__)
    10| def _make_exclusive_regex(services_cache):
    11|     exclusive_user_regexes = [
    12|         regex.pattern
    13|         for service in services_cache
    14|         for regex in service.get_exclusive_user_regexes()
    15|     ]
    16|     if exclusive_user_regexes:
    17|         exclusive_user_regex = "|".join("(" + r + ")" for r in exclusive_user_regexes)
    18|         exclusive_user_regex = re.compile(exclusive_user_regex)
    19|     else:
    20|         exclusive_user_regex = None
    21|     return exclusive_user_regex
    22| class ApplicationServiceWorkerStore(SQLBaseStore):
    23|     def __init__(self, database: DatabasePool, db_conn, hs):
    24|         self.services_cache = load_appservices(
    25|             hs.hostname, hs.config.app_service_config_files
    26|         )
    27|         self.exclusive_user_regex = _make_exclusive_regex(self.services_cache)
    28|         super(ApplicationServiceWorkerStore, self).__init__(database, db_conn, hs)
    29|     def get_app_services(self):
    30|         return self.services_cache
    31|     def get_if_app_services_interested_in_user(self, user_id):
    32|         """Check if the user is one associated with an app service (exclusively)
    33|         """
    34|         if self.exclusive_user_regex:
    35|             return bool(self.exclusive_user_regex.match(user_id))
    36|         else:
    37|             return False
    38|     def get_app_service_by_user_id(self, user_id):
    39|         """Retrieve an application service from their user ID.
    40|         All application services have associated with them a particular user ID.
    41|         There is no distinguishing feature on the user ID which indicates it
    42|         represents an application service. This function allows you to map from
    43|         a user ID to an application service.
    44|         Args:
    45|             user_id(str): The user ID to see if it is an application service.
    46|         Returns:
    47|             synapse.appservice.ApplicationService or None.
    48|         """


# ====================================================================
# FILE: synapse/storage/databases/main/client_ips.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| import logging
     2| from typing import Dict, Optional, Tuple
     3| from synapse.metrics.background_process_metrics import wrap_as_background_process
     4| from synapse.storage._base import SQLBaseStore
     5| from synapse.storage.database import DatabasePool, make_tuple_comparison_clause
     6| from synapse.util.caches.descriptors import Cache
     7| logger = logging.getLogger(__name__)
     8| LAST_SEEN_GRANULARITY = 120 * 1000
     9| class ClientIpBackgroundUpdateStore(SQLBaseStore):
    10|     def __init__(self, database: DatabasePool, db_conn, hs):
    11|         super(ClientIpBackgroundUpdateStore, self).__init__(database, db_conn, hs)
    12|         self.db_pool.updates.register_background_index_update(
    13|             "user_ips_device_index",
    14|             index_name="user_ips_device_id",
    15|             table="user_ips",
    16|             columns=["user_id", "device_id", "last_seen"],
    17|         )
    18|         self.db_pool.updates.register_background_index_update(
    19|             "user_ips_last_seen_index",
    20|             index_name="user_ips_last_seen",
    21|             table="user_ips",
    22|             columns=["user_id", "last_seen"],
    23|         )
    24|         self.db_pool.updates.register_background_index_update(
    25|             "user_ips_last_seen_only_index",
    26|             index_name="user_ips_last_seen_only",
    27|             table="user_ips",
    28|             columns=["last_seen"],
    29|         )
    30|         self.db_pool.updates.register_background_update_handler(
    31|             "user_ips_analyze", self._analyze_user_ip

# --- HUNK 2: Lines 190-230 ---
   190|             """
   191|             txn.execute_batch(sql, rows)
   192|             _, _, _, user_id, device_id = rows[-1]
   193|             self.db_pool.updates._background_update_progress_txn(
   194|                 txn,
   195|                 "devices_last_seen",
   196|                 {"last_user_id": user_id, "last_device_id": device_id},
   197|             )
   198|             return len(rows)
   199|         updated = await self.db_pool.runInteraction(
   200|             "_devices_last_seen_update", _devices_last_seen_update_txn
   201|         )
   202|         if not updated:
   203|             await self.db_pool.updates._end_background_update("devices_last_seen")
   204|         return updated
   205| class ClientIpStore(ClientIpBackgroundUpdateStore):
   206|     def __init__(self, database: DatabasePool, db_conn, hs):
   207|         self.client_ip_last_seen = Cache(
   208|             name="client_ip_last_seen", keylen=4, max_entries=50000
   209|         )
   210|         super(ClientIpStore, self).__init__(database, db_conn, hs)
   211|         self.user_ips_max_age = hs.config.user_ips_max_age
   212|         self._batch_row_update = {}
   213|         self._client_ip_looper = self._clock.looping_call(
   214|             self._update_client_ips_batch, 5 * 1000
   215|         )
   216|         self.hs.get_reactor().addSystemEventTrigger(
   217|             "before", "shutdown", self._update_client_ips_batch
   218|         )
   219|         if self.user_ips_max_age:
   220|             self._clock.looping_call(self._prune_old_user_ips, 5 * 1000)
   221|     async def insert_client_ip(
   222|         self, user_id, access_token, ip, user_agent, device_id, now=None
   223|     ):
   224|         if not now:
   225|             now = int(self._clock.time_msec())
   226|         key = (user_id, access_token, ip)
   227|         try:
   228|             last_seen = self.client_ip_last_seen.get(key)
   229|         except KeyError:
   230|             last_seen = None


# ====================================================================
# FILE: synapse/storage/databases/main/deviceinbox.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 201-335 ---
   201|                 "SELECT max(stream_id), destination"
   202|                 " FROM device_federation_outbox"
   203|                 " WHERE ? < stream_id AND stream_id <= ?"
   204|                 " GROUP BY destination"
   205|             )
   206|             txn.execute(sql, (last_id, upper_pos))
   207|             updates.extend((row[0], row[1:]) for row in txn)
   208|             updates.sort()
   209|             limited = False
   210|             upto_token = current_id
   211|             if len(updates) >= limit:
   212|                 upto_token = updates[-1][0]
   213|                 limited = True
   214|             return updates, upto_token, limited
   215|         return await self.db_pool.runInteraction(
   216|             "get_all_new_device_messages", get_all_new_device_messages_txn
   217|         )
   218| class DeviceInboxBackgroundUpdateStore(SQLBaseStore):
   219|     DEVICE_INBOX_STREAM_ID = "device_inbox_stream_drop"
   220|     def __init__(self, database: DatabasePool, db_conn, hs):
   221|         super(DeviceInboxBackgroundUpdateStore, self).__init__(database, db_conn, hs)
   222|         self.db_pool.updates.register_background_index_update(
   223|             "device_inbox_stream_index",
   224|             index_name="device_inbox_stream_id_user_id",
   225|             table="device_inbox",
   226|             columns=["stream_id", "user_id"],
   227|         )
   228|         self.db_pool.updates.register_background_update_handler(
   229|             self.DEVICE_INBOX_STREAM_ID, self._background_drop_index_device_inbox
   230|         )
   231|     async def _background_drop_index_device_inbox(self, progress, batch_size):
   232|         def reindex_txn(conn):
   233|             txn = conn.cursor()
   234|             txn.execute("DROP INDEX IF EXISTS device_inbox_stream_id")
   235|             txn.close()
   236|         await self.db_pool.runWithConnection(reindex_txn)
   237|         await self.db_pool.updates._end_background_update(self.DEVICE_INBOX_STREAM_ID)
   238|         return 1
   239| class DeviceInboxStore(DeviceInboxWorkerStore, DeviceInboxBackgroundUpdateStore):
   240|     DEVICE_INBOX_STREAM_ID = "device_inbox_stream_drop"
   241|     def __init__(self, database: DatabasePool, db_conn, hs):
   242|         super(DeviceInboxStore, self).__init__(database, db_conn, hs)
   243|         self._last_device_delete_cache = ExpiringCache(
   244|             cache_name="last_device_delete_cache",
   245|             clock=self._clock,
   246|             max_len=10000,
   247|             expiry_ms=30 * 60 * 1000,
   248|         )
   249|     @trace
   250|     async def add_messages_to_device_inbox(
   251|         self,
   252|         local_messages_by_user_then_device: dict,
   253|         remote_messages_by_destination: dict,
   254|     ) -> int:
   255|         """Used to send messages from this server.
   256|         Args:
   257|             local_messages_by_user_and_device:
   258|                 Dictionary of user_id to device_id to message.
   259|             remote_messages_by_destination:
   260|                 Dictionary of destination server_name to the EDU JSON to send.
   261|         Returns:
   262|             The new stream_id.
   263|         """
   264|         def add_messages_txn(txn, now_ms, stream_id):
   265|             self._add_messages_to_local_device_inbox_txn(
   266|                 txn, stream_id, local_messages_by_user_then_device
   267|             )
   268|             sql = (
   269|                 "INSERT INTO device_federation_outbox"
   270|                 " (destination, stream_id, queued_ts, messages_json)"
   271|                 " VALUES (?,?,?,?)"
   272|             )
   273|             rows = []
   274|             for destination, edu in remote_messages_by_destination.items():
   275|                 edu_json = json_encoder.encode(edu)
   276|                 rows.append((destination, stream_id, now_ms, edu_json))
   277|             txn.executemany(sql, rows)
   278|         with await self._device_inbox_id_gen.get_next() as stream_id:
   279|             now_ms = self.clock.time_msec()
   280|             await self.db_pool.runInteraction(
   281|                 "add_messages_to_device_inbox", add_messages_txn, now_ms, stream_id
   282|             )
   283|             for user_id in local_messages_by_user_then_device.keys():
   284|                 self._device_inbox_stream_cache.entity_has_changed(user_id, stream_id)
   285|             for destination in remote_messages_by_destination.keys():
   286|                 self._device_federation_outbox_stream_cache.entity_has_changed(
   287|                     destination, stream_id
   288|                 )
   289|         return self._device_inbox_id_gen.get_current_token()
   290|     async def add_messages_from_remote_to_device_inbox(
   291|         self, origin: str, message_id: str, local_messages_by_user_then_device: dict
   292|     ) -> int:
   293|         def add_messages_txn(txn, now_ms, stream_id):
   294|             already_inserted = self.db_pool.simple_select_one_txn(
   295|                 txn,
   296|                 table="device_federation_inbox",
   297|                 keyvalues={"origin": origin, "message_id": message_id},
   298|                 retcols=("message_id",),
   299|                 allow_none=True,
   300|             )
   301|             if already_inserted is not None:
   302|                 return
   303|             self.db_pool.simple_insert_txn(
   304|                 txn,
   305|                 table="device_federation_inbox",
   306|                 values={
   307|                     "origin": origin,
   308|                     "message_id": message_id,
   309|                     "received_ts": now_ms,
   310|                 },
   311|             )
   312|             self._add_messages_to_local_device_inbox_txn(
   313|                 txn, stream_id, local_messages_by_user_then_device
   314|             )
   315|         with await self._device_inbox_id_gen.get_next() as stream_id:
   316|             now_ms = self.clock.time_msec()
   317|             await self.db_pool.runInteraction(
   318|                 "add_messages_from_remote_to_device_inbox",
   319|                 add_messages_txn,
   320|                 now_ms,
   321|                 stream_id,
   322|             )
   323|             for user_id in local_messages_by_user_then_device.keys():
   324|                 self._device_inbox_stream_cache.entity_has_changed(user_id, stream_id)
   325|         return stream_id
   326|     def _add_messages_to_local_device_inbox_txn(
   327|         self, txn, stream_id, messages_by_user_then_device
   328|     ):
   329|         local_by_user_then_device = {}
   330|         for user_id, messages_by_device in messages_by_user_then_device.items():
   331|             messages_json_for_user = {}
   332|             devices = list(messages_by_device.keys())
   333|             if len(devices) == 1 and devices[0] == "*":
   334|                 sql = "SELECT device_id FROM devices WHERE user_id = ?"
   335|                 txn.execute(sql, (user_id,))


# ====================================================================
# FILE: synapse/storage/databases/main/devices.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 266-306 ---
   266|             key_names=("destination", "user_id"),
   267|             key_values=((destination, user_id) for user_id, _ in rows),
   268|             value_names=("stream_id",),
   269|             value_values=((stream_id,) for _, stream_id in rows),
   270|         )
   271|         sql = """
   272|             DELETE FROM device_lists_outbound_pokes
   273|             WHERE destination = ? AND stream_id <= ?
   274|         """
   275|         txn.execute(sql, (destination, stream_id))
   276|     async def add_user_signature_change_to_streams(
   277|         self, from_user_id: str, user_ids: List[str]
   278|     ) -> int:
   279|         """Persist that a user has made new signatures
   280|         Args:
   281|             from_user_id: the user who made the signatures
   282|             user_ids: the users who were signed
   283|         Returns:
   284|             THe new stream ID.
   285|         """
   286|         with await self._device_list_id_gen.get_next() as stream_id:
   287|             await self.db_pool.runInteraction(
   288|                 "add_user_sig_change_to_streams",
   289|                 self._add_user_signature_change_txn,
   290|                 from_user_id,
   291|                 user_ids,
   292|                 stream_id,
   293|             )
   294|         return stream_id
   295|     def _add_user_signature_change_txn(
   296|         self,
   297|         txn: LoggingTransaction,
   298|         from_user_id: str,
   299|         user_ids: List[str],
   300|         stream_id: int,
   301|     ) -> None:
   302|         txn.call_after(
   303|             self._user_signature_stream_cache.entity_has_changed,
   304|             from_user_id,
   305|             stream_id,
   306|         )

# --- HUNK 2: Lines 355-439 ---
   355|     async def _get_cached_user_device(self, user_id: str, device_id: str) -> JsonDict:
   356|         content = await self.db_pool.simple_select_one_onecol(
   357|             table="device_lists_remote_cache",
   358|             keyvalues={"user_id": user_id, "device_id": device_id},
   359|             retcol="content",
   360|             desc="_get_cached_user_device",
   361|         )
   362|         return db_to_json(content)
   363|     @cached()
   364|     async def get_cached_devices_for_user(self, user_id: str) -> Dict[str, JsonDict]:
   365|         devices = await self.db_pool.simple_select_list(
   366|             table="device_lists_remote_cache",
   367|             keyvalues={"user_id": user_id},
   368|             retcols=("device_id", "content"),
   369|             desc="get_cached_devices_for_user",
   370|         )
   371|         return {
   372|             device["device_id"]: db_to_json(device["content"]) for device in devices
   373|         }
   374|     async def get_users_whose_devices_changed(
   375|         self, from_key: str, user_ids: Iterable[str]
   376|     ) -> Set[str]:
   377|         """Get set of users whose devices have changed since `from_key` that
   378|         are in the given list of user_ids.
   379|         Args:
   380|             from_key: The device lists stream token
   381|             user_ids: The user IDs to query for devices.
   382|         Returns:
   383|             The set of user_ids whose devices have changed since `from_key`
   384|         """
   385|         from_key = int(from_key)
   386|         to_check = self._device_list_stream_cache.get_entities_changed(
   387|             user_ids, from_key
   388|         )
   389|         if not to_check:
   390|             return set()
   391|         def _get_users_whose_devices_changed_txn(txn):
   392|             changes = set()
   393|             sql = """
   394|                 SELECT DISTINCT user_id FROM device_lists_stream
   395|                 WHERE stream_id > ?
   396|                 AND
   397|             """
   398|             for chunk in batch_iter(to_check, 100):
   399|                 clause, args = make_in_list_sql_clause(
   400|                     txn.database_engine, "user_id", chunk
   401|                 )
   402|                 txn.execute(sql + clause, (from_key,) + tuple(args))
   403|                 changes.update(user_id for user_id, in txn)
   404|             return changes
   405|         return await self.db_pool.runInteraction(
   406|             "get_users_whose_devices_changed", _get_users_whose_devices_changed_txn
   407|         )
   408|     async def get_users_whose_signatures_changed(
   409|         self, user_id: str, from_key: str
   410|     ) -> Set[str]:
   411|         """Get the users who have new cross-signing signatures made by `user_id` since
   412|         `from_key`.
   413|         Args:
   414|             user_id: the user who made the signatures
   415|             from_key: The device lists stream token
   416|         Returns:
   417|             A set of user IDs with updated signatures.
   418|         """
   419|         from_key = int(from_key)
   420|         if self._user_signature_stream_cache.has_entity_changed(user_id, from_key):
   421|             sql = """
   422|                 SELECT DISTINCT user_ids FROM user_signature_stream
   423|                 WHERE from_user_id = ? AND stream_id > ?
   424|             """
   425|             rows = await self.db_pool.execute(
   426|                 "get_users_whose_signatures_changed", None, sql, user_id, from_key
   427|             )
   428|             return {user for row in rows for user in db_to_json(row[0])}
   429|         else:
   430|             return set()
   431|     async def get_all_device_list_changes_for_remotes(
   432|         self, instance_name: str, last_id: int, current_id: int, limit: int
   433|     ) -> Tuple[List[Tuple[int, tuple]], int, bool]:
   434|         """Get updates for device lists replication stream.
   435|         Args:
   436|             instance_name: The writer we want to fetch updates from. Unused
   437|                 here since there is only ever one writer.
   438|             last_id: The token to fetch updates from. Exclusive.
   439|             current_id: The token to fetch updates up to. Inclusive.

# --- HUNK 3: Lines 537-577 ---
   537|             desc="make_remote_user_device_cache_as_stale",
   538|         )
   539|     async def mark_remote_user_device_list_as_unsubscribed(self, user_id: str) -> None:
   540|         """Mark that we no longer track device lists for remote user.
   541|         """
   542|         def _mark_remote_user_device_list_as_unsubscribed_txn(txn):
   543|             self.db_pool.simple_delete_txn(
   544|                 txn,
   545|                 table="device_lists_remote_extremeties",
   546|                 keyvalues={"user_id": user_id},
   547|             )
   548|             self._invalidate_cache_and_stream(
   549|                 txn, self.get_device_list_last_stream_id_for_remote, (user_id,)
   550|             )
   551|         await self.db_pool.runInteraction(
   552|             "mark_remote_user_device_list_as_unsubscribed",
   553|             _mark_remote_user_device_list_as_unsubscribed_txn,
   554|         )
   555| class DeviceBackgroundUpdateStore(SQLBaseStore):
   556|     def __init__(self, database: DatabasePool, db_conn, hs):
   557|         super(DeviceBackgroundUpdateStore, self).__init__(database, db_conn, hs)
   558|         self.db_pool.updates.register_background_index_update(
   559|             "device_lists_stream_idx",
   560|             index_name="device_lists_stream_user_id",
   561|             table="device_lists_stream",
   562|             columns=["user_id", "device_id"],
   563|         )
   564|         self.db_pool.updates.register_background_index_update(
   565|             "device_lists_remote_cache_unique_idx",
   566|             index_name="device_lists_remote_cache_unique_id",
   567|             table="device_lists_remote_cache",
   568|             columns=["user_id", "device_id"],
   569|             unique=True,
   570|         )
   571|         self.db_pool.updates.register_background_index_update(
   572|             "device_lists_remote_extremeties_unique_idx",
   573|             index_name="device_lists_remote_extremeties_unique_idx",
   574|             table="device_lists_remote_extremeties",
   575|             columns=["user_id"],
   576|             unique=True,
   577|         )

# --- HUNK 4: Lines 631-671 ---
   631|                 )
   632|                 row["sent"] = False
   633|                 self.db_pool.simple_insert_txn(
   634|                     txn, "device_lists_outbound_pokes", row,
   635|                 )
   636|             if row:
   637|                 self.db_pool.updates._background_update_progress_txn(
   638|                     txn, BG_UPDATE_REMOVE_DUP_OUTBOUND_POKES, {"last_row": row},
   639|                 )
   640|             return len(rows)
   641|         rows = await self.db_pool.runInteraction(
   642|             BG_UPDATE_REMOVE_DUP_OUTBOUND_POKES, _txn
   643|         )
   644|         if not rows:
   645|             await self.db_pool.updates._end_background_update(
   646|                 BG_UPDATE_REMOVE_DUP_OUTBOUND_POKES
   647|             )
   648|         return rows
   649| class DeviceStore(DeviceWorkerStore, DeviceBackgroundUpdateStore):
   650|     def __init__(self, database: DatabasePool, db_conn, hs):
   651|         super(DeviceStore, self).__init__(database, db_conn, hs)
   652|         self.device_id_exists_cache = Cache(
   653|             name="device_id_exists", keylen=2, max_entries=10000
   654|         )
   655|         self._clock.looping_call(self._prune_old_outbound_device_pokes, 60 * 60 * 1000)
   656|     async def store_device(
   657|         self, user_id: str, device_id: str, initial_device_display_name: str
   658|     ) -> bool:
   659|         """Ensure the given device is known; add it to the store if not
   660|         Args:
   661|             user_id: id of user associated with the device
   662|             device_id: id of device
   663|             initial_device_display_name: initial displayname of the device.
   664|                 Ignored if device exists.
   665|         Returns:
   666|             Whether the device was inserted or an existing device existed with that ID.
   667|         Raises:
   668|             StoreError: if the device is already in use
   669|         """
   670|         key = (user_id, device_id)
   671|         if self.device_id_exists_cache.get(key, None):

# --- HUNK 5: Lines 854-907 ---
   854|             self.get_device_list_last_stream_id_for_remote.invalidate, (user_id,)
   855|         )
   856|         self.db_pool.simple_upsert_txn(
   857|             txn,
   858|             table="device_lists_remote_extremeties",
   859|             keyvalues={"user_id": user_id},
   860|             values={"stream_id": stream_id},
   861|             lock=False,
   862|         )
   863|         self.db_pool.simple_delete_txn(
   864|             txn, table="device_lists_remote_resync", keyvalues={"user_id": user_id},
   865|         )
   866|     async def add_device_change_to_streams(
   867|         self, user_id: str, device_ids: Collection[str], hosts: List[str]
   868|     ):
   869|         """Persist that a user's devices have been updated, and which hosts
   870|         (if any) should be poked.
   871|         """
   872|         if not device_ids:
   873|             return
   874|         with await self._device_list_id_gen.get_next_mult(
   875|             len(device_ids)
   876|         ) as stream_ids:
   877|             await self.db_pool.runInteraction(
   878|                 "add_device_change_to_stream",
   879|                 self._add_device_change_to_stream_txn,
   880|                 user_id,
   881|                 device_ids,
   882|                 stream_ids,
   883|             )
   884|         if not hosts:
   885|             return stream_ids[-1]
   886|         context = get_active_span_text_map()
   887|         with await self._device_list_id_gen.get_next_mult(
   888|             len(hosts) * len(device_ids)
   889|         ) as stream_ids:
   890|             await self.db_pool.runInteraction(
   891|                 "add_device_outbound_poke_to_stream",
   892|                 self._add_device_outbound_poke_to_stream_txn,
   893|                 user_id,
   894|                 device_ids,
   895|                 hosts,
   896|                 stream_ids,
   897|                 context,
   898|             )
   899|         return stream_ids[-1]
   900|     def _add_device_change_to_stream_txn(
   901|         self,
   902|         txn: LoggingTransaction,
   903|         user_id: str,
   904|         device_ids: Collection[str],
   905|         stream_ids: List[str],
   906|     ):
   907|         txn.call_after(


# ====================================================================
# FILE: synapse/storage/databases/main/end_to_end_keys.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-36 ---
     1| import abc
     2| from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Tuple
     3| import attr
     4| from canonicaljson import encode_canonical_json
     5| from twisted.enterprise.adbapi import Connection
     6| from synapse.logging.opentracing import log_kv, set_tag, trace
     7| from synapse.storage._base import SQLBaseStore, db_to_json
     8| from synapse.storage.database import make_in_list_sql_clause
     9| from synapse.storage.types import Cursor
    10| from synapse.types import JsonDict
    11| from synapse.util import json_encoder
    12| from synapse.util.caches.descriptors import cached, cachedList
    13| from synapse.util.iterutils import batch_iter
    14| if TYPE_CHECKING:
    15|     from synapse.handlers.e2e_keys import SignatureListItem
    16| @attr.s
    17| class DeviceKeyLookupResult:
    18|     """The type returned by get_e2e_device_keys_and_signatures"""
    19|     display_name = attr.ib(type=Optional[str])
    20|     keys = attr.ib(type=Optional[JsonDict])
    21| class EndToEndKeyWorkerStore(SQLBaseStore):
    22|     async def get_e2e_device_keys_for_federation_query(
    23|         self, user_id: str
    24|     ) -> Tuple[int, List[JsonDict]]:
    25|         """Get all devices (with any device keys) for a user
    26|         Returns:
    27|             (stream_id, devices)
    28|         """
    29|         now_stream_id = self.get_device_stream_token()
    30|         devices = await self.get_e2e_device_keys_and_signatures([(user_id, None)])
    31|         if devices:
    32|             user_devices = devices[user_id]
    33|             results = []
    34|             for device_id, device in user_devices.items():
    35|                 result = {"device_id": device_id}
    36|                 keys = device.keys

# --- HUNK 2: Lines 625-665 ---
   625|         self.db_pool.simple_insert_txn(
   626|             txn,
   627|             "e2e_cross_signing_keys",
   628|             values={
   629|                 "user_id": user_id,
   630|                 "keytype": key_type,
   631|                 "keydata": json_encoder.encode(key),
   632|                 "stream_id": stream_id,
   633|             },
   634|         )
   635|         self._invalidate_cache_and_stream(
   636|             txn, self._get_bare_e2e_cross_signing_keys, (user_id,)
   637|         )
   638|     async def set_e2e_cross_signing_key(self, user_id, key_type, key):
   639|         """Set a user's cross-signing key.
   640|         Args:
   641|             user_id (str): the user to set the user-signing key for
   642|             key_type (str): the type of cross-signing key to set
   643|             key (dict): the key data
   644|         """
   645|         with await self._cross_signing_id_gen.get_next() as stream_id:
   646|             return await self.db_pool.runInteraction(
   647|                 "add_e2e_cross_signing_key",
   648|                 self._set_e2e_cross_signing_key_txn,
   649|                 user_id,
   650|                 key_type,
   651|                 key,
   652|                 stream_id,
   653|             )
   654|     async def store_e2e_cross_signing_signatures(
   655|         self, user_id: str, signatures: "Iterable[SignatureListItem]"
   656|     ) -> None:
   657|         """Stores cross-signing signatures.
   658|         Args:
   659|             user_id: the user who made the signatures
   660|             signatures: signatures to add
   661|         """
   662|         await self.db_pool.simple_insert_many(
   663|             "e2e_cross_signing_signatures",
   664|             [
   665|                 {


# ====================================================================
# FILE: synapse/storage/databases/main/event_federation.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 260-300 ---
   260|         stream_orderings from that point.
   261|         Args:
   262|             room_id:
   263|             stream_ordering:
   264|         Returns:
   265|             A list of event_ids
   266|         """
   267|         last_change = self._events_stream_cache.get_max_pos_of_last_change(room_id)
   268|         last_change = max(self._stream_order_on_start, last_change)
   269|         if last_change > self.stream_ordering_month_ago:
   270|             stream_ordering = min(last_change, stream_ordering)
   271|         return await self._get_forward_extremeties_for_room(room_id, stream_ordering)
   272|     @cached(max_entries=5000, num_args=2)
   273|     async def _get_forward_extremeties_for_room(self, room_id, stream_ordering):
   274|         """For a given room_id and stream_ordering, return the forward
   275|         extremeties of the room at that point in "time".
   276|         Throws a StoreError if we have since purged the index for
   277|         stream_orderings from that point.
   278|         """
   279|         if stream_ordering <= self.stream_ordering_month_ago:
   280|             raise StoreError(400, "stream_ordering too old")
   281|         sql = """
   282|                 SELECT event_id FROM stream_ordering_to_exterm
   283|                 INNER JOIN (
   284|                     SELECT room_id, MAX(stream_ordering) AS stream_ordering
   285|                     FROM stream_ordering_to_exterm
   286|                     WHERE stream_ordering <= ? GROUP BY room_id
   287|                 ) AS rms USING (room_id, stream_ordering)
   288|                 WHERE room_id = ?
   289|         """
   290|         def get_forward_extremeties_for_room_txn(txn):
   291|             txn.execute(sql, (stream_ordering, room_id))
   292|             return [event_id for event_id, in txn]
   293|         return await self.db_pool.runInteraction(
   294|             "get_forward_extremeties_for_room", get_forward_extremeties_for_room_txn
   295|         )
   296|     async def get_backfill_events(self, room_id: str, event_list: list, limit: int):
   297|         """Get a list of Events for a given topic that occurred before (and
   298|         including) the events in event_list. Return a list of max size `limit`
   299|         Args:
   300|             room_id

# --- HUNK 2: Lines 383-423 ---
   383|             event_ids: The events to use as the previous events.
   384|         """
   385|         rows = await self.db_pool.simple_select_many_batch(
   386|             table="event_edges",
   387|             column="prev_event_id",
   388|             iterable=event_ids,
   389|             retcols=("event_id",),
   390|             desc="get_successor_events",
   391|         )
   392|         return [row["event_id"] for row in rows]
   393| class EventFederationStore(EventFederationWorkerStore):
   394|     """ Responsible for storing and serving up the various graphs associated
   395|     with an event. Including the main event graph and the auth chains for an
   396|     event.
   397|     Also has methods for getting the front (latest) and back (oldest) edges
   398|     of the event graphs. These are used to generate the parents for new events
   399|     and backfilling from another server respectively.
   400|     """
   401|     EVENT_AUTH_STATE_ONLY = "event_auth_state_only"
   402|     def __init__(self, database: DatabasePool, db_conn, hs):
   403|         super(EventFederationStore, self).__init__(database, db_conn, hs)
   404|         self.db_pool.updates.register_background_update_handler(
   405|             self.EVENT_AUTH_STATE_ONLY, self._background_delete_non_state_event_auth
   406|         )
   407|         hs.get_clock().looping_call(
   408|             self._delete_old_forward_extrem_cache, 60 * 60 * 1000
   409|         )
   410|     def _delete_old_forward_extrem_cache(self):
   411|         def _delete_old_forward_extrem_cache_txn(txn):
   412|             sql = """
   413|                 DELETE FROM stream_ordering_to_exterm
   414|                 WHERE
   415|                 room_id IN (
   416|                     SELECT room_id
   417|                     FROM stream_ordering_to_exterm
   418|                     WHERE stream_ordering > ?
   419|                 ) AND stream_ordering < ?
   420|             """
   421|             txn.execute(
   422|                 sql, (self.stream_ordering_month_ago, self.stream_ordering_month_ago)
   423|             )


# ====================================================================
# FILE: synapse/storage/databases/main/event_push_actions.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 22-62 ---
    22|     any real JSON actions
    23|     """
    24|     if is_highlight:
    25|         if actions == DEFAULT_HIGHLIGHT_ACTION:
    26|             return ""  # We use empty string as the column is non-NULL
    27|     else:
    28|         if actions == DEFAULT_NOTIF_ACTION:
    29|             return ""
    30|     return json_encoder.encode(actions)
    31| def _deserialize_action(actions, is_highlight):
    32|     """Custom deserializer for actions. This allows us to "compress" common actions
    33|     """
    34|     if actions:
    35|         return db_to_json(actions)
    36|     if is_highlight:
    37|         return DEFAULT_HIGHLIGHT_ACTION
    38|     else:
    39|         return DEFAULT_NOTIF_ACTION
    40| class EventPushActionsWorkerStore(SQLBaseStore):
    41|     def __init__(self, database: DatabasePool, db_conn, hs):
    42|         super(EventPushActionsWorkerStore, self).__init__(database, db_conn, hs)
    43|         self.stream_ordering_month_ago = None
    44|         self.stream_ordering_day_ago = None
    45|         cur = LoggingTransaction(
    46|             db_conn.cursor(),
    47|             name="_find_stream_orderings_for_times_txn",
    48|             database_engine=self.database_engine,
    49|         )
    50|         self._find_stream_orderings_for_times_txn(cur)
    51|         cur.close()
    52|         self.find_stream_orderings_looping_call = self._clock.looping_call(
    53|             self._find_stream_orderings_for_times, 10 * 60 * 1000
    54|         )
    55|         self._rotate_delay = 3
    56|         self._rotate_count = 10000
    57|     @cached(num_args=3, tree=True, max_entries=5000)
    58|     async def get_unread_event_push_actions_by_room_for_user(
    59|         self, room_id: str, user_id: str, last_read_event_id: Optional[str],
    60|     ) -> Dict[str, int]:
    61|         """Get the notification count, the highlight count and the unread message count
    62|         for a given user in a given room after the given read receipt.

# --- HUNK 2: Lines 476-516 ---
   476|         return range_end
   477|     async def get_time_of_last_push_action_before(self, stream_ordering):
   478|         def f(txn):
   479|             sql = (
   480|                 "SELECT e.received_ts"
   481|                 " FROM event_push_actions AS ep"
   482|                 " JOIN events e ON ep.room_id = e.room_id AND ep.event_id = e.event_id"
   483|                 " WHERE ep.stream_ordering > ? AND notif = 1"
   484|                 " ORDER BY ep.stream_ordering ASC"
   485|                 " LIMIT 1"
   486|             )
   487|             txn.execute(sql, (stream_ordering,))
   488|             return txn.fetchone()
   489|         result = await self.db_pool.runInteraction(
   490|             "get_time_of_last_push_action_before", f
   491|         )
   492|         return result[0] if result else None
   493| class EventPushActionsStore(EventPushActionsWorkerStore):
   494|     EPA_HIGHLIGHT_INDEX = "epa_highlight_index"
   495|     def __init__(self, database: DatabasePool, db_conn, hs):
   496|         super(EventPushActionsStore, self).__init__(database, db_conn, hs)
   497|         self.db_pool.updates.register_background_index_update(
   498|             self.EPA_HIGHLIGHT_INDEX,
   499|             index_name="event_push_actions_u_highlight",
   500|             table="event_push_actions",
   501|             columns=["user_id", "stream_ordering"],
   502|         )
   503|         self.db_pool.updates.register_background_index_update(
   504|             "event_push_actions_highlights_index",
   505|             index_name="event_push_actions_highlights_index",
   506|             table="event_push_actions",
   507|             columns=["user_id", "room_id", "topological_ordering", "stream_ordering"],
   508|             where_clause="highlight=1",
   509|         )
   510|         self._doing_notif_rotation = False
   511|         self._rotate_notif_loop = self._clock.looping_call(
   512|             self._start_rotate_notifs, 30 * 60 * 1000
   513|         )
   514|     async def get_push_actions_for_user(
   515|         self, user_id, before=None, limit=50, only_highlight=False
   516|     ):

# --- HUNK 3: Lines 715-743 ---
   715|             ),
   716|         )
   717|         txn.execute(
   718|             "DELETE FROM event_push_actions"
   719|             " WHERE ? <= stream_ordering AND stream_ordering < ? AND highlight = 0",
   720|             (old_rotate_stream_ordering, rotate_to_stream_ordering),
   721|         )
   722|         logger.info("Rotating notifications, deleted %s push actions", txn.rowcount)
   723|         txn.execute(
   724|             "UPDATE event_push_summary_stream_ordering SET stream_ordering = ?",
   725|             (rotate_to_stream_ordering,),
   726|         )
   727| def _action_has_highlight(actions):
   728|     for action in actions:
   729|         try:
   730|             if action.get("set_tweak", None) == "highlight":
   731|                 return action.get("value", True)
   732|         except AttributeError:
   733|             pass
   734|     return False
   735| @attr.s
   736| class _EventPushSummary:
   737|     """Summary of pending event push actions for a given user in a given room.
   738|     Used in _rotate_notifs_before_txn to manipulate results from event_push_actions.
   739|     """
   740|     unread_count = attr.ib(type=int)
   741|     stream_ordering = attr.ib(type=int)
   742|     old_user_id = attr.ib(type=str)
   743|     notif_count = attr.ib(type=int)


# ====================================================================
# FILE: synapse/storage/databases/main/events.py
# Total hunks: 8
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| import itertools
     2| import logging
     3| from collections import OrderedDict, namedtuple
     4| from typing import TYPE_CHECKING, Dict, Iterable, List, Set, Tuple
     5| import attr
     6| from prometheus_client import Counter
     7| import synapse.metrics
     8| from synapse.api.constants import EventContentFields, EventTypes, RelationTypes
     9| from synapse.api.room_versions import RoomVersions
    10| from synapse.crypto.event_signing import compute_event_reference_hash
    11| from synapse.events import EventBase  # noqa: F401
    12| from synapse.events.snapshot import EventContext  # noqa: F401
    13| from synapse.logging.utils import log_function
    14| from synapse.storage._base import db_to_json, make_in_list_sql_clause
    15| from synapse.storage.database import DatabasePool, LoggingTransaction
    16| from synapse.storage.databases.main.search import SearchEntry
    17| from synapse.storage.util.id_generators import StreamIdGenerator
    18| from synapse.types import StateMap, get_domain_from_id
    19| from synapse.util.frozenutils import frozendict_json_encoder
    20| from synapse.util.iterutils import batch_iter
    21| if TYPE_CHECKING:
    22|     from synapse.server import HomeServer
    23|     from synapse.storage.databases.main import DataStore
    24| logger = logging.getLogger(__name__)
    25| persist_event_counter = Counter("synapse_storage_events_persisted_events", "")
    26| event_counter = Counter(
    27|     "synapse_storage_events_persisted_events_sep",
    28|     "",
    29|     ["type", "origin_type", "origin_entity"],
    30| )
    31| def encode_json(json_object):
    32|     """
    33|     Encode a Python object as JSON and return it in a Unicode string.
    34|     """
    35|     out = frozendict_json_encoder.encode(json_object)
    36|     if isinstance(out, bytes):
    37|         out = out.decode("utf8")

# --- HUNK 2: Lines 45-163 ---
    45|         to_insert: Map of state to upsert into current state
    46|         no_longer_in_room: The server is not longer in the room, so the room
    47|             should e.g. be removed from `current_state_events` table.
    48|     """
    49|     to_delete = attr.ib(type=List[Tuple[str, str]])
    50|     to_insert = attr.ib(type=StateMap[str])
    51|     no_longer_in_room = attr.ib(type=bool, default=False)
    52| class PersistEventsStore:
    53|     """Contains all the functions for writing events to the database.
    54|     Should only be instantiated on one process (when using a worker mode setup).
    55|     Note: This is not part of the `DataStore` mixin.
    56|     """
    57|     def __init__(
    58|         self, hs: "HomeServer", db: DatabasePool, main_data_store: "DataStore"
    59|     ):
    60|         self.hs = hs
    61|         self.db_pool = db
    62|         self.store = main_data_store
    63|         self.database_engine = db.engine
    64|         self._clock = hs.get_clock()
    65|         self._ephemeral_messages_enabled = hs.config.enable_ephemeral_messages
    66|         self.is_mine_id = hs.is_mine_id
    67|         self._backfill_id_gen = self.store._backfill_id_gen  # type: StreamIdGenerator
    68|         self._stream_id_gen = self.store._stream_id_gen  # type: StreamIdGenerator
    69|         assert (
    70|             hs.config.worker.writers.events == hs.get_instance_name()
    71|         ), "Can only instantiate EventsStore on master"
    72|     async def _persist_events_and_state_updates(
    73|         self,
    74|         events_and_contexts: List[Tuple[EventBase, EventContext]],
    75|         current_state_for_room: Dict[str, StateMap[str]],
    76|         state_delta_for_room: Dict[str, DeltaState],
    77|         new_forward_extremeties: Dict[str, List[str]],
    78|         backfilled: bool = False,
    79|     ) -> None:
    80|         """Persist a set of events alongside updates to the current state and
    81|         forward extremities tables.
    82|         Args:
    83|             events_and_contexts:
    84|             current_state_for_room: Map from room_id to the current state of
    85|                 the room based on forward extremities
    86|             state_delta_for_room: Map from room_id to the delta to apply to
    87|                 room state
    88|             new_forward_extremities: Map from room_id to list of event IDs
    89|                 that are the new forward extremities of the room.
    90|             backfilled
    91|         Returns:
    92|             Resolves when the events have been persisted
    93|         """
    94|         if backfilled:
    95|             stream_ordering_manager = await self._backfill_id_gen.get_next_mult(
    96|                 len(events_and_contexts)
    97|             )
    98|         else:
    99|             stream_ordering_manager = await self._stream_id_gen.get_next_mult(
   100|                 len(events_and_contexts)
   101|             )
   102|         with stream_ordering_manager as stream_orderings:
   103|             for (event, context), stream in zip(events_and_contexts, stream_orderings):
   104|                 event.internal_metadata.stream_ordering = stream
   105|             await self.db_pool.runInteraction(
   106|                 "persist_events",
   107|                 self._persist_events_txn,
   108|                 events_and_contexts=events_and_contexts,
   109|                 backfilled=backfilled,
   110|                 state_delta_for_room=state_delta_for_room,
   111|                 new_forward_extremeties=new_forward_extremeties,
   112|             )
   113|             persist_event_counter.inc(len(events_and_contexts))
   114|             if not backfilled:
   115|                 synapse.metrics.event_persisted_position.set(
   116|                     events_and_contexts[-1][0].internal_metadata.stream_ordering
   117|                 )
   118|             for event, context in events_and_contexts:
   119|                 if context.app_service:
   120|                     origin_type = "local"
   121|                     origin_entity = context.app_service.id
   122|                 elif self.hs.is_mine_id(event.sender):
   123|                     origin_type = "local"
   124|                     origin_entity = "*client*"
   125|                 else:
   126|                     origin_type = "remote"
   127|                     origin_entity = get_domain_from_id(event.sender)
   128|                 event_counter.labels(event.type, origin_type, origin_entity).inc()
   129|             for room_id, new_state in current_state_for_room.items():
   130|                 self.store.get_current_state_ids.prefill((room_id,), new_state)
   131|             for room_id, latest_event_ids in new_forward_extremeties.items():
   132|                 self.store.get_latest_event_ids_in_room.prefill(
   133|                     (room_id,), list(latest_event_ids)
   134|                 )
   135|     async def _get_events_which_are_prevs(self, event_ids: Iterable[str]) -> List[str]:
   136|         """Filter the supplied list of event_ids to get those which are prev_events of
   137|         existing (non-outlier/rejected) events.
   138|         Args:
   139|             event_ids: event ids to filter
   140|         Returns:
   141|             Filtered event ids
   142|         """
   143|         results = []
   144|         def _get_events_which_are_prevs_txn(txn, batch):
   145|             sql = """
   146|             SELECT prev_event_id, internal_metadata
   147|             FROM event_edges
   148|                 INNER JOIN events USING (event_id)
   149|                 LEFT JOIN rejections USING (event_id)
   150|                 LEFT JOIN event_json USING (event_id)
   151|             WHERE
   152|                 NOT events.outlier
   153|                 AND rejections.event_id IS NULL
   154|                 AND
   155|             """
   156|             clause, args = make_in_list_sql_clause(
   157|                 self.database_engine, "prev_event_id", batch
   158|             )
   159|             txn.execute(sql + clause, args)
   160|             results.extend(r[0] for r in txn if not db_to_json(r[1]).get("soft_failed"))
   161|         for chunk in batch_iter(event_ids, 100):
   162|             await self.db_pool.runInteraction(
   163|                 "_get_events_which_are_prevs", _get_events_which_are_prevs_txn, chunk

# --- HUNK 3: Lines 424-491 ---
   424|             values=[
   425|                 {"event_id": ev_id, "room_id": room_id}
   426|                 for room_id, new_extrem in new_forward_extremities.items()
   427|                 for ev_id in new_extrem
   428|             ],
   429|         )
   430|         self.db_pool.simple_insert_many_txn(
   431|             txn,
   432|             table="stream_ordering_to_exterm",
   433|             values=[
   434|                 {
   435|                     "room_id": room_id,
   436|                     "event_id": event_id,
   437|                     "stream_ordering": max_stream_order,
   438|                 }
   439|                 for room_id, new_extrem in new_forward_extremities.items()
   440|                 for event_id in new_extrem
   441|             ],
   442|         )
   443|     @classmethod
   444|     def _filter_events_and_contexts_for_duplicates(cls, events_and_contexts):
   445|         """Ensure that we don't have the same event twice.
   446|         Pick the earliest non-outlier if there is one, else the earliest one.
   447|         Args:
   448|             events_and_contexts (list[(EventBase, EventContext)]):
   449|         Returns:
   450|             list[(EventBase, EventContext)]: filtered list
   451|         """
   452|         new_events_and_contexts = OrderedDict()
   453|         for event, context in events_and_contexts:
   454|             prev_event_context = new_events_and_contexts.get(event.event_id)
   455|             if prev_event_context:
   456|                 if not event.internal_metadata.is_outlier():
   457|                     if prev_event_context[0].internal_metadata.is_outlier():
   458|                         new_events_and_contexts.pop(event.event_id, None)
   459|                         new_events_and_contexts[event.event_id] = (event, context)
   460|             else:
   461|                 new_events_and_contexts[event.event_id] = (event, context)
   462|         return list(new_events_and_contexts.values())
   463|     def _update_room_depths_txn(self, txn, events_and_contexts, backfilled):
   464|         """Update min_depth for each room
   465|         Args:
   466|             txn (twisted.enterprise.adbapi.Connection): db connection
   467|             events_and_contexts (list[(EventBase, EventContext)]): events
   468|                 we are persisting
   469|             backfilled (bool): True if the events were backfilled
   470|         """
   471|         depth_updates = {}
   472|         for event, context in events_and_contexts:
   473|             txn.call_after(self.store._invalidate_get_event_cache, event.event_id)
   474|             if not backfilled:
   475|                 txn.call_after(
   476|                     self.store._events_stream_cache.entity_has_changed,
   477|                     event.room_id,
   478|                     event.internal_metadata.stream_ordering,
   479|                 )
   480|             if not event.internal_metadata.is_outlier() and not context.rejected:
   481|                 depth_updates[event.room_id] = max(
   482|                     event.depth, depth_updates.get(event.room_id, event.depth)
   483|                 )
   484|         for room_id, depth in depth_updates.items():
   485|             self._update_min_depth_for_room_txn(txn, room_id, depth)
   486|     def _update_outliers_txn(self, txn, events_and_contexts):
   487|         """Update any outliers with new event info.
   488|         This turns outliers into ex-outliers (unless the new event was
   489|         rejected).
   490|         Args:
   491|             txn (twisted.enterprise.adbapi.Connection): db connection

# --- HUNK 4: Lines 551-590 ---
   551|             txn,
   552|             table="event_json",
   553|             values=[
   554|                 {
   555|                     "event_id": event.event_id,
   556|                     "room_id": event.room_id,
   557|                     "internal_metadata": encode_json(
   558|                         event.internal_metadata.get_dict()
   559|                     ),
   560|                     "json": encode_json(event_dict(event)),
   561|                     "format_version": event.format_version,
   562|                 }
   563|                 for event, _ in events_and_contexts
   564|             ],
   565|         )
   566|         self.db_pool.simple_insert_many_txn(
   567|             txn,
   568|             table="events",
   569|             values=[
   570|                 {
   571|                     "stream_ordering": event.internal_metadata.stream_ordering,
   572|                     "topological_ordering": event.depth,
   573|                     "depth": event.depth,
   574|                     "event_id": event.event_id,
   575|                     "room_id": event.room_id,
   576|                     "type": event.type,
   577|                     "processed": True,
   578|                     "outlier": event.internal_metadata.is_outlier(),
   579|                     "origin_server_ts": int(event.origin_server_ts),
   580|                     "received_ts": self._clock.time_msec(),
   581|                     "sender": event.sender,
   582|                     "contains_url": (
   583|                         "url" in event.content and isinstance(event.content["url"], str)
   584|                     ),
   585|                 }
   586|                 for event, _ in events_and_contexts
   587|             ],
   588|         )
   589|         for event, _ in events_and_contexts:
   590|             if not event.internal_metadata.is_redacted():

# --- HUNK 5: Lines 780-831 ---
   780|         Args:
   781|             txn (cursor):
   782|             events (list): list of Events.
   783|         """
   784|         vals = []
   785|         for event in events:
   786|             ref_alg, ref_hash_bytes = compute_event_reference_hash(event)
   787|             vals.append(
   788|                 {
   789|                     "event_id": event.event_id,
   790|                     "algorithm": ref_alg,
   791|                     "hash": memoryview(ref_hash_bytes),
   792|                 }
   793|             )
   794|         self.db_pool.simple_insert_many_txn(
   795|             txn, table="event_reference_hashes", values=vals
   796|         )
   797|     def _store_room_members_txn(self, txn, events, backfilled):
   798|         """Store a room member in the database.
   799|         """
   800|         self.db_pool.simple_insert_many_txn(
   801|             txn,
   802|             table="room_memberships",
   803|             values=[
   804|                 {
   805|                     "event_id": event.event_id,
   806|                     "user_id": event.state_key,
   807|                     "sender": event.user_id,
   808|                     "room_id": event.room_id,
   809|                     "membership": event.membership,
   810|                     "display_name": event.content.get("displayname", None),
   811|                     "avatar_url": event.content.get("avatar_url", None),
   812|                 }
   813|                 for event in events
   814|             ],
   815|         )
   816|         for event in events:
   817|             txn.call_after(
   818|                 self.store._membership_stream_cache.entity_has_changed,
   819|                 event.state_key,
   820|                 event.internal_metadata.stream_ordering,
   821|             )
   822|             txn.call_after(
   823|                 self.store.get_invited_rooms_for_local_user.invalidate,
   824|                 (event.state_key,),
   825|             )
   826|             if (
   827|                 self.is_mine_id(event.state_key)
   828|                 and not backfilled
   829|                 and event.internal_metadata.is_outlier()
   830|                 and event.internal_metadata.is_out_of_band_membership()
   831|             ):

# --- HUNK 6: Lines 1062-1102 ---
  1062|             table="event_edges",
  1063|             values=[
  1064|                 {
  1065|                     "event_id": ev.event_id,
  1066|                     "prev_event_id": e_id,
  1067|                     "room_id": ev.room_id,
  1068|                     "is_state": False,
  1069|                 }
  1070|                 for ev in events
  1071|                 for e_id in ev.prev_event_ids()
  1072|             ],
  1073|         )
  1074|         self._update_backward_extremeties(txn, events)
  1075|     def _update_backward_extremeties(self, txn, events):
  1076|         """Updates the event_backward_extremities tables based on the new/updated
  1077|         events being persisted.
  1078|         This is called for new events *and* for events that were outliers, but
  1079|         are now being persisted as non-outliers.
  1080|         Forward extremities are handled when we first start persisting the events.
  1081|         """
  1082|         events_by_room = {}
  1083|         for ev in events:
  1084|             events_by_room.setdefault(ev.room_id, []).append(ev)
  1085|         query = (
  1086|             "INSERT INTO event_backward_extremities (event_id, room_id)"
  1087|             " SELECT ?, ? WHERE NOT EXISTS ("
  1088|             " SELECT 1 FROM event_backward_extremities"
  1089|             " WHERE event_id = ? AND room_id = ?"
  1090|             " )"
  1091|             " AND NOT EXISTS ("
  1092|             " SELECT 1 FROM events WHERE event_id = ? AND room_id = ? "
  1093|             " AND outlier = ?"
  1094|             " )"
  1095|         )
  1096|         txn.executemany(
  1097|             query,
  1098|             [
  1099|                 (e_id, ev.room_id, e_id, ev.room_id, e_id, ev.room_id, False)
  1100|                 for ev in events
  1101|                 for e_id in ev.prev_event_ids()
  1102|                 if not ev.internal_metadata.is_outlier()


# ====================================================================
# FILE: synapse/storage/databases/main/events_bg_updates.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| import logging
     2| from synapse.api.constants import EventContentFields
     3| from synapse.storage._base import SQLBaseStore, db_to_json, make_in_list_sql_clause
     4| from synapse.storage.database import DatabasePool
     5| logger = logging.getLogger(__name__)
     6| class EventsBackgroundUpdatesStore(SQLBaseStore):
     7|     EVENT_ORIGIN_SERVER_TS_NAME = "event_origin_server_ts"
     8|     EVENT_FIELDS_SENDER_URL_UPDATE_NAME = "event_fields_sender_url"
     9|     DELETE_SOFT_FAILED_EXTREMITIES = "delete_soft_failed_extremities"
    10|     def __init__(self, database: DatabasePool, db_conn, hs):
    11|         super(EventsBackgroundUpdatesStore, self).__init__(database, db_conn, hs)
    12|         self.db_pool.updates.register_background_update_handler(
    13|             self.EVENT_ORIGIN_SERVER_TS_NAME, self._background_reindex_origin_server_ts
    14|         )
    15|         self.db_pool.updates.register_background_update_handler(
    16|             self.EVENT_FIELDS_SENDER_URL_UPDATE_NAME,
    17|             self._background_reindex_fields_sender,
    18|         )
    19|         self.db_pool.updates.register_background_index_update(
    20|             "event_contains_url_index",
    21|             index_name="event_contains_url_index",
    22|             table="events",
    23|             columns=["room_id", "topological_ordering", "stream_ordering"],
    24|             where_clause="contains_url = true AND outlier = false",
    25|         )
    26|         self.db_pool.updates.register_background_index_update(
    27|             "event_search_event_id_idx",
    28|             index_name="event_search_event_id_idx",
    29|             table="event_search",
    30|             columns=["event_id"],
    31|             unique=True,


# ====================================================================
# FILE: synapse/storage/databases/main/events_worker.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-81 ---
     1| from __future__ import division
     2| import itertools
     3| import logging
     4| import threading
     5| from collections import namedtuple
     6| from typing import Dict, Iterable, List, Optional, Tuple, overload
     7| from constantly import NamedConstant, Names
     8| from typing_extensions import Literal
     9| from twisted.internet import defer
    10| from synapse.api.constants import EventTypes
    11| from synapse.api.errors import NotFoundError, SynapseError
    12| from synapse.api.room_versions import (
    13|     KNOWN_ROOM_VERSIONS,
    14|     EventFormatVersions,
    15|     RoomVersions,
    16| )
    17| from synapse.events import EventBase, make_event_from_dict
    18| from synapse.events.utils import prune_event
    19| from synapse.logging.context import PreserveLoggingContext, current_context
    20| from synapse.metrics.background_process_metrics import run_as_background_process
    21| from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
    22| from synapse.replication.tcp.streams import BackfillStream
    23| from synapse.replication.tcp.streams.events import EventsStream
    24| from synapse.storage._base import SQLBaseStore, db_to_json, make_in_list_sql_clause
    25| from synapse.storage.database import DatabasePool
    26| from synapse.storage.util.id_generators import StreamIdGenerator
    27| from synapse.types import Collection, get_domain_from_id
    28| from synapse.util.caches.descriptors import Cache, cached
    29| from synapse.util.iterutils import batch_iter
    30| from synapse.util.metrics import Measure
    31| logger = logging.getLogger(__name__)
    32| EVENT_QUEUE_THREADS = 3  # Max number of threads that will fetch events
    33| EVENT_QUEUE_ITERATIONS = 3  # No. times we block waiting for requests for events
    34| EVENT_QUEUE_TIMEOUT_S = 0.1  # Timeout when waiting for requests for events
    35| _EventCacheEntry = namedtuple("_EventCacheEntry", ("event", "redacted_event"))
    36| class EventRedactBehaviour(Names):
    37|     """
    38|     What to do when retrieving a redacted event from the database.
    39|     """
    40|     AS_IS = NamedConstant()
    41|     REDACT = NamedConstant()
    42|     BLOCK = NamedConstant()
    43| class EventsWorkerStore(SQLBaseStore):
    44|     def __init__(self, database: DatabasePool, db_conn, hs):
    45|         super(EventsWorkerStore, self).__init__(database, db_conn, hs)
    46|         if hs.config.worker.writers.events == hs.get_instance_name():
    47|             self._stream_id_gen = StreamIdGenerator(
    48|                 db_conn, "events", "stream_ordering",
    49|             )
    50|             self._backfill_id_gen = StreamIdGenerator(
    51|                 db_conn,
    52|                 "events",
    53|                 "stream_ordering",
    54|                 step=-1,
    55|                 extra_tables=[("ex_outlier_stream", "event_stream_ordering")],
    56|             )
    57|         else:
    58|             self._stream_id_gen = SlavedIdTracker(db_conn, "events", "stream_ordering")
    59|             self._backfill_id_gen = SlavedIdTracker(
    60|                 db_conn, "events", "stream_ordering", step=-1
    61|             )
    62|         self._get_event_cache = Cache(
    63|             "*getEvent*",
    64|             keylen=3,
    65|             max_entries=hs.config.caches.event_cache_size,
    66|             apply_cache_factor_from_config=False,
    67|         )
    68|         self._event_fetch_lock = threading.Condition()
    69|         self._event_fetch_list = []
    70|         self._event_fetch_ongoing = 0
    71|     def process_replication_rows(self, stream_name, instance_name, token, rows):
    72|         if stream_name == EventsStream.NAME:
    73|             self._stream_id_gen.advance(instance_name, token)
    74|         elif stream_name == BackfillStream.NAME:
    75|             self._backfill_id_gen.advance(instance_name, -token)
    76|         super().process_replication_rows(stream_name, instance_name, token, rows)
    77|     async def get_received_ts(self, event_id: str) -> Optional[int]:
    78|         """Get received_ts (when it was persisted) for the event.
    79|         Raises an exception for unknown events.
    80|         Args:
    81|             event_id: The event ID to query.


# ====================================================================
# FILE: synapse/storage/databases/main/group_server.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1068-1108 ---
  1068|                         table="group_attestations_remote",
  1069|                         values={
  1070|                             "group_id": group_id,
  1071|                             "user_id": user_id,
  1072|                             "valid_until_ms": remote_attestation["valid_until_ms"],
  1073|                             "attestation_json": json_encoder.encode(remote_attestation),
  1074|                         },
  1075|                     )
  1076|             else:
  1077|                 self.db_pool.simple_delete_txn(
  1078|                     txn,
  1079|                     table="group_attestations_renewals",
  1080|                     keyvalues={"group_id": group_id, "user_id": user_id},
  1081|                 )
  1082|                 self.db_pool.simple_delete_txn(
  1083|                     txn,
  1084|                     table="group_attestations_remote",
  1085|                     keyvalues={"group_id": group_id, "user_id": user_id},
  1086|                 )
  1087|             return next_id
  1088|         with await self._group_updates_id_gen.get_next() as next_id:
  1089|             res = await self.db_pool.runInteraction(
  1090|                 "register_user_group_membership",
  1091|                 _register_user_group_membership_txn,
  1092|                 next_id,
  1093|             )
  1094|         return res
  1095|     async def create_group(
  1096|         self, group_id, user_id, name, avatar_url, short_description, long_description
  1097|     ) -> None:
  1098|         await self.db_pool.simple_insert(
  1099|             table="groups",
  1100|             values={
  1101|                 "group_id": group_id,
  1102|                 "name": name,
  1103|                 "avatar_url": avatar_url,
  1104|                 "short_description": short_description,
  1105|                 "long_description": long_description,
  1106|                 "is_public": True,
  1107|             },
  1108|             desc="create_group",


# ====================================================================
# FILE: synapse/storage/databases/main/media_repository.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-39 ---
     1| from typing import Any, Dict, Iterable, List, Optional, Tuple
     2| from synapse.storage._base import SQLBaseStore
     3| from synapse.storage.database import DatabasePool
     4| class MediaRepositoryBackgroundUpdateStore(SQLBaseStore):
     5|     def __init__(self, database: DatabasePool, db_conn, hs):
     6|         super(MediaRepositoryBackgroundUpdateStore, self).__init__(
     7|             database, db_conn, hs
     8|         )
     9|         self.db_pool.updates.register_background_index_update(
    10|             update_name="local_media_repository_url_idx",
    11|             index_name="local_media_repository_url_idx",
    12|             table="local_media_repository",
    13|             columns=["created_ts"],
    14|             where_clause="url_cache IS NOT NULL",
    15|         )
    16| class MediaRepositoryStore(MediaRepositoryBackgroundUpdateStore):
    17|     """Persistence for attachments and avatars"""
    18|     def __init__(self, database: DatabasePool, db_conn, hs):
    19|         super(MediaRepositoryStore, self).__init__(database, db_conn, hs)
    20|     async def get_local_media(self, media_id: str) -> Optional[Dict[str, Any]]:
    21|         """Get the metadata for a local piece of media
    22|         Returns:
    23|             None if the media_id doesn't exist.
    24|         """
    25|         return await self.db_pool.simple_select_one(
    26|             "local_media_repository",
    27|             {"media_id": media_id},
    28|             (
    29|                 "media_type",
    30|                 "media_length",
    31|                 "upload_name",
    32|                 "created_ts",
    33|                 "quarantined_by",
    34|                 "url_cache",
    35|             ),
    36|             allow_none=True,
    37|             desc="get_local_media",
    38|         )
    39|     async def store_local_media(


# ====================================================================
# FILE: synapse/storage/databases/main/metrics.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-59 ---
     1| import typing
     2| from collections import Counter
     3| from synapse.metrics import BucketCollector
     4| from synapse.metrics.background_process_metrics import run_as_background_process
     5| from synapse.storage._base import SQLBaseStore
     6| from synapse.storage.database import DatabasePool
     7| from synapse.storage.databases.main.event_push_actions import (
     8|     EventPushActionsWorkerStore,
     9| )
    10| class ServerMetricsStore(EventPushActionsWorkerStore, SQLBaseStore):
    11|     """Functions to pull various metrics from the DB, for e.g. phone home
    12|     stats and prometheus metrics.
    13|     """
    14|     def __init__(self, database: DatabasePool, db_conn, hs):
    15|         super().__init__(database, db_conn, hs)
    16|         self._current_forward_extremities_amount = (
    17|             Counter()
    18|         )  # type: typing.Counter[int]
    19|         BucketCollector(
    20|             "synapse_forward_extremities",
    21|             lambda: self._current_forward_extremities_amount,
    22|             buckets=[1, 2, 3, 5, 7, 10, 15, 20, 50, 100, 200, 500, "+Inf"],
    23|         )
    24|         def read_forward_extremities():
    25|             return run_as_background_process(
    26|                 "read_forward_extremities", self._read_forward_extremities
    27|             )
    28|         hs.get_clock().looping_call(read_forward_extremities, 60 * 60 * 1000)
    29|     async def _read_forward_extremities(self):
    30|         def fetch(txn):
    31|             txn.execute(
    32|                 """
    33|                 select count(*) c from event_forward_extremities
    34|                 group by room_id
    35|                 """
    36|             )
    37|             return txn.fetchall()
    38|         res = await self.db_pool.runInteraction("read_forward_extremities", fetch)
    39|         self._current_forward_extremities_amount = Counter([x[0] for x in res])
    40|     async def count_daily_messages(self):
    41|         """
    42|         Returns an estimate of the number of messages sent in the last day.
    43|         If it has been significantly less or more than one day since the last
    44|         call to this function, it will return None.
    45|         """
    46|         def _count_messages(txn):
    47|             sql = """
    48|                 SELECT COALESCE(COUNT(*), 0) FROM events
    49|                 WHERE type = 'm.room.message'
    50|                 AND stream_ordering > ?
    51|             """
    52|             txn.execute(sql, (self.stream_ordering_day_ago,))
    53|             (count,) = txn.fetchone()
    54|             return count
    55|         return await self.db_pool.runInteraction("count_messages", _count_messages)
    56|     async def count_daily_sent_messages(self):
    57|         def _count_messages(txn):
    58|             like_clause = "%:" + self.hs.hostname
    59|             sql = """


# ====================================================================
# FILE: synapse/storage/databases/main/monthly_active_users.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-40 ---
     1| import logging
     2| from typing import Dict, List
     3| from synapse.storage._base import SQLBaseStore
     4| from synapse.storage.database import DatabasePool, make_in_list_sql_clause
     5| from synapse.util.caches.descriptors import cached
     6| logger = logging.getLogger(__name__)
     7| LAST_SEEN_GRANULARITY = 60 * 60 * 1000
     8| class MonthlyActiveUsersWorkerStore(SQLBaseStore):
     9|     def __init__(self, database: DatabasePool, db_conn, hs):
    10|         super(MonthlyActiveUsersWorkerStore, self).__init__(database, db_conn, hs)
    11|         self._clock = hs.get_clock()
    12|         self.hs = hs
    13|     @cached(num_args=0)
    14|     async def get_monthly_active_count(self) -> int:
    15|         """Generates current count of monthly active users
    16|         Returns:
    17|             Number of current monthly active users
    18|         """
    19|         def _count_users(txn):
    20|             sql = "SELECT COALESCE(count(*), 0) FROM monthly_active_users"
    21|             txn.execute(sql)
    22|             (count,) = txn.fetchone()
    23|             return count
    24|         return await self.db_pool.runInteraction("count_users", _count_users)
    25|     @cached(num_args=0)
    26|     async def get_monthly_active_count_by_service(self) -> Dict[str, int]:
    27|         """Generates current count of monthly active users broken down by service.
    28|         A service is typically an appservice but also includes native matrix users.
    29|         Since the `monthly_active_users` table is populated from the `user_ips` table
    30|         `config.track_appservice_user_ips` must be set to `true` for this
    31|         method to return anything other than native matrix users.
    32|         Returns:
    33|             A mapping between app_service_id and the number of occurrences.
    34|         """
    35|         def _count_users_by_service(txn):
    36|             sql = """
    37|                 SELECT COALESCE(appservice_id, 'native'), COALESCE(count(*), 0)
    38|                 FROM monthly_active_users
    39|                 LEFT JOIN users ON monthly_active_users.user_id=users.name
    40|                 GROUP BY appservice_id;

# --- HUNK 2: Lines 62-102 ---
    62|                 users.append(user_id)
    63|         return users
    64|     @cached(num_args=1)
    65|     async def user_last_seen_monthly_active(self, user_id: str) -> int:
    66|         """
    67|         Checks if a given user is part of the monthly active user group
    68|         Arguments:
    69|             user_id: user to add/update
    70|         Return:
    71|             Timestamp since last seen, None if never seen
    72|         """
    73|         return await self.db_pool.simple_select_one_onecol(
    74|             table="monthly_active_users",
    75|             keyvalues={"user_id": user_id},
    76|             retcol="timestamp",
    77|             allow_none=True,
    78|             desc="user_last_seen_monthly_active",
    79|         )
    80| class MonthlyActiveUsersStore(MonthlyActiveUsersWorkerStore):
    81|     def __init__(self, database: DatabasePool, db_conn, hs):
    82|         super(MonthlyActiveUsersStore, self).__init__(database, db_conn, hs)
    83|         self._limit_usage_by_mau = hs.config.limit_usage_by_mau
    84|         self._mau_stats_only = hs.config.mau_stats_only
    85|         self._max_mau_value = hs.config.max_mau_value
    86|         self.db_pool.new_transaction(
    87|             db_conn,
    88|             "initialise_mau_threepids",
    89|             [],
    90|             [],
    91|             self._initialise_reserved_users,
    92|             hs.config.mau_limits_reserved_threepids[: self._max_mau_value],
    93|         )
    94|     def _initialise_reserved_users(self, txn, threepids):
    95|         """Ensures that reserved threepids are accounted for in the MAU table, should
    96|         be called on start up.
    97|         Args:
    98|             txn (cursor):
    99|             threepids (list[dict]): List of threepid dicts to reserve
   100|         """
   101|         for tp in threepids:
   102|             user_id = self.get_user_id_by_threepid_txn(txn, tp["medium"], tp["address"])


# ====================================================================
# FILE: synapse/storage/databases/main/presence.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| from typing import List, Tuple
     2| from synapse.api.presence import UserPresenceState
     3| from synapse.storage._base import SQLBaseStore, make_in_list_sql_clause
     4| from synapse.util.caches.descriptors import cached, cachedList
     5| from synapse.util.iterutils import batch_iter
     6| class PresenceStore(SQLBaseStore):
     7|     async def update_presence(self, presence_states):
     8|         stream_ordering_manager = await self._presence_id_gen.get_next_mult(
     9|             len(presence_states)
    10|         )
    11|         with stream_ordering_manager as stream_orderings:
    12|             await self.db_pool.runInteraction(
    13|                 "update_presence",
    14|                 self._update_presence_txn,
    15|                 stream_orderings,
    16|                 presence_states,
    17|             )
    18|         return stream_orderings[-1], self._presence_id_gen.get_current_token()
    19|     def _update_presence_txn(self, txn, stream_orderings, presence_states):
    20|         for stream_id, state in zip(stream_orderings, presence_states):
    21|             txn.call_after(
    22|                 self.presence_stream_cache.entity_has_changed, state.user_id, stream_id
    23|             )
    24|             txn.call_after(self._get_presence_for_user.invalidate, (state.user_id,))
    25|         self.db_pool.simple_insert_many_txn(
    26|             txn,
    27|             table="presence_stream",
    28|             values=[
    29|                 {
    30|                     "stream_id": stream_id,
    31|                     "user_id": state.user_id,


# ====================================================================
# FILE: synapse/storage/databases/main/purge_events.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 3-51 ---
     3| from synapse.api.errors import SynapseError
     4| from synapse.storage._base import SQLBaseStore
     5| from synapse.storage.databases.main.state import StateGroupWorkerStore
     6| from synapse.types import RoomStreamToken
     7| logger = logging.getLogger(__name__)
     8| class PurgeEventsStore(StateGroupWorkerStore, SQLBaseStore):
     9|     async def purge_history(
    10|         self, room_id: str, token: str, delete_local_events: bool
    11|     ) -> Set[int]:
    12|         """Deletes room history before a certain point
    13|         Args:
    14|             room_id:
    15|             token: A topological token to delete events before
    16|             delete_local_events:
    17|                 if True, we will delete local events as well as remote ones
    18|                 (instead of just marking them as outliers and deleting their
    19|                 state groups).
    20|         Returns:
    21|             The set of state groups that are referenced by deleted events.
    22|         """
    23|         return await self.db_pool.runInteraction(
    24|             "purge_history",
    25|             self._purge_history_txn,
    26|             room_id,
    27|             token,
    28|             delete_local_events,
    29|         )
    30|     def _purge_history_txn(self, txn, room_id, token_str, delete_local_events):
    31|         token = RoomStreamToken.parse(token_str)
    32|         txn.execute("DROP TABLE IF EXISTS events_to_purge")
    33|         txn.execute(
    34|             "CREATE TEMPORARY TABLE events_to_purge ("
    35|             "    event_id TEXT NOT NULL,"
    36|             "    should_delete BOOLEAN NOT NULL"
    37|             ")"
    38|         )
    39|         txn.execute(
    40|             "SELECT e.event_id, e.depth FROM events as e "
    41|             "INNER JOIN event_forward_extremities as f "
    42|             "ON e.event_id = f.event_id "
    43|             "AND e.room_id = f.room_id "
    44|             "WHERE f.room_id = ?",
    45|             (room_id,),
    46|         )
    47|         rows = txn.fetchall()
    48|         max_depth = max(row[1] for row in rows)
    49|         if max_depth < token.topological:
    50|             raise SynapseError(
    51|                 400, "topological_ordering is greater than forward extremeties"

# --- HUNK 2: Lines 191-230 ---
   191|             "event_push_actions_staging",
   192|             "event_reference_hashes",
   193|             "event_relations",
   194|             "event_to_state_groups",
   195|             "redactions",
   196|             "rejections",
   197|             "state_events",
   198|         ):
   199|             logger.info("[purge] removing %s from %s", room_id, table)
   200|             txn.execute(
   201|                 """
   202|                 DELETE FROM %s WHERE event_id IN (
   203|                   SELECT event_id FROM events WHERE room_id=?
   204|                 )
   205|                 """
   206|                 % (table,),
   207|                 (room_id,),
   208|             )
   209|         for table in (
   210|             "current_state_events",
   211|             "event_backward_extremities",
   212|             "event_forward_extremities",
   213|             "event_json",
   214|             "event_push_actions",
   215|             "event_search",
   216|             "events",
   217|             "group_rooms",
   218|             "public_room_list_stream",
   219|             "receipts_graph",
   220|             "receipts_linearized",
   221|             "room_aliases",
   222|             "room_depth",
   223|             "room_memberships",
   224|             "room_stats_state",
   225|             "room_stats_current",
   226|             "room_stats_historical",
   227|             "room_stats_earliest_token",
   228|             "rooms",
   229|             "stream_ordering_to_exterm",
   230|             "users_in_public_rooms",


# ====================================================================
# FILE: synapse/storage/databases/main/push_rule.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-69 ---
     1| import abc
     2| import logging
     3| from typing import List, Tuple, Union
     4| from synapse.push.baserules import list_with_base_rules
     5| from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
     6| from synapse.storage._base import SQLBaseStore, db_to_json
     7| from synapse.storage.database import DatabasePool
     8| from synapse.storage.databases.main.appservice import ApplicationServiceWorkerStore
     9| from synapse.storage.databases.main.events_worker import EventsWorkerStore
    10| from synapse.storage.databases.main.pusher import PusherWorkerStore
    11| from synapse.storage.databases.main.receipts import ReceiptsWorkerStore
    12| from synapse.storage.databases.main.roommember import RoomMemberWorkerStore
    13| from synapse.storage.push_rule import InconsistentRuleException, RuleNotFoundException
    14| from synapse.storage.util.id_generators import StreamIdGenerator
    15| from synapse.util import json_encoder
    16| from synapse.util.caches.descriptors import cached, cachedList
    17| from synapse.util.caches.stream_change_cache import StreamChangeCache
    18| logger = logging.getLogger(__name__)
    19| def _load_rules(rawrules, enabled_map, use_new_defaults=False):
    20|     ruleslist = []
    21|     for rawrule in rawrules:
    22|         rule = dict(rawrule)
    23|         rule["conditions"] = db_to_json(rawrule["conditions"])
    24|         rule["actions"] = db_to_json(rawrule["actions"])
    25|         rule["default"] = False
    26|         ruleslist.append(rule)
    27|     rules = list(list_with_base_rules(ruleslist, use_new_defaults))
    28|     for i, rule in enumerate(rules):
    29|         rule_id = rule["rule_id"]
    30|         if rule_id in enabled_map:
    31|             if rule.get("enabled", True) != bool(enabled_map[rule_id]):
    32|                 rule = dict(rule)
    33|                 rule["enabled"] = bool(enabled_map[rule_id])
    34|                 rules[i] = rule
    35|     return rules
    36| class PushRulesWorkerStore(
    37|     ApplicationServiceWorkerStore,
    38|     ReceiptsWorkerStore,
    39|     PusherWorkerStore,
    40|     RoomMemberWorkerStore,
    41|     EventsWorkerStore,
    42|     SQLBaseStore,
    43| ):
    44|     """This is an abstract base class where subclasses must implement
    45|     `get_max_push_rules_stream_id` which can be called in the initializer.
    46|     """
    47|     __metaclass__ = abc.ABCMeta
    48|     def __init__(self, database: DatabasePool, db_conn, hs):
    49|         super(PushRulesWorkerStore, self).__init__(database, db_conn, hs)
    50|         if hs.config.worker.worker_app is None:
    51|             self._push_rules_stream_id_gen = StreamIdGenerator(
    52|                 db_conn, "push_rules_stream", "stream_id"
    53|             )  # type: Union[StreamIdGenerator, SlavedIdTracker]
    54|         else:
    55|             self._push_rules_stream_id_gen = SlavedIdTracker(
    56|                 db_conn, "push_rules_stream", "stream_id"
    57|             )
    58|         push_rules_prefill, push_rules_id = self.db_pool.get_cache_dict(
    59|             db_conn,
    60|             "push_rules_stream",
    61|             entity_column="user_id",
    62|             stream_column="stream_id",
    63|             max_value=self.get_max_push_rules_stream_id(),
    64|         )
    65|         self.push_rules_stream_cache = StreamChangeCache(
    66|             "PushRulesStreamChangeCache",
    67|             push_rules_id,
    68|             prefilled_cache=push_rules_prefill,
    69|         )

# --- HUNK 2: Lines 239-279 ---
   239|             if len(updates) == limit:
   240|                 limited = True
   241|                 upper_bound = updates[-1][0]
   242|             return updates, upper_bound, limited
   243|         return await self.db_pool.runInteraction(
   244|             "get_all_push_rule_updates", get_all_push_rule_updates_txn
   245|         )
   246| class PushRuleStore(PushRulesWorkerStore):
   247|     async def add_push_rule(
   248|         self,
   249|         user_id,
   250|         rule_id,
   251|         priority_class,
   252|         conditions,
   253|         actions,
   254|         before=None,
   255|         after=None,
   256|     ) -> None:
   257|         conditions_json = json_encoder.encode(conditions)
   258|         actions_json = json_encoder.encode(actions)
   259|         with await self._push_rules_stream_id_gen.get_next() as stream_id:
   260|             event_stream_ordering = self._stream_id_gen.get_current_token()
   261|             if before or after:
   262|                 await self.db_pool.runInteraction(
   263|                     "_add_push_rule_relative_txn",
   264|                     self._add_push_rule_relative_txn,
   265|                     stream_id,
   266|                     event_stream_ordering,
   267|                     user_id,
   268|                     rule_id,
   269|                     priority_class,
   270|                     conditions_json,
   271|                     actions_json,
   272|                     before,
   273|                     after,
   274|                 )
   275|             else:
   276|                 await self.db_pool.runInteraction(
   277|                     "_add_push_rule_highest_priority_txn",
   278|                     self._add_push_rule_highest_priority_txn,
   279|                     stream_id,

# --- HUNK 3: Lines 407-538 ---
   407|                     "priority": priority,
   408|                     "conditions": conditions_json,
   409|                     "actions": actions_json,
   410|                 },
   411|             )
   412|         if update_stream:
   413|             self._insert_push_rules_update_txn(
   414|                 txn,
   415|                 stream_id,
   416|                 event_stream_ordering,
   417|                 user_id,
   418|                 rule_id,
   419|                 op="ADD",
   420|                 data={
   421|                     "priority_class": priority_class,
   422|                     "priority": priority,
   423|                     "conditions": conditions_json,
   424|                     "actions": actions_json,
   425|                 },
   426|             )
   427|     async def delete_push_rule(self, user_id: str, rule_id: str) -> None:
   428|         """
   429|         Delete a push rule. Args specify the row to be deleted and can be
   430|         any of the columns in the push_rule table, but below are the
   431|         standard ones
   432|         Args:
   433|             user_id: The matrix ID of the push rule owner
   434|             rule_id: The rule_id of the rule to be deleted
   435|         """
   436|         def delete_push_rule_txn(txn, stream_id, event_stream_ordering):
   437|             self.db_pool.simple_delete_one_txn(
   438|                 txn, "push_rules", {"user_name": user_id, "rule_id": rule_id}
   439|             )
   440|             self._insert_push_rules_update_txn(
   441|                 txn, stream_id, event_stream_ordering, user_id, rule_id, op="DELETE"
   442|             )
   443|         with await self._push_rules_stream_id_gen.get_next() as stream_id:
   444|             event_stream_ordering = self._stream_id_gen.get_current_token()
   445|             await self.db_pool.runInteraction(
   446|                 "delete_push_rule",
   447|                 delete_push_rule_txn,
   448|                 stream_id,
   449|                 event_stream_ordering,
   450|             )
   451|     async def set_push_rule_enabled(self, user_id, rule_id, enabled) -> None:
   452|         with await self._push_rules_stream_id_gen.get_next() as stream_id:
   453|             event_stream_ordering = self._stream_id_gen.get_current_token()
   454|             await self.db_pool.runInteraction(
   455|                 "_set_push_rule_enabled_txn",
   456|                 self._set_push_rule_enabled_txn,
   457|                 stream_id,
   458|                 event_stream_ordering,
   459|                 user_id,
   460|                 rule_id,
   461|                 enabled,
   462|             )
   463|     def _set_push_rule_enabled_txn(
   464|         self, txn, stream_id, event_stream_ordering, user_id, rule_id, enabled
   465|     ):
   466|         new_id = self._push_rules_enable_id_gen.get_next()
   467|         self.db_pool.simple_upsert_txn(
   468|             txn,
   469|             "push_rules_enable",
   470|             {"user_name": user_id, "rule_id": rule_id},
   471|             {"enabled": 1 if enabled else 0},
   472|             {"id": new_id},
   473|         )
   474|         self._insert_push_rules_update_txn(
   475|             txn,
   476|             stream_id,
   477|             event_stream_ordering,
   478|             user_id,
   479|             rule_id,
   480|             op="ENABLE" if enabled else "DISABLE",
   481|         )
   482|     async def set_push_rule_actions(
   483|         self, user_id, rule_id, actions, is_default_rule
   484|     ) -> None:
   485|         actions_json = json_encoder.encode(actions)
   486|         def set_push_rule_actions_txn(txn, stream_id, event_stream_ordering):
   487|             if is_default_rule:
   488|                 priority_class = -1
   489|                 priority = 1
   490|                 self._upsert_push_rule_txn(
   491|                     txn,
   492|                     stream_id,
   493|                     event_stream_ordering,
   494|                     user_id,
   495|                     rule_id,
   496|                     priority_class,
   497|                     priority,
   498|                     "[]",
   499|                     actions_json,
   500|                     update_stream=False,
   501|                 )
   502|             else:
   503|                 self.db_pool.simple_update_one_txn(
   504|                     txn,
   505|                     "push_rules",
   506|                     {"user_name": user_id, "rule_id": rule_id},
   507|                     {"actions": actions_json},
   508|                 )
   509|             self._insert_push_rules_update_txn(
   510|                 txn,
   511|                 stream_id,
   512|                 event_stream_ordering,
   513|                 user_id,
   514|                 rule_id,
   515|                 op="ACTIONS",
   516|                 data={"actions": actions_json},
   517|             )
   518|         with await self._push_rules_stream_id_gen.get_next() as stream_id:
   519|             event_stream_ordering = self._stream_id_gen.get_current_token()
   520|             await self.db_pool.runInteraction(
   521|                 "set_push_rule_actions",
   522|                 set_push_rule_actions_txn,
   523|                 stream_id,
   524|                 event_stream_ordering,
   525|             )
   526|     def _insert_push_rules_update_txn(
   527|         self, txn, stream_id, event_stream_ordering, user_id, rule_id, op, data=None
   528|     ):
   529|         values = {
   530|             "stream_id": stream_id,
   531|             "event_stream_ordering": event_stream_ordering,
   532|             "user_id": user_id,
   533|             "rule_id": rule_id,
   534|             "op": op,
   535|         }
   536|         if data is not None:
   537|             values.update(data)
   538|         self.db_pool.simple_insert_txn(txn, "push_rules_stream", values=values)


# ====================================================================
# FILE: synapse/storage/databases/main/pusher.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 202-242 ---
   202|             lock=False,
   203|         )
   204| class PusherStore(PusherWorkerStore):
   205|     def get_pushers_stream_token(self):
   206|         return self._pushers_id_gen.get_current_token()
   207|     async def add_pusher(
   208|         self,
   209|         user_id,
   210|         access_token,
   211|         kind,
   212|         app_id,
   213|         app_display_name,
   214|         device_display_name,
   215|         pushkey,
   216|         pushkey_ts,
   217|         lang,
   218|         data,
   219|         last_stream_ordering,
   220|         profile_tag="",
   221|     ) -> None:
   222|         with await self._pushers_id_gen.get_next() as stream_id:
   223|             await self.db_pool.simple_upsert(
   224|                 table="pushers",
   225|                 keyvalues={"app_id": app_id, "pushkey": pushkey, "user_name": user_id},
   226|                 values={
   227|                     "access_token": access_token,
   228|                     "kind": kind,
   229|                     "app_display_name": app_display_name,
   230|                     "device_display_name": device_display_name,
   231|                     "ts": pushkey_ts,
   232|                     "lang": lang,
   233|                     "data": bytearray(encode_canonical_json(data)),
   234|                     "last_stream_ordering": last_stream_ordering,
   235|                     "profile_tag": profile_tag,
   236|                     "id": stream_id,
   237|                 },
   238|                 desc="add_pusher",
   239|                 lock=False,
   240|             )
   241|             user_has_pusher = self.get_if_user_has_pusher.cache.get(
   242|                 (user_id,), None, update_metrics=False

# --- HUNK 2: Lines 253-276 ---
   253|     ) -> None:
   254|         def delete_pusher_txn(txn, stream_id):
   255|             self._invalidate_cache_and_stream(
   256|                 txn, self.get_if_user_has_pusher, (user_id,)
   257|             )
   258|             self.db_pool.simple_delete_one_txn(
   259|                 txn,
   260|                 "pushers",
   261|                 {"app_id": app_id, "pushkey": pushkey, "user_name": user_id},
   262|             )
   263|             self.db_pool.simple_insert_txn(
   264|                 txn,
   265|                 table="deleted_pushers",
   266|                 values={
   267|                     "stream_id": stream_id,
   268|                     "app_id": app_id,
   269|                     "pushkey": pushkey,
   270|                     "user_id": user_id,
   271|                 },
   272|             )
   273|         with await self._pushers_id_gen.get_next() as stream_id:
   274|             await self.db_pool.runInteraction(
   275|                 "delete_pusher", delete_pusher_txn, stream_id
   276|             )


# ====================================================================
# FILE: synapse/storage/databases/main/receipts.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-39 ---
     1| import abc
     2| import logging
     3| from typing import Any, Dict, List, Optional, Tuple
     4| from twisted.internet import defer
     5| from synapse.storage._base import SQLBaseStore, db_to_json, make_in_list_sql_clause
     6| from synapse.storage.database import DatabasePool
     7| from synapse.storage.util.id_generators import StreamIdGenerator
     8| from synapse.util import json_encoder
     9| from synapse.util.async_helpers import ObservableDeferred
    10| from synapse.util.caches.descriptors import cached, cachedList
    11| from synapse.util.caches.stream_change_cache import StreamChangeCache
    12| logger = logging.getLogger(__name__)
    13| class ReceiptsWorkerStore(SQLBaseStore):
    14|     """This is an abstract base class where subclasses must implement
    15|     `get_max_receipt_stream_id` which can be called in the initializer.
    16|     """
    17|     __metaclass__ = abc.ABCMeta
    18|     def __init__(self, database: DatabasePool, db_conn, hs):
    19|         super(ReceiptsWorkerStore, self).__init__(database, db_conn, hs)
    20|         self._receipts_stream_cache = StreamChangeCache(
    21|             "ReceiptsRoomChangeCache", self.get_max_receipt_stream_id()
    22|         )
    23|     @abc.abstractmethod
    24|     def get_max_receipt_stream_id(self):
    25|         """Get the current max stream ID for receipts stream
    26|         Returns:
    27|             int
    28|         """
    29|         raise NotImplementedError()
    30|     @cached()
    31|     async def get_users_with_read_receipts_in_room(self, room_id):
    32|         receipts = await self.get_receipts_for_room(room_id, "m.read")
    33|         return {r["user_id"] for r in receipts}
    34|     @cached(num_args=2)
    35|     async def get_receipts_for_room(
    36|         self, room_id: str, receipt_type: str
    37|     ) -> List[Dict[str, Any]]:
    38|         return await self.db_pool.simple_select_list(
    39|             table="receipts_linearized",

# --- HUNK 2: Lines 265-305 ---
   265|         self, room_id: str, receipt_type: str, user_id: str
   266|     ):
   267|         if receipt_type != "m.read":
   268|             return
   269|         res = self.get_users_with_read_receipts_in_room.cache.get(
   270|             room_id, None, update_metrics=False
   271|         )
   272|         if isinstance(res, ObservableDeferred):
   273|             if res.has_called():
   274|                 res = res.get_result()
   275|             else:
   276|                 res = None
   277|         if res and user_id in res:
   278|             return
   279|         self.get_users_with_read_receipts_in_room.invalidate((room_id,))
   280| class ReceiptsStore(ReceiptsWorkerStore):
   281|     def __init__(self, database: DatabasePool, db_conn, hs):
   282|         self._receipts_id_gen = StreamIdGenerator(
   283|             db_conn, "receipts_linearized", "stream_id"
   284|         )
   285|         super(ReceiptsStore, self).__init__(database, db_conn, hs)
   286|     def get_max_receipt_stream_id(self):
   287|         return self._receipts_id_gen.get_current_token()
   288|     def insert_linearized_receipt_txn(
   289|         self, txn, room_id, receipt_type, user_id, event_id, data, stream_id
   290|     ):
   291|         """Inserts a read-receipt into the database if it's newer than the current RR
   292|         Returns: int|None
   293|             None if the RR is older than the current RR
   294|             otherwise, the rx timestamp of the event that the RR corresponds to
   295|                 (or 0 if the event is unknown)
   296|         """
   297|         res = self.db_pool.simple_select_one_txn(
   298|             txn,
   299|             table="events",
   300|             retcols=["stream_ordering", "received_ts"],
   301|             keyvalues={"event_id": event_id},
   302|             allow_none=True,
   303|         )
   304|         stream_ordering = int(res["stream_ordering"]) if res else None
   305|         rx_ts = res["received_ts"] if res else 0

# --- HUNK 3: Lines 377-417 ---
   377|             def graph_to_linear(txn):
   378|                 clause, args = make_in_list_sql_clause(
   379|                     self.database_engine, "event_id", event_ids
   380|                 )
   381|                 sql = """
   382|                     SELECT event_id WHERE room_id = ? AND stream_ordering IN (
   383|                         SELECT max(stream_ordering) WHERE %s
   384|                     )
   385|                 """ % (
   386|                     clause,
   387|                 )
   388|                 txn.execute(sql, [room_id] + list(args))
   389|                 rows = txn.fetchall()
   390|                 if rows:
   391|                     return rows[0][0]
   392|                 else:
   393|                     raise RuntimeError("Unrecognized event_ids: %r" % (event_ids,))
   394|             linearized_event_id = await self.db_pool.runInteraction(
   395|                 "insert_receipt_conv", graph_to_linear
   396|             )
   397|         with await self._receipts_id_gen.get_next() as stream_id:
   398|             event_ts = await self.db_pool.runInteraction(
   399|                 "insert_linearized_receipt",
   400|                 self.insert_linearized_receipt_txn,
   401|                 room_id,
   402|                 receipt_type,
   403|                 user_id,
   404|                 linearized_event_id,
   405|                 data,
   406|                 stream_id=stream_id,
   407|             )
   408|         if event_ts is None:
   409|             return None
   410|         now = self._clock.time_msec()
   411|         logger.debug(
   412|             "RR for event %s in %s (%i ms old)",
   413|             linearized_event_id,
   414|             room_id,
   415|             now - event_ts,
   416|         )
   417|         await self.insert_graph_receipt(room_id, receipt_type, user_id, event_ids, data)


# ====================================================================
# FILE: synapse/storage/databases/main/registration.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| import logging
     2| import re
     3| from typing import Any, Dict, List, Optional, Tuple
     4| from synapse.api.constants import UserTypes
     5| from synapse.api.errors import Codes, StoreError, SynapseError, ThreepidValidationError
     6| from synapse.metrics.background_process_metrics import run_as_background_process
     7| from synapse.storage._base import SQLBaseStore
     8| from synapse.storage.database import DatabasePool
     9| from synapse.storage.types import Cursor
    10| from synapse.storage.util.sequence import build_sequence_generator
    11| from synapse.types import UserID
    12| from synapse.util.caches.descriptors import cached
    13| THIRTY_MINUTES_IN_MS = 30 * 60 * 1000
    14| logger = logging.getLogger(__name__)
    15| class RegistrationWorkerStore(SQLBaseStore):
    16|     def __init__(self, database: DatabasePool, db_conn, hs):
    17|         super(RegistrationWorkerStore, self).__init__(database, db_conn, hs)
    18|         self.config = hs.config
    19|         self.clock = hs.get_clock()
    20|         self._user_id_seq = build_sequence_generator(
    21|             database.engine, find_max_generated_user_id_localpart, "user_id_seq",
    22|         )
    23|     @cached()
    24|     async def get_user_by_id(self, user_id: str) -> Optional[Dict[str, Any]]:
    25|         return await self.db_pool.simple_select_one(
    26|             table="users",
    27|             keyvalues={"name": user_id},
    28|             retcols=[
    29|                 "name",
    30|                 "password_hash",
    31|                 "is_guest",
    32|                 "admin",
    33|                 "consent_version",
    34|                 "consent_server_notice_sent",
    35|                 "appservice_id",
    36|                 "creation_ts",
    37|                 "user_type",

# --- HUNK 2: Lines 65-104 ---
    65|         """
    66|         return await self.db_pool.runInteraction(
    67|             "get_user_by_access_token", self._query_for_auth, token
    68|         )
    69|     @cached()
    70|     async def get_expiration_ts_for_user(self, user_id: str) -> Optional[int]:
    71|         """Get the expiration timestamp for the account bearing a given user ID.
    72|         Args:
    73|             user_id: The ID of the user.
    74|         Returns:
    75|             None, if the account has no expiration timestamp, otherwise int
    76|             representation of the timestamp (as a number of milliseconds since epoch).
    77|         """
    78|         return await self.db_pool.simple_select_one_onecol(
    79|             table="account_validity",
    80|             keyvalues={"user_id": user_id},
    81|             retcol="expiration_ts_ms",
    82|             allow_none=True,
    83|             desc="get_expiration_ts_for_user",
    84|         )
    85|     async def set_account_validity_for_user(
    86|         self,
    87|         user_id: str,
    88|         expiration_ts: int,
    89|         email_sent: bool,
    90|         renewal_token: Optional[str] = None,
    91|     ) -> None:
    92|         """Updates the account validity properties of the given account, with the
    93|         given values.
    94|         Args:
    95|             user_id: ID of the account to update properties for.
    96|             expiration_ts: New expiration date, as a timestamp in milliseconds
    97|                 since epoch.
    98|             email_sent: True means a renewal email has been sent for this account
    99|                 and there's no need to send another one for the current validity
   100|                 period.
   101|             renewal_token: Renewal token the user can use to extend the validity
   102|                 of their account. Defaults to no token.
   103|         """
   104|         def set_account_validity_for_user_txn(txn):

# --- HUNK 3: Lines 284-330 ---
   284|         res = self.db_pool.simple_select_one_onecol_txn(
   285|             txn=txn,
   286|             table="users",
   287|             keyvalues={"name": user_id},
   288|             retcol="user_type",
   289|             allow_none=True,
   290|         )
   291|         return True if res == UserTypes.SUPPORT else False
   292|     async def get_users_by_id_case_insensitive(self, user_id: str) -> Dict[str, str]:
   293|         """Gets users that match user_id case insensitively.
   294|         Returns:
   295|              A mapping of user_id -> password_hash.
   296|         """
   297|         def f(txn):
   298|             sql = "SELECT name, password_hash FROM users WHERE lower(name) = lower(?)"
   299|             txn.execute(sql, (user_id,))
   300|             return dict(txn)
   301|         return await self.db_pool.runInteraction("get_users_by_id_case_insensitive", f)
   302|     async def get_user_by_external_id(
   303|         self, auth_provider: str, external_id: str
   304|     ) -> str:
   305|         """Look up a user by their external auth id
   306|         Args:
   307|             auth_provider: identifier for the remote auth provider
   308|             external_id: id on that system
   309|         Returns:
   310|             str|None: the mxid of the user, or None if they are not known
   311|         """
   312|         return await self.db_pool.simple_select_one_onecol(
   313|             table="user_external_ids",
   314|             keyvalues={"auth_provider": auth_provider, "external_id": external_id},
   315|             retcol="user_id",
   316|             allow_none=True,
   317|             desc="get_user_by_external_id",
   318|         )
   319|     async def count_all_users(self):
   320|         """Counts all users registered on the homeserver."""
   321|         def _count_users(txn):
   322|             txn.execute("SELECT COUNT(*) AS users FROM users")
   323|             rows = self.db_pool.cursor_to_dict(txn)
   324|             if rows:
   325|                 return rows[0]["users"]
   326|             return 0
   327|         return await self.db_pool.runInteraction("count_users", _count_users)
   328|     async def count_daily_user_type(self) -> Dict[str, int]:
   329|         """
   330|         Counts 1) native non guest users

# --- HUNK 4: Lines 604-644 ---
   604|         waiting on it has been carried out
   605|         Args:
   606|             session_id: The ID of the session to delete
   607|         """
   608|         def delete_threepid_session_txn(txn):
   609|             self.db_pool.simple_delete_txn(
   610|                 txn,
   611|                 table="threepid_validation_token",
   612|                 keyvalues={"session_id": session_id},
   613|             )
   614|             self.db_pool.simple_delete_txn(
   615|                 txn,
   616|                 table="threepid_validation_session",
   617|                 keyvalues={"session_id": session_id},
   618|             )
   619|         await self.db_pool.runInteraction(
   620|             "delete_threepid_session", delete_threepid_session_txn
   621|         )
   622| class RegistrationBackgroundUpdateStore(RegistrationWorkerStore):
   623|     def __init__(self, database: DatabasePool, db_conn, hs):
   624|         super(RegistrationBackgroundUpdateStore, self).__init__(database, db_conn, hs)
   625|         self.clock = hs.get_clock()
   626|         self.config = hs.config
   627|         self.db_pool.updates.register_background_index_update(
   628|             "access_tokens_device_index",
   629|             index_name="access_tokens_device_id",
   630|             table="access_tokens",
   631|             columns=["user_id", "device_id"],
   632|         )
   633|         self.db_pool.updates.register_background_index_update(
   634|             "users_creation_ts",
   635|             index_name="users_creation_ts",
   636|             table="users",
   637|             columns=["creation_ts"],
   638|         )
   639|         self.db_pool.updates.register_noop_background_update(
   640|             "refresh_tokens_device_index"
   641|         )
   642|         self.db_pool.updates.register_background_update_handler(
   643|             "user_threepids_grandfather", self._bg_user_threepids_grandfather
   644|         )

# --- HUNK 5: Lines 701-741 ---
   701|         We do this by grandfathering in existing user threepids assuming that
   702|         they used one of the server configured trusted identity servers.
   703|         """
   704|         id_servers = set(self.config.trusted_third_party_id_servers)
   705|         def _bg_user_threepids_grandfather_txn(txn):
   706|             sql = """
   707|                 INSERT INTO user_threepid_id_server
   708|                     (user_id, medium, address, id_server)
   709|                 SELECT user_id, medium, address, ?
   710|                 FROM user_threepids
   711|             """
   712|             txn.executemany(sql, [(id_server,) for id_server in id_servers])
   713|         if id_servers:
   714|             await self.db_pool.runInteraction(
   715|                 "_bg_user_threepids_grandfather", _bg_user_threepids_grandfather_txn
   716|             )
   717|         await self.db_pool.updates._end_background_update("user_threepids_grandfather")
   718|         return 1
   719| class RegistrationStore(RegistrationBackgroundUpdateStore):
   720|     def __init__(self, database: DatabasePool, db_conn, hs):
   721|         super(RegistrationStore, self).__init__(database, db_conn, hs)
   722|         self._account_validity = hs.config.account_validity
   723|         self._ignore_unknown_session_error = hs.config.request_token_inhibit_3pid_errors
   724|         if self._account_validity.enabled:
   725|             self._clock.call_later(
   726|                 0.0,
   727|                 run_as_background_process,
   728|                 "account_validity_set_expiration_dates",
   729|                 self._set_expiration_date_when_missing,
   730|             )
   731|         def start_cull():
   732|             return run_as_background_process(
   733|                 "cull_expired_threepid_validation_tokens",
   734|                 self.cull_expired_threepid_validation_tokens,
   735|             )
   736|         hs.get_clock().looping_call(start_cull, THIRTY_MINUTES_IN_MS)
   737|     async def add_access_token_to_user(
   738|         self,
   739|         user_id: str,
   740|         token: str,
   741|         device_id: Optional[str],


# ====================================================================
# FILE: synapse/storage/databases/main/room.py
# Total hunks: 8
# ====================================================================
# --- HUNK 1: Lines 23-92 ---
    23|     NAME = sort rooms alphabetically by name
    24|     JOINED_MEMBERS = sort rooms by membership size, highest to lowest
    25|     """
    26|     ALPHABETICAL = "alphabetical"
    27|     SIZE = "size"
    28|     NAME = "name"
    29|     CANONICAL_ALIAS = "canonical_alias"
    30|     JOINED_MEMBERS = "joined_members"
    31|     JOINED_LOCAL_MEMBERS = "joined_local_members"
    32|     VERSION = "version"
    33|     CREATOR = "creator"
    34|     ENCRYPTION = "encryption"
    35|     FEDERATABLE = "federatable"
    36|     PUBLIC = "public"
    37|     JOIN_RULES = "join_rules"
    38|     GUEST_ACCESS = "guest_access"
    39|     HISTORY_VISIBILITY = "history_visibility"
    40|     STATE_EVENTS = "state_events"
    41| class RoomWorkerStore(SQLBaseStore):
    42|     def __init__(self, database: DatabasePool, db_conn, hs):
    43|         super(RoomWorkerStore, self).__init__(database, db_conn, hs)
    44|         self.config = hs.config
    45|     async def get_room(self, room_id: str) -> dict:
    46|         """Retrieve a room.
    47|         Args:
    48|             room_id: The ID of the room to retrieve.
    49|         Returns:
    50|             A dict containing the room information, or None if the room is unknown.
    51|         """
    52|         return await self.db_pool.simple_select_one(
    53|             table="rooms",
    54|             keyvalues={"room_id": room_id},
    55|             retcols=("room_id", "is_public", "creator"),
    56|             desc="get_room",
    57|             allow_none=True,
    58|         )
    59|     async def get_room_with_stats(self, room_id: str) -> Optional[Dict[str, Any]]:
    60|         """Retrieve room with statistics.
    61|         Args:
    62|             room_id: The ID of the room to retrieve.
    63|         Returns:
    64|             A dict containing the room information, or None if the room is unknown.
    65|         """
    66|         def get_room_with_stats_txn(txn, room_id):
    67|             sql = """
    68|                 SELECT room_id, state.name, state.canonical_alias, curr.joined_members,
    69|                   curr.local_users_in_room AS joined_local_members, rooms.room_version AS version,
    70|                   rooms.creator, state.encryption, state.is_federatable AS federatable,
    71|                   rooms.is_public AS public, state.join_rules, state.guest_access,
    72|                   state.history_visibility, curr.current_state_events AS state_events
    73|                 FROM rooms
    74|                 LEFT JOIN room_stats_state state USING (room_id)
    75|                 LEFT JOIN room_stats_current curr USING (room_id)
    76|                 WHERE room_id = ?
    77|                 """
    78|             txn.execute(sql, [room_id])
    79|             try:
    80|                 res = self.db_pool.cursor_to_dict(txn)[0]
    81|             except IndexError:
    82|                 return None
    83|             res["federatable"] = bool(res["federatable"])
    84|             res["public"] = bool(res["public"])
    85|             return res
    86|         return await self.db_pool.runInteraction(
    87|             "get_room_with_stats", get_room_with_stats_txn, room_id
    88|         )
    89|     async def get_public_room_ids(self) -> List[str]:
    90|         return await self.db_pool.simple_select_onecol(
    91|             table="rooms",
    92|             keyvalues={"is_public": True},

# --- HUNK 2: Lines 662-702 ---
   662|                 FROM public_room_list_stream
   663|                 WHERE stream_id > ? AND stream_id <= ?
   664|                 ORDER BY stream_id ASC
   665|                 LIMIT ?
   666|             """
   667|             txn.execute(sql, (last_id, current_id, limit))
   668|             updates = [(row[0], row[1:]) for row in txn]
   669|             limited = False
   670|             upto_token = current_id
   671|             if len(updates) >= limit:
   672|                 upto_token = updates[-1][0]
   673|                 limited = True
   674|             return updates, upto_token, limited
   675|         return await self.db_pool.runInteraction(
   676|             "get_all_new_public_rooms", get_all_new_public_rooms
   677|         )
   678| class RoomBackgroundUpdateStore(SQLBaseStore):
   679|     REMOVE_TOMESTONED_ROOMS_BG_UPDATE = "remove_tombstoned_rooms_from_directory"
   680|     ADD_ROOMS_ROOM_VERSION_COLUMN = "add_rooms_room_version_column"
   681|     def __init__(self, database: DatabasePool, db_conn, hs):
   682|         super(RoomBackgroundUpdateStore, self).__init__(database, db_conn, hs)
   683|         self.config = hs.config
   684|         self.db_pool.updates.register_background_update_handler(
   685|             "insert_room_retention", self._background_insert_retention,
   686|         )
   687|         self.db_pool.updates.register_background_update_handler(
   688|             self.REMOVE_TOMESTONED_ROOMS_BG_UPDATE,
   689|             self._remove_tombstoned_rooms_from_directory,
   690|         )
   691|         self.db_pool.updates.register_background_update_handler(
   692|             self.ADD_ROOMS_ROOM_VERSION_COLUMN,
   693|             self._background_add_rooms_room_version_column,
   694|         )
   695|     async def _background_insert_retention(self, progress, batch_size):
   696|         """Retrieves a list of all rooms within a range and inserts an entry for each of
   697|         them into the room_retention table.
   698|         NULLs the property's columns if missing from the retention event in the room's
   699|         state (or NULLs all of them if there's no retention event in the room's state),
   700|         so that we fall back to the server's retention policy.
   701|         """
   702|         last_room = progress.get("room_id", "")

# --- HUNK 3: Lines 821-861 ---
   821|         rooms = await self.db_pool.runInteraction(
   822|             "get_tombstoned_directory_rooms", _get_rooms
   823|         )
   824|         if not rooms:
   825|             await self.db_pool.updates._end_background_update(
   826|                 self.REMOVE_TOMESTONED_ROOMS_BG_UPDATE
   827|             )
   828|             return 0
   829|         for room_id in rooms:
   830|             logger.info("Removing tombstoned room %s from the directory", room_id)
   831|             await self.set_room_is_public(room_id, False)
   832|         await self.db_pool.updates._background_update_progress(
   833|             self.REMOVE_TOMESTONED_ROOMS_BG_UPDATE, {"room_id": rooms[-1]}
   834|         )
   835|         return len(rooms)
   836|     @abstractmethod
   837|     def set_room_is_public(self, room_id, is_public):
   838|         raise NotImplementedError()
   839| class RoomStore(RoomBackgroundUpdateStore, RoomWorkerStore, SearchStore):
   840|     def __init__(self, database: DatabasePool, db_conn, hs):
   841|         super(RoomStore, self).__init__(database, db_conn, hs)
   842|         self.config = hs.config
   843|     async def upsert_room_on_join(self, room_id: str, room_version: RoomVersion):
   844|         """Ensure that the room is stored in the table
   845|         Called when we join a room over federation, and overwrites any room version
   846|         currently in the table.
   847|         """
   848|         await self.db_pool.simple_upsert(
   849|             desc="upsert_room_on_join",
   850|             table="rooms",
   851|             keyvalues={"room_id": room_id},
   852|             values={"room_version": room_version.identifier},
   853|             insertion_values={"is_public": False, "creator": ""},
   854|             lock=False,
   855|         )
   856|     async def store_room(
   857|         self,
   858|         room_id: str,
   859|         room_creator_user_id: str,
   860|         is_public: bool,
   861|         room_version: RoomVersion,

# --- HUNK 4: Lines 875-915 ---
   875|                 self.db_pool.simple_insert_txn(
   876|                     txn,
   877|                     "rooms",
   878|                     {
   879|                         "room_id": room_id,
   880|                         "creator": room_creator_user_id,
   881|                         "is_public": is_public,
   882|                         "room_version": room_version.identifier,
   883|                     },
   884|                 )
   885|                 if is_public:
   886|                     self.db_pool.simple_insert_txn(
   887|                         txn,
   888|                         table="public_room_list_stream",
   889|                         values={
   890|                             "stream_id": next_id,
   891|                             "room_id": room_id,
   892|                             "visibility": is_public,
   893|                         },
   894|                     )
   895|             with await self._public_room_id_gen.get_next() as next_id:
   896|                 await self.db_pool.runInteraction(
   897|                     "store_room_txn", store_room_txn, next_id
   898|                 )
   899|         except Exception as e:
   900|             logger.error("store_room with room_id=%s failed: %s", room_id, e)
   901|             raise StoreError(500, "Problem creating room.")
   902|     async def maybe_store_room_on_invite(self, room_id: str, room_version: RoomVersion):
   903|         """
   904|         When we receive an invite over federation, store the version of the room if we
   905|         don't already know the room version.
   906|         """
   907|         await self.db_pool.simple_upsert(
   908|             desc="maybe_store_room_on_invite",
   909|             table="rooms",
   910|             keyvalues={"room_id": room_id},
   911|             values={},
   912|             insertion_values={
   913|                 "room_version": room_version.identifier,
   914|                 "is_public": False,
   915|                 "creator": "",

# --- HUNK 5: Lines 933-973 ---
   933|                     "network_id": None,
   934|                 },
   935|                 retcols=("stream_id", "visibility"),
   936|             )
   937|             entries.sort(key=lambda r: r["stream_id"])
   938|             add_to_stream = True
   939|             if entries:
   940|                 add_to_stream = bool(entries[-1]["visibility"]) != is_public
   941|             if add_to_stream:
   942|                 self.db_pool.simple_insert_txn(
   943|                     txn,
   944|                     table="public_room_list_stream",
   945|                     values={
   946|                         "stream_id": next_id,
   947|                         "room_id": room_id,
   948|                         "visibility": is_public,
   949|                         "appservice_id": None,
   950|                         "network_id": None,
   951|                     },
   952|                 )
   953|         with await self._public_room_id_gen.get_next() as next_id:
   954|             await self.db_pool.runInteraction(
   955|                 "set_room_is_public", set_room_is_public_txn, next_id
   956|             )
   957|         self.hs.get_notifier().on_new_replication_data()
   958|     async def set_room_is_public_appservice(
   959|         self, room_id, appservice_id, network_id, is_public
   960|     ):
   961|         """Edit the appservice/network specific public room list.
   962|         Each appservice can have a number of published room lists associated
   963|         with them, keyed off of an appservice defined `network_id`, which
   964|         basically represents a single instance of a bridge to a third party
   965|         network.
   966|         Args:
   967|             room_id (str)
   968|             appservice_id (str)
   969|             network_id (str)
   970|             is_public (bool): Whether to publish or unpublish the room from the
   971|                 list.
   972|         """
   973|         def set_room_is_public_appservice_txn(txn, next_id):

# --- HUNK 6: Lines 1003-1080 ---
  1003|                     "network_id": network_id,
  1004|                 },
  1005|                 retcols=("stream_id", "visibility"),
  1006|             )
  1007|             entries.sort(key=lambda r: r["stream_id"])
  1008|             add_to_stream = True
  1009|             if entries:
  1010|                 add_to_stream = bool(entries[-1]["visibility"]) != is_public
  1011|             if add_to_stream:
  1012|                 self.db_pool.simple_insert_txn(
  1013|                     txn,
  1014|                     table="public_room_list_stream",
  1015|                     values={
  1016|                         "stream_id": next_id,
  1017|                         "room_id": room_id,
  1018|                         "visibility": is_public,
  1019|                         "appservice_id": appservice_id,
  1020|                         "network_id": network_id,
  1021|                     },
  1022|                 )
  1023|         with await self._public_room_id_gen.get_next() as next_id:
  1024|             await self.db_pool.runInteraction(
  1025|                 "set_room_is_public_appservice",
  1026|                 set_room_is_public_appservice_txn,
  1027|                 next_id,
  1028|             )
  1029|         self.hs.get_notifier().on_new_replication_data()
  1030|     async def get_room_count(self) -> int:
  1031|         """Retrieve the total number of rooms.
  1032|         """
  1033|         def f(txn):
  1034|             sql = "SELECT count(*)  FROM rooms"
  1035|             txn.execute(sql)
  1036|             row = txn.fetchone()
  1037|             return row[0] or 0
  1038|         return await self.db_pool.runInteraction("get_rooms", f)
  1039|     async def add_event_report(
  1040|         self,
  1041|         room_id: str,
  1042|         event_id: str,
  1043|         user_id: str,
  1044|         reason: str,
  1045|         content: JsonDict,
  1046|         received_ts: int,
  1047|     ) -> None:
  1048|         next_id = self._event_reports_id_gen.get_next()
  1049|         await self.db_pool.simple_insert(
  1050|             table="event_reports",
  1051|             values={
  1052|                 "id": next_id,
  1053|                 "received_ts": received_ts,
  1054|                 "room_id": room_id,
  1055|                 "event_id": event_id,
  1056|                 "user_id": user_id,
  1057|                 "reason": reason,
  1058|                 "content": json_encoder.encode(content),
  1059|             },
  1060|             desc="add_event_report",
  1061|         )
  1062|     def get_current_public_room_stream_id(self):
  1063|         return self._public_room_id_gen.get_current_token()
  1064|     async def block_room(self, room_id: str, user_id: str) -> None:
  1065|         """Marks the room as blocked. Can be called multiple times.
  1066|         Args:
  1067|             room_id: Room to block
  1068|             user_id: Who blocked it
  1069|         """
  1070|         await self.db_pool.simple_upsert(
  1071|             table="blocked_rooms",
  1072|             keyvalues={"room_id": room_id},
  1073|             values={},
  1074|             insertion_values={"user_id": user_id},
  1075|             desc="block_room",
  1076|         )
  1077|         await self.db_pool.runInteraction(
  1078|             "block_room_invalidation",
  1079|             self._invalidate_cache_and_stream,
  1080|             self.is_room_blocked,


# ====================================================================
# FILE: synapse/storage/databases/main/roommember.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 3-55 ---
     3| from synapse.api.constants import EventTypes, Membership
     4| from synapse.events import EventBase
     5| from synapse.events.snapshot import EventContext
     6| from synapse.metrics import LaterGauge
     7| from synapse.metrics.background_process_metrics import run_as_background_process
     8| from synapse.storage._base import (
     9|     LoggingTransaction,
    10|     SQLBaseStore,
    11|     db_to_json,
    12|     make_in_list_sql_clause,
    13| )
    14| from synapse.storage.database import DatabasePool
    15| from synapse.storage.databases.main.events_worker import EventsWorkerStore
    16| from synapse.storage.engines import Sqlite3Engine
    17| from synapse.storage.roommember import (
    18|     GetRoomsForUserWithStreamOrdering,
    19|     MemberSummary,
    20|     ProfileInfo,
    21|     RoomsForUser,
    22| )
    23| from synapse.types import Collection, get_domain_from_id
    24| from synapse.util.async_helpers import Linearizer
    25| from synapse.util.caches import intern_string
    26| from synapse.util.caches.descriptors import _CacheContext, cached, cachedList
    27| from synapse.util.metrics import Measure
    28| if TYPE_CHECKING:
    29|     from synapse.state import _StateCacheEntry
    30| logger = logging.getLogger(__name__)
    31| _MEMBERSHIP_PROFILE_UPDATE_NAME = "room_membership_profile_update"
    32| _CURRENT_STATE_MEMBERSHIP_UPDATE_NAME = "current_state_events_membership"
    33| class RoomMemberWorkerStore(EventsWorkerStore):
    34|     def __init__(self, database: DatabasePool, db_conn, hs):
    35|         super(RoomMemberWorkerStore, self).__init__(database, db_conn, hs)
    36|         self._current_state_events_membership_up_to_date = False
    37|         txn = LoggingTransaction(
    38|             db_conn.cursor(),
    39|             name="_check_safe_current_state_events_membership_updated",
    40|             database_engine=self.database_engine,
    41|         )
    42|         self._check_safe_current_state_events_membership_updated_txn(txn)
    43|         txn.close()
    44|         if self.hs.config.metrics_flags.known_servers:
    45|             self._known_servers_count = 1
    46|             self.hs.get_clock().looping_call(
    47|                 run_as_background_process,
    48|                 60 * 1000,
    49|                 "_count_known_servers",
    50|                 self._count_known_servers,
    51|             )
    52|             self.hs.get_clock().call_later(
    53|                 1000,
    54|                 run_as_background_process,
    55|                 "_count_known_servers",

# --- HUNK 2: Lines 273-333 ---
   273|     ) -> FrozenSet[GetRoomsForUserWithStreamOrdering]:
   274|         """Returns a set of room_ids the user is currently joined to.
   275|         If a remote user only returns rooms this server is currently
   276|         participating in.
   277|         Args:
   278|             user_id
   279|         Returns:
   280|             Returns the rooms the user is in currently, along with the stream
   281|             ordering of the most recent join for that user and room.
   282|         """
   283|         return await self.db_pool.runInteraction(
   284|             "get_rooms_for_user_with_stream_ordering",
   285|             self._get_rooms_for_user_with_stream_ordering_txn,
   286|             user_id,
   287|         )
   288|     def _get_rooms_for_user_with_stream_ordering_txn(
   289|         self, txn, user_id: str
   290|     ) -> FrozenSet[GetRoomsForUserWithStreamOrdering]:
   291|         if self._current_state_events_membership_up_to_date:
   292|             sql = """
   293|                 SELECT room_id, e.stream_ordering
   294|                 FROM current_state_events AS c
   295|                 INNER JOIN events AS e USING (room_id, event_id)
   296|                 WHERE
   297|                     c.type = 'm.room.member'
   298|                     AND state_key = ?
   299|                     AND c.membership = ?
   300|             """
   301|         else:
   302|             sql = """
   303|                 SELECT room_id, e.stream_ordering
   304|                 FROM current_state_events AS c
   305|                 INNER JOIN room_memberships AS m USING (room_id, event_id)
   306|                 INNER JOIN events AS e USING (room_id, event_id)
   307|                 WHERE
   308|                     c.type = 'm.room.member'
   309|                     AND state_key = ?
   310|                     AND m.membership = ?
   311|             """
   312|         txn.execute(sql, (user_id, Membership.JOIN))
   313|         return frozenset(GetRoomsForUserWithStreamOrdering(*row) for row in txn)
   314|     async def get_users_server_still_shares_room_with(
   315|         self, user_ids: Collection[str]
   316|     ) -> Set[str]:
   317|         """Given a list of users return the set that the server still share a
   318|         room with.
   319|         """
   320|         if not user_ids:
   321|             return set()
   322|         def _get_users_server_still_shares_room_with_txn(txn):
   323|             sql = """
   324|                 SELECT state_key FROM current_state_events
   325|                 WHERE
   326|                     type = 'm.room.member'
   327|                     AND membership = 'join'
   328|                     AND %s
   329|                 GROUP BY state_key
   330|             """
   331|             clause, args = make_in_list_sql_clause(
   332|                 self.database_engine, "state_key", user_ids
   333|             )

# --- HUNK 3: Lines 596-636 ---
   596|             self.database_engine, "user_id", ignore_users
   597|         )
   598|         sql = """
   599|             SELECT 1 FROM local_current_membership
   600|             WHERE
   601|                 room_id = ? AND membership = ?
   602|                 AND NOT (%s)
   603|                 LIMIT 1
   604|         """ % (
   605|             clause,
   606|         )
   607|         def _is_local_host_in_room_ignoring_users_txn(txn):
   608|             txn.execute(sql, (room_id, Membership.JOIN, *args))
   609|             return bool(txn.fetchone())
   610|         return await self.db_pool.runInteraction(
   611|             "is_local_host_in_room_ignoring_users",
   612|             _is_local_host_in_room_ignoring_users_txn,
   613|         )
   614| class RoomMemberBackgroundUpdateStore(SQLBaseStore):
   615|     def __init__(self, database: DatabasePool, db_conn, hs):
   616|         super(RoomMemberBackgroundUpdateStore, self).__init__(database, db_conn, hs)
   617|         self.db_pool.updates.register_background_update_handler(
   618|             _MEMBERSHIP_PROFILE_UPDATE_NAME, self._background_add_membership_profile
   619|         )
   620|         self.db_pool.updates.register_background_update_handler(
   621|             _CURRENT_STATE_MEMBERSHIP_UPDATE_NAME,
   622|             self._background_current_state_membership,
   623|         )
   624|         self.db_pool.updates.register_background_index_update(
   625|             "room_membership_forgotten_idx",
   626|             index_name="room_memberships_user_room_forgotten",
   627|             table="room_memberships",
   628|             columns=["user_id", "room_id"],
   629|             where_clause="forgotten = 1",
   630|         )
   631|     async def _background_add_membership_profile(self, progress, batch_size):
   632|         target_min_stream_id = progress.get(
   633|             "target_min_stream_id_inclusive", self._min_stream_order_on_start
   634|         )
   635|         max_stream_id = progress.get(
   636|             "max_stream_id_exclusive", self._stream_order_on_start + 1

# --- HUNK 4: Lines 718-758 ---
   718|                 last_processed_room = next_room
   719|             self.db_pool.updates._background_update_progress_txn(
   720|                 txn,
   721|                 _CURRENT_STATE_MEMBERSHIP_UPDATE_NAME,
   722|                 {"last_processed_room": last_processed_room},
   723|             )
   724|             return processed, False
   725|         last_processed_room = progress.get("last_processed_room", "")
   726|         row_count, finished = await self.db_pool.runInteraction(
   727|             "_background_current_state_membership_update",
   728|             _background_current_state_membership_txn,
   729|             last_processed_room,
   730|         )
   731|         if finished:
   732|             await self.db_pool.updates._end_background_update(
   733|                 _CURRENT_STATE_MEMBERSHIP_UPDATE_NAME
   734|             )
   735|         return row_count
   736| class RoomMemberStore(RoomMemberWorkerStore, RoomMemberBackgroundUpdateStore):
   737|     def __init__(self, database: DatabasePool, db_conn, hs):
   738|         super(RoomMemberStore, self).__init__(database, db_conn, hs)
   739|     async def forget(self, user_id: str, room_id: str) -> None:
   740|         """Indicate that user_id wishes to discard history for room_id."""
   741|         def f(txn):
   742|             sql = (
   743|                 "UPDATE"
   744|                 "  room_memberships"
   745|                 " SET"
   746|                 "  forgotten = 1"
   747|                 " WHERE"
   748|                 "  user_id = ?"
   749|                 " AND"
   750|                 "  room_id = ?"
   751|             )
   752|             txn.execute(sql, (user_id, room_id))
   753|             self._invalidate_cache_and_stream(txn, self.did_forget, (user_id, room_id))
   754|             self._invalidate_cache_and_stream(
   755|                 txn, self.get_forgotten_rooms_for_user, (user_id,)
   756|             )
   757|         await self.db_pool.runInteraction("forget_membership", f)
   758| class _JoinedHostsCache:


# ====================================================================
# FILE: synapse/storage/databases/main/search.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 42-82 ---
    42|             )
    43|             txn.executemany(sql, args)
    44|         elif isinstance(self.database_engine, Sqlite3Engine):
    45|             sql = (
    46|                 "INSERT INTO event_search (event_id, room_id, key, value)"
    47|                 " VALUES (?,?,?,?)"
    48|             )
    49|             args = (
    50|                 (entry.event_id, entry.room_id, entry.key, entry.value)
    51|                 for entry in entries
    52|             )
    53|             txn.executemany(sql, args)
    54|         else:
    55|             raise Exception("Unrecognized database engine")
    56| class SearchBackgroundUpdateStore(SearchWorkerStore):
    57|     EVENT_SEARCH_UPDATE_NAME = "event_search"
    58|     EVENT_SEARCH_ORDER_UPDATE_NAME = "event_search_order"
    59|     EVENT_SEARCH_USE_GIST_POSTGRES_NAME = "event_search_postgres_gist"
    60|     EVENT_SEARCH_USE_GIN_POSTGRES_NAME = "event_search_postgres_gin"
    61|     def __init__(self, database: DatabasePool, db_conn, hs):
    62|         super(SearchBackgroundUpdateStore, self).__init__(database, db_conn, hs)
    63|         if not hs.config.enable_search:
    64|             return
    65|         self.db_pool.updates.register_background_update_handler(
    66|             self.EVENT_SEARCH_UPDATE_NAME, self._background_reindex_search
    67|         )
    68|         self.db_pool.updates.register_background_update_handler(
    69|             self.EVENT_SEARCH_ORDER_UPDATE_NAME, self._background_reindex_search_order
    70|         )
    71|         self.db_pool.updates.register_noop_background_update(
    72|             self.EVENT_SEARCH_USE_GIST_POSTGRES_NAME
    73|         )
    74|         self.db_pool.updates.register_background_update_handler(
    75|             self.EVENT_SEARCH_USE_GIN_POSTGRES_NAME, self._background_reindex_gin_search
    76|         )
    77|     async def _background_reindex_search(self, progress, batch_size):
    78|         target_min_stream_id = progress["target_min_stream_id_inclusive"]
    79|         max_stream_id = progress["max_stream_id_exclusive"]
    80|         rows_inserted = progress.get("rows_inserted", 0)
    81|         TYPES = ["m.room.name", "m.room.message", "m.room.topic"]
    82|         def reindex_search_txn(txn):

# --- HUNK 2: Lines 223-263 ---
   223|             progress = {
   224|                 "target_min_stream_id_inclusive": target_min_stream_id,
   225|                 "max_stream_id_exclusive": min_stream_id,
   226|                 "rows_inserted": rows_inserted + len(rows),
   227|                 "have_added_indexes": True,
   228|             }
   229|             self.db_pool.updates._background_update_progress_txn(
   230|                 txn, self.EVENT_SEARCH_ORDER_UPDATE_NAME, progress
   231|             )
   232|             return len(rows), True
   233|         num_rows, finished = await self.db_pool.runInteraction(
   234|             self.EVENT_SEARCH_ORDER_UPDATE_NAME, reindex_search_txn
   235|         )
   236|         if not finished:
   237|             await self.db_pool.updates._end_background_update(
   238|                 self.EVENT_SEARCH_ORDER_UPDATE_NAME
   239|             )
   240|         return num_rows
   241| class SearchStore(SearchBackgroundUpdateStore):
   242|     def __init__(self, database: DatabasePool, db_conn, hs):
   243|         super(SearchStore, self).__init__(database, db_conn, hs)
   244|     async def search_msgs(self, room_ids, search_term, keys):
   245|         """Performs a full text search over events with given keys.
   246|         Args:
   247|             room_ids (list): List of room ids to search in
   248|             search_term (str): Search term to search for
   249|             keys (list): List of keys to search in, currently supports
   250|                 "content.body", "content.name", "content.topic"
   251|         Returns:
   252|             list of dicts
   253|         """
   254|         clauses = []
   255|         search_query = _parse_query(self.database_engine, search_term)
   256|         args = []
   257|         if len(room_ids) < 500:
   258|             clause, args = make_in_list_sql_clause(
   259|                 self.database_engine, "room_id", room_ids
   260|             )
   261|             clauses = [clause]
   262|         local_clauses = []
   263|         for key in keys:


# ====================================================================
# FILE: synapse/storage/databases/main/state.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 12-52 ---
    12| from synapse.storage.databases.main.roommember import RoomMemberWorkerStore
    13| from synapse.storage.state import StateFilter
    14| from synapse.types import StateMap
    15| from synapse.util.caches import intern_string
    16| from synapse.util.caches.descriptors import cached, cachedList
    17| logger = logging.getLogger(__name__)
    18| MAX_STATE_DELTA_HOPS = 100
    19| class _GetStateGroupDelta(
    20|     namedtuple("_GetStateGroupDelta", ("prev_group", "delta_ids"))
    21| ):
    22|     """Return type of get_state_group_delta that implements __len__, which lets
    23|     us use the itrable flag when caching
    24|     """
    25|     __slots__ = []
    26|     def __len__(self):
    27|         return len(self.delta_ids) if self.delta_ids else 0
    28| class StateGroupWorkerStore(EventsWorkerStore, SQLBaseStore):
    29|     """The parts of StateGroupStore that can be called from workers.
    30|     """
    31|     def __init__(self, database: DatabasePool, db_conn, hs):
    32|         super(StateGroupWorkerStore, self).__init__(database, db_conn, hs)
    33|     async def get_room_version(self, room_id: str) -> RoomVersion:
    34|         """Get the room_version of a given room
    35|         Raises:
    36|             NotFoundError: if the room is unknown
    37|             UnsupportedRoomVersionError: if the room uses an unknown room version.
    38|                 Typically this happens if support for the room's version has been
    39|                 removed from Synapse.
    40|         """
    41|         room_version_id = await self.get_room_version_id(room_id)
    42|         v = KNOWN_ROOM_VERSIONS.get(room_version_id)
    43|         if not v:
    44|             raise UnsupportedRoomVersionError(
    45|                 "Room %s uses a room version %s which is no longer supported"
    46|                 % (room_id, room_version_id)
    47|             )
    48|         return v
    49|     @cached(max_entries=10000)
    50|     async def get_room_version_id(self, room_id: str) -> str:
    51|         """Get the room_version of a given room
    52|         Raises:

# --- HUNK 2: Lines 202-242 ---
   202|         """Check if the state groups are referenced by events.
   203|         Args:
   204|             state_groups
   205|         Returns:
   206|             The subset of state groups that are referenced.
   207|         """
   208|         rows = await self.db_pool.simple_select_many_batch(
   209|             table="event_to_state_groups",
   210|             column="state_group",
   211|             iterable=state_groups,
   212|             keyvalues={},
   213|             retcols=("DISTINCT state_group",),
   214|             desc="get_referenced_state_groups",
   215|         )
   216|         return {row["state_group"] for row in rows}
   217| class MainStateBackgroundUpdateStore(RoomMemberWorkerStore):
   218|     CURRENT_STATE_INDEX_UPDATE_NAME = "current_state_members_idx"
   219|     EVENT_STATE_GROUP_INDEX_UPDATE_NAME = "event_to_state_groups_sg_index"
   220|     DELETE_CURRENT_STATE_UPDATE_NAME = "delete_old_current_state_events"
   221|     def __init__(self, database: DatabasePool, db_conn, hs):
   222|         super(MainStateBackgroundUpdateStore, self).__init__(database, db_conn, hs)
   223|         self.server_name = hs.hostname
   224|         self.db_pool.updates.register_background_index_update(
   225|             self.CURRENT_STATE_INDEX_UPDATE_NAME,
   226|             index_name="current_state_events_member_index",
   227|             table="current_state_events",
   228|             columns=["state_key"],
   229|             where_clause="type='m.room.member'",
   230|         )
   231|         self.db_pool.updates.register_background_index_update(
   232|             self.EVENT_STATE_GROUP_INDEX_UPDATE_NAME,
   233|             index_name="event_to_state_groups_sg_index",
   234|             table="event_to_state_groups",
   235|             columns=["state_group"],
   236|         )
   237|         self.db_pool.updates.register_background_update_handler(
   238|             self.DELETE_CURRENT_STATE_UPDATE_NAME, self._background_remove_left_rooms,
   239|         )
   240|     async def _background_remove_left_rooms(self, progress, batch_size):
   241|         """Background update to delete rows from `current_state_events` and
   242|         `event_forward_extremities` tables of rooms that the server is no

# --- HUNK 3: Lines 320-340 ---
   320|         )
   321|         for user_id in potentially_left_users - joined_users:
   322|             await self.mark_remote_user_device_list_as_unsubscribed(user_id)
   323|         return batch_size
   324| class StateStore(StateGroupWorkerStore, MainStateBackgroundUpdateStore):
   325|     """ Keeps track of the state at a given event.
   326|     This is done by the concept of `state groups`. Every event is a assigned
   327|     a state group (identified by an arbitrary string), which references a
   328|     collection of state events. The current state of an event is then the
   329|     collection of state events referenced by the event's state group.
   330|     Hence, every change in the current state causes a new state group to be
   331|     generated. However, if no change happens (e.g., if we get a message event
   332|     with only one parent it inherits the state group from its parent.)
   333|     There are three tables:
   334|       * `state_groups`: Stores group name, first event with in the group and
   335|         room id.
   336|       * `event_to_state_groups`: Maps events to state groups.
   337|       * `state_groups_state`: Maps state group to state events.
   338|     """
   339|     def __init__(self, database: DatabasePool, db_conn, hs):
   340|         super(StateStore, self).__init__(database, db_conn, hs)


# ====================================================================
# FILE: synapse/storage/databases/main/stats.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 11-61 ---
    11| logger = logging.getLogger(__name__)
    12| ABSOLUTE_STATS_FIELDS = {
    13|     "room": (
    14|         "current_state_events",
    15|         "joined_members",
    16|         "invited_members",
    17|         "left_members",
    18|         "banned_members",
    19|         "local_users_in_room",
    20|     ),
    21|     "user": ("joined_rooms",),
    22| }
    23| PER_SLICE_FIELDS = {
    24|     "room": ("total_events", "total_event_bytes"),
    25|     "user": ("invites_sent", "rooms_created", "total_events", "total_event_bytes"),
    26| }
    27| TYPE_TO_TABLE = {"room": ("room_stats", "room_id"), "user": ("user_stats", "user_id")}
    28| TYPE_TO_ORIGIN_TABLE = {"room": ("rooms", "room_id"), "user": ("users", "name")}
    29| class StatsStore(StateDeltasStore):
    30|     def __init__(self, database: DatabasePool, db_conn, hs):
    31|         super(StatsStore, self).__init__(database, db_conn, hs)
    32|         self.server_name = hs.hostname
    33|         self.clock = self.hs.get_clock()
    34|         self.stats_enabled = hs.config.stats_enabled
    35|         self.stats_bucket_size = hs.config.stats_bucket_size
    36|         self.stats_delta_processing_lock = DeferredLock()
    37|         self.db_pool.updates.register_background_update_handler(
    38|             "populate_stats_process_rooms", self._populate_stats_process_rooms
    39|         )
    40|         self.db_pool.updates.register_background_update_handler(
    41|             "populate_stats_process_rooms_2", self._populate_stats_process_rooms_2
    42|         )
    43|         self.db_pool.updates.register_background_update_handler(
    44|             "populate_stats_process_users", self._populate_stats_process_users
    45|         )
    46|         self.db_pool.updates.register_noop_background_update("populate_stats_cleanup")
    47|         self.db_pool.updates.register_noop_background_update("populate_stats_prepare")
    48|     def quantise_stats_time(self, ts):
    49|         """
    50|         Quantises a timestamp to be a multiple of the bucket size.
    51|         Args:
    52|             ts (int): the timestamp to quantise, in milliseconds since the Unix
    53|                 Epoch
    54|         Returns:
    55|             int: a timestamp which
    56|               - is divisible by the bucket size;
    57|               - is no later than `ts`; and
    58|               - is the largest such timestamp.
    59|         """
    60|         return (ts // self.stats_bucket_size) * self.stats_bucket_size
    61|     async def _populate_stats_process_users(self, progress, batch_size):

# --- HUNK 2: Lines 79-204 ---
    79|             return [r for r, in txn]
    80|         users_to_work_on = await self.db_pool.runInteraction(
    81|             "_populate_stats_process_users", _get_next_batch
    82|         )
    83|         if not users_to_work_on:
    84|             await self.db_pool.updates._end_background_update(
    85|                 "populate_stats_process_users"
    86|             )
    87|             return 1
    88|         for user_id in users_to_work_on:
    89|             await self._calculate_and_set_initial_state_for_user(user_id)
    90|             progress["last_user_id"] = user_id
    91|         await self.db_pool.runInteraction(
    92|             "populate_stats_process_users",
    93|             self.db_pool.updates._background_update_progress_txn,
    94|             "populate_stats_process_users",
    95|             progress,
    96|         )
    97|         return len(users_to_work_on)
    98|     async def _populate_stats_process_rooms(self, progress, batch_size):
    99|         """
   100|         This was a background update which regenerated statistics for rooms.
   101|         It has been replaced by StatsStore._populate_stats_process_rooms_2. This background
   102|         job has been scheduled to run as part of Synapse v1.0.0, and again now. To ensure
   103|         someone upgrading from <v1.0.0, this background task has been turned into a no-op
   104|         so that the potentially expensive task is not run twice.
   105|         Further context: https://github.com/matrix-org/synapse/pull/7977
   106|         """
   107|         await self.db_pool.updates._end_background_update(
   108|             "populate_stats_process_rooms"
   109|         )
   110|         return 1
   111|     async def _populate_stats_process_rooms_2(self, progress, batch_size):
   112|         """
   113|         This is a background update which regenerates statistics for rooms.
   114|         It replaces StatsStore._populate_stats_process_rooms. See its docstring for the
   115|         reasoning.
   116|         """
   117|         if not self.stats_enabled:
   118|             await self.db_pool.updates._end_background_update(
   119|                 "populate_stats_process_rooms_2"
   120|             )
   121|             return 1
   122|         last_room_id = progress.get("last_room_id", "")
   123|         def _get_next_batch(txn):
   124|             sql = """
   125|                     SELECT DISTINCT room_id FROM current_state_events
   126|                     WHERE room_id > ?
   127|                     ORDER BY room_id ASC
   128|                     LIMIT ?
   129|                 """
   130|             txn.execute(sql, (last_room_id, batch_size))
   131|             return [r for r, in txn]
   132|         rooms_to_work_on = await self.db_pool.runInteraction(
   133|             "populate_stats_rooms_2_get_batch", _get_next_batch
   134|         )
   135|         if not rooms_to_work_on:
   136|             await self.db_pool.updates._end_background_update(
   137|                 "populate_stats_process_rooms_2"
   138|             )
   139|             return 1
   140|         for room_id in rooms_to_work_on:
   141|             await self._calculate_and_set_initial_state_for_room(room_id)
   142|             progress["last_room_id"] = room_id
   143|         await self.db_pool.runInteraction(
   144|             "_populate_stats_process_rooms_2",
   145|             self.db_pool.updates._background_update_progress_txn,
   146|             "populate_stats_process_rooms_2",
   147|             progress,
   148|         )
   149|         return len(rooms_to_work_on)
   150|     async def get_stats_positions(self) -> int:
   151|         """
   152|         Returns the stats processor positions.
   153|         """
   154|         return await self.db_pool.simple_select_one_onecol(
   155|             table="stats_incremental_position",
   156|             keyvalues={},
   157|             retcol="stream_id",
   158|             desc="stats_incremental_position",
   159|         )
   160|     async def update_room_state(self, room_id: str, fields: Dict[str, Any]) -> None:
   161|         """Update the state of a room.
   162|         fields can contain the following keys with string values:
   163|         * join_rules
   164|         * history_visibility
   165|         * encryption
   166|         * name
   167|         * topic
   168|         * avatar
   169|         * canonical_alias
   170|         A is_federatable key can also be included with a boolean value.
   171|         Args:
   172|             room_id: The room ID to update the state of.
   173|             fields: The fields to update. This can include a partial list of the
   174|                 above fields to only update some room information.
   175|         """
   176|         sentinel = object()
   177|         for col in (
   178|             "join_rules",
   179|             "history_visibility",
   180|             "encryption",
   181|             "name",
   182|             "topic",
   183|             "avatar",
   184|             "canonical_alias",
   185|         ):
   186|             field = fields.get(col, sentinel)
   187|             if field is not sentinel and (not isinstance(field, str) or "\0" in field):
   188|                 fields[col] = None
   189|         await self.db_pool.simple_upsert(
   190|             table="room_stats_state",
   191|             keyvalues={"room_id": room_id},
   192|             values=fields,
   193|             desc="update_room_state",
   194|         )
   195|     async def get_statistics_for_subject(
   196|         self, stats_type: str, stats_id: str, start: str, size: int = 100
   197|     ) -> List[dict]:
   198|         """
   199|         Get statistics for a given subject.
   200|         Args:
   201|             stats_type: The type of subject
   202|             stats_id: The ID of the subject (e.g. room_id or user_id)
   203|             start: Pagination start. Number of entries, not timestamp.
   204|             size: How many entries to return.


# ====================================================================
# FILE: synapse/storage/databases/main/stream.py
# Total hunks: 11
# ====================================================================
# --- HUNK 1: Lines 13-68 ---
    13|     - stream tokens are of the form: "s%d", which maps directly to the column
    14|     - topological tokems: "t%d-%d", where the integers map to the topological
    15|       and stream ordering columns respectively.
    16| """
    17| import abc
    18| import logging
    19| from collections import namedtuple
    20| from typing import TYPE_CHECKING, Dict, List, Optional, Set, Tuple
    21| from twisted.internet import defer
    22| from synapse.api.filtering import Filter
    23| from synapse.events import EventBase
    24| from synapse.logging.context import make_deferred_yieldable, run_in_background
    25| from synapse.storage._base import SQLBaseStore
    26| from synapse.storage.database import (
    27|     DatabasePool,
    28|     LoggingTransaction,
    29|     make_in_list_sql_clause,
    30| )
    31| from synapse.storage.databases.main.events_worker import EventsWorkerStore
    32| from synapse.storage.engines import BaseDatabaseEngine, PostgresEngine
    33| from synapse.types import Collection, RoomStreamToken
    34| from synapse.util.caches.stream_change_cache import StreamChangeCache
    35| if TYPE_CHECKING:
    36|     from synapse.server import HomeServer
    37| logger = logging.getLogger(__name__)
    38| MAX_STREAM_SIZE = 1000
    39| _STREAM_TOKEN = "stream"
    40| _TOPOLOGICAL_TOKEN = "topological"
    41| _EventDictReturn = namedtuple(
    42|     "_EventDictReturn", ("event_id", "topological_ordering", "stream_ordering")
    43| )
    44| def generate_pagination_where_clause(
    45|     direction: str,
    46|     column_names: Tuple[str, str],
    47|     from_token: Optional[Tuple[int, int]],
    48|     to_token: Optional[Tuple[int, int]],
    49|     engine: BaseDatabaseEngine,
    50| ) -> str:
    51|     """Creates an SQL expression to bound the columns by the pagination
    52|     tokens.
    53|     For example creates an SQL expression like:
    54|         (6, 7) >= (topological_ordering, stream_ordering)
    55|         AND (5, 3) < (topological_ordering, stream_ordering)
    56|     would be generated for dir=b, from_token=(6, 7) and to_token=(5, 3).
    57|     Note that tokens are considered to be after the row they are in, e.g. if
    58|     a row A has a token T, then we consider A to be before T. This convention
    59|     is important when figuring out inequalities for the generated SQL, and
    60|     produces the following result:
    61|         - If paginating forwards then we exclude any rows matching the from
    62|           token, but include those that match the to token.
    63|         - If paginating backwards then we include any rows matching the from
    64|           token, but include those that match the to token.
    65|     Args:
    66|         direction: Whether we're paginating backwards("b") or forwards ("f").
    67|         column_names: The column names to bound. Must *not* be user defined as
    68|             these get inserted directly into the SQL statement without escapes.

# --- HUNK 2: Lines 155-412 ---
   155|         args.append(typ)
   156|     if event_filter.senders:
   157|         clauses.append("(%s)" % " OR ".join("sender = ?" for _ in event_filter.senders))
   158|         args.extend(event_filter.senders)
   159|     for sender in event_filter.not_senders:
   160|         clauses.append("sender != ?")
   161|         args.append(sender)
   162|     if event_filter.rooms:
   163|         clauses.append("(%s)" % " OR ".join("room_id = ?" for _ in event_filter.rooms))
   164|         args.extend(event_filter.rooms)
   165|     for room_id in event_filter.not_rooms:
   166|         clauses.append("room_id != ?")
   167|         args.append(room_id)
   168|     if event_filter.contains_url:
   169|         clauses.append("contains_url = ?")
   170|         args.append(event_filter.contains_url)
   171|     if event_filter.labels:
   172|         clauses.append("(%s)" % " OR ".join("label = ?" for _ in event_filter.labels))
   173|         args.extend(event_filter.labels)
   174|     return " AND ".join(clauses), args
   175| class StreamWorkerStore(EventsWorkerStore, SQLBaseStore):
   176|     """This is an abstract base class where subclasses must implement
   177|     `get_room_max_stream_ordering` and `get_room_min_stream_ordering`
   178|     which can be called in the initializer.
   179|     """
   180|     __metaclass__ = abc.ABCMeta
   181|     def __init__(self, database: DatabasePool, db_conn, hs: "HomeServer"):
   182|         super(StreamWorkerStore, self).__init__(database, db_conn, hs)
   183|         self._instance_name = hs.get_instance_name()
   184|         self._send_federation = hs.should_send_federation()
   185|         self._federation_shard_config = hs.config.worker.federation_shard_config
   186|         self._need_to_reset_federation_stream_positions = self._send_federation
   187|         events_max = self.get_room_max_stream_ordering()
   188|         event_cache_prefill, min_event_val = self.db_pool.get_cache_dict(
   189|             db_conn,
   190|             "events",
   191|             entity_column="room_id",
   192|             stream_column="stream_ordering",
   193|             max_value=events_max,
   194|         )
   195|         self._events_stream_cache = StreamChangeCache(
   196|             "EventsRoomStreamChangeCache",
   197|             min_event_val,
   198|             prefilled_cache=event_cache_prefill,
   199|         )
   200|         self._membership_stream_cache = StreamChangeCache(
   201|             "MembershipStreamChangeCache", events_max
   202|         )
   203|         self._stream_order_on_start = self.get_room_max_stream_ordering()
   204|     @abc.abstractmethod
   205|     def get_room_max_stream_ordering(self) -> int:
   206|         raise NotImplementedError()
   207|     @abc.abstractmethod
   208|     def get_room_min_stream_ordering(self) -> int:
   209|         raise NotImplementedError()
   210|     async def get_room_events_stream_for_rooms(
   211|         self,
   212|         room_ids: Collection[str],
   213|         from_key: str,
   214|         to_key: str,
   215|         limit: int = 0,
   216|         order: str = "DESC",
   217|     ) -> Dict[str, Tuple[List[EventBase], str]]:
   218|         """Get new room events in stream ordering since `from_key`.
   219|         Args:
   220|             room_ids
   221|             from_key: Token from which no events are returned before
   222|             to_key: Token from which no events are returned after. (This
   223|                 is typically the current stream token)
   224|             limit: Maximum number of events to return
   225|             order: Either "DESC" or "ASC". Determines which events are
   226|                 returned when the result is limited. If "DESC" then the most
   227|                 recent `limit` events are returned, otherwise returns the
   228|                 oldest `limit` events.
   229|         Returns:
   230|             A map from room id to a tuple containing:
   231|                 - list of recent events in the room
   232|                 - stream ordering key for the start of the chunk of events returned.
   233|         """
   234|         from_id = RoomStreamToken.parse_stream_token(from_key).stream
   235|         room_ids = self._events_stream_cache.get_entities_changed(room_ids, from_id)
   236|         if not room_ids:
   237|             return {}
   238|         results = {}
   239|         room_ids = list(room_ids)
   240|         for rm_ids in (room_ids[i : i + 20] for i in range(0, len(room_ids), 20)):
   241|             res = await make_deferred_yieldable(
   242|                 defer.gatherResults(
   243|                     [
   244|                         run_in_background(
   245|                             self.get_room_events_stream_for_room,
   246|                             room_id,
   247|                             from_key,
   248|                             to_key,
   249|                             limit,
   250|                             order=order,
   251|                         )
   252|                         for room_id in rm_ids
   253|                     ],
   254|                     consumeErrors=True,
   255|                 )
   256|             )
   257|             results.update(dict(zip(rm_ids, res)))
   258|         return results
   259|     def get_rooms_that_changed(
   260|         self, room_ids: Collection[str], from_key: str
   261|     ) -> Set[str]:
   262|         """Given a list of rooms and a token, return rooms where there may have
   263|         been changes.
   264|         Args:
   265|             room_ids
   266|             from_key: The room_key portion of a StreamToken
   267|         """
   268|         from_id = RoomStreamToken.parse_stream_token(from_key).stream
   269|         return {
   270|             room_id
   271|             for room_id in room_ids
   272|             if self._events_stream_cache.has_entity_changed(room_id, from_id)
   273|         }
   274|     async def get_room_events_stream_for_room(
   275|         self,
   276|         room_id: str,
   277|         from_key: str,
   278|         to_key: str,
   279|         limit: int = 0,
   280|         order: str = "DESC",
   281|     ) -> Tuple[List[EventBase], str]:
   282|         """Get new room events in stream ordering since `from_key`.
   283|         Args:
   284|             room_id
   285|             from_key: Token from which no events are returned before
   286|             to_key: Token from which no events are returned after. (This
   287|                 is typically the current stream token)
   288|             limit: Maximum number of events to return
   289|             order: Either "DESC" or "ASC". Determines which events are
   290|                 returned when the result is limited. If "DESC" then the most
   291|                 recent `limit` events are returned, otherwise returns the
   292|                 oldest `limit` events.
   293|         Returns:
   294|             The list of events (in ascending order) and the token from the start
   295|             of the chunk of events returned.
   296|         """
   297|         if from_key == to_key:
   298|             return [], from_key
   299|         from_id = RoomStreamToken.parse_stream_token(from_key).stream
   300|         to_id = RoomStreamToken.parse_stream_token(to_key).stream
   301|         has_changed = self._events_stream_cache.has_entity_changed(room_id, from_id)
   302|         if not has_changed:
   303|             return [], from_key
   304|         def f(txn):
   305|             sql = (
   306|                 "SELECT event_id, stream_ordering FROM events WHERE"
   307|                 " room_id = ?"
   308|                 " AND not outlier"
   309|                 " AND stream_ordering > ? AND stream_ordering <= ?"
   310|                 " ORDER BY stream_ordering %s LIMIT ?"
   311|             ) % (order,)
   312|             txn.execute(sql, (room_id, from_id, to_id, limit))
   313|             rows = [_EventDictReturn(row[0], None, row[1]) for row in txn]
   314|             return rows
   315|         rows = await self.db_pool.runInteraction("get_room_events_stream_for_room", f)
   316|         ret = await self.get_events_as_list(
   317|             [r.event_id for r in rows], get_prev_content=True
   318|         )
   319|         self._set_before_and_after(ret, rows, topo_order=from_id is None)
   320|         if order.lower() == "desc":
   321|             ret.reverse()
   322|         if rows:
   323|             key = "s%d" % min(r.stream_ordering for r in rows)
   324|         else:
   325|             key = from_key
   326|         return ret, key
   327|     async def get_membership_changes_for_user(
   328|         self, user_id: str, from_key: str, to_key: str
   329|     ) -> List[EventBase]:
   330|         from_id = RoomStreamToken.parse_stream_token(from_key).stream
   331|         to_id = RoomStreamToken.parse_stream_token(to_key).stream
   332|         if from_key == to_key:
   333|             return []
   334|         if from_id:
   335|             has_changed = self._membership_stream_cache.has_entity_changed(
   336|                 user_id, int(from_id)
   337|             )
   338|             if not has_changed:
   339|                 return []
   340|         def f(txn):
   341|             sql = (
   342|                 "SELECT m.event_id, stream_ordering FROM events AS e,"
   343|                 " room_memberships AS m"
   344|                 " WHERE e.event_id = m.event_id"
   345|                 " AND m.user_id = ?"
   346|                 " AND e.stream_ordering > ? AND e.stream_ordering <= ?"
   347|                 " ORDER BY e.stream_ordering ASC"
   348|             )
   349|             txn.execute(sql, (user_id, from_id, to_id))
   350|             rows = [_EventDictReturn(row[0], None, row[1]) for row in txn]
   351|             return rows
   352|         rows = await self.db_pool.runInteraction("get_membership_changes_for_user", f)
   353|         ret = await self.get_events_as_list(
   354|             [r.event_id for r in rows], get_prev_content=True
   355|         )
   356|         self._set_before_and_after(ret, rows, topo_order=False)
   357|         return ret
   358|     async def get_recent_events_for_room(
   359|         self, room_id: str, limit: int, end_token: str
   360|     ) -> Tuple[List[EventBase], str]:
   361|         """Get the most recent events in the room in topological ordering.
   362|         Args:
   363|             room_id
   364|             limit
   365|             end_token: The stream token representing now.
   366|         Returns:
   367|             A list of events and a token pointing to the start of the returned
   368|             events. The events returned are in ascending order.
   369|         """
   370|         rows, token = await self.get_recent_event_ids_for_room(
   371|             room_id, limit, end_token
   372|         )
   373|         events = await self.get_events_as_list(
   374|             [r.event_id for r in rows], get_prev_content=True
   375|         )
   376|         self._set_before_and_after(events, rows)
   377|         return (events, token)
   378|     async def get_recent_event_ids_for_room(
   379|         self, room_id: str, limit: int, end_token: str
   380|     ) -> Tuple[List[_EventDictReturn], str]:
   381|         """Get the most recent events in the room in topological ordering.
   382|         Args:
   383|             room_id
   384|             limit
   385|             end_token: The stream token representing now.
   386|         Returns:
   387|             A list of _EventDictReturn and a token pointing to the start of the
   388|             returned events. The events returned are in ascending order.
   389|         """
   390|         if limit == 0:
   391|             return [], end_token
   392|         end_token = RoomStreamToken.parse(end_token)
   393|         rows, token = await self.db_pool.runInteraction(
   394|             "get_recent_event_ids_for_room",
   395|             self._paginate_room_events_txn,
   396|             room_id,
   397|             from_token=end_token,
   398|             limit=limit,
   399|         )
   400|         rows.reverse()
   401|         return rows, token
   402|     async def get_room_event_before_stream_ordering(
   403|         self, room_id: str, stream_ordering: int
   404|     ) -> Tuple[int, int, str]:
   405|         """Gets details of the first event in a room at or before a stream ordering
   406|         Args:
   407|             room_id:
   408|             stream_ordering:
   409|         Returns:
   410|             A tuple of (stream ordering, topological ordering, event_id)
   411|         """
   412|         def _f(txn):

# --- HUNK 3: Lines 442-508 ---
   442|         Args:
   443|             event_id: The id of the event to look up a stream token for.
   444|         Raises:
   445|             StoreError if the event wasn't in the database.
   446|         Returns:
   447|             A stream ID.
   448|         """
   449|         return await self.db_pool.runInteraction(
   450|             "get_stream_id_for_event", self.get_stream_id_for_event_txn, event_id,
   451|         )
   452|     def get_stream_id_for_event_txn(
   453|         self, txn: LoggingTransaction, event_id: str, allow_none=False,
   454|     ) -> int:
   455|         return self.db_pool.simple_select_one_onecol_txn(
   456|             txn=txn,
   457|             table="events",
   458|             keyvalues={"event_id": event_id},
   459|             retcol="stream_ordering",
   460|             allow_none=allow_none,
   461|         )
   462|     async def get_stream_token_for_event(self, event_id: str) -> str:
   463|         """The stream token for an event
   464|         Args:
   465|             event_id: The id of the event to look up a stream token for.
   466|         Raises:
   467|             StoreError if the event wasn't in the database.
   468|         Returns:
   469|             A "s%d" stream token.
   470|         """
   471|         stream_id = await self.get_stream_id_for_event(event_id)
   472|         return "s%d" % (stream_id,)
   473|     async def get_topological_token_for_event(self, event_id: str) -> str:
   474|         """The stream token for an event
   475|         Args:
   476|             event_id: The id of the event to look up a stream token for.
   477|         Raises:
   478|             StoreError if the event wasn't in the database.
   479|         Returns:
   480|             A "t%d-%d" topological token.
   481|         """
   482|         row = await self.db_pool.simple_select_one(
   483|             table="events",
   484|             keyvalues={"event_id": event_id},
   485|             retcols=("stream_ordering", "topological_ordering"),
   486|             desc="get_topological_token_for_event",
   487|         )
   488|         return "t%d-%d" % (row["topological_ordering"], row["stream_ordering"])
   489|     async def get_current_topological_token(self, room_id: str, stream_key: int) -> int:
   490|         """Gets the topological token in a room after or at the given stream
   491|         ordering.
   492|         Args:
   493|             room_id
   494|             stream_key
   495|         """
   496|         sql = (
   497|             "SELECT coalesce(MIN(topological_ordering), 0) FROM events"
   498|             " WHERE room_id = ? AND stream_ordering >= ?"
   499|         )
   500|         row = await self.db_pool.execute(
   501|             "get_current_topological_token", None, sql, room_id, stream_key
   502|         )
   503|         return row[0][0] if row else 0
   504|     def _get_max_topological_txn(self, txn: LoggingTransaction, room_id: str) -> int:
   505|         txn.execute(
   506|             "SELECT MAX(topological_ordering) FROM events WHERE room_id = ?",
   507|             (room_id,),
   508|         )

# --- HUNK 4: Lines 511-552 ---
   511|     @staticmethod
   512|     def _set_before_and_after(
   513|         events: List[EventBase], rows: List[_EventDictReturn], topo_order: bool = True
   514|     ):
   515|         """Inserts ordering information to events' internal metadata from
   516|         the DB rows.
   517|         Args:
   518|             events
   519|             rows
   520|             topo_order: Whether the events were ordered topologically or by stream
   521|                 ordering. If true then all rows should have a non null
   522|                 topological_ordering.
   523|         """
   524|         for event, row in zip(events, rows):
   525|             stream = row.stream_ordering
   526|             if topo_order and row.topological_ordering:
   527|                 topo = row.topological_ordering
   528|             else:
   529|                 topo = None
   530|             internal = event.internal_metadata
   531|             internal.before = str(RoomStreamToken(topo, stream - 1))
   532|             internal.after = str(RoomStreamToken(topo, stream))
   533|             internal.order = (int(topo) if topo else 0, int(stream))
   534|     async def get_events_around(
   535|         self,
   536|         room_id: str,
   537|         event_id: str,
   538|         before_limit: int,
   539|         after_limit: int,
   540|         event_filter: Optional[Filter] = None,
   541|     ) -> dict:
   542|         """Retrieve events and pagination tokens around a given event in a
   543|         room.
   544|         """
   545|         results = await self.db_pool.runInteraction(
   546|             "get_events_around",
   547|             self._get_events_around_txn,
   548|             room_id,
   549|             event_id,
   550|             before_limit,
   551|             after_limit,
   552|             event_filter,

# --- HUNK 5: Lines 712-847 ---
   712|         )
   713|         txn.execute(sql % (clause,), args)
   714|         for typ, stream_id in min_positions.items():
   715|             self.db_pool.simple_upsert_txn(
   716|                 txn,
   717|                 table="federation_stream_position",
   718|                 keyvalues={"type": typ, "instance_name": self._instance_name},
   719|                 values={"stream_id": stream_id},
   720|             )
   721|     def has_room_changed_since(self, room_id: str, stream_id: int) -> bool:
   722|         return self._events_stream_cache.has_entity_changed(room_id, stream_id)
   723|     def _paginate_room_events_txn(
   724|         self,
   725|         txn: LoggingTransaction,
   726|         room_id: str,
   727|         from_token: RoomStreamToken,
   728|         to_token: Optional[RoomStreamToken] = None,
   729|         direction: str = "b",
   730|         limit: int = -1,
   731|         event_filter: Optional[Filter] = None,
   732|     ) -> Tuple[List[_EventDictReturn], str]:
   733|         """Returns list of events before or after a given token.
   734|         Args:
   735|             txn
   736|             room_id
   737|             from_token: The token used to stream from
   738|             to_token: A token which if given limits the results to only those before
   739|             direction: Either 'b' or 'f' to indicate whether we are paginating
   740|                 forwards or backwards from `from_key`.
   741|             limit: The maximum number of events to return.
   742|             event_filter: If provided filters the events to
   743|                 those that match the filter.
   744|         Returns:
   745|             A list of _EventDictReturn and a token that points to the end of the
   746|             result set. If no events are returned then the end of the stream has
   747|             been reached (i.e. there are no events between `from_token` and
   748|             `to_token`), or `limit` is zero.
   749|         """
   750|         assert int(limit) >= 0
   751|         args = [False, room_id]
   752|         if direction == "b":
   753|             order = "DESC"
   754|         else:
   755|             order = "ASC"
   756|         bounds = generate_pagination_where_clause(
   757|             direction=direction,
   758|             column_names=("topological_ordering", "stream_ordering"),
   759|             from_token=from_token,
   760|             to_token=to_token,
   761|             engine=self.database_engine,
   762|         )
   763|         filter_clause, filter_args = filter_to_clause(event_filter)
   764|         if filter_clause:
   765|             bounds += " AND " + filter_clause
   766|             args.extend(filter_args)
   767|         args.append(int(limit))
   768|         select_keywords = "SELECT"
   769|         join_clause = ""
   770|         if event_filter and event_filter.labels:
   771|             join_clause = """
   772|                 LEFT JOIN event_labels
   773|                 USING (event_id, room_id, topological_ordering)
   774|             """
   775|             if len(event_filter.labels) > 1:
   776|                 select_keywords += "DISTINCT"
   777|         sql = """
   778|             %(select_keywords)s event_id, topological_ordering, stream_ordering
   779|             FROM events
   780|             %(join_clause)s
   781|             WHERE outlier = ? AND room_id = ? AND %(bounds)s
   782|             ORDER BY topological_ordering %(order)s,
   783|             stream_ordering %(order)s LIMIT ?
   784|         """ % {
   785|             "select_keywords": select_keywords,
   786|             "join_clause": join_clause,
   787|             "bounds": bounds,
   788|             "order": order,
   789|         }
   790|         txn.execute(sql, args)
   791|         rows = [_EventDictReturn(row[0], row[1], row[2]) for row in txn]
   792|         if rows:
   793|             topo = rows[-1].topological_ordering
   794|             toke = rows[-1].stream_ordering
   795|             if direction == "b":
   796|                 toke -= 1
   797|             next_token = RoomStreamToken(topo, toke)
   798|         else:
   799|             next_token = to_token if to_token else from_token
   800|         return rows, str(next_token)
   801|     async def paginate_room_events(
   802|         self,
   803|         room_id: str,
   804|         from_key: str,
   805|         to_key: Optional[str] = None,
   806|         direction: str = "b",
   807|         limit: int = -1,
   808|         event_filter: Optional[Filter] = None,
   809|     ) -> Tuple[List[EventBase], str]:
   810|         """Returns list of events before or after a given token.
   811|         Args:
   812|             room_id
   813|             from_key: The token used to stream from
   814|             to_key: A token which if given limits the results to only those before
   815|             direction: Either 'b' or 'f' to indicate whether we are paginating
   816|                 forwards or backwards from `from_key`.
   817|             limit: The maximum number of events to return.
   818|             event_filter: If provided filters the events to those that match the filter.
   819|         Returns:
   820|             The results as a list of events and a token that points to the end
   821|             of the result set. If no events are returned then the end of the
   822|             stream has been reached (i.e. there are no events between `from_key`
   823|             and `to_key`).
   824|         """
   825|         from_key = RoomStreamToken.parse(from_key)
   826|         if to_key:
   827|             to_key = RoomStreamToken.parse(to_key)
   828|         rows, token = await self.db_pool.runInteraction(
   829|             "paginate_room_events",
   830|             self._paginate_room_events_txn,
   831|             room_id,
   832|             from_key,
   833|             to_key,
   834|             direction,
   835|             limit,
   836|             event_filter,
   837|         )
   838|         events = await self.get_events_as_list(
   839|             [r.event_id for r in rows], get_prev_content=True
   840|         )
   841|         self._set_before_and_after(events, rows)
   842|         return (events, token)
   843| class StreamStore(StreamWorkerStore):
   844|     def get_room_max_stream_ordering(self) -> int:
   845|         return self._stream_id_gen.get_current_token()
   846|     def get_room_min_stream_ordering(self) -> int:
   847|         return self._backfill_id_gen.get_current_token()


# ====================================================================
# FILE: synapse/storage/databases/main/tags.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 139-195 ---
   139|         self, user_id: str, room_id: str, tag: str, content: JsonDict
   140|     ) -> int:
   141|         """Add a tag to a room for a user.
   142|         Args:
   143|             user_id: The user to add a tag for.
   144|             room_id: The room to add a tag for.
   145|             tag: The tag name to add.
   146|             content: A json object to associate with the tag.
   147|         Returns:
   148|             The next account data ID.
   149|         """
   150|         content_json = json_encoder.encode(content)
   151|         def add_tag_txn(txn, next_id):
   152|             self.db_pool.simple_upsert_txn(
   153|                 txn,
   154|                 table="room_tags",
   155|                 keyvalues={"user_id": user_id, "room_id": room_id, "tag": tag},
   156|                 values={"content": content_json},
   157|             )
   158|             self._update_revision_txn(txn, user_id, room_id, next_id)
   159|         with await self._account_data_id_gen.get_next() as next_id:
   160|             await self.db_pool.runInteraction("add_tag", add_tag_txn, next_id)
   161|         self.get_tags_for_user.invalidate((user_id,))
   162|         return self._account_data_id_gen.get_current_token()
   163|     async def remove_tag_from_room(self, user_id: str, room_id: str, tag: str) -> int:
   164|         """Remove a tag from a room for a user.
   165|         Returns:
   166|             The next account data ID.
   167|         """
   168|         def remove_tag_txn(txn, next_id):
   169|             sql = (
   170|                 "DELETE FROM room_tags "
   171|                 " WHERE user_id = ? AND room_id = ? AND tag = ?"
   172|             )
   173|             txn.execute(sql, (user_id, room_id, tag))
   174|             self._update_revision_txn(txn, user_id, room_id, next_id)
   175|         with await self._account_data_id_gen.get_next() as next_id:
   176|             await self.db_pool.runInteraction("remove_tag", remove_tag_txn, next_id)
   177|         self.get_tags_for_user.invalidate((user_id,))
   178|         return self._account_data_id_gen.get_current_token()
   179|     def _update_revision_txn(
   180|         self, txn, user_id: str, room_id: str, next_id: int
   181|     ) -> None:
   182|         """Update the latest revision of the tags for the given user and room.
   183|         Args:
   184|             txn: The database cursor
   185|             user_id: The ID of the user.
   186|             room_id: The ID of the room.
   187|             next_id: The the revision to advance to.
   188|         """
   189|         txn.call_after(
   190|             self._account_data_stream_cache.entity_has_changed, user_id, next_id
   191|         )
   192|         update_max_id_sql = (
   193|             "UPDATE account_data_max_stream_id"
   194|             " SET stream_id = ?"
   195|             " WHERE stream_id < ?"


# ====================================================================
# FILE: synapse/storage/databases/main/transactions.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 1-44 ---
     1| import logging
     2| from collections import namedtuple
     3| from typing import Optional, Tuple
     4| from canonicaljson import encode_canonical_json
     5| from synapse.metrics.background_process_metrics import run_as_background_process
     6| from synapse.storage._base import SQLBaseStore, db_to_json
     7| from synapse.storage.database import DatabasePool
     8| from synapse.types import JsonDict
     9| from synapse.util.caches.expiringcache import ExpiringCache
    10| db_binary_type = memoryview
    11| logger = logging.getLogger(__name__)
    12| _TransactionRow = namedtuple(
    13|     "_TransactionRow",
    14|     ("id", "transaction_id", "destination", "ts", "response_code", "response_json"),
    15| )
    16| _UpdateTransactionRow = namedtuple(
    17|     "_TransactionRow", ("response_code", "response_json")
    18| )
    19| SENTINEL = object()
    20| class TransactionStore(SQLBaseStore):
    21|     """A collection of queries for handling PDUs.
    22|     """
    23|     def __init__(self, database: DatabasePool, db_conn, hs):
    24|         super(TransactionStore, self).__init__(database, db_conn, hs)
    25|         self._clock.looping_call(self._start_cleanup_transactions, 30 * 60 * 1000)
    26|         self._destination_retry_cache = ExpiringCache(
    27|             cache_name="get_destination_retry_timings",
    28|             clock=self._clock,
    29|             expiry_ms=5 * 60 * 1000,
    30|         )
    31|     async def get_received_txn_response(
    32|         self, transaction_id: str, origin: str
    33|     ) -> Optional[Tuple[int, JsonDict]]:
    34|         """For an incoming transaction from a given origin, check if we have
    35|         already responded to it. If so, return the response code and response
    36|         body (as a dict).
    37|         Args:
    38|             transaction_id
    39|             origin
    40|         Returns:
    41|             None if we have not previously responded to this transaction or a
    42|             2-tuple of (int, dict)
    43|         """
    44|         return await self.db_pool.runInteraction(

# --- HUNK 2: Lines 99-139 ---
    99|             Otherwise a dict for the retry scheme
   100|         """
   101|         result = self._destination_retry_cache.get(destination, SENTINEL)
   102|         if result is not SENTINEL:
   103|             return result
   104|         result = await self.db_pool.runInteraction(
   105|             "get_destination_retry_timings",
   106|             self._get_destination_retry_timings,
   107|             destination,
   108|         )
   109|         self._destination_retry_cache[destination] = result
   110|         return result
   111|     def _get_destination_retry_timings(self, txn, destination):
   112|         result = self.db_pool.simple_select_one_txn(
   113|             txn,
   114|             table="destinations",
   115|             keyvalues={"destination": destination},
   116|             retcols=("destination", "failure_ts", "retry_last_ts", "retry_interval"),
   117|             allow_none=True,
   118|         )
   119|         if result and result["retry_last_ts"] > 0:
   120|             return result
   121|         else:
   122|             return None
   123|     async def set_destination_retry_timings(
   124|         self,
   125|         destination: str,
   126|         failure_ts: Optional[int],
   127|         retry_last_ts: int,
   128|         retry_interval: int,
   129|     ) -> None:
   130|         """Sets the current retry timings for a given destination.
   131|         Both timings should be zero if retrying is no longer occuring.
   132|         Args:
   133|             destination
   134|             failure_ts: when the server started failing (ms since epoch)
   135|             retry_last_ts: time of last retry attempt in unix epoch ms
   136|             retry_interval: how long until next retry in ms
   137|         """
   138|         self._destination_retry_cache.pop(destination, None)
   139|         return await self.db_pool.runInteraction(

# --- HUNK 3: Lines 142-207 ---
   142|             destination,
   143|             failure_ts,
   144|             retry_last_ts,
   145|             retry_interval,
   146|         )
   147|     def _set_destination_retry_timings(
   148|         self, txn, destination, failure_ts, retry_last_ts, retry_interval
   149|     ):
   150|         if self.database_engine.can_native_upsert:
   151|             sql = """
   152|                 INSERT INTO destinations (
   153|                     destination, failure_ts, retry_last_ts, retry_interval
   154|                 )
   155|                     VALUES (?, ?, ?, ?)
   156|                 ON CONFLICT (destination) DO UPDATE SET
   157|                         failure_ts = EXCLUDED.failure_ts,
   158|                         retry_last_ts = EXCLUDED.retry_last_ts,
   159|                         retry_interval = EXCLUDED.retry_interval
   160|                     WHERE
   161|                         EXCLUDED.retry_interval = 0
   162|                         OR destinations.retry_interval < EXCLUDED.retry_interval
   163|             """
   164|             txn.execute(sql, (destination, failure_ts, retry_last_ts, retry_interval))
   165|             return
   166|         self.database_engine.lock_table(txn, "destinations")
   167|         prev_row = self.db_pool.simple_select_one_txn(
   168|             txn,
   169|             table="destinations",
   170|             keyvalues={"destination": destination},
   171|             retcols=("failure_ts", "retry_last_ts", "retry_interval"),
   172|             allow_none=True,
   173|         )
   174|         if not prev_row:
   175|             self.db_pool.simple_insert_txn(
   176|                 txn,
   177|                 table="destinations",
   178|                 values={
   179|                     "destination": destination,
   180|                     "failure_ts": failure_ts,
   181|                     "retry_last_ts": retry_last_ts,
   182|                     "retry_interval": retry_interval,
   183|                 },
   184|             )
   185|         elif retry_interval == 0 or prev_row["retry_interval"] < retry_interval:
   186|             self.db_pool.simple_update_one_txn(
   187|                 txn,
   188|                 "destinations",
   189|                 keyvalues={"destination": destination},
   190|                 updatevalues={
   191|                     "failure_ts": failure_ts,
   192|                     "retry_last_ts": retry_last_ts,
   193|                     "retry_interval": retry_interval,
   194|                 },
   195|             )
   196|     def _start_cleanup_transactions(self):
   197|         return run_as_background_process(
   198|             "cleanup_transactions", self._cleanup_transactions
   199|         )
   200|     async def _cleanup_transactions(self) -> None:
   201|         now = self._clock.time_msec()
   202|         month_ago = now - 30 * 24 * 60 * 60 * 1000
   203|         def _cleanup_transactions_txn(txn):
   204|             txn.execute("DELETE FROM received_transactions WHERE ts < ?", (month_ago,))
   205|         await self.db_pool.runInteraction(
   206|             "_cleanup_transactions", _cleanup_transactions_txn
   207|         )


# ====================================================================
# FILE: synapse/storage/databases/main/ui_auth.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| from typing import Any, Dict, List, Optional, Tuple, Union
     2| import attr
     3| from synapse.api.errors import StoreError
     4| from synapse.storage._base import SQLBaseStore, db_to_json
     5| from synapse.storage.database import LoggingTransaction
     6| from synapse.types import JsonDict
     7| from synapse.util import json_encoder, stringutils
     8| @attr.s
     9| class UIAuthSessionData:
    10|     session_id = attr.ib(type=str)
    11|     clientdict = attr.ib(type=JsonDict)
    12|     uri = attr.ib(type=str)
    13|     method = attr.ib(type=str)
    14|     description = attr.ib(type=str)
    15| class UIAuthWorkerStore(SQLBaseStore):
    16|     """
    17|     Manage user interactive authentication sessions.
    18|     """
    19|     async def create_ui_auth_session(
    20|         self, clientdict: JsonDict, uri: str, method: str, description: str,
    21|     ) -> UIAuthSessionData:
    22|         """
    23|         Creates a new user interactive authentication session.
    24|         The session can be used to track the stages necessary to authenticate a
    25|         user across multiple HTTP requests.
    26|         Args:
    27|             clientdict:
    28|                 The dictionary from the client root level, not the 'auth' key.


# ====================================================================
# FILE: synapse/storage/databases/main/user_directory.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-36 ---
     1| import logging
     2| import re
     3| from typing import Any, Dict, Iterable, Optional, Set, Tuple
     4| from synapse.api.constants import EventTypes, JoinRules
     5| from synapse.storage.database import DatabasePool
     6| from synapse.storage.databases.main.state import StateFilter
     7| from synapse.storage.databases.main.state_deltas import StateDeltasStore
     8| from synapse.storage.engines import PostgresEngine, Sqlite3Engine
     9| from synapse.types import get_domain_from_id, get_localpart_from_id
    10| from synapse.util.caches.descriptors import cached
    11| logger = logging.getLogger(__name__)
    12| TEMP_TABLE = "_temp_populate_user_directory"
    13| class UserDirectoryBackgroundUpdateStore(StateDeltasStore):
    14|     SHARE_PRIVATE_WORKING_SET = 500
    15|     def __init__(self, database: DatabasePool, db_conn, hs):
    16|         super(UserDirectoryBackgroundUpdateStore, self).__init__(database, db_conn, hs)
    17|         self.server_name = hs.hostname
    18|         self.db_pool.updates.register_background_update_handler(
    19|             "populate_user_directory_createtables",
    20|             self._populate_user_directory_createtables,
    21|         )
    22|         self.db_pool.updates.register_background_update_handler(
    23|             "populate_user_directory_process_rooms",
    24|             self._populate_user_directory_process_rooms,
    25|         )
    26|         self.db_pool.updates.register_background_update_handler(
    27|             "populate_user_directory_process_users",
    28|             self._populate_user_directory_process_users,
    29|         )
    30|         self.db_pool.updates.register_background_update_handler(
    31|             "populate_user_directory_cleanup", self._populate_user_directory_cleanup
    32|         )
    33|     async def _populate_user_directory_createtables(self, progress, batch_size):
    34|         def _make_staging_area(txn):
    35|             sql = (
    36|                 "CREATE TABLE IF NOT EXISTS "

# --- HUNK 2: Lines 409-449 ---
   409|         )
   410|     @cached()
   411|     async def get_user_in_directory(self, user_id: str) -> Optional[Dict[str, Any]]:
   412|         return await self.db_pool.simple_select_one(
   413|             table="user_directory",
   414|             keyvalues={"user_id": user_id},
   415|             retcols=("display_name", "avatar_url"),
   416|             allow_none=True,
   417|             desc="get_user_in_directory",
   418|         )
   419|     async def update_user_directory_stream_pos(self, stream_id: str) -> None:
   420|         await self.db_pool.simple_update_one(
   421|             table="user_directory_stream_pos",
   422|             keyvalues={},
   423|             updatevalues={"stream_id": stream_id},
   424|             desc="update_user_directory_stream_pos",
   425|         )
   426| class UserDirectoryStore(UserDirectoryBackgroundUpdateStore):
   427|     SHARE_PRIVATE_WORKING_SET = 500
   428|     def __init__(self, database: DatabasePool, db_conn, hs):
   429|         super(UserDirectoryStore, self).__init__(database, db_conn, hs)
   430|     async def remove_from_user_dir(self, user_id: str) -> None:
   431|         def _remove_from_user_dir_txn(txn):
   432|             self.db_pool.simple_delete_txn(
   433|                 txn, table="user_directory", keyvalues={"user_id": user_id}
   434|             )
   435|             self.db_pool.simple_delete_txn(
   436|                 txn, table="user_directory_search", keyvalues={"user_id": user_id}
   437|             )
   438|             self.db_pool.simple_delete_txn(
   439|                 txn, table="users_in_public_rooms", keyvalues={"user_id": user_id}
   440|             )
   441|             self.db_pool.simple_delete_txn(
   442|                 txn,
   443|                 table="users_who_share_private_rooms",
   444|                 keyvalues={"user_id": user_id},
   445|             )
   446|             self.db_pool.simple_delete_txn(
   447|                 txn,
   448|                 table="users_who_share_private_rooms",
   449|                 keyvalues={"other_user_id": user_id},


# ====================================================================
# FILE: synapse/storage/databases/main/user_erasure_store.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 42-66 ---
    42|         """Indicate that user_id wishes their message history to be erased.
    43|         Args:
    44|             user_id: full user_id to be erased
    45|         """
    46|         def f(txn):
    47|             txn.execute("SELECT 1 FROM erased_users WHERE user_id = ?", (user_id,))
    48|             if txn.fetchone():
    49|                 return
    50|             txn.execute("INSERT INTO erased_users (user_id) VALUES (?)", (user_id,))
    51|             self._invalidate_cache_and_stream(txn, self.is_user_erased, (user_id,))
    52|         await self.db_pool.runInteraction("mark_user_erased", f)
    53|     async def mark_user_not_erased(self, user_id: str) -> None:
    54|         """Indicate that user_id is no longer erased.
    55|         Args:
    56|             user_id: full user_id to be un-erased
    57|         """
    58|         def f(txn):
    59|             txn.execute("SELECT 1 FROM erased_users WHERE user_id = ?", (user_id,))
    60|             if not txn.fetchone():
    61|                 return
    62|             self.simple_delete_one_txn(
    63|                 txn, "erased_users", keyvalues={"user_id": user_id}
    64|             )
    65|             self._invalidate_cache_and_stream(txn, self.is_user_erased, (user_id,))
    66|         await self.db_pool.runInteraction("mark_user_not_erased", f)


# ====================================================================
# FILE: synapse/storage/databases/state/bg_updates.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 93-133 ---
    93|                         if (typ, state_key) not in results[group]
    94|                     )
    95|                     if (
    96|                         max_entries_returned is not None
    97|                         and len(results[group]) == max_entries_returned
    98|                     ):
    99|                         break
   100|                     next_group = self.db_pool.simple_select_one_onecol_txn(
   101|                         txn,
   102|                         table="state_group_edges",
   103|                         keyvalues={"state_group": next_group},
   104|                         retcol="prev_state_group",
   105|                         allow_none=True,
   106|                     )
   107|         return results
   108| class StateBackgroundUpdateStore(StateGroupBackgroundUpdateStore):
   109|     STATE_GROUP_DEDUPLICATION_UPDATE_NAME = "state_group_state_deduplication"
   110|     STATE_GROUP_INDEX_UPDATE_NAME = "state_group_state_type_index"
   111|     STATE_GROUPS_ROOM_INDEX_UPDATE_NAME = "state_groups_room_id_idx"
   112|     def __init__(self, database: DatabasePool, db_conn, hs):
   113|         super(StateBackgroundUpdateStore, self).__init__(database, db_conn, hs)
   114|         self.db_pool.updates.register_background_update_handler(
   115|             self.STATE_GROUP_DEDUPLICATION_UPDATE_NAME,
   116|             self._background_deduplicate_state,
   117|         )
   118|         self.db_pool.updates.register_background_update_handler(
   119|             self.STATE_GROUP_INDEX_UPDATE_NAME, self._background_index_state
   120|         )
   121|         self.db_pool.updates.register_background_index_update(
   122|             self.STATE_GROUPS_ROOM_INDEX_UPDATE_NAME,
   123|             index_name="state_groups_room_id_idx",
   124|             table="state_groups",
   125|             columns=["room_id"],
   126|         )
   127|     async def _background_deduplicate_state(self, progress, batch_size):
   128|         """This background update will slowly deduplicate state by reencoding
   129|         them as deltas.
   130|         """
   131|         last_state_group = progress.get("last_state_group", 0)
   132|         rows_inserted = progress.get("rows_inserted", 0)
   133|         max_group = progress.get("max_group", None)


# ====================================================================
# FILE: synapse/storage/databases/state/store.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-61 ---
     1| import logging
     2| from collections import namedtuple
     3| from typing import Dict, Iterable, List, Set, Tuple
     4| from synapse.api.constants import EventTypes
     5| from synapse.storage._base import SQLBaseStore
     6| from synapse.storage.database import DatabasePool
     7| from synapse.storage.databases.state.bg_updates import StateBackgroundUpdateStore
     8| from synapse.storage.state import StateFilter
     9| from synapse.storage.types import Cursor
    10| from synapse.storage.util.sequence import build_sequence_generator
    11| from synapse.types import StateMap
    12| from synapse.util.caches.descriptors import cached
    13| from synapse.util.caches.dictionary_cache import DictionaryCache
    14| logger = logging.getLogger(__name__)
    15| MAX_STATE_DELTA_HOPS = 100
    16| class _GetStateGroupDelta(
    17|     namedtuple("_GetStateGroupDelta", ("prev_group", "delta_ids"))
    18| ):
    19|     """Return type of get_state_group_delta that implements __len__, which lets
    20|     us use the itrable flag when caching
    21|     """
    22|     __slots__ = []
    23|     def __len__(self):
    24|         return len(self.delta_ids) if self.delta_ids else 0
    25| class StateGroupDataStore(StateBackgroundUpdateStore, SQLBaseStore):
    26|     """A data store for fetching/storing state groups.
    27|     """
    28|     def __init__(self, database: DatabasePool, db_conn, hs):
    29|         super(StateGroupDataStore, self).__init__(database, db_conn, hs)
    30|         self._state_group_cache = DictionaryCache(
    31|             "*stateGroupCache*",
    32|             50000,
    33|         )
    34|         self._state_group_members_cache = DictionaryCache(
    35|             "*stateGroupMembersCache*", 500000,
    36|         )
    37|         def get_max_state_group_txn(txn: Cursor):
    38|             txn.execute("SELECT COALESCE(max(id), 0) FROM state_groups")
    39|             return txn.fetchone()[0]
    40|         self._state_group_seq_gen = build_sequence_generator(
    41|             self.database_engine, get_max_state_group_txn, "state_group_id_seq"
    42|         )
    43|     @cached(max_entries=10000, iterable=True)
    44|     async def get_state_group_delta(self, state_group):
    45|         """Given a state group try to return a previous group and a delta between
    46|         the old and the new.
    47|         Returns:
    48|             (prev_group, delta_ids), where both may be None.
    49|         """
    50|         def _get_state_group_delta_txn(txn):
    51|             prev_group = self.db_pool.simple_select_one_onecol_txn(
    52|                 txn,
    53|                 table="state_group_edges",
    54|                 keyvalues={"state_group": state_group},
    55|                 retcol="prev_state_group",
    56|                 allow_none=True,
    57|             )
    58|             if not prev_group:
    59|                 return _GetStateGroupDelta(None, None)
    60|             delta_ids = self.db_pool.simple_select_list_txn(
    61|                 txn,

# --- HUNK 2: Lines 102-142 ---
   102|                 from the database.
   103|         Returns 2-tuple (`state_dict`, `got_all`).
   104|         `got_all` is a bool indicating if we successfully retrieved all
   105|         requests state from the cache, if False we need to query the DB for the
   106|         missing state.
   107|         """
   108|         is_all, known_absent, state_dict_ids = cache.get(group)
   109|         if is_all or state_filter.is_full():
   110|             return state_filter.filter_state(state_dict_ids), is_all
   111|         missing_types = False
   112|         if state_filter.has_wildcards():
   113|             missing_types = True
   114|         else:
   115|             for key in state_filter.concrete_types():
   116|                 if key not in state_dict_ids and key not in known_absent:
   117|                     missing_types = True
   118|                     break
   119|         return state_filter.filter_state(state_dict_ids), not missing_types
   120|     async def _get_state_for_groups(
   121|         self, groups: Iterable[int], state_filter: StateFilter = StateFilter.all()
   122|     ) -> Dict[int, StateMap[str]]:
   123|         """Gets the state at each of a list of state groups, optionally
   124|         filtering by type/state_key
   125|         Args:
   126|             groups: list of state groups for which we want
   127|                 to get the state.
   128|             state_filter: The state filter used to fetch state
   129|                 from the database.
   130|         Returns:
   131|             Dict of state group to state map.
   132|         """
   133|         member_filter, non_member_filter = state_filter.get_member_split()
   134|         (
   135|             non_member_state,
   136|             incomplete_groups_nm,
   137|         ) = self._get_state_for_groups_using_cache(
   138|             groups, self._state_group_cache, state_filter=non_member_filter
   139|         )
   140|         (member_state, incomplete_groups_m,) = self._get_state_for_groups_using_cache(
   141|             groups, self._state_group_members_cache, state_filter=member_filter
   142|         )


# ====================================================================
# FILE: synapse/storage/engines/_base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 47-66 ---
    47|     def convert_param_style(self, sql: str) -> str:
    48|         ...
    49|     @abc.abstractmethod
    50|     def on_new_connection(self, db_conn: ConnectionType) -> None:
    51|         ...
    52|     @abc.abstractmethod
    53|     def is_deadlock(self, error: Exception) -> bool:
    54|         ...
    55|     @abc.abstractmethod
    56|     def is_connection_closed(self, conn: ConnectionType) -> bool:
    57|         ...
    58|     @abc.abstractmethod
    59|     def lock_table(self, txn, table: str) -> None:
    60|         ...
    61|     @property
    62|     @abc.abstractmethod
    63|     def server_version(self) -> str:
    64|         """Gets a string giving the server version. For example: '3.22.0'
    65|         """
    66|         ...


# ====================================================================
# FILE: synapse/storage/engines/postgres.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-22 ---
     1| import logging
     2| from ._base import BaseDatabaseEngine, IncorrectDatabaseSetup
     3| logger = logging.getLogger(__name__)
     4| class PostgresEngine(BaseDatabaseEngine):
     5|     def __init__(self, database_module, database_config):
     6|         super().__init__(database_module, database_config)
     7|         self.module.extensions.register_type(self.module.extensions.UNICODE)
     8|         def _disable_bytes_adapter(_):
     9|             raise Exception("Passing bytes to DB is disabled.")
    10|         self.module.extensions.register_adapter(bytes, _disable_bytes_adapter)
    11|         self.synchronous_commit = database_config.get("synchronous_commit", True)
    12|         self._version = None  # unknown as yet
    13|     @property
    14|     def single_threaded(self) -> bool:
    15|         return False
    16|     def check_database(self, db_conn, allow_outdated_version: bool = False):
    17|         self._version = db_conn.server_version
    18|         if not allow_outdated_version and self._version < 90500:
    19|             raise RuntimeError("Synapse requires PostgreSQL 9.5+ or above.")
    20|         with db_conn.cursor() as txn:
    21|             txn.execute("SHOW SERVER_ENCODING")
    22|             rows = txn.fetchall()

# --- HUNK 2: Lines 53-109 ---
    53|         if collation != "C":
    54|             errors.append("    - 'COLLATE' is set to %r. Should be 'C'" % (collation,))
    55|         if ctype != "C":
    56|             errors.append("    - 'CTYPE' is set to %r. Should be 'C'" % (ctype,))
    57|         if errors:
    58|             raise IncorrectDatabaseSetup(
    59|                 "Database is incorrectly configured:\n\n%s\n\n"
    60|                 "See docs/postgres.md for more information." % ("\n".join(errors))
    61|             )
    62|     def convert_param_style(self, sql):
    63|         return sql.replace("?", "%s")
    64|     def on_new_connection(self, db_conn):
    65|         db_conn.set_isolation_level(
    66|             self.module.extensions.ISOLATION_LEVEL_REPEATABLE_READ
    67|         )
    68|         cursor = db_conn.cursor()
    69|         cursor.execute("SET bytea_output TO escape")
    70|         if not self.synchronous_commit:
    71|             cursor.execute("SET synchronous_commit TO OFF")
    72|         cursor.close()
    73|     @property
    74|     def can_native_upsert(self):
    75|         """
    76|         Can we use native UPSERTs?
    77|         """
    78|         return True
    79|     @property
    80|     def supports_tuple_comparison(self):
    81|         """
    82|         Do we support comparing tuples, i.e. `(a, b) > (c, d)`?
    83|         """
    84|         return True
    85|     @property
    86|     def supports_using_any_list(self):
    87|         """Do we support using `a = ANY(?)` and passing a list
    88|         """
    89|         return True
    90|     def is_deadlock(self, error):
    91|         if isinstance(error, self.module.DatabaseError):
    92|             return error.pgcode in ["40001", "40P01"]
    93|         return False
    94|     def is_connection_closed(self, conn):
    95|         return bool(conn.closed)
    96|     def lock_table(self, txn, table):
    97|         txn.execute("LOCK TABLE %s in EXCLUSIVE MODE" % (table,))
    98|     @property
    99|     def server_version(self):
   100|         """Returns a string giving the server version. For example: '8.1.5'
   101|         Returns:
   102|             string
   103|         """
   104|         numver = self._version
   105|         assert numver is not None
   106|         if numver >= 100000:
   107|             return "%i.%i" % (numver / 10000, numver % 10000)
   108|         else:
   109|             return "%i.%i.%i" % (numver / 10000, (numver % 10000) / 100, numver % 100)


# ====================================================================
# FILE: synapse/storage/engines/sqlite.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-24 ---
     1| import struct
     2| import threading
     3| import typing
     4| from synapse.storage.engines import BaseDatabaseEngine
     5| if typing.TYPE_CHECKING:
     6|     import sqlite3  # noqa: F401
     7| class Sqlite3Engine(BaseDatabaseEngine["sqlite3.Connection"]):
     8|     def __init__(self, database_module, database_config):
     9|         super().__init__(database_module, database_config)
    10|         database = database_config.get("args", {}).get("database")
    11|         self._is_in_memory = database in (None, ":memory:",)
    12|         self._current_state_group_id = None
    13|         self._current_state_group_id_lock = threading.Lock()
    14|     @property
    15|     def single_threaded(self) -> bool:
    16|         return True
    17|     @property
    18|     def can_native_upsert(self):
    19|         """
    20|         Do we support native UPSERTs? This requires SQLite3 3.24+, plus some
    21|         more work we haven't done yet to tell what was inserted vs updated.
    22|         """
    23|         return self.module.sqlite_version_info >= (3, 24, 0)
    24|     @property

# --- HUNK 2: Lines 33-83 ---
    33|         """Do we support using `a = ANY(?)` and passing a list
    34|         """
    35|         return False
    36|     def check_database(self, db_conn, allow_outdated_version: bool = False):
    37|         if not allow_outdated_version:
    38|             version = self.module.sqlite_version_info
    39|             if version < (3, 11, 0):
    40|                 raise RuntimeError("Synapse requires sqlite 3.11 or above.")
    41|     def check_new_database(self, txn):
    42|         """Gets called when setting up a brand new database. This allows us to
    43|         apply stricter checks on new databases versus existing database.
    44|         """
    45|     def convert_param_style(self, sql):
    46|         return sql
    47|     def on_new_connection(self, db_conn):
    48|         from synapse.storage.prepare_database import prepare_database
    49|         if self._is_in_memory:
    50|             prepare_database(db_conn, self, config=None)
    51|         db_conn.create_function("rank", 1, _rank)
    52|         db_conn.execute("PRAGMA foreign_keys = ON;")
    53|     def is_deadlock(self, error):
    54|         return False
    55|     def is_connection_closed(self, conn):
    56|         return False
    57|     def lock_table(self, txn, table):
    58|         return
    59|     @property
    60|     def server_version(self):
    61|         """Gets a string giving the server version. For example: '3.22.0'
    62|         Returns:
    63|             string
    64|         """
    65|         return "%i.%i.%i" % self.module.sqlite_version_info
    66| def _parse_match_info(buf):
    67|     bufsize = len(buf)
    68|     return [struct.unpack("@I", buf[i : i + 4])[0] for i in range(0, bufsize, 4)]
    69| def _rank(raw_match_info):
    70|     """Handle match_info called w/default args 'pcx' - based on the example rank
    71|     function http://sqlite.org/fts3.html#appendix_a
    72|     """
    73|     match_info = _parse_match_info(raw_match_info)
    74|     score = 0.0
    75|     p, c = match_info[:2]
    76|     for phrase_num in range(p):
    77|         phrase_info_idx = 2 + (phrase_num * c * 3)
    78|         for col_num in range(c):
    79|             col_idx = phrase_info_idx + (col_num * 3)
    80|             x1, x2 = match_info[col_idx : col_idx + 2]
    81|             if x1 > 0:
    82|                 score += float(x1) / x2
    83|     return score


# ====================================================================
# FILE: synapse/storage/persist_events.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-34 ---
     1| import itertools
     2| import logging
     3| from collections import deque, namedtuple
     4| from typing import Iterable, List, Optional, Set, Tuple
     5| from prometheus_client import Counter, Histogram
     6| from twisted.internet import defer
     7| from synapse.api.constants import EventTypes, Membership
     8| from synapse.events import EventBase
     9| from synapse.events.snapshot import EventContext
    10| from synapse.logging.context import PreserveLoggingContext, make_deferred_yieldable
    11| from synapse.metrics.background_process_metrics import run_as_background_process
    12| from synapse.storage.databases import Databases
    13| from synapse.storage.databases.main.events import DeltaState
    14| from synapse.types import StateMap
    15| from synapse.util.async_helpers import ObservableDeferred
    16| from synapse.util.metrics import Measure
    17| logger = logging.getLogger(__name__)
    18| state_delta_counter = Counter("synapse_storage_events_state_delta", "")
    19| state_delta_single_event_counter = Counter(
    20|     "synapse_storage_events_state_delta_single_event", ""
    21| )
    22| state_delta_reuse_delta_counter = Counter(
    23|     "synapse_storage_events_state_delta_reuse_delta", ""
    24| )
    25| forward_extremities_counter = Histogram(
    26|     "synapse_storage_events_forward_extremities_persisted",
    27|     "Number of forward extremities for each new event",
    28|     buckets=(1, 2, 3, 5, 7, 10, 15, 20, 50, 100, 200, 500, "+Inf"),
    29| )
    30| stale_forward_extremities_counter = Histogram(
    31|     "synapse_storage_events_stale_forward_extremities_persisted",
    32|     "Number of unchanged forward extremities for each new event",
    33|     buckets=(0, 1, 2, 3, 5, 7, 10, 15, 20, 50, 100, 200, 500, "+Inf"),
    34| )

# --- HUNK 2: Lines 101-219 ---
   101|                 queue = self._event_persist_queues.pop(room_id, None)
   102|                 if queue:
   103|                     self._event_persist_queues[room_id] = queue
   104|                 self._currently_persisting_rooms.discard(room_id)
   105|         run_as_background_process("persist_events", handle_queue_loop)
   106|     def _get_drainining_queue(self, room_id):
   107|         queue = self._event_persist_queues.setdefault(room_id, deque())
   108|         try:
   109|             while True:
   110|                 yield queue.popleft()
   111|         except IndexError:
   112|             pass
   113| class EventsPersistenceStorage:
   114|     """High level interface for handling persisting newly received events.
   115|     Takes care of batching up events by room, and calculating the necessary
   116|     current state and forward extremity changes.
   117|     """
   118|     def __init__(self, hs, stores: Databases):
   119|         self.main_store = stores.main
   120|         self.state_store = stores.state
   121|         self.persist_events_store = stores.persist_events
   122|         self._clock = hs.get_clock()
   123|         self.is_mine_id = hs.is_mine_id
   124|         self._event_persist_queue = _EventPeristenceQueue()
   125|         self._state_resolution_handler = hs.get_state_resolution_handler()
   126|     async def persist_events(
   127|         self,
   128|         events_and_contexts: List[Tuple[EventBase, EventContext]],
   129|         backfilled: bool = False,
   130|     ) -> int:
   131|         """
   132|         Write events to the database
   133|         Args:
   134|             events_and_contexts: list of tuples of (event, context)
   135|             backfilled: Whether the results are retrieved from federation
   136|                 via backfill or not. Used to determine if they're "new" events
   137|                 which might update the current state etc.
   138|         Returns:
   139|             the stream ordering of the latest persisted event
   140|         """
   141|         partitioned = {}
   142|         for event, ctx in events_and_contexts:
   143|             partitioned.setdefault(event.room_id, []).append((event, ctx))
   144|         deferreds = []
   145|         for room_id, evs_ctxs in partitioned.items():
   146|             d = self._event_persist_queue.add_to_queue(
   147|                 room_id, evs_ctxs, backfilled=backfilled
   148|             )
   149|             deferreds.append(d)
   150|         for room_id in partitioned:
   151|             self._maybe_start_persisting(room_id)
   152|         await make_deferred_yieldable(
   153|             defer.gatherResults(deferreds, consumeErrors=True)
   154|         )
   155|         return self.main_store.get_current_events_token()
   156|     async def persist_event(
   157|         self, event: EventBase, context: EventContext, backfilled: bool = False
   158|     ) -> Tuple[int, int]:
   159|         """
   160|         Returns:
   161|             The stream ordering of `event`, and the stream ordering of the
   162|             latest persisted event
   163|         """
   164|         deferred = self._event_persist_queue.add_to_queue(
   165|             event.room_id, [(event, context)], backfilled=backfilled
   166|         )
   167|         self._maybe_start_persisting(event.room_id)
   168|         await make_deferred_yieldable(deferred)
   169|         max_persisted_id = self.main_store.get_current_events_token()
   170|         return (event.internal_metadata.stream_ordering, max_persisted_id)
   171|     def _maybe_start_persisting(self, room_id: str):
   172|         async def persisting_queue(item):
   173|             with Measure(self._clock, "persist_events"):
   174|                 await self._persist_events(
   175|                     item.events_and_contexts, backfilled=item.backfilled
   176|                 )
   177|         self._event_persist_queue.handle_queue(room_id, persisting_queue)
   178|     async def _persist_events(
   179|         self,
   180|         events_and_contexts: List[Tuple[EventBase, EventContext]],
   181|         backfilled: bool = False,
   182|     ):
   183|         """Calculates the change to current state and forward extremities, and
   184|         persists the given events and with those updates.
   185|         """
   186|         if not events_and_contexts:
   187|             return
   188|         chunks = [
   189|             events_and_contexts[x : x + 100]
   190|             for x in range(0, len(events_and_contexts), 100)
   191|         ]
   192|         for chunk in chunks:
   193|             new_forward_extremeties = {}
   194|             current_state_for_room = {}
   195|             state_delta_for_room = {}
   196|             potentially_left_users = set()  # type: Set[str]
   197|             if not backfilled:
   198|                 with Measure(self._clock, "_calculate_state_and_extrem"):
   199|                     events_by_room = {}
   200|                     for event, context in chunk:
   201|                         events_by_room.setdefault(event.room_id, []).append(
   202|                             (event, context)
   203|                         )
   204|                     for room_id, ev_ctx_rm in events_by_room.items():
   205|                         latest_event_ids = await self.main_store.get_latest_event_ids_in_room(
   206|                             room_id
   207|                         )
   208|                         new_latest_event_ids = await self._calculate_new_extremities(
   209|                             room_id, ev_ctx_rm, latest_event_ids
   210|                         )
   211|                         latest_event_ids = set(latest_event_ids)
   212|                         if new_latest_event_ids == latest_event_ids:
   213|                             continue
   214|                         assert new_latest_event_ids, "No forward extremities left!"
   215|                         new_forward_extremeties[room_id] = new_latest_event_ids
   216|                         len_1 = (
   217|                             len(latest_event_ids) == 1
   218|                             and len(new_latest_event_ids) == 1
   219|                         )

# --- HUNK 3: Lines 265-326 ---
   265|                             if not is_still_joined:
   266|                                 logger.info("Server no longer in room %s", room_id)
   267|                                 latest_event_ids = []
   268|                                 current_state = {}
   269|                                 delta.no_longer_in_room = True
   270|                             state_delta_for_room[room_id] = delta
   271|                         if current_state is not None:
   272|                             current_state_for_room[room_id] = current_state
   273|             await self.persist_events_store._persist_events_and_state_updates(
   274|                 chunk,
   275|                 current_state_for_room=current_state_for_room,
   276|                 state_delta_for_room=state_delta_for_room,
   277|                 new_forward_extremeties=new_forward_extremeties,
   278|                 backfilled=backfilled,
   279|             )
   280|             await self._handle_potentially_left_users(potentially_left_users)
   281|     async def _calculate_new_extremities(
   282|         self,
   283|         room_id: str,
   284|         event_contexts: List[Tuple[EventBase, EventContext]],
   285|         latest_event_ids: List[str],
   286|     ):
   287|         """Calculates the new forward extremities for a room given events to
   288|         persist.
   289|         Assumes that we are only persisting events for one room at a time.
   290|         """
   291|         new_events = [
   292|             event
   293|             for event, ctx in event_contexts
   294|             if not event.internal_metadata.is_outlier()
   295|             and not ctx.rejected
   296|             and not event.internal_metadata.is_soft_failed()
   297|         ]
   298|         latest_event_ids = set(latest_event_ids)
   299|         result = set(latest_event_ids)
   300|         result.update(event.event_id for event in new_events)
   301|         result.difference_update(
   302|             e_id for event in new_events for e_id in event.prev_event_ids()
   303|         )
   304|         existing_prevs = await self.persist_events_store._get_events_which_are_prevs(
   305|             result
   306|         )
   307|         result.difference_update(existing_prevs)
   308|         existing_prevs = await self.persist_events_store._get_prevs_before_rejected(
   309|             e_id for event in new_events for e_id in event.prev_event_ids()
   310|         )
   311|         result.difference_update(existing_prevs)
   312|         if result != latest_event_ids:
   313|             forward_extremities_counter.observe(len(result))
   314|             stale = latest_event_ids & result
   315|             stale_forward_extremities_counter.observe(len(stale))
   316|         return result
   317|     async def _get_new_state_after_events(
   318|         self,
   319|         room_id: str,
   320|         events_context: List[Tuple[EventBase, EventContext]],
   321|         old_latest_event_ids: Iterable[str],
   322|         new_latest_event_ids: Iterable[str],
   323|     ) -> Tuple[Optional[StateMap[str]], Optional[StateMap[str]]]:
   324|         """Calculate the current state dict after adding some new events to
   325|         a room
   326|         Args:


# ====================================================================
# FILE: synapse/storage/prepare_database.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-64 ---
     1| import imp
     2| import logging
     3| import os
     4| import re
     5| from collections import Counter
     6| from typing import TextIO
     7| import attr
     8| from synapse.storage.engines.postgres import PostgresEngine
     9| from synapse.storage.types import Cursor
    10| logger = logging.getLogger(__name__)
    11| SCHEMA_VERSION = 58
    12| dir_path = os.path.abspath(os.path.dirname(__file__))
    13| class PrepareDatabaseException(Exception):
    14|     pass
    15| class UpgradeDatabaseException(PrepareDatabaseException):
    16|     pass
    17| OUTDATED_SCHEMA_ON_WORKER_ERROR = (
    18|     "Expected database schema version %i but got %i: run the main synapse process to "
    19|     "upgrade the database schema before starting worker processes."
    20| )
    21| EMPTY_DATABASE_ON_WORKER_ERROR = (
    22|     "Uninitialised database: run the main synapse process to prepare the database "
    23|     "schema before starting worker processes."
    24| )
    25| UNAPPLIED_DELTA_ON_WORKER_ERROR = (
    26|     "Database schema delta %s has not been applied: run the main synapse process to "
    27|     "upgrade the database schema before starting worker processes."
    28| )
    29| def prepare_database(db_conn, database_engine, config, databases=["main", "state"]):
    30|     """Prepares a physical database for usage. Will either create all necessary tables
    31|     or upgrade from an older schema version.
    32|     If `config` is None then prepare_database will assert that no upgrade is
    33|     necessary, *or* will create a fresh database if the database is empty.
    34|     Args:
    35|         db_conn:
    36|         database_engine:
    37|         config (synapse.config.homeserver.HomeServerConfig|None):
    38|             application config, or None if we are connecting to an existing
    39|             database which we expect to be configured already
    40|         databases (list[str]): The name of the databases that will be used
    41|             with this physical database. Defaults to all databases.
    42|     """
    43|     try:
    44|         cur = db_conn.cursor()
    45|         logger.info("%r: Checking existing schema version", databases)
    46|         version_info = _get_or_create_schema_state(cur, database_engine)
    47|         if version_info:
    48|             user_version, delta_files, upgraded = version_info
    49|             logger.info(
    50|                 "%r: Existing schema is %i (+%i deltas)",
    51|                 databases,
    52|                 user_version,
    53|                 len(delta_files),
    54|             )
    55|             if config is None:
    56|                 raise ValueError(
    57|                     "config==None in prepare_database, but databse is not empty"
    58|                 )
    59|             if config.worker_app is not None and user_version != SCHEMA_VERSION:
    60|                 raise UpgradeDatabaseException(
    61|                     OUTDATED_SCHEMA_ON_WORKER_ERROR % (SCHEMA_VERSION, user_version)
    62|                 )
    63|             _upgrade_existing_database(
    64|                 cur,

# --- HUNK 2: Lines 423-451 ---
   423| def execute_statements_from_stream(cur: Cursor, f: TextIO):
   424|     for statement in get_statements(f):
   425|         cur.execute(statement)
   426| def _get_or_create_schema_state(txn, database_engine):
   427|     schema_path = os.path.join(dir_path, "schema", "schema_version.sql")
   428|     executescript(txn, schema_path)
   429|     txn.execute("SELECT version, upgraded FROM schema_version")
   430|     row = txn.fetchone()
   431|     current_version = int(row[0]) if row else None
   432|     upgraded = bool(row[1]) if row else None
   433|     if current_version:
   434|         txn.execute(
   435|             database_engine.convert_param_style(
   436|                 "SELECT file FROM applied_schema_deltas WHERE version >= ?"
   437|             ),
   438|             (current_version,),
   439|         )
   440|         applied_deltas = [d for d, in txn]
   441|         return current_version, applied_deltas, upgraded
   442|     return None
   443| @attr.s()
   444| class _DirectoryListing:
   445|     """Helper class to store schema file name and the
   446|     absolute path to it.
   447|     These entries get sorted, so for consistency we want to ensure that
   448|     `file_name` attr is kept first.
   449|     """
   450|     file_name = attr.ib()
   451|     absolute_path = attr.ib()


# ====================================================================
# FILE: synapse/storage/relations.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| import logging
     2| import attr
     3| from synapse.api.errors import SynapseError
     4| logger = logging.getLogger(__name__)
     5| @attr.s
     6| class PaginationChunk:
     7|     """Returned by relation pagination APIs.
     8|     Attributes:
     9|         chunk (list): The rows returned by pagination
    10|         next_batch (Any|None): Token to fetch next set of results with, if
    11|             None then there are no more results.
    12|         prev_batch (Any|None): Token to fetch previous set of results with, if
    13|             None then there are no previous results.
    14|     """
    15|     chunk = attr.ib()
    16|     next_batch = attr.ib(default=None)
    17|     prev_batch = attr.ib(default=None)
    18|     def to_dict(self):
    19|         d = {"chunk": self.chunk}
    20|         if self.next_batch:
    21|             d["next_batch"] = self.next_batch.to_string()
    22|         if self.prev_batch:
    23|             d["prev_batch"] = self.prev_batch.to_string()
    24|         return d
    25| @attr.s(frozen=True, slots=True)


# ====================================================================
# FILE: synapse/storage/roommember.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-11 ---
     1| import logging
     2| from collections import namedtuple
     3| logger = logging.getLogger(__name__)
     4| RoomsForUser = namedtuple(
     5|     "RoomsForUser", ("room_id", "sender", "membership", "event_id", "stream_ordering")
     6| )
     7| GetRoomsForUserWithStreamOrdering = namedtuple(
     8|     "_GetRoomsForUserWithStreamOrdering", ("room_id", "stream_ordering")
     9| )
    10| ProfileInfo = namedtuple("ProfileInfo", ("avatar_url", "display_name"))
    11| MemberSummary = namedtuple("MemberSummary", ("members", "count"))


# ====================================================================
# FILE: synapse/storage/state.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-26 ---
     1| import logging
     2| from typing import Awaitable, Dict, Iterable, List, Optional, Set, Tuple, TypeVar
     3| import attr
     4| from synapse.api.constants import EventTypes
     5| from synapse.events import EventBase
     6| from synapse.types import StateMap
     7| logger = logging.getLogger(__name__)
     8| T = TypeVar("T")
     9| @attr.s(slots=True)
    10| class StateFilter:
    11|     """A filter used when querying for state.
    12|     Attributes:
    13|         types: Map from type to set of state keys (or None). This specifies
    14|             which state_keys for the given type to fetch from the DB. If None
    15|             then all events with that type are fetched. If the set is empty
    16|             then no events with that type are fetched.
    17|         include_others: Whether to fetch events with types that do not
    18|             appear in `types`.
    19|     """
    20|     types = attr.ib(type=Dict[str, Optional[Set[str]]])
    21|     include_others = attr.ib(default=False, type=bool)
    22|     def __attrs_post_init__(self):
    23|         if self.include_others:
    24|             self.types = {k: v for k, v in self.types.items() if v is not None}
    25|     @staticmethod
    26|     def all() -> "StateFilter":

# --- HUNK 2: Lines 225-265 ---
   225|             include_others=self.include_others,
   226|         )
   227|         return member_filter, non_member_filter
   228| class StateGroupStorage:
   229|     """High level interface to fetching state for event.
   230|     """
   231|     def __init__(self, hs, stores):
   232|         self.stores = stores
   233|     async def get_state_group_delta(self, state_group: int):
   234|         """Given a state group try to return a previous group and a delta between
   235|         the old and the new.
   236|         Args:
   237|             state_group: The state group used to retrieve state deltas.
   238|         Returns:
   239|             Tuple[Optional[int], Optional[StateMap[str]]]:
   240|                 (prev_group, delta_ids)
   241|         """
   242|         return await self.stores.state.get_state_group_delta(state_group)
   243|     async def get_state_groups_ids(
   244|         self, _room_id: str, event_ids: Iterable[str]
   245|     ) -> Dict[int, StateMap[str]]:
   246|         """Get the event IDs of all the state for the state groups for the given events
   247|         Args:
   248|             _room_id: id of the room for these events
   249|             event_ids: ids of the events
   250|         Returns:
   251|             dict of state_group_id -> (dict of (type, state_key) -> event id)
   252|         """
   253|         if not event_ids:
   254|             return {}
   255|         event_to_groups = await self.stores.main._get_state_group_for_events(event_ids)
   256|         groups = set(event_to_groups.values())
   257|         group_to_state = await self.stores.state._get_state_for_groups(groups)
   258|         return group_to_state
   259|     async def get_state_ids_for_group(self, state_group: int) -> StateMap[str]:
   260|         """Get the event IDs of all the state in the given state group
   261|         Args:
   262|             state_group: A state group for which we want to get the state IDs.
   263|         Returns:
   264|             Resolves to a map of (type, state_key) -> event_id
   265|         """

# --- HUNK 3: Lines 369-409 ---
   369|         Returns:
   370|             A dict from (type, state_key) -> state_event
   371|         """
   372|         state_map = await self.get_state_for_events([event_id], state_filter)
   373|         return state_map[event_id]
   374|     async def get_state_ids_for_event(
   375|         self, event_id: str, state_filter: StateFilter = StateFilter.all()
   376|     ):
   377|         """
   378|         Get the state dict corresponding to a particular event
   379|         Args:
   380|             event_id: event whose state should be returned
   381|             state_filter: The state filter used to fetch state from the database.
   382|         Returns:
   383|             A dict from (type, state_key) -> state_event
   384|         """
   385|         state_map = await self.get_state_ids_for_events([event_id], state_filter)
   386|         return state_map[event_id]
   387|     def _get_state_for_groups(
   388|         self, groups: Iterable[int], state_filter: StateFilter = StateFilter.all()
   389|     ) -> Awaitable[Dict[int, StateMap[str]]]:
   390|         """Gets the state at each of a list of state groups, optionally
   391|         filtering by type/state_key
   392|         Args:
   393|             groups: list of state groups for which we want to get the state.
   394|             state_filter: The state filter used to fetch state.
   395|                 from the database.
   396|         Returns:
   397|             Dict of state group to state map.
   398|         """
   399|         return self.stores.state._get_state_for_groups(groups, state_filter)
   400|     async def store_state_group(
   401|         self,
   402|         event_id: str,
   403|         room_id: str,
   404|         prev_group: Optional[int],
   405|         delta_ids: Optional[dict],
   406|         current_state_ids: dict,
   407|     ) -> int:
   408|         """Store a new set of state, returning a newly assigned state group.
   409|         Args:


# ====================================================================
# FILE: synapse/storage/util/id_generators.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| import contextlib
     2| import heapq
     3| import logging
     4| import threading
     5| from collections import deque
     6| from typing import Dict, List, Set
     7| from typing_extensions import Deque
     8| from synapse.storage.database import DatabasePool, LoggingTransaction
     9| from synapse.storage.util.sequence import PostgresSequenceGenerator
    10| logger = logging.getLogger(__name__)
    11| class IdGenerator:
    12|     def __init__(self, db_conn, table, column):
    13|         self._lock = threading.Lock()
    14|         self._next_id = _load_current_id(db_conn, table, column)
    15|     def get_next(self):
    16|         with self._lock:
    17|             self._next_id += 1
    18|             return self._next_id
    19| def _load_current_id(db_conn, table, column, step=1):
    20|     """
    21|     Args:
    22|         db_conn (object):
    23|         table (str):
    24|         column (str):
    25|         step (int):
    26|     Returns:
    27|         int
    28|     """

# --- HUNK 2: Lines 38-299 ---
    38|     return (max if step > 0 else min)(current_id, step)
    39| class StreamIdGenerator:
    40|     """Used to generate new stream ids when persisting events while keeping
    41|     track of which transactions have been completed.
    42|     This allows us to get the "current" stream id, i.e. the stream id such that
    43|     all ids less than or equal to it have completed. This handles the fact that
    44|     persistence of events can complete out of order.
    45|     Args:
    46|         db_conn(connection):  A database connection to use to fetch the
    47|             initial value of the generator from.
    48|         table(str): A database table to read the initial value of the id
    49|             generator from.
    50|         column(str): The column of the database table to read the initial
    51|             value from the id generator from.
    52|         extra_tables(list): List of pairs of database tables and columns to
    53|             use to source the initial value of the generator from. The value
    54|             with the largest magnitude is used.
    55|         step(int): which direction the stream ids grow in. +1 to grow
    56|             upwards, -1 to grow downwards.
    57|     Usage:
    58|         with await stream_id_gen.get_next() as stream_id:
    59|     """
    60|     def __init__(self, db_conn, table, column, extra_tables=[], step=1):
    61|         assert step != 0
    62|         self._lock = threading.Lock()
    63|         self._step = step
    64|         self._current = _load_current_id(db_conn, table, column, step)
    65|         for table, column in extra_tables:
    66|             self._current = (max if step > 0 else min)(
    67|                 self._current, _load_current_id(db_conn, table, column, step)
    68|             )
    69|         self._unfinished_ids = deque()  # type: Deque[int]
    70|     async def get_next(self):
    71|         """
    72|         Usage:
    73|             with await stream_id_gen.get_next() as stream_id:
    74|         """
    75|         with self._lock:
    76|             self._current += self._step
    77|             next_id = self._current
    78|             self._unfinished_ids.append(next_id)
    79|         @contextlib.contextmanager
    80|         def manager():
    81|             try:
    82|                 yield next_id
    83|             finally:
    84|                 with self._lock:
    85|                     self._unfinished_ids.remove(next_id)
    86|         return manager()
    87|     async def get_next_mult(self, n):
    88|         """
    89|         Usage:
    90|             with await stream_id_gen.get_next(n) as stream_ids:
    91|         """
    92|         with self._lock:
    93|             next_ids = range(
    94|                 self._current + self._step,
    95|                 self._current + self._step * (n + 1),
    96|                 self._step,
    97|             )
    98|             self._current += n * self._step
    99|             for next_id in next_ids:
   100|                 self._unfinished_ids.append(next_id)
   101|         @contextlib.contextmanager
   102|         def manager():
   103|             try:
   104|                 yield next_ids
   105|             finally:
   106|                 with self._lock:
   107|                     for next_id in next_ids:
   108|                         self._unfinished_ids.remove(next_id)
   109|         return manager()
   110|     def get_current_token(self):
   111|         """Returns the maximum stream id such that all stream ids less than or
   112|         equal to it have been successfully persisted.
   113|         Returns:
   114|             int
   115|         """
   116|         with self._lock:
   117|             if self._unfinished_ids:
   118|                 return self._unfinished_ids[0] - self._step
   119|             return self._current
   120|     def get_current_token_for_writer(self, instance_name: str) -> int:
   121|         """Returns the position of the given writer.
   122|         For streams with single writers this is equivalent to
   123|         `get_current_token`.
   124|         """
   125|         return self.get_current_token()
   126| class MultiWriterIdGenerator:
   127|     """An ID generator that tracks a stream that can have multiple writers.
   128|     Uses a Postgres sequence to coordinate ID assignment, but positions of other
   129|     writers will only get updated when `advance` is called (by replication).
   130|     Note: Only works with Postgres.
   131|     Args:
   132|         db_conn
   133|         db
   134|         instance_name: The name of this instance.
   135|         table: Database table associated with stream.
   136|         instance_column: Column that stores the row's writer's instance name
   137|         id_column: Column that stores the stream ID.
   138|         sequence_name: The name of the postgres sequence used to generate new
   139|             IDs.
   140|         positive: Whether the IDs are positive (true) or negative (false).
   141|             When using negative IDs we go backwards from -1 to -2, -3, etc.
   142|     """
   143|     def __init__(
   144|         self,
   145|         db_conn,
   146|         db: DatabasePool,
   147|         instance_name: str,
   148|         table: str,
   149|         instance_column: str,
   150|         id_column: str,
   151|         sequence_name: str,
   152|         positive: bool = True,
   153|     ):
   154|         self._db = db
   155|         self._instance_name = instance_name
   156|         self._positive = positive
   157|         self._return_factor = 1 if positive else -1
   158|         self._lock = threading.Lock()
   159|         self._current_positions = self._load_current_ids(
   160|             db_conn, table, instance_column, id_column
   161|         )
   162|         self._unfinished_ids = set()  # type: Set[int]
   163|         self._persisted_upto_position = (
   164|             min(self._current_positions.values()) if self._current_positions else 0
   165|         )
   166|         self._known_persisted_positions = []  # type: List[int]
   167|         self._sequence_gen = PostgresSequenceGenerator(sequence_name)
   168|     def _load_current_ids(
   169|         self, db_conn, table: str, instance_column: str, id_column: str
   170|     ) -> Dict[str, int]:
   171|         sql = """
   172|             SELECT %(instance)s, %(agg)s(%(id)s) FROM %(table)s
   173|             GROUP BY %(instance)s
   174|         """ % {
   175|             "instance": instance_column,
   176|             "id": id_column,
   177|             "table": table,
   178|             "agg": "MAX" if self._positive else "-MIN",
   179|         }
   180|         cur = db_conn.cursor()
   181|         cur.execute(sql)
   182|         current_positions = dict(cur)
   183|         cur.close()
   184|         return current_positions
   185|     def _load_next_id_txn(self, txn) -> int:
   186|         return self._sequence_gen.get_next_id_txn(txn)
   187|     def _load_next_mult_id_txn(self, txn, n: int) -> List[int]:
   188|         return self._sequence_gen.get_next_mult_txn(txn, n)
   189|     async def get_next(self):
   190|         """
   191|         Usage:
   192|             with await stream_id_gen.get_next() as stream_id:
   193|         """
   194|         next_id = await self._db.runInteraction("_load_next_id", self._load_next_id_txn)
   195|         with self._lock:
   196|             assert self._current_positions.get(self._instance_name, 0) < next_id
   197|             self._unfinished_ids.add(next_id)
   198|         @contextlib.contextmanager
   199|         def manager():
   200|             try:
   201|                 yield self._return_factor * next_id
   202|             finally:
   203|                 self._mark_id_as_finished(next_id)
   204|         return manager()
   205|     async def get_next_mult(self, n: int):
   206|         """
   207|         Usage:
   208|             with await stream_id_gen.get_next_mult(5) as stream_ids:
   209|         """
   210|         next_ids = await self._db.runInteraction(
   211|             "_load_next_mult_id", self._load_next_mult_id_txn, n
   212|         )
   213|         with self._lock:
   214|             assert max(self._current_positions.values(), default=0) < min(next_ids)
   215|             self._unfinished_ids.update(next_ids)
   216|         @contextlib.contextmanager
   217|         def manager():
   218|             try:
   219|                 yield [self._return_factor * i for i in next_ids]
   220|             finally:
   221|                 for i in next_ids:
   222|                     self._mark_id_as_finished(i)
   223|         return manager()
   224|     def get_next_txn(self, txn: LoggingTransaction):
   225|         """
   226|         Usage:
   227|             stream_id = stream_id_gen.get_next(txn)
   228|         """
   229|         next_id = self._load_next_id_txn(txn)
   230|         with self._lock:
   231|             self._unfinished_ids.add(next_id)
   232|         txn.call_after(self._mark_id_as_finished, next_id)
   233|         txn.call_on_exception(self._mark_id_as_finished, next_id)
   234|         return self._return_factor * next_id
   235|     def _mark_id_as_finished(self, next_id: int):
   236|         """The ID has finished being processed so we should advance the
   237|         current poistion if possible.
   238|         """
   239|         with self._lock:
   240|             self._unfinished_ids.discard(next_id)
   241|             if all(c > next_id for c in self._unfinished_ids):
   242|                 curr = self._current_positions.get(self._instance_name, 0)
   243|                 self._current_positions[self._instance_name] = max(curr, next_id)
   244|             self._add_persisted_position(next_id)
   245|     def get_current_token(self) -> int:
   246|         """Returns the maximum stream id such that all stream ids less than or
   247|         equal to it have been successfully persisted.
   248|         """
   249|         raise NotImplementedError()
   250|     def get_current_token_for_writer(self, instance_name: str) -> int:
   251|         """Returns the position of the given writer.
   252|         """
   253|         with self._lock:
   254|             return self._return_factor * self._current_positions.get(instance_name, 0)
   255|     def get_positions(self) -> Dict[str, int]:
   256|         """Get a copy of the current positon map.
   257|         """
   258|         with self._lock:
   259|             return {
   260|                 name: self._return_factor * i
   261|                 for name, i in self._current_positions.items()
   262|             }
   263|     def advance(self, instance_name: str, new_id: int):
   264|         """Advance the postion of the named writer to the given ID, if greater
   265|         than existing entry.
   266|         """
   267|         new_id *= self._return_factor
   268|         with self._lock:
   269|             self._current_positions[instance_name] = max(
   270|                 new_id, self._current_positions.get(instance_name, 0)
   271|             )
   272|             self._add_persisted_position(new_id)
   273|     def get_persisted_upto_position(self) -> int:
   274|         """Get the max position where all previous positions have been
   275|         persisted.
   276|         Note: In the worst case scenario this will be equal to the minimum
   277|         position across writers. This means that the returned position here can
   278|         lag if one writer doesn't write very often.
   279|         """
   280|         with self._lock:
   281|             return self._return_factor * self._persisted_upto_position
   282|     def _add_persisted_position(self, new_id: int):
   283|         """Record that we have persisted a position.
   284|         This is used to keep the `_current_positions` up to date.
   285|         """
   286|         assert self._lock.locked()
   287|         heapq.heappush(self._known_persisted_positions, new_id)
   288|         min_curr = min(self._current_positions.values())
   289|         self._persisted_upto_position = max(min_curr, self._persisted_upto_position)
   290|         while self._known_persisted_positions:
   291|             if self._known_persisted_positions[0] <= self._persisted_upto_position:
   292|                 heapq.heappop(self._known_persisted_positions)
   293|             elif (
   294|                 self._known_persisted_positions[0] == self._persisted_upto_position + 1
   295|             ):
   296|                 heapq.heappop(self._known_persisted_positions)
   297|                 self._persisted_upto_position += 1
   298|             else:
   299|                 break


# ====================================================================
# FILE: synapse/storage/util/sequence.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-64 ---
     1| import abc
     2| import threading
     3| from typing import Callable, List, Optional
     4| from synapse.storage.engines import BaseDatabaseEngine, PostgresEngine
     5| from synapse.storage.types import Cursor
     6| class SequenceGenerator(metaclass=abc.ABCMeta):
     7|     """A class which generates a unique sequence of integers"""
     8|     @abc.abstractmethod
     9|     def get_next_id_txn(self, txn: Cursor) -> int:
    10|         """Gets the next ID in the sequence"""
    11|         ...
    12| class PostgresSequenceGenerator(SequenceGenerator):
    13|     """An implementation of SequenceGenerator which uses a postgres sequence"""
    14|     def __init__(self, sequence_name: str):
    15|         self._sequence_name = sequence_name
    16|     def get_next_id_txn(self, txn: Cursor) -> int:
    17|         txn.execute("SELECT nextval(?)", (self._sequence_name,))
    18|         return txn.fetchone()[0]
    19|     def get_next_mult_txn(self, txn: Cursor, n: int) -> List[int]:
    20|         txn.execute(
    21|             "SELECT nextval(?) FROM generate_series(1, ?)", (self._sequence_name, n)
    22|         )
    23|         return [i for (i,) in txn]
    24| GetFirstCallbackType = Callable[[Cursor], int]
    25| class LocalSequenceGenerator(SequenceGenerator):
    26|     """An implementation of SequenceGenerator which uses local locking
    27|     This only works reliably if there are no other worker processes generating IDs at
    28|     the same time.
    29|     """
    30|     def __init__(self, get_first_callback: GetFirstCallbackType):
    31|         """
    32|         Args:
    33|             get_first_callback: a callback which is called on the first call to
    34|                  get_next_id_txn; should return the curreent maximum id
    35|         """
    36|         self._callback = get_first_callback  # type: Optional[GetFirstCallbackType]
    37|         self._current_max_id = None  # type: Optional[int]
    38|         self._lock = threading.Lock()
    39|     def get_next_id_txn(self, txn: Cursor) -> int:
    40|         with self._lock:
    41|             if self._current_max_id is None:
    42|                 assert self._callback is not None
    43|                 self._current_max_id = self._callback(txn)
    44|                 self._callback = None
    45|             self._current_max_id += 1
    46|             return self._current_max_id
    47| def build_sequence_generator(
    48|     database_engine: BaseDatabaseEngine,
    49|     get_first_callback: GetFirstCallbackType,
    50|     sequence_name: str,
    51| ) -> SequenceGenerator:
    52|     """Get the best impl of SequenceGenerator available
    53|     This uses PostgresSequenceGenerator on postgres, and a locally-locked impl on
    54|     sqlite.
    55|     Args:
    56|         database_engine: the database engine we are connected to
    57|         get_first_callback: a callback which gets the next sequence ID. Used if
    58|             we're on sqlite.
    59|         sequence_name: the name of a postgres sequence to use.
    60|     """
    61|     if isinstance(database_engine, PostgresEngine):
    62|         return PostgresSequenceGenerator(sequence_name)
    63|     else:
    64|         return LocalSequenceGenerator(get_first_callback)


# ====================================================================
# FILE: synapse/streams/config.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-68 ---
     1| import logging
     2| from synapse.api.errors import SynapseError
     3| from synapse.http.servlet import parse_integer, parse_string
     4| from synapse.types import StreamToken
     5| logger = logging.getLogger(__name__)
     6| MAX_LIMIT = 1000
     7| class SourcePaginationConfig:
     8|     """A configuration object which stores pagination parameters for a
     9|     specific event source."""
    10|     def __init__(self, from_key=None, to_key=None, direction="f", limit=None):
    11|         self.from_key = from_key
    12|         self.to_key = to_key
    13|         self.direction = "f" if direction == "f" else "b"
    14|         self.limit = min(int(limit), MAX_LIMIT) if limit is not None else None
    15|     def __repr__(self):
    16|         return "StreamConfig(from_key=%r, to_key=%r, direction=%r, limit=%r)" % (
    17|             self.from_key,
    18|             self.to_key,
    19|             self.direction,
    20|             self.limit,
    21|         )
    22| class PaginationConfig:
    23|     """A configuration object which stores pagination parameters."""
    24|     def __init__(self, from_token=None, to_token=None, direction="f", limit=None):
    25|         self.from_token = from_token
    26|         self.to_token = to_token
    27|         self.direction = "f" if direction == "f" else "b"
    28|         self.limit = min(int(limit), MAX_LIMIT) if limit is not None else None
    29|     @classmethod
    30|     def from_request(cls, request, raise_invalid_params=True, default_limit=None):
    31|         direction = parse_string(request, "dir", default="f", allowed_values=["f", "b"])
    32|         from_tok = parse_string(request, "from")
    33|         to_tok = parse_string(request, "to")
    34|         try:
    35|             if from_tok == "END":
    36|                 from_tok = None  # For backwards compat.
    37|             elif from_tok:
    38|                 from_tok = StreamToken.from_string(from_tok)
    39|         except Exception:
    40|             raise SynapseError(400, "'from' parameter is invalid")
    41|         try:
    42|             if to_tok:
    43|                 to_tok = StreamToken.from_string(to_tok)
    44|         except Exception:
    45|             raise SynapseError(400, "'to' parameter is invalid")
    46|         limit = parse_integer(request, "limit", default=default_limit)
    47|         if limit and limit < 0:
    48|             raise SynapseError(400, "Limit must be 0 or above")
    49|         try:
    50|             return PaginationConfig(from_tok, to_tok, direction, limit)
    51|         except Exception:
    52|             logger.exception("Failed to create pagination config")
    53|             raise SynapseError(400, "Invalid request.")
    54|     def __repr__(self):
    55|         return ("PaginationConfig(from_tok=%r, to_tok=%r, direction=%r, limit=%r)") % (
    56|             self.from_token,
    57|             self.to_token,
    58|             self.direction,
    59|             self.limit,
    60|         )
    61|     def get_source_config(self, source_name):
    62|         keyname = "%s_key" % source_name
    63|         return SourcePaginationConfig(
    64|             from_key=getattr(self.from_token, keyname),
    65|             to_key=getattr(self.to_token, keyname) if self.to_token else None,
    66|             direction=self.direction,
    67|             limit=self.limit,
    68|         )


# ====================================================================
# FILE: synapse/types.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-30 ---
     1| import abc
     2| import re
     3| import string
     4| import sys
     5| from collections import namedtuple
     6| from typing import Any, Dict, Mapping, MutableMapping, Tuple, Type, TypeVar
     7| import attr
     8| from signedjson.key import decode_verify_key_bytes
     9| from unpaddedbase64 import decode_base64
    10| from synapse.api.errors import Codes, SynapseError
    11| if sys.version_info[:3] >= (3, 6, 0):
    12|     from typing import Collection
    13| else:
    14|     from typing import Container, Iterable, Sized
    15|     T_co = TypeVar("T_co", covariant=True)
    16|     class Collection(Iterable[T_co], Container[T_co], Sized):  # type: ignore
    17|         __slots__ = ()
    18| T = TypeVar("T")
    19| StateKey = Tuple[str, str]
    20| StateMap = Mapping[StateKey, T]
    21| MutableStateMap = MutableMapping[StateKey, T]
    22| JsonDict = Dict[str, Any]
    23| class Requester(
    24|     namedtuple(
    25|         "Requester",
    26|         [
    27|             "user",
    28|             "access_token_id",
    29|             "is_guest",
    30|             "shadow_banned",

# --- HUNK 2: Lines 99-146 ---
    99|         app_service (ApplicationService|None):  the AS requesting on behalf of the user
   100|     Returns:
   101|         Requester
   102|     """
   103|     if not isinstance(user_id, UserID):
   104|         user_id = UserID.from_string(user_id)
   105|     return Requester(
   106|         user_id, access_token_id, is_guest, shadow_banned, device_id, app_service
   107|     )
   108| def get_domain_from_id(string):
   109|     idx = string.find(":")
   110|     if idx == -1:
   111|         raise SynapseError(400, "Invalid ID: %r" % (string,))
   112|     return string[idx + 1 :]
   113| def get_localpart_from_id(string):
   114|     idx = string.find(":")
   115|     if idx == -1:
   116|         raise SynapseError(400, "Invalid ID: %r" % (string,))
   117|     return string[1:idx]
   118| DS = TypeVar("DS", bound="DomainSpecificString")
   119| class DomainSpecificString(namedtuple("DomainSpecificString", ("localpart", "domain"))):
   120|     """Common base class among ID/name strings that have a local part and a
   121|     domain name, prefixed with a sigil.
   122|     Has the fields:
   123|         'localpart' : The local part of the name (without the leading sigil)
   124|         'domain' : The domain part of the name
   125|     """
   126|     __metaclass__ = abc.ABCMeta
   127|     SIGIL = abc.abstractproperty()  # type: str  # type: ignore
   128|     def __iter__(self):
   129|         raise ValueError("Attempted to iterate a %s" % (type(self).__name__,))
   130|     def __copy__(self):
   131|         return self
   132|     def __deepcopy__(self, memo):
   133|         return self
   134|     @classmethod
   135|     def from_string(cls: Type[DS], s: str) -> DS:
   136|         """Parse the string given by 's' into a structure object."""
   137|         if len(s) < 1 or s[0:1] != cls.SIGIL:
   138|             raise SynapseError(
   139|                 400,
   140|                 "Expected %s string to start with '%s'" % (cls.__name__, cls.SIGIL),
   141|                 Codes.INVALID_PARAM,
   142|             )
   143|         parts = s[1:].split(":", 1)
   144|         if len(parts) != 2:
   145|             raise SynapseError(
   146|                 400,

# --- HUNK 3: Lines 215-362 ---
   215|             onto different mxids
   216|     Returns:
   217|         unicode: string suitable for a mxid localpart
   218|     """
   219|     if not isinstance(username, bytes):
   220|         username = username.encode("utf-8")
   221|     if case_sensitive:
   222|         def f1(m):
   223|             return b"_" + m.group().lower()
   224|         username = UPPER_CASE_PATTERN.sub(f1, username)
   225|     else:
   226|         username = username.lower()
   227|     def f2(m):
   228|         g = m.group()[0]
   229|         if isinstance(g, str):
   230|             g = ord(g)
   231|         return b"=%02x" % (g,)
   232|     username = NON_MXID_CHARACTER_PATTERN.sub(f2, username)
   233|     username = re.sub(b"^_", b"=5f", username)
   234|     return username.decode("ascii")
   235| class StreamToken(
   236|     namedtuple(
   237|         "Token",
   238|         (
   239|             "room_key",
   240|             "presence_key",
   241|             "typing_key",
   242|             "receipt_key",
   243|             "account_data_key",
   244|             "push_rules_key",
   245|             "to_device_key",
   246|             "device_list_key",
   247|             "groups_key",
   248|         ),
   249|     )
   250| ):
   251|     _SEPARATOR = "_"
   252|     START = None  # type: StreamToken
   253|     @classmethod
   254|     def from_string(cls, string):
   255|         try:
   256|             keys = string.split(cls._SEPARATOR)
   257|             while len(keys) < len(cls._fields):
   258|                 keys.append("0")
   259|             return cls(*keys)
   260|         except Exception:
   261|             raise SynapseError(400, "Invalid Token")
   262|     def to_string(self):
   263|         return self._SEPARATOR.join([str(k) for k in self])
   264|     @property
   265|     def room_stream_id(self):
   266|         if type(self.room_key) is int:
   267|             return self.room_key
   268|         else:
   269|             return int(self.room_key[1:].split("-")[-1])
   270|     def is_after(self, other):
   271|         """Does this token contain events that the other doesn't?"""
   272|         return (
   273|             (other.room_stream_id < self.room_stream_id)
   274|             or (int(other.presence_key) < int(self.presence_key))
   275|             or (int(other.typing_key) < int(self.typing_key))
   276|             or (int(other.receipt_key) < int(self.receipt_key))
   277|             or (int(other.account_data_key) < int(self.account_data_key))
   278|             or (int(other.push_rules_key) < int(self.push_rules_key))
   279|             or (int(other.to_device_key) < int(self.to_device_key))
   280|             or (int(other.device_list_key) < int(self.device_list_key))
   281|             or (int(other.groups_key) < int(self.groups_key))
   282|         )
   283|     def copy_and_advance(self, key, new_value):
   284|         """Advance the given key in the token to a new value if and only if the
   285|         new value is after the old value.
   286|         """
   287|         new_token = self.copy_and_replace(key, new_value)
   288|         if key == "room_key":
   289|             new_id = new_token.room_stream_id
   290|             old_id = self.room_stream_id
   291|         else:
   292|             new_id = int(getattr(new_token, key))
   293|             old_id = int(getattr(self, key))
   294|         if old_id < new_id:
   295|             return new_token
   296|         else:
   297|             return self
   298|     def copy_and_replace(self, key, new_value):
   299|         return self._replace(**{key: new_value})
   300| StreamToken.START = StreamToken(*(["s0"] + ["0"] * (len(StreamToken._fields) - 1)))
   301| class RoomStreamToken(namedtuple("_StreamToken", "topological stream")):
   302|     """Tokens are positions between events. The token "s1" comes after event 1.
   303|             s0    s1
   304|             |     |
   305|         [0] V [1] V [2]
   306|     Tokens can either be a point in the live event stream or a cursor going
   307|     through historic events.
   308|     When traversing the live event stream events are ordered by when they
   309|     arrived at the homeserver.
   310|     When traversing historic events the events are ordered by their depth in
   311|     the event graph "topological_ordering" and then by when they arrived at the
   312|     homeserver "stream_ordering".
   313|     Live tokens start with an "s" followed by the "stream_ordering" id of the
   314|     event it comes after. Historic tokens start with a "t" followed by the
   315|     "topological_ordering" id of the event it comes after, followed by "-",
   316|     followed by the "stream_ordering" id of the event it comes after.
   317|     """
   318|     __slots__ = []  # type: list
   319|     @classmethod
   320|     def parse(cls, string):
   321|         try:
   322|             if string[0] == "s":
   323|                 return cls(topological=None, stream=int(string[1:]))
   324|             if string[0] == "t":
   325|                 parts = string[1:].split("-", 1)
   326|                 return cls(topological=int(parts[0]), stream=int(parts[1]))
   327|         except Exception:
   328|             pass
   329|         raise SynapseError(400, "Invalid token %r" % (string,))
   330|     @classmethod
   331|     def parse_stream_token(cls, string):
   332|         try:
   333|             if string[0] == "s":
   334|                 return cls(topological=None, stream=int(string[1:]))
   335|         except Exception:
   336|             pass
   337|         raise SynapseError(400, "Invalid token %r" % (string,))
   338|     def __str__(self):
   339|         if self.topological is not None:
   340|             return "t%d-%d" % (self.topological, self.stream)
   341|         else:
   342|             return "s%d" % (self.stream,)
   343| class ThirdPartyInstanceID(
   344|     namedtuple("ThirdPartyInstanceID", ("appservice_id", "network_id"))
   345| ):
   346|     def __iter__(self):
   347|         raise ValueError("Attempted to iterate a %s" % (type(self).__name__,))
   348|     def __copy__(self):
   349|         return self
   350|     def __deepcopy__(self, memo):
   351|         return self
   352|     @classmethod
   353|     def from_string(cls, s):
   354|         bits = s.split("|", 2)
   355|         if len(bits) != 2:
   356|             raise SynapseError(400, "Invalid ID %r" % (s,))
   357|         return cls(appservice_id=bits[0], network_id=bits[1])
   358|     def to_string(self):
   359|         return "%s|%s" % (self.appservice_id, self.network_id)
   360|     __str__ = to_string
   361|     @classmethod
   362|     def create(cls, appservice_id, network_id):


# ====================================================================
# FILE: synapse/util/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-36 ---
     1| import logging
     2| import re
     3| import attr
     4| from canonicaljson import json
     5| from twisted.internet import defer, task
     6| from synapse.logging import context
     7| logger = logging.getLogger(__name__)
     8| def _reject_invalid_json(val):
     9|     """Do not allow Infinity, -Infinity, or NaN values in JSON."""
    10|     raise ValueError("Invalid JSON value: '%s'" % val)
    11| json_encoder = json.JSONEncoder(allow_nan=False, separators=(",", ":"))
    12| json_decoder = json.JSONDecoder(parse_constant=_reject_invalid_json)
    13| def unwrapFirstError(failure):
    14|     failure.trap(defer.FirstError)
    15|     return failure.value.subFailure
    16| @attr.s
    17| class Clock:
    18|     """
    19|     A Clock wraps a Twisted reactor and provides utilities on top of it.
    20|     Args:
    21|         reactor: The Twisted reactor to use.
    22|     """
    23|     _reactor = attr.ib()
    24|     @defer.inlineCallbacks
    25|     def sleep(self, seconds):
    26|         d = defer.Deferred()
    27|         with context.PreserveLoggingContext():
    28|             self._reactor.callLater(seconds, d.callback, seconds)
    29|             res = yield d
    30|         return res
    31|     def time(self):
    32|         """Returns the current system time in seconds since epoch."""
    33|         return self._reactor.seconds()
    34|     def time_msec(self):
    35|         """Returns the current system time in milliseconds since epoch."""
    36|         return int(self.time() * 1000)


# ====================================================================
# FILE: synapse/util/async_helpers.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-50 ---
     1| import collections
     2| import logging
     3| from contextlib import contextmanager
     4| from typing import Dict, Sequence, Set, Union
     5| import attr
     6| from typing_extensions import ContextManager
     7| from twisted.internet import defer
     8| from twisted.internet.defer import CancelledError
     9| from twisted.python import failure
    10| from synapse.logging.context import (
    11|     PreserveLoggingContext,
    12|     make_deferred_yieldable,
    13|     run_in_background,
    14| )
    15| from synapse.util import Clock, unwrapFirstError
    16| logger = logging.getLogger(__name__)
    17| class ObservableDeferred:
    18|     """Wraps a deferred object so that we can add observer deferreds. These
    19|     observer deferreds do not affect the callback chain of the original
    20|     deferred.
    21|     If consumeErrors is true errors will be captured from the origin deferred.
    22|     Cancelling or otherwise resolving an observer will not affect the original
    23|     ObservableDeferred.
    24|     NB that it does not attempt to do anything with logcontexts; in general
    25|     you should probably make_deferred_yieldable the deferreds
    26|     returned by `observe`, and ensure that the original deferred runs its
    27|     callbacks in the sentinel logcontext.
    28|     """
    29|     __slots__ = ["_deferred", "_observers", "_result"]
    30|     def __init__(self, deferred, consumeErrors=False):
    31|         object.__setattr__(self, "_deferred", deferred)
    32|         object.__setattr__(self, "_result", None)
    33|         object.__setattr__(self, "_observers", set())
    34|         def callback(r):
    35|             object.__setattr__(self, "_result", (True, r))
    36|             while self._observers:
    37|                 try:
    38|                     self._observers.pop().callback(r)
    39|                 except Exception:
    40|                     pass
    41|             return r
    42|         def errback(f):
    43|             object.__setattr__(self, "_result", (False, f))
    44|             while self._observers:
    45|                 f.value.__failure__ = f
    46|                 try:
    47|                     self._observers.pop().errback(f)
    48|                 except Exception:
    49|                     pass
    50|             if consumeErrors:

# --- HUNK 2: Lines 52-233 ---
    52|             else:
    53|                 return f
    54|         deferred.addCallbacks(callback, errback)
    55|     def observe(self) -> defer.Deferred:
    56|         """Observe the underlying deferred.
    57|         This returns a brand new deferred that is resolved when the underlying
    58|         deferred is resolved. Interacting with the returned deferred does not
    59|         effect the underlying deferred.
    60|         """
    61|         if not self._result:
    62|             d = defer.Deferred()
    63|             def remove(r):
    64|                 self._observers.discard(d)
    65|                 return r
    66|             d.addBoth(remove)
    67|             self._observers.add(d)
    68|             return d
    69|         else:
    70|             success, res = self._result
    71|             return defer.succeed(res) if success else defer.fail(res)
    72|     def observers(self):
    73|         return self._observers
    74|     def has_called(self):
    75|         return self._result is not None
    76|     def has_succeeded(self):
    77|         return self._result is not None and self._result[0] is True
    78|     def get_result(self):
    79|         return self._result[1]
    80|     def __getattr__(self, name):
    81|         return getattr(self._deferred, name)
    82|     def __setattr__(self, name, value):
    83|         setattr(self._deferred, name, value)
    84|     def __repr__(self):
    85|         return "<ObservableDeferred object at %s, result=%r, _deferred=%r>" % (
    86|             id(self),
    87|             self._result,
    88|             self._deferred,
    89|         )
    90| def concurrently_execute(func, args, limit):
    91|     """Executes the function with each argument conncurrently while limiting
    92|     the number of concurrent executions.
    93|     Args:
    94|         func (func): Function to execute, should return a deferred or coroutine.
    95|         args (Iterable): List of arguments to pass to func, each invocation of func
    96|             gets a single argument.
    97|         limit (int): Maximum number of conccurent executions.
    98|     Returns:
    99|         deferred: Resolved when all function invocations have finished.
   100|     """
   101|     it = iter(args)
   102|     async def _concurrently_execute_inner():
   103|         try:
   104|             while True:
   105|                 await maybe_awaitable(func(next(it)))
   106|         except StopIteration:
   107|             pass
   108|     return make_deferred_yieldable(
   109|         defer.gatherResults(
   110|             [run_in_background(_concurrently_execute_inner) for _ in range(limit)],
   111|             consumeErrors=True,
   112|         )
   113|     ).addErrback(unwrapFirstError)
   114| def yieldable_gather_results(func, iter, *args, **kwargs):
   115|     """Executes the function with each argument concurrently.
   116|     Args:
   117|         func (func): Function to execute that returns a Deferred
   118|         iter (iter): An iterable that yields items that get passed as the first
   119|             argument to the function
   120|         *args: Arguments to be passed to each call to func
   121|     Returns
   122|         Deferred[list]: Resolved when all functions have been invoked, or errors if
   123|         one of the function calls fails.
   124|     """
   125|     return make_deferred_yieldable(
   126|         defer.gatherResults(
   127|             [run_in_background(func, item, *args, **kwargs) for item in iter],
   128|             consumeErrors=True,
   129|         )
   130|     ).addErrback(unwrapFirstError)
   131| class Linearizer:
   132|     """Limits concurrent access to resources based on a key. Useful to ensure
   133|     only a few things happen at a time on a given resource.
   134|     Example:
   135|         with (yield limiter.queue("test_key")):
   136|     """
   137|     def __init__(self, name=None, max_count=1, clock=None):
   138|         """
   139|         Args:
   140|             max_count(int): The maximum number of concurrent accesses
   141|         """
   142|         if name is None:
   143|             self.name = id(self)
   144|         else:
   145|             self.name = name
   146|         if not clock:
   147|             from twisted.internet import reactor
   148|             clock = Clock(reactor)
   149|         self._clock = clock
   150|         self.max_count = max_count
   151|         self.key_to_defer = (
   152|             {}
   153|         )  # type: Dict[str, Sequence[Union[int, Dict[defer.Deferred, int]]]]
   154|     def is_queued(self, key) -> bool:
   155|         """Checks whether there is a process queued up waiting
   156|         """
   157|         entry = self.key_to_defer.get(key)
   158|         if not entry:
   159|             return False
   160|         return bool(entry[1])
   161|     def queue(self, key):
   162|         entry = self.key_to_defer.setdefault(key, [0, collections.OrderedDict()])
   163|         if entry[0] >= self.max_count:
   164|             res = self._await_lock(key)
   165|         else:
   166|             logger.debug(
   167|                 "Acquired uncontended linearizer lock %r for key %r", self.name, key
   168|             )
   169|             entry[0] += 1
   170|             res = defer.succeed(None)
   171|         @contextmanager
   172|         def _ctx_manager(_):
   173|             try:
   174|                 yield
   175|             finally:
   176|                 logger.debug("Releasing linearizer lock %r for key %r", self.name, key)
   177|                 entry[0] -= 1
   178|                 if entry[1]:
   179|                     (next_def, _) = entry[1].popitem(last=False)
   180|                     with PreserveLoggingContext():
   181|                         next_def.callback(None)
   182|                 elif entry[0] == 0:
   183|                     del self.key_to_defer[key]
   184|         res.addCallback(_ctx_manager)
   185|         return res
   186|     def _await_lock(self, key):
   187|         """Helper for queue: adds a deferred to the queue
   188|         Assumes that we've already checked that we've reached the limit of the number
   189|         of lock-holders we allow. Creates a new deferred which is added to the list, and
   190|         adds some management around cancellations.
   191|         Returns the deferred, which will callback once we have secured the lock.
   192|         """
   193|         entry = self.key_to_defer[key]
   194|         logger.debug("Waiting to acquire linearizer lock %r for key %r", self.name, key)
   195|         new_defer = make_deferred_yieldable(defer.Deferred())
   196|         entry[1][new_defer] = 1
   197|         def cb(_r):
   198|             logger.debug("Acquired linearizer lock %r for key %r", self.name, key)
   199|             entry[0] += 1
   200|             return self._clock.sleep(0)
   201|         def eb(e):
   202|             logger.info("defer %r got err %r", new_defer, e)
   203|             if isinstance(e, CancelledError):
   204|                 logger.debug(
   205|                     "Cancelling wait for linearizer lock %r for key %r", self.name, key
   206|                 )
   207|             else:
   208|                 logger.warning(
   209|                     "Unexpected exception waiting for linearizer lock %r for key %r",
   210|                     self.name,
   211|                     key,
   212|                 )
   213|             del entry[1][new_defer]
   214|             return e
   215|         new_defer.addCallbacks(cb, eb)
   216|         return new_defer
   217| class ReadWriteLock:
   218|     """An async read write lock.
   219|     Example:
   220|         with await read_write_lock.read("test_key"):
   221|     """
   222|     def __init__(self):
   223|         self.key_to_current_readers = {}  # type: Dict[str, Set[defer.Deferred]]
   224|         self.key_to_current_writer = {}  # type: Dict[str, defer.Deferred]
   225|     async def read(self, key: str) -> ContextManager:
   226|         new_defer = defer.Deferred()
   227|         curr_readers = self.key_to_current_readers.setdefault(key, set())
   228|         curr_writer = self.key_to_current_writer.get(key, None)
   229|         curr_readers.add(new_defer)
   230|         if curr_writer:
   231|             await make_deferred_yieldable(curr_writer)
   232|         @contextmanager
   233|         def _ctx_manager():

# --- HUNK 3: Lines 239-322 ---
   239|         return _ctx_manager()
   240|     async def write(self, key: str) -> ContextManager:
   241|         new_defer = defer.Deferred()
   242|         curr_readers = self.key_to_current_readers.get(key, set())
   243|         curr_writer = self.key_to_current_writer.get(key, None)
   244|         to_wait_on = list(curr_readers)
   245|         if curr_writer:
   246|             to_wait_on.append(curr_writer)
   247|         curr_readers.clear()
   248|         self.key_to_current_writer[key] = new_defer
   249|         await make_deferred_yieldable(defer.gatherResults(to_wait_on))
   250|         @contextmanager
   251|         def _ctx_manager():
   252|             try:
   253|                 yield
   254|             finally:
   255|                 new_defer.callback(None)
   256|                 if self.key_to_current_writer[key] == new_defer:
   257|                     self.key_to_current_writer.pop(key)
   258|         return _ctx_manager()
   259| def _cancelled_to_timed_out_error(value, timeout):
   260|     if isinstance(value, failure.Failure):
   261|         value.trap(CancelledError)
   262|         raise defer.TimeoutError(timeout, "Deferred")
   263|     return value
   264| def timeout_deferred(deferred, timeout, reactor, on_timeout_cancel=None):
   265|     """The in built twisted `Deferred.addTimeout` fails to time out deferreds
   266|     that have a canceller that throws exceptions. This method creates a new
   267|     deferred that wraps and times out the given deferred, correctly handling
   268|     the case where the given deferred's canceller throws.
   269|     (See https://twistedmatrix.com/trac/ticket/9534)
   270|     NOTE: Unlike `Deferred.addTimeout`, this function returns a new deferred
   271|     Args:
   272|         deferred (Deferred)
   273|         timeout (float): Timeout in seconds
   274|         reactor (twisted.interfaces.IReactorTime): The twisted reactor to use
   275|         on_timeout_cancel (callable): A callable which is called immediately
   276|             after the deferred times out, and not if this deferred is
   277|             otherwise cancelled before the timeout.
   278|             It takes an arbitrary value, which is the value of the deferred at
   279|             that exact point in time (probably a CancelledError Failure), and
   280|             the timeout.
   281|             The default callable (if none is provided) will translate a
   282|             CancelledError Failure into a defer.TimeoutError.
   283|     Returns:
   284|         Deferred
   285|     """
   286|     new_d = defer.Deferred()
   287|     timed_out = [False]
   288|     def time_it_out():
   289|         timed_out[0] = True
   290|         try:
   291|             deferred.cancel()
   292|         except:  # noqa: E722, if we throw any exception it'll break time outs
   293|             logger.exception("Canceller failed during timeout")
   294|         if not new_d.called:
   295|             new_d.errback(defer.TimeoutError(timeout, "Deferred"))
   296|     delayed_call = reactor.callLater(timeout, time_it_out)
   297|     def convert_cancelled(value):
   298|         if timed_out[0]:
   299|             to_call = on_timeout_cancel or _cancelled_to_timed_out_error
   300|             return to_call(value, timeout)
   301|         return value
   302|     deferred.addBoth(convert_cancelled)
   303|     def cancel_timeout(result):
   304|         if delayed_call.active():
   305|             delayed_call.cancel()
   306|         return result
   307|     deferred.addBoth(cancel_timeout)
   308|     def success_cb(val):
   309|         if not new_d.called:
   310|             new_d.callback(val)
   311|     def failure_cb(val):
   312|         if not new_d.called:
   313|             new_d.errback(val)
   314|     deferred.addCallbacks(success_cb, failure_cb)
   315|     return new_d
   316| @attr.s(slots=True, frozen=True)
   317| class DoneAwaitable:
   318|     """Simple awaitable that returns the provided value.
   319|     """
   320|     value = attr.ib()
   321|     def __await__(self):
   322|         return self


# ====================================================================
# FILE: synapse/util/caches/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-41 ---
     1| import logging
     2| from sys import intern
     3| from typing import Callable, Dict, Optional
     4| import attr
     5| from prometheus_client.core import Gauge
     6| from synapse.config.cache import add_resizable_cache
     7| logger = logging.getLogger(__name__)
     8| caches_by_name = {}
     9| collectors_by_name = {}  # type: Dict
    10| cache_size = Gauge("synapse_util_caches_cache:size", "", ["name"])
    11| cache_hits = Gauge("synapse_util_caches_cache:hits", "", ["name"])
    12| cache_evicted = Gauge("synapse_util_caches_cache:evicted_size", "", ["name"])
    13| cache_total = Gauge("synapse_util_caches_cache:total", "", ["name"])
    14| cache_max_size = Gauge("synapse_util_caches_cache_max_size", "", ["name"])
    15| response_cache_size = Gauge("synapse_util_caches_response_cache:size", "", ["name"])
    16| response_cache_hits = Gauge("synapse_util_caches_response_cache:hits", "", ["name"])
    17| response_cache_evicted = Gauge(
    18|     "synapse_util_caches_response_cache:evicted_size", "", ["name"]
    19| )
    20| response_cache_total = Gauge("synapse_util_caches_response_cache:total", "", ["name"])
    21| @attr.s
    22| class CacheMetric:
    23|     _cache = attr.ib()
    24|     _cache_type = attr.ib(type=str)
    25|     _cache_name = attr.ib(type=str)
    26|     _collect_callback = attr.ib(type=Optional[Callable])
    27|     hits = attr.ib(default=0)
    28|     misses = attr.ib(default=0)
    29|     evicted_size = attr.ib(default=0)
    30|     def inc_hits(self):
    31|         self.hits += 1
    32|     def inc_misses(self):
    33|         self.misses += 1
    34|     def inc_evictions(self, size=1):
    35|         self.evicted_size += size
    36|     def describe(self):
    37|         return []
    38|     def collect(self):
    39|         try:
    40|             if self._cache_type == "response_cache":
    41|                 response_cache_size.labels(self._cache_name).set(len(self._cache))


# ====================================================================
# FILE: synapse/util/distributor.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-104 ---
     1| import inspect
     2| import logging
     3| from twisted.internet import defer
     4| from twisted.internet.defer import Deferred, fail, succeed
     5| from twisted.python import failure
     6| from synapse.logging.context import make_deferred_yieldable, run_in_background
     7| from synapse.metrics.background_process_metrics import run_as_background_process
     8| logger = logging.getLogger(__name__)
     9| def user_left_room(distributor, user, room_id):
    10|     distributor.fire("user_left_room", user=user, room_id=room_id)
    11| def user_joined_room(distributor, user, room_id):
    12|     distributor.fire("user_joined_room", user=user, room_id=room_id)
    13| class Distributor:
    14|     """A central dispatch point for loosely-connected pieces of code to
    15|     register, observe, and fire signals.
    16|     Signals are named simply by strings.
    17|     TODO(paul): It would be nice to give signals stronger object identities,
    18|       so we can attach metadata, docstrings, detect typos, etc... But this
    19|       model will do for today.
    20|     """
    21|     def __init__(self):
    22|         self.signals = {}
    23|         self.pre_registration = {}
    24|     def declare(self, name):
    25|         if name in self.signals:
    26|             raise KeyError("%r already has a signal named %s" % (self, name))
    27|         self.signals[name] = Signal(name)
    28|         if name in self.pre_registration:
    29|             signal = self.signals[name]
    30|             for observer in self.pre_registration[name]:
    31|                 signal.observe(observer)
    32|     def observe(self, name, observer):
    33|         if name in self.signals:
    34|             self.signals[name].observe(observer)
    35|         else:
    36|             if name not in self.pre_registration:
    37|                 self.pre_registration[name] = []
    38|             self.pre_registration[name].append(observer)
    39|     def fire(self, name, *args, **kwargs):
    40|         """Dispatches the given signal to the registered observers.
    41|         Runs the observers as a background process. Does not return a deferred.
    42|         """
    43|         if name not in self.signals:
    44|             raise KeyError("%r does not have a signal named %s" % (self, name))
    45|         run_as_background_process(name, self.signals[name].fire, *args, **kwargs)
    46| def maybeAwaitableDeferred(f, *args, **kw):
    47|     """
    48|     Invoke a function that may or may not return a Deferred or an Awaitable.
    49|     This is a modified version of twisted.internet.defer.maybeDeferred.
    50|     """
    51|     try:
    52|         result = f(*args, **kw)
    53|     except Exception:
    54|         return fail(failure.Failure(captureVars=Deferred.debug))
    55|     if isinstance(result, Deferred):
    56|         return result
    57|     elif inspect.isawaitable(result):
    58|         return defer.ensureDeferred(result)
    59|     elif isinstance(result, failure.Failure):
    60|         return fail(result)
    61|     else:
    62|         return succeed(result)
    63| class Signal:
    64|     """A Signal is a dispatch point that stores a list of callables as
    65|     observers of it.
    66|     Signals can be "fired", meaning that every callable observing it is
    67|     invoked. Firing a signal does not change its state; it can be fired again
    68|     at any later point. Firing a signal passes any arguments from the fire
    69|     method into all of the observers.
    70|     """
    71|     def __init__(self, name):
    72|         self.name = name
    73|         self.observers = []
    74|     def observe(self, observer):
    75|         """Adds a new callable to the observer list which will be invoked by
    76|         the 'fire' method.
    77|         Each observer callable may return a Deferred."""
    78|         self.observers.append(observer)
    79|     def fire(self, *args, **kwargs):
    80|         """Invokes every callable in the observer list, passing in the args and
    81|         kwargs. Exceptions thrown by observers are logged but ignored. It is
    82|         not an error to fire a signal with no observers.
    83|         Returns a Deferred that will complete when all the observers have
    84|         completed."""
    85|         def do(observer):
    86|             def eb(failure):
    87|                 logger.warning(
    88|                     "%s signal observer %s failed: %r",
    89|                     self.name,
    90|                     observer,
    91|                     failure,
    92|                     exc_info=(
    93|                         failure.type,
    94|                         failure.value,
    95|                         failure.getTracebackObject(),
    96|                     ),
    97|                 )
    98|             return maybeAwaitableDeferred(observer, *args, **kwargs).addErrback(eb)
    99|         deferreds = [run_in_background(do, o) for o in self.observers]
   100|         return make_deferred_yieldable(
   101|             defer.gatherResults(deferreds, consumeErrors=True)
   102|         )
   103|     def __repr__(self):
   104|         return "<Signal name=%r>" % (self.name,)


# ====================================================================
# FILE: synapse/util/frozenutils.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-36 ---
     1| from canonicaljson import json
     2| from frozendict import frozendict
     3| def freeze(o):
     4|     if isinstance(o, dict):
     5|         return frozendict({k: freeze(v) for k, v in o.items()})
     6|     if isinstance(o, frozendict):
     7|         return o
     8|     if isinstance(o, (bytes, str)):
     9|         return o
    10|     try:
    11|         return tuple(freeze(i) for i in o)
    12|     except TypeError:
    13|         pass
    14|     return o
    15| def unfreeze(o):
    16|     if isinstance(o, (dict, frozendict)):
    17|         return dict({k: unfreeze(v) for k, v in o.items()})
    18|     if isinstance(o, (bytes, str)):
    19|         return o
    20|     try:
    21|         return [unfreeze(i) for i in o]
    22|     except TypeError:
    23|         pass
    24|     return o
    25| def _handle_frozendict(obj):
    26|     """Helper for EventEncoder. Makes frozendicts serializable by returning
    27|     the underlying dict
    28|     """
    29|     if type(obj) is frozendict:
    30|         return obj._dict
    31|     raise TypeError(
    32|         "Object of type %s is not JSON serializable" % obj.__class__.__name__
    33|     )
    34| frozendict_json_encoder = json.JSONEncoder(
    35|     default=_handle_frozendict, separators=(",", ":"),
    36| )


# ====================================================================
# FILE: synapse/util/manhole.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 49-89 ---
    49|         username(str): The username ssh clients should auth with.
    50|         password(str): The password ssh clients should auth with.
    51|         globals(dict): The variables to expose in the shell.
    52|     Returns:
    53|         twisted.internet.protocol.Factory: A factory to pass to ``listenTCP``
    54|     """
    55|     if not isinstance(password, bytes):
    56|         password = password.encode("ascii")
    57|     checker = checkers.InMemoryUsernamePasswordDatabaseDontUse(**{username: password})
    58|     rlm = manhole_ssh.TerminalRealm()
    59|     rlm.chainedProtocolFactory = lambda: insults.ServerProtocol(
    60|         SynapseManhole, dict(globals, __name__="__console__")
    61|     )
    62|     factory = manhole_ssh.ConchFactory(portal.Portal(rlm, [checker]))
    63|     factory.publicKeys[b"ssh-rsa"] = Key.fromString(PUBLIC_KEY)
    64|     factory.privateKeys[b"ssh-rsa"] = Key.fromString(PRIVATE_KEY)
    65|     return factory
    66| class SynapseManhole(ColoredManhole):
    67|     """Overrides connectionMade to create our own ManholeInterpreter"""
    68|     def connectionMade(self):
    69|         super(SynapseManhole, self).connectionMade()
    70|         self.interpreter = SynapseManholeInterpreter(self, self.namespace)
    71| class SynapseManholeInterpreter(ManholeInterpreter):
    72|     def showsyntaxerror(self, filename=None):
    73|         """Display the syntax error that just occurred.
    74|         Overrides the base implementation, ignoring sys.excepthook. We always want
    75|         any syntax errors to be sent to the terminal, rather than sentry.
    76|         """
    77|         type, value, tb = sys.exc_info()
    78|         sys.last_type = type
    79|         sys.last_value = value
    80|         sys.last_traceback = tb
    81|         if filename and type is SyntaxError:
    82|             try:
    83|                 msg, (dummy_filename, lineno, offset, line) = value.args
    84|             except ValueError:
    85|                 pass
    86|             else:
    87|                 value = SyntaxError(msg, (filename, lineno, offset, line))
    88|                 sys.last_value = value
    89|         lines = traceback.format_exception_only(type, value)


# ====================================================================
# FILE: synapse/util/metrics.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| import logging
     2| from functools import wraps
     3| from typing import Any, Callable, Optional, TypeVar, cast
     4| from prometheus_client import Counter
     5| from synapse.logging.context import LoggingContext, current_context
     6| from synapse.metrics import InFlightGauge
     7| logger = logging.getLogger(__name__)
     8| block_counter = Counter("synapse_util_metrics_block_count", "", ["block_name"])
     9| block_timer = Counter("synapse_util_metrics_block_time_seconds", "", ["block_name"])
    10| block_ru_utime = Counter(
    11|     "synapse_util_metrics_block_ru_utime_seconds", "", ["block_name"]
    12| )
    13| block_ru_stime = Counter(
    14|     "synapse_util_metrics_block_ru_stime_seconds", "", ["block_name"]
    15| )
    16| block_db_txn_count = Counter(
    17|     "synapse_util_metrics_block_db_txn_count", "", ["block_name"]
    18| )
    19| block_db_txn_duration = Counter(
    20|     "synapse_util_metrics_block_db_txn_duration_seconds", "", ["block_name"]
    21| )
    22| block_db_sched_duration = Counter(
    23|     "synapse_util_metrics_block_db_sched_duration_seconds", "", ["block_name"]
    24| )
    25| in_flight = InFlightGauge(

# --- HUNK 2: Lines 43-97 ---
    43|     """
    44|     def wrapper(func: T) -> T:
    45|         block_name = func.__name__ if name is None else name
    46|         @wraps(func)
    47|         async def measured_func(self, *args, **kwargs):
    48|             with Measure(self.clock, block_name):
    49|                 r = await func(self, *args, **kwargs)
    50|             return r
    51|         return cast(T, measured_func)
    52|     return wrapper
    53| class Measure:
    54|     __slots__ = [
    55|         "clock",
    56|         "name",
    57|         "_logging_context",
    58|         "start",
    59|     ]
    60|     def __init__(self, clock, name):
    61|         self.clock = clock
    62|         self.name = name
    63|         self._logging_context = None
    64|         self.start = None
    65|     def __enter__(self):
    66|         if self._logging_context:
    67|             raise RuntimeError("Measure() objects cannot be re-used")
    68|         self.start = self.clock.time()
    69|         parent_context = current_context()
    70|         self._logging_context = LoggingContext(
    71|             "Measure[%s]" % (self.name,), parent_context
    72|         )
    73|         self._logging_context.__enter__()
    74|         in_flight.register((self.name,), self._update_in_flight)
    75|     def __exit__(self, exc_type, exc_val, exc_tb):
    76|         if not self._logging_context:
    77|             raise RuntimeError("Measure() block exited without being entered")
    78|         duration = self.clock.time() - self.start
    79|         usage = self._logging_context.get_resource_usage()
    80|         in_flight.unregister((self.name,), self._update_in_flight)
    81|         self._logging_context.__exit__(exc_type, exc_val, exc_tb)
    82|         try:
    83|             block_counter.labels(self.name).inc()
    84|             block_timer.labels(self.name).inc(duration)
    85|             block_ru_utime.labels(self.name).inc(usage.ru_utime)
    86|             block_ru_stime.labels(self.name).inc(usage.ru_stime)
    87|             block_db_txn_count.labels(self.name).inc(usage.db_txn_count)
    88|             block_db_txn_duration.labels(self.name).inc(usage.db_txn_duration_sec)
    89|             block_db_sched_duration.labels(self.name).inc(usage.db_sched_duration_sec)
    90|         except ValueError:
    91|             logger.warning("Failed to save metrics! Usage: %s", usage)
    92|     def _update_in_flight(self, metrics):
    93|         """Gets called when processing in flight metrics
    94|         """
    95|         duration = self.clock.time() - self.start
    96|         metrics.real_time_max = max(metrics.real_time_max, duration)
    97|         metrics.real_time_sum += duration


# ====================================================================
# FILE: synapse/util/patch_inline_callbacks.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-21 ---
     1| from __future__ import print_function
     2| import functools
     3| import sys
     4| from typing import Any, Callable, List
     5| from twisted.internet import defer
     6| from twisted.internet.defer import Deferred
     7| from twisted.python.failure import Failure
     8| _already_patched = False
     9| def do_patch():
    10|     """
    11|     Patch defer.inlineCallbacks so that it checks the state of the logcontext on exit
    12|     """
    13|     from synapse.logging.context import current_context
    14|     global _already_patched
    15|     orig_inline_callbacks = defer.inlineCallbacks
    16|     if _already_patched:
    17|         return
    18|     def new_inline_callbacks(f):
    19|         @functools.wraps(f)
    20|         def wrapped(*args, **kwargs):
    21|             start_context = current_context()


# ====================================================================
# FILE: synapse/util/retryutils.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 2-42 ---
     2| import random
     3| import synapse.logging.context
     4| from synapse.api.errors import CodeMessageException
     5| logger = logging.getLogger(__name__)
     6| MIN_RETRY_INTERVAL = 10 * 60 * 1000
     7| RETRY_MULTIPLIER = 5
     8| MAX_RETRY_INTERVAL = 2 ** 62
     9| class NotRetryingDestination(Exception):
    10|     def __init__(self, retry_last_ts, retry_interval, destination):
    11|         """Raised by the limiter (and federation client) to indicate that we are
    12|         are deliberately not attempting to contact a given server.
    13|         Args:
    14|             retry_last_ts (int): the unix ts in milliseconds of our last attempt
    15|                 to contact the server.  0 indicates that the last attempt was
    16|                 successful or that we've never actually attempted to connect.
    17|             retry_interval (int): the time in milliseconds to wait until the next
    18|                 attempt.
    19|             destination (str): the domain in question
    20|         """
    21|         msg = "Not retrying server %s." % (destination,)
    22|         super(NotRetryingDestination, self).__init__(msg)
    23|         self.retry_last_ts = retry_last_ts
    24|         self.retry_interval = retry_interval
    25|         self.destination = destination
    26| async def get_retry_limiter(destination, clock, store, ignore_backoff=False, **kwargs):
    27|     """For a given destination check if we have previously failed to
    28|     send a request there and are waiting before retrying the destination.
    29|     If we are not ready to retry the destination, this will raise a
    30|     NotRetryingDestination exception. Otherwise, will return a Context Manager
    31|     that will mark the destination as down if an exception is thrown (excluding
    32|     CodeMessageException with code < 500)
    33|     Args:
    34|         destination (str): name of homeserver
    35|         clock (synapse.util.clock): timing source
    36|         store (synapse.storage.transactions.TransactionStore): datastore
    37|         ignore_backoff (bool): true to ignore the historical backoff data and
    38|             try the request anyway. We will still reset the retry_interval on success.
    39|     Example usage:
    40|         try:
    41|             limiter = await get_retry_limiter(destination, clock, store)
    42|             with limiter:

