# ====================================================================
# FILE: contrib/cmdclient/console.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-21 ---
     1| """ Starts a synapse client console. """
     2| import argparse
     3| import cmd
     4| import getpass
     5| import json
     6| import shlex
     7| import sys
     8| import time
     9| import urllib
    10| from http import TwistedHttpClient
    11| import nacl.encoding
    12| import nacl.signing
    13| import urlparse
    14| from signedjson.sign import SignatureVerifyException, verify_signed_json
    15| from twisted.internet import defer, reactor, threads
    16| CONFIG_JSON = "cmdclient_config.json"
    17| TRUSTED_ID_SERVERS = ["localhost:8001"]
    18| class SynapseCmd(cmd.Cmd):
    19|     """Basic synapse command-line processor.
    20|     This processes commands from the user and calls the relevant HTTP methods.
    21|     """


# ====================================================================
# FILE: contrib/cmdclient/http.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-20 ---
     1| import json
     2| import urllib
     3| from pprint import pformat
     4| from twisted.internet import defer, reactor
     5| from twisted.web.client import Agent, readBody
     6| from twisted.web.http_headers import Headers
     7| class HttpClient:
     8|     """ Interface for talking json over http
     9|     """
    10|     def put_json(self, url, data):
    11|         """ Sends the specifed json data using PUT
    12|         Args:
    13|             url (str): The URL to PUT data to.
    14|             data (dict): A dict containing the data that will be used as
    15|                 the request body. This will be encoded as JSON.
    16|         Returns:
    17|             Deferred: Succeeds when we get a 2xx HTTP response. The result
    18|             will be the decoded JSON body.
    19|         """
    20|         pass


# ====================================================================
# FILE: contrib/graph/graph.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-20 ---
     1| import argparse
     2| import cgi
     3| import datetime
     4| import json
     5| import pydot
     6| import urllib2
     7| def make_name(pdu_id, origin):
     8|     return "%s@%s" % (pdu_id, origin)
     9| def make_graph(pdus, room, filename_prefix):
    10|     pdu_map = {}
    11|     node_map = {}
    12|     origins = set()
    13|     colors = {"red", "green", "blue", "yellow", "purple"}
    14|     for pdu in pdus:
    15|         origins.add(pdu.get("origin"))
    16|     color_map = {color: color for color in colors if color in origins}
    17|     colors -= set(color_map.values())
    18|     color_map[None] = "black"
    19|     for o in origins:
    20|         if o in color_map:


# ====================================================================
# FILE: contrib/graph/graph3.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-20 ---
     1| import argparse
     2| import cgi
     3| import datetime
     4| import pydot
     5| import simplejson as json
     6| from synapse.events import FrozenEvent
     7| from synapse.util.frozenutils import unfreeze
     8| def make_graph(file_name, room_id, file_prefix, limit):
     9|     print("Reading lines")
    10|     with open(file_name) as f:
    11|         lines = f.readlines()
    12|     print("Read lines")
    13|     events = [FrozenEvent(json.loads(line)) for line in lines]
    14|     print("Loaded events.")
    15|     events.sort(key=lambda e: e.depth)
    16|     print("Sorted events")
    17|     if limit:
    18|         events = events[-int(limit) :]
    19|     node_map = {}
    20|     graph = pydot.Dot(graph_name="Test")


# ====================================================================
# FILE: contrib/jitsimeetbridge/jitsimeetbridge.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-29 ---
     1| """
     2| This is an attempt at bridging matrix clients into a Jitis meet room via Matrix
     3| video call.  It uses hard-coded xml strings overg XMPP BOSH. It can display one
     4| of the streams from the Jitsi bridge until the second lot of SDP comes down and
     5| we set the remote SDP at which point the stream ends. Our video never gets to
     6| the bridge.
     7| Requires:
     8| npm install jquery jsdom
     9| """
    10| import json
    11| import subprocess
    12| import time
    13| import gevent
    14| import grequests
    15| from BeautifulSoup import BeautifulSoup
    16| ACCESS_TOKEN = ""
    17| MATRIXBASE = "https://matrix.org/_matrix/client/api/v1/"
    18| MYUSERNAME = "@davetest:matrix.org"
    19| HTTPBIND = "https://meet.jit.si/http-bind"
    20| ROOMNAME = "pibble"
    21| HOST = "guest.jit.si"
    22| TURNSERVER = "turn.guest.jit.si"
    23| ROOMDOMAIN = "meet.jit.si"
    24| class TrivialMatrixClient:
    25|     def __init__(self, access_token):
    26|         self.token = None
    27|         self.access_token = access_token
    28|     def getEvent(self):
    29|         while True:


# ====================================================================
# FILE: contrib/scripts/kick_users.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-65 ---
     1| import json
     2| import sys
     3| import urllib
     4| from argparse import ArgumentParser
     5| import requests
     6| def _mkurl(template, kws):
     7|     for key in kws:
     8|         template = template.replace(key, kws[key])
     9|     return template
    10| def main(hs, room_id, access_token, user_id_prefix, why):
    11|     if not why:
    12|         why = "Automated kick."
    13|     print(
    14|         "Kicking members on %s in room %s matching %s" % (hs, room_id, user_id_prefix)
    15|     )
    16|     room_state_url = _mkurl(
    17|         "$HS/_matrix/client/api/v1/rooms/$ROOM/state?access_token=$TOKEN",
    18|         {"$HS": hs, "$ROOM": room_id, "$TOKEN": access_token},
    19|     )
    20|     print("Getting room state => %s" % room_state_url)
    21|     res = requests.get(room_state_url)
    22|     print("HTTP %s" % res.status_code)
    23|     state_events = res.json()
    24|     if "error" in state_events:
    25|         print("FATAL")
    26|         print(state_events)
    27|         return
    28|     kick_list = []
    29|     room_name = room_id
    30|     for event in state_events:
    31|         if not event["type"] == "m.room.member":
    32|             if event["type"] == "m.room.name":
    33|                 room_name = event["content"].get("name")
    34|             continue
    35|         if not event["content"].get("membership") == "join":
    36|             continue
    37|         if event["state_key"].startswith(user_id_prefix):
    38|             kick_list.append(event["state_key"])
    39|     if len(kick_list) == 0:
    40|         print("No user IDs match the prefix '%s'" % user_id_prefix)
    41|         return
    42|     print("The following user IDs will be kicked from %s" % room_name)
    43|     for uid in kick_list:
    44|         print(uid)
    45|     doit = input("Continue? [Y]es\n")
    46|     if len(doit) > 0 and doit.lower() == "y":
    47|         print("Kicking members...")
    48|         kick_list = [urllib.quote(uid) for uid in kick_list]
    49|         for uid in kick_list:
    50|             kick_url = _mkurl(
    51|                 "$HS/_matrix/client/api/v1/rooms/$ROOM/state/m.room.member/$UID?access_token=$TOKEN",
    52|                 {"$HS": hs, "$UID": uid, "$ROOM": room_id, "$TOKEN": access_token},
    53|             )
    54|             kick_body = {"membership": "leave", "reason": why}
    55|             print("Kicking %s" % uid)
    56|             res = requests.put(kick_url, data=json.dumps(kick_body))
    57|             if res.status_code != 200:
    58|                 print("ERROR: HTTP %s" % res.status_code)
    59|             if res.json().get("error"):
    60|                 print("ERROR: JSON %s" % res.json())
    61| if __name__ == "__main__":
    62|     parser = ArgumentParser("Kick members in a room matching a certain user ID prefix.")
    63|     parser.add_argument("-u", "--user-id", help="The user ID prefix e.g. '@irc_'")
    64|     parser.add_argument("-t", "--token", help="Your access_token")
    65|     parser.add_argument("-r", "--room", help="The room ID to kick members in")


# ====================================================================
# FILE: scripts-dev/complement.sh
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-7 ---
     1| cd "$(dirname $0)/.."
     2| docker build -t matrixdotorg/synapse:latest -f docker/Dockerfile .
     3| wget -N https://github.com/matrix-org/complement/archive/master.tar.gz
     4| tar -xzf master.tar.gz
     5| cd complement-master
     6| docker build -t complement-synapse -f dockerfiles/Synapse.Dockerfile ./dockerfiles
     7| COMPLEMENT_BASE_IMAGE=complement-synapse go test -v -count=1 ./tests


# ====================================================================
# FILE: scripts-dev/definitions.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-29 ---
     1| import argparse
     2| import ast
     3| import os
     4| import re
     5| import sys
     6| import yaml
     7| class DefinitionVisitor(ast.NodeVisitor):
     8|     def __init__(self):
     9|         super().__init__()
    10|         self.functions = {}
    11|         self.classes = {}
    12|         self.names = {}
    13|         self.attrs = set()
    14|         self.definitions = {
    15|             "def": self.functions,
    16|             "class": self.classes,
    17|             "names": self.names,
    18|             "attrs": self.attrs,
    19|         }
    20|     def visit_Name(self, node):
    21|         self.names.setdefault(type(node.ctx).__name__, set()).add(node.id)
    22|     def visit_Attribute(self, node):
    23|         self.attrs.add(node.attr)
    24|         for child in ast.iter_child_nodes(node):
    25|             self.visit(child)
    26|     def visit_ClassDef(self, node):
    27|         visitor = DefinitionVisitor()
    28|         self.classes[node.name] = visitor.definitions
    29|         for child in ast.iter_child_nodes(node):


# ====================================================================
# FILE: scripts-dev/dump_macaroon.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-17 ---
     1| import sys
     2| import pymacaroons
     3| if len(sys.argv) == 1:
     4|     sys.stderr.write("usage: %s macaroon [key]\n" % (sys.argv[0],))
     5|     sys.exit(1)
     6| macaroon_string = sys.argv[1]
     7| key = sys.argv[2] if len(sys.argv) > 2 else None
     8| macaroon = pymacaroons.Macaroon.deserialize(macaroon_string)
     9| print(macaroon.inspect())
    10| print("")
    11| verifier = pymacaroons.Verifier()
    12| verifier.satisfy_general(lambda c: True)
    13| try:
    14|     verifier.verify(macaroon, key)
    15|     print("Signature is correct")
    16| except Exception as e:
    17|     print(str(e))


# ====================================================================
# FILE: scripts-dev/federation_client.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-20 ---
     1| import argparse
     2| import base64
     3| import json
     4| import sys
     5| from typing import Any, Optional
     6| from urllib import parse as urlparse
     7| import nacl.signing
     8| import requests
     9| import signedjson.types
    10| import srvlookup
    11| import yaml
    12| from requests.adapters import HTTPAdapter
    13| def encode_base64(input_bytes):
    14|     """Encode bytes as a base64 string without any padding."""
    15|     input_len = len(input_bytes)
    16|     output_len = 4 * ((input_len + 2) // 3) + (input_len + 2) % 3 - 2
    17|     output_bytes = base64.b64encode(input_bytes)
    18|     output_string = output_bytes[:output_len].decode("ascii")
    19|     return output_string
    20| def decode_base64(input_string):

# --- HUNK 2: Lines 214-236 ---
   214|                 return None
   215|             parsed_well_known = resp.json()
   216|             if not isinstance(parsed_well_known, dict):
   217|                 raise Exception("not a dict")
   218|             if "m.server" not in parsed_well_known:
   219|                 raise Exception("Missing key 'm.server'")
   220|             new_name = parsed_well_known["m.server"]
   221|             print("well-known lookup gave %s" % (new_name,), file=sys.stderr)
   222|             return new_name
   223|         except Exception as e:
   224|             print("Invalid response from %s: %s" % (uri, e), file=sys.stderr)
   225|         return None
   226|     def get_connection(self, url, proxies=None):
   227|         parsed = urlparse.urlparse(url)
   228|         (host, port) = self.lookup(parsed.netloc)
   229|         netloc = "%s:%d" % (host, port)
   230|         print("Connecting to %s" % (netloc,), file=sys.stderr)
   231|         url = urlparse.urlunparse(
   232|             ("https", netloc, parsed.path, parsed.params, parsed.query, parsed.fragment)
   233|         )
   234|         return super().get_connection(url, proxies)
   235| if __name__ == "__main__":
   236|     main()


# ====================================================================
# FILE: scripts-dev/hash_history.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-20 ---
     1| import sqlite3
     2| import sys
     3| from unpaddedbase64 import decode_base64, encode_base64
     4| from synapse.crypto.event_signing import (
     5|     add_event_pdu_content_hash,
     6|     compute_pdu_event_reference_hash,
     7| )
     8| from synapse.federation.units import Pdu
     9| from synapse.storage._base import SQLBaseStore
    10| from synapse.storage.pdu import PduStore
    11| from synapse.storage.signatures import SignatureStore
    12| class Store:
    13|     _get_pdu_tuples = PduStore.__dict__["_get_pdu_tuples"]
    14|     _get_pdu_content_hashes_txn = SignatureStore.__dict__["_get_pdu_content_hashes_txn"]
    15|     _get_prev_pdu_hashes_txn = SignatureStore.__dict__["_get_prev_pdu_hashes_txn"]
    16|     _get_pdu_origin_signatures_txn = SignatureStore.__dict__[
    17|         "_get_pdu_origin_signatures_txn"
    18|     ]
    19|     _store_pdu_content_hash_txn = SignatureStore.__dict__["_store_pdu_content_hash_txn"]
    20|     _store_pdu_reference_hash_txn = SignatureStore.__dict__[


# ====================================================================
# FILE: scripts/move_remote_media_to_new_store.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| """
     2| Moves a list of remote media from one media store to another.
     3| The input should be a list of media files to be moved, one per line. Each line
     4| should be formatted::
     5|     <origin server>|<file id>
     6| This can be extracted from postgres with::
     7|     psql --tuples-only -A -c "select media_origin, filesystem_id from
     8|         matrix.remote_media_cache where ..."
     9| To use, pipe the above into::
    10|     PYTHON_PATH=. ./scripts/move_remote_media_to_new_store.py <source repo> <dest repo>
    11| """
    12| import argparse
    13| import logging
    14| import os
    15| import shutil
    16| import sys
    17| from synapse.rest.media.v1.filepath import MediaFilePaths
    18| logger = logging.getLogger()
    19| def main(src_repo, dest_repo):
    20|     src_paths = MediaFilePaths(src_repo)
    21|     dest_paths = MediaFilePaths(dest_repo)
    22|     for line in sys.stdin:
    23|         line = line.strip()
    24|         parts = line.split("|")
    25|         if len(parts) != 2:
    26|             print("Unable to parse input line %s" % line, file=sys.stderr)
    27|             exit(1)
    28|         move_media(parts[0], parts[1], src_paths, dest_paths)
    29| def move_media(origin_server, file_id, src_paths, dest_paths):
    30|     """Move the given file, and any thumbnails, to the dest repo
    31|     Args:


# ====================================================================
# FILE: setup.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 17-63 ---
    17|         )
    18| def read_file(path_segments):
    19|     """Read a file from the package. Takes a list of strings to join to
    20|     make the path"""
    21|     file_path = os.path.join(here, *path_segments)
    22|     with open(file_path) as f:
    23|         return f.read()
    24| def exec_file(path_segments):
    25|     """Execute a single python file to get the variables defined in it"""
    26|     result = {}
    27|     code = read_file(path_segments)
    28|     exec(code, result)
    29|     return result
    30| version = exec_file(("synapse", "__init__.py"))["__version__"]
    31| dependencies = exec_file(("synapse", "python_dependencies.py"))
    32| long_description = read_file(("README.rst",))
    33| REQUIREMENTS = dependencies["REQUIREMENTS"]
    34| CONDITIONAL_REQUIREMENTS = dependencies["CONDITIONAL_REQUIREMENTS"]
    35| ALL_OPTIONAL_REQUIREMENTS = dependencies["ALL_OPTIONAL_REQUIREMENTS"]
    36| CONDITIONAL_REQUIREMENTS["all"] = list(ALL_OPTIONAL_REQUIREMENTS)
    37| CONDITIONAL_REQUIREMENTS["lint"] = [
    38|     "isort==5.0.3",
    39|     "black==19.10b0",
    40|     "flake8-comprehensions",
    41|     "flake8",
    42| ]
    43| CONDITIONAL_REQUIREMENTS["test"] = ["mock>=2.0", "parameterized>=0.7.0"]
    44| setup(
    45|     name="matrix-synapse",
    46|     version=version,
    47|     packages=find_packages(exclude=["tests", "tests.*"]),
    48|     description="Reference homeserver for the Matrix decentralised comms protocol",
    49|     install_requires=REQUIREMENTS,
    50|     extras_require=CONDITIONAL_REQUIREMENTS,
    51|     include_package_data=True,
    52|     zip_safe=False,
    53|     long_description=long_description,
    54|     python_requires="~=3.5",
    55|     classifiers=[
    56|         "Development Status :: 5 - Production/Stable",
    57|         "Topic :: Communications :: Chat",
    58|         "License :: OSI Approved :: Apache Software License",
    59|         "Programming Language :: Python :: 3 :: Only",
    60|         "Programming Language :: Python :: 3.5",
    61|         "Programming Language :: Python :: 3.6",
    62|         "Programming Language :: Python :: 3.7",
    63|         "Programming Language :: Python :: 3.8",


# ====================================================================
# FILE: synapse/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 3-26 ---
     3| import json
     4| import os
     5| import sys
     6| if sys.version_info < (3, 5):
     7|     print("Synapse requires Python 3.5 or above.")
     8|     sys.exit(1)
     9| try:
    10|     from twisted.internet import protocol
    11|     from twisted.internet.protocol import Factory
    12|     from twisted.names.dns import DNSDatagramProtocol
    13|     protocol.Factory.noisy = False
    14|     Factory.noisy = False
    15|     DNSDatagramProtocol.noisy = False
    16| except ImportError:
    17|     pass
    18| try:
    19|     from canonicaljson import set_json_library
    20|     set_json_library(json)
    21| except ImportError:
    22|     pass
    23| __version__ = "1.21.0"
    24| if bool(os.environ.get("SYNAPSE_TEST_PATCH_LOG_CONTEXTS", False)):
    25|     from synapse.util.patch_inline_callbacks import do_patch
    26|     do_patch()


# ====================================================================
# FILE: synapse/_scripts/register_new_matrix_user.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-20 ---
     1| import argparse
     2| import getpass
     3| import hashlib
     4| import hmac
     5| import logging
     6| import sys
     7| import requests as _requests
     8| import yaml
     9| def request_registration(
    10|     user,
    11|     password,
    12|     server_location,
    13|     shared_secret,
    14|     admin=False,
    15|     user_type=None,
    16|     requests=_requests,
    17|     _print=print,
    18|     exit=sys.exit,
    19| ):
    20|     url = "%s/_matrix/client/r0/admin/register" % (server_location,)


# ====================================================================
# FILE: synapse/api/auth.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 144-184 ---
   144|                 opentracing.set_tag("authenticated_entity", user_id)
   145|                 opentracing.set_tag("appservice_id", app_service.id)
   146|                 if ip_addr and self._track_appservice_user_ips:
   147|                     await self.store.insert_client_ip(
   148|                         user_id=user_id,
   149|                         access_token=access_token,
   150|                         ip=ip_addr,
   151|                         user_agent=user_agent,
   152|                         device_id="dummy-device",  # stubbed
   153|                     )
   154|                 return synapse.types.create_requester(user_id, app_service=app_service)
   155|             user_info = await self.get_user_by_access_token(
   156|                 access_token, rights, allow_expired=allow_expired
   157|             )
   158|             user = user_info["user"]
   159|             token_id = user_info["token_id"]
   160|             is_guest = user_info["is_guest"]
   161|             shadow_banned = user_info["shadow_banned"]
   162|             if self._account_validity.enabled and not allow_expired:
   163|                 user_id = user.to_string()
   164|                 if await self.store.is_account_expired(user_id, self.clock.time_msec()):
   165|                     raise AuthError(
   166|                         403, "User account has expired", errcode=Codes.EXPIRED_ACCOUNT
   167|                     )
   168|             device_id = user_info.get("device_id")
   169|             if user and access_token and ip_addr:
   170|                 await self.store.insert_client_ip(
   171|                     user_id=user.to_string(),
   172|                     access_token=access_token,
   173|                     ip=ip_addr,
   174|                     user_agent=user_agent,
   175|                     device_id=device_id,
   176|                 )
   177|             if is_guest and not allow_guest:
   178|                 raise AuthError(
   179|                     403,
   180|                     "Guest access not allowed",
   181|                     errcode=Codes.GUEST_ACCESS_FORBIDDEN,
   182|                 )
   183|             request.authenticated_entity = user.to_string()
   184|             opentracing.set_tag("authenticated_entity", user.to_string())


# ====================================================================
# FILE: synapse/api/errors.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 43-369 ---
    43|     INCOMPATIBLE_ROOM_VERSION = "M_INCOMPATIBLE_ROOM_VERSION"
    44|     WRONG_ROOM_KEYS_VERSION = "M_WRONG_ROOM_KEYS_VERSION"
    45|     EXPIRED_ACCOUNT = "ORG_MATRIX_EXPIRED_ACCOUNT"
    46|     PASSWORD_TOO_SHORT = "M_PASSWORD_TOO_SHORT"
    47|     PASSWORD_NO_DIGIT = "M_PASSWORD_NO_DIGIT"
    48|     PASSWORD_NO_UPPERCASE = "M_PASSWORD_NO_UPPERCASE"
    49|     PASSWORD_NO_LOWERCASE = "M_PASSWORD_NO_LOWERCASE"
    50|     PASSWORD_NO_SYMBOL = "M_PASSWORD_NO_SYMBOL"
    51|     PASSWORD_IN_DICTIONARY = "M_PASSWORD_IN_DICTIONARY"
    52|     WEAK_PASSWORD = "M_WEAK_PASSWORD"
    53|     INVALID_SIGNATURE = "M_INVALID_SIGNATURE"
    54|     USER_DEACTIVATED = "M_USER_DEACTIVATED"
    55|     BAD_ALIAS = "M_BAD_ALIAS"
    56| class CodeMessageException(RuntimeError):
    57|     """An exception with integer code and message string attributes.
    58|     Attributes:
    59|         code: HTTP error code
    60|         msg: string describing the error
    61|     """
    62|     def __init__(self, code: Union[int, HTTPStatus], msg: str):
    63|         super().__init__("%d: %s" % (code, msg))
    64|         self.code = int(code)
    65|         self.msg = msg
    66| class RedirectException(CodeMessageException):
    67|     """A pseudo-error indicating that we want to redirect the client to a different
    68|     location
    69|     Attributes:
    70|         cookies: a list of set-cookies values to add to the response. For example:
    71|            b"sessionId=a3fWa; Expires=Wed, 21 Oct 2015 07:28:00 GMT"
    72|     """
    73|     def __init__(self, location: bytes, http_code: int = http.FOUND):
    74|         """
    75|         Args:
    76|             location: the URI to redirect to
    77|             http_code: the HTTP response code
    78|         """
    79|         msg = "Redirect to %s" % (location.decode("utf-8"),)
    80|         super().__init__(code=http_code, msg=msg)
    81|         self.location = location
    82|         self.cookies = []  # type: List[bytes]
    83| class SynapseError(CodeMessageException):
    84|     """A base exception type for matrix errors which have an errcode and error
    85|     message (as well as an HTTP status code).
    86|     Attributes:
    87|         errcode: Matrix error code e.g 'M_FORBIDDEN'
    88|     """
    89|     def __init__(self, code: int, msg: str, errcode: str = Codes.UNKNOWN):
    90|         """Constructs a synapse error.
    91|         Args:
    92|             code: The integer error code (an HTTP response code)
    93|             msg: The human-readable error message.
    94|             errcode: The matrix error code e.g 'M_FORBIDDEN'
    95|         """
    96|         super().__init__(code, msg)
    97|         self.errcode = errcode
    98|     def error_dict(self):
    99|         return cs_error(self.msg, self.errcode)
   100| class ProxiedRequestError(SynapseError):
   101|     """An error from a general matrix endpoint, eg. from a proxied Matrix API call.
   102|     Attributes:
   103|         errcode: Matrix error code e.g 'M_FORBIDDEN'
   104|     """
   105|     def __init__(
   106|         self,
   107|         code: int,
   108|         msg: str,
   109|         errcode: str = Codes.UNKNOWN,
   110|         additional_fields: Optional[Dict] = None,
   111|     ):
   112|         super().__init__(code, msg, errcode)
   113|         if additional_fields is None:
   114|             self._additional_fields = {}  # type: Dict
   115|         else:
   116|             self._additional_fields = dict(additional_fields)
   117|     def error_dict(self):
   118|         return cs_error(self.msg, self.errcode, **self._additional_fields)
   119| class ConsentNotGivenError(SynapseError):
   120|     """The error returned to the client when the user has not consented to the
   121|     privacy policy.
   122|     """
   123|     def __init__(self, msg: str, consent_uri: str):
   124|         """Constructs a ConsentNotGivenError
   125|         Args:
   126|             msg: The human-readable error message
   127|             consent_url: The URL where the user can give their consent
   128|         """
   129|         super().__init__(
   130|             code=HTTPStatus.FORBIDDEN, msg=msg, errcode=Codes.CONSENT_NOT_GIVEN
   131|         )
   132|         self._consent_uri = consent_uri
   133|     def error_dict(self):
   134|         return cs_error(self.msg, self.errcode, consent_uri=self._consent_uri)
   135| class UserDeactivatedError(SynapseError):
   136|     """The error returned to the client when the user attempted to access an
   137|     authenticated endpoint, but the account has been deactivated.
   138|     """
   139|     def __init__(self, msg: str):
   140|         """Constructs a UserDeactivatedError
   141|         Args:
   142|             msg: The human-readable error message
   143|         """
   144|         super().__init__(
   145|             code=HTTPStatus.FORBIDDEN, msg=msg, errcode=Codes.USER_DEACTIVATED
   146|         )
   147| class FederationDeniedError(SynapseError):
   148|     """An error raised when the server tries to federate with a server which
   149|     is not on its federation whitelist.
   150|     Attributes:
   151|         destination: The destination which has been denied
   152|     """
   153|     def __init__(self, destination: Optional[str]):
   154|         """Raised by federation client or server to indicate that we are
   155|         are deliberately not attempting to contact a given server because it is
   156|         not on our federation whitelist.
   157|         Args:
   158|             destination: the domain in question
   159|         """
   160|         self.destination = destination
   161|         super().__init__(
   162|             code=403,
   163|             msg="Federation denied with %s." % (self.destination,),
   164|             errcode=Codes.FORBIDDEN,
   165|         )
   166| class InteractiveAuthIncompleteError(Exception):
   167|     """An error raised when UI auth is not yet complete
   168|     (This indicates we should return a 401 with 'result' as the body)
   169|     Attributes:
   170|         session_id: The ID of the ongoing interactive auth session.
   171|         result: the server response to the request, which should be
   172|             passed back to the client
   173|     """
   174|     def __init__(self, session_id: str, result: "JsonDict"):
   175|         super().__init__("Interactive auth not yet complete")
   176|         self.session_id = session_id
   177|         self.result = result
   178| class UnrecognizedRequestError(SynapseError):
   179|     """An error indicating we don't understand the request you're trying to make"""
   180|     def __init__(self, *args, **kwargs):
   181|         if "errcode" not in kwargs:
   182|             kwargs["errcode"] = Codes.UNRECOGNIZED
   183|         if len(args) == 0:
   184|             message = "Unrecognized request"
   185|         else:
   186|             message = args[0]
   187|         super().__init__(400, message, **kwargs)
   188| class NotFoundError(SynapseError):
   189|     """An error indicating we can't find the thing you asked for"""
   190|     def __init__(self, msg: str = "Not found", errcode: str = Codes.NOT_FOUND):
   191|         super().__init__(404, msg, errcode=errcode)
   192| class AuthError(SynapseError):
   193|     """An error raised when there was a problem authorising an event, and at various
   194|     other poorly-defined times.
   195|     """
   196|     def __init__(self, *args, **kwargs):
   197|         if "errcode" not in kwargs:
   198|             kwargs["errcode"] = Codes.FORBIDDEN
   199|         super().__init__(*args, **kwargs)
   200| class InvalidClientCredentialsError(SynapseError):
   201|     """An error raised when there was a problem with the authorisation credentials
   202|     in a client request.
   203|     https://matrix.org/docs/spec/client_server/r0.5.0#using-access-tokens:
   204|     When credentials are required but missing or invalid, the HTTP call will
   205|     return with a status of 401 and the error code, M_MISSING_TOKEN or
   206|     M_UNKNOWN_TOKEN respectively.
   207|     """
   208|     def __init__(self, msg: str, errcode: str):
   209|         super().__init__(code=401, msg=msg, errcode=errcode)
   210| class MissingClientTokenError(InvalidClientCredentialsError):
   211|     """Raised when we couldn't find the access token in a request"""
   212|     def __init__(self, msg: str = "Missing access token"):
   213|         super().__init__(msg=msg, errcode="M_MISSING_TOKEN")
   214| class InvalidClientTokenError(InvalidClientCredentialsError):
   215|     """Raised when we didn't understand the access token in a request"""
   216|     def __init__(
   217|         self, msg: str = "Unrecognised access token", soft_logout: bool = False
   218|     ):
   219|         super().__init__(msg=msg, errcode="M_UNKNOWN_TOKEN")
   220|         self._soft_logout = soft_logout
   221|     def error_dict(self):
   222|         d = super().error_dict()
   223|         d["soft_logout"] = self._soft_logout
   224|         return d
   225| class ResourceLimitError(SynapseError):
   226|     """
   227|     Any error raised when there is a problem with resource usage.
   228|     For instance, the monthly active user limit for the server has been exceeded
   229|     """
   230|     def __init__(
   231|         self,
   232|         code: int,
   233|         msg: str,
   234|         errcode: str = Codes.RESOURCE_LIMIT_EXCEEDED,
   235|         admin_contact: Optional[str] = None,
   236|         limit_type: Optional[str] = None,
   237|     ):
   238|         self.admin_contact = admin_contact
   239|         self.limit_type = limit_type
   240|         super().__init__(code, msg, errcode=errcode)
   241|     def error_dict(self):
   242|         return cs_error(
   243|             self.msg,
   244|             self.errcode,
   245|             admin_contact=self.admin_contact,
   246|             limit_type=self.limit_type,
   247|         )
   248| class EventSizeError(SynapseError):
   249|     """An error raised when an event is too big."""
   250|     def __init__(self, *args, **kwargs):
   251|         if "errcode" not in kwargs:
   252|             kwargs["errcode"] = Codes.TOO_LARGE
   253|         super().__init__(413, *args, **kwargs)
   254| class EventStreamError(SynapseError):
   255|     """An error raised when there a problem with the event stream."""
   256|     def __init__(self, *args, **kwargs):
   257|         if "errcode" not in kwargs:
   258|             kwargs["errcode"] = Codes.BAD_PAGINATION
   259|         super().__init__(*args, **kwargs)
   260| class LoginError(SynapseError):
   261|     """An error raised when there was a problem logging in."""
   262|     pass
   263| class StoreError(SynapseError):
   264|     """An error raised when there was a problem storing some data."""
   265|     pass
   266| class InvalidCaptchaError(SynapseError):
   267|     def __init__(
   268|         self,
   269|         code: int = 400,
   270|         msg: str = "Invalid captcha.",
   271|         error_url: Optional[str] = None,
   272|         errcode: str = Codes.CAPTCHA_INVALID,
   273|     ):
   274|         super().__init__(code, msg, errcode)
   275|         self.error_url = error_url
   276|     def error_dict(self):
   277|         return cs_error(self.msg, self.errcode, error_url=self.error_url)
   278| class LimitExceededError(SynapseError):
   279|     """A client has sent too many requests and is being throttled.
   280|     """
   281|     def __init__(
   282|         self,
   283|         code: int = 429,
   284|         msg: str = "Too Many Requests",
   285|         retry_after_ms: Optional[int] = None,
   286|         errcode: str = Codes.LIMIT_EXCEEDED,
   287|     ):
   288|         super().__init__(code, msg, errcode)
   289|         self.retry_after_ms = retry_after_ms
   290|     def error_dict(self):
   291|         return cs_error(self.msg, self.errcode, retry_after_ms=self.retry_after_ms)
   292| class RoomKeysVersionError(SynapseError):
   293|     """A client has tried to upload to a non-current version of the room_keys store
   294|     """
   295|     def __init__(self, current_version: str):
   296|         """
   297|         Args:
   298|             current_version: the current version of the store they should have used
   299|         """
   300|         super().__init__(403, "Wrong room_keys version", Codes.WRONG_ROOM_KEYS_VERSION)
   301|         self.current_version = current_version
   302| class UnsupportedRoomVersionError(SynapseError):
   303|     """The client's request to create a room used a room version that the server does
   304|     not support."""
   305|     def __init__(self, msg: str = "Homeserver does not support this room version"):
   306|         super().__init__(
   307|             code=400, msg=msg, errcode=Codes.UNSUPPORTED_ROOM_VERSION,
   308|         )
   309| class ThreepidValidationError(SynapseError):
   310|     """An error raised when there was a problem authorising an event."""
   311|     def __init__(self, *args, **kwargs):
   312|         if "errcode" not in kwargs:
   313|             kwargs["errcode"] = Codes.FORBIDDEN
   314|         super().__init__(*args, **kwargs)
   315| class IncompatibleRoomVersionError(SynapseError):
   316|     """A server is trying to join a room whose version it does not support.
   317|     Unlike UnsupportedRoomVersionError, it is specific to the case of the make_join
   318|     failing.
   319|     """
   320|     def __init__(self, room_version: str):
   321|         super().__init__(
   322|             code=400,
   323|             msg="Your homeserver does not support the features required to "
   324|             "join this room",
   325|             errcode=Codes.INCOMPATIBLE_ROOM_VERSION,
   326|         )
   327|         self._room_version = room_version
   328|     def error_dict(self):
   329|         return cs_error(self.msg, self.errcode, room_version=self._room_version)
   330| class PasswordRefusedError(SynapseError):
   331|     """A password has been refused, either during password reset/change or registration.
   332|     """
   333|     def __init__(
   334|         self,
   335|         msg: str = "This password doesn't comply with the server's policy",
   336|         errcode: str = Codes.WEAK_PASSWORD,
   337|     ):
   338|         super().__init__(
   339|             code=400, msg=msg, errcode=errcode,
   340|         )
   341| class RequestSendFailed(RuntimeError):
   342|     """Sending a HTTP request over federation failed due to not being able to
   343|     talk to the remote server for some reason.
   344|     This exception is used to differentiate "expected" errors that arise due to
   345|     networking (e.g. DNS failures, connection timeouts etc), versus unexpected
   346|     errors (like programming errors).
   347|     """
   348|     def __init__(self, inner_exception, can_retry):
   349|         super().__init__(
   350|             "Failed to send request: %s: %s"
   351|             % (type(inner_exception).__name__, inner_exception)
   352|         )
   353|         self.inner_exception = inner_exception
   354|         self.can_retry = can_retry
   355| def cs_error(msg: str, code: str = Codes.UNKNOWN, **kwargs):
   356|     """ Utility method for constructing an error response for client-server
   357|     interactions.
   358|     Args:
   359|         msg: The error message.
   360|         code: The error code.
   361|         kwargs: Additional keys to add to the response.
   362|     Returns:
   363|         A dict representing the error response JSON.
   364|     """
   365|     err = {"error": msg, "errcode": code}
   366|     for key, value in kwargs.items():
   367|         err[key] = value
   368|     return err
   369| class FederationError(RuntimeError):

# --- HUNK 2: Lines 375-437 ---
   375|         check (e.g. auth)
   376|     WARN: The remote server accepted the event, but believes some part of it
   377|         is wrong (e.g., it referred to an invalid event)
   378|     """
   379|     def __init__(
   380|         self,
   381|         level: str,
   382|         code: int,
   383|         reason: str,
   384|         affected: str,
   385|         source: Optional[str] = None,
   386|     ):
   387|         if level not in ["FATAL", "ERROR", "WARN"]:
   388|             raise ValueError("Level is not valid: %s" % (level,))
   389|         self.level = level
   390|         self.code = code
   391|         self.reason = reason
   392|         self.affected = affected
   393|         self.source = source
   394|         msg = "%s %s: %s" % (level, code, reason)
   395|         super().__init__(msg)
   396|     def get_dict(self):
   397|         return {
   398|             "level": self.level,
   399|             "code": self.code,
   400|             "reason": self.reason,
   401|             "affected": self.affected,
   402|             "source": self.source if self.source else self.affected,
   403|         }
   404| class HttpResponseException(CodeMessageException):
   405|     """
   406|     Represents an HTTP-level failure of an outbound request
   407|     Attributes:
   408|         response: body of response
   409|     """
   410|     def __init__(self, code: int, msg: str, response: bytes):
   411|         """
   412|         Args:
   413|             code: HTTP status code
   414|             msg: reason phrase from HTTP response status line
   415|             response: body of response
   416|         """
   417|         super().__init__(code, msg)
   418|         self.response = response
   419|     def to_synapse_error(self):
   420|         """Make a SynapseError based on an HTTPResponseException
   421|         This is useful when a proxied request has failed, and we need to
   422|         decide how to map the failure onto a matrix error to send back to the
   423|         client.
   424|         An attempt is made to parse the body of the http response as a matrix
   425|         error. If that succeeds, the errcode and error message from the body
   426|         are used as the errcode and error message in the new synapse error.
   427|         Otherwise, the errcode is set to M_UNKNOWN, and the error message is
   428|         set to the reason code from the HTTP response.
   429|         Returns:
   430|             SynapseError:
   431|         """
   432|         try:
   433|             j = json_decoder.decode(self.response.decode("utf-8"))
   434|         except ValueError:
   435|             j = {}
   436|         if not isinstance(j, dict):
   437|             j = {}


# ====================================================================
# FILE: synapse/api/filtering.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-23 ---
     1| import json
     2| from typing import List
     3| import jsonschema
     4| from jsonschema import FormatChecker
     5| from synapse.api.constants import EventContentFields
     6| from synapse.api.errors import SynapseError
     7| from synapse.api.presence import UserPresenceState
     8| from synapse.types import RoomID, UserID
     9| FILTER_SCHEMA = {
    10|     "additionalProperties": False,
    11|     "type": "object",
    12|     "properties": {
    13|         "limit": {"type": "number"},
    14|         "senders": {"$ref": "#/definitions/user_id_array"},
    15|         "not_senders": {"$ref": "#/definitions/user_id_array"},
    16|         "types": {"type": "array", "items": {"type": "string"}},
    17|         "not_types": {"type": "array", "items": {"type": "string"}},
    18|     },
    19| }
    20| ROOM_FILTER_SCHEMA = {
    21|     "additionalProperties": False,
    22|     "type": "object",
    23|     "properties": {

# --- HUNK 2: Lines 73-113 ---
    73|         "room": {"$ref": "#/definitions/room_filter"},
    74|         "event_format": {"type": "string", "enum": ["client", "federation"]},
    75|         "event_fields": {
    76|             "type": "array",
    77|             "items": {
    78|                 "type": "string",
    79|                 "pattern": r"^((?!\\\\).)*$",
    80|             },
    81|         },
    82|     },
    83|     "additionalProperties": False,
    84| }
    85| @FormatChecker.cls_checks("matrix_room_id")
    86| def matrix_room_id_validator(room_id_str):
    87|     return RoomID.from_string(room_id_str)
    88| @FormatChecker.cls_checks("matrix_user_id")
    89| def matrix_user_id_validator(user_id_str):
    90|     return UserID.from_string(user_id_str)
    91| class Filtering:
    92|     def __init__(self, hs):
    93|         super().__init__()
    94|         self.store = hs.get_datastore()
    95|     async def get_user_filter(self, user_localpart, filter_id):
    96|         result = await self.store.get_user_filter(user_localpart, filter_id)
    97|         return FilterCollection(result)
    98|     def add_user_filter(self, user_localpart, user_filter):
    99|         self.check_valid_filter(user_filter)
   100|         return self.store.add_user_filter(user_localpart, user_filter)
   101|     def check_valid_filter(self, user_filter_json):
   102|         """Check if the provided filter is valid.
   103|         This inspects all definitions contained within the filter.
   104|         Args:
   105|             user_filter_json(dict): The filter
   106|         Raises:
   107|             SynapseError: If the filter is not valid.
   108|         """
   109|         try:
   110|             jsonschema.validate(
   111|                 user_filter_json, USER_FILTER_SCHEMA, format_checker=FormatChecker()
   112|             )
   113|         except jsonschema.ValidationError as e:


# ====================================================================
# FILE: synapse/api/urls.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-26 ---
     1| """Contains the URL paths to prefix various aspects of the server with. """
     2| import hmac
     3| from hashlib import sha256
     4| from urllib.parse import urlencode
     5| from synapse.config import ConfigError
     6| SYNAPSE_CLIENT_API_PREFIX = "/_synapse/client"
     7| CLIENT_API_PREFIX = "/_matrix/client"
     8| FEDERATION_PREFIX = "/_matrix/federation"
     9| FEDERATION_V1_PREFIX = FEDERATION_PREFIX + "/v1"
    10| FEDERATION_V2_PREFIX = FEDERATION_PREFIX + "/v2"
    11| FEDERATION_UNSTABLE_PREFIX = FEDERATION_PREFIX + "/unstable"
    12| STATIC_PREFIX = "/_matrix/static"
    13| WEB_CLIENT_PREFIX = "/_matrix/client"
    14| SERVER_KEY_V2_PREFIX = "/_matrix/key/v2"
    15| MEDIA_PREFIX = "/_matrix/media/r0"
    16| LEGACY_MEDIA_PREFIX = "/_matrix/media/v1"
    17| class ConsentURIBuilder:
    18|     def __init__(self, hs_config):
    19|         """
    20|         Args:
    21|             hs_config (synapse.config.homeserver.HomeServerConfig):
    22|         """
    23|         if hs_config.form_secret is None:
    24|             raise ConfigError("form_secret not set in config")
    25|         if hs_config.public_baseurl is None:
    26|             raise ConfigError("public_baseurl not set in config")


# ====================================================================
# FILE: synapse/app/admin_cmd.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-26 ---
     1| import argparse
     2| import json
     3| import logging
     4| import os
     5| import sys
     6| import tempfile
     7| from twisted.internet import defer, task
     8| import synapse
     9| from synapse.app import _base
    10| from synapse.config._base import ConfigError
    11| from synapse.config.homeserver import HomeServerConfig
    12| from synapse.config.logger import setup_logging
    13| from synapse.handlers.admin import ExfiltrationWriter
    14| from synapse.replication.slave.storage._base import BaseSlavedStore
    15| from synapse.replication.slave.storage.account_data import SlavedAccountDataStore
    16| from synapse.replication.slave.storage.appservice import SlavedApplicationServiceStore
    17| from synapse.replication.slave.storage.client_ips import SlavedClientIpStore
    18| from synapse.replication.slave.storage.deviceinbox import SlavedDeviceInboxStore
    19| from synapse.replication.slave.storage.devices import SlavedDeviceStore
    20| from synapse.replication.slave.storage.events import SlavedEventStore
    21| from synapse.replication.slave.storage.filtering import SlavedFilteringStore
    22| from synapse.replication.slave.storage.groups import SlavedGroupServerStore
    23| from synapse.replication.slave.storage.presence import SlavedPresenceStore
    24| from synapse.replication.slave.storage.push_rule import SlavedPushRuleStore
    25| from synapse.replication.slave.storage.receipts import SlavedReceiptsStore
    26| from synapse.replication.slave.storage.registration import SlavedRegistrationStore


# ====================================================================
# FILE: synapse/app/generic_worker.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 111-169 ---
   111| from synapse.storage.databases.main.media_repository import MediaRepositoryStore
   112| from synapse.storage.databases.main.monthly_active_users import (
   113|     MonthlyActiveUsersWorkerStore,
   114| )
   115| from synapse.storage.databases.main.presence import UserPresenceState
   116| from synapse.storage.databases.main.search import SearchWorkerStore
   117| from synapse.storage.databases.main.ui_auth import UIAuthWorkerStore
   118| from synapse.storage.databases.main.user_directory import UserDirectoryStore
   119| from synapse.types import ReadReceipt
   120| from synapse.util.async_helpers import Linearizer
   121| from synapse.util.httpresourcetree import create_resource_tree
   122| from synapse.util.manhole import manhole
   123| from synapse.util.versionstring import get_version_string
   124| logger = logging.getLogger("synapse.app.generic_worker")
   125| class PresenceStatusStubServlet(RestServlet):
   126|     """If presence is disabled this servlet can be used to stub out setting
   127|     presence status.
   128|     """
   129|     PATTERNS = client_patterns("/presence/(?P<user_id>[^/]*)/status")
   130|     def __init__(self, hs):
   131|         super().__init__()
   132|         self.auth = hs.get_auth()
   133|     async def on_GET(self, request, user_id):
   134|         await self.auth.get_user_by_req(request)
   135|         return 200, {"presence": "offline"}
   136|     async def on_PUT(self, request, user_id):
   137|         await self.auth.get_user_by_req(request)
   138|         return 200, {}
   139| class KeyUploadServlet(RestServlet):
   140|     """An implementation of the `KeyUploadServlet` that responds to read only
   141|     requests, but otherwise proxies through to the master instance.
   142|     """
   143|     PATTERNS = client_patterns("/keys/upload(/(?P<device_id>[^/]+))?$")
   144|     def __init__(self, hs):
   145|         """
   146|         Args:
   147|             hs (synapse.server.HomeServer): server
   148|         """
   149|         super().__init__()
   150|         self.auth = hs.get_auth()
   151|         self.store = hs.get_datastore()
   152|         self.http_client = hs.get_simple_http_client()
   153|         self.main_uri = hs.config.worker_main_http_uri
   154|     async def on_POST(self, request, device_id):
   155|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   156|         user_id = requester.user.to_string()
   157|         body = parse_json_object_from_request(request)
   158|         if device_id is not None:
   159|             if requester.device_id is not None and device_id != requester.device_id:
   160|                 logger.warning(
   161|                     "Client uploading keys for a different device "
   162|                     "(logged in as %s, uploading for %s)",
   163|                     requester.device_id,
   164|                     device_id,
   165|                 )
   166|         else:
   167|             device_id = requester.device_id
   168|         if device_id is None:
   169|             raise SynapseError(

# --- HUNK 2: Lines 486-526 ---
   486|                         (
   487|                             "Metrics listener configured, but "
   488|                             "enable_metrics is not True!"
   489|                         )
   490|                     )
   491|                 else:
   492|                     _base.listen_metrics(listener.bind_addresses, listener.port)
   493|             else:
   494|                 logger.warning("Unsupported listener type: %s", listener.type)
   495|         self.get_tcp_replication().start_replication(self)
   496|     async def remove_pusher(self, app_id, push_key, user_id):
   497|         self.get_tcp_replication().send_remove_pusher(app_id, push_key, user_id)
   498|     @cache_in_self
   499|     def get_replication_data_handler(self):
   500|         return GenericWorkerReplicationHandler(self)
   501|     @cache_in_self
   502|     def get_presence_handler(self):
   503|         return GenericWorkerPresence(self)
   504| class GenericWorkerReplicationHandler(ReplicationDataHandler):
   505|     def __init__(self, hs):
   506|         super().__init__(hs)
   507|         self.store = hs.get_datastore()
   508|         self.presence_handler = hs.get_presence_handler()  # type: GenericWorkerPresence
   509|         self.notifier = hs.get_notifier()
   510|         self.notify_pushers = hs.config.start_pushers
   511|         self.pusher_pool = hs.get_pusherpool()
   512|         self.send_handler = None  # type: Optional[FederationSenderHandler]
   513|         if hs.config.send_federation:
   514|             self.send_handler = FederationSenderHandler(hs)
   515|     async def on_rdata(self, stream_name, instance_name, token, rows):
   516|         await super().on_rdata(stream_name, instance_name, token, rows)
   517|         await self._process_and_notify(stream_name, instance_name, token, rows)
   518|     async def _process_and_notify(self, stream_name, instance_name, token, rows):
   519|         try:
   520|             if self.send_handler:
   521|                 await self.send_handler.process_replication_rows(
   522|                     stream_name, token, rows
   523|                 )
   524|             if stream_name == PushRulesStream.NAME:
   525|                 self.notifier.on_new_event(
   526|                     "push_rules_key", token, users=[row.user_id for row in rows]


# ====================================================================
# FILE: synapse/app/homeserver.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-49 ---
     1| import gc
     2| import logging
     3| import math
     4| import os
     5| import resource
     6| import sys
     7| from typing import Iterable
     8| from prometheus_client import Gauge
     9| from twisted.application import service
    10| from twisted.internet import defer, reactor
    11| from twisted.python.failure import Failure
    12| from twisted.web.resource import EncodingResourceWrapper, IResource
    13| from twisted.web.server import GzipEncoderFactory
    14| from twisted.web.static import File
    15| import synapse
    16| import synapse.config.logger
    17| from synapse import events
    18| from synapse.api.urls import (
    19|     FEDERATION_PREFIX,
    20|     LEGACY_MEDIA_PREFIX,
    21|     MEDIA_PREFIX,
    22|     SERVER_KEY_V2_PREFIX,
    23|     STATIC_PREFIX,
    24|     WEB_CLIENT_PREFIX,
    25| )
    26| from synapse.app import _base
    27| from synapse.app._base import listen_ssl, listen_tcp, quit_with_error
    28| from synapse.config._base import ConfigError
    29| from synapse.config.emailconfig import ThreepidBehaviour
    30| from synapse.config.homeserver import HomeServerConfig
    31| from synapse.config.server import ListenerConfig
    32| from synapse.federation.transport.server import TransportLayerServer
    33| from synapse.http.additional_resource import AdditionalResource
    34| from synapse.http.server import (
    35|     OptionsResource,
    36|     RootOptionsRedirectResource,
    37|     RootRedirect,
    38|     StaticResource,
    39| )
    40| from synapse.http.site import SynapseSite
    41| from synapse.logging.context import LoggingContext
    42| from synapse.metrics import METRICS_PREFIX, MetricsResource, RegistryProxy
    43| from synapse.metrics.background_process_metrics import run_as_background_process
    44| from synapse.module_api import ModuleApi
    45| from synapse.python_dependencies import check_requirements
    46| from synapse.replication.http import REPLICATION_PREFIX, ReplicationRestResource
    47| from synapse.replication.tcp.resource import ReplicationStreamProtocolFactory
    48| from synapse.rest import ClientRestResource
    49| from synapse.rest.admin import AdminRestResource

# --- HUNK 2: Lines 144-190 ---
   144|             client_resource = ClientRestResource(self)
   145|             if compress:
   146|                 client_resource = gz_wrap(client_resource)
   147|             resources.update(
   148|                 {
   149|                     "/_matrix/client/api/v1": client_resource,
   150|                     "/_matrix/client/r0": client_resource,
   151|                     "/_matrix/client/unstable": client_resource,
   152|                     "/_matrix/client/v2_alpha": client_resource,
   153|                     "/_matrix/client/versions": client_resource,
   154|                     "/.well-known/matrix/client": WellKnownResource(self),
   155|                     "/_synapse/admin": AdminRestResource(self),
   156|                 }
   157|             )
   158|             if self.get_config().oidc_enabled:
   159|                 from synapse.rest.oidc import OIDCResource
   160|                 resources["/_synapse/oidc"] = OIDCResource(self)
   161|             if self.get_config().saml2_enabled:
   162|                 from synapse.rest.saml2 import SAML2Resource
   163|                 resources["/_matrix/saml2"] = SAML2Resource(self)
   164|             if self.get_config().threepid_behaviour_email == ThreepidBehaviour.LOCAL:
   165|                 from synapse.rest.synapse.client.password_reset import (
   166|                     PasswordResetSubmitTokenResource,
   167|                 )
   168|                 resources[
   169|                     "/_synapse/client/password_reset/email/submit_token"
   170|                 ] = PasswordResetSubmitTokenResource(self)
   171|         if name == "consent":
   172|             from synapse.rest.consent.consent_resource import ConsentResource
   173|             consent_resource = ConsentResource(self)
   174|             if compress:
   175|                 consent_resource = gz_wrap(consent_resource)
   176|             resources.update({"/_matrix/consent": consent_resource})
   177|         if name == "federation":
   178|             resources.update({FEDERATION_PREFIX: TransportLayerServer(self)})
   179|         if name == "openid":
   180|             resources.update(
   181|                 {
   182|                     FEDERATION_PREFIX: TransportLayerServer(
   183|                         self, servlet_groups=["openid"]
   184|                     )
   185|                 }
   186|             )
   187|         if name in ["static", "client"]:
   188|             resources.update(
   189|                 {
   190|                     STATIC_PREFIX: StaticResource(


# ====================================================================
# FILE: synapse/appservice/api.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 37-77 ---
    37|         return False
    38|     for k in (field, "protocol"):
    39|         if k not in r:
    40|             return False
    41|         if not isinstance(r[k], str):
    42|             return False
    43|     if "fields" not in r:
    44|         return False
    45|     fields = r["fields"]
    46|     if not isinstance(fields, dict):
    47|         return False
    48|     for k in fields.keys():
    49|         if not isinstance(fields[k], str):
    50|             return False
    51|     return True
    52| class ApplicationServiceApi(SimpleHttpClient):
    53|     """This class manages HS -> AS communications, including querying and
    54|     pushing.
    55|     """
    56|     def __init__(self, hs):
    57|         super().__init__(hs)
    58|         self.clock = hs.get_clock()
    59|         self.protocol_meta_cache = ResponseCache(
    60|             hs, "as_protocol_meta", timeout_ms=HOUR_IN_MS
    61|         )
    62|     async def query_user(self, service, user_id):
    63|         if service.url is None:
    64|             return False
    65|         uri = service.url + ("/users/%s" % urllib.parse.quote(user_id))
    66|         try:
    67|             response = await self.get_json(uri, {"access_token": service.hs_token})
    68|             if response is not None:  # just an empty json object
    69|                 return True
    70|         except CodeMessageException as e:
    71|             if e.code == 404:
    72|                 return False
    73|             logger.warning("query_user to %s received %s", uri, e.code)
    74|         except Exception as ex:
    75|             logger.warning("query_user to %s threw exception %s", uri, ex)
    76|         return False
    77|     async def query_alias(self, service, alias):

# --- HUNK 2: Lines 118-158 ---
   118|                 else:
   119|                     logger.warning(
   120|                         "query_3pe to %s returned an invalid result %r", uri, r
   121|                     )
   122|             return ret
   123|         except Exception as ex:
   124|             logger.warning("query_3pe to %s threw exception %s", uri, ex)
   125|             return []
   126|     async def get_3pe_protocol(
   127|         self, service: "ApplicationService", protocol: str
   128|     ) -> Optional[JsonDict]:
   129|         if service.url is None:
   130|             return {}
   131|         async def _get() -> Optional[JsonDict]:
   132|             uri = "%s%s/thirdparty/protocol/%s" % (
   133|                 service.url,
   134|                 APP_SERVICE_PREFIX,
   135|                 urllib.parse.quote(protocol),
   136|             )
   137|             try:
   138|                 info = await self.get_json(uri)
   139|                 if not _is_valid_3pe_metadata(info):
   140|                     logger.warning(
   141|                         "query_3pe_protocol to %s did not return a valid result", uri
   142|                     )
   143|                     return None
   144|                 for instance in info.get("instances", []):
   145|                     network_id = instance.get("network_id", None)
   146|                     if network_id is not None:
   147|                         instance["instance_id"] = ThirdPartyInstanceID(
   148|                             service.id, network_id
   149|                         ).to_string()
   150|                 return info
   151|             except Exception as ex:
   152|                 logger.warning("query_3pe_protocol to %s threw exception %s", uri, ex)
   153|                 return None
   154|         key = (service.id, protocol)
   155|         return await self.protocol_meta_cache.wrap(key, _get)
   156|     async def push_bulk(self, service, events, txn_id=None):
   157|         if service.url is None:
   158|             return True


# ====================================================================
# FILE: synapse/config/_base.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 150-194 ---
   150|             custom_template_directory: A directory to try to look for the templates
   151|                 before using the default Synapse template directory instead.
   152|             autoescape: Whether to autoescape variables before inserting them into the
   153|                 template.
   154|         Raises:
   155|             ConfigError: if the file's path is incorrect or otherwise cannot be read.
   156|         Returns:
   157|             A list of jinja2 templates.
   158|         """
   159|         templates = []
   160|         search_directories = [self.default_template_dir]
   161|         if custom_template_directory:
   162|             if not self.path_exists(custom_template_directory):
   163|                 raise ConfigError(
   164|                     "Configured template directory does not exist: %s"
   165|                     % (custom_template_directory,)
   166|                 )
   167|             search_directories.insert(0, custom_template_directory)
   168|         loader = jinja2.FileSystemLoader(search_directories)
   169|         env = jinja2.Environment(loader=loader, autoescape=autoescape)
   170|         env.filters.update({"format_ts": _format_ts_filter})
   171|         if self.public_baseurl:
   172|             env.filters.update(
   173|                 {"mxc_to_http": _create_mxc_to_http_filter(self.public_baseurl)}
   174|             )
   175|         for filename in filenames:
   176|             template = env.get_template(filename)
   177|             templates.append(template)
   178|         return templates
   179| def _format_ts_filter(value: int, format: str):
   180|     return time.strftime(format, time.localtime(value / 1000))
   181| def _create_mxc_to_http_filter(public_baseurl: str) -> Callable:
   182|     """Create and return a jinja2 filter that converts MXC urls to HTTP
   183|     Args:
   184|         public_baseurl: The public, accessible base URL of the homeserver
   185|     """
   186|     def mxc_to_http_filter(value, width, height, resize_method="crop"):
   187|         if value[0:6] != "mxc://":
   188|             return ""
   189|         server_and_media_id = value[6:]
   190|         fragment = None
   191|         if "#" in server_and_media_id:
   192|             server_and_media_id, fragment = server_and_media_id.split("#", 1)
   193|             fragment = "#" + fragment
   194|         params = {"width": width, "height": height, "method": resize_method}

# --- HUNK 2: Lines 625-660 ---
   625|                         continue
   626|                     files.append(entry_path)
   627|                 config_files.extend(sorted(files))
   628|             else:
   629|                 config_files.append(config_path)
   630|     return config_files
   631| @attr.s
   632| class ShardedWorkerHandlingConfig:
   633|     """Algorithm for choosing which instance is responsible for handling some
   634|     sharded work.
   635|     For example, the federation senders use this to determine which instances
   636|     handles sending stuff to a given destination (which is used as the `key`
   637|     below).
   638|     """
   639|     instances = attr.ib(type=List[str])
   640|     def should_handle(self, instance_name: str, key: str) -> bool:
   641|         """Whether this instance is responsible for handling the given key.
   642|         """
   643|         if not self.instances or len(self.instances) == 1:
   644|             return True
   645|         return self.get_instance(key) == instance_name
   646|     def get_instance(self, key: str) -> str:
   647|         """Get the instance responsible for handling the given key.
   648|         Note: For things like federation sending the config for which instance
   649|         is sending is known only to the sender instance if there is only one.
   650|         Therefore `should_handle` should be used where possible.
   651|         """
   652|         if not self.instances:
   653|             return "master"
   654|         if len(self.instances) == 1:
   655|             return self.instances[0]
   656|         dest_hash = sha256(key.encode("utf8")).digest()
   657|         dest_int = int.from_bytes(dest_hash, byteorder="little")
   658|         remainder = dest_int % (len(self.instances))
   659|         return self.instances[remainder]
   660| __all__ = ["Config", "RootConfig", "ShardedWorkerHandlingConfig"]


# ====================================================================
# FILE: synapse/config/_util.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-27 ---
     1| from typing import Any, Iterable
     2| import jsonschema
     3| from synapse.config._base import ConfigError
     4| from synapse.types import JsonDict
     5| def validate_config(
     6|     json_schema: JsonDict, config: Any, config_path: Iterable[str]
     7| ) -> None:
     8|     """Validates a config setting against a JsonSchema definition
     9|     This can be used to validate a section of the config file against a schema
    10|     definition. If the validation fails, a ConfigError is raised with a textual
    11|     description of the problem.
    12|     Args:
    13|         json_schema: the schema to validate against
    14|         config: the configuration value to be validated
    15|         config_path: the path within the config file. This will be used as a basis
    16|            for the error message.
    17|     """
    18|     try:
    19|         jsonschema.validate(config, json_schema)
    20|     except jsonschema.ValidationError as e:
    21|         path = list(config_path)
    22|         for p in list(e.path):
    23|             if isinstance(p, int):
    24|                 path.append("<item %i>" % p)
    25|             else:
    26|                 path.append(str(p))
    27|         raise ConfigError(


# ====================================================================
# FILE: synapse/config/captcha.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-19 ---
     1| from ._base import Config
     2| class CaptchaConfig(Config):
     3|     section = "captcha"
     4|     def read_config(self, config, **kwargs):
     5|         self.recaptcha_private_key = config.get("recaptcha_private_key")
     6|         self.recaptcha_public_key = config.get("recaptcha_public_key")
     7|         self.enable_registration_captcha = config.get(
     8|             "enable_registration_captcha", False
     9|         )
    10|         self.recaptcha_siteverify_api = config.get(
    11|             "recaptcha_siteverify_api",
    12|             "https://www.recaptcha.net/recaptcha/api/siteverify",
    13|         )
    14|         self.recaptcha_template = self.read_templates(
    15|             ["recaptcha.html"], autoescape=True
    16|         )[0]
    17|     def generate_config_section(self, **kwargs):
    18|         return """\
    19|         """


# ====================================================================
# FILE: synapse/config/consent_config.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-39 ---
     1| from os import path
     2| from synapse.config import ConfigError
     3| from ._base import Config
     4| DEFAULT_CONFIG = """\
     5| """
     6| class ConsentConfig(Config):
     7|     section = "consent"
     8|     def __init__(self, *args):
     9|         super().__init__(*args)
    10|         self.user_consent_version = None
    11|         self.user_consent_template_dir = None
    12|         self.user_consent_server_notice_content = None
    13|         self.user_consent_server_notice_to_guests = False
    14|         self.block_events_without_consent_error = None
    15|         self.user_consent_at_registration = False
    16|         self.user_consent_policy_name = "Privacy Policy"
    17|     def read_config(self, config, **kwargs):
    18|         consent_config = config.get("user_consent")
    19|         self.terms_template = self.read_templates(["terms.html"], autoescape=True)[0]
    20|         if consent_config is None:
    21|             return
    22|         self.user_consent_version = str(consent_config["version"])
    23|         self.user_consent_template_dir = self.abspath(consent_config["template_dir"])
    24|         if not path.isdir(self.user_consent_template_dir):
    25|             raise ConfigError(
    26|                 "Could not find template directory '%s'"
    27|                 % (self.user_consent_template_dir,)
    28|             )
    29|         self.user_consent_server_notice_content = consent_config.get(
    30|             "server_notice_content"
    31|         )
    32|         self.block_events_without_consent_error = consent_config.get(
    33|             "block_events_error"
    34|         )
    35|         self.user_consent_server_notice_to_guests = bool(
    36|             consent_config.get("send_server_notice_to_guests", False)
    37|         )
    38|         self.user_consent_at_registration = bool(
    39|             consent_config.get("require_at_registration", False)


# ====================================================================
# FILE: synapse/config/emailconfig.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-20 ---
     1| import email.utils
     2| import os
     3| from enum import Enum
     4| from typing import Optional
     5| import attr
     6| from ._base import Config, ConfigError
     7| MISSING_PASSWORD_RESET_CONFIG_ERROR = """\
     8| Password reset emails are enabled on this homeserver due to a partial
     9| 'email' block. However, the following required keys are missing:
    10|     %s
    11| """
    12| DEFAULT_SUBJECTS = {
    13|     "message_from_person_in_room": "[%(app)s] You have a message on %(app)s from %(person)s in the %(room)s room...",
    14|     "message_from_person": "[%(app)s] You have a message on %(app)s from %(person)s...",
    15|     "messages_from_person": "[%(app)s] You have messages on %(app)s from %(person)s...",
    16|     "messages_in_room": "[%(app)s] You have messages on %(app)s in the %(room)s room...",
    17|     "messages_in_room_and_others": "[%(app)s] You have messages on %(app)s in the %(room)s room and others...",
    18|     "messages_from_person_and_others": "[%(app)s] You have messages on %(app)s from %(person)s and others...",
    19|     "invite_from_person": "[%(app)s] %(person)s has invited you to chat on %(app)s...",
    20|     "invite_from_person_to_room": "[%(app)s] %(person)s has invited you to join the %(room)s room on %(app)s...",

# --- HUNK 2: Lines 129-184 ---
   129|             )
   130|             add_threepid_template_failure_html = email_config.get(
   131|                 "add_threepid_template_failure_html", "add_threepid_failure.html"
   132|             )
   133|             password_reset_template_success_html = email_config.get(
   134|                 "password_reset_template_success_html", "password_reset_success.html"
   135|             )
   136|             registration_template_success_html = email_config.get(
   137|                 "registration_template_success_html", "registration_success.html"
   138|             )
   139|             add_threepid_template_success_html = email_config.get(
   140|                 "add_threepid_template_success_html", "add_threepid_success.html"
   141|             )
   142|             (
   143|                 self.email_password_reset_template_html,
   144|                 self.email_password_reset_template_text,
   145|                 self.email_registration_template_html,
   146|                 self.email_registration_template_text,
   147|                 self.email_add_threepid_template_html,
   148|                 self.email_add_threepid_template_text,
   149|                 self.email_password_reset_template_confirmation_html,
   150|                 self.email_password_reset_template_failure_html,
   151|                 self.email_registration_template_failure_html,
   152|                 self.email_add_threepid_template_failure_html,
   153|                 password_reset_template_success_html_template,
   154|                 registration_template_success_html_template,
   155|                 add_threepid_template_success_html_template,
   156|             ) = self.read_templates(
   157|                 [
   158|                     password_reset_template_html,
   159|                     password_reset_template_text,
   160|                     registration_template_html,
   161|                     registration_template_text,
   162|                     add_threepid_template_html,
   163|                     add_threepid_template_text,
   164|                     "password_reset_confirmation.html",
   165|                     password_reset_template_failure_html,
   166|                     registration_template_failure_html,
   167|                     add_threepid_template_failure_html,
   168|                     password_reset_template_success_html,
   169|                     registration_template_success_html,
   170|                     add_threepid_template_success_html,
   171|                 ],
   172|                 template_dir,
   173|             )
   174|             self.email_password_reset_template_success_html_content = (
   175|                 password_reset_template_success_html_template.render()
   176|             )
   177|             self.email_registration_template_success_html_content = (
   178|                 registration_template_success_html_template.render()
   179|             )
   180|             self.email_add_threepid_template_success_html_content = (
   181|                 add_threepid_template_success_html_template.render()
   182|             )
   183|         if self.email_enable_notifs:
   184|             missing = []


# ====================================================================
# FILE: synapse/config/federation.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-46 ---
     1| from typing import Optional
     2| from netaddr import IPSet
     3| from synapse.config._base import Config, ConfigError
     4| from synapse.config._util import validate_config
     5| class FederationConfig(Config):
     6|     section = "federation"
     7|     def read_config(self, config, **kwargs):
     8|         self.federation_domain_whitelist = None  # type: Optional[dict]
     9|         federation_domain_whitelist = config.get("federation_domain_whitelist", None)
    10|         if federation_domain_whitelist is not None:
    11|             self.federation_domain_whitelist = {}
    12|             for domain in federation_domain_whitelist:
    13|                 self.federation_domain_whitelist[domain] = True
    14|         self.federation_ip_range_blacklist = config.get(
    15|             "federation_ip_range_blacklist", []
    16|         )
    17|         try:
    18|             self.federation_ip_range_blacklist = IPSet(
    19|                 self.federation_ip_range_blacklist
    20|             )
    21|             self.federation_ip_range_blacklist.update(["0.0.0.0", "::"])
    22|         except Exception as e:
    23|             raise ConfigError(
    24|                 "Invalid range(s) provided in federation_ip_range_blacklist: %s" % e
    25|             )
    26|         federation_metrics_domains = config.get("federation_metrics_domains") or []
    27|         validate_config(
    28|             _METRICS_FOR_DOMAINS_SCHEMA,
    29|             federation_metrics_domains,
    30|             ("federation_metrics_domains",),
    31|         )
    32|         self.federation_metrics_domains = set(federation_metrics_domains)
    33|     def generate_config_section(self, config_dir_path, server_name, **kwargs):
    34|         return """\
    35|         federation_ip_range_blacklist:
    36|           - '127.0.0.0/8'
    37|           - '10.0.0.0/8'
    38|           - '172.16.0.0/12'
    39|           - '192.168.0.0/16'
    40|           - '100.64.0.0/10'
    41|           - '169.254.0.0/16'
    42|           - '::1/128'
    43|           - 'fe80::/64'
    44|           - 'fc00::/7'
    45|         """
    46| _METRICS_FOR_DOMAINS_SCHEMA = {"type": "array", "items": {"type": "string"}}


# ====================================================================
# FILE: synapse/config/homeserver.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 56-76 ---
    56|         OIDCConfig,
    57|         CasConfig,
    58|         SSOConfig,
    59|         JWTConfig,
    60|         PasswordConfig,
    61|         EmailConfig,
    62|         PasswordAuthProviderConfig,
    63|         PushConfig,
    64|         SpamCheckerConfig,
    65|         RoomConfig,
    66|         GroupsConfig,
    67|         UserDirectoryConfig,
    68|         ConsentConfig,
    69|         StatsConfig,
    70|         ServerNoticesConfig,
    71|         RoomDirectoryConfig,
    72|         ThirdPartyRulesConfig,
    73|         TracerConfig,
    74|         WorkerConfig,
    75|         RedisConfig,
    76|     ]


# ====================================================================
# FILE: synapse/config/logger.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-33 ---
     1| import argparse
     2| import logging
     3| import logging.config
     4| import os
     5| import sys
     6| import threading
     7| from string import Template
     8| import yaml
     9| from twisted.logger import (
    10|     ILogObserver,
    11|     LogBeginner,
    12|     STDLibLogObserver,
    13|     eventAsText,
    14|     globalLogBeginner,
    15| )
    16| import synapse
    17| from synapse.app import _base as appbase
    18| from synapse.logging._structured import (
    19|     reload_structured_logging,
    20|     setup_structured_logging,
    21| )
    22| from synapse.logging.context import LoggingContextFilter
    23| from synapse.util.versionstring import get_version_string
    24| from ._base import Config, ConfigError
    25| DEFAULT_LOG_CONFIG = Template(
    26|     """\
    27| version: 1
    28| formatters:
    29|     precise:
    30|         format: '%(asctime)s - %(name)s - %(lineno)d - %(levelname)s - \
    31| %(request)s - %(message)s'
    32| handlers:
    33|     file:

# --- HUNK 2: Lines 113-173 ---
   113|             "%(asctime)s - %(name)s - %(lineno)d - %(levelname)s - %(request)s"
   114|             " - %(message)s"
   115|         )
   116|         logger = logging.getLogger("")
   117|         logger.setLevel(logging.INFO)
   118|         logging.getLogger("synapse.storage.SQL").setLevel(logging.INFO)
   119|         formatter = logging.Formatter(log_format)
   120|         handler = logging.StreamHandler()
   121|         handler.setFormatter(formatter)
   122|         logger.addHandler(handler)
   123|     else:
   124|         logging.config.dictConfig(log_config)
   125|     log_filter = LoggingContextFilter(request="")
   126|     old_factory = logging.getLogRecordFactory()
   127|     def factory(*args, **kwargs):
   128|         record = old_factory(*args, **kwargs)
   129|         log_filter.filter(record)
   130|         return record
   131|     logging.setLogRecordFactory(factory)
   132|     observer = STDLibLogObserver()
   133|     threadlocal = threading.local()
   134|     def _log(event):
   135|         if "log_text" in event:
   136|             if event["log_text"].startswith("DNSDatagramProtocol starting on "):
   137|                 return
   138|             if event["log_text"].startswith("(UDP Port "):
   139|                 return
   140|             if event["log_text"].startswith("Timing out client"):
   141|                 return
   142|         if getattr(threadlocal, "active", False):
   143|             try:
   144|                 event_text = eventAsText(event)
   145|                 print("logging during logging: %s" % event_text, file=sys.__stderr__)
   146|             except Exception:
   147|                 pass
   148|             return
   149|         try:
   150|             threadlocal.active = True
   151|             return observer(event)
   152|         finally:
   153|             threadlocal.active = False
   154|     logBeginner.beginLoggingTo([_log], redirectStandardIO=not config.no_redirect_stdio)
   155|     if not config.no_redirect_stdio:
   156|         print("Redirected stdout/stderr to logs")
   157|     return observer
   158| def _reload_stdlib_logging(*args, log_config=None):
   159|     logger = logging.getLogger("")
   160|     if not log_config:
   161|         logger.warning("Reloaded a blank config?")
   162|     logging.config.dictConfig(log_config)
   163| def setup_logging(
   164|     hs, config, use_worker_options=False, logBeginner: LogBeginner = globalLogBeginner
   165| ) -> ILogObserver:
   166|     """
   167|     Set up the logging subsystem.
   168|     Args:
   169|         config (LoggingConfig | synapse.config.worker.WorkerConfig):
   170|             configuration data
   171|         use_worker_options (bool): True to use the 'worker_log_config' option
   172|             instead of 'log_config'.
   173|         logBeginner: The Twisted logBeginner to use.


# ====================================================================
# FILE: synapse/config/oidc_config.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 14-54 ---
    14|         except DependencyException as e:
    15|             raise ConfigError(e.message)
    16|         public_baseurl = self.public_baseurl
    17|         if public_baseurl is None:
    18|             raise ConfigError("oidc_config requires a public_baseurl to be set")
    19|         self.oidc_callback_url = public_baseurl + "_synapse/oidc/callback"
    20|         self.oidc_enabled = True
    21|         self.oidc_discover = oidc_config.get("discover", True)
    22|         self.oidc_issuer = oidc_config["issuer"]
    23|         self.oidc_client_id = oidc_config["client_id"]
    24|         self.oidc_client_secret = oidc_config["client_secret"]
    25|         self.oidc_client_auth_method = oidc_config.get(
    26|             "client_auth_method", "client_secret_basic"
    27|         )
    28|         self.oidc_scopes = oidc_config.get("scopes", ["openid"])
    29|         self.oidc_authorization_endpoint = oidc_config.get("authorization_endpoint")
    30|         self.oidc_token_endpoint = oidc_config.get("token_endpoint")
    31|         self.oidc_userinfo_endpoint = oidc_config.get("userinfo_endpoint")
    32|         self.oidc_jwks_uri = oidc_config.get("jwks_uri")
    33|         self.oidc_skip_verification = oidc_config.get("skip_verification", False)
    34|         self.oidc_allow_existing_users = oidc_config.get("allow_existing_users", False)
    35|         ump_config = oidc_config.get("user_mapping_provider", {})
    36|         ump_config.setdefault("module", DEFAULT_USER_MAPPING_PROVIDER)
    37|         ump_config.setdefault("config", {})
    38|         (
    39|             self.oidc_user_mapping_provider_class,
    40|             self.oidc_user_mapping_provider_config,
    41|         ) = load_module(ump_config)
    42|         required_methods = [
    43|             "get_remote_user_id",
    44|             "map_user_attributes",
    45|         ]
    46|         missing_methods = [
    47|             method
    48|             for method in required_methods
    49|             if not hasattr(self.oidc_user_mapping_provider_class, method)
    50|         ]
    51|         if missing_methods:
    52|             raise ConfigError(
    53|                 "Class specified by oidc_config."
    54|                 "user_mapping_provider.module is missing required "


# ====================================================================
# FILE: synapse/config/registration.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-33 ---
     1| import os
     2| from distutils.util import strtobool
     3| import pkg_resources
     4| from synapse.api.constants import RoomCreationPreset
     5| from synapse.config._base import Config, ConfigError
     6| from synapse.types import RoomAlias, UserID
     7| from synapse.util.stringutils import random_string_with_symbols
     8| class AccountValidityConfig(Config):
     9|     section = "accountvalidity"
    10|     def __init__(self, config, synapse_config):
    11|         if config is None:
    12|             return
    13|         super().__init__()
    14|         self.enabled = config.get("enabled", False)
    15|         self.renew_by_email_enabled = "renew_at" in config
    16|         if self.enabled:
    17|             if "period" in config:
    18|                 self.period = self.parse_duration(config["period"])
    19|             else:
    20|                 raise ConfigError("'period' is required when using account validity")
    21|             if "renew_at" in config:
    22|                 self.renew_at = self.parse_duration(config["renew_at"])
    23|             if "renew_email_subject" in config:
    24|                 self.renew_email_subject = config["renew_email_subject"]
    25|             else:
    26|                 self.renew_email_subject = "Renew your %(app)s account"
    27|             self.startup_job_max_delta = self.period * 10.0 / 100.0
    28|         if self.renew_by_email_enabled:
    29|             if "public_baseurl" not in synapse_config:
    30|                 raise ConfigError("Can't send renewal emails without 'public_baseurl'")
    31|         template_dir = config.get("template_dir")
    32|         if not template_dir:
    33|             template_dir = pkg_resources.resource_filename("synapse", "res/templates")

# --- HUNK 2: Lines 115-157 ---
   115|                 raise ConfigError("Invalid value for autocreate_auto_join_room_preset")
   116|             if self.auto_join_room_requires_invite:
   117|                 if not mxid_localpart:
   118|                     raise ConfigError(
   119|                         "The configuration option `auto_join_mxid_localpart` is required if "
   120|                         "`autocreate_auto_join_room_preset` is set to private_chat or trusted_private_chat, such that "
   121|                         "Synapse knows who to send invitations from. Please "
   122|                         "configure `auto_join_mxid_localpart`."
   123|                     )
   124|         self.auto_join_rooms_for_guests = config.get("auto_join_rooms_for_guests", True)
   125|         self.enable_set_displayname = config.get("enable_set_displayname", True)
   126|         self.enable_set_avatar_url = config.get("enable_set_avatar_url", True)
   127|         self.enable_3pid_changes = config.get("enable_3pid_changes", True)
   128|         self.disable_msisdn_registration = config.get(
   129|             "disable_msisdn_registration", False
   130|         )
   131|         session_lifetime = config.get("session_lifetime")
   132|         if session_lifetime is not None:
   133|             session_lifetime = self.parse_duration(session_lifetime)
   134|         self.session_lifetime = session_lifetime
   135|         self.fallback_success_template = self.read_templates(
   136|             ["auth_success.html"], autoescape=True
   137|         )[0]
   138|     def generate_config_section(self, generate_secrets=False, **kwargs):
   139|         if generate_secrets:
   140|             registration_shared_secret = 'registration_shared_secret: "%s"' % (
   141|                 random_string_with_symbols(50),
   142|             )
   143|         else:
   144|             registration_shared_secret = "#registration_shared_secret: <PRIVATE STRING>"
   145|         return (
   146|             """\
   147|         account_validity:
   148|         %(registration_shared_secret)s
   149|         account_threepid_delegates:
   150|         """
   151|             % locals()
   152|         )
   153|     @staticmethod
   154|     def add_arguments(parser):
   155|         reg_group = parser.add_argument_group("registration")
   156|         reg_group.add_argument(
   157|             "--enable-registration",


# ====================================================================
# FILE: synapse/config/saml2_config.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 90-129 ---
    90|                 "methods: %s" % (", ".join(missing_methods),)
    91|             )
    92|         saml2_config_dict = self._default_saml_config_dict(
    93|             *self.saml2_user_mapping_provider_class.get_saml_attributes(
    94|                 self.saml2_user_mapping_provider_config
    95|             )
    96|         )
    97|         _dict_merge(
    98|             merge_dict=saml2_config.get("sp_config", {}), into_dict=saml2_config_dict
    99|         )
   100|         config_path = saml2_config.get("config_path", None)
   101|         if config_path is not None:
   102|             mod = load_python_module(config_path)
   103|             _dict_merge(merge_dict=mod.CONFIG, into_dict=saml2_config_dict)
   104|         import saml2.config
   105|         self.saml2_sp_config = saml2.config.SPConfig()
   106|         self.saml2_sp_config.load(saml2_config_dict)
   107|         self.saml2_session_lifetime = self.parse_duration(
   108|             saml2_config.get("saml_session_lifetime", "15m")
   109|         )
   110|     def _default_saml_config_dict(
   111|         self, required_attributes: set, optional_attributes: set
   112|     ):
   113|         """Generate a configuration dictionary with required and optional attributes that
   114|         will be needed to process new user registration
   115|         Args:
   116|             required_attributes: SAML auth response attributes that are
   117|                 necessary to function
   118|             optional_attributes: SAML auth response attributes that can be used to add
   119|                 additional information to Synapse user accounts, but are not required
   120|         Returns:
   121|             dict: A SAML configuration dictionary
   122|         """
   123|         import saml2
   124|         public_baseurl = self.public_baseurl
   125|         if public_baseurl is None:
   126|             raise ConfigError("saml2_config requires a public_baseurl to be set")
   127|         if self.saml2_grandfathered_mxid_source_attribute:
   128|             optional_attributes.add(self.saml2_grandfathered_mxid_source_attribute)
   129|         optional_attributes -= required_attributes


# ====================================================================
# FILE: synapse/config/server.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| import logging
     2| import os.path
     3| import re
     4| from textwrap import indent
     5| from typing import Any, Dict, Iterable, List, Optional, Set
     6| import attr
     7| import yaml
     8| from synapse.api.room_versions import KNOWN_ROOM_VERSIONS
     9| from synapse.http.endpoint import parse_and_validate_server_name
    10| from ._base import Config, ConfigError
    11| logger = logging.Logger(__name__)
    12| DEFAULT_BIND_ADDRESSES = ["::", "0.0.0.0"]
    13| DEFAULT_ROOM_VERSION = "5"
    14| ROOM_COMPLEXITY_TOO_GREAT = (
    15|     "Your homeserver is unable to join rooms this large or complex. "
    16|     "Please speak to your server administrator, or upgrade your instance "
    17|     "to join this room."
    18| )
    19| METRICS_PORT_WARNING = """\
    20| The metrics_port configuration option is deprecated in Synapse 0.31 in favour of
    21| a listener. Please see
    22| https://github.com/matrix-org/synapse/blob/master/docs/metrics-howto.md
    23| on how to configure the new listener.
    24| --------------------------------------------------------------------------------"""
    25| KNOWN_LISTENER_TYPES = {

# --- HUNK 2: Lines 354-401 ---
   354|                         resources=[HttpResourceConfig(names=["metrics"])]
   355|                     ),
   356|                 )
   357|             )
   358|         self.cleanup_extremities_with_dummy_events = config.get(
   359|             "cleanup_extremities_with_dummy_events", True
   360|         )
   361|         self.dummy_events_threshold = config.get("dummy_events_threshold", 10)
   362|         self.enable_ephemeral_messages = config.get("enable_ephemeral_messages", False)
   363|         self.request_token_inhibit_3pid_errors = config.get(
   364|             "request_token_inhibit_3pid_errors", False,
   365|         )
   366|         users_new_default_push_rules = (
   367|             config.get("users_new_default_push_rules") or []
   368|         )  # type: list
   369|         if not isinstance(users_new_default_push_rules, list):
   370|             raise ConfigError("'users_new_default_push_rules' must be a list")
   371|         self.users_new_default_push_rules = set(
   372|             users_new_default_push_rules
   373|         )  # type: set
   374|         next_link_domain_whitelist = config.get(
   375|             "next_link_domain_whitelist"
   376|         )  # type: Optional[List[str]]
   377|         self.next_link_domain_whitelist = None  # type: Optional[Set[str]]
   378|         if next_link_domain_whitelist is not None:
   379|             if not isinstance(next_link_domain_whitelist, list):
   380|                 raise ConfigError("'next_link_domain_whitelist' must be a list")
   381|             self.next_link_domain_whitelist = set(next_link_domain_whitelist)
   382|     def has_tls_listener(self) -> bool:
   383|         return any(listener.tls for listener in self.listeners)
   384|     def generate_config_section(
   385|         self, server_name, data_dir_path, open_private_ports, listeners, **kwargs
   386|     ):
   387|         _, bind_port = parse_and_validate_server_name(server_name)
   388|         if bind_port is not None:
   389|             unsecure_port = bind_port - 400
   390|         else:
   391|             bind_port = 8448
   392|             unsecure_port = 8008
   393|         pid_file = os.path.join(data_dir_path, "homeserver.pid")
   394|         default_room_version = DEFAULT_ROOM_VERSION
   395|         secure_listeners = []
   396|         unsecure_listeners = []
   397|         private_addresses = ["::1", "127.0.0.1"]
   398|         if listeners:
   399|             for listener in listeners:
   400|                 if listener["tls"]:
   401|                     secure_listeners.append(listener)


# ====================================================================
# FILE: synapse/config/server_notices_config.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 3-38 ---
     3| DEFAULT_CONFIG = """\
     4| """
     5| class ServerNoticesConfig(Config):
     6|     """Configuration for the server notices room.
     7|     Attributes:
     8|         server_notices_mxid (str|None):
     9|             The MXID to use for server notices.
    10|             None if server notices are not enabled.
    11|         server_notices_mxid_display_name (str|None):
    12|             The display name to use for the server notices user.
    13|             None if server notices are not enabled.
    14|         server_notices_mxid_avatar_url (str|None):
    15|             The MXC URL for the avatar of the server notices user.
    16|             None if server notices are not enabled.
    17|         server_notices_room_name (str|None):
    18|             The name to use for the server notices room.
    19|             None if server notices are not enabled.
    20|     """
    21|     section = "servernotices"
    22|     def __init__(self, *args):
    23|         super().__init__(*args)
    24|         self.server_notices_mxid = None
    25|         self.server_notices_mxid_display_name = None
    26|         self.server_notices_mxid_avatar_url = None
    27|         self.server_notices_room_name = None
    28|     def read_config(self, config, **kwargs):
    29|         c = config.get("server_notices")
    30|         if c is None:
    31|             return
    32|         mxid_localpart = c["system_mxid_localpart"]
    33|         self.server_notices_mxid = UserID(mxid_localpart, self.server_name).to_string()
    34|         self.server_notices_mxid_display_name = c.get("system_mxid_display_name", None)
    35|         self.server_notices_mxid_avatar_url = c.get("system_mxid_avatar_url", None)
    36|         self.server_notices_room_name = c.get("room_name", "Server Notices")
    37|     def generate_config_section(self, **kwargs):
    38|         return DEFAULT_CONFIG


# ====================================================================
# FILE: synapse/config/stats.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-20 ---
     1| import sys
     2| from ._base import Config
     3| class StatsConfig(Config):
     4|     """Stats Configuration
     5|     Configuration for the behaviour of synapse's stats engine
     6|     """
     7|     section = "stats"
     8|     def read_config(self, config, **kwargs):
     9|         self.stats_enabled = True
    10|         self.stats_bucket_size = 86400 * 1000
    11|         self.stats_retention = sys.maxsize
    12|         stats_config = config.get("stats", None)
    13|         if stats_config:
    14|             self.stats_enabled = stats_config.get("enabled", self.stats_enabled)
    15|             self.stats_bucket_size = self.parse_duration(
    16|                 stats_config.get("bucket_size", "1d")
    17|             )
    18|             self.stats_retention = self.parse_duration(
    19|                 stats_config.get("retention", "%ds" % (sys.maxsize,))
    20|             )


# ====================================================================
# FILE: synapse/config/workers.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-83 ---
     1| from typing import List, Union
     2| import attr
     3| from ._base import Config, ConfigError, ShardedWorkerHandlingConfig
     4| from .server import ListenerConfig, parse_listener_def
     5| def _instance_to_list_converter(obj: Union[str, List[str]]) -> List[str]:
     6|     """Helper for allowing parsing a string or list of strings to a config
     7|     option expecting a list of strings.
     8|     """
     9|     if isinstance(obj, str):
    10|         return [obj]
    11|     return obj
    12| @attr.s
    13| class InstanceLocationConfig:
    14|     """The host and port to talk to an instance via HTTP replication.
    15|     """
    16|     host = attr.ib(type=str)
    17|     port = attr.ib(type=int)
    18| @attr.s
    19| class WriterLocations:
    20|     """Specifies the instances that write various streams.
    21|     Attributes:
    22|         events: The instances that write to the event and backfill streams.
    23|         typing: The instance that writes to the typing stream.
    24|     """
    25|     events = attr.ib(
    26|         default=["master"], type=List[str], converter=_instance_to_list_converter
    27|     )
    28|     typing = attr.ib(default="master", type=str)
    29| class WorkerConfig(Config):
    30|     """The workers are processes run separately to the main synapse process.
    31|     They have their own pid_file and listener configuration. They use the
    32|     replication_url to talk to the main synapse process."""
    33|     section = "worker"
    34|     def read_config(self, config, **kwargs):
    35|         self.worker_app = config.get("worker_app")
    36|         if self.worker_app == "synapse.app.homeserver":
    37|             self.worker_app = None
    38|         self.worker_listeners = [
    39|             parse_listener_def(x) for x in config.get("worker_listeners", [])
    40|         ]
    41|         self.worker_daemonize = config.get("worker_daemonize")
    42|         self.worker_pid_file = config.get("worker_pid_file")
    43|         self.worker_log_config = config.get("worker_log_config")
    44|         self.worker_replication_host = config.get("worker_replication_host", None)
    45|         self.worker_replication_port = config.get("worker_replication_port", None)
    46|         self.worker_replication_http_port = config.get("worker_replication_http_port")
    47|         self.worker_name = config.get("worker_name", self.worker_app)
    48|         self.worker_main_http_uri = config.get("worker_main_http_uri", None)
    49|         manhole = config.get("worker_manhole")
    50|         if manhole:
    51|             self.worker_listeners.append(
    52|                 ListenerConfig(
    53|                     port=manhole, bind_addresses=["127.0.0.1"], type="manhole",
    54|                 )
    55|             )
    56|         self.send_federation = config.get("send_federation", True)
    57|         federation_sender_instances = config.get("federation_sender_instances") or []
    58|         self.federation_shard_config = ShardedWorkerHandlingConfig(
    59|             federation_sender_instances
    60|         )
    61|         instance_map = config.get("instance_map") or {}
    62|         self.instance_map = {
    63|             name: InstanceLocationConfig(**c) for name, c in instance_map.items()
    64|         }
    65|         writers = config.get("stream_writers") or {}
    66|         self.writers = WriterLocations(**writers)
    67|         for stream in ("events", "typing"):
    68|             instances = _instance_to_list_converter(getattr(self.writers, stream))
    69|             for instance in instances:
    70|                 if instance != "master" and instance not in self.instance_map:
    71|                     raise ConfigError(
    72|                         "Instance %r is configured to write %s but does not appear in `instance_map` config."
    73|                         % (instance, stream)
    74|                     )
    75|         self.events_shard_config = ShardedWorkerHandlingConfig(self.writers.events)
    76|     def generate_config_section(self, config_dir_path, server_name, **kwargs):
    77|         return """\
    78|         """
    79|     def read_arguments(self, args):
    80|         if args.daemonize is not None:
    81|             self.worker_daemonize = args.daemonize
    82|         if args.manhole is not None:
    83|             self.worker_manhole = args.worker_manhole


# ====================================================================
# FILE: synapse/crypto/context_factory.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 6-49 ---
     6| from twisted.internet._sslverify import _defaultCurveName
     7| from twisted.internet.abstract import isIPAddress, isIPv6Address
     8| from twisted.internet.interfaces import IOpenSSLClientConnectionCreator
     9| from twisted.internet.ssl import (
    10|     CertificateOptions,
    11|     ContextFactory,
    12|     TLSVersion,
    13|     platformTrust,
    14| )
    15| from twisted.python.failure import Failure
    16| from twisted.web.iweb import IPolicyForHTTPS
    17| logger = logging.getLogger(__name__)
    18| _TLS_VERSION_MAP = {
    19|     "1": TLSVersion.TLSv1_0,
    20|     "1.1": TLSVersion.TLSv1_1,
    21|     "1.2": TLSVersion.TLSv1_2,
    22|     "1.3": TLSVersion.TLSv1_3,
    23| }
    24| class ServerContextFactory(ContextFactory):
    25|     """Factory for PyOpenSSL SSL contexts that are used to handle incoming
    26|     connections.
    27|     TODO: replace this with an implementation of IOpenSSLServerConnectionCreator,
    28|     per https://github.com/matrix-org/synapse/issues/1691
    29|     """
    30|     def __init__(self, config):
    31|         self._context = SSL.Context(SSL.SSLv23_METHOD)
    32|         self.configure_context(self._context, config)
    33|     @staticmethod
    34|     def configure_context(context, config):
    35|         try:
    36|             _ecCurve = crypto.get_elliptic_curve(_defaultCurveName)
    37|             context.set_tmp_ecdh(_ecCurve)
    38|         except Exception:
    39|             logger.exception("Failed to enable elliptic curve for TLS")
    40|         context.set_options(
    41|             SSL.OP_NO_SSLv2 | SSL.OP_NO_SSLv3 | SSL.OP_NO_TLSv1 | SSL.OP_NO_TLSv1_1
    42|         )
    43|         context.use_certificate_chain_file(config.tls_certificate_file)
    44|         context.use_privatekey(config.tls_private_key)
    45|         context.set_cipher_list(
    46|             "ECDH+AESGCM:ECDH+CHACHA20:ECDH+AES256:ECDH+AES128:!aNULL:!SHA1:!AESCCM"
    47|         )
    48|     def getContext(self):
    49|         return self._context


# ====================================================================
# FILE: synapse/crypto/keyring.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 6-45 ---
     6|     decode_verify_key_bytes,
     7|     encode_verify_key_base64,
     8|     is_signing_algorithm_supported,
     9| )
    10| from signedjson.sign import (
    11|     SignatureVerifyException,
    12|     encode_canonical_json,
    13|     signature_ids,
    14|     verify_signed_json,
    15| )
    16| from unpaddedbase64 import decode_base64
    17| from twisted.internet import defer
    18| from synapse.api.errors import (
    19|     Codes,
    20|     HttpResponseException,
    21|     RequestSendFailed,
    22|     SynapseError,
    23| )
    24| from synapse.logging.context import (
    25|     PreserveLoggingContext,
    26|     make_deferred_yieldable,
    27|     preserve_fn,
    28|     run_in_background,
    29| )
    30| from synapse.storage.keys import FetchKeyResult
    31| from synapse.util import unwrapFirstError
    32| from synapse.util.async_helpers import yieldable_gather_results
    33| from synapse.util.metrics import Measure
    34| from synapse.util.retryutils import NotRetryingDestination
    35| logger = logging.getLogger(__name__)
    36| @attr.s(slots=True, cmp=False)
    37| class VerifyJsonRequest:
    38|     """
    39|     A request to verify a JSON object.
    40|     Attributes:
    41|         server_name(str): The name of the server to verify against.
    42|         key_ids(set[str]): The set of key_ids to that could be used to verify the
    43|             JSON object
    44|         json_object(dict): The JSON object to verify.
    45|         minimum_valid_until_ts (int): time at which we require the signing key to

# --- HUNK 2: Lines 140-198 ---
   140|             logger.debug(
   141|                 "Verifying %s for %s with key_ids %s, min_validity %i",
   142|                 verify_request.request_name,
   143|                 verify_request.server_name,
   144|                 verify_request.key_ids,
   145|                 verify_request.minimum_valid_until_ts,
   146|             )
   147|             key_lookups.append(verify_request)
   148|             return handle(verify_request)
   149|         results = [process(r) for r in verify_requests]
   150|         if key_lookups:
   151|             run_in_background(self._start_key_lookups, key_lookups)
   152|         return results
   153|     async def _start_key_lookups(self, verify_requests):
   154|         """Sets off the key fetches for each verify request
   155|         Once each fetch completes, verify_request.key_ready will be resolved.
   156|         Args:
   157|             verify_requests (List[VerifyJsonRequest]):
   158|         """
   159|         try:
   160|             server_to_request_ids = {}
   161|             for verify_request in verify_requests:
   162|                 server_name = verify_request.server_name
   163|                 request_id = id(verify_request)
   164|                 server_to_request_ids.setdefault(server_name, set()).add(request_id)
   165|             await self.wait_for_previous_lookups(server_to_request_ids.keys())
   166|             for server_name in server_to_request_ids.keys():
   167|                 self.key_downloads[server_name] = defer.Deferred()
   168|                 logger.debug("Got key lookup lock on %s", server_name)
   169|             def drop_server_lock(server_name):
   170|                 d = self.key_downloads.pop(server_name)
   171|                 d.callback(None)
   172|             def lookup_done(res, verify_request):
   173|                 server_name = verify_request.server_name
   174|                 server_requests = server_to_request_ids[server_name]
   175|                 server_requests.remove(id(verify_request))
   176|                 if not server_requests:
   177|                     logger.debug("Releasing key lookup lock on %s", server_name)
   178|                     drop_server_lock(server_name)
   179|                 return res
   180|             for verify_request in verify_requests:
   181|                 verify_request.key_ready.addBoth(lookup_done, verify_request)
   182|             self._get_server_verify_keys(verify_requests)
   183|         except Exception:
   184|             logger.exception("Error starting key lookups")
   185|     async def wait_for_previous_lookups(self, server_names) -> None:
   186|         """Waits for any previous key lookups for the given servers to finish.
   187|         Args:
   188|             server_names (Iterable[str]): list of servers which we want to look up
   189|         Returns:
   190|             Resolves once all key lookups for the given servers have
   191|                 completed. Follows the synapse rules of logcontext preservation.
   192|         """
   193|         loop_count = 1
   194|         while True:
   195|             wait_on = [
   196|                 (server_name, self.key_downloads[server_name])
   197|                 for server_name in server_names
   198|                 if server_name in self.key_downloads

# --- HUNK 3: Lines 208-314 ---
   208|                 await defer.DeferredList((w[1] for w in wait_on))
   209|             loop_count += 1
   210|     def _get_server_verify_keys(self, verify_requests):
   211|         """Tries to find at least one key for each verify request
   212|         For each verify_request, verify_request.key_ready is called back with
   213|         params (server_name, key_id, VerifyKey) if a key is found, or errbacked
   214|         with a SynapseError if none of the keys are found.
   215|         Args:
   216|             verify_requests (list[VerifyJsonRequest]): list of verify requests
   217|         """
   218|         remaining_requests = {rq for rq in verify_requests if not rq.key_ready.called}
   219|         async def do_iterations():
   220|             try:
   221|                 with Measure(self.clock, "get_server_verify_keys"):
   222|                     for f in self._key_fetchers:
   223|                         if not remaining_requests:
   224|                             return
   225|                         await self._attempt_key_fetches_with_fetcher(
   226|                             f, remaining_requests
   227|                         )
   228|                     while remaining_requests:
   229|                         verify_request = remaining_requests.pop()
   230|                         rq_str = (
   231|                             "VerifyJsonRequest(server=%s, key_ids=%s, min_valid=%i)"
   232|                             % (
   233|                                 verify_request.server_name,
   234|                                 verify_request.key_ids,
   235|                                 verify_request.minimum_valid_until_ts,
   236|                             )
   237|                         )
   238|                         self.clock.call_later(
   239|                             0,
   240|                             verify_request.key_ready.errback,
   241|                             SynapseError(
   242|                                 401,
   243|                                 "Failed to find any key to satisfy %s" % (rq_str,),
   244|                                 Codes.UNAUTHORIZED,
   245|                             ),
   246|                         )
   247|             except Exception as err:
   248|                 logger.error("Unexpected error in _get_server_verify_keys: %s", err)
   249|                 with PreserveLoggingContext():
   250|                     for verify_request in remaining_requests:
   251|                         if not verify_request.key_ready.called:
   252|                             verify_request.key_ready.errback(err)
   253|         run_in_background(do_iterations)
   254|     async def _attempt_key_fetches_with_fetcher(self, fetcher, remaining_requests):
   255|         """Use a key fetcher to attempt to satisfy some key requests
   256|         Args:
   257|             fetcher (KeyFetcher): fetcher to use to fetch the keys
   258|             remaining_requests (set[VerifyJsonRequest]): outstanding key requests.
   259|                 Any successfully-completed requests will be removed from the list.
   260|         """
   261|         missing_keys = defaultdict(dict)
   262|         for verify_request in remaining_requests:
   263|             assert not verify_request.key_ready.called
   264|             keys_for_server = missing_keys[verify_request.server_name]
   265|             for key_id in verify_request.key_ids:
   266|                 keys_for_server[key_id] = max(
   267|                     keys_for_server.get(key_id, -1),
   268|                     verify_request.minimum_valid_until_ts,
   269|                 )
   270|         results = await fetcher.get_keys(missing_keys)
   271|         completed = []
   272|         for verify_request in remaining_requests:
   273|             server_name = verify_request.server_name
   274|             result_keys = results.get(server_name, {})
   275|             for key_id in verify_request.key_ids:
   276|                 fetch_key_result = result_keys.get(key_id)
   277|                 if not fetch_key_result:
   278|                     continue
   279|                 if (
   280|                     fetch_key_result.valid_until_ts
   281|                     < verify_request.minimum_valid_until_ts
   282|                 ):
   283|                     continue
   284|                 logger.debug(
   285|                     "Found key %s:%s for %s",
   286|                     server_name,
   287|                     key_id,
   288|                     verify_request.request_name,
   289|                 )
   290|                 self.clock.call_later(
   291|                     0,
   292|                     verify_request.key_ready.callback,
   293|                     (server_name, key_id, fetch_key_result.verify_key),
   294|                 )
   295|                 completed.append(verify_request)
   296|                 break
   297|         remaining_requests.difference_update(completed)
   298| class KeyFetcher:
   299|     async def get_keys(self, keys_to_fetch):
   300|         """
   301|         Args:
   302|             keys_to_fetch (dict[str, dict[str, int]]):
   303|                 the keys to be fetched. server_name -> key_id -> min_valid_ts
   304|         Returns:
   305|             Deferred[dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]]:
   306|                 map from server_name -> key_id -> FetchKeyResult
   307|         """
   308|         raise NotImplementedError
   309| class StoreKeyFetcher(KeyFetcher):
   310|     """KeyFetcher impl which fetches keys from our data store"""
   311|     def __init__(self, hs):
   312|         self.store = hs.get_datastore()
   313|     async def get_keys(self, keys_to_fetch):
   314|         """see KeyFetcher.get_keys"""

# --- HUNK 4: Lines 381-421 ---
   381|             defer.gatherResults(
   382|                 [
   383|                     run_in_background(
   384|                         self.store.store_server_keys_json,
   385|                         server_name=server_name,
   386|                         key_id=key_id,
   387|                         from_server=from_server,
   388|                         ts_now_ms=time_added_ms,
   389|                         ts_expires_ms=ts_valid_until_ms,
   390|                         key_json_bytes=key_json_bytes,
   391|                     )
   392|                     for key_id in verify_keys
   393|                 ],
   394|                 consumeErrors=True,
   395|             ).addErrback(unwrapFirstError)
   396|         )
   397|         return verify_keys
   398| class PerspectivesKeyFetcher(BaseV2KeyFetcher):
   399|     """KeyFetcher impl which fetches keys from the "perspectives" servers"""
   400|     def __init__(self, hs):
   401|         super().__init__(hs)
   402|         self.clock = hs.get_clock()
   403|         self.client = hs.get_http_client()
   404|         self.key_servers = self.config.key_servers
   405|     async def get_keys(self, keys_to_fetch):
   406|         """see KeyFetcher.get_keys"""
   407|         async def get_key(key_server):
   408|             try:
   409|                 result = await self.get_server_verify_key_v2_indirect(
   410|                     keys_to_fetch, key_server
   411|                 )
   412|                 return result
   413|             except KeyLookupError as e:
   414|                 logger.warning(
   415|                     "Key lookup failed from %r: %s", key_server.server_name, e
   416|                 )
   417|             except Exception as e:
   418|                 logger.exception(
   419|                     "Unable to get key from %r: %s %s",
   420|                     key_server.server_name,
   421|                     type(e).__name__,

# --- HUNK 5: Lines 518-558 ---
   518|             "signatures" not in response
   519|             or perspective_name not in response["signatures"]
   520|         ):
   521|             raise KeyLookupError("Response not signed by the notary server")
   522|         verified = False
   523|         for key_id in response["signatures"][perspective_name]:
   524|             if key_id in perspective_keys:
   525|                 verify_signed_json(response, perspective_name, perspective_keys[key_id])
   526|                 verified = True
   527|         if not verified:
   528|             raise KeyLookupError(
   529|                 "Response not signed with a known key: signed with: %r, known keys: %r"
   530|                 % (
   531|                     list(response["signatures"][perspective_name].keys()),
   532|                     list(perspective_keys.keys()),
   533|                 )
   534|             )
   535| class ServerKeyFetcher(BaseV2KeyFetcher):
   536|     """KeyFetcher impl which fetches keys from the origin servers"""
   537|     def __init__(self, hs):
   538|         super().__init__(hs)
   539|         self.clock = hs.get_clock()
   540|         self.client = hs.get_http_client()
   541|     async def get_keys(self, keys_to_fetch):
   542|         """
   543|         Args:
   544|             keys_to_fetch (dict[str, iterable[str]]):
   545|                 the keys to be fetched. server_name -> key_ids
   546|         Returns:
   547|             dict[str, dict[str, synapse.storage.keys.FetchKeyResult|None]]:
   548|                 map from server_name -> key_id -> FetchKeyResult
   549|         """
   550|         results = {}
   551|         async def get_key(key_to_fetch_item):
   552|             server_name, key_ids = key_to_fetch_item
   553|             try:
   554|                 keys = await self.get_server_verify_key_v2_direct(server_name, key_ids)
   555|                 results[server_name] = keys
   556|             except KeyLookupError as e:
   557|                 logger.warning(
   558|                     "Error looking up keys %s from %s: %s", key_ids, server_name, e


# ====================================================================
# FILE: synapse/events/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-27 ---
     1| import abc
     2| import os
     3| from distutils.util import strtobool
     4| from typing import Dict, Optional, Tuple, Type
     5| from unpaddedbase64 import encode_base64
     6| from synapse.api.room_versions import EventFormatVersions, RoomVersion, RoomVersions
     7| from synapse.types import JsonDict, RoomStreamToken
     8| from synapse.util.caches import intern_dict
     9| from synapse.util.frozenutils import freeze
    10| USE_FROZEN_DICTS = strtobool(os.environ.get("SYNAPSE_USE_FROZEN_DICTS", "0"))
    11| class DictProperty:
    12|     """An object property which delegates to the `_dict` within its parent object."""
    13|     __slots__ = ["key"]
    14|     def __init__(self, key: str):
    15|         self.key = key
    16|     def __get__(self, instance, owner=None):
    17|         if instance is None:
    18|             return self
    19|         try:
    20|             return instance._dict[self.key]
    21|         except KeyError as e1:
    22|             raise AttributeError(
    23|                 "'%s' has no '%s' property" % (type(instance), self.key)
    24|             ) from e1.__context__
    25|     def __set__(self, instance, v):
    26|         instance._dict[self.key] = v
    27|     def __delete__(self, instance):

# --- HUNK 2: Lines 41-82 ---
    41|         super().__init__(key)
    42|         self.default = default
    43|     def __get__(self, instance, owner=None):
    44|         if instance is None:
    45|             return self
    46|         return instance._dict.get(self.key, self.default)
    47| class _EventInternalMetadata:
    48|     __slots__ = ["_dict"]
    49|     def __init__(self, internal_metadata_dict: JsonDict):
    50|         self._dict = dict(internal_metadata_dict)
    51|     outlier = DictProperty("outlier")  # type: bool
    52|     out_of_band_membership = DictProperty("out_of_band_membership")  # type: bool
    53|     send_on_behalf_of = DictProperty("send_on_behalf_of")  # type: str
    54|     recheck_redaction = DictProperty("recheck_redaction")  # type: bool
    55|     soft_failed = DictProperty("soft_failed")  # type: bool
    56|     proactively_send = DictProperty("proactively_send")  # type: bool
    57|     redacted = DictProperty("redacted")  # type: bool
    58|     txn_id = DictProperty("txn_id")  # type: str
    59|     token_id = DictProperty("token_id")  # type: str
    60|     stream_ordering = DictProperty("stream_ordering")  # type: int
    61|     before = DictProperty("before")  # type: RoomStreamToken
    62|     after = DictProperty("after")  # type: RoomStreamToken
    63|     order = DictProperty("order")  # type: Tuple[int, int]
    64|     def get_dict(self) -> JsonDict:
    65|         return dict(self._dict)
    66|     def is_outlier(self) -> bool:
    67|         return self._dict.get("outlier", False)
    68|     def is_out_of_band_membership(self) -> bool:
    69|         """Whether this is an out of band membership, like an invite or an invite
    70|         rejection. This is needed as those events are marked as outliers, but
    71|         they still need to be processed as if they're new events (e.g. updating
    72|         invite state in the database, relaying to clients, etc).
    73|         (Added in synapse 0.99.0, so may be unreliable for events received before that)
    74|         """
    75|         return self._dict.get("out_of_band_membership", False)
    76|     def get_send_on_behalf_of(self) -> Optional[str]:
    77|         """Whether this server should send the event on behalf of another server.
    78|         This is used by the federation "send_join" API to forward the initial join
    79|         event for a server in the room.
    80|         returns a str with the name of the server this event is sent on behalf of.
    81|         """
    82|         return self._dict.get("send_on_behalf_of")


# ====================================================================
# FILE: synapse/federation/federation_client.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-75 ---
     1| import copy
     2| import itertools
     3| import logging
     4| from typing import (
     5|     Any,
     6|     Awaitable,
     7|     Callable,
     8|     Dict,
     9|     Iterable,
    10|     List,
    11|     Mapping,
    12|     Optional,
    13|     Sequence,
    14|     Tuple,
    15|     TypeVar,
    16|     Union,
    17| )
    18| from prometheus_client import Counter
    19| from twisted.internet import defer
    20| from twisted.internet.defer import Deferred
    21| from synapse.api.constants import EventTypes, Membership
    22| from synapse.api.errors import (
    23|     CodeMessageException,
    24|     Codes,
    25|     FederationDeniedError,
    26|     HttpResponseException,
    27|     SynapseError,
    28|     UnsupportedRoomVersionError,
    29| )
    30| from synapse.api.room_versions import (
    31|     KNOWN_ROOM_VERSIONS,
    32|     EventFormatVersions,
    33|     RoomVersion,
    34|     RoomVersions,
    35| )
    36| from synapse.events import EventBase, builder
    37| from synapse.federation.federation_base import FederationBase, event_from_pdu_json
    38| from synapse.logging.context import make_deferred_yieldable, preserve_fn
    39| from synapse.logging.utils import log_function
    40| from synapse.types import JsonDict, get_domain_from_id
    41| from synapse.util import unwrapFirstError
    42| from synapse.util.caches.expiringcache import ExpiringCache
    43| from synapse.util.retryutils import NotRetryingDestination
    44| logger = logging.getLogger(__name__)
    45| sent_queries_counter = Counter("synapse_federation_client_sent_queries", "", ["type"])
    46| PDU_RETRY_TIME_MS = 1 * 60 * 1000
    47| T = TypeVar("T")
    48| class InvalidResponseError(RuntimeError):
    49|     """Helper for _try_destination_list: indicates that the server returned a response
    50|     we couldn't parse
    51|     """
    52|     pass
    53| class FederationClient(FederationBase):
    54|     def __init__(self, hs):
    55|         super().__init__(hs)
    56|         self.pdu_destination_tried = {}
    57|         self._clock.looping_call(self._clear_tried_cache, 60 * 1000)
    58|         self.state = hs.get_state_handler()
    59|         self.transport_layer = hs.get_federation_transport_client()
    60|         self.hostname = hs.hostname
    61|         self.signing_key = hs.signing_key
    62|         self._get_pdu_cache = ExpiringCache(
    63|             cache_name="get_pdu_cache",
    64|             clock=self._clock,
    65|             max_len=1000,
    66|             expiry_ms=120 * 1000,
    67|             reset_expiry_on_get=False,
    68|         )
    69|     def _clear_tried_cache(self):
    70|         """Clear pdu_destination_tried cache"""
    71|         now = self._clock.time_msec()
    72|         old_dict = self.pdu_destination_tried
    73|         self.pdu_destination_tried = {}
    74|         for event_id, destination_dict in old_dict.items():
    75|             destination_dict = {

# --- HUNK 2: Lines 372-412 ---
   372|                 else:
   373|                     logger.warning(
   374|                         "Failed to %s via %s: %i %s",
   375|                         description,
   376|                         destination,
   377|                         e.code,
   378|                         e.args[0],
   379|                     )
   380|             except Exception:
   381|                 logger.warning(
   382|                     "Failed to %s via %s", description, destination, exc_info=True
   383|                 )
   384|         raise SynapseError(502, "Failed to %s via any server" % (description,))
   385|     async def make_membership_event(
   386|         self,
   387|         destinations: Iterable[str],
   388|         room_id: str,
   389|         user_id: str,
   390|         membership: str,
   391|         content: dict,
   392|         params: Optional[Mapping[str, Union[str, Iterable[str]]]],
   393|     ) -> Tuple[str, EventBase, RoomVersion]:
   394|         """
   395|         Creates an m.room.member event, with context, without participating in the room.
   396|         Does so by asking one of the already participating servers to create an
   397|         event with proper context.
   398|         Returns a fully signed and hashed event.
   399|         Note that this does not append any events to any graphs.
   400|         Args:
   401|             destinations: Candidate homeservers which are probably
   402|                 participating in the room.
   403|             room_id: The room in which the event will happen.
   404|             user_id: The user whose membership is being evented.
   405|             membership: The "membership" property of the event. Must be one of
   406|                 "join" or "leave".
   407|             content: Any additional data to put into the content field of the
   408|                 event.
   409|             params: Query parameters to include in the request.
   410|         Returns:
   411|             `(origin, event, room_version)` where origin is the remote
   412|             homeserver which generated the event, and room_version is the


# ====================================================================
# FILE: synapse/federation/federation_server.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 1-34 ---
     1| import logging
     2| from typing import (
     3|     TYPE_CHECKING,
     4|     Any,
     5|     Awaitable,
     6|     Callable,
     7|     Dict,
     8|     List,
     9|     Match,
    10|     Optional,
    11|     Tuple,
    12|     Union,
    13| )
    14| from prometheus_client import Counter, Gauge, Histogram
    15| from twisted.internet import defer
    16| from twisted.internet.abstract import isIPAddress
    17| from twisted.python import failure
    18| from synapse.api.constants import EventTypes, Membership
    19| from synapse.api.errors import (
    20|     AuthError,
    21|     Codes,
    22|     FederationError,
    23|     IncompatibleRoomVersionError,
    24|     NotFoundError,
    25|     SynapseError,
    26|     UnsupportedRoomVersionError,
    27| )
    28| from synapse.api.room_versions import KNOWN_ROOM_VERSIONS
    29| from synapse.events import EventBase
    30| from synapse.federation.federation_base import FederationBase, event_from_pdu_json
    31| from synapse.federation.persistence import TransactionActions
    32| from synapse.federation.units import Edu, Transaction
    33| from synapse.http.endpoint import parse_server_name
    34| from synapse.logging.context import (

# --- HUNK 2: Lines 41-106 ---
    41| from synapse.replication.http.federation import (
    42|     ReplicationFederationSendEduRestServlet,
    43|     ReplicationGetQueryRestServlet,
    44| )
    45| from synapse.types import JsonDict, get_domain_from_id
    46| from synapse.util import glob_to_regex, json_decoder, unwrapFirstError
    47| from synapse.util.async_helpers import Linearizer, concurrently_execute
    48| from synapse.util.caches.response_cache import ResponseCache
    49| if TYPE_CHECKING:
    50|     from synapse.server import HomeServer
    51| TRANSACTION_CONCURRENCY_LIMIT = 10
    52| logger = logging.getLogger(__name__)
    53| received_pdus_counter = Counter("synapse_federation_server_received_pdus", "")
    54| received_edus_counter = Counter("synapse_federation_server_received_edus", "")
    55| received_queries_counter = Counter(
    56|     "synapse_federation_server_received_queries", "", ["type"]
    57| )
    58| pdu_process_time = Histogram(
    59|     "synapse_federation_server_pdu_process_time", "Time taken to process an event",
    60| )
    61| last_pdu_age_metric = Gauge(
    62|     "synapse_federation_last_received_pdu_age",
    63|     "The age (in seconds) of the last PDU successfully received from the given domain",
    64|     labelnames=("server_name",),
    65| )
    66| class FederationServer(FederationBase):
    67|     def __init__(self, hs):
    68|         super().__init__(hs)
    69|         self.auth = hs.get_auth()
    70|         self.handler = hs.get_handlers().federation_handler
    71|         self.state = hs.get_state_handler()
    72|         self.device_handler = hs.get_device_handler()
    73|         self._federation_ratelimiter = hs.get_federation_ratelimiter()
    74|         self._server_linearizer = Linearizer("fed_server")
    75|         self._transaction_linearizer = Linearizer("fed_txn_handler")
    76|         self._transaction_resp_cache = ResponseCache(
    77|             hs, "fed_txn_handler", timeout_ms=30000
    78|         )
    79|         self.transaction_actions = TransactionActions(self.store)
    80|         self.registry = hs.get_federation_registry()
    81|         self._state_resp_cache = ResponseCache(hs, "state_resp", timeout_ms=30000)
    82|         self._state_ids_resp_cache = ResponseCache(
    83|             hs, "state_ids_resp", timeout_ms=30000
    84|         )
    85|         self._federation_metrics_domains = (
    86|             hs.get_config().federation.federation_metrics_domains
    87|         )
    88|     async def on_backfill_request(
    89|         self, origin: str, room_id: str, versions: List[str], limit: int
    90|     ) -> Tuple[int, Dict[str, Any]]:
    91|         with (await self._server_linearizer.queue((origin, room_id))):
    92|             origin_host, _ = parse_server_name(origin)
    93|             await self.check_server_matches_acl(origin_host, room_id)
    94|             pdus = await self.handler.on_backfill_request(
    95|                 origin, room_id, versions, limit
    96|             )
    97|             res = self._transaction_from_pdus(pdus).get_dict()
    98|         return 200, res
    99|     async def on_incoming_transaction(
   100|         self, origin: str, transaction_data: JsonDict
   101|     ) -> Tuple[int, Dict[str, Any]]:
   102|         request_time = self._clock.time_msec()
   103|         transaction = Transaction(**transaction_data)
   104|         transaction_id = transaction.transaction_id  # type: ignore
   105|         if not transaction_id:
   106|             raise Exception("Transaction missing transaction_id")

# --- HUNK 3: Lines 163-266 ---
   163|         )
   164|         response = {"pdus": pdu_results}
   165|         logger.debug("Returning: %s", str(response))
   166|         await self.transaction_actions.set_response(origin, transaction, 200, response)
   167|         return 200, response
   168|     async def _handle_pdus_in_txn(
   169|         self, origin: str, transaction: Transaction, request_time: int
   170|     ) -> Dict[str, dict]:
   171|         """Process the PDUs in a received transaction.
   172|         Args:
   173|             origin: the server making the request
   174|             transaction: incoming transaction
   175|             request_time: timestamp that the HTTP request arrived at
   176|         Returns:
   177|             A map from event ID of a processed PDU to any errors we should
   178|             report back to the sending server.
   179|         """
   180|         received_pdus_counter.inc(len(transaction.pdus))  # type: ignore
   181|         origin_host, _ = parse_server_name(origin)
   182|         pdus_by_room = {}  # type: Dict[str, List[EventBase]]
   183|         newest_pdu_ts = 0
   184|         for p in transaction.pdus:  # type: ignore
   185|             if "unsigned" in p:
   186|                 unsigned = p["unsigned"]
   187|                 if "age" in unsigned:
   188|                     p["age"] = unsigned["age"]
   189|             if "age" in p:
   190|                 p["age_ts"] = request_time - int(p["age"])
   191|                 del p["age"]
   192|             possible_event_id = p.get("event_id", "<Unknown>")
   193|             room_id = p.get("room_id")
   194|             if not room_id:
   195|                 logger.info(
   196|                     "Ignoring PDU as does not have a room_id. Event ID: %s",
   197|                     possible_event_id,
   198|                 )
   199|                 continue
   200|             try:
   201|                 room_version = await self.store.get_room_version(room_id)
   202|             except NotFoundError:
   203|                 logger.info("Ignoring PDU for unknown room_id: %s", room_id)
   204|                 continue
   205|             except UnsupportedRoomVersionError as e:
   206|                 logger.info("Ignoring PDU: %s", e)
   207|                 continue
   208|             event = event_from_pdu_json(p, room_version)
   209|             pdus_by_room.setdefault(room_id, []).append(event)
   210|             if event.origin_server_ts > newest_pdu_ts:
   211|                 newest_pdu_ts = event.origin_server_ts
   212|         pdu_results = {}
   213|         async def process_pdus_for_room(room_id: str):
   214|             logger.debug("Processing PDUs for %s", room_id)
   215|             try:
   216|                 await self.check_server_matches_acl(origin_host, room_id)
   217|             except AuthError as e:
   218|                 logger.warning("Ignoring PDUs for room %s from banned server", room_id)
   219|                 for pdu in pdus_by_room[room_id]:
   220|                     event_id = pdu.event_id
   221|                     pdu_results[event_id] = e.error_dict()
   222|                 return
   223|             for pdu in pdus_by_room[room_id]:
   224|                 event_id = pdu.event_id
   225|                 with pdu_process_time.time():
   226|                     with nested_logging_context(event_id):
   227|                         try:
   228|                             await self._handle_received_pdu(origin, pdu)
   229|                             pdu_results[event_id] = {}
   230|                         except FederationError as e:
   231|                             logger.warning("Error handling PDU %s: %s", event_id, e)
   232|                             pdu_results[event_id] = {"error": str(e)}
   233|                         except Exception as e:
   234|                             f = failure.Failure()
   235|                             pdu_results[event_id] = {"error": str(e)}
   236|                             logger.error(
   237|                                 "Failed to handle PDU %s",
   238|                                 event_id,
   239|                                 exc_info=(f.type, f.value, f.getTracebackObject()),
   240|                             )
   241|         await concurrently_execute(
   242|             process_pdus_for_room, pdus_by_room.keys(), TRANSACTION_CONCURRENCY_LIMIT
   243|         )
   244|         if newest_pdu_ts and origin in self._federation_metrics_domains:
   245|             newest_pdu_age = self._clock.time_msec() - newest_pdu_ts
   246|             last_pdu_age_metric.labels(server_name=origin).set(newest_pdu_age / 1000)
   247|         return pdu_results
   248|     async def _handle_edus_in_txn(self, origin: str, transaction: Transaction):
   249|         """Process the EDUs in a received transaction.
   250|         """
   251|         async def _process_edu(edu_dict):
   252|             received_edus_counter.inc()
   253|             edu = Edu(
   254|                 origin=origin,
   255|                 destination=self.server_name,
   256|                 edu_type=edu_dict["edu_type"],
   257|                 content=edu_dict["content"],
   258|             )
   259|             await self.registry.on_edu(edu.edu_type, origin, edu.content)
   260|         await concurrently_execute(
   261|             _process_edu,
   262|             getattr(transaction, "edus", []),
   263|             TRANSACTION_CONCURRENCY_LIMIT,
   264|         )
   265|     async def on_context_state_request(
   266|         self, origin: str, room_id: str, event_id: str


# ====================================================================
# FILE: synapse/federation/sender/__init__.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 16-57 ---
    16|     run_in_background,
    17| )
    18| from synapse.metrics import (
    19|     LaterGauge,
    20|     event_processing_loop_counter,
    21|     event_processing_loop_room_count,
    22|     events_processed_counter,
    23| )
    24| from synapse.metrics.background_process_metrics import run_as_background_process
    25| from synapse.types import ReadReceipt
    26| from synapse.util.metrics import Measure, measure_func
    27| logger = logging.getLogger(__name__)
    28| sent_pdus_destination_dist_count = Counter(
    29|     "synapse_federation_client_sent_pdu_destinations:count",
    30|     "Number of PDUs queued for sending to one or more destinations",
    31| )
    32| sent_pdus_destination_dist_total = Counter(
    33|     "synapse_federation_client_sent_pdu_destinations:total",
    34|     "Total number of PDUs queued for sending across all destinations",
    35| )
    36| CATCH_UP_STARTUP_DELAY_SEC = 15
    37| CATCH_UP_STARTUP_INTERVAL_SEC = 5
    38| class FederationSender:
    39|     def __init__(self, hs: "synapse.server.HomeServer"):
    40|         self.hs = hs
    41|         self.server_name = hs.hostname
    42|         self.store = hs.get_datastore()
    43|         self.state = hs.get_state_handler()
    44|         self.clock = hs.get_clock()
    45|         self.is_mine_id = hs.is_mine_id
    46|         self._transaction_manager = TransactionManager(hs)
    47|         self._instance_name = hs.get_instance_name()
    48|         self._federation_shard_config = hs.config.worker.federation_shard_config
    49|         self._per_destination_queues = {}  # type: Dict[str, PerDestinationQueue]
    50|         LaterGauge(
    51|             "synapse_federation_transaction_queue_pending_destinations",
    52|             "",
    53|             [],
    54|             lambda: sum(
    55|                 1
    56|                 for d in self._per_destination_queues.values()
    57|                 if d.transmission_loop_running

# --- HUNK 2: Lines 65-110 ---
    65|             lambda: sum(
    66|                 d.pending_pdu_count() for d in self._per_destination_queues.values()
    67|             ),
    68|         )
    69|         LaterGauge(
    70|             "synapse_federation_transaction_queue_pending_edus",
    71|             "",
    72|             [],
    73|             lambda: sum(
    74|                 d.pending_edu_count() for d in self._per_destination_queues.values()
    75|             ),
    76|         )
    77|         self._is_processing = False
    78|         self._last_poked_id = -1
    79|         self._processing_pending_presence = False
    80|         self._queues_awaiting_rr_flush_by_room = (
    81|             {}
    82|         )  # type: Dict[str, Set[PerDestinationQueue]]
    83|         self._rr_txn_interval_per_room_ms = (
    84|             1000.0 / hs.config.federation_rr_transactions_per_room_per_second
    85|         )
    86|         self._catchup_after_startup_timer = self.clock.call_later(
    87|             CATCH_UP_STARTUP_DELAY_SEC,
    88|             run_as_background_process,
    89|             "wake_destinations_needing_catchup",
    90|             self._wake_destinations_needing_catchup,
    91|         )
    92|     def _get_per_destination_queue(self, destination: str) -> PerDestinationQueue:
    93|         """Get or create a PerDestinationQueue for the given destination
    94|         Args:
    95|             destination: server_name of remote server
    96|         """
    97|         queue = self._per_destination_queues.get(destination)
    98|         if not queue:
    99|             queue = PerDestinationQueue(self.hs, self._transaction_manager, destination)
   100|             self._per_destination_queues[destination] = queue
   101|         return queue
   102|     def notify_new_events(self, current_id: int) -> None:
   103|         """This gets called when we have some new events we might want to
   104|         send out to other servers.
   105|         """
   106|         self._last_poked_id = max(current_id, self._last_poked_id)
   107|         if self._is_processing:
   108|             return
   109|         run_as_background_process(
   110|             "process_event_queue_for_federation", self._process_event_queue_loop

# --- HUNK 3: Lines 131-171 ---
   131|                         destinations = await self.state.get_hosts_in_room_at_events(
   132|                             event.room_id, event_ids=event.prev_event_ids()
   133|                         )
   134|                     except Exception:
   135|                         logger.exception(
   136|                             "Failed to calculate hosts in room for event: %s",
   137|                             event.event_id,
   138|                         )
   139|                         return
   140|                     destinations = {
   141|                         d
   142|                         for d in destinations
   143|                         if self._federation_shard_config.should_handle(
   144|                             self._instance_name, d
   145|                         )
   146|                     }
   147|                     if send_on_behalf_of is not None:
   148|                         destinations.discard(send_on_behalf_of)
   149|                     logger.debug("Sending %s to %r", event, destinations)
   150|                     if destinations:
   151|                         await self._send_pdu(event, destinations)
   152|                         now = self.clock.time_msec()
   153|                         ts = await self.store.get_received_ts(event.event_id)
   154|                         synapse.metrics.event_processing_lag_by_event.labels(
   155|                             "federation_sender"
   156|                         ).observe((now - ts) / 1000)
   157|                 async def handle_room_events(events: Iterable[EventBase]) -> None:
   158|                     with Measure(self.clock, "handle_room_events"):
   159|                         for event in events:
   160|                             await handle_event(event)
   161|                 events_by_room = {}  # type: Dict[str, List[EventBase]]
   162|                 for event in events:
   163|                     events_by_room.setdefault(event.room_id, []).append(event)
   164|                 await make_deferred_yieldable(
   165|                     defer.gatherResults(
   166|                         [
   167|                             run_in_background(handle_room_events, evs)
   168|                             for evs in events_by_room.values()
   169|                         ],
   170|                         consumeErrors=True,
   171|                     )

# --- HUNK 4: Lines 173-223 ---
   173|                 await self.store.update_federation_out_pos("events", next_token)
   174|                 if events:
   175|                     now = self.clock.time_msec()
   176|                     ts = await self.store.get_received_ts(events[-1].event_id)
   177|                     synapse.metrics.event_processing_lag.labels(
   178|                         "federation_sender"
   179|                     ).set(now - ts)
   180|                     synapse.metrics.event_processing_last_ts.labels(
   181|                         "federation_sender"
   182|                     ).set(ts)
   183|                     events_processed_counter.inc(len(events))
   184|                     event_processing_loop_room_count.labels("federation_sender").inc(
   185|                         len(events_by_room)
   186|                     )
   187|                 event_processing_loop_counter.labels("federation_sender").inc()
   188|                 synapse.metrics.event_processing_positions.labels(
   189|                     "federation_sender"
   190|                 ).set(next_token)
   191|         finally:
   192|             self._is_processing = False
   193|     async def _send_pdu(self, pdu: EventBase, destinations: Iterable[str]) -> None:
   194|         destinations = set(destinations)
   195|         destinations.discard(self.server_name)
   196|         logger.debug("Sending to: %s", str(destinations))
   197|         if not destinations:
   198|             return
   199|         sent_pdus_destination_dist_total.inc(len(destinations))
   200|         sent_pdus_destination_dist_count.inc()
   201|         await self.store.store_destination_rooms_entries(
   202|             destinations, pdu.room_id, pdu.internal_metadata.stream_ordering,
   203|         )
   204|         for destination in destinations:
   205|             self._get_per_destination_queue(destination).send_pdu(pdu)
   206|     async def send_read_receipt(self, receipt: ReadReceipt) -> None:
   207|         """Send a RR to any other servers in the room
   208|         Args:
   209|             receipt: receipt to be sent
   210|         """
   211|         room_id = receipt.room_id
   212|         domains_set = await self.state.get_current_hosts_in_room(room_id)
   213|         domains = [
   214|             d
   215|             for d in domains_set
   216|             if d != self.server_name
   217|             and self._federation_shard_config.should_handle(self._instance_name, d)
   218|         ]
   219|         if not domains:
   220|             return
   221|         queues_pending_flush = self._queues_awaiting_rr_flush_by_room.get(room_id)
   222|         if queues_pending_flush is not None:
   223|             logger.debug("Queuing receipt for: %r", domains)

# --- HUNK 5: Lines 356-401 ---
   356|         """Called when we want to retry sending transactions to a remote.
   357|         This is mainly useful if the remote server has been down and we think it
   358|         might have come back.
   359|         """
   360|         if destination == self.server_name:
   361|             logger.warning("Not waking up ourselves")
   362|             return
   363|         if not self._federation_shard_config.should_handle(
   364|             self._instance_name, destination
   365|         ):
   366|             return
   367|         self._get_per_destination_queue(destination).attempt_new_transaction()
   368|     @staticmethod
   369|     def get_current_token() -> int:
   370|         return 0
   371|     @staticmethod
   372|     async def get_replication_rows(
   373|         instance_name: str, from_token: int, to_token: int, target_row_count: int
   374|     ) -> Tuple[List[Tuple[int, Tuple]], int, bool]:
   375|         return [], 0, False
   376|     async def _wake_destinations_needing_catchup(self):
   377|         """
   378|         Wakes up destinations that need catch-up and are not currently being
   379|         backed off from.
   380|         In order to reduce load spikes, adds a delay between each destination.
   381|         """
   382|         last_processed = None  # type: Optional[str]
   383|         while True:
   384|             destinations_to_wake = await self.store.get_catch_up_outstanding_destinations(
   385|                 last_processed
   386|             )
   387|             if not destinations_to_wake:
   388|                 self._catchup_after_startup_timer = None
   389|                 break
   390|             destinations_to_wake = [
   391|                 d
   392|                 for d in destinations_to_wake
   393|                 if self._federation_shard_config.should_handle(self._instance_name, d)
   394|             ]
   395|             for last_processed in destinations_to_wake:
   396|                 logger.info(
   397|                     "Destination %s has outstanding catch-up, waking up.",
   398|                     last_processed,
   399|                 )
   400|                 self.wake_destination(last_processed)
   401|                 await self.clock.sleep(CATCH_UP_STARTUP_INTERVAL_SEC)


# ====================================================================
# FILE: synapse/federation/sender/per_destination_queue.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 1-23 ---
     1| import datetime
     2| import logging
     3| from typing import TYPE_CHECKING, Dict, Hashable, Iterable, List, Optional, Tuple, cast
     4| from prometheus_client import Counter
     5| from synapse.api.errors import (
     6|     FederationDeniedError,
     7|     HttpResponseException,
     8|     RequestSendFailed,
     9| )
    10| from synapse.api.presence import UserPresenceState
    11| from synapse.events import EventBase
    12| from synapse.federation.units import Edu
    13| from synapse.handlers.presence import format_user_presence_state
    14| from synapse.metrics import sent_transactions_counter
    15| from synapse.metrics.background_process_metrics import run_as_background_process
    16| from synapse.types import ReadReceipt
    17| from synapse.util.retryutils import NotRetryingDestination, get_retry_limiter
    18| if TYPE_CHECKING:
    19|     import synapse.server
    20| MAX_EDUS_PER_TRANSACTION = 100
    21| logger = logging.getLogger(__name__)
    22| sent_edus_counter = Counter(
    23|     "synapse_federation_client_sent_edus", "Total number of EDUs successfully sent"

# --- HUNK 2: Lines 41-110 ---
    41|         hs: "synapse.server.HomeServer",
    42|         transaction_manager: "synapse.federation.sender.TransactionManager",
    43|         destination: str,
    44|     ):
    45|         self._server_name = hs.hostname
    46|         self._clock = hs.get_clock()
    47|         self._store = hs.get_datastore()
    48|         self._transaction_manager = transaction_manager
    49|         self._instance_name = hs.get_instance_name()
    50|         self._federation_shard_config = hs.config.worker.federation_shard_config
    51|         self._should_send_on_this_instance = True
    52|         if not self._federation_shard_config.should_handle(
    53|             self._instance_name, destination
    54|         ):
    55|             logger.error(
    56|                 "Create a per destination queue for %s on wrong worker", destination,
    57|             )
    58|             self._should_send_on_this_instance = False
    59|         self._destination = destination
    60|         self.transmission_loop_running = False
    61|         self._catching_up = True  # type: bool
    62|         self._catchup_last_skipped = 0  # type: int
    63|         self._last_successful_stream_ordering = None  # type: Optional[int]
    64|         self._pending_pdus = []  # type: List[EventBase]
    65|         self._pending_edus = []  # type: List[Edu]
    66|         self._pending_edus_keyed = {}  # type: Dict[Tuple[str, Hashable], Edu]
    67|         self._pending_presence = {}  # type: Dict[str, UserPresenceState]
    68|         self._pending_rrs = {}  # type: Dict[str, Dict[str, Dict[str, dict]]]
    69|         self._rrs_pending_flush = False
    70|         self._last_device_stream_id = 0
    71|         self._last_device_list_stream_id = 0
    72|     def __str__(self) -> str:
    73|         return "PerDestinationQueue[%s]" % self._destination
    74|     def pending_pdu_count(self) -> int:
    75|         return len(self._pending_pdus)
    76|     def pending_edu_count(self) -> int:
    77|         return (
    78|             len(self._pending_edus)
    79|             + len(self._pending_presence)
    80|             + len(self._pending_edus_keyed)
    81|         )
    82|     def send_pdu(self, pdu: EventBase) -> None:
    83|         """Add a PDU to the queue, and start the transmission loop if necessary
    84|         Args:
    85|             pdu: pdu to send
    86|         """
    87|         if not self._catching_up or self._last_successful_stream_ordering is None:
    88|             self._pending_pdus.append(pdu)
    89|         else:
    90|             self._catchup_last_skipped = pdu.internal_metadata.stream_ordering
    91|         self.attempt_new_transaction()
    92|     def send_presence(self, states: Iterable[UserPresenceState]) -> None:
    93|         """Add presence updates to the queue. Start the transmission loop if necessary.
    94|         Args:
    95|             states: presence to send
    96|         """
    97|         self._pending_presence.update({state.user_id: state for state in states})
    98|         self.attempt_new_transaction()
    99|     def queue_read_receipt(self, receipt: ReadReceipt) -> None:
   100|         """Add a RR to the list to be sent. Doesn't start the transmission loop yet
   101|         (see flush_read_receipts_for_room)
   102|         Args:
   103|             receipt: receipt to be queued
   104|         """
   105|         self._pending_rrs.setdefault(receipt.room_id, {}).setdefault(
   106|             receipt.receipt_type, {}
   107|         )[receipt.user_id] = {"event_ids": receipt.event_ids, "data": receipt.data}
   108|     def flush_read_receipts_for_room(self, room_id: str) -> None:
   109|         if room_id not in self._pending_rrs:
   110|             return

# --- HUNK 3: Lines 123-166 ---
   123|         transaction in the background.
   124|         """
   125|         if self.transmission_loop_running:
   126|             logger.debug("TX [%s] Transaction already in progress", self._destination)
   127|             return
   128|         if not self._should_send_on_this_instance:
   129|             logger.error(
   130|                 "Trying to start a transaction to %s on wrong worker", self._destination
   131|             )
   132|             return
   133|         logger.debug("TX [%s] Starting transaction loop", self._destination)
   134|         run_as_background_process(
   135|             "federation_transaction_transmission_loop",
   136|             self._transaction_transmission_loop,
   137|         )
   138|     async def _transaction_transmission_loop(self) -> None:
   139|         pending_pdus = []  # type: List[EventBase]
   140|         try:
   141|             self.transmission_loop_running = True
   142|             await get_retry_limiter(self._destination, self._clock, self._store)
   143|             if self._catching_up:
   144|                 await self._catch_up_transmission_loop()
   145|                 if self._catching_up:
   146|                     return
   147|             pending_pdus = []
   148|             while True:
   149|                 limit = MAX_EDUS_PER_TRANSACTION - 2
   150|                 device_update_edus, dev_list_id = await self._get_device_update_edus(
   151|                     limit
   152|                 )
   153|                 limit -= len(device_update_edus)
   154|                 (
   155|                     to_device_edus,
   156|                     device_stream_id,
   157|                 ) = await self._get_to_device_message_edus(limit)
   158|                 pending_edus = device_update_edus + to_device_edus
   159|                 pending_pdus = self._pending_pdus
   160|                 pending_pdus, self._pending_pdus = pending_pdus[:50], pending_pdus[50:]
   161|                 pending_edus.extend(self._get_rr_edus(force_flush=False))
   162|                 pending_presence = self._pending_presence
   163|                 self._pending_presence = {}
   164|                 if pending_presence:
   165|                     pending_edus.append(
   166|                         Edu(

# --- HUNK 4: Lines 202-336 ---
   202|                     self._destination, pending_pdus, pending_edus
   203|                 )
   204|                 if success:
   205|                     sent_transactions_counter.inc()
   206|                     sent_edus_counter.inc(len(pending_edus))
   207|                     for edu in pending_edus:
   208|                         sent_edus_by_type.labels(edu.edu_type).inc()
   209|                     if to_device_edus:
   210|                         await self._store.delete_device_msgs_for_remote(
   211|                             self._destination, device_stream_id
   212|                         )
   213|                     if device_update_edus:
   214|                         logger.info(
   215|                             "Marking as sent %r %r", self._destination, dev_list_id
   216|                         )
   217|                         await self._store.mark_as_sent_devices_by_remote(
   218|                             self._destination, dev_list_id
   219|                         )
   220|                     self._last_device_stream_id = device_stream_id
   221|                     self._last_device_list_stream_id = dev_list_id
   222|                     if pending_pdus:
   223|                         final_pdu = pending_pdus[-1]
   224|                         last_successful_stream_ordering = (
   225|                             final_pdu.internal_metadata.stream_ordering
   226|                         )
   227|                         await self._store.set_destination_last_successful_stream_ordering(
   228|                             self._destination, last_successful_stream_ordering
   229|                         )
   230|                 else:
   231|                     break
   232|         except NotRetryingDestination as e:
   233|             logger.debug(
   234|                 "TX [%s] not ready for retry yet (next retry at %s) - "
   235|                 "dropping transaction for now",
   236|                 self._destination,
   237|                 datetime.datetime.fromtimestamp(
   238|                     (e.retry_last_ts + e.retry_interval) / 1000.0
   239|                 ),
   240|             )
   241|             if e.retry_interval > 60 * 60 * 1000:
   242|                 self._pending_edus = []
   243|                 self._pending_edus_keyed = {}
   244|                 self._pending_presence = {}
   245|                 self._pending_rrs = {}
   246|             self._start_catching_up()
   247|         except FederationDeniedError as e:
   248|             logger.info(e)
   249|         except HttpResponseException as e:
   250|             logger.warning(
   251|                 "TX [%s] Received %d response to transaction: %s",
   252|                 self._destination,
   253|                 e.code,
   254|                 e,
   255|             )
   256|             self._start_catching_up()
   257|         except RequestSendFailed as e:
   258|             logger.warning(
   259|                 "TX [%s] Failed to send transaction: %s", self._destination, e
   260|             )
   261|             for p in pending_pdus:
   262|                 logger.info(
   263|                     "Failed to send event %s to %s", p.event_id, self._destination
   264|                 )
   265|             self._start_catching_up()
   266|         except Exception:
   267|             logger.exception("TX [%s] Failed to send transaction", self._destination)
   268|             for p in pending_pdus:
   269|                 logger.info(
   270|                     "Failed to send event %s to %s", p.event_id, self._destination
   271|                 )
   272|             self._start_catching_up()
   273|         finally:
   274|             self.transmission_loop_running = False
   275|     async def _catch_up_transmission_loop(self) -> None:
   276|         first_catch_up_check = self._last_successful_stream_ordering is None
   277|         if first_catch_up_check:
   278|             self._last_successful_stream_ordering = await self._store.get_destination_last_successful_stream_ordering(
   279|                 self._destination
   280|             )
   281|         if self._last_successful_stream_ordering is None:
   282|             self._catching_up = False
   283|             return
   284|         while True:
   285|             event_ids = await self._store.get_catch_up_room_event_ids(
   286|                 self._destination, self._last_successful_stream_ordering,
   287|             )
   288|             if not event_ids:
   289|                 if self._catchup_last_skipped > self._last_successful_stream_ordering:
   290|                     continue
   291|                 self._catching_up = False
   292|                 break
   293|             if first_catch_up_check:
   294|                 self._start_catching_up()
   295|             catchup_pdus = await self._store.get_events_as_list(event_ids)
   296|             if not catchup_pdus:
   297|                 raise AssertionError(
   298|                     "No events retrieved when we asked for %r. "
   299|                     "This should not happen." % event_ids
   300|                 )
   301|             if logger.isEnabledFor(logging.INFO):
   302|                 rooms = [p.room_id for p in catchup_pdus]
   303|                 logger.info("Catching up rooms to %s: %r", self._destination, rooms)
   304|             success = await self._transaction_manager.send_new_transaction(
   305|                 self._destination, catchup_pdus, []
   306|             )
   307|             if not success:
   308|                 return
   309|             sent_transactions_counter.inc()
   310|             final_pdu = catchup_pdus[-1]
   311|             self._last_successful_stream_ordering = cast(
   312|                 int, final_pdu.internal_metadata.stream_ordering
   313|             )
   314|             await self._store.set_destination_last_successful_stream_ordering(
   315|                 self._destination, self._last_successful_stream_ordering
   316|             )
   317|     def _get_rr_edus(self, force_flush: bool) -> Iterable[Edu]:
   318|         if not self._pending_rrs:
   319|             return
   320|         if not force_flush and not self._rrs_pending_flush:
   321|             return
   322|         edu = Edu(
   323|             origin=self._server_name,
   324|             destination=self._destination,
   325|             edu_type="m.receipt",
   326|             content=self._pending_rrs,
   327|         )
   328|         self._pending_rrs = {}
   329|         self._rrs_pending_flush = False
   330|         yield edu
   331|     def _pop_pending_edus(self, limit: int) -> List[Edu]:
   332|         pending_edus = self._pending_edus
   333|         pending_edus, self._pending_edus = pending_edus[:limit], pending_edus[limit:]
   334|         return pending_edus
   335|     async def _get_device_update_edus(self, limit: int) -> Tuple[List[Edu], int]:
   336|         last_device_list = self._last_device_list_stream_id

# --- HUNK 5: Lines 347-373 ---
   347|             for (edu_type, content) in results
   348|         ]
   349|         assert len(edus) <= limit, "get_device_updates_by_remote returned too many EDUs"
   350|         return (edus, now_stream_id)
   351|     async def _get_to_device_message_edus(self, limit: int) -> Tuple[List[Edu], int]:
   352|         last_device_stream_id = self._last_device_stream_id
   353|         to_device_stream_id = self._store.get_to_device_stream_token()
   354|         contents, stream_id = await self._store.get_new_device_msgs_for_remote(
   355|             self._destination, last_device_stream_id, to_device_stream_id, limit
   356|         )
   357|         edus = [
   358|             Edu(
   359|                 origin=self._server_name,
   360|                 destination=self._destination,
   361|                 edu_type="m.direct_to_device",
   362|                 content=content,
   363|             )
   364|             for content in contents
   365|         ]
   366|         return (edus, stream_id)
   367|     def _start_catching_up(self) -> None:
   368|         """
   369|         Marks this destination as being in catch-up mode.
   370|         This throws away the PDU queue.
   371|         """
   372|         self._catching_up = True
   373|         self._pending_pdus = []


# ====================================================================
# FILE: synapse/federation/sender/transaction_manager.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-57 ---
     1| import logging
     2| from typing import TYPE_CHECKING, List
     3| from prometheus_client import Gauge
     4| from synapse.api.errors import HttpResponseException
     5| from synapse.events import EventBase
     6| from synapse.federation.persistence import TransactionActions
     7| from synapse.federation.units import Edu, Transaction
     8| from synapse.logging.opentracing import (
     9|     extract_text_map,
    10|     set_tag,
    11|     start_active_span_follows_from,
    12|     tags,
    13|     whitelisted_homeserver,
    14| )
    15| from synapse.util import json_decoder
    16| from synapse.util.metrics import measure_func
    17| if TYPE_CHECKING:
    18|     import synapse.server
    19| logger = logging.getLogger(__name__)
    20| last_pdu_age_metric = Gauge(
    21|     "synapse_federation_last_sent_pdu_age",
    22|     "The age (in seconds) of the last PDU successfully sent to the given domain",
    23|     labelnames=("server_name",),
    24| )
    25| class TransactionManager:
    26|     """Helper class which handles building and sending transactions
    27|     shared between PerDestinationQueue objects
    28|     """
    29|     def __init__(self, hs: "synapse.server.HomeServer"):
    30|         self._server_name = hs.hostname
    31|         self.clock = hs.get_clock()  # nb must be called this for @measure_func
    32|         self._store = hs.get_datastore()
    33|         self._transaction_actions = TransactionActions(self._store)
    34|         self._transport_layer = hs.get_federation_transport_client()
    35|         self._federation_metrics_domains = (
    36|             hs.get_config().federation.federation_metrics_domains
    37|         )
    38|         self._next_txn_id = int(self.clock.time_msec())
    39|     @measure_func("_send_new_transaction")
    40|     async def send_new_transaction(
    41|         self, destination: str, pdus: List[EventBase], edus: List[Edu],
    42|     ) -> bool:
    43|         """
    44|         Args:
    45|             destination: The destination to send to (e.g. 'example.org')
    46|             pdus: In-order list of PDUs to send
    47|             edus: List of EDUs to send
    48|         Returns:
    49|             True iff the transaction was successful
    50|         """
    51|         span_contexts = []
    52|         keep_destination = whitelisted_homeserver(destination)
    53|         for edu in edus:
    54|             context = edu.get_context()
    55|             if context:
    56|                 span_contexts.append(extract_text_map(json_decoder.decode(context)))
    57|             if keep_destination:

# --- HUNK 2: Lines 110-137 ---
   110|             logger.info("TX [%s] {%s} got %d response", destination, txn_id, code)
   111|             if code == 200:
   112|                 for e_id, r in response.get("pdus", {}).items():
   113|                     if "error" in r:
   114|                         logger.warning(
   115|                             "TX [%s] {%s} Remote returned error for %s: %s",
   116|                             destination,
   117|                             txn_id,
   118|                             e_id,
   119|                             r,
   120|                         )
   121|             else:
   122|                 for p in pdus:
   123|                     logger.warning(
   124|                         "TX [%s] {%s} Failed to send event %s",
   125|                         destination,
   126|                         txn_id,
   127|                         p.event_id,
   128|                     )
   129|                 success = False
   130|             if success and pdus and destination in self._federation_metrics_domains:
   131|                 last_pdu = pdus[-1]
   132|                 last_pdu_age = self.clock.time_msec() - last_pdu.origin_server_ts
   133|                 last_pdu_age_metric.labels(server_name=destination).set(
   134|                     last_pdu_age / 1000
   135|                 )
   136|             set_tag(tags.ERROR, not success)
   137|             return success


# ====================================================================
# FILE: synapse/federation/transport/server.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 26-66 ---
    26|     whitelisted_homeserver,
    27| )
    28| from synapse.server import HomeServer
    29| from synapse.types import ThirdPartyInstanceID, get_domain_from_id
    30| from synapse.util.versionstring import get_version_string
    31| logger = logging.getLogger(__name__)
    32| class TransportLayerServer(JsonResource):
    33|     """Handles incoming federation HTTP requests"""
    34|     def __init__(self, hs, servlet_groups=None):
    35|         """Initialize the TransportLayerServer
    36|         Will by default register all servlets. For custom behaviour, pass in
    37|         a list of servlet_groups to register.
    38|         Args:
    39|             hs (synapse.server.HomeServer): homeserver
    40|             servlet_groups (list[str], optional): List of servlet groups to register.
    41|                 Defaults to ``DEFAULT_SERVLET_GROUPS``.
    42|         """
    43|         self.hs = hs
    44|         self.clock = hs.get_clock()
    45|         self.servlet_groups = servlet_groups
    46|         super().__init__(hs, canonical_json=False)
    47|         self.authenticator = Authenticator(hs)
    48|         self.ratelimiter = hs.get_federation_ratelimiter()
    49|         self.register_servlets()
    50|     def register_servlets(self):
    51|         register_servlets(
    52|             self.hs,
    53|             resource=self,
    54|             ratelimiter=self.ratelimiter,
    55|             authenticator=self.authenticator,
    56|             servlet_groups=self.servlet_groups,
    57|         )
    58| class AuthenticationError(SynapseError):
    59|     """There was a problem authenticating the request"""
    60|     pass
    61| class NoAuthenticationError(AuthenticationError):
    62|     """The request had no authentication information"""
    63|     pass
    64| class Authenticator:
    65|     def __init__(self, hs: HomeServer):
    66|         self._clock = hs.get_clock()

# --- HUNK 2: Lines 248-288 ---
   248|                         )
   249|                 else:
   250|                     response = await func(
   251|                         origin, content, request.args, *args, **kwargs
   252|                     )
   253|             return response
   254|         return new_func
   255|     def register(self, server):
   256|         pattern = re.compile("^" + self.PREFIX + self.PATH + "$")
   257|         for method in ("GET", "PUT", "POST"):
   258|             code = getattr(self, "on_%s" % (method), None)
   259|             if code is None:
   260|                 continue
   261|             server.register_paths(
   262|                 method, (pattern,), self._wrap(code), self.__class__.__name__,
   263|             )
   264| class FederationSendServlet(BaseFederationServlet):
   265|     PATH = "/send/(?P<transaction_id>[^/]*)/?"
   266|     RATELIMIT = False
   267|     def __init__(self, handler, server_name, **kwargs):
   268|         super().__init__(handler, server_name=server_name, **kwargs)
   269|         self.server_name = server_name
   270|     async def on_PUT(self, origin, content, query, transaction_id):
   271|         """ Called on PUT /send/<transaction_id>/
   272|         Args:
   273|             request (twisted.web.http.Request): The HTTP request.
   274|             transaction_id (str): The transaction_id associated with this
   275|                 request. This is *not* None.
   276|         Returns:
   277|             Tuple of `(code, response)`, where
   278|             `response` is a python dict to be converted into JSON that is
   279|             used as the response body.
   280|         """
   281|         try:
   282|             transaction_data = content
   283|             logger.debug("Decoded %s: %s", transaction_id, str(transaction_data))
   284|             logger.info(
   285|                 "Received txn %s from %s. (PDUs: %d, EDUs: %d)",
   286|                 transaction_id,
   287|                 origin,
   288|                 len(transaction_data.get("pdus", [])),

# --- HUNK 3: Lines 511-551 ---
   511|     Content-Type: application/json
   512|     {
   513|         "chunk": [
   514|             {
   515|                 "aliases": [
   516|                     "#test:localhost"
   517|                 ],
   518|                 "guest_can_join": false,
   519|                 "name": "test room",
   520|                 "num_joined_members": 3,
   521|                 "room_id": "!whkydVegtvatLfXmPN:localhost",
   522|                 "world_readable": false
   523|             }
   524|         ],
   525|         "end": "END",
   526|         "start": "START"
   527|     }
   528|     """
   529|     PATH = "/publicRooms"
   530|     def __init__(self, handler, authenticator, ratelimiter, server_name, allow_access):
   531|         super().__init__(handler, authenticator, ratelimiter, server_name)
   532|         self.allow_access = allow_access
   533|     async def on_GET(self, origin, content, query):
   534|         if not self.allow_access:
   535|             raise FederationDeniedError(origin)
   536|         limit = parse_integer_from_args(query, "limit", 0)
   537|         since_token = parse_string_from_args(query, "since", None)
   538|         include_all_networks = parse_boolean_from_args(
   539|             query, "include_all_networks", False
   540|         )
   541|         third_party_instance_id = parse_string_from_args(
   542|             query, "third_party_instance_id", None
   543|         )
   544|         if include_all_networks:
   545|             network_tuple = None
   546|         elif third_party_instance_id:
   547|             network_tuple = ThirdPartyInstanceID.from_string(third_party_instance_id)
   548|         else:
   549|             network_tuple = ThirdPartyInstanceID(None, None)
   550|         if limit == 0:
   551|             limit = None


# ====================================================================
# FILE: synapse/groups/groups_server.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 211-251 ---
   211|             requester_user_id, group_id
   212|         )
   213|         room_results = await self.store.get_rooms_in_group(
   214|             group_id, include_private=is_user_in_group
   215|         )
   216|         chunk = []
   217|         for room_result in room_results:
   218|             room_id = room_result["room_id"]
   219|             joined_users = await self.store.get_users_in_room(room_id)
   220|             entry = await self.room_list_handler.generate_room_entry(
   221|                 room_id, len(joined_users), with_alias=False, allow_private=True
   222|             )
   223|             if not entry:
   224|                 continue
   225|             entry["is_public"] = bool(room_result["is_public"])
   226|             chunk.append(entry)
   227|         chunk.sort(key=lambda e: -e["num_joined_members"])
   228|         return {"chunk": chunk, "total_room_count_estimate": len(room_results)}
   229| class GroupsServerHandler(GroupsServerWorkerHandler):
   230|     def __init__(self, hs):
   231|         super().__init__(hs)
   232|         hs.get_groups_attestation_renewer()
   233|     async def update_group_summary_room(
   234|         self, group_id, requester_user_id, room_id, category_id, content
   235|     ):
   236|         """Add/update a room to the group summary
   237|         """
   238|         await self.check_group_is_ours(
   239|             group_id, requester_user_id, and_exists=True, and_is_admin=requester_user_id
   240|         )
   241|         RoomID.from_string(room_id)  # Ensure valid room id
   242|         order = content.get("order", None)
   243|         is_public = _parse_visibility_from_contents(content)
   244|         await self.store.add_room_to_summary(
   245|             group_id=group_id,
   246|             room_id=room_id,
   247|             category_id=category_id,
   248|             order=order,
   249|             is_public=is_public,
   250|         )
   251|         return {}


# ====================================================================
# FILE: synapse/handlers/acme_issuing_service.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 30-70 ---
    30|             we will attach a child resource for "acme-challenge".
    31|     Returns:
    32|         AcmeIssuingService
    33|     """
    34|     responder = HTTP01Responder()
    35|     well_known_resource.putChild(b"acme-challenge", responder.resource)
    36|     store = ErsatzStore()
    37|     return AcmeIssuingService(
    38|         cert_store=store,
    39|         client_creator=(
    40|             lambda: Client.from_url(
    41|                 reactor=reactor,
    42|                 url=URL.from_text(acme_url),
    43|                 key=load_or_create_client_key(account_key_file),
    44|                 alg=RS256,
    45|             )
    46|         ),
    47|         clock=reactor,
    48|         responders=[responder],
    49|     )
    50| @attr.s(slots=True)
    51| @implementer(ICertificateStore)
    52| class ErsatzStore:
    53|     """
    54|     A store that only stores in memory.
    55|     """
    56|     certs = attr.ib(default=attr.Factory(dict))
    57|     def store(self, server_name, pem_objects):
    58|         self.certs[server_name] = [o.as_bytes() for o in pem_objects]
    59|         return defer.succeed(None)
    60| def load_or_create_client_key(key_file):
    61|     """Load the ACME account key from a file, creating it if it does not exist.
    62|     Args:
    63|         key_file (str): name of the file to use as the account key
    64|     """
    65|     acme_key_file = FilePath(key_file)
    66|     if acme_key_file.exists():
    67|         logger.info("Loading ACME account key from '%s'", acme_key_file)
    68|         key = serialization.load_pem_private_key(
    69|             acme_key_file.getContent(), password=None, backend=default_backend()
    70|         )


# ====================================================================
# FILE: synapse/handlers/admin.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| import logging
     2| from typing import List
     3| from synapse.api.constants import Membership
     4| from synapse.events import FrozenEvent
     5| from synapse.types import RoomStreamToken, StateMap
     6| from synapse.visibility import filter_events_for_client
     7| from ._base import BaseHandler
     8| logger = logging.getLogger(__name__)
     9| class AdminHandler(BaseHandler):
    10|     def __init__(self, hs):
    11|         super().__init__(hs)
    12|         self.storage = hs.get_storage()
    13|         self.state_store = self.storage.state
    14|     async def get_whois(self, user):
    15|         connections = []
    16|         sessions = await self.store.get_user_ip_and_agents(user)
    17|         for session in sessions:
    18|             connections.append(
    19|                 {
    20|                     "ip": session["ip"],
    21|                     "last_seen": session["last_seen"],
    22|                     "user_agent": session["user_agent"],
    23|                 }
    24|             )
    25|         ret = {
    26|             "user_id": user.to_string(),
    27|             "devices": {"": {"sessions": [{"connections": connections}]}},
    28|         }
    29|         return ret
    30|     async def get_user(self, user):
    31|         """Function to get user details"""

# --- HUNK 2: Lines 60-101 ---
    60|             room_id = room.room_id
    61|             logger.info(
    62|                 "[%s] Handling room %s, %d/%d", user_id, room_id, index + 1, len(rooms)
    63|             )
    64|             forgotten = await self.store.did_forget(user_id, room_id)
    65|             if forgotten:
    66|                 logger.info("[%s] User forgot room %d, ignoring", user_id, room_id)
    67|                 continue
    68|             if room_id not in rooms_user_has_been_in:
    69|                 if room.membership == Membership.INVITE:
    70|                     event_id = room.event_id
    71|                     invite = await self.store.get_event(event_id, allow_none=True)
    72|                     if invite:
    73|                         invited_state = invite.unsigned["invite_room_state"]
    74|                         writer.write_invite(room_id, invite, invited_state)
    75|                 continue
    76|             if room.membership == Membership.JOIN:
    77|                 stream_ordering = self.store.get_room_max_stream_ordering()
    78|             else:
    79|                 stream_ordering = room.stream_ordering
    80|             from_key = RoomStreamToken(0, 0)
    81|             to_key = RoomStreamToken(None, stream_ordering)
    82|             written_events = set()  # Events that we've processed in this room
    83|             event_to_unseen_prevs = {}
    84|             unseen_to_child_events = {}
    85|             while True:
    86|                 events, _ = await self.store.paginate_room_events(
    87|                     room_id, from_key, to_key, limit=100, direction="f"
    88|                 )
    89|                 if not events:
    90|                     break
    91|                 from_key = events[-1].internal_metadata.after
    92|                 events = await filter_events_for_client(self.storage, user_id, events)
    93|                 writer.write_events(room_id, events)
    94|                 for event in events:
    95|                     unseen_events = set(event.prev_event_ids()) - written_events
    96|                     if unseen_events:
    97|                         event_to_unseen_prevs[event.event_id] = unseen_events
    98|                         for unseen in unseen_events:
    99|                             unseen_to_child_events.setdefault(unseen, set()).add(
   100|                                 event.event_id
   101|                             )


# ====================================================================
# FILE: synapse/handlers/auth.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 71-123 ---
    71|     Convert a phone login identifier type to a generic threepid identifier.
    72|     Args:
    73|         identifier: Login identifier dict of type 'm.id.phone'
    74|     Returns:
    75|         An equivalent m.id.thirdparty identifier dict
    76|     """
    77|     if "country" not in identifier or (
    78|         "phone" not in identifier
    79|         and "number" not in identifier
    80|     ):
    81|         raise SynapseError(
    82|             400, "Invalid phone-type identifier", errcode=Codes.INVALID_PARAM
    83|         )
    84|     phone_number = identifier.get("phone", identifier["number"])
    85|     msisdn = phone_number_to_msisdn(identifier["country"], phone_number)
    86|     return {
    87|         "type": "m.id.thirdparty",
    88|         "medium": "msisdn",
    89|         "address": msisdn,
    90|     }
    91| @attr.s(slots=True)
    92| class SsoLoginExtraAttributes:
    93|     """Data we track about SAML2 sessions"""
    94|     creation_time = attr.ib(type=int)
    95|     extra_attributes = attr.ib(type=JsonDict)
    96| class AuthHandler(BaseHandler):
    97|     SESSION_EXPIRE_MS = 48 * 60 * 60 * 1000
    98|     def __init__(self, hs):
    99|         """
   100|         Args:
   101|             hs (synapse.server.HomeServer):
   102|         """
   103|         super().__init__(hs)
   104|         self.checkers = {}  # type: Dict[str, UserInteractiveAuthChecker]
   105|         for auth_checker_class in INTERACTIVE_AUTH_CHECKERS:
   106|             inst = auth_checker_class(hs)
   107|             if inst.is_enabled():
   108|                 self.checkers[inst.AUTH_TYPE] = inst  # type: ignore
   109|         self.bcrypt_rounds = hs.config.bcrypt_rounds
   110|         account_handler = ModuleApi(hs, self)
   111|         self.password_providers = [
   112|             module(config=config, account_handler=account_handler)
   113|             for module, config in hs.config.password_providers
   114|         ]
   115|         logger.info("Extra password_providers: %r", self.password_providers)
   116|         self.hs = hs  # FIXME better possibility to access registrationHandler later?
   117|         self.macaroon_gen = hs.get_macaroon_generator()
   118|         self._password_enabled = hs.config.password_enabled
   119|         self._sso_enabled = (
   120|             hs.config.cas_enabled or hs.config.saml2_enabled or hs.config.oidc_enabled
   121|         )
   122|         login_types = []
   123|         if self._password_enabled:

# --- HUNK 2: Lines 136-176 ---
   136|             clock=self.clock,
   137|             rate_hz=self.hs.config.rc_login_failed_attempts.per_second,
   138|             burst_count=self.hs.config.rc_login_failed_attempts.burst_count,
   139|         )
   140|         self._clock = self.hs.get_clock()
   141|         if hs.config.worker_app is None:
   142|             self._clock.looping_call(
   143|                 run_as_background_process,
   144|                 5 * 60 * 1000,
   145|                 "expire_old_sessions",
   146|                 self._expire_old_sessions,
   147|             )
   148|         self._sso_redirect_confirm_template = hs.config.sso_redirect_confirm_template
   149|         self._sso_auth_confirm_template = hs.config.sso_auth_confirm_template
   150|         self._sso_auth_success_template = hs.config.sso_auth_success_template
   151|         self._sso_account_deactivated_template = (
   152|             hs.config.sso_account_deactivated_template
   153|         )
   154|         self._server_name = hs.config.server_name
   155|         self._whitelisted_sso_clients = tuple(hs.config.sso_client_whitelist)
   156|         self._extra_attributes = {}  # type: Dict[str, SsoLoginExtraAttributes]
   157|     async def validate_user_via_ui_auth(
   158|         self,
   159|         requester: Requester,
   160|         request: SynapseRequest,
   161|         request_body: Dict[str, Any],
   162|         clientip: str,
   163|         description: str,
   164|     ) -> Tuple[dict, str]:
   165|         """
   166|         Checks that the user is who they claim to be, via a UI auth.
   167|         This is used for things like device deletion and password reset where
   168|         the user already has a valid access token, but we want to double-check
   169|         that it isn't stolen by re-authenticating them.
   170|         Args:
   171|             requester: The user, as given by the access token
   172|             request: The request sent by the client.
   173|             request_body: The body of the request sent by the client
   174|             clientip: The IP address of the client.
   175|             description: A human readable string to be displayed to the user that
   176|                          describes the operation happening on their account.

# --- HUNK 3: Lines 804-925 ---
   804|     async def complete_sso_ui_auth(
   805|         self, registered_user_id: str, session_id: str, request: SynapseRequest,
   806|     ):
   807|         """Having figured out a mxid for this user, complete the HTTP request
   808|         Args:
   809|             registered_user_id: The registered user ID to complete SSO login for.
   810|             request: The request to complete.
   811|             client_redirect_url: The URL to which to redirect the user at the end of the
   812|                 process.
   813|         """
   814|         await self.store.mark_ui_auth_stage_complete(
   815|             session_id, LoginType.SSO, registered_user_id
   816|         )
   817|         html = self._sso_auth_success_template
   818|         respond_with_html(request, 200, html)
   819|     async def complete_sso_login(
   820|         self,
   821|         registered_user_id: str,
   822|         request: SynapseRequest,
   823|         client_redirect_url: str,
   824|         extra_attributes: Optional[JsonDict] = None,
   825|     ):
   826|         """Having figured out a mxid for this user, complete the HTTP request
   827|         Args:
   828|             registered_user_id: The registered user ID to complete SSO login for.
   829|             request: The request to complete.
   830|             client_redirect_url: The URL to which to redirect the user at the end of the
   831|                 process.
   832|             extra_attributes: Extra attributes which will be passed to the client
   833|                 during successful login. Must be JSON serializable.
   834|         """
   835|         deactivated = await self.store.get_user_deactivated_status(registered_user_id)
   836|         if deactivated:
   837|             respond_with_html(request, 403, self._sso_account_deactivated_template)
   838|             return
   839|         self._complete_sso_login(
   840|             registered_user_id, request, client_redirect_url, extra_attributes
   841|         )
   842|     def _complete_sso_login(
   843|         self,
   844|         registered_user_id: str,
   845|         request: SynapseRequest,
   846|         client_redirect_url: str,
   847|         extra_attributes: Optional[JsonDict] = None,
   848|     ):
   849|         """
   850|         The synchronous portion of complete_sso_login.
   851|         This exists purely for backwards compatibility of synapse.module_api.ModuleApi.
   852|         """
   853|         if extra_attributes:
   854|             self._extra_attributes[registered_user_id] = SsoLoginExtraAttributes(
   855|                 self._clock.time_msec(), extra_attributes,
   856|             )
   857|         login_token = self.macaroon_gen.generate_short_term_login_token(
   858|             registered_user_id
   859|         )
   860|         redirect_url = self.add_query_param_to_url(
   861|             client_redirect_url, "loginToken", login_token
   862|         )
   863|         if client_redirect_url.startswith(self._whitelisted_sso_clients):
   864|             request.redirect(redirect_url)
   865|             finish_request(request)
   866|             return
   867|         redirect_url_no_params = client_redirect_url.split("?")[0]
   868|         html = self._sso_redirect_confirm_template.render(
   869|             display_url=redirect_url_no_params,
   870|             redirect_url=redirect_url,
   871|             server_name=self._server_name,
   872|         )
   873|         respond_with_html(request, 200, html)
   874|     async def _sso_login_callback(self, login_result: JsonDict) -> None:
   875|         """
   876|         A login callback which might add additional attributes to the login response.
   877|         Args:
   878|             login_result: The data to be sent to the client. Includes the user
   879|                 ID and access token.
   880|         """
   881|         self._expire_sso_extra_attributes()
   882|         extra_attributes = self._extra_attributes.get(login_result["user_id"])
   883|         if extra_attributes:
   884|             login_result.update(extra_attributes.extra_attributes)
   885|     def _expire_sso_extra_attributes(self) -> None:
   886|         """
   887|         Iterate through the mapping of user IDs to extra attributes and remove any that are no longer valid.
   888|         """
   889|         LOGIN_TOKEN_EXPIRATION_TIME = 2 * 60 * 1000
   890|         expire_before = self._clock.time_msec() - LOGIN_TOKEN_EXPIRATION_TIME
   891|         to_expire = set()
   892|         for user_id, data in self._extra_attributes.items():
   893|             if data.creation_time < expire_before:
   894|                 to_expire.add(user_id)
   895|         for user_id in to_expire:
   896|             logger.debug("Expiring extra attributes for user %s", user_id)
   897|             del self._extra_attributes[user_id]
   898|     @staticmethod
   899|     def add_query_param_to_url(url: str, param_name: str, param: Any):
   900|         url_parts = list(urllib.parse.urlparse(url))
   901|         query = dict(urllib.parse.parse_qsl(url_parts[4]))
   902|         query.update({param_name: param})
   903|         url_parts[4] = urllib.parse.urlencode(query)
   904|         return urllib.parse.urlunparse(url_parts)
   905| @attr.s(slots=True)
   906| class MacaroonGenerator:
   907|     hs = attr.ib()
   908|     def generate_access_token(
   909|         self, user_id: str, extra_caveats: Optional[List[str]] = None
   910|     ) -> str:
   911|         extra_caveats = extra_caveats or []
   912|         macaroon = self._generate_base_macaroon(user_id)
   913|         macaroon.add_first_party_caveat("type = access")
   914|         macaroon.add_first_party_caveat(
   915|             "nonce = %s" % (stringutils.random_string_with_symbols(16),)
   916|         )
   917|         for caveat in extra_caveats:
   918|             macaroon.add_first_party_caveat(caveat)
   919|         return macaroon.serialize()
   920|     def generate_short_term_login_token(
   921|         self, user_id: str, duration_in_ms: int = (2 * 60 * 1000)
   922|     ) -> str:
   923|         macaroon = self._generate_base_macaroon(user_id)
   924|         macaroon.add_first_party_caveat("type = login")
   925|         now = self.hs.get_clock().time_msec()


# ====================================================================
# FILE: synapse/handlers/deactivate_account.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| import logging
     2| from typing import Optional
     3| from synapse.api.errors import SynapseError
     4| from synapse.metrics.background_process_metrics import run_as_background_process
     5| from synapse.types import UserID, create_requester
     6| from ._base import BaseHandler
     7| logger = logging.getLogger(__name__)
     8| class DeactivateAccountHandler(BaseHandler):
     9|     """Handler which deals with deactivating user accounts."""
    10|     def __init__(self, hs):
    11|         super().__init__(hs)
    12|         self.hs = hs
    13|         self._auth_handler = hs.get_auth_handler()
    14|         self._device_handler = hs.get_device_handler()
    15|         self._room_member_handler = hs.get_room_member_handler()
    16|         self._identity_handler = hs.get_handlers().identity_handler
    17|         self.user_directory_handler = hs.get_user_directory_handler()
    18|         self._user_parter_running = False
    19|         if hs.config.worker_app is None:
    20|             hs.get_reactor().callWhenRunning(self._start_user_parting)
    21|         self._account_validity_enabled = hs.config.account_validity.enabled
    22|     async def deactivate_account(
    23|         self, user_id: str, erase_data: bool, id_server: Optional[str] = None
    24|     ) -> bool:
    25|         """Deactivate a user's account
    26|         Args:
    27|             user_id: ID of user to be deactivated
    28|             erase_data: whether to GDPR-erase the user's data
    29|             id_server: Use the given identity server when unbinding
    30|                 any threepids. If None then will attempt to unbind using the
    31|                 identity server specified when binding (if known).


# ====================================================================
# FILE: synapse/handlers/device.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-49 ---
     1| import logging
     2| from typing import Any, Dict, List, Optional
     3| from synapse.api import errors
     4| from synapse.api.constants import EventTypes
     5| from synapse.api.errors import (
     6|     Codes,
     7|     FederationDeniedError,
     8|     HttpResponseException,
     9|     RequestSendFailed,
    10|     SynapseError,
    11| )
    12| from synapse.logging.opentracing import log_kv, set_tag, trace
    13| from synapse.metrics.background_process_metrics import run_as_background_process
    14| from synapse.types import (
    15|     StreamToken,
    16|     get_domain_from_id,
    17|     get_verify_key_from_cross_signing_key,
    18| )
    19| from synapse.util import stringutils
    20| from synapse.util.async_helpers import Linearizer
    21| from synapse.util.caches.expiringcache import ExpiringCache
    22| from synapse.util.metrics import measure_func
    23| from synapse.util.retryutils import NotRetryingDestination
    24| from ._base import BaseHandler
    25| logger = logging.getLogger(__name__)
    26| MAX_DEVICE_DISPLAY_NAME_LEN = 100
    27| class DeviceWorkerHandler(BaseHandler):
    28|     def __init__(self, hs):
    29|         super().__init__(hs)
    30|         self.hs = hs
    31|         self.state = hs.get_state_handler()
    32|         self.state_store = hs.get_storage().state
    33|         self._auth_handler = hs.get_auth_handler()
    34|     @trace
    35|     async def get_devices_by_user(self, user_id: str) -> List[Dict[str, Any]]:
    36|         """
    37|         Retrieve the given user's devices
    38|         Args:
    39|             user_id: The user ID to query for devices.
    40|         Returns:
    41|             info on each device
    42|         """
    43|         set_tag("user_id", user_id)
    44|         device_map = await self.store.get_devices_by_user(user_id)
    45|         ips = await self.store.get_last_client_ip_by_device(user_id, device_id=None)
    46|         devices = list(device_map.values())
    47|         for device in devices:
    48|             _update_device_from_client_ips(device, ips)
    49|         log_kv(device_map)

# --- HUNK 2: Lines 53-114 ---
    53|         """ Retrieve the given device
    54|         Args:
    55|             user_id: The user to get the device from
    56|             device_id: The device to fetch.
    57|         Returns:
    58|             info on the device
    59|         Raises:
    60|             errors.NotFoundError: if the device was not found
    61|         """
    62|         try:
    63|             device = await self.store.get_device(user_id, device_id)
    64|         except errors.StoreError:
    65|             raise errors.NotFoundError
    66|         ips = await self.store.get_last_client_ip_by_device(user_id, device_id)
    67|         _update_device_from_client_ips(device, ips)
    68|         set_tag("device", device)
    69|         set_tag("ips", ips)
    70|         return device
    71|     @trace
    72|     @measure_func("device.get_user_ids_changed")
    73|     async def get_user_ids_changed(self, user_id: str, from_token: StreamToken):
    74|         """Get list of users that have had the devices updated, or have newly
    75|         joined a room, that `user_id` may be interested in.
    76|         """
    77|         set_tag("user_id", user_id)
    78|         set_tag("from_token", from_token)
    79|         now_room_key = self.store.get_room_max_token()
    80|         room_ids = await self.store.get_rooms_for_user(user_id)
    81|         users_who_share_room = await self.store.get_users_who_share_room_with_user(
    82|             user_id
    83|         )
    84|         tracked_users = set(users_who_share_room)
    85|         tracked_users.add(user_id)
    86|         changed = await self.store.get_users_whose_devices_changed(
    87|             from_token.device_list_key, tracked_users
    88|         )
    89|         rooms_changed = self.store.get_rooms_that_changed(room_ids, from_token.room_key)
    90|         member_events = await self.store.get_membership_changes_for_user(
    91|             user_id, from_token.room_key, now_room_key
    92|         )
    93|         rooms_changed.update(event.room_id for event in member_events)
    94|         stream_ordering = from_token.room_key.stream
    95|         possibly_changed = set(changed)
    96|         possibly_left = set()
    97|         for room_id in rooms_changed:
    98|             current_state_ids = await self.store.get_current_state_ids(room_id)
    99|             if room_id not in room_ids:
   100|                 for key, event_id in current_state_ids.items():
   101|                     etype, state_key = key
   102|                     if etype != EventTypes.Member:
   103|                         continue
   104|                     possibly_left.add(state_key)
   105|                 continue
   106|             try:
   107|                 event_ids = await self.store.get_forward_extremeties_for_room(
   108|                     room_id, stream_ordering=stream_ordering
   109|                 )
   110|             except errors.StoreError:
   111|                 event_ids = []
   112|             if not event_ids:
   113|                 log_kv(
   114|                     {"event": "encountered empty previous state", "room_id": room_id}

# --- HUNK 3: Lines 151-229 ---
   151|         result = {"changed": list(possibly_joined), "left": list(possibly_left)}
   152|         log_kv(result)
   153|         return result
   154|     async def on_federation_query_user_devices(self, user_id):
   155|         stream_id, devices = await self.store.get_e2e_device_keys_for_federation_query(
   156|             user_id
   157|         )
   158|         master_key = await self.store.get_e2e_cross_signing_key(user_id, "master")
   159|         self_signing_key = await self.store.get_e2e_cross_signing_key(
   160|             user_id, "self_signing"
   161|         )
   162|         return {
   163|             "user_id": user_id,
   164|             "stream_id": stream_id,
   165|             "devices": devices,
   166|             "master_key": master_key,
   167|             "self_signing_key": self_signing_key,
   168|         }
   169| class DeviceHandler(DeviceWorkerHandler):
   170|     def __init__(self, hs):
   171|         super().__init__(hs)
   172|         self.federation_sender = hs.get_federation_sender()
   173|         self.device_list_updater = DeviceListUpdater(hs, self)
   174|         federation_registry = hs.get_federation_registry()
   175|         federation_registry.register_edu_handler(
   176|             "m.device_list_update", self.device_list_updater.incoming_device_list_update
   177|         )
   178|         hs.get_distributor().observe("user_left_room", self.user_left_room)
   179|     def _check_device_name_length(self, name: str):
   180|         """
   181|         Checks whether a device name is longer than the maximum allowed length.
   182|         Args:
   183|             name: The name of the device.
   184|         Raises:
   185|             SynapseError: if the device name is too long.
   186|         """
   187|         if name and len(name) > MAX_DEVICE_DISPLAY_NAME_LEN:
   188|             raise SynapseError(
   189|                 400,
   190|                 "Device display name is too long (max %i)"
   191|                 % (MAX_DEVICE_DISPLAY_NAME_LEN,),
   192|                 errcode=Codes.TOO_LARGE,
   193|             )
   194|     async def check_device_registered(
   195|         self, user_id, device_id, initial_device_display_name=None
   196|     ):
   197|         """
   198|         If the given device has not been registered, register it with the
   199|         supplied display name.
   200|         If no device_id is supplied, we make one up.
   201|         Args:
   202|             user_id (str):  @user:id
   203|             device_id (str | None): device id supplied by client
   204|             initial_device_display_name (str | None): device display name from
   205|                  client
   206|         Returns:
   207|             str: device id (generated if none was supplied)
   208|         """
   209|         self._check_device_name_length(initial_device_display_name)
   210|         if device_id is not None:
   211|             new_device = await self.store.store_device(
   212|                 user_id=user_id,
   213|                 device_id=device_id,
   214|                 initial_device_display_name=initial_device_display_name,
   215|             )
   216|             if new_device:
   217|                 await self.notify_device_update(user_id, [device_id])
   218|             return device_id
   219|         attempts = 0
   220|         while attempts < 5:
   221|             device_id = stringutils.random_string(10).upper()
   222|             new_device = await self.store.store_device(
   223|                 user_id=user_id,
   224|                 device_id=device_id,
   225|                 initial_device_display_name=initial_device_display_name,
   226|             )
   227|             if new_device:
   228|                 await self.notify_device_update(user_id, [device_id])
   229|                 return device_id

# --- HUNK 4: Lines 280-320 ---
   280|                 set_tag("reason", "User doesn't have that device id.")
   281|                 pass
   282|             else:
   283|                 raise
   284|         for device_id in device_ids:
   285|             await self._auth_handler.delete_access_tokens_for_user(
   286|                 user_id, device_id=device_id
   287|             )
   288|             await self.store.delete_e2e_keys_by_device(
   289|                 user_id=user_id, device_id=device_id
   290|             )
   291|         await self.notify_device_update(user_id, device_ids)
   292|     async def update_device(self, user_id: str, device_id: str, content: dict) -> None:
   293|         """ Update the given device
   294|         Args:
   295|             user_id: The user to update devices of.
   296|             device_id: The device to update.
   297|             content: body of update request
   298|         """
   299|         new_display_name = content.get("display_name")
   300|         self._check_device_name_length(new_display_name)
   301|         try:
   302|             await self.store.update_device(
   303|                 user_id, device_id, new_display_name=new_display_name
   304|             )
   305|             await self.notify_device_update(user_id, [device_id])
   306|         except errors.StoreError as e:
   307|             if e.code == 404:
   308|                 raise errors.NotFoundError()
   309|             else:
   310|                 raise
   311|     @trace
   312|     @measure_func("notify_device_update")
   313|     async def notify_device_update(self, user_id, device_ids):
   314|         """Notify that a user's device(s) has changed. Pokes the notifier, and
   315|         remote servers if the user is local.
   316|         """
   317|         if not device_ids:
   318|             return
   319|         users_who_share_room = await self.store.get_users_who_share_room_with_user(
   320|             user_id


# ====================================================================
# FILE: synapse/handlers/directory.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-40 ---
     1| import logging
     2| import string
     3| from typing import Iterable, List, Optional
     4| from synapse.api.constants import MAX_ALIAS_LENGTH, EventTypes
     5| from synapse.api.errors import (
     6|     AuthError,
     7|     CodeMessageException,
     8|     Codes,
     9|     NotFoundError,
    10|     ShadowBanError,
    11|     StoreError,
    12|     SynapseError,
    13| )
    14| from synapse.appservice import ApplicationService
    15| from synapse.types import Requester, RoomAlias, UserID, get_domain_from_id
    16| from ._base import BaseHandler
    17| logger = logging.getLogger(__name__)
    18| class DirectoryHandler(BaseHandler):
    19|     def __init__(self, hs):
    20|         super().__init__(hs)
    21|         self.state = hs.get_state_handler()
    22|         self.appservice_handler = hs.get_application_service_handler()
    23|         self.event_creation_handler = hs.get_event_creation_handler()
    24|         self.store = hs.get_datastore()
    25|         self.config = hs.config
    26|         self.enable_room_list_search = hs.config.enable_room_list_search
    27|         self.require_membership = hs.config.require_membership_for_aliases
    28|         self.federation = hs.get_federation_client()
    29|         hs.get_federation_registry().register_query_handler(
    30|             "directory", self.on_directory_query
    31|         )
    32|         self.spam_checker = hs.get_spam_checker()
    33|     async def _create_association(
    34|         self,
    35|         room_alias: RoomAlias,
    36|         room_id: str,
    37|         servers: Optional[Iterable[str]] = None,
    38|         creator: Optional[str] = None,
    39|     ):
    40|         for wchar in string.whitespace:


# ====================================================================
# FILE: synapse/handlers/e2e_keys.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 857-897 ---
   857|         verify_signed_json(signed_device, user_id, verify_key)
   858|     except SignatureVerifyException:
   859|         logger.debug("invalid signature on key")
   860|         raise SynapseError(400, "Invalid signature", Codes.INVALID_SIGNATURE)
   861| def _exception_to_failure(e):
   862|     if isinstance(e, SynapseError):
   863|         return {"status": e.code, "errcode": e.errcode, "message": str(e)}
   864|     if isinstance(e, CodeMessageException):
   865|         return {"status": e.code, "message": str(e)}
   866|     if isinstance(e, NotRetryingDestination):
   867|         return {"status": 503, "message": "Not ready for retry"}
   868|     return {"status": 503, "message": str(e)}
   869| def _one_time_keys_match(old_key_json, new_key):
   870|     old_key = json_decoder.decode(old_key_json)
   871|     if not isinstance(old_key, dict) or not isinstance(new_key, dict):
   872|         return old_key == new_key
   873|     old_key.pop("signatures", None)
   874|     new_key_copy = dict(new_key)
   875|     new_key_copy.pop("signatures", None)
   876|     return old_key == new_key_copy
   877| @attr.s(slots=True)
   878| class SignatureListItem:
   879|     """An item in the signature list as used by upload_signatures_for_device_keys.
   880|     """
   881|     signing_key_id = attr.ib()
   882|     target_user_id = attr.ib()
   883|     target_device_id = attr.ib()
   884|     signature = attr.ib()
   885| class SigningKeyEduUpdater:
   886|     """Handles incoming signing key updates from federation and updates the DB"""
   887|     def __init__(self, hs, e2e_keys_handler):
   888|         self.store = hs.get_datastore()
   889|         self.federation = hs.get_federation_client()
   890|         self.clock = hs.get_clock()
   891|         self.e2e_keys_handler = e2e_keys_handler
   892|         self._remote_edu_linearizer = Linearizer(name="remote_signing_key")
   893|         self._pending_updates = {}
   894|         self._seen_updates = ExpiringCache(
   895|             cache_name="signing_key_update_edu",
   896|             clock=self.clock,
   897|             max_len=10000,


# ====================================================================
# FILE: synapse/handlers/events.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-38 ---
     1| import logging
     2| import random
     3| from typing import TYPE_CHECKING, Iterable, List, Optional
     4| from synapse.api.constants import EventTypes, Membership
     5| from synapse.api.errors import AuthError, SynapseError
     6| from synapse.events import EventBase
     7| from synapse.handlers.presence import format_user_presence_state
     8| from synapse.logging.utils import log_function
     9| from synapse.streams.config import PaginationConfig
    10| from synapse.types import JsonDict, UserID
    11| from synapse.visibility import filter_events_for_client
    12| from ._base import BaseHandler
    13| if TYPE_CHECKING:
    14|     from synapse.server import HomeServer
    15| logger = logging.getLogger(__name__)
    16| class EventStreamHandler(BaseHandler):
    17|     def __init__(self, hs: "HomeServer"):
    18|         super().__init__(hs)
    19|         self.clock = hs.get_clock()
    20|         self.notifier = hs.get_notifier()
    21|         self.state = hs.get_state_handler()
    22|         self._server_notices_sender = hs.get_server_notices_sender()
    23|         self._event_serializer = hs.get_event_client_serializer()
    24|     @log_function
    25|     async def get_stream(
    26|         self,
    27|         auth_user_id: str,
    28|         pagin_config: PaginationConfig,
    29|         timeout: int = 0,
    30|         as_client_event: bool = True,
    31|         affect_presence: bool = True,
    32|         room_id: Optional[str] = None,
    33|         is_guest: bool = False,
    34|     ) -> JsonDict:
    35|         """Fetches the events stream for a given user.
    36|         """
    37|         if room_id:
    38|             blocked = await self.store.is_room_blocked(room_id)

# --- HUNK 2: Lines 69-115 ---
    69|                         )  # type: Iterable[str]
    70|                     else:
    71|                         users = [event.state_key]
    72|                     states = await presence_handler.get_states(users)
    73|                     to_add.extend(
    74|                         {
    75|                             "type": EventTypes.Presence,
    76|                             "content": format_user_presence_state(state, time_now),
    77|                         }
    78|                         for state in states
    79|                     )
    80|             events.extend(to_add)
    81|             chunks = await self._event_serializer.serialize_events(
    82|                 events,
    83|                 time_now,
    84|                 as_client_event=as_client_event,
    85|                 bundle_aggregations=False,
    86|             )
    87|             chunk = {
    88|                 "chunk": chunks,
    89|                 "start": await tokens[0].to_string(self.store),
    90|                 "end": await tokens[1].to_string(self.store),
    91|             }
    92|             return chunk
    93| class EventHandler(BaseHandler):
    94|     def __init__(self, hs: "HomeServer"):
    95|         super().__init__(hs)
    96|         self.storage = hs.get_storage()
    97|     async def get_event(
    98|         self, user: UserID, room_id: Optional[str], event_id: str
    99|     ) -> Optional[EventBase]:
   100|         """Retrieve a single specified event.
   101|         Args:
   102|             user: The user requesting the event
   103|             room_id: The expected room id. We'll return None if the
   104|                 event's room does not match.
   105|             event_id: The event ID to obtain.
   106|         Returns:
   107|             An event, or None if there is no event matching this ID.
   108|         Raises:
   109|             SynapseError if there was a problem retrieving this event, or
   110|             AuthError if the user does not have the rights to inspect this
   111|             event.
   112|         """
   113|         event = await self.store.get_event(event_id, check_room_id=room_id)
   114|         if not event:
   115|             return None


# ====================================================================
# FILE: synapse/handlers/federation.py
# Total hunks: 16
# ====================================================================
# --- HUNK 1: Lines 1-26 ---
     1| """Contains handlers for federation events."""
     2| import itertools
     3| import logging
     4| from collections.abc import Container
     5| from http import HTTPStatus
     6| from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union
     7| import attr
     8| from signedjson.key import decode_verify_key_bytes
     9| from signedjson.sign import verify_signed_json
    10| from unpaddedbase64 import decode_base64
    11| from twisted.internet import defer
    12| from synapse import event_auth
    13| from synapse.api.constants import (
    14|     EventTypes,
    15|     Membership,
    16|     RejectedReason,
    17|     RoomEncryptionAlgorithms,
    18| )
    19| from synapse.api.errors import (
    20|     AuthError,
    21|     CodeMessageException,
    22|     Codes,
    23|     FederationDeniedError,
    24|     FederationError,
    25|     HttpResponseException,
    26|     NotFoundError,

# --- HUNK 2: Lines 31-145 ---
    31| from synapse.crypto.event_signing import compute_event_signature
    32| from synapse.event_auth import auth_types_for_event
    33| from synapse.events import EventBase
    34| from synapse.events.snapshot import EventContext
    35| from synapse.events.validator import EventValidator
    36| from synapse.handlers._base import BaseHandler
    37| from synapse.logging.context import (
    38|     make_deferred_yieldable,
    39|     nested_logging_context,
    40|     preserve_fn,
    41|     run_in_background,
    42| )
    43| from synapse.logging.utils import log_function
    44| from synapse.metrics.background_process_metrics import run_as_background_process
    45| from synapse.replication.http.devices import ReplicationUserDevicesResyncRestServlet
    46| from synapse.replication.http.federation import (
    47|     ReplicationCleanRoomRestServlet,
    48|     ReplicationFederationSendEventsRestServlet,
    49|     ReplicationStoreRoomOnInviteRestServlet,
    50| )
    51| from synapse.state import StateResolutionStore
    52| from synapse.storage.databases.main.events_worker import EventRedactBehaviour
    53| from synapse.types import (
    54|     JsonDict,
    55|     MutableStateMap,
    56|     PersistedEventPosition,
    57|     RoomStreamToken,
    58|     StateMap,
    59|     UserID,
    60|     get_domain_from_id,
    61| )
    62| from synapse.util.async_helpers import Linearizer, concurrently_execute
    63| from synapse.util.retryutils import NotRetryingDestination
    64| from synapse.util.stringutils import shortstr
    65| from synapse.visibility import filter_events_for_server
    66| if TYPE_CHECKING:
    67|     from synapse.server import HomeServer
    68| logger = logging.getLogger(__name__)
    69| @attr.s(slots=True)
    70| class _NewEventInfo:
    71|     """Holds information about a received event, ready for passing to _handle_new_events
    72|     Attributes:
    73|         event: the received event
    74|         state: the state at that event
    75|         auth_events: the auth_event map for that event
    76|     """
    77|     event = attr.ib(type=EventBase)
    78|     state = attr.ib(type=Optional[Sequence[EventBase]], default=None)
    79|     auth_events = attr.ib(type=Optional[MutableStateMap[EventBase]], default=None)
    80| class FederationHandler(BaseHandler):
    81|     """Handles events that originated from federation.
    82|         Responsible for:
    83|         a) handling received Pdus before handing them on as Events to the rest
    84|         of the homeserver (including auth and state conflict resoultion)
    85|         b) converting events that were produced by local clients that may need
    86|         to be sent to remote homeservers.
    87|         c) doing the necessary dances to invite remote users and join remote
    88|         rooms.
    89|     """
    90|     def __init__(self, hs: "HomeServer"):
    91|         super().__init__(hs)
    92|         self.hs = hs
    93|         self.store = hs.get_datastore()
    94|         self.storage = hs.get_storage()
    95|         self.state_store = self.storage.state
    96|         self.federation_client = hs.get_federation_client()
    97|         self.state_handler = hs.get_state_handler()
    98|         self._state_resolution_handler = hs.get_state_resolution_handler()
    99|         self.server_name = hs.hostname
   100|         self.keyring = hs.get_keyring()
   101|         self.action_generator = hs.get_action_generator()
   102|         self.is_mine_id = hs.is_mine_id
   103|         self.spam_checker = hs.get_spam_checker()
   104|         self.event_creation_handler = hs.get_event_creation_handler()
   105|         self._message_handler = hs.get_message_handler()
   106|         self._server_notices_mxid = hs.config.server_notices_mxid
   107|         self.config = hs.config
   108|         self.http_client = hs.get_simple_http_client()
   109|         self._instance_name = hs.get_instance_name()
   110|         self._replication = hs.get_replication_data_handler()
   111|         self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)
   112|         self._clean_room_for_join_client = ReplicationCleanRoomRestServlet.make_client(
   113|             hs
   114|         )
   115|         if hs.config.worker_app:
   116|             self._user_device_resync = ReplicationUserDevicesResyncRestServlet.make_client(
   117|                 hs
   118|             )
   119|             self._maybe_store_room_on_invite = ReplicationStoreRoomOnInviteRestServlet.make_client(
   120|                 hs
   121|             )
   122|         else:
   123|             self._device_list_updater = hs.get_device_handler().device_list_updater
   124|             self._maybe_store_room_on_invite = self.store.maybe_store_room_on_invite
   125|         self.room_queues = {}  # type: Dict[str, List[Tuple[EventBase, str]]]
   126|         self._room_pdu_linearizer = Linearizer("fed_room_pdu")
   127|         self.third_party_event_rules = hs.get_third_party_event_rules()
   128|         self._ephemeral_messages_enabled = hs.config.enable_ephemeral_messages
   129|     async def on_receive_pdu(self, origin, pdu, sent_to_us_directly=False) -> None:
   130|         """ Process a PDU received via a federation /send/ transaction, or
   131|         via backfill of missing prev_events
   132|         Args:
   133|             origin (str): server which initiated the /send/ transaction. Will
   134|                 be used to fetch missing events or state.
   135|             pdu (FrozenEvent): received PDU
   136|             sent_to_us_directly (bool): True if this event was pushed to us; False if
   137|                 we pulled it as the result of a missing prev_event.
   138|         """
   139|         room_id = pdu.room_id
   140|         event_id = pdu.event_id
   141|         logger.info("handling received PDU: %s", pdu)
   142|         existing = await self.store.get_event(
   143|             event_id, allow_none=True, allow_rejected=True
   144|         )
   145|         already_seen = existing and (

# --- HUNK 3: Lines 190-230 ---
   190|                         room_id,
   191|                         event_id,
   192|                         len(missing_prevs),
   193|                         shortstr(missing_prevs),
   194|                     )
   195|                     with (await self._room_pdu_linearizer.queue(pdu.room_id)):
   196|                         logger.info(
   197|                             "[%s %s] Acquired room lock to fetch %d missing prev_events",
   198|                             room_id,
   199|                             event_id,
   200|                             len(missing_prevs),
   201|                         )
   202|                         try:
   203|                             await self._get_missing_events_for_pdu(
   204|                                 origin, pdu, prevs, min_depth
   205|                             )
   206|                         except Exception as e:
   207|                             raise Exception(
   208|                                 "Error fetching missing prev_events for %s: %s"
   209|                                 % (event_id, e)
   210|                             ) from e
   211|                         seen = await self.store.have_events_in_timeline(prevs)
   212|                         if not prevs - seen:
   213|                             logger.info(
   214|                                 "[%s %s] Found all missing prev_events",
   215|                                 room_id,
   216|                                 event_id,
   217|                             )
   218|             if prevs - seen:
   219|                 if sent_to_us_directly:
   220|                     logger.warning(
   221|                         "[%s %s] Rejecting: failed to fetch %d prev events: %s",
   222|                         room_id,
   223|                         event_id,
   224|                         len(prevs - seen),
   225|                         shortstr(prevs - seen),
   226|                     )
   227|                     raise FederationError(
   228|                         "ERROR",
   229|                         403,
   230|                         (

# --- HUNK 4: Lines 241-281 ---
   241|                 event_map = {event_id: pdu}
   242|                 try:
   243|                     ours = await self.state_store.get_state_groups_ids(room_id, seen)
   244|                     state_maps = list(ours.values())  # type: List[StateMap[str]]
   245|                     del ours
   246|                     for p in prevs - seen:
   247|                         logger.info(
   248|                             "Requesting state at missing prev_event %s", event_id,
   249|                         )
   250|                         with nested_logging_context(p):
   251|                             (remote_state, _,) = await self._get_state_for_room(
   252|                                 origin, room_id, p, include_event_in_state=True
   253|                             )
   254|                             remote_state_map = {
   255|                                 (x.type, x.state_key): x.event_id for x in remote_state
   256|                             }
   257|                             state_maps.append(remote_state_map)
   258|                             for x in remote_state:
   259|                                 event_map[x.event_id] = x
   260|                     room_version = await self.store.get_room_version_id(room_id)
   261|                     state_map = await self._state_resolution_handler.resolve_events_with_store(
   262|                         room_id,
   263|                         room_version,
   264|                         state_maps,
   265|                         event_map,
   266|                         state_res_store=StateResolutionStore(self.store),
   267|                     )
   268|                     evs = await self.store.get_events(
   269|                         list(state_map.values()),
   270|                         get_prev_content=False,
   271|                         redact_behaviour=EventRedactBehaviour.AS_IS,
   272|                     )
   273|                     event_map.update(evs)
   274|                     state = [event_map[e] for e in state_map.values()]
   275|                 except Exception:
   276|                     logger.warning(
   277|                         "[%s %s] Error attempting to resolve state at missing "
   278|                         "prev_events",
   279|                         room_id,
   280|                         event_id,
   281|                         exc_info=True,

# --- HUNK 5: Lines 446-488 ---
   446|                 room_id,
   447|             )
   448|             del fetched_events[bad_event_id]
   449|         return fetched_events
   450|     async def _process_received_pdu(
   451|         self, origin: str, event: EventBase, state: Optional[Iterable[EventBase]],
   452|     ):
   453|         """ Called when we have a new pdu. We need to do auth checks and put it
   454|         through the StateHandler.
   455|         Args:
   456|             origin: server sending the event
   457|             event: event to be persisted
   458|             state: Normally None, but if we are handling a gap in the graph
   459|                 (ie, we are missing one or more prev_events), the resolved state at the
   460|                 event
   461|         """
   462|         room_id = event.room_id
   463|         event_id = event.event_id
   464|         logger.debug("[%s %s] Processing event: %s", room_id, event_id, event)
   465|         try:
   466|             await self._handle_new_event(origin, event, state=state)
   467|         except AuthError as e:
   468|             raise FederationError("ERROR", e.code, e.msg, affected=event.event_id)
   469|         if event.type == EventTypes.Encrypted:
   470|             device_id = event.content.get("device_id")
   471|             sender_key = event.content.get("sender_key")
   472|             cached_devices = await self.store.get_cached_devices_for_user(event.sender)
   473|             resync = False  # Whether we should resync device lists.
   474|             device = None
   475|             if device_id is not None:
   476|                 device = cached_devices.get(device_id)
   477|                 if device is None:
   478|                     logger.info(
   479|                         "Received event from remote device not in our cache: %s %s",
   480|                         event.sender,
   481|                         device_id,
   482|                     )
   483|                     resync = True
   484|             if sender_key is not None:
   485|                 current_keys = []  # type: Container[str]
   486|                 if device:
   487|                     keys = device.get("keys", {}).get("keys", {})
   488|                     if (

# --- HUNK 6: Lines 525-566 ---
   525|                 await self._device_list_updater.user_device_resync(sender)
   526|         except Exception:
   527|             logger.exception("Failed to resync device for %s", sender)
   528|     @log_function
   529|     async def backfill(self, dest, room_id, limit, extremities):
   530|         """ Trigger a backfill request to `dest` for the given `room_id`
   531|         This will attempt to get more events from the remote. If the other side
   532|         has no new events to offer, this will return an empty list.
   533|         As the events are received, we check their signatures, and also do some
   534|         sanity-checking on them. If any of the backfilled events are invalid,
   535|         this method throws a SynapseError.
   536|         TODO: make this more useful to distinguish failures of the remote
   537|         server from invalid events (there is probably no point in trying to
   538|         re-fetch invalid events from every other HS in the room.)
   539|         """
   540|         if dest == self.server_name:
   541|             raise SynapseError(400, "Can't backfill from self.")
   542|         events = await self.federation_client.backfill(
   543|             dest, room_id, limit=limit, extremities=extremities
   544|         )
   545|         if not events:
   546|             return []
   547|         seen_events = await self.store.have_events_in_timeline(
   548|             {e.event_id for e in events}
   549|         )
   550|         events = [e for e in events if e.event_id not in seen_events]
   551|         if not events:
   552|             return []
   553|         event_map = {e.event_id: e for e in events}
   554|         event_ids = {e.event_id for e in events}
   555|         edges = [ev.event_id for ev in events if set(ev.prev_event_ids()) - event_ids]
   556|         logger.info("backfill: Got %d events with %d edges", len(events), len(edges))
   557|         auth_events = {}
   558|         state_events = {}
   559|         events_to_state = {}
   560|         for e_id in edges:
   561|             state, auth = await self._get_state_for_room(
   562|                 destination=dest,
   563|                 room_id=room_id,
   564|                 event_id=e_id,
   565|                 include_event_in_state=False,
   566|             )

# --- HUNK 7: Lines 579-620 ---
   579|             {e_id: event_map[e_id] for e_id in required_auth if e_id in event_map}
   580|         )
   581|         ev_infos = []
   582|         for e_id in events_to_state:
   583|             ev = event_map[e_id]
   584|             assert not ev.internal_metadata.is_outlier()
   585|             ev_infos.append(
   586|                 _NewEventInfo(
   587|                     event=ev,
   588|                     state=events_to_state[e_id],
   589|                     auth_events={
   590|                         (
   591|                             auth_events[a_id].type,
   592|                             auth_events[a_id].state_key,
   593|                         ): auth_events[a_id]
   594|                         for a_id in ev.auth_event_ids()
   595|                         if a_id in auth_events
   596|                     },
   597|                 )
   598|             )
   599|         if ev_infos:
   600|             await self._handle_new_events(dest, room_id, ev_infos, backfilled=True)
   601|         events.sort(key=lambda e: e.depth)
   602|         for event in events:
   603|             if event in events_to_state:
   604|                 continue
   605|             assert not event.internal_metadata.is_outlier()
   606|             await self._handle_new_event(dest, event, backfilled=True)
   607|         return events
   608|     async def maybe_backfill(
   609|         self, room_id: str, current_depth: int, limit: int
   610|     ) -> bool:
   611|         """Checks the database to see if we should backfill before paginating,
   612|         and if so do.
   613|         Args:
   614|             room_id
   615|             current_depth: The depth from which we're paginating from. This is
   616|                 used to decide if we should backfill and what extremities to
   617|                 use.
   618|             limit: The number of events that the pagination request will
   619|                 return. This is used as part of the heuristic to decide if we
   620|                 should back paginate.

# --- HUNK 8: Lines 794-834 ---
   794|         auth_events = [
   795|             aid
   796|             for event in event_map.values()
   797|             for aid in event.auth_event_ids()
   798|             if aid not in event_map
   799|         ]
   800|         persisted_events = await self.store.get_events(
   801|             auth_events, allow_rejected=True,
   802|         )
   803|         event_infos = []
   804|         for event in event_map.values():
   805|             auth = {}
   806|             for auth_event_id in event.auth_event_ids():
   807|                 ae = persisted_events.get(auth_event_id) or event_map.get(auth_event_id)
   808|                 if ae:
   809|                     auth[(ae.type, ae.state_key)] = ae
   810|                 else:
   811|                     logger.info("Missing auth event %s", auth_event_id)
   812|             event_infos.append(_NewEventInfo(event, None, auth))
   813|         await self._handle_new_events(
   814|             destination, room_id, event_infos,
   815|         )
   816|     def _sanity_check_event(self, ev):
   817|         """
   818|         Do some early sanity checks of a received event
   819|         In particular, checks it doesn't have an excessive number of
   820|         prev_events or auth_events, which could cause a huge state resolution
   821|         or cascade of event fetches.
   822|         Args:
   823|             ev (synapse.events.EventBase): event to be checked
   824|         Returns: None
   825|         Raises:
   826|             SynapseError if the event does not pass muster
   827|         """
   828|         if len(ev.prev_event_ids()) > 20:
   829|             logger.warning(
   830|                 "Rejecting event %s which has %i prev_events",
   831|                 ev.event_id,
   832|                 len(ev.prev_event_ids()),
   833|             )
   834|             raise SynapseError(HTTPStatus.BAD_REQUEST, "Too many prev_events")

# --- HUNK 9: Lines 894-939 ---
   894|                 host_list.insert(0, origin)
   895|             except ValueError:
   896|                 pass
   897|             ret = await self.federation_client.send_join(
   898|                 host_list, event, room_version_obj
   899|             )
   900|             origin = ret["origin"]
   901|             state = ret["state"]
   902|             auth_chain = ret["auth_chain"]
   903|             auth_chain.sort(key=lambda e: e.depth)
   904|             handled_events.update([s.event_id for s in state])
   905|             handled_events.update([a.event_id for a in auth_chain])
   906|             handled_events.add(event.event_id)
   907|             logger.debug("do_invite_join auth_chain: %s", auth_chain)
   908|             logger.debug("do_invite_join state: %s", state)
   909|             logger.debug("do_invite_join event: %s", event)
   910|             await self.store.upsert_room_on_join(
   911|                 room_id=room_id, room_version=room_version_obj,
   912|             )
   913|             max_stream_id = await self._persist_auth_tree(
   914|                 origin, room_id, auth_chain, state, event, room_version_obj
   915|             )
   916|             await self._replication.wait_for_stream_position(
   917|                 self.config.worker.events_shard_config.get_instance(room_id),
   918|                 "events",
   919|                 max_stream_id,
   920|             )
   921|             predecessor = await self.store.get_room_predecessor(room_id)
   922|             if not predecessor or not isinstance(predecessor.get("room_id"), str):
   923|                 return event.event_id, max_stream_id
   924|             old_room_id = predecessor["room_id"]
   925|             logger.debug(
   926|                 "Found predecessor for %s during remote join: %s", room_id, old_room_id
   927|             )
   928|             member_handler = self.hs.get_room_member_handler()
   929|             await member_handler.transfer_room_state_on_room_upgrade(
   930|                 old_room_id, room_id
   931|             )
   932|             logger.debug("Finished joining %s to %s", joinee, room_id)
   933|             return event.event_id, max_stream_id
   934|         finally:
   935|             room_queue = self.room_queues[room_id]
   936|             del self.room_queues[room_id]
   937|             run_in_background(self._handle_queued_pdus, room_queue)
   938|     async def _handle_queued_pdus(self, room_queue):
   939|         """Process PDUs which got queued up while we were busy send_joining.

# --- HUNK 10: Lines 1027-1066 ---
  1027|                 event.sender,
  1028|                 origin,
  1029|             )
  1030|             raise SynapseError(403, "User not from origin", Codes.FORBIDDEN)
  1031|         event.internal_metadata.outlier = False
  1032|         event.internal_metadata.send_on_behalf_of = origin
  1033|         context = await self._handle_new_event(origin, event)
  1034|         event_allowed = await self.third_party_event_rules.check_event_allowed(
  1035|             event, context
  1036|         )
  1037|         if not event_allowed:
  1038|             logger.info("Sending of join %s forbidden by third-party rules", event)
  1039|             raise SynapseError(
  1040|                 403, "This event is not allowed in this context", Codes.FORBIDDEN
  1041|             )
  1042|         logger.debug(
  1043|             "on_send_join_request: After _handle_new_event: %s, sigs: %s",
  1044|             event.event_id,
  1045|             event.signatures,
  1046|         )
  1047|         prev_state_ids = await context.get_prev_state_ids()
  1048|         state_ids = list(prev_state_ids.values())
  1049|         auth_chain = await self.store.get_auth_chain(state_ids)
  1050|         state = await self.store.get_events(list(prev_state_ids.values()))
  1051|         return {"state": list(state.values()), "auth_chain": auth_chain}
  1052|     async def on_invite_request(
  1053|         self, origin: str, event: EventBase, room_version: RoomVersion
  1054|     ):
  1055|         """ We've got an invite event. Process and persist it. Sign it.
  1056|         Respond with the now signed event.
  1057|         """
  1058|         if event.state_key is None:
  1059|             raise SynapseError(400, "The invite event did not have a state key")
  1060|         is_blocked = await self.store.is_room_blocked(event.room_id)
  1061|         if is_blocked:
  1062|             raise SynapseError(403, "This room has been blocked on this server")
  1063|         if self.hs.config.block_non_admin_invites:
  1064|             raise SynapseError(403, "This server does not accept room invites")
  1065|         if not self.spam_checker.user_may_invite(
  1066|             event.sender, event.state_key, event.room_id

# --- HUNK 11: Lines 1077-1137 ---
  1077|                 400, "The invite event was not from the server sending it"
  1078|             )
  1079|         if not self.is_mine_id(event.state_key):
  1080|             raise SynapseError(400, "The invite event must be for this server")
  1081|         if event.state_key == self._server_notices_mxid:
  1082|             raise SynapseError(HTTPStatus.FORBIDDEN, "Cannot invite this user")
  1083|         await self._maybe_store_room_on_invite(
  1084|             room_id=event.room_id, room_version=room_version
  1085|         )
  1086|         event.internal_metadata.outlier = True
  1087|         event.internal_metadata.out_of_band_membership = True
  1088|         event.signatures.update(
  1089|             compute_event_signature(
  1090|                 room_version,
  1091|                 event.get_pdu_json(),
  1092|                 self.hs.hostname,
  1093|                 self.hs.signing_key,
  1094|             )
  1095|         )
  1096|         context = await self.state_handler.compute_event_context(event)
  1097|         await self.persist_events_and_notify(event.room_id, [(event, context)])
  1098|         return event
  1099|     async def do_remotely_reject_invite(
  1100|         self, target_hosts: Iterable[str], room_id: str, user_id: str, content: JsonDict
  1101|     ) -> Tuple[EventBase, int]:
  1102|         origin, event, room_version = await self._make_and_verify_event(
  1103|             target_hosts, room_id, user_id, "leave", content=content
  1104|         )
  1105|         event.internal_metadata.outlier = True
  1106|         event.internal_metadata.out_of_band_membership = True
  1107|         host_list = list(target_hosts)
  1108|         try:
  1109|             host_list.remove(origin)
  1110|             host_list.insert(0, origin)
  1111|         except ValueError:
  1112|             pass
  1113|         await self.federation_client.send_leave(host_list, event)
  1114|         context = await self.state_handler.compute_event_context(event)
  1115|         stream_id = await self.persist_events_and_notify(
  1116|             event.room_id, [(event, context)]
  1117|         )
  1118|         return event, stream_id
  1119|     async def _make_and_verify_event(
  1120|         self,
  1121|         target_hosts: Iterable[str],
  1122|         room_id: str,
  1123|         user_id: str,
  1124|         membership: str,
  1125|         content: JsonDict = {},
  1126|         params: Optional[Dict[str, Union[str, Iterable[str]]]] = None,
  1127|     ) -> Tuple[str, EventBase, RoomVersion]:
  1128|         (
  1129|             origin,
  1130|             event,
  1131|             room_version,
  1132|         ) = await self.federation_client.make_membership_event(
  1133|             target_hosts, room_id, user_id, membership, content, params=params
  1134|         )
  1135|         logger.debug("Got response to make_%s: %s", membership, event)
  1136|         assert event.type == EventTypes.Member
  1137|         assert event.user_id == user_id

# --- HUNK 12: Lines 1293-1395 ---
  1293|         else:
  1294|             return None
  1295|     async def get_min_depth_for_context(self, context):
  1296|         return await self.store.get_min_depth(context)
  1297|     async def _handle_new_event(
  1298|         self, origin, event, state=None, auth_events=None, backfilled=False
  1299|     ):
  1300|         context = await self._prep_event(
  1301|             origin, event, state=state, auth_events=auth_events, backfilled=backfilled
  1302|         )
  1303|         try:
  1304|             if (
  1305|                 not event.internal_metadata.is_outlier()
  1306|                 and not backfilled
  1307|                 and not context.rejected
  1308|             ):
  1309|                 await self.action_generator.handle_push_actions_for_event(
  1310|                     event, context
  1311|                 )
  1312|             await self.persist_events_and_notify(
  1313|                 event.room_id, [(event, context)], backfilled=backfilled
  1314|             )
  1315|         except Exception:
  1316|             run_in_background(
  1317|                 self.store.remove_push_actions_from_staging, event.event_id
  1318|             )
  1319|             raise
  1320|         return context
  1321|     async def _handle_new_events(
  1322|         self,
  1323|         origin: str,
  1324|         room_id: str,
  1325|         event_infos: Iterable[_NewEventInfo],
  1326|         backfilled: bool = False,
  1327|     ) -> None:
  1328|         """Creates the appropriate contexts and persists events. The events
  1329|         should not depend on one another, e.g. this should be used to persist
  1330|         a bunch of outliers, but not a chunk of individual events that depend
  1331|         on each other for state calculations.
  1332|         Notifies about the events where appropriate.
  1333|         """
  1334|         async def prep(ev_info: _NewEventInfo):
  1335|             event = ev_info.event
  1336|             with nested_logging_context(suffix=event.event_id):
  1337|                 res = await self._prep_event(
  1338|                     origin,
  1339|                     event,
  1340|                     state=ev_info.state,
  1341|                     auth_events=ev_info.auth_events,
  1342|                     backfilled=backfilled,
  1343|                 )
  1344|             return res
  1345|         contexts = await make_deferred_yieldable(
  1346|             defer.gatherResults(
  1347|                 [run_in_background(prep, ev_info) for ev_info in event_infos],
  1348|                 consumeErrors=True,
  1349|             )
  1350|         )
  1351|         await self.persist_events_and_notify(
  1352|             room_id,
  1353|             [
  1354|                 (ev_info.event, context)
  1355|                 for ev_info, context in zip(event_infos, contexts)
  1356|             ],
  1357|             backfilled=backfilled,
  1358|         )
  1359|     async def _persist_auth_tree(
  1360|         self,
  1361|         origin: str,
  1362|         room_id: str,
  1363|         auth_events: List[EventBase],
  1364|         state: List[EventBase],
  1365|         event: EventBase,
  1366|         room_version: RoomVersion,
  1367|     ) -> int:
  1368|         """Checks the auth chain is valid (and passes auth checks) for the
  1369|         state and event. Then persists the auth chain and state atomically.
  1370|         Persists the event separately. Notifies about the persisted events
  1371|         where appropriate.
  1372|         Will attempt to fetch missing auth events.
  1373|         Args:
  1374|             origin: Where the events came from
  1375|             room_id,
  1376|             auth_events
  1377|             state
  1378|             event
  1379|             room_version: The room version we expect this room to have, and
  1380|                 will raise if it doesn't match the version in the create event.
  1381|         """
  1382|         events_to_context = {}
  1383|         for e in itertools.chain(auth_events, state):
  1384|             e.internal_metadata.outlier = True
  1385|             ctx = await self.state_handler.compute_event_context(e)
  1386|             events_to_context[e.event_id] = ctx
  1387|         event_map = {
  1388|             e.event_id: e for e in itertools.chain(auth_events, state, [event])
  1389|         }
  1390|         create_event = None
  1391|         for e in auth_events:
  1392|             if (e.type, e.state_key) == (EventTypes.Create, ""):
  1393|                 create_event = e
  1394|                 break
  1395|         if create_event is None:

# --- HUNK 13: Lines 1411-1462 ---
  1411|             if m_ev and m_ev.event_id == e_id:
  1412|                 event_map[e_id] = m_ev
  1413|             else:
  1414|                 logger.info("Failed to find auth event %r", e_id)
  1415|         for e in itertools.chain(auth_events, state, [event]):
  1416|             auth_for_e = {
  1417|                 (event_map[e_id].type, event_map[e_id].state_key): event_map[e_id]
  1418|                 for e_id in e.auth_event_ids()
  1419|                 if e_id in event_map
  1420|             }
  1421|             if create_event:
  1422|                 auth_for_e[(EventTypes.Create, "")] = create_event
  1423|             try:
  1424|                 event_auth.check(room_version, e, auth_events=auth_for_e)
  1425|             except SynapseError as err:
  1426|                 logger.warning("Rejecting %s because %s", e.event_id, err.msg)
  1427|                 if e == event:
  1428|                     raise
  1429|                 events_to_context[e.event_id].rejected = RejectedReason.AUTH_ERROR
  1430|         await self.persist_events_and_notify(
  1431|             room_id,
  1432|             [
  1433|                 (e, events_to_context[e.event_id])
  1434|                 for e in itertools.chain(auth_events, state)
  1435|             ],
  1436|         )
  1437|         new_event_context = await self.state_handler.compute_event_context(
  1438|             event, old_state=state
  1439|         )
  1440|         return await self.persist_events_and_notify(
  1441|             room_id, [(event, new_event_context)]
  1442|         )
  1443|     async def _prep_event(
  1444|         self,
  1445|         origin: str,
  1446|         event: EventBase,
  1447|         state: Optional[Iterable[EventBase]],
  1448|         auth_events: Optional[MutableStateMap[EventBase]],
  1449|         backfilled: bool,
  1450|     ) -> EventContext:
  1451|         context = await self.state_handler.compute_event_context(event, old_state=state)
  1452|         if not auth_events:
  1453|             prev_state_ids = await context.get_prev_state_ids()
  1454|             auth_events_ids = self.auth.compute_auth_events(
  1455|                 event, prev_state_ids, for_verification=True
  1456|             )
  1457|             auth_events_x = await self.store.get_events(auth_events_ids)
  1458|             auth_events = {(e.type, e.state_key): e for e in auth_events_x.values()}
  1459|         if event.type == EventTypes.Member and not event.auth_event_ids():
  1460|             if len(event.prev_event_ids()) == 1 and event.depth < 5:
  1461|                 c = await self.store.get_event(
  1462|                     event.prev_event_ids()[0], allow_none=True

# --- HUNK 14: Lines 1472-1515 ---
  1472|     async def _check_for_soft_fail(
  1473|         self, event: EventBase, state: Optional[Iterable[EventBase]], backfilled: bool
  1474|     ) -> None:
  1475|         """Checks if we should soft fail the event; if so, marks the event as
  1476|         such.
  1477|         Args:
  1478|             event
  1479|             state: The state at the event if we don't have all the event's prev events
  1480|             backfilled: Whether the event is from backfill
  1481|         """
  1482|         if backfilled or event.internal_metadata.is_outlier():
  1483|             return
  1484|         extrem_ids_list = await self.store.get_latest_event_ids_in_room(event.room_id)
  1485|         extrem_ids = set(extrem_ids_list)
  1486|         prev_event_ids = set(event.prev_event_ids())
  1487|         if extrem_ids == prev_event_ids:
  1488|             return
  1489|         room_version = await self.store.get_room_version_id(event.room_id)
  1490|         room_version_obj = KNOWN_ROOM_VERSIONS[room_version]
  1491|         if state is not None:
  1492|             state_sets_d = await self.state_store.get_state_groups(
  1493|                 event.room_id, extrem_ids
  1494|             )
  1495|             state_sets = list(state_sets_d.values())  # type: List[Iterable[EventBase]]
  1496|             state_sets.append(state)
  1497|             current_states = await self.state_handler.resolve_events(
  1498|                 room_version, state_sets, event
  1499|             )
  1500|             current_state_ids = {
  1501|                 k: e.event_id for k, e in current_states.items()
  1502|             }  # type: StateMap[str]
  1503|         else:
  1504|             current_state_ids = await self.state_handler.get_current_state_ids(
  1505|                 event.room_id, latest_event_ids=extrem_ids
  1506|             )
  1507|         logger.debug(
  1508|             "Doing soft-fail check for %s: state %s", event.event_id, current_state_ids,
  1509|         )
  1510|         auth_types = auth_types_for_event(event)
  1511|         current_state_ids_list = [
  1512|             e for k, e in current_state_ids.items() if k in auth_types
  1513|         ]
  1514|         auth_events_map = await self.store.get_events(current_state_ids_list)
  1515|         current_auth_events = {

# --- HUNK 15: Lines 2030-2138 ---
  2030|         raise last_exception
  2031|     async def _check_key_revocation(self, public_key, url):
  2032|         """
  2033|         Checks whether public_key has been revoked.
  2034|         Args:
  2035|             public_key (str): base-64 encoded public key.
  2036|             url (str): Key revocation URL.
  2037|         Raises:
  2038|             AuthError: if they key has been revoked.
  2039|             SynapseError: if a transient error meant a key couldn't be checked
  2040|                 for revocation.
  2041|         """
  2042|         try:
  2043|             response = await self.http_client.get_json(url, {"public_key": public_key})
  2044|         except Exception:
  2045|             raise SynapseError(502, "Third party certificate could not be checked")
  2046|         if "valid" not in response or not response["valid"]:
  2047|             raise AuthError(403, "Third party certificate was invalid")
  2048|     async def persist_events_and_notify(
  2049|         self,
  2050|         room_id: str,
  2051|         event_and_contexts: Sequence[Tuple[EventBase, EventContext]],
  2052|         backfilled: bool = False,
  2053|     ) -> int:
  2054|         """Persists events and tells the notifier/pushers about them, if
  2055|         necessary.
  2056|         Args:
  2057|             room_id: The room ID of events being persisted.
  2058|             event_and_contexts: Sequence of events with their associated
  2059|                 context that should be persisted. All events must belong to
  2060|                 the same room.
  2061|             backfilled: Whether these events are a result of
  2062|                 backfilling or not
  2063|         """
  2064|         instance = self.config.worker.events_shard_config.get_instance(room_id)
  2065|         if instance != self._instance_name:
  2066|             result = await self._send_events(
  2067|                 instance_name=instance,
  2068|                 store=self.store,
  2069|                 room_id=room_id,
  2070|                 event_and_contexts=event_and_contexts,
  2071|                 backfilled=backfilled,
  2072|             )
  2073|             return result["max_stream_id"]
  2074|         else:
  2075|             assert self.storage.persistence
  2076|             max_stream_token = await self.storage.persistence.persist_events(
  2077|                 event_and_contexts, backfilled=backfilled
  2078|             )
  2079|             if self._ephemeral_messages_enabled:
  2080|                 for (event, context) in event_and_contexts:
  2081|                     self._message_handler.maybe_schedule_expiry(event)
  2082|             if not backfilled:  # Never notify for backfilled events
  2083|                 for event, _ in event_and_contexts:
  2084|                     await self._notify_persisted_event(event, max_stream_token)
  2085|             return max_stream_token.stream
  2086|     async def _notify_persisted_event(
  2087|         self, event: EventBase, max_stream_token: RoomStreamToken
  2088|     ) -> None:
  2089|         """Checks to see if notifier/pushers should be notified about the
  2090|         event or not.
  2091|         Args:
  2092|             event:
  2093|             max_stream_id: The max_stream_id returned by persist_events
  2094|         """
  2095|         extra_users = []
  2096|         if event.type == EventTypes.Member:
  2097|             target_user_id = event.state_key
  2098|             if event.internal_metadata.is_outlier():
  2099|                 if event.membership != Membership.INVITE:
  2100|                     if not self.is_mine_id(target_user_id):
  2101|                         return
  2102|             target_user = UserID.from_string(target_user_id)
  2103|             extra_users.append(target_user)
  2104|         elif event.internal_metadata.is_outlier():
  2105|             return
  2106|         event_pos = PersistedEventPosition(
  2107|             self._instance_name, event.internal_metadata.stream_ordering
  2108|         )
  2109|         self.notifier.on_new_room_event(
  2110|             event, event_pos, max_stream_token, extra_users=extra_users
  2111|         )
  2112|     async def _clean_room_for_join(self, room_id: str) -> None:
  2113|         """Called to clean up any data in DB for a given room, ready for the
  2114|         server to join the room.
  2115|         Args:
  2116|             room_id
  2117|         """
  2118|         if self.config.worker_app:
  2119|             await self._clean_room_for_join_client(room_id)
  2120|         else:
  2121|             await self.store.clean_room_for_join(room_id)
  2122|     async def get_room_complexity(
  2123|         self, remote_room_hosts: List[str], room_id: str
  2124|     ) -> Optional[dict]:
  2125|         """
  2126|         Fetch the complexity of a remote room over federation.
  2127|         Args:
  2128|             remote_room_hosts (list[str]): The remote servers to ask.
  2129|             room_id (str): The room ID to ask about.
  2130|         Returns:
  2131|             Dict contains the complexity
  2132|             metric versions, while None means we could not fetch the complexity.
  2133|         """
  2134|         for host in remote_room_hosts:
  2135|             res = await self.federation_client.get_room_complexity(host, room_id)
  2136|             if res:
  2137|                 return res
  2138|         return None


# ====================================================================
# FILE: synapse/handlers/groups_local.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 151-191 ---
   151|                 destinations.setdefault(get_domain_from_id(user_id), set()).add(user_id)
   152|         if not proxy and destinations:
   153|             raise SynapseError(400, "Some user_ids are not local")
   154|         results = {}
   155|         failed_results = []
   156|         for destination, dest_user_ids in destinations.items():
   157|             try:
   158|                 r = await self.transport_client.bulk_get_publicised_groups(
   159|                     destination, list(dest_user_ids)
   160|                 )
   161|                 results.update(r["users"])
   162|             except Exception:
   163|                 failed_results.extend(dest_user_ids)
   164|         for uid in local_users:
   165|             results[uid] = await self.store.get_publicised_groups_for_user(uid)
   166|             for app_service in self.store.get_app_services():
   167|                 results[uid].extend(app_service.get_groups_for_user(uid))
   168|         return {"users": results}
   169| class GroupsLocalHandler(GroupsLocalWorkerHandler):
   170|     def __init__(self, hs):
   171|         super().__init__(hs)
   172|         hs.get_groups_attestation_renewer()
   173|     update_group_profile = _create_rerouter("update_group_profile")
   174|     add_room_to_group = _create_rerouter("add_room_to_group")
   175|     update_room_in_group = _create_rerouter("update_room_in_group")
   176|     remove_room_from_group = _create_rerouter("remove_room_from_group")
   177|     update_group_summary_room = _create_rerouter("update_group_summary_room")
   178|     delete_group_summary_room = _create_rerouter("delete_group_summary_room")
   179|     update_group_category = _create_rerouter("update_group_category")
   180|     delete_group_category = _create_rerouter("delete_group_category")
   181|     update_group_summary_user = _create_rerouter("update_group_summary_user")
   182|     delete_group_summary_user = _create_rerouter("delete_group_summary_user")
   183|     update_group_role = _create_rerouter("update_group_role")
   184|     delete_group_role = _create_rerouter("delete_group_role")
   185|     set_group_join_policy = _create_rerouter("set_group_join_policy")
   186|     async def create_group(self, group_id, user_id, content):
   187|         """Create a group
   188|         """
   189|         logger.info("Asking to create group with ID: %r", group_id)
   190|         if self.is_mine_id(group_id):
   191|             res = await self.groups_server_handler.create_group(


# ====================================================================
# FILE: synapse/handlers/identity.py
# Total hunks: 11
# ====================================================================
# --- HUNK 1: Lines 1-81 ---
     1| """Utilities for interacting with Identity Servers"""
     2| import logging
     3| import urllib.parse
     4| from typing import Awaitable, Callable, Dict, List, Optional, Tuple
     5| from synapse.api.errors import (
     6|     CodeMessageException,
     7|     Codes,
     8|     HttpResponseException,
     9|     SynapseError,
    10| )
    11| from synapse.config.emailconfig import ThreepidBehaviour
    12| from synapse.http import RequestTimedOutError
    13| from synapse.http.client import SimpleHttpClient
    14| from synapse.types import JsonDict, Requester
    15| from synapse.util import json_decoder
    16| from synapse.util.hash import sha256_and_url_safe_base64
    17| from synapse.util.stringutils import assert_valid_client_secret, random_string
    18| from ._base import BaseHandler
    19| logger = logging.getLogger(__name__)
    20| id_server_scheme = "https://"
    21| class IdentityHandler(BaseHandler):
    22|     def __init__(self, hs):
    23|         super().__init__(hs)
    24|         self.http_client = SimpleHttpClient(hs)
    25|         self.blacklisting_http_client = SimpleHttpClient(
    26|             hs, ip_blacklist=hs.config.federation_ip_range_blacklist
    27|         )
    28|         self.federation_http_client = hs.get_http_client()
    29|         self.hs = hs
    30|     async def threepid_from_creds(
    31|         self, id_server: str, creds: Dict[str, str]
    32|     ) -> Optional[JsonDict]:
    33|         """
    34|         Retrieve and validate a threepid identifier from a "credentials" dictionary against a
    35|         given identity server
    36|         Args:
    37|             id_server: The identity server to validate 3PIDs against. Must be a
    38|                 complete URL including the protocol (http(s)://)
    39|             creds: Dictionary containing the following keys:
    40|                 * client_secret|clientSecret: A unique secret str provided by the client
    41|                 * sid: The ID of the validation session
    42|         Returns:
    43|             A dictionary consisting of response params to the /getValidated3pid
    44|             endpoint of the Identity Service API, or None if the threepid was not found
    45|         """
    46|         client_secret = creds.get("client_secret") or creds.get("clientSecret")
    47|         if not client_secret:
    48|             raise SynapseError(
    49|                 400, "Missing param client_secret in creds", errcode=Codes.MISSING_PARAM
    50|             )
    51|         assert_valid_client_secret(client_secret)
    52|         session_id = creds.get("sid")
    53|         if not session_id:
    54|             raise SynapseError(
    55|                 400, "Missing param session_id in creds", errcode=Codes.MISSING_PARAM
    56|             )
    57|         query_params = {"sid": session_id, "client_secret": client_secret}
    58|         url = id_server + "/_matrix/identity/api/v1/3pid/getValidated3pid"
    59|         try:
    60|             data = await self.http_client.get_json(url, query_params)
    61|         except RequestTimedOutError:
    62|             raise SynapseError(500, "Timed out contacting identity server")
    63|         except HttpResponseException as e:
    64|             logger.info(
    65|                 "%s returned %i for threepid validation for: %s",
    66|                 id_server,
    67|                 e.code,
    68|                 creds,
    69|             )
    70|             return None
    71|         if "medium" in data:
    72|             return data
    73|         logger.info("%s reported non-validated threepid: %s", id_server, creds)
    74|         return None
    75|     async def bind_threepid(
    76|         self,
    77|         client_secret: str,
    78|         sid: str,
    79|         mxid: str,
    80|         id_server: str,
    81|         id_access_token: Optional[str] = None,

# --- HUNK 2: Lines 101-141 ---
   101|         if use_v2:
   102|             bind_url = "https://%s/_matrix/identity/v2/3pid/bind" % (id_server,)
   103|             headers["Authorization"] = create_id_access_token_header(id_access_token)  # type: ignore
   104|         else:
   105|             bind_url = "https://%s/_matrix/identity/api/v1/3pid/bind" % (id_server,)
   106|         try:
   107|             data = await self.blacklisting_http_client.post_json_get_json(
   108|                 bind_url, bind_data, headers=headers
   109|             )
   110|             await self.store.add_user_bound_threepid(
   111|                 user_id=mxid,
   112|                 medium=data["medium"],
   113|                 address=data["address"],
   114|                 id_server=id_server,
   115|             )
   116|             return data
   117|         except HttpResponseException as e:
   118|             if e.code != 404 or not use_v2:
   119|                 logger.error("3PID bind failed with Matrix error: %r", e)
   120|                 raise e.to_synapse_error()
   121|         except RequestTimedOutError:
   122|             raise SynapseError(500, "Timed out contacting identity server")
   123|         except CodeMessageException as e:
   124|             data = json_decoder.decode(e.msg)  # XXX WAT?
   125|             return data
   126|         logger.info("Got 404 when POSTing JSON %s, falling back to v1 URL", bind_url)
   127|         res = await self.bind_threepid(
   128|             client_secret, sid, mxid, id_server, id_access_token, use_v2=False
   129|         )
   130|         return res
   131|     async def try_unbind_threepid(self, mxid: str, threepid: dict) -> bool:
   132|         """Attempt to remove a 3PID from an identity server, or if one is not provided, all
   133|         identity servers we're aware the binding is present on
   134|         Args:
   135|             mxid: Matrix user ID of binding to be removed
   136|             threepid: Dict with medium & address of binding to be
   137|                 removed, and an optional id_server.
   138|         Raises:
   139|             SynapseError: If we failed to contact the identity server
   140|         Returns:
   141|             True on success, otherwise False if the identity

# --- HUNK 3: Lines 179-219 ---
   179|         auth_headers = self.federation_http_client.build_auth_headers(
   180|             destination=None,
   181|             method=b"POST",
   182|             url_bytes=url_bytes,
   183|             content=content,
   184|             destination_is=id_server.encode("ascii"),
   185|         )
   186|         headers = {b"Authorization": auth_headers}
   187|         try:
   188|             await self.blacklisting_http_client.post_json_get_json(
   189|                 url, content, headers
   190|             )
   191|             changed = True
   192|         except HttpResponseException as e:
   193|             changed = False
   194|             if e.code in (400, 404, 501):
   195|                 logger.warning("Received %d response while unbinding threepid", e.code)
   196|             else:
   197|                 logger.error("Failed to unbind threepid on identity server: %s", e)
   198|                 raise SynapseError(500, "Failed to contact identity server")
   199|         except RequestTimedOutError:
   200|             raise SynapseError(500, "Timed out contacting identity server")
   201|         await self.store.remove_user_bound_threepid(
   202|             user_id=mxid,
   203|             medium=threepid["medium"],
   204|             address=threepid["address"],
   205|             id_server=id_server,
   206|         )
   207|         return changed
   208|     async def send_threepid_validation(
   209|         self,
   210|         email_address: str,
   211|         client_secret: str,
   212|         send_attempt: int,
   213|         send_email_func: Callable[[str, str, str, str], Awaitable],
   214|         next_link: Optional[str] = None,
   215|     ) -> str:
   216|         """Send a threepid validation email for password reset or
   217|         registration purposes
   218|         Args:
   219|             email_address: The user's email address

# --- HUNK 4: Lines 292-332 ---
   292|             "send_attempt": send_attempt,
   293|         }
   294|         if next_link:
   295|             params["next_link"] = next_link
   296|         if self.hs.config.using_identity_server_from_trusted_list:
   297|             logger.warning(
   298|                 'The config option "trust_identity_server_for_password_resets" '
   299|                 'has been replaced by "account_threepid_delegate". '
   300|                 "Please consult the sample config at docs/sample_config.yaml for "
   301|                 "details and update your config file."
   302|             )
   303|         try:
   304|             data = await self.http_client.post_json_get_json(
   305|                 id_server + "/_matrix/identity/api/v1/validate/email/requestToken",
   306|                 params,
   307|             )
   308|             return data
   309|         except HttpResponseException as e:
   310|             logger.info("Proxied requestToken failed: %r", e)
   311|             raise e.to_synapse_error()
   312|         except RequestTimedOutError:
   313|             raise SynapseError(500, "Timed out contacting identity server")
   314|     async def requestMsisdnToken(
   315|         self,
   316|         id_server: str,
   317|         country: str,
   318|         phone_number: str,
   319|         client_secret: str,
   320|         send_attempt: int,
   321|         next_link: Optional[str] = None,
   322|     ) -> JsonDict:
   323|         """
   324|         Request an external server send an SMS message on our behalf for the purposes of
   325|         threepid validation.
   326|         Args:
   327|             id_server: The identity server to proxy to
   328|             country: The country code of the phone number
   329|             phone_number: The number to send the message to
   330|             client_secret: The unique client_secret sends by the user
   331|             send_attempt: Which attempt this is
   332|             next_link: A link to redirect the user to once they submit the token

# --- HUNK 5: Lines 339-379 ---
   339|             "client_secret": client_secret,
   340|             "send_attempt": send_attempt,
   341|         }
   342|         if next_link:
   343|             params["next_link"] = next_link
   344|         if self.hs.config.using_identity_server_from_trusted_list:
   345|             logger.warning(
   346|                 'The config option "trust_identity_server_for_password_resets" '
   347|                 'has been replaced by "account_threepid_delegate". '
   348|                 "Please consult the sample config at docs/sample_config.yaml for "
   349|                 "details and update your config file."
   350|             )
   351|         try:
   352|             data = await self.http_client.post_json_get_json(
   353|                 id_server + "/_matrix/identity/api/v1/validate/msisdn/requestToken",
   354|                 params,
   355|             )
   356|         except HttpResponseException as e:
   357|             logger.info("Proxied requestToken failed: %r", e)
   358|             raise e.to_synapse_error()
   359|         except RequestTimedOutError:
   360|             raise SynapseError(500, "Timed out contacting identity server")
   361|         assert self.hs.config.public_baseurl
   362|         data["submit_url"] = (
   363|             self.hs.config.public_baseurl
   364|             + "_matrix/client/unstable/add_threepid/msisdn/submit_token"
   365|         )
   366|         return data
   367|     async def validate_threepid_session(
   368|         self, client_secret: str, sid: str
   369|     ) -> Optional[JsonDict]:
   370|         """Validates a threepid session with only the client secret and session ID
   371|         Tries validating against any configured account_threepid_delegates as well as locally.
   372|         Args:
   373|             client_secret: A secret provided by the client
   374|             sid: The ID of the session
   375|         Returns:
   376|             The json response if validation was successful, otherwise None
   377|         """
   378|         threepid_creds = {"client_secret": client_secret, "sid": sid}
   379|         validation_session = None

# --- HUNK 6: Lines 395-435 ---
   395|     async def proxy_msisdn_submit_token(
   396|         self, id_server: str, client_secret: str, sid: str, token: str
   397|     ) -> JsonDict:
   398|         """Proxy a POST submitToken request to an identity server for verification purposes
   399|         Args:
   400|             id_server: The identity server URL to contact
   401|             client_secret: Secret provided by the client
   402|             sid: The ID of the session
   403|             token: The verification token
   404|         Raises:
   405|             SynapseError: If we failed to contact the identity server
   406|         Returns:
   407|             The response dict from the identity server
   408|         """
   409|         body = {"client_secret": client_secret, "sid": sid, "token": token}
   410|         try:
   411|             return await self.http_client.post_json_get_json(
   412|                 id_server + "/_matrix/identity/api/v1/validate/msisdn/submitToken",
   413|                 body,
   414|             )
   415|         except RequestTimedOutError:
   416|             raise SynapseError(500, "Timed out contacting identity server")
   417|         except HttpResponseException as e:
   418|             logger.warning("Error contacting msisdn account_threepid_delegate: %s", e)
   419|             raise SynapseError(400, "Error contacting the identity server")
   420|     async def lookup_3pid(
   421|         self,
   422|         id_server: str,
   423|         medium: str,
   424|         address: str,
   425|         id_access_token: Optional[str] = None,
   426|     ) -> Optional[str]:
   427|         """Looks up a 3pid in the passed identity server.
   428|         Args:
   429|             id_server: The server name (including port, if required)
   430|                 of the identity server to use.
   431|             medium: The type of the third party identifier (e.g. "email").
   432|             address: The third party identifier (e.g. "foo@example.com").
   433|             id_access_token: The access token to authenticate to the identity
   434|                 server with
   435|         Returns:

# --- HUNK 7: Lines 454-517 ---
   454|         return await self._lookup_3pid_v1(id_server, medium, address)
   455|     async def _lookup_3pid_v1(
   456|         self, id_server: str, medium: str, address: str
   457|     ) -> Optional[str]:
   458|         """Looks up a 3pid in the passed identity server using v1 lookup.
   459|         Args:
   460|             id_server: The server name (including port, if required)
   461|                 of the identity server to use.
   462|             medium: The type of the third party identifier (e.g. "email").
   463|             address: The third party identifier (e.g. "foo@example.com").
   464|         Returns:
   465|             the matrix ID of the 3pid, or None if it is not recognized.
   466|         """
   467|         try:
   468|             data = await self.blacklisting_http_client.get_json(
   469|                 "%s%s/_matrix/identity/api/v1/lookup" % (id_server_scheme, id_server),
   470|                 {"medium": medium, "address": address},
   471|             )
   472|             if "mxid" in data:
   473|                 return data["mxid"]
   474|         except RequestTimedOutError:
   475|             raise SynapseError(500, "Timed out contacting identity server")
   476|         except IOError as e:
   477|             logger.warning("Error from v1 identity server lookup: %s" % (e,))
   478|         return None
   479|     async def _lookup_3pid_v2(
   480|         self, id_server: str, id_access_token: str, medium: str, address: str
   481|     ) -> Optional[str]:
   482|         """Looks up a 3pid in the passed identity server using v2 lookup.
   483|         Args:
   484|             id_server: The server name (including port, if required)
   485|                 of the identity server to use.
   486|             id_access_token: The access token to authenticate to the identity server with
   487|             medium: The type of the third party identifier (e.g. "email").
   488|             address: The third party identifier (e.g. "foo@example.com").
   489|         Returns:
   490|             the matrix ID of the 3pid, or None if it is not recognised.
   491|         """
   492|         try:
   493|             hash_details = await self.blacklisting_http_client.get_json(
   494|                 "%s%s/_matrix/identity/v2/hash_details" % (id_server_scheme, id_server),
   495|                 {"access_token": id_access_token},
   496|             )
   497|         except RequestTimedOutError:
   498|             raise SynapseError(500, "Timed out contacting identity server")
   499|         if not isinstance(hash_details, dict):
   500|             logger.warning(
   501|                 "Got non-dict object when checking hash details of %s%s: %s",
   502|                 id_server_scheme,
   503|                 id_server,
   504|                 hash_details,
   505|             )
   506|             raise SynapseError(
   507|                 400,
   508|                 "Non-dict object from %s%s during v2 hash_details request: %s"
   509|                 % (id_server_scheme, id_server, hash_details),
   510|             )
   511|         supported_lookup_algorithms = hash_details.get("algorithms")
   512|         lookup_pepper = hash_details.get("lookup_pepper")
   513|         if (
   514|             not supported_lookup_algorithms
   515|             or not isinstance(supported_lookup_algorithms, list)
   516|             or not lookup_pepper
   517|             or not isinstance(lookup_pepper, str)

# --- HUNK 8: Lines 533-573 ---
   533|                 "None of the provided lookup algorithms of %s are supported: %s",
   534|                 id_server,
   535|                 supported_lookup_algorithms,
   536|             )
   537|             raise SynapseError(
   538|                 400,
   539|                 "Provided identity server does not support any v2 lookup "
   540|                 "algorithms that this homeserver supports.",
   541|             )
   542|         headers = {"Authorization": create_id_access_token_header(id_access_token)}
   543|         try:
   544|             lookup_results = await self.blacklisting_http_client.post_json_get_json(
   545|                 "%s%s/_matrix/identity/v2/lookup" % (id_server_scheme, id_server),
   546|                 {
   547|                     "addresses": [lookup_value],
   548|                     "algorithm": lookup_algorithm,
   549|                     "pepper": lookup_pepper,
   550|                 },
   551|                 headers=headers,
   552|             )
   553|         except RequestTimedOutError:
   554|             raise SynapseError(500, "Timed out contacting identity server")
   555|         except Exception as e:
   556|             logger.warning("Error when performing a v2 3pid lookup: %s", e)
   557|             raise SynapseError(
   558|                 500, "Unknown error occurred during identity server lookup"
   559|             )
   560|         if "mappings" not in lookup_results or not isinstance(
   561|             lookup_results["mappings"], dict
   562|         ):
   563|             logger.warning("No results from 3pid lookup")
   564|             return None
   565|         mxid = lookup_results["mappings"].get(lookup_value)
   566|         return mxid
   567|     async def ask_id_server_for_third_party_invite(
   568|         self,
   569|         requester: Requester,
   570|         id_server: str,
   571|         medium: str,
   572|         address: str,
   573|         room_id: str,

# --- HUNK 9: Lines 616-672 ---
   616|             "room_join_rules": room_join_rules,
   617|             "room_name": room_name,
   618|             "sender": inviter_user_id,
   619|             "sender_display_name": inviter_display_name,
   620|             "sender_avatar_url": inviter_avatar_url,
   621|         }
   622|         data = None
   623|         base_url = "%s%s/_matrix/identity" % (id_server_scheme, id_server)
   624|         if id_access_token:
   625|             key_validity_url = "%s%s/_matrix/identity/v2/pubkey/isvalid" % (
   626|                 id_server_scheme,
   627|                 id_server,
   628|             )
   629|             url = base_url + "/v2/store-invite"
   630|             try:
   631|                 data = await self.blacklisting_http_client.post_json_get_json(
   632|                     url,
   633|                     invite_config,
   634|                     {"Authorization": create_id_access_token_header(id_access_token)},
   635|                 )
   636|             except RequestTimedOutError:
   637|                 raise SynapseError(500, "Timed out contacting identity server")
   638|             except HttpResponseException as e:
   639|                 if e.code != 404:
   640|                     logger.info("Failed to POST %s with JSON: %s", url, e)
   641|                     raise e
   642|         if data is None:
   643|             key_validity_url = "%s%s/_matrix/identity/api/v1/pubkey/isvalid" % (
   644|                 id_server_scheme,
   645|                 id_server,
   646|             )
   647|             url = base_url + "/api/v1/store-invite"
   648|             try:
   649|                 data = await self.blacklisting_http_client.post_json_get_json(
   650|                     url, invite_config
   651|                 )
   652|             except RequestTimedOutError:
   653|                 raise SynapseError(500, "Timed out contacting identity server")
   654|             except HttpResponseException as e:
   655|                 logger.warning(
   656|                     "Error trying to call /store-invite on %s%s: %s",
   657|                     id_server_scheme,
   658|                     id_server,
   659|                     e,
   660|                 )
   661|             if data is None:
   662|                 try:
   663|                     data = await self.blacklisting_http_client.post_urlencoded_get_json(
   664|                         url, invite_config
   665|                     )
   666|                 except HttpResponseException as e:
   667|                     logger.warning(
   668|                         "Error calling /store-invite on %s%s with fallback "
   669|                         "encoding: %s",
   670|                         id_server_scheme,
   671|                         id_server,
   672|                         e,


# ====================================================================
# FILE: synapse/handlers/initial_sync.py
# Total hunks: 7
# ====================================================================
# --- HUNK 1: Lines 1-42 ---
     1| import logging
     2| from typing import TYPE_CHECKING
     3| from twisted.internet import defer
     4| from synapse.api.constants import EventTypes, Membership
     5| from synapse.api.errors import SynapseError
     6| from synapse.events.validator import EventValidator
     7| from synapse.handlers.presence import format_user_presence_state
     8| from synapse.logging.context import make_deferred_yieldable, run_in_background
     9| from synapse.storage.roommember import RoomsForUser
    10| from synapse.streams.config import PaginationConfig
    11| from synapse.types import JsonDict, Requester, RoomStreamToken, StreamToken, UserID
    12| from synapse.util import unwrapFirstError
    13| from synapse.util.async_helpers import concurrently_execute
    14| from synapse.util.caches.response_cache import ResponseCache
    15| from synapse.visibility import filter_events_for_client
    16| from ._base import BaseHandler
    17| if TYPE_CHECKING:
    18|     from synapse.server import HomeServer
    19| logger = logging.getLogger(__name__)
    20| class InitialSyncHandler(BaseHandler):
    21|     def __init__(self, hs: "HomeServer"):
    22|         super().__init__(hs)
    23|         self.hs = hs
    24|         self.state = hs.get_state_handler()
    25|         self.clock = hs.get_clock()
    26|         self.validator = EventValidator()
    27|         self.snapshot_cache = ResponseCache(hs, "initial_sync_cache")
    28|         self._event_serializer = hs.get_event_client_serializer()
    29|         self.storage = hs.get_storage()
    30|         self.state_store = self.storage.state
    31|     def snapshot_all_rooms(
    32|         self,
    33|         user_id: str,
    34|         pagin_config: PaginationConfig,
    35|         as_client_event: bool = True,
    36|         include_archived: bool = False,
    37|     ) -> JsonDict:
    38|         """Retrieve a snapshot of all rooms the user is invited or has joined.
    39|         This snapshot may include messages for all rooms where the user is
    40|         joined, depending on the pagination config.
    41|         Args:
    42|             user_id: The ID of the user making the request.

# --- HUNK 2: Lines 65-215 ---
    65|             as_client_event,
    66|             include_archived,
    67|         )
    68|     async def _snapshot_all_rooms(
    69|         self,
    70|         user_id: str,
    71|         pagin_config: PaginationConfig,
    72|         as_client_event: bool = True,
    73|         include_archived: bool = False,
    74|     ) -> JsonDict:
    75|         memberships = [Membership.INVITE, Membership.JOIN]
    76|         if include_archived:
    77|             memberships.append(Membership.LEAVE)
    78|         room_list = await self.store.get_rooms_for_local_user_where_membership_is(
    79|             user_id=user_id, membership_list=memberships
    80|         )
    81|         user = UserID.from_string(user_id)
    82|         rooms_ret = []
    83|         now_token = self.hs.get_event_sources().get_current_token()
    84|         presence_stream = self.hs.get_event_sources().sources["presence"]
    85|         presence, _ = await presence_stream.get_new_events(
    86|             user, from_key=None, include_offline=False
    87|         )
    88|         joined_rooms = [r.room_id for r in room_list if r.membership == Membership.JOIN]
    89|         receipt = await self.store.get_linearized_receipts_for_rooms(
    90|             joined_rooms, to_key=int(now_token.receipt_key),
    91|         )
    92|         tags_by_room = await self.store.get_tags_for_user(user_id)
    93|         account_data, account_data_by_room = await self.store.get_account_data_for_user(
    94|             user_id
    95|         )
    96|         public_room_ids = await self.store.get_public_room_ids()
    97|         limit = pagin_config.limit
    98|         if limit is None:
    99|             limit = 10
   100|         async def handle_room(event: RoomsForUser):
   101|             d = {
   102|                 "room_id": event.room_id,
   103|                 "membership": event.membership,
   104|                 "visibility": (
   105|                     "public" if event.room_id in public_room_ids else "private"
   106|                 ),
   107|             }
   108|             if event.membership == Membership.INVITE:
   109|                 time_now = self.clock.time_msec()
   110|                 d["inviter"] = event.sender
   111|                 invite_event = await self.store.get_event(event.event_id)
   112|                 d["invite"] = await self._event_serializer.serialize_event(
   113|                     invite_event, time_now, as_client_event
   114|                 )
   115|             rooms_ret.append(d)
   116|             if event.membership not in (Membership.JOIN, Membership.LEAVE):
   117|                 return
   118|             try:
   119|                 if event.membership == Membership.JOIN:
   120|                     room_end_token = now_token.room_key
   121|                     deferred_room_state = run_in_background(
   122|                         self.state_handler.get_current_state, event.room_id
   123|                     )
   124|                 elif event.membership == Membership.LEAVE:
   125|                     room_end_token = RoomStreamToken(None, event.stream_ordering,)
   126|                     deferred_room_state = run_in_background(
   127|                         self.state_store.get_state_for_events, [event.event_id]
   128|                     )
   129|                     deferred_room_state.addCallback(
   130|                         lambda states: states[event.event_id]
   131|                     )
   132|                 (messages, token), current_state = await make_deferred_yieldable(
   133|                     defer.gatherResults(
   134|                         [
   135|                             run_in_background(
   136|                                 self.store.get_recent_events_for_room,
   137|                                 event.room_id,
   138|                                 limit=limit,
   139|                                 end_token=room_end_token,
   140|                             ),
   141|                             deferred_room_state,
   142|                         ]
   143|                     )
   144|                 ).addErrback(unwrapFirstError)
   145|                 messages = await filter_events_for_client(
   146|                     self.storage, user_id, messages
   147|                 )
   148|                 start_token = now_token.copy_and_replace("room_key", token)
   149|                 end_token = now_token.copy_and_replace("room_key", room_end_token)
   150|                 time_now = self.clock.time_msec()
   151|                 d["messages"] = {
   152|                     "chunk": (
   153|                         await self._event_serializer.serialize_events(
   154|                             messages, time_now=time_now, as_client_event=as_client_event
   155|                         )
   156|                     ),
   157|                     "start": await start_token.to_string(self.store),
   158|                     "end": await end_token.to_string(self.store),
   159|                 }
   160|                 d["state"] = await self._event_serializer.serialize_events(
   161|                     current_state.values(),
   162|                     time_now=time_now,
   163|                     as_client_event=as_client_event,
   164|                 )
   165|                 account_data_events = []
   166|                 tags = tags_by_room.get(event.room_id)
   167|                 if tags:
   168|                     account_data_events.append(
   169|                         {"type": "m.tag", "content": {"tags": tags}}
   170|                     )
   171|                 account_data = account_data_by_room.get(event.room_id, {})
   172|                 for account_data_type, content in account_data.items():
   173|                     account_data_events.append(
   174|                         {"type": account_data_type, "content": content}
   175|                     )
   176|                 d["account_data"] = account_data_events
   177|             except Exception:
   178|                 logger.exception("Failed to get snapshot")
   179|         await concurrently_execute(handle_room, room_list, 10)
   180|         account_data_events = []
   181|         for account_data_type, content in account_data.items():
   182|             account_data_events.append({"type": account_data_type, "content": content})
   183|         now = self.clock.time_msec()
   184|         ret = {
   185|             "rooms": rooms_ret,
   186|             "presence": [
   187|                 {
   188|                     "type": "m.presence",
   189|                     "content": format_user_presence_state(event, now),
   190|                 }
   191|                 for event in presence
   192|             ],
   193|             "account_data": account_data_events,
   194|             "receipts": receipt,
   195|             "end": await now_token.to_string(self.store),
   196|         }
   197|         return ret
   198|     async def room_initial_sync(
   199|         self, requester: Requester, room_id: str, pagin_config: PaginationConfig
   200|     ) -> JsonDict:
   201|         """Capture the a snapshot of a room. If user is currently a member of
   202|         the room this will be what is currently in the room. If the user left
   203|         the room this will be what was in the room when they left.
   204|         Args:
   205|             requester: The user to get a snapshot for.
   206|             room_id: The room to get a snapshot of.
   207|             pagin_config: The pagination config used to determine how many
   208|                 messages to return.
   209|         Raises:
   210|             AuthError if the user wasn't in the room.
   211|         Returns:
   212|             A JSON serialisable dict with the snapshot of the room.
   213|         """
   214|         blocked = await self.store.is_room_blocked(room_id)
   215|         if blocked:

# --- HUNK 3: Lines 236-295 ---
   236|             account_data_events.append({"type": "m.tag", "content": {"tags": tags}})
   237|         account_data = await self.store.get_account_data_for_room(user_id, room_id)
   238|         for account_data_type, content in account_data.items():
   239|             account_data_events.append({"type": account_data_type, "content": content})
   240|         result["account_data"] = account_data_events
   241|         return result
   242|     async def _room_initial_sync_parted(
   243|         self,
   244|         user_id: str,
   245|         room_id: str,
   246|         pagin_config: PaginationConfig,
   247|         membership: Membership,
   248|         member_event_id: str,
   249|         is_peeking: bool,
   250|     ) -> JsonDict:
   251|         room_state = await self.state_store.get_state_for_events([member_event_id])
   252|         room_state = room_state[member_event_id]
   253|         limit = pagin_config.limit if pagin_config else None
   254|         if limit is None:
   255|             limit = 10
   256|         leave_position = await self.store.get_position_for_event(member_event_id)
   257|         stream_token = leave_position.to_room_stream_token()
   258|         messages, token = await self.store.get_recent_events_for_room(
   259|             room_id, limit=limit, end_token=stream_token
   260|         )
   261|         messages = await filter_events_for_client(
   262|             self.storage, user_id, messages, is_peeking=is_peeking
   263|         )
   264|         start_token = StreamToken.START.copy_and_replace("room_key", token)
   265|         end_token = StreamToken.START.copy_and_replace("room_key", stream_token)
   266|         time_now = self.clock.time_msec()
   267|         return {
   268|             "membership": membership,
   269|             "room_id": room_id,
   270|             "messages": {
   271|                 "chunk": (
   272|                     await self._event_serializer.serialize_events(messages, time_now)
   273|                 ),
   274|                 "start": await start_token.to_string(self.store),
   275|                 "end": await end_token.to_string(self.store),
   276|             },
   277|             "state": (
   278|                 await self._event_serializer.serialize_events(
   279|                     room_state.values(), time_now
   280|                 )
   281|             ),
   282|             "presence": [],
   283|             "receipts": [],
   284|         }
   285|     async def _room_initial_sync_joined(
   286|         self,
   287|         user_id: str,
   288|         room_id: str,
   289|         pagin_config: PaginationConfig,
   290|         membership: Membership,
   291|         is_peeking: bool,
   292|     ) -> JsonDict:
   293|         current_state = await self.state.get_current_state(room_id=room_id)
   294|         time_now = self.clock.time_msec()
   295|         state = await self._event_serializer.serialize_events(

# --- HUNK 4: Lines 336-365 ---
   336|                         room_id,
   337|                         limit=limit,
   338|                         end_token=now_token.room_key,
   339|                     ),
   340|                 ],
   341|                 consumeErrors=True,
   342|             ).addErrback(unwrapFirstError)
   343|         )
   344|         messages = await filter_events_for_client(
   345|             self.storage, user_id, messages, is_peeking=is_peeking
   346|         )
   347|         start_token = now_token.copy_and_replace("room_key", token)
   348|         end_token = now_token
   349|         time_now = self.clock.time_msec()
   350|         ret = {
   351|             "room_id": room_id,
   352|             "messages": {
   353|                 "chunk": (
   354|                     await self._event_serializer.serialize_events(messages, time_now)
   355|                 ),
   356|                 "start": await start_token.to_string(self.store),
   357|                 "end": await end_token.to_string(self.store),
   358|             },
   359|             "state": state,
   360|             "presence": presence,
   361|             "receipts": receipts,
   362|         }
   363|         if not is_peeking:
   364|             ret["membership"] = membership
   365|         return ret


# ====================================================================
# FILE: synapse/handlers/message.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 258-302 ---
   258|             await self.store.expire_event(event_id)
   259|         except Exception as e:
   260|             logger.error("Could not expire event %s: %r", event_id, e)
   261|         await self._schedule_next_expiry()
   262| _DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY = 7 * 24 * 60 * 60 * 1000
   263| class EventCreationHandler:
   264|     def __init__(self, hs: "HomeServer"):
   265|         self.hs = hs
   266|         self.auth = hs.get_auth()
   267|         self.store = hs.get_datastore()
   268|         self.storage = hs.get_storage()
   269|         self.state = hs.get_state_handler()
   270|         self.clock = hs.get_clock()
   271|         self.validator = EventValidator()
   272|         self.profile_handler = hs.get_profile_handler()
   273|         self.event_builder_factory = hs.get_event_builder_factory()
   274|         self.server_name = hs.hostname
   275|         self.notifier = hs.get_notifier()
   276|         self.config = hs.config
   277|         self.require_membership_for_aliases = hs.config.require_membership_for_aliases
   278|         self._events_shard_config = self.config.worker.events_shard_config
   279|         self._instance_name = hs.get_instance_name()
   280|         self.room_invite_state_types = self.hs.config.room_invite_state_types
   281|         self.send_event = ReplicationSendEventRestServlet.make_client(hs)
   282|         self.base_handler = BaseHandler(hs)
   283|         self.limiter = Linearizer(max_count=5, name="room_event_creation_limit")
   284|         self.action_generator = hs.get_action_generator()
   285|         self.spam_checker = hs.get_spam_checker()
   286|         self.third_party_event_rules = hs.get_third_party_event_rules()
   287|         self._block_events_without_consent_error = (
   288|             self.config.block_events_without_consent_error
   289|         )
   290|         self._rooms_to_exclude_from_dummy_event_insertion = {}  # type: Dict[str, int]
   291|         if self._block_events_without_consent_error:
   292|             self._consent_uri_builder = ConsentURIBuilder(self.config)
   293|         if (
   294|             not self.config.worker_app
   295|             and self.config.cleanup_extremities_with_dummy_events
   296|         ):
   297|             self.clock.looping_call(
   298|                 lambda: run_as_background_process(
   299|                     "send_dummy_events_to_fill_extremities",
   300|                     self._send_dummy_events_to_fill_extremities,
   301|                 ),
   302|                 5 * 60 * 1000,

# --- HUNK 2: Lines 647-690 ---
   647|             raise SynapseError(
   648|                 403, "This event is not allowed in this context", Codes.FORBIDDEN
   649|             )
   650|         if event.internal_metadata.is_out_of_band_membership():
   651|             assert event.type == EventTypes.Member
   652|             assert event.content["membership"] == Membership.LEAVE
   653|         else:
   654|             try:
   655|                 await self.auth.check_from_context(room_version, event, context)
   656|             except AuthError as err:
   657|                 logger.warning("Denying new event %r because %s", event, err)
   658|                 raise err
   659|         try:
   660|             dump = frozendict_json_encoder.encode(event.content)
   661|             json_decoder.decode(dump)
   662|         except Exception:
   663|             logger.exception("Failed to encode content: %r", event.content)
   664|             raise
   665|         await self.action_generator.handle_push_actions_for_event(event, context)
   666|         try:
   667|             writer_instance = self._events_shard_config.get_instance(event.room_id)
   668|             if writer_instance != self._instance_name:
   669|                 result = await self.send_event(
   670|                     instance_name=writer_instance,
   671|                     event_id=event.event_id,
   672|                     store=self.store,
   673|                     requester=requester,
   674|                     event=event,
   675|                     context=context,
   676|                     ratelimit=ratelimit,
   677|                     extra_users=extra_users,
   678|                 )
   679|                 stream_id = result["stream_id"]
   680|                 event.internal_metadata.stream_ordering = stream_id
   681|                 return stream_id
   682|             stream_id = await self.persist_and_notify_client_event(
   683|                 requester, event, context, ratelimit=ratelimit, extra_users=extra_users
   684|             )
   685|             return stream_id
   686|         except Exception:
   687|             await self.store.remove_push_actions_from_staging(event.event_id)
   688|             raise
   689|     async def _validate_canonical_alias(
   690|         self, directory_handler, room_alias_str: str, expected_room_id: str

# --- HUNK 3: Lines 708-751 ---
   708|                 )
   709|             raise
   710|         if mapping["room_id"] != expected_room_id:
   711|             raise SynapseError(
   712|                 400,
   713|                 "Room alias %s does not point to the room" % (room_alias_str,),
   714|                 Codes.BAD_ALIAS,
   715|             )
   716|     async def persist_and_notify_client_event(
   717|         self,
   718|         requester: Requester,
   719|         event: EventBase,
   720|         context: EventContext,
   721|         ratelimit: bool = True,
   722|         extra_users: List[UserID] = [],
   723|     ) -> int:
   724|         """Called when we have fully built the event, have already
   725|         calculated the push actions for the event, and checked auth.
   726|         This should only be run on the instance in charge of persisting events.
   727|         """
   728|         assert self.storage.persistence is not None
   729|         assert self._events_shard_config.should_handle(
   730|             self._instance_name, event.room_id
   731|         )
   732|         if ratelimit:
   733|             is_admin_redaction = False
   734|             if event.type == EventTypes.Redaction:
   735|                 original_event = await self.store.get_event(
   736|                     event.redacts,
   737|                     redact_behaviour=EventRedactBehaviour.AS_IS,
   738|                     get_prev_content=False,
   739|                     allow_rejected=False,
   740|                     allow_none=True,
   741|                 )
   742|                 is_admin_redaction = bool(
   743|                     original_event and event.sender != original_event.sender
   744|                 )
   745|             await self.base_handler.ratelimit(
   746|                 requester, is_admin_redaction=is_admin_redaction
   747|             )
   748|         await self.base_handler.maybe_kick_guest_users(event, context)
   749|         if event.type == EventTypes.CanonicalAlias:
   750|             original_alias = None
   751|             original_alt_aliases = []  # type: List[str]

# --- HUNK 4: Lines 820-935 ---
   820|             prev_state_ids = await context.get_prev_state_ids()
   821|             auth_events_ids = self.auth.compute_auth_events(
   822|                 event, prev_state_ids, for_verification=True
   823|             )
   824|             auth_events_map = await self.store.get_events(auth_events_ids)
   825|             auth_events = {(e.type, e.state_key): e for e in auth_events_map.values()}
   826|             room_version = await self.store.get_room_version_id(event.room_id)
   827|             room_version_obj = KNOWN_ROOM_VERSIONS[room_version]
   828|             if event_auth.check_redaction(
   829|                 room_version_obj, event, auth_events=auth_events
   830|             ):
   831|                 if not original_event:
   832|                     raise NotFoundError("Could not find event %s" % (event.redacts,))
   833|                 if event.user_id != original_event.user_id:
   834|                     raise AuthError(403, "You don't have permission to redact events")
   835|                 event.internal_metadata.recheck_redaction = False
   836|         if event.type == EventTypes.Create:
   837|             prev_state_ids = await context.get_prev_state_ids()
   838|             if prev_state_ids:
   839|                 raise AuthError(403, "Changing the room create event is forbidden")
   840|         event_pos, max_stream_token = await self.storage.persistence.persist_event(
   841|             event, context=context
   842|         )
   843|         if self._ephemeral_events_enabled:
   844|             self._message_handler.maybe_schedule_expiry(event)
   845|         def _notify():
   846|             try:
   847|                 self.notifier.on_new_room_event(
   848|                     event, event_pos, max_stream_token, extra_users=extra_users
   849|                 )
   850|             except Exception:
   851|                 logger.exception("Error notifying about new room event")
   852|         run_in_background(_notify)
   853|         if event.type == EventTypes.Message:
   854|             run_in_background(self._bump_active_time, requester.user)
   855|         return event_pos.stream
   856|     async def _bump_active_time(self, user: UserID) -> None:
   857|         try:
   858|             presence = self.hs.get_presence_handler()
   859|             await presence.bump_presence_active_time(user)
   860|         except Exception:
   861|             logger.exception("Error bumping presence active time")
   862|     async def _send_dummy_events_to_fill_extremities(self):
   863|         """Background task to send dummy events into rooms that have a large
   864|         number of extremities
   865|         """
   866|         self._expire_rooms_to_exclude_from_dummy_event_insertion()
   867|         room_ids = await self.store.get_rooms_with_many_extremities(
   868|             min_count=self._dummy_events_threshold,
   869|             limit=5,
   870|             room_id_filter=self._rooms_to_exclude_from_dummy_event_insertion.keys(),
   871|         )
   872|         for room_id in room_ids:
   873|             dummy_event_sent = await self._send_dummy_event_for_room(room_id)
   874|             if not dummy_event_sent:
   875|                 logger.info(
   876|                     "Failed to send dummy event into room %s. Will exclude it from "
   877|                     "future attempts until cache expires" % (room_id,)
   878|                 )
   879|                 now = self.clock.time_msec()
   880|                 self._rooms_to_exclude_from_dummy_event_insertion[room_id] = now
   881|     async def _send_dummy_event_for_room(self, room_id: str) -> bool:
   882|         """Attempt to send a dummy event for the given room.
   883|         Args:
   884|             room_id: room to try to send an event from
   885|         Returns:
   886|             True if a dummy event was successfully sent. False if no user was able
   887|             to send an event.
   888|         """
   889|         latest_event_ids = await self.store.get_prev_events_for_room(room_id)
   890|         members = await self.state.get_current_users_in_room(
   891|             room_id, latest_event_ids=latest_event_ids
   892|         )
   893|         for user_id in members:
   894|             if not self.hs.is_mine_id(user_id):
   895|                 continue
   896|             requester = create_requester(user_id)
   897|             try:
   898|                 event, context = await self.create_event(
   899|                     requester,
   900|                     {
   901|                         "type": "org.matrix.dummy_event",
   902|                         "content": {},
   903|                         "room_id": room_id,
   904|                         "sender": user_id,
   905|                     },
   906|                     prev_event_ids=latest_event_ids,
   907|                 )
   908|                 event.internal_metadata.proactively_send = False
   909|                 await self.send_nonmember_event(
   910|                     requester, event, context, ratelimit=False, ignore_shadow_ban=True,
   911|                 )
   912|                 return True
   913|             except ConsentNotGivenError:
   914|                 logger.info(
   915|                     "Failed to send dummy event into room %s for user %s due to "
   916|                     "lack of consent. Will try another user" % (room_id, user_id)
   917|                 )
   918|             except AuthError:
   919|                 logger.info(
   920|                     "Failed to send dummy event into room %s for user %s due to "
   921|                     "lack of power. Will try another user" % (room_id, user_id)
   922|                 )
   923|         return False
   924|     def _expire_rooms_to_exclude_from_dummy_event_insertion(self):
   925|         expire_before = self.clock.time_msec() - _DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY
   926|         to_expire = set()
   927|         for room_id, time in self._rooms_to_exclude_from_dummy_event_insertion.items():
   928|             if time < expire_before:
   929|                 to_expire.add(room_id)
   930|         for room_id in to_expire:
   931|             logger.debug(
   932|                 "Expiring room id %s from dummy event insertion exclusion cache",
   933|                 room_id,
   934|             )
   935|             del self._rooms_to_exclude_from_dummy_event_insertion[room_id]


# ====================================================================
# FILE: synapse/handlers/oidc_handler.py
# Total hunks: 7
# ====================================================================
# --- HUNK 1: Lines 3-43 ---
     3| from urllib.parse import urlencode
     4| import attr
     5| import pymacaroons
     6| from authlib.common.security import generate_token
     7| from authlib.jose import JsonWebToken
     8| from authlib.oauth2.auth import ClientAuth
     9| from authlib.oauth2.rfc6749.parameters import prepare_grant_uri
    10| from authlib.oidc.core import CodeIDToken, ImplicitIDToken, UserInfo
    11| from authlib.oidc.discovery import OpenIDProviderMetadata, get_well_known_url
    12| from jinja2 import Environment, Template
    13| from pymacaroons.exceptions import (
    14|     MacaroonDeserializationException,
    15|     MacaroonInvalidSignatureException,
    16| )
    17| from typing_extensions import TypedDict
    18| from twisted.web.client import readBody
    19| from synapse.config import ConfigError
    20| from synapse.http.server import respond_with_html
    21| from synapse.http.site import SynapseRequest
    22| from synapse.logging.context import make_deferred_yieldable
    23| from synapse.types import JsonDict, UserID, map_username_to_mxid_localpart
    24| from synapse.util import json_decoder
    25| if TYPE_CHECKING:
    26|     from synapse.server import HomeServer
    27| logger = logging.getLogger(__name__)
    28| SESSION_COOKIE_NAME = b"oidc_session"
    29| Token = TypedDict(
    30|     "Token",
    31|     {
    32|         "access_token": str,
    33|         "token_type": str,
    34|         "id_token": Optional[str],
    35|         "refresh_token": Optional[str],
    36|         "expires_in": int,
    37|         "scope": Optional[str],
    38|     },
    39| )
    40| JWK = Dict[str, str]
    41| JWKS = TypedDict("JWKS", {"keys": List[JWK]})
    42| class OidcError(Exception):
    43|     """Used to catch errors when calling the token_endpoint

# --- HUNK 2: Lines 60-116 ---
    60|         self._callback_url = hs.config.oidc_callback_url  # type: str
    61|         self._scopes = hs.config.oidc_scopes  # type: List[str]
    62|         self._client_auth = ClientAuth(
    63|             hs.config.oidc_client_id,
    64|             hs.config.oidc_client_secret,
    65|             hs.config.oidc_client_auth_method,
    66|         )  # type: ClientAuth
    67|         self._client_auth_method = hs.config.oidc_client_auth_method  # type: str
    68|         self._provider_metadata = OpenIDProviderMetadata(
    69|             issuer=hs.config.oidc_issuer,
    70|             authorization_endpoint=hs.config.oidc_authorization_endpoint,
    71|             token_endpoint=hs.config.oidc_token_endpoint,
    72|             userinfo_endpoint=hs.config.oidc_userinfo_endpoint,
    73|             jwks_uri=hs.config.oidc_jwks_uri,
    74|         )  # type: OpenIDProviderMetadata
    75|         self._provider_needs_discovery = hs.config.oidc_discover  # type: bool
    76|         self._user_mapping_provider = hs.config.oidc_user_mapping_provider_class(
    77|             hs.config.oidc_user_mapping_provider_config
    78|         )  # type: OidcMappingProvider
    79|         self._skip_verification = hs.config.oidc_skip_verification  # type: bool
    80|         self._allow_existing_users = hs.config.oidc_allow_existing_users  # type: bool
    81|         self._http_client = hs.get_proxied_http_client()
    82|         self._auth_handler = hs.get_auth_handler()
    83|         self._registration_handler = hs.get_registration_handler()
    84|         self._datastore = hs.get_datastore()
    85|         self._clock = hs.get_clock()
    86|         self._hostname = hs.hostname  # type: str
    87|         self._server_name = hs.config.server_name  # type: str
    88|         self._macaroon_secret_key = hs.config.macaroon_secret_key
    89|         self._error_template = hs.config.sso_error_template
    90|         self._auth_provider_id = "oidc"
    91|     def _render_error(
    92|         self, request, error: str, error_description: Optional[str] = None
    93|     ) -> None:
    94|         """Render the error template and respond to the request with it.
    95|         This is used to show errors to the user. The template of this page can
    96|         be found under `synapse/res/templates/sso_error.html`.
    97|         Args:
    98|             request: The incoming request from the browser.
    99|                 We'll respond with an HTML page describing the error.
   100|             error: A technical identifier for this error. Those include
   101|                 well-known OAuth2/OIDC error types like invalid_request or
   102|                 access_denied.
   103|             error_description: A human-readable description of the error.
   104|         """
   105|         html = self._error_template.render(
   106|             error=error, error_description=error_description
   107|         )
   108|         respond_with_html(request, 400, html)
   109|     def _validate_metadata(self):
   110|         """Verifies the provider metadata.
   111|         This checks the validity of the currently loaded provider. Not
   112|         everything is checked, only:
   113|           - ``issuer``
   114|           - ``authorization_endpoint``
   115|           - ``token_endpoint``
   116|           - ``response_types_supported`` (checks if "code" is in it)

# --- HUNK 3: Lines 498-550 ---
   498|         else:
   499|             logger.debug("Extracting userinfo from id_token")
   500|             try:
   501|                 userinfo = await self._parse_id_token(token, nonce=nonce)
   502|             except Exception as e:
   503|                 logger.exception("Invalid id_token")
   504|                 self._render_error(request, "invalid_token", str(e))
   505|                 return
   506|         user_agent = request.requestHeaders.getRawHeaders(b"User-Agent", default=[b""])[
   507|             0
   508|         ].decode("ascii", "surrogateescape")
   509|         ip_address = self.hs.get_ip_from_request(request)
   510|         try:
   511|             user_id = await self._map_userinfo_to_user(
   512|                 userinfo, token, user_agent, ip_address
   513|             )
   514|         except MappingException as e:
   515|             logger.exception("Could not map user")
   516|             self._render_error(request, "mapping_error", str(e))
   517|             return
   518|         extra_attributes = None
   519|         get_extra_attributes = getattr(
   520|             self._user_mapping_provider, "get_extra_attributes", None
   521|         )
   522|         if get_extra_attributes:
   523|             extra_attributes = await get_extra_attributes(userinfo, token)
   524|         if ui_auth_session_id:
   525|             await self._auth_handler.complete_sso_ui_auth(
   526|                 user_id, ui_auth_session_id, request
   527|             )
   528|         else:
   529|             await self._auth_handler.complete_sso_login(
   530|                 user_id, request, client_redirect_url, extra_attributes
   531|             )
   532|     def _generate_oidc_session_token(
   533|         self,
   534|         state: str,
   535|         nonce: str,
   536|         client_redirect_url: str,
   537|         ui_auth_session_id: Optional[str],
   538|         duration_in_ms: int = (60 * 60 * 1000),
   539|     ) -> str:
   540|         """Generates a signed token storing data about an OIDC session.
   541|         When Synapse initiates an authorization flow, it creates a random state
   542|         and a random nonce. Those parameters are given to the provider and
   543|         should be verified when the client comes back from the provider.
   544|         It is also used to store the client_redirect_url, which is used to
   545|         complete the SSO login flow.
   546|         Args:
   547|             state: The ``state`` parameter passed to the OIDC provider.
   548|             nonce: The ``nonce`` parameter passed to the OIDC provider.
   549|             client_redirect_url: The URL the client gave when it initiated the
   550|                 flow.

# --- HUNK 4: Lines 620-661 ---
   620|         for caveat in macaroon.caveats:
   621|             if caveat.caveat_id.startswith(prefix):
   622|                 return caveat.caveat_id[len(prefix) :]
   623|         raise ValueError("No %s caveat in macaroon" % (key,))
   624|     def _verify_expiry(self, caveat: str) -> bool:
   625|         prefix = "time < "
   626|         if not caveat.startswith(prefix):
   627|             return False
   628|         expiry = int(caveat[len(prefix) :])
   629|         now = self._clock.time_msec()
   630|         return now < expiry
   631|     async def _map_userinfo_to_user(
   632|         self, userinfo: UserInfo, token: Token, user_agent: str, ip_address: str
   633|     ) -> str:
   634|         """Maps a UserInfo object to a mxid.
   635|         UserInfo should have a claim that uniquely identifies users. This claim
   636|         is usually `sub`, but can be configured with `oidc_config.subject_claim`.
   637|         It is then used as an `external_id`.
   638|         If we don't find the user that way, we should register the user,
   639|         mapping the localpart and the display name from the UserInfo.
   640|         If a user already exists with the mxid we've mapped and allow_existing_users
   641|         is disabled, raise an exception.
   642|         Args:
   643|             userinfo: an object representing the user
   644|             token: a dict with the tokens obtained from the provider
   645|             user_agent: The user agent of the client making the request.
   646|             ip_address: The IP address of the client making the request.
   647|         Raises:
   648|             MappingException: if there was an error while mapping some properties
   649|         Returns:
   650|             The mxid of the user
   651|         """
   652|         try:
   653|             remote_user_id = self._user_mapping_provider.get_remote_user_id(userinfo)
   654|         except Exception as e:
   655|             raise MappingException(
   656|                 "Failed to extract subject from OIDC response: %s" % (e,)
   657|             )
   658|         remote_user_id = str(remote_user_id)
   659|         logger.info(
   660|             "Looking for existing mapping for user %s:%s",
   661|             self._auth_provider_id,

# --- HUNK 5: Lines 664-841 ---
   664|         registered_user_id = await self._datastore.get_user_by_external_id(
   665|             self._auth_provider_id, remote_user_id,
   666|         )
   667|         if registered_user_id is not None:
   668|             logger.info("Found existing mapping %s", registered_user_id)
   669|             return registered_user_id
   670|         try:
   671|             attributes = await self._user_mapping_provider.map_user_attributes(
   672|                 userinfo, token
   673|             )
   674|         except Exception as e:
   675|             raise MappingException(
   676|                 "Could not extract user attributes from OIDC response: " + str(e)
   677|             )
   678|         logger.debug(
   679|             "Retrieved user attributes from user mapping provider: %r", attributes
   680|         )
   681|         if not attributes["localpart"]:
   682|             raise MappingException("localpart is empty")
   683|         localpart = map_username_to_mxid_localpart(attributes["localpart"])
   684|         user_id = UserID(localpart, self._hostname).to_string()
   685|         users = await self._datastore.get_users_by_id_case_insensitive(user_id)
   686|         if users:
   687|             if self._allow_existing_users:
   688|                 if len(users) == 1:
   689|                     registered_user_id = next(iter(users))
   690|                 elif user_id in users:
   691|                     registered_user_id = user_id
   692|                 else:
   693|                     raise MappingException(
   694|                         "Attempted to login as '{}' but it matches more than one user inexactly: {}".format(
   695|                             user_id, list(users.keys())
   696|                         )
   697|                     )
   698|             else:
   699|                 raise MappingException("mxid '{}' is already taken".format(user_id))
   700|         else:
   701|             registered_user_id = await self._registration_handler.register_user(
   702|                 localpart=localpart,
   703|                 default_display_name=attributes["display_name"],
   704|                 user_agent_ips=(user_agent, ip_address),
   705|             )
   706|         await self._datastore.record_user_external_id(
   707|             self._auth_provider_id, remote_user_id, registered_user_id,
   708|         )
   709|         return registered_user_id
   710| UserAttribute = TypedDict(
   711|     "UserAttribute", {"localpart": str, "display_name": Optional[str]}
   712| )
   713| C = TypeVar("C")
   714| class OidcMappingProvider(Generic[C]):
   715|     """A mapping provider maps a UserInfo object to user attributes.
   716|     It should provide the API described by this class.
   717|     """
   718|     def __init__(self, config: C):
   719|         """
   720|         Args:
   721|             config: A custom config object from this module, parsed by ``parse_config()``
   722|         """
   723|     @staticmethod
   724|     def parse_config(config: dict) -> C:
   725|         """Parse the dict provided by the homeserver's config
   726|         Args:
   727|             config: A dictionary containing configuration options for this provider
   728|         Returns:
   729|             A custom config object for this module
   730|         """
   731|         raise NotImplementedError()
   732|     def get_remote_user_id(self, userinfo: UserInfo) -> str:
   733|         """Get a unique user ID for this user.
   734|         Usually, in an OIDC-compliant scenario, it should be the ``sub`` claim from the UserInfo object.
   735|         Args:
   736|             userinfo: An object representing the user given by the OIDC provider
   737|         Returns:
   738|             A unique user ID
   739|         """
   740|         raise NotImplementedError()
   741|     async def map_user_attributes(
   742|         self, userinfo: UserInfo, token: Token
   743|     ) -> UserAttribute:
   744|         """Map a `UserInfo` object into user attributes.
   745|         Args:
   746|             userinfo: An object representing the user given by the OIDC provider
   747|             token: A dict with the tokens returned by the provider
   748|         Returns:
   749|             A dict containing the ``localpart`` and (optionally) the ``display_name``
   750|         """
   751|         raise NotImplementedError()
   752|     async def get_extra_attributes(self, userinfo: UserInfo, token: Token) -> JsonDict:
   753|         """Map a `UserInfo` object into additional attributes passed to the client during login.
   754|         Args:
   755|             userinfo: An object representing the user given by the OIDC provider
   756|             token: A dict with the tokens returned by the provider
   757|         Returns:
   758|             A dict containing additional attributes. Must be JSON serializable.
   759|         """
   760|         return {}
   761| def jinja_finalize(thing):
   762|     return thing if thing is not None else ""
   763| env = Environment(finalize=jinja_finalize)
   764| @attr.s
   765| class JinjaOidcMappingConfig:
   766|     subject_claim = attr.ib()  # type: str
   767|     localpart_template = attr.ib()  # type: Template
   768|     display_name_template = attr.ib()  # type: Optional[Template]
   769|     extra_attributes = attr.ib()  # type: Dict[str, Template]
   770| class JinjaOidcMappingProvider(OidcMappingProvider[JinjaOidcMappingConfig]):
   771|     """An implementation of a mapping provider based on Jinja templates.
   772|     This is the default mapping provider.
   773|     """
   774|     def __init__(self, config: JinjaOidcMappingConfig):
   775|         self._config = config
   776|     @staticmethod
   777|     def parse_config(config: dict) -> JinjaOidcMappingConfig:
   778|         subject_claim = config.get("subject_claim", "sub")
   779|         if "localpart_template" not in config:
   780|             raise ConfigError(
   781|                 "missing key: oidc_config.user_mapping_provider.config.localpart_template"
   782|             )
   783|         try:
   784|             localpart_template = env.from_string(config["localpart_template"])
   785|         except Exception as e:
   786|             raise ConfigError(
   787|                 "invalid jinja template for oidc_config.user_mapping_provider.config.localpart_template: %r"
   788|                 % (e,)
   789|             )
   790|         display_name_template = None  # type: Optional[Template]
   791|         if "display_name_template" in config:
   792|             try:
   793|                 display_name_template = env.from_string(config["display_name_template"])
   794|             except Exception as e:
   795|                 raise ConfigError(
   796|                     "invalid jinja template for oidc_config.user_mapping_provider.config.display_name_template: %r"
   797|                     % (e,)
   798|                 )
   799|         extra_attributes = {}  # type Dict[str, Template]
   800|         if "extra_attributes" in config:
   801|             extra_attributes_config = config.get("extra_attributes") or {}
   802|             if not isinstance(extra_attributes_config, dict):
   803|                 raise ConfigError(
   804|                     "oidc_config.user_mapping_provider.config.extra_attributes must be a dict"
   805|                 )
   806|             for key, value in extra_attributes_config.items():
   807|                 try:
   808|                     extra_attributes[key] = env.from_string(value)
   809|                 except Exception as e:
   810|                     raise ConfigError(
   811|                         "invalid jinja template for oidc_config.user_mapping_provider.config.extra_attributes.%s: %r"
   812|                         % (key, e)
   813|                     )
   814|         return JinjaOidcMappingConfig(
   815|             subject_claim=subject_claim,
   816|             localpart_template=localpart_template,
   817|             display_name_template=display_name_template,
   818|             extra_attributes=extra_attributes,
   819|         )
   820|     def get_remote_user_id(self, userinfo: UserInfo) -> str:
   821|         return userinfo[self._config.subject_claim]
   822|     async def map_user_attributes(
   823|         self, userinfo: UserInfo, token: Token
   824|     ) -> UserAttribute:
   825|         localpart = self._config.localpart_template.render(user=userinfo).strip()
   826|         display_name = None  # type: Optional[str]
   827|         if self._config.display_name_template is not None:
   828|             display_name = self._config.display_name_template.render(
   829|                 user=userinfo
   830|             ).strip()
   831|             if display_name == "":
   832|                 display_name = None
   833|         return UserAttribute(localpart=localpart, display_name=display_name)
   834|     async def get_extra_attributes(self, userinfo: UserInfo, token: Token) -> JsonDict:
   835|         extras = {}  # type: Dict[str, str]
   836|         for key, template in self._config.extra_attributes.items():
   837|             try:
   838|                 extras[key] = template.render(user=userinfo).strip()
   839|             except Exception as e:
   840|                 logger.error("Failed to render OIDC extra attribute %s: %s" % (key, e))
   841|         return extras


# ====================================================================
# FILE: synapse/handlers/pagination.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| import logging
     2| from typing import TYPE_CHECKING, Any, Dict, Optional, Set
     3| from twisted.python.failure import Failure
     4| from synapse.api.constants import EventTypes, Membership
     5| from synapse.api.errors import SynapseError
     6| from synapse.api.filtering import Filter
     7| from synapse.logging.context import run_in_background
     8| from synapse.metrics.background_process_metrics import run_as_background_process
     9| from synapse.storage.state import StateFilter
    10| from synapse.streams.config import PaginationConfig
    11| from synapse.types import Requester
    12| from synapse.util.async_helpers import ReadWriteLock
    13| from synapse.util.stringutils import random_string
    14| from synapse.visibility import filter_events_for_client
    15| if TYPE_CHECKING:
    16|     from synapse.server import HomeServer
    17| logger = logging.getLogger(__name__)
    18| class PurgeStatus:
    19|     """Object tracking the status of a purge request
    20|     This class contains information on the progress of a purge request, for
    21|     return by get_purge_status.
    22|     Attributes:
    23|         status (int): Tracks whether this request has completed. One of
    24|             STATUS_{ACTIVE,COMPLETE,FAILED}
    25|     """
    26|     STATUS_ACTIVE = 0
    27|     STATUS_COMPLETE = 1
    28|     STATUS_FAILED = 2
    29|     STATUS_TEXT = {
    30|         STATUS_ACTIVE: "active",
    31|         STATUS_COMPLETE: "complete",

# --- HUNK 2: Lines 208-310 ---
   208|     async def get_messages(
   209|         self,
   210|         requester: Requester,
   211|         room_id: str,
   212|         pagin_config: PaginationConfig,
   213|         as_client_event: bool = True,
   214|         event_filter: Optional[Filter] = None,
   215|     ) -> Dict[str, Any]:
   216|         """Get messages in a room.
   217|         Args:
   218|             requester: The user requesting messages.
   219|             room_id: The room they want messages from.
   220|             pagin_config: The pagination config rules to apply, if any.
   221|             as_client_event: True to get events in client-server format.
   222|             event_filter: Filter to apply to results or None
   223|         Returns:
   224|             Pagination API results
   225|         """
   226|         user_id = requester.user.to_string()
   227|         if pagin_config.from_token:
   228|             from_token = pagin_config.from_token
   229|         else:
   230|             from_token = self.hs.get_event_sources().get_current_token_for_pagination()
   231|         if pagin_config.limit is None:
   232|             raise Exception("limit not set")
   233|         room_token = from_token.room_key
   234|         with await self.pagination_lock.read(room_id):
   235|             (
   236|                 membership,
   237|                 member_event_id,
   238|             ) = await self.auth.check_user_in_room_or_world_readable(
   239|                 room_id, user_id, allow_departed_users=True
   240|             )
   241|             if pagin_config.direction == "b":
   242|                 if room_token.topological:
   243|                     curr_topo = room_token.topological
   244|                 else:
   245|                     curr_topo = await self.store.get_current_topological_token(
   246|                         room_id, room_token.stream
   247|                     )
   248|                 if membership == Membership.LEAVE:
   249|                     assert member_event_id
   250|                     leave_token = await self.store.get_topological_token_for_event(
   251|                         member_event_id
   252|                     )
   253|                     assert leave_token.topological is not None
   254|                     if leave_token.topological < curr_topo:
   255|                         from_token = from_token.copy_and_replace(
   256|                             "room_key", leave_token
   257|                         )
   258|                 await self.hs.get_handlers().federation_handler.maybe_backfill(
   259|                     room_id, curr_topo, limit=pagin_config.limit,
   260|                 )
   261|             to_room_key = None
   262|             if pagin_config.to_token:
   263|                 to_room_key = pagin_config.to_token.room_key
   264|             events, next_key = await self.store.paginate_room_events(
   265|                 room_id=room_id,
   266|                 from_key=from_token.room_key,
   267|                 to_key=to_room_key,
   268|                 direction=pagin_config.direction,
   269|                 limit=pagin_config.limit,
   270|                 event_filter=event_filter,
   271|             )
   272|             next_token = from_token.copy_and_replace("room_key", next_key)
   273|         if events:
   274|             if event_filter:
   275|                 events = event_filter.filter(events)
   276|             events = await filter_events_for_client(
   277|                 self.storage, user_id, events, is_peeking=(member_event_id is None)
   278|             )
   279|         if not events:
   280|             return {
   281|                 "chunk": [],
   282|                 "start": await from_token.to_string(self.store),
   283|                 "end": await next_token.to_string(self.store),
   284|             }
   285|         state = None
   286|         if event_filter and event_filter.lazy_load_members() and len(events) > 0:
   287|             state_filter = StateFilter.from_types(
   288|                 (EventTypes.Member, event.sender) for event in events
   289|             )
   290|             state_ids = await self.state_store.get_state_ids_for_event(
   291|                 events[0].event_id, state_filter=state_filter
   292|             )
   293|             if state_ids:
   294|                 state_dict = await self.store.get_events(list(state_ids.values()))
   295|                 state = state_dict.values()
   296|         time_now = self.clock.time_msec()
   297|         chunk = {
   298|             "chunk": (
   299|                 await self._event_serializer.serialize_events(
   300|                     events, time_now, as_client_event=as_client_event
   301|                 )
   302|             ),
   303|             "start": await from_token.to_string(self.store),
   304|             "end": await next_token.to_string(self.store),
   305|         }
   306|         if state:
   307|             chunk["state"] = await self._event_serializer.serialize_events(
   308|                 state, time_now, as_client_event=as_client_event
   309|             )
   310|         return chunk


# ====================================================================
# FILE: synapse/handlers/presence.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 755-794 ---
   755|                     if other_user_id in users_interested_in:
   756|                         user_ids_changed.add(other_user_id)
   757|             else:
   758|                 get_updates_counter.labels("full").inc()
   759|                 if from_key:
   760|                     user_ids_changed = stream_change_cache.get_entities_changed(
   761|                         users_interested_in, from_key
   762|                     )
   763|                 else:
   764|                     user_ids_changed = users_interested_in
   765|             updates = await presence.current_state_for_users(user_ids_changed)
   766|         if include_offline:
   767|             return (list(updates.values()), max_token)
   768|         else:
   769|             return (
   770|                 [s for s in updates.values() if s.state != PresenceState.OFFLINE],
   771|                 max_token,
   772|             )
   773|     def get_current_key(self):
   774|         return self.store.get_current_presence_token()
   775|     @cached(num_args=2, cache_context=True)
   776|     async def _get_interested_in(self, user, explicit_room_id, cache_context):
   777|         """Returns the set of users that the given user should see presence
   778|         updates for
   779|         """
   780|         user_id = user.to_string()
   781|         users_interested_in = set()
   782|         users_interested_in.add(user_id)  # So that we receive our own presence
   783|         users_who_share_room = await self.store.get_users_who_share_room_with_user(
   784|             user_id, on_invalidate=cache_context.invalidate
   785|         )
   786|         users_interested_in.update(users_who_share_room)
   787|         if explicit_room_id:
   788|             user_ids = await self.store.get_users_in_room(
   789|                 explicit_room_id, on_invalidate=cache_context.invalidate
   790|             )
   791|             users_interested_in.update(user_ids)
   792|         return users_interested_in
   793| def handle_timeouts(user_states, is_mine_fn, syncing_user_ids, now):
   794|     """Checks the presence of users that have timed out and updates as


# ====================================================================
# FILE: synapse/handlers/profile.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 4-44 ---
     4|     AuthError,
     5|     Codes,
     6|     HttpResponseException,
     7|     RequestSendFailed,
     8|     StoreError,
     9|     SynapseError,
    10| )
    11| from synapse.metrics.background_process_metrics import run_as_background_process
    12| from synapse.types import UserID, create_requester, get_domain_from_id
    13| from ._base import BaseHandler
    14| logger = logging.getLogger(__name__)
    15| MAX_DISPLAYNAME_LEN = 256
    16| MAX_AVATAR_URL_LEN = 1000
    17| class BaseProfileHandler(BaseHandler):
    18|     """Handles fetching and updating user profile information.
    19|     BaseProfileHandler can be instantiated directly on workers and will
    20|     delegate to master when necessary. The master process should use the
    21|     subclass MasterProfileHandler
    22|     """
    23|     def __init__(self, hs):
    24|         super().__init__(hs)
    25|         self.federation = hs.get_federation_client()
    26|         hs.get_federation_registry().register_query_handler(
    27|             "profile", self.on_profile_query
    28|         )
    29|         self.user_directory_handler = hs.get_user_directory_handler()
    30|     async def get_profile(self, user_id):
    31|         target_user = UserID.from_string(user_id)
    32|         if self.hs.is_mine(target_user):
    33|             try:
    34|                 displayname = await self.store.get_profile_displayname(
    35|                     target_user.localpart
    36|                 )
    37|                 avatar_url = await self.store.get_profile_avatar_url(
    38|                     target_user.localpart
    39|                 )
    40|             except StoreError as e:
    41|                 if e.code == 404:
    42|                     raise SynapseError(404, "Profile was not found", Codes.NOT_FOUND)
    43|                 raise
    44|             return {"displayname": displayname, "avatar_url": avatar_url}

# --- HUNK 2: Lines 259-299 ---
   259|             or not requester
   260|         ):
   261|             return
   262|         if target_user.to_string() == requester.to_string():
   263|             return
   264|         try:
   265|             requester_rooms = await self.store.get_rooms_for_user(requester.to_string())
   266|             target_user_rooms = await self.store.get_rooms_for_user(
   267|                 target_user.to_string()
   268|             )
   269|             if requester_rooms.isdisjoint(target_user_rooms):
   270|                 raise SynapseError(403, "Profile isn't available", Codes.FORBIDDEN)
   271|         except StoreError as e:
   272|             if e.code == 404:
   273|                 raise SynapseError(403, "Profile isn't available", Codes.FORBIDDEN)
   274|             raise
   275| class MasterProfileHandler(BaseProfileHandler):
   276|     PROFILE_UPDATE_MS = 60 * 1000
   277|     PROFILE_UPDATE_EVERY_MS = 24 * 60 * 60 * 1000
   278|     def __init__(self, hs):
   279|         super().__init__(hs)
   280|         assert hs.config.worker_app is None
   281|         self.clock.looping_call(
   282|             self._start_update_remote_profile_cache, self.PROFILE_UPDATE_MS
   283|         )
   284|     def _start_update_remote_profile_cache(self):
   285|         return run_as_background_process(
   286|             "Update remote profile", self._update_remote_profile_cache
   287|         )
   288|     async def _update_remote_profile_cache(self):
   289|         """Called periodically to check profiles of remote users we haven't
   290|         checked in a while.
   291|         """
   292|         entries = await self.store.get_remote_profile_cache_entries_that_expire(
   293|             last_checked=self.clock.time_msec() - self.PROFILE_UPDATE_EVERY_MS
   294|         )
   295|         for user_id, displayname, avatar_url in entries:
   296|             is_subscribed = await self.store.is_subscribed_remote_profile_for_user(
   297|                 user_id
   298|             )
   299|             if not is_subscribed:


# ====================================================================
# FILE: synapse/handlers/read_marker.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-27 ---
     1| import logging
     2| from synapse.util.async_helpers import Linearizer
     3| from ._base import BaseHandler
     4| logger = logging.getLogger(__name__)
     5| class ReadMarkerHandler(BaseHandler):
     6|     def __init__(self, hs):
     7|         super().__init__(hs)
     8|         self.server_name = hs.config.server_name
     9|         self.store = hs.get_datastore()
    10|         self.read_marker_linearizer = Linearizer(name="read_marker")
    11|         self.notifier = hs.get_notifier()
    12|     async def received_client_read_marker(self, room_id, user_id, event_id):
    13|         """Updates the read marker for a given user in a given room if the event ID given
    14|         is ahead in the stream relative to the current read marker.
    15|         This uses a notifier to indicate that account data should be sent down /sync if
    16|         the read marker has changed.
    17|         """
    18|         with await self.read_marker_linearizer.queue((room_id, user_id)):
    19|             existing_read_marker = await self.store.get_account_data_for_room_and_type(
    20|                 user_id, room_id, "m.fully_read"
    21|             )
    22|             should_update = True
    23|             if existing_read_marker:
    24|                 should_update = await self.store.is_event_after(
    25|                     event_id, existing_read_marker["event_id"]
    26|                 )
    27|             if should_update:


# ====================================================================
# FILE: synapse/handlers/receipts.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| import logging
     2| from synapse.handlers._base import BaseHandler
     3| from synapse.types import ReadReceipt, get_domain_from_id
     4| from synapse.util.async_helpers import maybe_awaitable
     5| logger = logging.getLogger(__name__)
     6| class ReceiptsHandler(BaseHandler):
     7|     def __init__(self, hs):
     8|         super().__init__(hs)
     9|         self.server_name = hs.config.server_name
    10|         self.store = hs.get_datastore()
    11|         self.hs = hs
    12|         self.federation = hs.get_federation_sender()
    13|         hs.get_federation_registry().register_edu_handler(
    14|             "m.receipt", self._received_remote_receipt
    15|         )
    16|         self.clock = self.hs.get_clock()
    17|         self.state = hs.get_state_handler()
    18|     async def _received_remote_receipt(self, origin, content):
    19|         """Called when we receive an EDU of type m.receipt from a remote HS.
    20|         """
    21|         receipts = []
    22|         for room_id, room_values in content.items():
    23|             for receipt_type, users in room_values.items():
    24|                 for user_id, user_values in users.items():
    25|                     if get_domain_from_id(user_id) != origin:
    26|                         logger.info(
    27|                             "Received receipt for user %r from server %s, ignoring",
    28|                             user_id,

# --- HUNK 2: Lines 81-100 ---
    81|             data={"ts": int(self.clock.time_msec())},
    82|         )
    83|         is_new = await self._handle_new_receipts([receipt])
    84|         if not is_new:
    85|             return
    86|         await self.federation.send_read_receipt(receipt)
    87| class ReceiptEventSource:
    88|     def __init__(self, hs):
    89|         self.store = hs.get_datastore()
    90|     async def get_new_events(self, from_key, room_ids, **kwargs):
    91|         from_key = int(from_key)
    92|         to_key = self.get_current_key()
    93|         if from_key == to_key:
    94|             return [], to_key
    95|         events = await self.store.get_linearized_receipts_for_rooms(
    96|             room_ids, from_key=from_key, to_key=to_key
    97|         )
    98|         return (events, to_key)
    99|     def get_current_key(self, direction="f"):
   100|         return self.store.get_max_receipt_stream_id()


# ====================================================================
# FILE: synapse/handlers/register.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 4-44 ---
     4| from synapse.api.constants import MAX_USERID_LENGTH, EventTypes, JoinRules, LoginType
     5| from synapse.api.errors import AuthError, Codes, ConsentNotGivenError, SynapseError
     6| from synapse.config.server import is_threepid_reserved
     7| from synapse.http.servlet import assert_params_in_dict
     8| from synapse.replication.http.login import RegisterDeviceReplicationServlet
     9| from synapse.replication.http.register import (
    10|     ReplicationPostRegisterActionsServlet,
    11|     ReplicationRegisterServlet,
    12| )
    13| from synapse.spam_checker_api import RegistrationBehaviour
    14| from synapse.storage.state import StateFilter
    15| from synapse.types import RoomAlias, UserID, create_requester
    16| from ._base import BaseHandler
    17| logger = logging.getLogger(__name__)
    18| class RegistrationHandler(BaseHandler):
    19|     def __init__(self, hs):
    20|         """
    21|         Args:
    22|             hs (synapse.server.HomeServer):
    23|         """
    24|         super().__init__(hs)
    25|         self.hs = hs
    26|         self.auth = hs.get_auth()
    27|         self._auth_handler = hs.get_auth_handler()
    28|         self.profile_handler = hs.get_profile_handler()
    29|         self.user_directory_handler = hs.get_user_directory_handler()
    30|         self.identity_handler = self.hs.get_handlers().identity_handler
    31|         self.ratelimiter = hs.get_registration_ratelimiter()
    32|         self.macaroon_gen = hs.get_macaroon_generator()
    33|         self._server_notices_mxid = hs.config.server_notices_mxid
    34|         self.spam_checker = hs.get_spam_checker()
    35|         if hs.config.worker_app:
    36|             self._register_client = ReplicationRegisterServlet.make_client(hs)
    37|             self._register_device_client = RegisterDeviceReplicationServlet.make_client(
    38|                 hs
    39|             )
    40|             self._post_registration_client = ReplicationPostRegisterActionsServlet.make_client(
    41|                 hs
    42|             )
    43|         else:
    44|             self.device_handler = hs.get_device_handler()


# ====================================================================
# FILE: synapse/handlers/room.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 27-67 ---
    27|     RoomAlias,
    28|     RoomID,
    29|     RoomStreamToken,
    30|     StateMap,
    31|     StreamToken,
    32|     UserID,
    33|     create_requester,
    34| )
    35| from synapse.util import stringutils
    36| from synapse.util.async_helpers import Linearizer
    37| from synapse.util.caches.response_cache import ResponseCache
    38| from synapse.visibility import filter_events_for_client
    39| from ._base import BaseHandler
    40| if TYPE_CHECKING:
    41|     from synapse.server import HomeServer
    42| logger = logging.getLogger(__name__)
    43| id_server_scheme = "https://"
    44| FIVE_MINUTES_IN_MS = 5 * 60 * 1000
    45| class RoomCreationHandler(BaseHandler):
    46|     def __init__(self, hs: "HomeServer"):
    47|         super().__init__(hs)
    48|         self.spam_checker = hs.get_spam_checker()
    49|         self.event_creation_handler = hs.get_event_creation_handler()
    50|         self.room_member_handler = hs.get_room_member_handler()
    51|         self.config = hs.config
    52|         self._presets_dict = {
    53|             RoomCreationPreset.PRIVATE_CHAT: {
    54|                 "join_rules": JoinRules.INVITE,
    55|                 "history_visibility": "shared",
    56|                 "original_invitees_have_ops": False,
    57|                 "guest_can_join": True,
    58|                 "power_level_content_override": {"invite": 0},
    59|             },
    60|             RoomCreationPreset.TRUSTED_PRIVATE_CHAT: {
    61|                 "join_rules": JoinRules.INVITE,
    62|                 "history_visibility": "shared",
    63|                 "original_invitees_have_ops": True,
    64|                 "guest_can_join": True,
    65|                 "power_level_content_override": {"invite": 0},
    66|             },
    67|             RoomCreationPreset.PUBLIC_CHAT: {

# --- HUNK 2: Lines 578-620 ---
   578|             )
   579|         for invite_3pid in invite_3pid_list:
   580|             id_server = invite_3pid["id_server"]
   581|             id_access_token = invite_3pid.get("id_access_token")  # optional
   582|             address = invite_3pid["address"]
   583|             medium = invite_3pid["medium"]
   584|             last_stream_id = await self.hs.get_room_member_handler().do_3pid_invite(
   585|                 room_id,
   586|                 requester.user,
   587|                 medium,
   588|                 address,
   589|                 id_server,
   590|                 requester,
   591|                 txn_id=None,
   592|                 id_access_token=id_access_token,
   593|             )
   594|         result = {"room_id": room_id}
   595|         if room_alias:
   596|             result["room_alias"] = room_alias.to_string()
   597|         await self._replication.wait_for_stream_position(
   598|             self.hs.config.worker.events_shard_config.get_instance(room_id),
   599|             "events",
   600|             last_stream_id,
   601|         )
   602|         return result, last_stream_id
   603|     async def _send_events_for_new_room(
   604|         self,
   605|         creator: Requester,
   606|         room_id: str,
   607|         preset_config: str,
   608|         invite_list: List[str],
   609|         initial_state: MutableStateMap,
   610|         creation_content: JsonDict,
   611|         room_alias: Optional[RoomAlias] = None,
   612|         power_level_content_override: Optional[JsonDict] = None,
   613|         creator_join_profile: Optional[JsonDict] = None,
   614|     ) -> int:
   615|         """Sends the initial events into a new room.
   616|         `power_level_content_override` doesn't apply when initial state has
   617|         power level state event content.
   618|         Returns:
   619|             The stream_id of the last event persisted.
   620|         """

# --- HUNK 3: Lines 790-878 ---
   790|             last_event_id = event_id
   791|         if event_filter and event_filter.lazy_load_members():
   792|             state_filter = StateFilter.from_lazy_load_member_list(
   793|                 ev.sender
   794|                 for ev in itertools.chain(
   795|                     results["events_before"],
   796|                     (results["event"],),
   797|                     results["events_after"],
   798|                 )
   799|             )
   800|         else:
   801|             state_filter = StateFilter.all()
   802|         state = await self.state_store.get_state_for_events(
   803|             [last_event_id], state_filter=state_filter
   804|         )
   805|         state_events = list(state[last_event_id].values())
   806|         if event_filter:
   807|             state_events = event_filter.filter(state_events)
   808|         results["state"] = await filter_evts(state_events)
   809|         token = StreamToken.START
   810|         results["start"] = await token.copy_and_replace(
   811|             "room_key", results["start"]
   812|         ).to_string(self.store)
   813|         results["end"] = await token.copy_and_replace(
   814|             "room_key", results["end"]
   815|         ).to_string(self.store)
   816|         return results
   817| class RoomEventSource:
   818|     def __init__(self, hs: "HomeServer"):
   819|         self.store = hs.get_datastore()
   820|     async def get_new_events(
   821|         self,
   822|         user: UserID,
   823|         from_key: RoomStreamToken,
   824|         limit: int,
   825|         room_ids: List[str],
   826|         is_guest: bool,
   827|         explicit_room_id: Optional[str] = None,
   828|     ) -> Tuple[List[EventBase], RoomStreamToken]:
   829|         to_key = self.get_current_key()
   830|         if from_key.topological:
   831|             logger.warning("Stream has topological part!!!! %r", from_key)
   832|             from_key = RoomStreamToken(None, from_key.stream)
   833|         app_service = self.store.get_app_service_by_user_id(user.to_string())
   834|         if app_service:
   835|             raise NotImplementedError()
   836|         else:
   837|             room_events = await self.store.get_membership_changes_for_user(
   838|                 user.to_string(), from_key, to_key
   839|             )
   840|             room_to_events = await self.store.get_room_events_stream_for_rooms(
   841|                 room_ids=room_ids,
   842|                 from_key=from_key,
   843|                 to_key=to_key,
   844|                 limit=limit or 10,
   845|                 order="ASC",
   846|             )
   847|             events = list(room_events)
   848|             events.extend(e for evs, _ in room_to_events.values() for e in evs)
   849|             events.sort(key=lambda e: e.internal_metadata.order)
   850|             if limit:
   851|                 events[:] = events[:limit]
   852|             if events:
   853|                 end_key = events[-1].internal_metadata.after
   854|             else:
   855|                 end_key = to_key
   856|         return (events, end_key)
   857|     def get_current_key(self) -> RoomStreamToken:
   858|         return self.store.get_room_max_token()
   859|     def get_current_key_for_room(self, room_id: str) -> Awaitable[str]:
   860|         return self.store.get_room_events_max_id(room_id)
   861| class RoomShutdownHandler:
   862|     DEFAULT_MESSAGE = (
   863|         "Sharing illegal content on this server is not permitted and rooms in"
   864|         " violation will be blocked."
   865|     )
   866|     DEFAULT_ROOM_NAME = "Content Violation Notification"
   867|     def __init__(self, hs: "HomeServer"):
   868|         self.hs = hs
   869|         self.room_member_handler = hs.get_room_member_handler()
   870|         self._room_creation_handler = hs.get_room_creation_handler()
   871|         self._replication = hs.get_replication_data_handler()
   872|         self.event_creation_handler = hs.get_event_creation_handler()
   873|         self.state = hs.get_state_handler()
   874|         self.store = hs.get_datastore()
   875|     async def shutdown_room(
   876|         self,
   877|         room_id: str,
   878|         requester_user_id: str,

# --- HUNK 4: Lines 935-1003 ---
   935|         if new_room_user_id is not None:
   936|             if not self.hs.is_mine_id(new_room_user_id):
   937|                 raise SynapseError(
   938|                     400, "User must be our own: %s" % (new_room_user_id,)
   939|                 )
   940|             room_creator_requester = create_requester(new_room_user_id)
   941|             info, stream_id = await self._room_creation_handler.create_room(
   942|                 room_creator_requester,
   943|                 config={
   944|                     "preset": RoomCreationPreset.PUBLIC_CHAT,
   945|                     "name": new_room_name,
   946|                     "power_level_content_override": {"users_default": -10},
   947|                 },
   948|                 ratelimit=False,
   949|             )
   950|             new_room_id = info["room_id"]
   951|             logger.info(
   952|                 "Shutting down room %r, joining to new room: %r", room_id, new_room_id
   953|             )
   954|             await self._replication.wait_for_stream_position(
   955|                 self.hs.config.worker.events_shard_config.get_instance(new_room_id),
   956|                 "events",
   957|                 stream_id,
   958|             )
   959|         else:
   960|             new_room_id = None
   961|             logger.info("Shutting down room %r", room_id)
   962|         users = await self.state.get_current_users_in_room(room_id)
   963|         kicked_users = []
   964|         failed_to_kick_users = []
   965|         for user_id in users:
   966|             if not self.hs.is_mine_id(user_id):
   967|                 continue
   968|             logger.info("Kicking %r from %r...", user_id, room_id)
   969|             try:
   970|                 target_requester = create_requester(user_id)
   971|                 _, stream_id = await self.room_member_handler.update_membership(
   972|                     requester=target_requester,
   973|                     target=target_requester.user,
   974|                     room_id=room_id,
   975|                     action=Membership.LEAVE,
   976|                     content={},
   977|                     ratelimit=False,
   978|                     require_consent=False,
   979|                 )
   980|                 await self._replication.wait_for_stream_position(
   981|                     self.hs.config.worker.events_shard_config.get_instance(room_id),
   982|                     "events",
   983|                     stream_id,
   984|                 )
   985|                 await self.room_member_handler.forget(target_requester.user, room_id)
   986|                 if new_room_user_id:
   987|                     await self.room_member_handler.update_membership(
   988|                         requester=target_requester,
   989|                         target=target_requester.user,
   990|                         room_id=new_room_id,
   991|                         action=Membership.JOIN,
   992|                         content={},
   993|                         ratelimit=False,
   994|                         require_consent=False,
   995|                     )
   996|                 kicked_users.append(user_id)
   997|             except Exception:
   998|                 logger.exception(
   999|                     "Failed to leave old room and join new room for %r", user_id
  1000|                 )
  1001|                 failed_to_kick_users.append(user_id)
  1002|         if new_room_user_id:
  1003|             await self.event_creation_handler.create_and_send_nonmember_event(


# ====================================================================
# FILE: synapse/handlers/room_list.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| import logging
     2| from collections import namedtuple
     3| from typing import Any, Dict, Optional
     4| import msgpack
     5| from unpaddedbase64 import decode_base64, encode_base64
     6| from synapse.api.constants import EventTypes, JoinRules
     7| from synapse.api.errors import Codes, HttpResponseException
     8| from synapse.types import ThirdPartyInstanceID
     9| from synapse.util.caches.descriptors import cached
    10| from synapse.util.caches.response_cache import ResponseCache
    11| from ._base import BaseHandler
    12| logger = logging.getLogger(__name__)
    13| REMOTE_ROOM_LIST_POLL_INTERVAL = 60 * 1000
    14| EMPTY_THIRD_PARTY_ID = ThirdPartyInstanceID(None, None)
    15| class RoomListHandler(BaseHandler):
    16|     def __init__(self, hs):
    17|         super().__init__(hs)
    18|         self.enable_room_list_search = hs.config.enable_room_list_search
    19|         self.response_cache = ResponseCache(hs, "room_list")
    20|         self.remote_response_cache = ResponseCache(
    21|             hs, "remote_room_list", timeout_ms=30 * 1000
    22|         )
    23|     async def get_local_public_room_list(
    24|         self,
    25|         limit=None,
    26|         since_token=None,
    27|         search_filter=None,
    28|         network_tuple=EMPTY_THIRD_PARTY_ID,
    29|         from_federation=False,
    30|     ):
    31|         """Generate a local public room list.
    32|         There are multiple different lists: the main one plus one per third
    33|         party network. A client can ask for a specific list or to return all.
    34|         Args:
    35|             limit (int|None)
    36|             since_token (str|None)
    37|             search_filter (dict|None)


# ====================================================================
# FILE: synapse/handlers/room_member.py
# Total hunks: 7
# ====================================================================
# --- HUNK 1: Lines 6-70 ---
     6| from unpaddedbase64 import encode_base64
     7| from synapse import types
     8| from synapse.api.constants import MAX_DEPTH, EventTypes, Membership
     9| from synapse.api.errors import (
    10|     AuthError,
    11|     Codes,
    12|     LimitExceededError,
    13|     ShadowBanError,
    14|     SynapseError,
    15| )
    16| from synapse.api.ratelimiting import Ratelimiter
    17| from synapse.api.room_versions import EventFormatVersions
    18| from synapse.crypto.event_signing import compute_event_reference_hash
    19| from synapse.events import EventBase
    20| from synapse.events.builder import create_local_event_from_event_dict
    21| from synapse.events.snapshot import EventContext
    22| from synapse.events.validator import EventValidator
    23| from synapse.storage.roommember import RoomsForUser
    24| from synapse.types import JsonDict, Requester, RoomAlias, RoomID, StateMap, UserID
    25| from synapse.util.async_helpers import Linearizer
    26| from synapse.util.distributor import user_left_room
    27| from ._base import BaseHandler
    28| if TYPE_CHECKING:
    29|     from synapse.server import HomeServer
    30| logger = logging.getLogger(__name__)
    31| class RoomMemberHandler(metaclass=abc.ABCMeta):
    32|     def __init__(self, hs: "HomeServer"):
    33|         self.hs = hs
    34|         self.store = hs.get_datastore()
    35|         self.auth = hs.get_auth()
    36|         self.state_handler = hs.get_state_handler()
    37|         self.config = hs.config
    38|         self.federation_handler = hs.get_handlers().federation_handler
    39|         self.directory_handler = hs.get_handlers().directory_handler
    40|         self.identity_handler = hs.get_handlers().identity_handler
    41|         self.registration_handler = hs.get_registration_handler()
    42|         self.profile_handler = hs.get_profile_handler()
    43|         self.event_creation_handler = hs.get_event_creation_handler()
    44|         self.member_linearizer = Linearizer(name="member")
    45|         self.clock = hs.get_clock()
    46|         self.spam_checker = hs.get_spam_checker()
    47|         self.third_party_event_rules = hs.get_third_party_event_rules()
    48|         self._server_notices_mxid = self.config.server_notices_mxid
    49|         self._enable_lookup = hs.config.enable_3pid_lookup
    50|         self.allow_per_room_profiles = self.config.allow_per_room_profiles
    51|         self._join_rate_limiter_local = Ratelimiter(
    52|             clock=self.clock,
    53|             rate_hz=hs.config.ratelimiting.rc_joins_local.per_second,
    54|             burst_count=hs.config.ratelimiting.rc_joins_local.burst_count,
    55|         )
    56|         self._join_rate_limiter_remote = Ratelimiter(
    57|             clock=self.clock,
    58|             rate_hz=hs.config.ratelimiting.rc_joins_remote.per_second,
    59|             burst_count=hs.config.ratelimiting.rc_joins_remote.burst_count,
    60|         )
    61|         self.base_handler = BaseHandler(hs)
    62|     @abc.abstractmethod
    63|     async def _remote_join(
    64|         self,
    65|         requester: Requester,
    66|         remote_room_hosts: List[str],
    67|         room_id: str,
    68|         user: UserID,
    69|         content: dict,
    70|     ) -> Tuple[str, int]:

# --- HUNK 2: Lines 78-117 ---
    78|         """
    79|         raise NotImplementedError()
    80|     @abc.abstractmethod
    81|     async def remote_reject_invite(
    82|         self,
    83|         invite_event_id: str,
    84|         txn_id: Optional[str],
    85|         requester: Requester,
    86|         content: JsonDict,
    87|     ) -> Tuple[str, int]:
    88|         """
    89|         Rejects an out-of-band invite we have received from a remote server
    90|         Args:
    91|             invite_event_id: ID of the invite to be rejected
    92|             txn_id: optional transaction ID supplied by the client
    93|             requester: user making the rejection request, according to the access token
    94|             content: additional content to include in the rejection event.
    95|                Normally an empty dict.
    96|         Returns:
    97|             event id, stream_id of the leave event
    98|         """
    99|         raise NotImplementedError()
   100|     @abc.abstractmethod
   101|     async def _user_left_room(self, target: UserID, room_id: str) -> None:
   102|         """Notifies distributor on master process that the user has left the
   103|         room.
   104|         Args:
   105|             target
   106|             room_id
   107|         """
   108|         raise NotImplementedError()
   109|     async def _local_membership_update(
   110|         self,
   111|         requester: Requester,
   112|         target: UserID,
   113|         room_id: str,
   114|         membership: str,
   115|         prev_event_ids: List[str],
   116|         txn_id: Optional[str] = None,
   117|         ratelimit: bool = True,

# --- HUNK 3: Lines 130-188 ---
   130|                 "type": EventTypes.Member,
   131|                 "content": content,
   132|                 "room_id": room_id,
   133|                 "sender": requester.user.to_string(),
   134|                 "state_key": user_id,
   135|                 "membership": membership,
   136|             },
   137|             token_id=requester.access_token_id,
   138|             txn_id=txn_id,
   139|             prev_event_ids=prev_event_ids,
   140|             require_consent=require_consent,
   141|         )
   142|         duplicate = await self.event_creation_handler.deduplicate_state_event(
   143|             event, context
   144|         )
   145|         if duplicate is not None:
   146|             _, stream_id = await self.store.get_event_ordering(duplicate.event_id)
   147|             return duplicate.event_id, stream_id
   148|         prev_state_ids = await context.get_prev_state_ids()
   149|         prev_member_event_id = prev_state_ids.get((EventTypes.Member, user_id), None)
   150|         if event.membership == Membership.JOIN:
   151|             newly_joined = True
   152|             if prev_member_event_id:
   153|                 prev_member_event = await self.store.get_event(prev_member_event_id)
   154|                 newly_joined = prev_member_event.membership != Membership.JOIN
   155|             if newly_joined:
   156|                 time_now_s = self.clock.time()
   157|                 (
   158|                     allowed,
   159|                     time_allowed,
   160|                 ) = self._join_rate_limiter_local.can_requester_do_action(requester)
   161|                 if not allowed:
   162|                     raise LimitExceededError(
   163|                         retry_after_ms=int(1000 * (time_allowed - time_now_s))
   164|                     )
   165|         stream_id = await self.event_creation_handler.handle_new_client_event(
   166|             requester, event, context, extra_users=[target], ratelimit=ratelimit,
   167|         )
   168|         if event.membership == Membership.LEAVE:
   169|             if prev_member_event_id:
   170|                 prev_member_event = await self.store.get_event(prev_member_event_id)
   171|                 if prev_member_event.membership == Membership.JOIN:
   172|                     await self._user_left_room(target, room_id)
   173|         return event.event_id, stream_id
   174|     async def copy_room_tags_and_direct_to_room(
   175|         self, old_room_id, new_room_id, user_id
   176|     ) -> None:
   177|         """Copies the tags and direct room state from one room to another.
   178|         Args:
   179|             old_room_id: The room ID of the old room.
   180|             new_room_id: The room ID of the new room.
   181|             user_id: The user's ID.
   182|         """
   183|         user_account_data, _ = await self.store.get_account_data_for_user(user_id)
   184|         direct_rooms = user_account_data.get("m.direct", {})
   185|         if isinstance(direct_rooms, dict):
   186|             for key, room_id_list in direct_rooms.items():
   187|                 if old_room_id in room_id_list and new_room_id not in room_id_list:
   188|                     direct_rooms[key].append(new_room_id)

# --- HUNK 4: Lines 504-544 ---
   504|             event, context
   505|         )
   506|         if prev_event is not None:
   507|             return
   508|         prev_state_ids = await context.get_prev_state_ids()
   509|         if event.membership == Membership.JOIN:
   510|             if requester.is_guest:
   511|                 guest_can_join = await self._can_guest_join(prev_state_ids)
   512|                 if not guest_can_join:
   513|                     raise AuthError(403, "Guest access not allowed")
   514|         if event.membership not in (Membership.LEAVE, Membership.BAN):
   515|             is_blocked = await self.store.is_room_blocked(room_id)
   516|             if is_blocked:
   517|                 raise SynapseError(403, "This room has been blocked on this server")
   518|         await self.event_creation_handler.handle_new_client_event(
   519|             requester, event, context, extra_users=[target_user], ratelimit=ratelimit
   520|         )
   521|         prev_member_event_id = prev_state_ids.get(
   522|             (EventTypes.Member, event.state_key), None
   523|         )
   524|         if event.membership == Membership.LEAVE:
   525|             if prev_member_event_id:
   526|                 prev_member_event = await self.store.get_event(prev_member_event_id)
   527|                 if prev_member_event.membership == Membership.JOIN:
   528|                     await self._user_left_room(target_user, room_id)
   529|     async def _can_guest_join(self, current_state_ids: StateMap[str]) -> bool:
   530|         """
   531|         Returns whether a guest can join a room based on its current state.
   532|         """
   533|         guest_access_id = current_state_ids.get((EventTypes.GuestAccess, ""), None)
   534|         if not guest_access_id:
   535|             return False
   536|         guest_access = await self.store.get_event(guest_access_id)
   537|         return bool(
   538|             guest_access
   539|             and guest_access.content
   540|             and "guest_access" in guest_access.content
   541|             and guest_access.content["guest_access"] == "can_join"
   542|         )
   543|     async def lookup_room_alias(
   544|         self, room_alias: RoomAlias

# --- HUNK 5: Lines 721-762 ---
   721|         create_event_id = current_state_ids.get(("m.room.create", ""))
   722|         if len(current_state_ids) == 1 and create_event_id:
   723|             return True
   724|         for etype, state_key in current_state_ids:
   725|             if etype != EventTypes.Member or not self.hs.is_mine_id(state_key):
   726|                 continue
   727|             event_id = current_state_ids[(etype, state_key)]
   728|             event = await self.store.get_event(event_id, allow_none=True)
   729|             if not event:
   730|                 continue
   731|             if event.membership == Membership.JOIN:
   732|                 return True
   733|         return False
   734|     async def _is_server_notice_room(self, room_id: str) -> bool:
   735|         if self._server_notices_mxid is None:
   736|             return False
   737|         user_ids = await self.store.get_users_in_room(room_id)
   738|         return self._server_notices_mxid in user_ids
   739| class RoomMemberMasterHandler(RoomMemberHandler):
   740|     def __init__(self, hs):
   741|         super().__init__(hs)
   742|         self.distributor = hs.get_distributor()
   743|         self.distributor.declare("user_left_room")
   744|     async def _is_remote_room_too_complex(
   745|         self, room_id: str, remote_room_hosts: List[str]
   746|     ) -> Optional[bool]:
   747|         """
   748|         Check if complexity of a remote room is too great.
   749|         Args:
   750|             room_id
   751|             remote_room_hosts
   752|         Returns: bool of whether the complexity is too great, or None
   753|             if unable to be fetched
   754|         """
   755|         max_complexity = self.hs.config.limit_remote_rooms.complexity
   756|         complexity = await self.federation_handler.get_room_complexity(
   757|             remote_room_hosts, room_id
   758|         )
   759|         if complexity:
   760|             return complexity["v1"] > max_complexity
   761|         return None
   762|     async def _is_local_room_too_complex(self, room_id: str) -> bool:

# --- HUNK 6: Lines 782-821 ---
   782|             host for host in remote_room_hosts if host != self.hs.hostname
   783|         ]
   784|         if len(remote_room_hosts) == 0:
   785|             raise SynapseError(404, "No known servers")
   786|         check_complexity = self.hs.config.limit_remote_rooms.enabled
   787|         if check_complexity and self.hs.config.limit_remote_rooms.admins_can_join:
   788|             check_complexity = not await self.auth.is_server_admin(user)
   789|         if check_complexity:
   790|             too_complex = await self._is_remote_room_too_complex(
   791|                 room_id, remote_room_hosts
   792|             )
   793|             if too_complex is True:
   794|                 raise SynapseError(
   795|                     code=400,
   796|                     msg=self.hs.config.limit_remote_rooms.complexity_error,
   797|                     errcode=Codes.RESOURCE_LIMIT_EXCEEDED,
   798|                 )
   799|         event_id, stream_id = await self.federation_handler.do_invite_join(
   800|             remote_room_hosts, room_id, user.to_string(), content
   801|         )
   802|         if check_complexity:
   803|             if too_complex is False:
   804|                 return event_id, stream_id
   805|             too_complex = await self._is_local_room_too_complex(room_id)
   806|             if too_complex is False:
   807|                 return event_id, stream_id
   808|             requester = types.create_requester(user, None, False, False, None)
   809|             await self.update_membership(
   810|                 requester=requester, target=user, room_id=room_id, action="leave"
   811|             )
   812|             raise SynapseError(
   813|                 code=400,
   814|                 msg=self.hs.config.limit_remote_rooms.complexity_error,
   815|                 errcode=Codes.RESOURCE_LIMIT_EXCEEDED,
   816|             )
   817|         return event_id, stream_id
   818|     async def remote_reject_invite(
   819|         self,
   820|         invite_event_id: str,
   821|         txn_id: Optional[str],

# --- HUNK 7: Lines 882-918 ---
   882|         event = create_local_event_from_event_dict(
   883|             clock=self.clock,
   884|             hostname=self.hs.hostname,
   885|             signing_key=self.hs.signing_key,
   886|             room_version=room_version,
   887|             event_dict=event_dict,
   888|         )
   889|         event.internal_metadata.outlier = True
   890|         event.internal_metadata.out_of_band_membership = True
   891|         if txn_id is not None:
   892|             event.internal_metadata.txn_id = txn_id
   893|         if requester.access_token_id is not None:
   894|             event.internal_metadata.token_id = requester.access_token_id
   895|         EventValidator().validate_new(event, self.config)
   896|         context = await self.state_handler.compute_event_context(event)
   897|         context.app_service = requester.app_service
   898|         stream_id = await self.event_creation_handler.handle_new_client_event(
   899|             requester, event, context, extra_users=[UserID.from_string(target_user)],
   900|         )
   901|         return event.event_id, stream_id
   902|     async def _user_left_room(self, target: UserID, room_id: str) -> None:
   903|         """Implements RoomMemberHandler._user_left_room
   904|         """
   905|         user_left_room(self.distributor, target, room_id)
   906|     async def forget(self, user: UserID, room_id: str) -> None:
   907|         user_id = user.to_string()
   908|         member = await self.state_handler.get_current_state(
   909|             room_id=room_id, event_type=EventTypes.Member, state_key=user_id
   910|         )
   911|         membership = member.membership if member else None
   912|         if membership is not None and membership not in [
   913|             Membership.LEAVE,
   914|             Membership.BAN,
   915|         ]:
   916|             raise SynapseError(400, "User %s in room %s" % (user_id, room_id))
   917|         if membership:
   918|             await self.store.forget(user_id, room_id)


# ====================================================================
# FILE: synapse/handlers/room_member_worker.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-61 ---
     1| import logging
     2| from typing import List, Optional, Tuple
     3| from synapse.api.errors import SynapseError
     4| from synapse.handlers.room_member import RoomMemberHandler
     5| from synapse.replication.http.membership import (
     6|     ReplicationRemoteJoinRestServlet as ReplRemoteJoin,
     7|     ReplicationRemoteRejectInviteRestServlet as ReplRejectInvite,
     8|     ReplicationUserJoinedLeftRoomRestServlet as ReplJoinedLeft,
     9| )
    10| from synapse.types import Requester, UserID
    11| logger = logging.getLogger(__name__)
    12| class RoomMemberWorkerHandler(RoomMemberHandler):
    13|     def __init__(self, hs):
    14|         super().__init__(hs)
    15|         self._remote_join_client = ReplRemoteJoin.make_client(hs)
    16|         self._remote_reject_client = ReplRejectInvite.make_client(hs)
    17|         self._notify_change_client = ReplJoinedLeft.make_client(hs)
    18|     async def _remote_join(
    19|         self,
    20|         requester: Requester,
    21|         remote_room_hosts: List[str],
    22|         room_id: str,
    23|         user: UserID,
    24|         content: dict,
    25|     ) -> Tuple[str, int]:
    26|         """Implements RoomMemberHandler._remote_join
    27|         """
    28|         if len(remote_room_hosts) == 0:
    29|             raise SynapseError(404, "No known servers")
    30|         ret = await self._remote_join_client(
    31|             requester=requester,
    32|             remote_room_hosts=remote_room_hosts,
    33|             room_id=room_id,
    34|             user_id=user.to_string(),
    35|             content=content,
    36|         )
    37|         return ret["event_id"], ret["stream_id"]
    38|     async def remote_reject_invite(
    39|         self,
    40|         invite_event_id: str,
    41|         txn_id: Optional[str],
    42|         requester: Requester,
    43|         content: dict,
    44|     ) -> Tuple[str, int]:
    45|         """
    46|         Rejects an out-of-band invite received from a remote user
    47|         Implements RoomMemberHandler.remote_reject_invite
    48|         """
    49|         ret = await self._remote_reject_client(
    50|             invite_event_id=invite_event_id,
    51|             txn_id=txn_id,
    52|             requester=requester,
    53|             content=content,
    54|         )
    55|         return ret["event_id"], ret["stream_id"]
    56|     async def _user_left_room(self, target: UserID, room_id: str) -> None:
    57|         """Implements RoomMemberHandler._user_left_room
    58|         """
    59|         await self._notify_change_client(
    60|             user_id=target.to_string(), room_id=room_id, change="left"
    61|         )


# ====================================================================
# FILE: synapse/handlers/saml_handler.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-343 ---
     1| import logging
     2| import re
     3| from typing import TYPE_CHECKING, Callable, Dict, Optional, Set, Tuple
     4| import attr
     5| import saml2
     6| import saml2.response
     7| from saml2.client import Saml2Client
     8| from synapse.api.errors import SynapseError
     9| from synapse.config import ConfigError
    10| from synapse.config.saml2_config import SamlAttributeRequirement
    11| from synapse.http.server import respond_with_html
    12| from synapse.http.servlet import parse_string
    13| from synapse.http.site import SynapseRequest
    14| from synapse.module_api import ModuleApi
    15| from synapse.types import (
    16|     UserID,
    17|     map_username_to_mxid_localpart,
    18|     mxid_localpart_allowed_characters,
    19| )
    20| from synapse.util.async_helpers import Linearizer
    21| from synapse.util.iterutils import chunk_seq
    22| if TYPE_CHECKING:
    23|     import synapse.server
    24| logger = logging.getLogger(__name__)
    25| class MappingException(Exception):
    26|     """Used to catch errors when mapping the SAML2 response to a user."""
    27| @attr.s(slots=True)
    28| class Saml2SessionData:
    29|     """Data we track about SAML2 sessions"""
    30|     creation_time = attr.ib()
    31|     ui_auth_session_id = attr.ib(type=Optional[str], default=None)
    32| class SamlHandler:
    33|     def __init__(self, hs: "synapse.server.HomeServer"):
    34|         self.hs = hs
    35|         self._saml_client = Saml2Client(hs.config.saml2_sp_config)
    36|         self._auth = hs.get_auth()
    37|         self._auth_handler = hs.get_auth_handler()
    38|         self._registration_handler = hs.get_registration_handler()
    39|         self._clock = hs.get_clock()
    40|         self._datastore = hs.get_datastore()
    41|         self._hostname = hs.hostname
    42|         self._saml2_session_lifetime = hs.config.saml2_session_lifetime
    43|         self._grandfathered_mxid_source_attribute = (
    44|             hs.config.saml2_grandfathered_mxid_source_attribute
    45|         )
    46|         self._saml2_attribute_requirements = hs.config.saml2.attribute_requirements
    47|         self._error_template = hs.config.sso_error_template
    48|         self._user_mapping_provider = hs.config.saml2_user_mapping_provider_class(
    49|             hs.config.saml2_user_mapping_provider_config,
    50|             ModuleApi(hs, hs.get_auth_handler()),
    51|         )
    52|         self._auth_provider_id = "saml"
    53|         self._outstanding_requests_dict = {}  # type: Dict[str, Saml2SessionData]
    54|         self._mapping_lock = Linearizer(name="saml_mapping", clock=self._clock)
    55|     def _render_error(
    56|         self, request, error: str, error_description: Optional[str] = None
    57|     ) -> None:
    58|         """Render the error template and respond to the request with it.
    59|         This is used to show errors to the user. The template of this page can
    60|         be found under `synapse/res/templates/sso_error.html`.
    61|         Args:
    62|             request: The incoming request from the browser.
    63|                 We'll respond with an HTML page describing the error.
    64|             error: A technical identifier for this error.
    65|             error_description: A human-readable description of the error.
    66|         """
    67|         html = self._error_template.render(
    68|             error=error, error_description=error_description
    69|         )
    70|         respond_with_html(request, 400, html)
    71|     def handle_redirect_request(
    72|         self, client_redirect_url: bytes, ui_auth_session_id: Optional[str] = None
    73|     ) -> bytes:
    74|         """Handle an incoming request to /login/sso/redirect
    75|         Args:
    76|             client_redirect_url: the URL that we should redirect the
    77|                 client to when everything is done
    78|             ui_auth_session_id: The session ID of the ongoing UI Auth (or
    79|                 None if this is a login).
    80|         Returns:
    81|             URL to redirect to
    82|         """
    83|         reqid, info = self._saml_client.prepare_for_authenticate(
    84|             relay_state=client_redirect_url
    85|         )
    86|         logger.info("Initiating a new SAML session: %s" % (reqid,))
    87|         now = self._clock.time_msec()
    88|         self._outstanding_requests_dict[reqid] = Saml2SessionData(
    89|             creation_time=now, ui_auth_session_id=ui_auth_session_id,
    90|         )
    91|         for key, value in info["headers"]:
    92|             if key == "Location":
    93|                 return value
    94|         raise Exception("prepare_for_authenticate didn't return a Location header")
    95|     async def handle_saml_response(self, request: SynapseRequest) -> None:
    96|         """Handle an incoming request to /_matrix/saml2/authn_response
    97|         Args:
    98|             request: the incoming request from the browser. We'll
    99|                 respond to it with a redirect.
   100|         Returns:
   101|             Completes once we have handled the request.
   102|         """
   103|         resp_bytes = parse_string(request, "SAMLResponse", required=True)
   104|         relay_state = parse_string(request, "RelayState", required=True)
   105|         self.expire_sessions()
   106|         try:
   107|             saml2_auth = self._saml_client.parse_authn_request_response(
   108|                 resp_bytes,
   109|                 saml2.BINDING_HTTP_POST,
   110|                 outstanding=self._outstanding_requests_dict,
   111|             )
   112|         except saml2.response.UnsolicitedResponse as e:
   113|             logger.warning(str(e))
   114|             self._render_error(
   115|                 request, "unsolicited_response", "Unexpected SAML2 login."
   116|             )
   117|             return
   118|         except Exception as e:
   119|             self._render_error(
   120|                 request,
   121|                 "invalid_response",
   122|                 "Unable to parse SAML2 response: %s." % (e,),
   123|             )
   124|             return
   125|         if saml2_auth.not_signed:
   126|             self._render_error(
   127|                 request, "unsigned_respond", "SAML2 response was not signed."
   128|             )
   129|             return
   130|         logger.debug("SAML2 response: %s", saml2_auth.origxml)
   131|         for assertion in saml2_auth.assertions:
   132|             count = 0
   133|             for part in chunk_seq(str(assertion), 10000):
   134|                 logger.info(
   135|                     "SAML2 assertion: %s%s", "(%i)..." % (count,) if count else "", part
   136|                 )
   137|                 count += 1
   138|         logger.info("SAML2 mapped attributes: %s", saml2_auth.ava)
   139|         current_session = self._outstanding_requests_dict.pop(
   140|             saml2_auth.in_response_to, None
   141|         )
   142|         for requirement in self._saml2_attribute_requirements:
   143|             if not _check_attribute_requirement(saml2_auth.ava, requirement):
   144|                 self._render_error(
   145|                     request, "unauthorised", "You are not authorised to log in here."
   146|                 )
   147|                 return
   148|         user_agent = request.requestHeaders.getRawHeaders(b"User-Agent", default=[b""])[
   149|             0
   150|         ].decode("ascii", "surrogateescape")
   151|         ip_address = self.hs.get_ip_from_request(request)
   152|         try:
   153|             user_id = await self._map_saml_response_to_user(
   154|                 saml2_auth, relay_state, user_agent, ip_address
   155|             )
   156|         except MappingException as e:
   157|             logger.exception("Could not map user")
   158|             self._render_error(request, "mapping_error", str(e))
   159|             return
   160|         if current_session and current_session.ui_auth_session_id:
   161|             await self._auth_handler.complete_sso_ui_auth(
   162|                 user_id, current_session.ui_auth_session_id, request
   163|             )
   164|         else:
   165|             await self._auth_handler.complete_sso_login(user_id, request, relay_state)
   166|     async def _map_saml_response_to_user(
   167|         self,
   168|         saml2_auth: saml2.response.AuthnResponse,
   169|         client_redirect_url: str,
   170|         user_agent: str,
   171|         ip_address: str,
   172|     ) -> str:
   173|         """
   174|         Given a SAML response, retrieve the user ID for it and possibly register the user.
   175|         Args:
   176|             saml2_auth: The parsed SAML2 response.
   177|             client_redirect_url: The redirect URL passed in by the client.
   178|             user_agent: The user agent of the client making the request.
   179|             ip_address: The IP address of the client making the request.
   180|         Returns:
   181|              The user ID associated with this response.
   182|         Raises:
   183|             MappingException if there was a problem mapping the response to a user.
   184|             RedirectException: some mapping providers may raise this if they need
   185|                 to redirect to an interstitial page.
   186|         """
   187|         remote_user_id = self._user_mapping_provider.get_remote_user_id(
   188|             saml2_auth, client_redirect_url
   189|         )
   190|         if not remote_user_id:
   191|             raise MappingException(
   192|                 "Failed to extract remote user id from SAML response"
   193|             )
   194|         with (await self._mapping_lock.queue(self._auth_provider_id)):
   195|             logger.info(
   196|                 "Looking for existing mapping for user %s:%s",
   197|                 self._auth_provider_id,
   198|                 remote_user_id,
   199|             )
   200|             registered_user_id = await self._datastore.get_user_by_external_id(
   201|                 self._auth_provider_id, remote_user_id
   202|             )
   203|             if registered_user_id is not None:
   204|                 logger.info("Found existing mapping %s", registered_user_id)
   205|                 return registered_user_id
   206|             if (
   207|                 self._grandfathered_mxid_source_attribute
   208|                 and self._grandfathered_mxid_source_attribute in saml2_auth.ava
   209|             ):
   210|                 attrval = saml2_auth.ava[self._grandfathered_mxid_source_attribute][0]
   211|                 user_id = UserID(
   212|                     map_username_to_mxid_localpart(attrval), self._hostname
   213|                 ).to_string()
   214|                 logger.info(
   215|                     "Looking for existing account based on mapped %s %s",
   216|                     self._grandfathered_mxid_source_attribute,
   217|                     user_id,
   218|                 )
   219|                 users = await self._datastore.get_users_by_id_case_insensitive(user_id)
   220|                 if users:
   221|                     registered_user_id = list(users.keys())[0]
   222|                     logger.info("Grandfathering mapping to %s", registered_user_id)
   223|                     await self._datastore.record_user_external_id(
   224|                         self._auth_provider_id, remote_user_id, registered_user_id
   225|                     )
   226|                     return registered_user_id
   227|             for i in range(1000):
   228|                 attribute_dict = self._user_mapping_provider.saml_response_to_user_attributes(
   229|                     saml2_auth, i, client_redirect_url=client_redirect_url,
   230|                 )
   231|                 logger.debug(
   232|                     "Retrieved SAML attributes from user mapping provider: %s "
   233|                     "(attempt %d)",
   234|                     attribute_dict,
   235|                     i,
   236|                 )
   237|                 localpart = attribute_dict.get("mxid_localpart")
   238|                 if not localpart:
   239|                     raise MappingException(
   240|                         "Error parsing SAML2 response: SAML mapping provider plugin "
   241|                         "did not return a mxid_localpart value"
   242|                     )
   243|                 displayname = attribute_dict.get("displayname")
   244|                 emails = attribute_dict.get("emails", [])
   245|                 if not await self._datastore.get_users_by_id_case_insensitive(
   246|                     UserID(localpart, self._hostname).to_string()
   247|                 ):
   248|                     break
   249|             else:
   250|                 raise MappingException(
   251|                     "Unable to generate a Matrix ID from the SAML response"
   252|                 )
   253|             logger.info("Mapped SAML user to local part %s", localpart)
   254|             registered_user_id = await self._registration_handler.register_user(
   255|                 localpart=localpart,
   256|                 default_display_name=displayname,
   257|                 bind_emails=emails,
   258|                 user_agent_ips=(user_agent, ip_address),
   259|             )
   260|             await self._datastore.record_user_external_id(
   261|                 self._auth_provider_id, remote_user_id, registered_user_id
   262|             )
   263|             return registered_user_id
   264|     def expire_sessions(self):
   265|         expire_before = self._clock.time_msec() - self._saml2_session_lifetime
   266|         to_expire = set()
   267|         for reqid, data in self._outstanding_requests_dict.items():
   268|             if data.creation_time < expire_before:
   269|                 to_expire.add(reqid)
   270|         for reqid in to_expire:
   271|             logger.debug("Expiring session id %s", reqid)
   272|             del self._outstanding_requests_dict[reqid]
   273| def _check_attribute_requirement(ava: dict, req: SamlAttributeRequirement) -> bool:
   274|     values = ava.get(req.attribute, [])
   275|     for v in values:
   276|         if v == req.value:
   277|             return True
   278|     logger.info(
   279|         "SAML2 attribute %s did not match required value '%s' (was '%s')",
   280|         req.attribute,
   281|         req.value,
   282|         values,
   283|     )
   284|     return False
   285| DOT_REPLACE_PATTERN = re.compile(
   286|     ("[^%s]" % (re.escape("".join(mxid_localpart_allowed_characters)),))
   287| )
   288| def dot_replace_for_mxid(username: str) -> str:
   289|     """Replace any characters which are not allowed in Matrix IDs with a dot."""
   290|     username = username.lower()
   291|     username = DOT_REPLACE_PATTERN.sub(".", username)
   292|     username = re.sub("^_", "", username)
   293|     return username
   294| MXID_MAPPER_MAP = {
   295|     "hexencode": map_username_to_mxid_localpart,
   296|     "dotreplace": dot_replace_for_mxid,
   297| }  # type: Dict[str, Callable[[str], str]]
   298| @attr.s
   299| class SamlConfig:
   300|     mxid_source_attribute = attr.ib()
   301|     mxid_mapper = attr.ib()
   302| class DefaultSamlMappingProvider:
   303|     __version__ = "0.0.1"
   304|     def __init__(self, parsed_config: SamlConfig, module_api: ModuleApi):
   305|         """The default SAML user mapping provider
   306|         Args:
   307|             parsed_config: Module configuration
   308|             module_api: module api proxy
   309|         """
   310|         self._mxid_source_attribute = parsed_config.mxid_source_attribute
   311|         self._mxid_mapper = parsed_config.mxid_mapper
   312|         self._grandfathered_mxid_source_attribute = (
   313|             module_api._hs.config.saml2_grandfathered_mxid_source_attribute
   314|         )
   315|     def get_remote_user_id(
   316|         self, saml_response: saml2.response.AuthnResponse, client_redirect_url: str
   317|     ) -> str:
   318|         """Extracts the remote user id from the SAML response"""
   319|         try:
   320|             return saml_response.ava["uid"][0]
   321|         except KeyError:
   322|             logger.warning("SAML2 response lacks a 'uid' attestation")
   323|             raise MappingException("'uid' not in SAML2 response")
   324|     def saml_response_to_user_attributes(
   325|         self,
   326|         saml_response: saml2.response.AuthnResponse,
   327|         failures: int,
   328|         client_redirect_url: str,
   329|     ) -> dict:
   330|         """Maps some text from a SAML response to attributes of a new user
   331|         Args:
   332|             saml_response: A SAML auth response object
   333|             failures: How many times a call to this function with this
   334|                 saml_response has resulted in a failure
   335|             client_redirect_url: where the client wants to redirect to
   336|         Returns:
   337|             dict: A dict containing new user attributes. Possible keys:
   338|                 * mxid_localpart (str): Required. The localpart of the user's mxid
   339|                 * displayname (str): The displayname of the user
   340|                 * emails (list[str]): Any emails for the user
   341|         """
   342|         try:
   343|             mxid_source = saml_response.ava[self._mxid_source_attribute][0]


# ====================================================================
# FILE: synapse/handlers/search.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-34 ---
     1| import itertools
     2| import logging
     3| from typing import Iterable
     4| from unpaddedbase64 import decode_base64, encode_base64
     5| from synapse.api.constants import EventTypes, Membership
     6| from synapse.api.errors import NotFoundError, SynapseError
     7| from synapse.api.filtering import Filter
     8| from synapse.storage.state import StateFilter
     9| from synapse.visibility import filter_events_for_client
    10| from ._base import BaseHandler
    11| logger = logging.getLogger(__name__)
    12| class SearchHandler(BaseHandler):
    13|     def __init__(self, hs):
    14|         super().__init__(hs)
    15|         self._event_serializer = hs.get_event_client_serializer()
    16|         self.storage = hs.get_storage()
    17|         self.state_store = self.storage.state
    18|         self.auth = hs.get_auth()
    19|     async def get_old_rooms_from_upgraded_room(self, room_id: str) -> Iterable[str]:
    20|         """Retrieves room IDs of old rooms in the history of an upgraded room.
    21|         We do so by checking the m.room.create event of the room for a
    22|         `predecessor` key. If it exists, we add the room ID to our return
    23|         list and then check that room for a m.room.create event and so on
    24|         until we can no longer find any more previous rooms.
    25|         The full list of all found rooms in then returned.
    26|         Args:
    27|             room_id: id of the room to search through.
    28|         Returns:
    29|             Predecessor room ids
    30|         """
    31|         historical_room_ids = []
    32|         predecessor = await self.store.get_room_predecessor(room_id)
    33|         while True:
    34|             if not predecessor:

# --- HUNK 2: Lines 210-255 ---
   210|             raise NotImplementedError()
   211|         logger.info("Found %d events to return", len(allowed_events))
   212|         if event_context is not None:
   213|             now_token = self.hs.get_event_sources().get_current_token()
   214|             contexts = {}
   215|             for event in allowed_events:
   216|                 res = await self.store.get_events_around(
   217|                     event.room_id, event.event_id, before_limit, after_limit
   218|                 )
   219|                 logger.info(
   220|                     "Context for search returned %d and %d events",
   221|                     len(res["events_before"]),
   222|                     len(res["events_after"]),
   223|                 )
   224|                 res["events_before"] = await filter_events_for_client(
   225|                     self.storage, user.to_string(), res["events_before"]
   226|                 )
   227|                 res["events_after"] = await filter_events_for_client(
   228|                     self.storage, user.to_string(), res["events_after"]
   229|                 )
   230|                 res["start"] = await now_token.copy_and_replace(
   231|                     "room_key", res["start"]
   232|                 ).to_string(self.store)
   233|                 res["end"] = await now_token.copy_and_replace(
   234|                     "room_key", res["end"]
   235|                 ).to_string(self.store)
   236|                 if include_profile:
   237|                     senders = {
   238|                         ev.sender
   239|                         for ev in itertools.chain(
   240|                             res["events_before"], [event], res["events_after"]
   241|                         )
   242|                     }
   243|                     if res["events_after"]:
   244|                         last_event_id = res["events_after"][-1].event_id
   245|                     else:
   246|                         last_event_id = event.event_id
   247|                     state_filter = StateFilter.from_types(
   248|                         [(EventTypes.Member, sender) for sender in senders]
   249|                     )
   250|                     state = await self.state_store.get_state_for_event(
   251|                         last_event_id, state_filter
   252|                     )
   253|                     res["profile_info"] = {
   254|                         s.state_key: {
   255|                             "displayname": s.content.get("displayname", None),


# ====================================================================
# FILE: synapse/handlers/set_password.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-30 ---
     1| import logging
     2| from typing import Optional
     3| from synapse.api.errors import Codes, StoreError, SynapseError
     4| from synapse.types import Requester
     5| from ._base import BaseHandler
     6| logger = logging.getLogger(__name__)
     7| class SetPasswordHandler(BaseHandler):
     8|     """Handler which deals with changing user account passwords"""
     9|     def __init__(self, hs):
    10|         super().__init__(hs)
    11|         self._auth_handler = hs.get_auth_handler()
    12|         self._device_handler = hs.get_device_handler()
    13|         self._password_policy_handler = hs.get_password_policy_handler()
    14|     async def set_password(
    15|         self,
    16|         user_id: str,
    17|         password_hash: str,
    18|         logout_devices: bool,
    19|         requester: Optional[Requester] = None,
    20|     ):
    21|         if not self.hs.config.password_localdb_enabled:
    22|             raise SynapseError(403, "Password change disabled", errcode=Codes.FORBIDDEN)
    23|         try:
    24|             await self.store.user_set_password_hash(user_id, password_hash)
    25|         except StoreError as e:
    26|             if e.code == 404:
    27|                 raise SynapseError(404, "Unknown user", Codes.NOT_FOUND)
    28|             raise e
    29|         if logout_devices:
    30|             except_device_id = requester.device_id if requester else None


# ====================================================================
# FILE: synapse/handlers/sync.py
# Total hunks: 11
# ====================================================================
# --- HUNK 1: Lines 33-183 ---
    33|     "synapse_handlers_sync_nonempty_total",
    34|     "Count of non empty sync responses. type is initial_sync/full_state_sync"
    35|     "/incremental_sync. lazy_loaded indicates if lazy loaded members were "
    36|     "enabled for that request.",
    37|     ["type", "lazy_loaded"],
    38| )
    39| LAZY_LOADED_MEMBERS_CACHE_MAX_AGE = 30 * 60 * 1000
    40| LAZY_LOADED_MEMBERS_CACHE_MAX_SIZE = 100
    41| @attr.s(slots=True, frozen=True)
    42| class SyncConfig:
    43|     user = attr.ib(type=UserID)
    44|     filter_collection = attr.ib(type=FilterCollection)
    45|     is_guest = attr.ib(type=bool)
    46|     request_key = attr.ib(type=Tuple[Any, ...])
    47|     device_id = attr.ib(type=str)
    48| @attr.s(slots=True, frozen=True)
    49| class TimelineBatch:
    50|     prev_batch = attr.ib(type=StreamToken)
    51|     events = attr.ib(type=List[EventBase])
    52|     limited = attr.ib(bool)
    53|     def __bool__(self) -> bool:
    54|         """Make the result appear empty if there are no updates. This is used
    55|         to tell if room needs to be part of the sync result.
    56|         """
    57|         return bool(self.events)
    58| @attr.s(slots=True)
    59| class JoinedSyncResult:
    60|     room_id = attr.ib(type=str)
    61|     timeline = attr.ib(type=TimelineBatch)
    62|     state = attr.ib(type=StateMap[EventBase])
    63|     ephemeral = attr.ib(type=List[JsonDict])
    64|     account_data = attr.ib(type=List[JsonDict])
    65|     unread_notifications = attr.ib(type=JsonDict)
    66|     summary = attr.ib(type=Optional[JsonDict])
    67|     unread_count = attr.ib(type=int)
    68|     def __bool__(self) -> bool:
    69|         """Make the result appear empty if there are no updates. This is used
    70|         to tell if room needs to be part of the sync result.
    71|         """
    72|         return bool(
    73|             self.timeline
    74|             or self.state
    75|             or self.ephemeral
    76|             or self.account_data
    77|         )
    78| @attr.s(slots=True, frozen=True)
    79| class ArchivedSyncResult:
    80|     room_id = attr.ib(type=str)
    81|     timeline = attr.ib(type=TimelineBatch)
    82|     state = attr.ib(type=StateMap[EventBase])
    83|     account_data = attr.ib(type=List[JsonDict])
    84|     def __bool__(self) -> bool:
    85|         """Make the result appear empty if there are no updates. This is used
    86|         to tell if room needs to be part of the sync result.
    87|         """
    88|         return bool(self.timeline or self.state or self.account_data)
    89| @attr.s(slots=True, frozen=True)
    90| class InvitedSyncResult:
    91|     room_id = attr.ib(type=str)
    92|     invite = attr.ib(type=EventBase)
    93|     def __bool__(self) -> bool:
    94|         """Invited rooms should always be reported to the client"""
    95|         return True
    96| @attr.s(slots=True, frozen=True)
    97| class GroupsSyncResult:
    98|     join = attr.ib(type=JsonDict)
    99|     invite = attr.ib(type=JsonDict)
   100|     leave = attr.ib(type=JsonDict)
   101|     def __bool__(self) -> bool:
   102|         return bool(self.join or self.invite or self.leave)
   103| @attr.s(slots=True, frozen=True)
   104| class DeviceLists:
   105|     """
   106|     Attributes:
   107|         changed: List of user_ids whose devices may have changed
   108|         left: List of user_ids whose devices we no longer track
   109|     """
   110|     changed = attr.ib(type=Collection[str])
   111|     left = attr.ib(type=Collection[str])
   112|     def __bool__(self) -> bool:
   113|         return bool(self.changed or self.left)
   114| @attr.s(slots=True)
   115| class _RoomChanges:
   116|     """The set of room entries to include in the sync, plus the set of joined
   117|     and left room IDs since last sync.
   118|     """
   119|     room_entries = attr.ib(type=List["RoomSyncResultBuilder"])
   120|     invited = attr.ib(type=List[InvitedSyncResult])
   121|     newly_joined_rooms = attr.ib(type=List[str])
   122|     newly_left_rooms = attr.ib(type=List[str])
   123| @attr.s(slots=True, frozen=True)
   124| class SyncResult:
   125|     """
   126|     Attributes:
   127|         next_batch: Token for the next sync
   128|         presence: List of presence events for the user.
   129|         account_data: List of account_data events for the user.
   130|         joined: JoinedSyncResult for each joined room.
   131|         invited: InvitedSyncResult for each invited room.
   132|         archived: ArchivedSyncResult for each archived room.
   133|         to_device: List of direct messages for the device.
   134|         device_lists: List of user_ids whose devices have changed
   135|         device_one_time_keys_count: Dict of algorithm to count for one time keys
   136|             for this device
   137|         groups: Group updates, if any
   138|     """
   139|     next_batch = attr.ib(type=StreamToken)
   140|     presence = attr.ib(type=List[JsonDict])
   141|     account_data = attr.ib(type=List[JsonDict])
   142|     joined = attr.ib(type=List[JoinedSyncResult])
   143|     invited = attr.ib(type=List[InvitedSyncResult])
   144|     archived = attr.ib(type=List[ArchivedSyncResult])
   145|     to_device = attr.ib(type=List[JsonDict])
   146|     device_lists = attr.ib(type=DeviceLists)
   147|     device_one_time_keys_count = attr.ib(type=JsonDict)
   148|     groups = attr.ib(type=Optional[GroupsSyncResult])
   149|     def __bool__(self) -> bool:
   150|         """Make the result appear empty if there are no updates. This is used
   151|         to tell if the notifier needs to wait for more events when polling for
   152|         events.
   153|         """
   154|         return bool(
   155|             self.presence
   156|             or self.joined
   157|             or self.invited
   158|             or self.archived
   159|             or self.account_data
   160|             or self.to_device
   161|             or self.device_lists
   162|             or self.groups
   163|         )
   164| class SyncHandler:
   165|     def __init__(self, hs: "HomeServer"):
   166|         self.hs_config = hs.config
   167|         self.store = hs.get_datastore()
   168|         self.notifier = hs.get_notifier()
   169|         self.presence_handler = hs.get_presence_handler()
   170|         self.event_sources = hs.get_event_sources()
   171|         self.clock = hs.get_clock()
   172|         self.response_cache = ResponseCache(hs, "sync")
   173|         self.state = hs.get_state_handler()
   174|         self.auth = hs.get_auth()
   175|         self.storage = hs.get_storage()
   176|         self.state_store = self.storage.state
   177|         self.lazy_loaded_members_cache = ExpiringCache(
   178|             "lazy_loaded_members_cache",
   179|             self.clock,
   180|             max_len=0,
   181|             expiry_ms=LAZY_LOADED_MEMBERS_CACHE_MAX_AGE,
   182|         )
   183|     async def wait_for_sync_for_user(

# --- HUNK 2: Lines 255-311 ---
   255|         return rules
   256|     async def ephemeral_by_room(
   257|         self,
   258|         sync_result_builder: "SyncResultBuilder",
   259|         now_token: StreamToken,
   260|         since_token: Optional[StreamToken] = None,
   261|     ) -> Tuple[StreamToken, Dict[str, List[JsonDict]]]:
   262|         """Get the ephemeral events for each room the user is in
   263|         Args:
   264|             sync_result_builder
   265|             now_token: Where the server is currently up to.
   266|             since_token: Where the server was when the client
   267|                 last synced.
   268|         Returns:
   269|             A tuple of the now StreamToken, updated to reflect the which typing
   270|             events are included, and a dict mapping from room_id to a list of
   271|             typing events for that room.
   272|         """
   273|         sync_config = sync_result_builder.sync_config
   274|         with Measure(self.clock, "ephemeral_by_room"):
   275|             typing_key = since_token.typing_key if since_token else 0
   276|             room_ids = sync_result_builder.joined_room_ids
   277|             typing_source = self.event_sources.sources["typing"]
   278|             typing, typing_key = await typing_source.get_new_events(
   279|                 user=sync_config.user,
   280|                 from_key=typing_key,
   281|                 limit=sync_config.filter_collection.ephemeral_limit(),
   282|                 room_ids=room_ids,
   283|                 is_guest=sync_config.is_guest,
   284|             )
   285|             now_token = now_token.copy_and_replace("typing_key", typing_key)
   286|             ephemeral_by_room = {}  # type: JsonDict
   287|             for event in typing:
   288|                 room_id = event["room_id"]
   289|                 event_copy = {k: v for (k, v) in event.items() if k != "room_id"}
   290|                 ephemeral_by_room.setdefault(room_id, []).append(event_copy)
   291|             receipt_key = since_token.receipt_key if since_token else 0
   292|             receipt_source = self.event_sources.sources["receipt"]
   293|             receipts, receipt_key = await receipt_source.get_new_events(
   294|                 user=sync_config.user,
   295|                 from_key=receipt_key,
   296|                 limit=sync_config.filter_collection.ephemeral_limit(),
   297|                 room_ids=room_ids,
   298|                 is_guest=sync_config.is_guest,
   299|             )
   300|             now_token = now_token.copy_and_replace("receipt_key", receipt_key)
   301|             for event in receipts:
   302|                 room_id = event["room_id"]
   303|                 event_copy = {k: v for (k, v) in event.items() if k != "room_id"}
   304|                 ephemeral_by_room.setdefault(room_id, []).append(event_copy)
   305|         return now_token, ephemeral_by_room
   306|     async def _load_filtered_recents(
   307|         self,
   308|         room_id: str,
   309|         sync_config: SyncConfig,
   310|         now_token: StreamToken,
   311|         since_token: Optional[StreamToken] = None,

# --- HUNK 3: Lines 688-728 ---
   688|         self,
   689|         sync_config: SyncConfig,
   690|         since_token: Optional[StreamToken] = None,
   691|         full_state: bool = False,
   692|     ) -> SyncResult:
   693|         """Generates a sync result.
   694|         """
   695|         now_token = self.event_sources.get_current_token()
   696|         logger.debug(
   697|             "Calculating sync response for %r between %s and %s",
   698|             sync_config.user,
   699|             since_token,
   700|             now_token,
   701|         )
   702|         user_id = sync_config.user.to_string()
   703|         app_service = self.store.get_app_service_by_user_id(user_id)
   704|         if app_service:
   705|             raise NotImplementedError()
   706|         else:
   707|             joined_room_ids = await self.get_rooms_for_user_at(
   708|                 user_id, now_token.room_key
   709|             )
   710|         sync_result_builder = SyncResultBuilder(
   711|             sync_config,
   712|             full_state,
   713|             since_token=since_token,
   714|             now_token=now_token,
   715|             joined_room_ids=joined_room_ids,
   716|         )
   717|         logger.debug("Fetching account data")
   718|         account_data_by_room = await self._generate_sync_entry_for_account_data(
   719|             sync_result_builder
   720|         )
   721|         logger.debug("Fetching room data")
   722|         res = await self._generate_sync_entry_for_rooms(
   723|             sync_result_builder, account_data_by_room
   724|         )
   725|         newly_joined_rooms, newly_joined_or_invited_users, _, _ = res
   726|         _, _, newly_left_rooms, newly_left_users = res
   727|         block_all_presence_data = (
   728|             since_token is None and sync_config.filter_collection.blocks_all_presence()

# --- HUNK 4: Lines 939-990 ---
   939|         self,
   940|         sync_result_builder: "SyncResultBuilder",
   941|         newly_joined_rooms: Set[str],
   942|         newly_joined_or_invited_users: Set[str],
   943|     ) -> None:
   944|         """Generates the presence portion of the sync response. Populates the
   945|         `sync_result_builder` with the result.
   946|         Args:
   947|             sync_result_builder
   948|             newly_joined_rooms: Set of rooms that the user has joined since
   949|                 the last sync (or empty if an initial sync)
   950|             newly_joined_or_invited_users: Set of users that have joined or
   951|                 been invited to rooms since the last sync (or empty if an
   952|                 initial sync)
   953|         """
   954|         now_token = sync_result_builder.now_token
   955|         sync_config = sync_result_builder.sync_config
   956|         user = sync_result_builder.sync_config.user
   957|         presence_source = self.event_sources.sources["presence"]
   958|         since_token = sync_result_builder.since_token
   959|         presence_key = None
   960|         include_offline = False
   961|         if since_token and not sync_result_builder.full_state:
   962|             presence_key = since_token.presence_key
   963|             include_offline = True
   964|         presence, presence_key = await presence_source.get_new_events(
   965|             user=user,
   966|             from_key=presence_key,
   967|             is_guest=sync_config.is_guest,
   968|             include_offline=include_offline,
   969|         )
   970|         assert presence_key
   971|         sync_result_builder.now_token = now_token.copy_and_replace(
   972|             "presence_key", presence_key
   973|         )
   974|         extra_users_ids = set(newly_joined_or_invited_users)
   975|         for room_id in newly_joined_rooms:
   976|             users = await self.state.get_current_users_in_room(room_id)
   977|             extra_users_ids.update(users)
   978|         extra_users_ids.discard(user.to_string())
   979|         if extra_users_ids:
   980|             states = await self.presence_handler.get_states(extra_users_ids)
   981|             presence.extend(states)
   982|             presence = list({p.user_id: p for p in presence}.values())
   983|         presence = sync_config.filter_collection.filter_presence(presence)
   984|         sync_result_builder.presence = presence
   985|     async def _generate_sync_entry_for_rooms(
   986|         self,
   987|         sync_result_builder: "SyncResultBuilder",
   988|         account_data_by_room: Dict[str, Dict[str, JsonDict]],
   989|     ) -> Tuple[Set[str], Set[str], Set[str], Set[str]]:
   990|         """Generates the rooms portion of the sync response. Populates the

# --- HUNK 5: Lines 1082-1122 ---
  1082|             set(newly_joined_rooms),
  1083|             newly_joined_or_invited_users,
  1084|             set(newly_left_rooms),
  1085|             newly_left_users,
  1086|         )
  1087|     async def _have_rooms_changed(
  1088|         self, sync_result_builder: "SyncResultBuilder"
  1089|     ) -> bool:
  1090|         """Returns whether there may be any new events that should be sent down
  1091|         the sync. Returns True if there are.
  1092|         """
  1093|         user_id = sync_result_builder.sync_config.user.to_string()
  1094|         since_token = sync_result_builder.since_token
  1095|         now_token = sync_result_builder.now_token
  1096|         assert since_token
  1097|         rooms_changed = await self.store.get_membership_changes_for_user(
  1098|             user_id, since_token.room_key, now_token.room_key
  1099|         )
  1100|         if rooms_changed:
  1101|             return True
  1102|         stream_id = since_token.room_key.stream
  1103|         for room_id in sync_result_builder.joined_room_ids:
  1104|             if self.store.has_room_changed_since(room_id, stream_id):
  1105|                 return True
  1106|         return False
  1107|     async def _get_rooms_changed(
  1108|         self, sync_result_builder: "SyncResultBuilder", ignored_users: Set[str]
  1109|     ) -> _RoomChanges:
  1110|         """Gets the the changes that have happened since the last sync.
  1111|         """
  1112|         user_id = sync_result_builder.sync_config.user.to_string()
  1113|         since_token = sync_result_builder.since_token
  1114|         now_token = sync_result_builder.now_token
  1115|         sync_config = sync_result_builder.sync_config
  1116|         assert since_token
  1117|         rooms_changed = await self.store.get_membership_changes_for_user(
  1118|             user_id, since_token.room_key, now_token.room_key
  1119|         )
  1120|         mem_change_events_by_room_id = {}  # type: Dict[str, List[EventBase]]
  1121|         for event in rooms_changed:
  1122|             mem_change_events_by_room_id.setdefault(event.room_id, []).append(event)

# --- HUNK 6: Lines 1172-1221 ---
  1172|                         old_mem_ev = None
  1173|                         if old_mem_ev_id:
  1174|                             old_mem_ev = await self.store.get_event(
  1175|                                 old_mem_ev_id, allow_none=True
  1176|                             )
  1177|                     if old_mem_ev and old_mem_ev.membership == Membership.JOIN:
  1178|                         newly_left_rooms.append(room_id)
  1179|             should_invite = non_joins[-1].membership == Membership.INVITE
  1180|             if should_invite:
  1181|                 if event.sender not in ignored_users:
  1182|                     room_sync = InvitedSyncResult(room_id, invite=non_joins[-1])
  1183|                     if room_sync:
  1184|                         invited.append(room_sync)
  1185|             leave_events = [
  1186|                 e
  1187|                 for e in non_joins
  1188|                 if e.membership in (Membership.LEAVE, Membership.BAN)
  1189|             ]
  1190|             if leave_events:
  1191|                 leave_event = leave_events[-1]
  1192|                 leave_position = await self.store.get_position_for_event(
  1193|                     leave_event.event_id
  1194|                 )
  1195|                 if since_token and not leave_position.persisted_after(
  1196|                     since_token.room_key
  1197|                 ):
  1198|                     continue
  1199|                 leave_token = since_token.copy_and_replace(
  1200|                     "room_key", leave_position.to_room_stream_token()
  1201|                 )
  1202|                 if leave_event.internal_metadata.is_out_of_band_membership():
  1203|                     batch_events = [leave_event]  # type: Optional[List[EventBase]]
  1204|                 else:
  1205|                     batch_events = None
  1206|                 room_entries.append(
  1207|                     RoomSyncResultBuilder(
  1208|                         room_id=room_id,
  1209|                         rtype="archived",
  1210|                         events=batch_events,
  1211|                         newly_joined=room_id in newly_joined_rooms,
  1212|                         full_state=False,
  1213|                         since_token=since_token,
  1214|                         upto_token=leave_token,
  1215|                     )
  1216|                 )
  1217|         timeline_limit = sync_config.filter_collection.timeline_limit()
  1218|         room_to_events = await self.store.get_room_events_stream_for_rooms(
  1219|             room_ids=sync_result_builder.joined_room_ids,
  1220|             from_key=since_token.room_key,
  1221|             to_key=now_token.room_key,

# --- HUNK 7: Lines 1284-1324 ---
  1284|                         room_id=event.room_id,
  1285|                         rtype="joined",
  1286|                         events=None,
  1287|                         newly_joined=False,
  1288|                         full_state=True,
  1289|                         since_token=since_token,
  1290|                         upto_token=now_token,
  1291|                     )
  1292|                 )
  1293|             elif event.membership == Membership.INVITE:
  1294|                 if event.sender in ignored_users:
  1295|                     continue
  1296|                 invite = await self.store.get_event(event.event_id)
  1297|                 invited.append(InvitedSyncResult(room_id=event.room_id, invite=invite))
  1298|             elif event.membership in (Membership.LEAVE, Membership.BAN):
  1299|                 if not sync_config.filter_collection.include_leave:
  1300|                     if event.membership == Membership.LEAVE:
  1301|                         if user_id == event.sender:
  1302|                             continue
  1303|                 leave_token = now_token.copy_and_replace(
  1304|                     "room_key", RoomStreamToken(None, event.stream_ordering)
  1305|                 )
  1306|                 room_entries.append(
  1307|                     RoomSyncResultBuilder(
  1308|                         room_id=event.room_id,
  1309|                         rtype="archived",
  1310|                         events=None,
  1311|                         newly_joined=False,
  1312|                         full_state=True,
  1313|                         since_token=since_token,
  1314|                         upto_token=leave_token,
  1315|                     )
  1316|                 )
  1317|         return _RoomChanges(room_entries, invited, [], [])
  1318|     async def _generate_room_entry(
  1319|         self,
  1320|         sync_result_builder: "SyncResultBuilder",
  1321|         ignored_users: Set[str],
  1322|         room_builder: "RoomSyncResultBuilder",
  1323|         ephemeral: List[JsonDict],
  1324|         tags: Optional[Dict[str, Dict[str, Any]]],

# --- HUNK 8: Lines 1417-1477 ---
  1417|                 room_sync.unread_count = notifs["unread_count"]
  1418|                 sync_result_builder.joined.append(room_sync)
  1419|             if batch.limited and since_token:
  1420|                 user_id = sync_result_builder.sync_config.user.to_string()
  1421|                 logger.debug(
  1422|                     "Incremental gappy sync of %s for user %s with %d state events"
  1423|                     % (room_id, user_id, len(state))
  1424|                 )
  1425|         elif room_builder.rtype == "archived":
  1426|             archived_room_sync = ArchivedSyncResult(
  1427|                 room_id=room_id,
  1428|                 timeline=batch,
  1429|                 state=state,
  1430|                 account_data=account_data_events,
  1431|             )
  1432|             if archived_room_sync or always_include:
  1433|                 sync_result_builder.archived.append(archived_room_sync)
  1434|         else:
  1435|             raise Exception("Unrecognized rtype: %r", room_builder.rtype)
  1436|     async def get_rooms_for_user_at(
  1437|         self, user_id: str, room_key: RoomStreamToken
  1438|     ) -> FrozenSet[str]:
  1439|         """Get set of joined rooms for a user at the given stream ordering.
  1440|         The stream ordering *must* be recent, otherwise this may throw an
  1441|         exception if older than a month. (This function is called with the
  1442|         current token, which should be perfectly fine).
  1443|         Args:
  1444|             user_id
  1445|             stream_ordering
  1446|         ReturnValue:
  1447|             Set of room_ids the user is in at given stream_ordering.
  1448|         """
  1449|         joined_rooms = await self.store.get_rooms_for_user_with_stream_ordering(user_id)
  1450|         joined_room_ids = set()
  1451|         for room_id, event_pos in joined_rooms:
  1452|             if not event_pos.persisted_after(room_key):
  1453|                 joined_room_ids.add(room_id)
  1454|                 continue
  1455|             logger.info("User joined room after current token: %s", room_id)
  1456|             extrems = await self.store.get_forward_extremeties_for_room(
  1457|                 room_id, event_pos.stream
  1458|             )
  1459|             users_in_room = await self.state.get_current_users_in_room(room_id, extrems)
  1460|             if user_id in users_in_room:
  1461|                 joined_room_ids.add(room_id)
  1462|         return frozenset(joined_room_ids)
  1463| def _action_has_highlight(actions: List[JsonDict]) -> bool:
  1464|     for action in actions:
  1465|         try:
  1466|             if action.get("set_tweak", None) == "highlight":
  1467|                 return action.get("value", True)
  1468|         except AttributeError:
  1469|             pass
  1470|     return False
  1471| def _calculate_state(
  1472|     timeline_contains: StateMap[str],
  1473|     timeline_start: StateMap[str],
  1474|     previous: StateMap[str],
  1475|     current: StateMap[str],
  1476|     lazy_load_members: bool,
  1477| ) -> StateMap[str]:

# --- HUNK 9: Lines 1488-1557 ---
  1488|     """
  1489|     event_id_to_key = {
  1490|         e: key
  1491|         for key, e in itertools.chain(
  1492|             timeline_contains.items(),
  1493|             previous.items(),
  1494|             timeline_start.items(),
  1495|             current.items(),
  1496|         )
  1497|     }
  1498|     c_ids = set(current.values())
  1499|     ts_ids = set(timeline_start.values())
  1500|     p_ids = set(previous.values())
  1501|     tc_ids = set(timeline_contains.values())
  1502|     if lazy_load_members:
  1503|         p_ids.difference_update(
  1504|             e for t, e in timeline_start.items() if t[0] == EventTypes.Member
  1505|         )
  1506|     state_ids = ((c_ids | ts_ids) - p_ids) - tc_ids
  1507|     return {event_id_to_key[e]: e for e in state_ids}
  1508| @attr.s(slots=True)
  1509| class SyncResultBuilder:
  1510|     """Used to help build up a new SyncResult for a user
  1511|     Attributes:
  1512|         sync_config
  1513|         full_state: The full_state flag as specified by user
  1514|         since_token: The token supplied by user, or None.
  1515|         now_token: The token to sync up to.
  1516|         joined_room_ids: List of rooms the user is joined to
  1517|         presence (list)
  1518|         account_data (list)
  1519|         joined (list[JoinedSyncResult])
  1520|         invited (list[InvitedSyncResult])
  1521|         archived (list[ArchivedSyncResult])
  1522|         groups (GroupsSyncResult|None)
  1523|         to_device (list)
  1524|     """
  1525|     sync_config = attr.ib(type=SyncConfig)
  1526|     full_state = attr.ib(type=bool)
  1527|     since_token = attr.ib(type=Optional[StreamToken])
  1528|     now_token = attr.ib(type=StreamToken)
  1529|     joined_room_ids = attr.ib(type=FrozenSet[str])
  1530|     presence = attr.ib(type=List[JsonDict], default=attr.Factory(list))
  1531|     account_data = attr.ib(type=List[JsonDict], default=attr.Factory(list))
  1532|     joined = attr.ib(type=List[JoinedSyncResult], default=attr.Factory(list))
  1533|     invited = attr.ib(type=List[InvitedSyncResult], default=attr.Factory(list))
  1534|     archived = attr.ib(type=List[ArchivedSyncResult], default=attr.Factory(list))
  1535|     groups = attr.ib(type=Optional[GroupsSyncResult], default=None)
  1536|     to_device = attr.ib(type=List[JsonDict], default=attr.Factory(list))
  1537| @attr.s(slots=True)
  1538| class RoomSyncResultBuilder:
  1539|     """Stores information needed to create either a `JoinedSyncResult` or
  1540|     `ArchivedSyncResult`.
  1541|     Attributes:
  1542|         room_id
  1543|         rtype: One of `"joined"` or `"archived"`
  1544|         events: List of events to include in the room (more events may be added
  1545|             when generating result).
  1546|         newly_joined: If the user has newly joined the room
  1547|         full_state: Whether the full state should be sent in result
  1548|         since_token: Earliest point to return events from, or None
  1549|         upto_token: Latest point to return events from.
  1550|     """
  1551|     room_id = attr.ib(type=str)
  1552|     rtype = attr.ib(type=str)
  1553|     events = attr.ib(type=Optional[List[EventBase]])
  1554|     newly_joined = attr.ib(type=bool)
  1555|     full_state = attr.ib(type=bool)
  1556|     since_token = attr.ib(type=Optional[StreamToken])
  1557|     upto_token = attr.ib(type=StreamToken)


# ====================================================================
# FILE: synapse/handlers/user_directory.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-38 ---
     1| import logging
     2| import synapse.metrics
     3| from synapse.api.constants import EventTypes, JoinRules, Membership
     4| from synapse.handlers.state_deltas import StateDeltasHandler
     5| from synapse.metrics.background_process_metrics import run_as_background_process
     6| from synapse.storage.roommember import ProfileInfo
     7| from synapse.util.metrics import Measure
     8| logger = logging.getLogger(__name__)
     9| class UserDirectoryHandler(StateDeltasHandler):
    10|     """Handles querying of and keeping updated the user_directory.
    11|     N.B.: ASSUMES IT IS THE ONLY THING THAT MODIFIES THE USER DIRECTORY
    12|     The user directory is filled with users who this server can see are joined to a
    13|     world_readable or publically joinable room. We keep a database table up to date
    14|     by streaming changes of the current state and recalculating whether users should
    15|     be in the directory or not when necessary.
    16|     """
    17|     def __init__(self, hs):
    18|         super().__init__(hs)
    19|         self.store = hs.get_datastore()
    20|         self.state = hs.get_state_handler()
    21|         self.server_name = hs.hostname
    22|         self.clock = hs.get_clock()
    23|         self.notifier = hs.get_notifier()
    24|         self.is_mine_id = hs.is_mine_id
    25|         self.update_user_directory = hs.config.update_user_directory
    26|         self.search_all_users = hs.config.user_directory_search_all_users
    27|         self.spam_checker = hs.get_spam_checker()
    28|         self.pos = None
    29|         self._is_processing = False
    30|         if self.update_user_directory:
    31|             self.notifier.add_replication_callback(self.notify_new_event)
    32|             self.clock.call_later(0, self.notify_new_event)
    33|     async def search_users(self, user_id, search_term, limit):
    34|         """Searches for users in directory
    35|         Returns:
    36|             dict of the form::
    37|                 {
    38|                     "limited": <bool>,  # whether there were more results or not


# ====================================================================
# FILE: synapse/http/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-24 ---
     1| import re
     2| from twisted.internet import task
     3| from twisted.web.client import FileBodyProducer
     4| from synapse.api.errors import SynapseError
     5| class RequestTimedOutError(SynapseError):
     6|     """Exception representing timeout of an outbound request"""
     7|     def __init__(self, msg):
     8|         super().__init__(504, msg)
     9| ACCESS_TOKEN_RE = re.compile(r"(\?.*access(_|%5[Ff])token=)[^&]*(.*)$")
    10| CLIENT_SECRET_RE = re.compile(r"(\?.*client(_|%5[Ff])secret=)[^&]*(.*)$")
    11| def redact_uri(uri):
    12|     """Strips sensitive information from the uri replaces with <redacted>"""
    13|     uri = ACCESS_TOKEN_RE.sub(r"\1<redacted>\3", uri)
    14|     return CLIENT_SECRET_RE.sub(r"\1<redacted>\3", uri)
    15| class QuieterFileBodyProducer(FileBodyProducer):
    16|     """Wrapper for FileBodyProducer that avoids CRITICAL errors when the connection drops.
    17|     Workaround for https://github.com/matrix-org/synapse/issues/4003 /
    18|     https://twistedmatrix.com/trac/ticket/6528
    19|     """
    20|     def stopProducing(self):
    21|         try:
    22|             FileBodyProducer.stopProducing(self)
    23|         except task.TaskStopped:
    24|             pass


# ====================================================================
# FILE: synapse/http/client.py
# Total hunks: 8
# ====================================================================
# --- HUNK 1: Lines 1-75 ---
     1| import logging
     2| import urllib
     3| from io import BytesIO
     4| from typing import (
     5|     Any,
     6|     BinaryIO,
     7|     Dict,
     8|     Iterable,
     9|     List,
    10|     Mapping,
    11|     Optional,
    12|     Sequence,
    13|     Tuple,
    14|     Union,
    15| )
    16| import treq
    17| from canonicaljson import encode_canonical_json
    18| from netaddr import IPAddress
    19| from prometheus_client import Counter
    20| from zope.interface import implementer, provider
    21| from OpenSSL import SSL
    22| from OpenSSL.SSL import VERIFY_NONE
    23| from twisted.internet import defer, error as twisted_error, protocol, ssl
    24| from twisted.internet.interfaces import (
    25|     IReactorPluggableNameResolver,
    26|     IResolutionReceiver,
    27| )
    28| from twisted.internet.task import Cooperator
    29| from twisted.python.failure import Failure
    30| from twisted.web._newclient import ResponseDone
    31| from twisted.web.client import (
    32|     Agent,
    33|     HTTPConnectionPool,
    34|     ResponseNeverReceived,
    35|     readBody,
    36| )
    37| from twisted.web.http import PotentialDataLoss
    38| from twisted.web.http_headers import Headers
    39| from twisted.web.iweb import IResponse
    40| from synapse.api.errors import Codes, HttpResponseException, SynapseError
    41| from synapse.http import QuieterFileBodyProducer, RequestTimedOutError, redact_uri
    42| from synapse.http.proxyagent import ProxyAgent
    43| from synapse.logging.context import make_deferred_yieldable
    44| from synapse.logging.opentracing import set_tag, start_active_span, tags
    45| from synapse.util import json_decoder
    46| from synapse.util.async_helpers import timeout_deferred
    47| logger = logging.getLogger(__name__)
    48| outgoing_requests_counter = Counter("synapse_http_client_requests", "", ["method"])
    49| incoming_responses_counter = Counter(
    50|     "synapse_http_client_responses", "", ["method", "code"]
    51| )
    52| RawHeaders = Union[Mapping[str, "RawHeaderValue"], Mapping[bytes, "RawHeaderValue"]]
    53| RawHeaderValue = Sequence[Union[str, bytes]]
    54| QueryParamValue = Union[str, bytes, Iterable[Union[str, bytes]]]
    55| QueryParams = Union[Mapping[str, QueryParamValue], Mapping[bytes, QueryParamValue]]
    56| def check_against_blacklist(ip_address, ip_whitelist, ip_blacklist):
    57|     """
    58|     Args:
    59|         ip_address (netaddr.IPAddress)
    60|         ip_whitelist (netaddr.IPSet)
    61|         ip_blacklist (netaddr.IPSet)
    62|     """
    63|     if ip_address in ip_blacklist:
    64|         if ip_whitelist is None or ip_address not in ip_whitelist:
    65|             return True
    66|     return False
    67| _EPSILON = 0.00000001
    68| def _make_scheduler(reactor):
    69|     """Makes a schedular suitable for a Cooperator using the given reactor.
    70|     (This is effectively just a copy from `twisted.internet.task`)
    71|     """
    72|     def _scheduler(x):
    73|         return reactor.callLater(_EPSILON, x)
    74|     return _scheduler
    75| class IPBlacklistingResolver:

# --- HUNK 2: Lines 204-544 ---
   204|         else:
   205|             self.reactor = hs.get_reactor()
   206|         pool = HTTPConnectionPool(self.reactor)
   207|         pool.maxPersistentPerHost = max((100 * hs.config.caches.global_factor, 5))
   208|         pool.cachedConnectionTimeout = 2 * 60
   209|         self.agent = ProxyAgent(
   210|             self.reactor,
   211|             connectTimeout=15,
   212|             contextFactory=self.hs.get_http_client_context_factory(),
   213|             pool=pool,
   214|             http_proxy=http_proxy,
   215|             https_proxy=https_proxy,
   216|         )
   217|         if self._ip_blacklist:
   218|             self.agent = BlacklistingAgentWrapper(
   219|                 self.agent,
   220|                 self.reactor,
   221|                 ip_whitelist=self._ip_whitelist,
   222|                 ip_blacklist=self._ip_blacklist,
   223|             )
   224|     async def request(
   225|         self,
   226|         method: str,
   227|         uri: str,
   228|         data: Optional[bytes] = None,
   229|         headers: Optional[Headers] = None,
   230|     ) -> IResponse:
   231|         """
   232|         Args:
   233|             method: HTTP method to use.
   234|             uri: URI to query.
   235|             data: Data to send in the request body, if applicable.
   236|             headers: Request headers.
   237|         Returns:
   238|             Response object, once the headers have been read.
   239|         Raises:
   240|             RequestTimedOutError if the request times out before the headers are read
   241|         """
   242|         outgoing_requests_counter.labels(method).inc()
   243|         logger.debug("Sending request %s %s", method, redact_uri(uri))
   244|         with start_active_span(
   245|             "outgoing-client-request",
   246|             tags={
   247|                 tags.SPAN_KIND: tags.SPAN_KIND_RPC_CLIENT,
   248|                 tags.HTTP_METHOD: method,
   249|                 tags.HTTP_URL: uri,
   250|             },
   251|             finish_on_close=True,
   252|         ):
   253|             try:
   254|                 body_producer = None
   255|                 if data is not None:
   256|                     body_producer = QuieterFileBodyProducer(
   257|                         BytesIO(data), cooperator=self._cooperator,
   258|                     )
   259|                 request_deferred = treq.request(
   260|                     method,
   261|                     uri,
   262|                     agent=self.agent,
   263|                     data=body_producer,
   264|                     headers=headers,
   265|                     **self._extra_treq_args
   266|                 )  # type: defer.Deferred
   267|                 request_deferred = timeout_deferred(
   268|                     request_deferred, 60, self.hs.get_reactor(),
   269|                 )
   270|                 request_deferred.addErrback(_timeout_to_request_timed_out_error)
   271|                 response = await make_deferred_yieldable(request_deferred)
   272|                 incoming_responses_counter.labels(method, response.code).inc()
   273|                 logger.info(
   274|                     "Received response to %s %s: %s",
   275|                     method,
   276|                     redact_uri(uri),
   277|                     response.code,
   278|                 )
   279|                 return response
   280|             except Exception as e:
   281|                 incoming_responses_counter.labels(method, "ERR").inc()
   282|                 logger.info(
   283|                     "Error sending request to  %s %s: %s %s",
   284|                     method,
   285|                     redact_uri(uri),
   286|                     type(e).__name__,
   287|                     e.args[0],
   288|                 )
   289|                 set_tag(tags.ERROR, True)
   290|                 set_tag("error_reason", e.args[0])
   291|                 raise
   292|     async def post_urlencoded_get_json(
   293|         self,
   294|         uri: str,
   295|         args: Mapping[str, Union[str, List[str]]] = {},
   296|         headers: Optional[RawHeaders] = None,
   297|     ) -> Any:
   298|         """
   299|         Args:
   300|             uri: uri to query
   301|             args: parameters to be url-encoded in the body
   302|             headers: a map from header name to a list of values for that header
   303|         Returns:
   304|             parsed json
   305|         Raises:
   306|             RequestTimedOutError: if there is a timeout before the response headers
   307|                are received. Note there is currently no timeout on reading the response
   308|                body.
   309|             HttpResponseException: On a non-2xx HTTP response.
   310|             ValueError: if the response was not JSON
   311|         """
   312|         logger.debug("post_urlencoded_get_json args: %s", args)
   313|         query_bytes = urllib.parse.urlencode(encode_urlencode_args(args), True).encode(
   314|             "utf8"
   315|         )
   316|         actual_headers = {
   317|             b"Content-Type": [b"application/x-www-form-urlencoded"],
   318|             b"User-Agent": [self.user_agent],
   319|             b"Accept": [b"application/json"],
   320|         }
   321|         if headers:
   322|             actual_headers.update(headers)
   323|         response = await self.request(
   324|             "POST", uri, headers=Headers(actual_headers), data=query_bytes
   325|         )
   326|         body = await make_deferred_yieldable(readBody(response))
   327|         if 200 <= response.code < 300:
   328|             return json_decoder.decode(body.decode("utf-8"))
   329|         else:
   330|             raise HttpResponseException(
   331|                 response.code, response.phrase.decode("ascii", errors="replace"), body
   332|             )
   333|     async def post_json_get_json(
   334|         self, uri: str, post_json: Any, headers: Optional[RawHeaders] = None
   335|     ) -> Any:
   336|         """
   337|         Args:
   338|             uri: URI to query.
   339|             post_json: request body, to be encoded as json
   340|             headers: a map from header name to a list of values for that header
   341|         Returns:
   342|             parsed json
   343|         Raises:
   344|             RequestTimedOutError: if there is a timeout before the response headers
   345|                are received. Note there is currently no timeout on reading the response
   346|                body.
   347|             HttpResponseException: On a non-2xx HTTP response.
   348|             ValueError: if the response was not JSON
   349|         """
   350|         json_str = encode_canonical_json(post_json)
   351|         logger.debug("HTTP POST %s -> %s", json_str, uri)
   352|         actual_headers = {
   353|             b"Content-Type": [b"application/json"],
   354|             b"User-Agent": [self.user_agent],
   355|             b"Accept": [b"application/json"],
   356|         }
   357|         if headers:
   358|             actual_headers.update(headers)
   359|         response = await self.request(
   360|             "POST", uri, headers=Headers(actual_headers), data=json_str
   361|         )
   362|         body = await make_deferred_yieldable(readBody(response))
   363|         if 200 <= response.code < 300:
   364|             return json_decoder.decode(body.decode("utf-8"))
   365|         else:
   366|             raise HttpResponseException(
   367|                 response.code, response.phrase.decode("ascii", errors="replace"), body
   368|             )
   369|     async def get_json(
   370|         self, uri: str, args: QueryParams = {}, headers: Optional[RawHeaders] = None,
   371|     ) -> Any:
   372|         """Gets some json from the given URI.
   373|         Args:
   374|             uri: The URI to request, not including query parameters
   375|             args: A dictionary used to create query string
   376|             headers: a map from header name to a list of values for that header
   377|         Returns:
   378|             Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.
   379|         Raises:
   380|             RequestTimedOutError: if there is a timeout before the response headers
   381|                are received. Note there is currently no timeout on reading the response
   382|                body.
   383|             HttpResponseException On a non-2xx HTTP response.
   384|             ValueError: if the response was not JSON
   385|         """
   386|         actual_headers = {b"Accept": [b"application/json"]}
   387|         if headers:
   388|             actual_headers.update(headers)
   389|         body = await self.get_raw(uri, args, headers=headers)
   390|         return json_decoder.decode(body.decode("utf-8"))
   391|     async def put_json(
   392|         self,
   393|         uri: str,
   394|         json_body: Any,
   395|         args: QueryParams = {},
   396|         headers: RawHeaders = None,
   397|     ) -> Any:
   398|         """Puts some json to the given URI.
   399|         Args:
   400|             uri: The URI to request, not including query parameters
   401|             json_body: The JSON to put in the HTTP body,
   402|             args: A dictionary used to create query strings
   403|             headers: a map from header name to a list of values for that header
   404|         Returns:
   405|             Succeeds when we get a 2xx HTTP response, with the HTTP body as JSON.
   406|         Raises:
   407|              RequestTimedOutError: if there is a timeout before the response headers
   408|                are received. Note there is currently no timeout on reading the response
   409|                body.
   410|             HttpResponseException On a non-2xx HTTP response.
   411|             ValueError: if the response was not JSON
   412|         """
   413|         if len(args):
   414|             query_bytes = urllib.parse.urlencode(args, True)
   415|             uri = "%s?%s" % (uri, query_bytes)
   416|         json_str = encode_canonical_json(json_body)
   417|         actual_headers = {
   418|             b"Content-Type": [b"application/json"],
   419|             b"User-Agent": [self.user_agent],
   420|             b"Accept": [b"application/json"],
   421|         }
   422|         if headers:
   423|             actual_headers.update(headers)
   424|         response = await self.request(
   425|             "PUT", uri, headers=Headers(actual_headers), data=json_str
   426|         )
   427|         body = await make_deferred_yieldable(readBody(response))
   428|         if 200 <= response.code < 300:
   429|             return json_decoder.decode(body.decode("utf-8"))
   430|         else:
   431|             raise HttpResponseException(
   432|                 response.code, response.phrase.decode("ascii", errors="replace"), body
   433|             )
   434|     async def get_raw(
   435|         self, uri: str, args: QueryParams = {}, headers: Optional[RawHeaders] = None
   436|     ) -> bytes:
   437|         """Gets raw text from the given URI.
   438|         Args:
   439|             uri: The URI to request, not including query parameters
   440|             args: A dictionary used to create query strings
   441|             headers: a map from header name to a list of values for that header
   442|         Returns:
   443|             Succeeds when we get a 2xx HTTP response, with the
   444|             HTTP body as bytes.
   445|         Raises:
   446|             RequestTimedOutError: if there is a timeout before the response headers
   447|                are received. Note there is currently no timeout on reading the response
   448|                body.
   449|             HttpResponseException on a non-2xx HTTP response.
   450|         """
   451|         if len(args):
   452|             query_bytes = urllib.parse.urlencode(args, True)
   453|             uri = "%s?%s" % (uri, query_bytes)
   454|         actual_headers = {b"User-Agent": [self.user_agent]}
   455|         if headers:
   456|             actual_headers.update(headers)
   457|         response = await self.request("GET", uri, headers=Headers(actual_headers))
   458|         body = await make_deferred_yieldable(readBody(response))
   459|         if 200 <= response.code < 300:
   460|             return body
   461|         else:
   462|             raise HttpResponseException(
   463|                 response.code, response.phrase.decode("ascii", errors="replace"), body
   464|             )
   465|     async def get_file(
   466|         self,
   467|         url: str,
   468|         output_stream: BinaryIO,
   469|         max_size: Optional[int] = None,
   470|         headers: Optional[RawHeaders] = None,
   471|     ) -> Tuple[int, Dict[bytes, List[bytes]], str, int]:
   472|         """GETs a file from a given URL
   473|         Args:
   474|             url: The URL to GET
   475|             output_stream: File to write the response body to.
   476|             headers: A map from header name to a list of values for that header
   477|         Returns:
   478|             A tuple of the file length, dict of the response
   479|             headers, absolute URI of the response and HTTP response code.
   480|         Raises:
   481|             RequestTimedOutError: if there is a timeout before the response headers
   482|                are received. Note there is currently no timeout on reading the response
   483|                body.
   484|             SynapseError: if the response is not a 2xx, the remote file is too large, or
   485|                another exception happens during the download.
   486|         """
   487|         actual_headers = {b"User-Agent": [self.user_agent]}
   488|         if headers:
   489|             actual_headers.update(headers)
   490|         response = await self.request("GET", url, headers=Headers(actual_headers))
   491|         resp_headers = dict(response.headers.getAllRawHeaders())
   492|         if (
   493|             b"Content-Length" in resp_headers
   494|             and int(resp_headers[b"Content-Length"][0]) > max_size
   495|         ):
   496|             logger.warning("Requested URL is too large > %r bytes" % (self.max_size,))
   497|             raise SynapseError(
   498|                 502,
   499|                 "Requested file is too large > %r bytes" % (self.max_size,),
   500|                 Codes.TOO_LARGE,
   501|             )
   502|         if response.code > 299:
   503|             logger.warning("Got %d when downloading %s" % (response.code, url))
   504|             raise SynapseError(502, "Got error %d" % (response.code,), Codes.UNKNOWN)
   505|         try:
   506|             length = await make_deferred_yieldable(
   507|                 _readBodyToFile(response, output_stream, max_size)
   508|             )
   509|         except SynapseError:
   510|             raise
   511|         except Exception as e:
   512|             raise SynapseError(502, ("Failed to download remote body: %s" % e)) from e
   513|         return (
   514|             length,
   515|             resp_headers,
   516|             response.request.absoluteURI.decode("ascii"),
   517|             response.code,
   518|         )
   519| def _timeout_to_request_timed_out_error(f: Failure):
   520|     if f.check(twisted_error.TimeoutError, twisted_error.ConnectingCancelledError):
   521|         raise RequestTimedOutError("Timeout connecting to remote server")
   522|     elif f.check(defer.TimeoutError, ResponseNeverReceived):
   523|         raise RequestTimedOutError("Timeout waiting for response from remote server")
   524|     return f
   525| class _ReadBodyToFileProtocol(protocol.Protocol):
   526|     def __init__(self, stream, deferred, max_size):
   527|         self.stream = stream
   528|         self.deferred = deferred
   529|         self.length = 0
   530|         self.max_size = max_size
   531|     def dataReceived(self, data):
   532|         self.stream.write(data)
   533|         self.length += len(data)
   534|         if self.max_size is not None and self.length >= self.max_size:
   535|             self.deferred.errback(
   536|                 SynapseError(
   537|                     502,
   538|                     "Requested file is too large > %r bytes" % (self.max_size,),
   539|                     Codes.TOO_LARGE,
   540|                 )
   541|             )
   542|             self.deferred = defer.Deferred()
   543|             self.transport.loseConnection()
   544|     def connectionLost(self, reason):


# ====================================================================
# FILE: synapse/http/federation/well_known_resolver.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 178-200 ---
   178|                 return int(max_age)
   179|             except ValueError:
   180|                 pass
   181|     expires = headers.getRawHeaders(b"expires")
   182|     if expires is not None:
   183|         try:
   184|             expires_date = stringToDatetime(expires[-1])
   185|             return expires_date - time_now()
   186|         except ValueError:
   187|             return 0
   188|     return None
   189| def _parse_cache_control(headers: Headers) -> Dict[bytes, Optional[bytes]]:
   190|     cache_controls = {}
   191|     for hdr in headers.getRawHeaders(b"cache-control", []):
   192|         for directive in hdr.split(b","):
   193|             splits = [x.strip() for x in directive.split(b"=", 1)]
   194|             k = splits[0].lower()
   195|             v = splits[1] if len(splits) > 1 else None
   196|             cache_controls[k] = v
   197|     return cache_controls
   198| @attr.s(slots=True)
   199| class _FetchWellKnownFailure(Exception):
   200|     temporary = attr.ib()


# ====================================================================
# FILE: synapse/http/matrixfederationclient.py
# Total hunks: 9
# ====================================================================
# --- HUNK 1: Lines 33-73 ---
    33| from synapse.logging.opentracing import (
    34|     inject_active_span_byte_dict,
    35|     set_tag,
    36|     start_active_span,
    37|     tags,
    38| )
    39| from synapse.util import json_decoder
    40| from synapse.util.async_helpers import timeout_deferred
    41| from synapse.util.metrics import Measure
    42| logger = logging.getLogger(__name__)
    43| outgoing_requests_counter = Counter(
    44|     "synapse_http_matrixfederationclient_requests", "", ["method"]
    45| )
    46| incoming_responses_counter = Counter(
    47|     "synapse_http_matrixfederationclient_responses", "", ["method", "code"]
    48| )
    49| MAX_LONG_RETRIES = 10
    50| MAX_SHORT_RETRIES = 3
    51| MAXINT = sys.maxsize
    52| _next_id = 1
    53| @attr.s(slots=True, frozen=True)
    54| class MatrixFederationRequest:
    55|     method = attr.ib()
    56|     """HTTP method
    57|     :type: str
    58|     """
    59|     path = attr.ib()
    60|     """HTTP path
    61|     :type: str
    62|     """
    63|     destination = attr.ib()
    64|     """The remote server to send the HTTP request to.
    65|     :type: str"""
    66|     json = attr.ib(default=None)
    67|     """JSON to send in the body.
    68|     :type: dict|None
    69|     """
    70|     json_callback = attr.ib(default=None)
    71|     """A callback to generate the JSON.
    72|     :type: func|None
    73|     """

# --- HUNK 2: Lines 108-148 ---
   108|     response: IResponse,
   109|     start_ms: int,
   110| ):
   111|     """
   112|     Reads the JSON body of a response, with a timeout
   113|     Args:
   114|         reactor: twisted reactor, for the timeout
   115|         timeout_sec: number of seconds to wait for response to complete
   116|         request: the request that triggered the response
   117|         response: response to the request
   118|         start_ms: Timestamp when request was made
   119|     Returns:
   120|         dict: parsed JSON response
   121|     """
   122|     try:
   123|         check_content_type_is_json(response.headers)
   124|         d = treq.text_content(response, encoding="utf-8")
   125|         d.addCallback(json_decoder.decode)
   126|         d = timeout_deferred(d, timeout=timeout_sec, reactor=reactor)
   127|         body = await make_deferred_yieldable(d)
   128|     except defer.TimeoutError as e:
   129|         logger.warning(
   130|             "{%s} [%s] Timed out reading response - %s %s",
   131|             request.txn_id,
   132|             request.destination,
   133|             request.method,
   134|             request.uri.decode("ascii"),
   135|         )
   136|         raise RequestSendFailed(e, can_retry=True) from e
   137|     except Exception as e:
   138|         logger.warning(
   139|             "{%s} [%s] Error reading response %s %s: %s",
   140|             request.txn_id,
   141|             request.destination,
   142|             request.method,
   143|             request.uri.decode("ascii"),
   144|             e,
   145|         )
   146|         raise
   147|     time_taken_secs = reactor.seconds() - start_ms / 1000
   148|     logger.info(

# --- HUNK 3: Lines 341-380 ---
   341|                         request.destination,
   342|                         request.method,
   343|                         url_str,
   344|                         _sec_timeout,
   345|                     )
   346|                     outgoing_requests_counter.labels(request.method).inc()
   347|                     try:
   348|                         with Measure(self.clock, "outbound_request"):
   349|                             request_deferred = self.agent.request(
   350|                                 method_bytes,
   351|                                 url_bytes,
   352|                                 headers=Headers(headers_dict),
   353|                                 bodyProducer=producer,
   354|                             )
   355|                             request_deferred = timeout_deferred(
   356|                                 request_deferred,
   357|                                 timeout=_sec_timeout,
   358|                                 reactor=self.reactor,
   359|                             )
   360|                             response = await request_deferred
   361|                     except DNSLookupError as e:
   362|                         raise RequestSendFailed(e, can_retry=retry_on_dns_fail) from e
   363|                     except Exception as e:
   364|                         raise RequestSendFailed(e, can_retry=True) from e
   365|                     incoming_responses_counter.labels(
   366|                         request.method, response.code
   367|                     ).inc()
   368|                     set_tag(tags.HTTP_STATUS_CODE, response.code)
   369|                     response_phrase = response.phrase.decode("ascii", errors="replace")
   370|                     if 200 <= response.code < 300:
   371|                         logger.debug(
   372|                             "{%s} [%s] Got response headers: %d %s",
   373|                             request.txn_id,
   374|                             request.destination,
   375|                             response.code,
   376|                             response_phrase,
   377|                         )
   378|                         pass
   379|                     else:
   380|                         logger.info(

# --- HUNK 4: Lines 493-611 ---
   493|         data={},
   494|         json_data_callback=None,
   495|         long_retries=False,
   496|         timeout=None,
   497|         ignore_backoff=False,
   498|         backoff_on_404=False,
   499|         try_trailing_slash_on_400=False,
   500|     ):
   501|         """ Sends the specifed json data using PUT
   502|         Args:
   503|             destination (str): The remote server to send the HTTP request
   504|                 to.
   505|             path (str): The HTTP path.
   506|             args (dict): query params
   507|             data (dict): A dict containing the data that will be used as
   508|                 the request body. This will be encoded as JSON.
   509|             json_data_callback (callable): A callable returning the dict to
   510|                 use as the request body.
   511|             long_retries (bool): whether to use the long retry algorithm. See
   512|                 docs on _send_request for details.
   513|             timeout (int|None): number of milliseconds to wait for the response.
   514|                 self._default_timeout (60s) by default.
   515|                 Note that we may make several attempts to send the request; this
   516|                 timeout applies to the time spent waiting for response headers for
   517|                 *each* attempt (including connection time) as well as the time spent
   518|                 reading the response body after a 200 response.
   519|             ignore_backoff (bool): true to ignore the historical backoff data
   520|                 and try the request anyway.
   521|             backoff_on_404 (bool): True if we should count a 404 response as
   522|                 a failure of the server (and should therefore back off future
   523|                 requests).
   524|             try_trailing_slash_on_400 (bool): True if on a 400 M_UNRECOGNIZED
   525|                 response we should try appending a trailing slash to the end
   526|                 of the request. Workaround for #3622 in Synapse <= v0.99.3. This
   527|                 will be attempted before backing off if backing off has been
   528|                 enabled.
   529|         Returns:
   530|             dict|list: Succeeds when we get a 2xx HTTP response. The
   531|             result will be the decoded JSON body.
   532|         Raises:
   533|             HttpResponseException: If we get an HTTP response code >= 300
   534|                 (except 429).
   535|             NotRetryingDestination: If we are not yet ready to retry this
   536|                 server.
   537|             FederationDeniedError: If this destination  is not on our
   538|                 federation whitelist
   539|             RequestSendFailed: If there were problems connecting to the
   540|                 remote, due to e.g. DNS failures, connection timeouts etc.
   541|         """
   542|         request = MatrixFederationRequest(
   543|             method="PUT",
   544|             destination=destination,
   545|             path=path,
   546|             query=args,
   547|             json_callback=json_data_callback,
   548|             json=data,
   549|         )
   550|         start_ms = self.clock.time_msec()
   551|         response = await self._send_request_with_optional_trailing_slash(
   552|             request,
   553|             try_trailing_slash_on_400,
   554|             backoff_on_404=backoff_on_404,
   555|             ignore_backoff=ignore_backoff,
   556|             long_retries=long_retries,
   557|             timeout=timeout,
   558|         )
   559|         if timeout is not None:
   560|             _sec_timeout = timeout / 1000
   561|         else:
   562|             _sec_timeout = self.default_timeout
   563|         body = await _handle_json_response(
   564|             self.reactor, _sec_timeout, request, response, start_ms
   565|         )
   566|         return body
   567|     async def post_json(
   568|         self,
   569|         destination,
   570|         path,
   571|         data={},
   572|         long_retries=False,
   573|         timeout=None,
   574|         ignore_backoff=False,
   575|         args={},
   576|     ):
   577|         """ Sends the specifed json data using POST
   578|         Args:
   579|             destination (str): The remote server to send the HTTP request
   580|                 to.
   581|             path (str): The HTTP path.
   582|             data (dict): A dict containing the data that will be used as
   583|                 the request body. This will be encoded as JSON.
   584|             long_retries (bool): whether to use the long retry algorithm. See
   585|                 docs on _send_request for details.
   586|             timeout (int|None): number of milliseconds to wait for the response.
   587|                 self._default_timeout (60s) by default.
   588|                 Note that we may make several attempts to send the request; this
   589|                 timeout applies to the time spent waiting for response headers for
   590|                 *each* attempt (including connection time) as well as the time spent
   591|                 reading the response body after a 200 response.
   592|             ignore_backoff (bool): true to ignore the historical backoff data and
   593|                 try the request anyway.
   594|             args (dict): query params
   595|         Returns:
   596|             dict|list: Succeeds when we get a 2xx HTTP response. The
   597|             result will be the decoded JSON body.
   598|         Raises:
   599|             HttpResponseException: If we get an HTTP response code >= 300
   600|                 (except 429).
   601|             NotRetryingDestination: If we are not yet ready to retry this
   602|                 server.
   603|             FederationDeniedError: If this destination  is not on our
   604|                 federation whitelist
   605|             RequestSendFailed: If there were problems connecting to the
   606|                 remote, due to e.g. DNS failures, connection timeouts etc.
   607|         """
   608|         request = MatrixFederationRequest(
   609|             method="POST", destination=destination, path=path, query=args, json=data
   610|         )
   611|         start_ms = self.clock.time_msec()

# --- HUNK 5: Lines 623-760 ---
   623|             self.reactor, _sec_timeout, request, response, start_ms,
   624|         )
   625|         return body
   626|     async def get_json(
   627|         self,
   628|         destination,
   629|         path,
   630|         args=None,
   631|         retry_on_dns_fail=True,
   632|         timeout=None,
   633|         ignore_backoff=False,
   634|         try_trailing_slash_on_400=False,
   635|     ):
   636|         """ GETs some json from the given host homeserver and path
   637|         Args:
   638|             destination (str): The remote server to send the HTTP request
   639|                 to.
   640|             path (str): The HTTP path.
   641|             args (dict|None): A dictionary used to create query strings, defaults to
   642|                 None.
   643|             timeout (int|None): number of milliseconds to wait for the response.
   644|                 self._default_timeout (60s) by default.
   645|                 Note that we may make several attempts to send the request; this
   646|                 timeout applies to the time spent waiting for response headers for
   647|                 *each* attempt (including connection time) as well as the time spent
   648|                 reading the response body after a 200 response.
   649|             ignore_backoff (bool): true to ignore the historical backoff data
   650|                 and try the request anyway.
   651|             try_trailing_slash_on_400 (bool): True if on a 400 M_UNRECOGNIZED
   652|                 response we should try appending a trailing slash to the end of
   653|                 the request. Workaround for #3622 in Synapse <= v0.99.3.
   654|         Returns:
   655|             dict|list: Succeeds when we get a 2xx HTTP response. The
   656|             result will be the decoded JSON body.
   657|         Raises:
   658|             HttpResponseException: If we get an HTTP response code >= 300
   659|                 (except 429).
   660|             NotRetryingDestination: If we are not yet ready to retry this
   661|                 server.
   662|             FederationDeniedError: If this destination  is not on our
   663|                 federation whitelist
   664|             RequestSendFailed: If there were problems connecting to the
   665|                 remote, due to e.g. DNS failures, connection timeouts etc.
   666|         """
   667|         request = MatrixFederationRequest(
   668|             method="GET", destination=destination, path=path, query=args
   669|         )
   670|         start_ms = self.clock.time_msec()
   671|         response = await self._send_request_with_optional_trailing_slash(
   672|             request,
   673|             try_trailing_slash_on_400,
   674|             backoff_on_404=False,
   675|             ignore_backoff=ignore_backoff,
   676|             retry_on_dns_fail=retry_on_dns_fail,
   677|             timeout=timeout,
   678|         )
   679|         if timeout is not None:
   680|             _sec_timeout = timeout / 1000
   681|         else:
   682|             _sec_timeout = self.default_timeout
   683|         body = await _handle_json_response(
   684|             self.reactor, _sec_timeout, request, response, start_ms
   685|         )
   686|         return body
   687|     async def delete_json(
   688|         self,
   689|         destination,
   690|         path,
   691|         long_retries=False,
   692|         timeout=None,
   693|         ignore_backoff=False,
   694|         args={},
   695|     ):
   696|         """Send a DELETE request to the remote expecting some json response
   697|         Args:
   698|             destination (str): The remote server to send the HTTP request
   699|                 to.
   700|             path (str): The HTTP path.
   701|             long_retries (bool): whether to use the long retry algorithm. See
   702|                 docs on _send_request for details.
   703|             timeout (int|None): number of milliseconds to wait for the response.
   704|                 self._default_timeout (60s) by default.
   705|                 Note that we may make several attempts to send the request; this
   706|                 timeout applies to the time spent waiting for response headers for
   707|                 *each* attempt (including connection time) as well as the time spent
   708|                 reading the response body after a 200 response.
   709|             ignore_backoff (bool): true to ignore the historical backoff data and
   710|                 try the request anyway.
   711|             args (dict): query params
   712|         Returns:
   713|             dict|list: Succeeds when we get a 2xx HTTP response. The
   714|             result will be the decoded JSON body.
   715|         Raises:
   716|             HttpResponseException: If we get an HTTP response code >= 300
   717|                 (except 429).
   718|             NotRetryingDestination: If we are not yet ready to retry this
   719|                 server.
   720|             FederationDeniedError: If this destination  is not on our
   721|                 federation whitelist
   722|             RequestSendFailed: If there were problems connecting to the
   723|                 remote, due to e.g. DNS failures, connection timeouts etc.
   724|         """
   725|         request = MatrixFederationRequest(
   726|             method="DELETE", destination=destination, path=path, query=args
   727|         )
   728|         start_ms = self.clock.time_msec()
   729|         response = await self._send_request(
   730|             request,
   731|             long_retries=long_retries,
   732|             timeout=timeout,
   733|             ignore_backoff=ignore_backoff,
   734|         )
   735|         if timeout is not None:
   736|             _sec_timeout = timeout / 1000
   737|         else:
   738|             _sec_timeout = self.default_timeout
   739|         body = await _handle_json_response(
   740|             self.reactor, _sec_timeout, request, response, start_ms
   741|         )
   742|         return body
   743|     async def get_file(
   744|         self,
   745|         destination,
   746|         path,
   747|         output_stream,
   748|         args={},
   749|         retry_on_dns_fail=True,
   750|         max_size=None,
   751|         ignore_backoff=False,
   752|     ):
   753|         """GETs a file from a given homeserver
   754|         Args:
   755|             destination (str): The remote server to send the HTTP request to.
   756|             path (str): The HTTP path to GET.
   757|             output_stream (file): File to write the response body to.
   758|             args (dict): Optional dictionary used to create the query string.
   759|             ignore_backoff (bool): true to ignore the historical backoff data
   760|                 and try the request anyway.


# ====================================================================
# FILE: synapse/http/proxyagent.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 3-46 ---
     3| from zope.interface import implementer
     4| from twisted.internet import defer
     5| from twisted.internet.endpoints import HostnameEndpoint, wrapClientTLS
     6| from twisted.python.failure import Failure
     7| from twisted.web.client import URI, BrowserLikePolicyForHTTPS, _AgentBase
     8| from twisted.web.error import SchemeNotSupported
     9| from twisted.web.iweb import IAgent
    10| from synapse.http.connectproxyclient import HTTPConnectProxyEndpoint
    11| logger = logging.getLogger(__name__)
    12| _VALID_URI = re.compile(br"\A[\x21-\x7e]+\Z")
    13| @implementer(IAgent)
    14| class ProxyAgent(_AgentBase):
    15|     """An Agent implementation which will use an HTTP proxy if one was requested
    16|     Args:
    17|         reactor: twisted reactor to place outgoing
    18|             connections.
    19|         contextFactory (IPolicyForHTTPS): A factory for TLS contexts, to control the
    20|             verification parameters of OpenSSL.  The default is to use a
    21|             `BrowserLikePolicyForHTTPS`, so unless you have special
    22|             requirements you can leave this as-is.
    23|         connectTimeout (Optional[float]): The amount of time that this Agent will wait
    24|             for the peer to accept a connection, in seconds. If 'None',
    25|             HostnameEndpoint's default (30s) will be used.
    26|             This is used for connections to both proxies and destination servers.
    27|         bindAddress (bytes): The local address for client sockets to bind to.
    28|         pool (HTTPConnectionPool|None): connection pool to be used. If None, a
    29|             non-persistent pool instance will be created.
    30|     """
    31|     def __init__(
    32|         self,
    33|         reactor,
    34|         contextFactory=BrowserLikePolicyForHTTPS(),
    35|         connectTimeout=None,
    36|         bindAddress=None,
    37|         pool=None,
    38|         http_proxy=None,
    39|         https_proxy=None,
    40|     ):
    41|         _AgentBase.__init__(self, reactor, pool)
    42|         self._endpoint_kwargs = {}
    43|         if connectTimeout is not None:
    44|             self._endpoint_kwargs["timeout"] = connectTimeout
    45|         if bindAddress is not None:
    46|             self._endpoint_kwargs["bindAddress"] = bindAddress

# --- HUNK 2: Lines 53-98 ---
    53|         self._policy_for_https = contextFactory
    54|         self._reactor = reactor
    55|     def request(self, method, uri, headers=None, bodyProducer=None):
    56|         """
    57|         Issue a request to the server indicated by the given uri.
    58|         Supports `http` and `https` schemes.
    59|         An existing connection from the connection pool may be used or a new one may be
    60|         created.
    61|         See also: twisted.web.iweb.IAgent.request
    62|         Args:
    63|             method (bytes): The request method to use, such as `GET`, `POST`, etc
    64|             uri (bytes): The location of the resource to request.
    65|             headers (Headers|None): Extra headers to send with the request
    66|             bodyProducer (IBodyProducer|None): An object which can generate bytes to
    67|                 make up the body of this request (for example, the properly encoded
    68|                 contents of a file for a file upload). Or, None if the request is to
    69|                 have no body.
    70|         Returns:
    71|             Deferred[IResponse]: completes when the header of the response has
    72|                  been received (regardless of the response status code).
    73|                  Can fail with:
    74|                     SchemeNotSupported: if the uri is not http or https
    75|                     twisted.internet.error.TimeoutError if the server we are connecting
    76|                         to (proxy or destination) does not accept a connection before
    77|                         connectTimeout.
    78|                     ... other things too.
    79|         """
    80|         uri = uri.strip()
    81|         if not _VALID_URI.match(uri):
    82|             raise ValueError("Invalid URI {!r}".format(uri))
    83|         parsed_uri = URI.fromBytes(uri)
    84|         pool_key = (parsed_uri.scheme, parsed_uri.host, parsed_uri.port)
    85|         request_path = parsed_uri.originForm
    86|         if parsed_uri.scheme == b"http" and self.http_proxy_endpoint:
    87|             pool_key = ("http-proxy", self.http_proxy_endpoint)
    88|             endpoint = self.http_proxy_endpoint
    89|             request_path = uri
    90|         elif parsed_uri.scheme == b"https" and self.https_proxy_endpoint:
    91|             endpoint = HTTPConnectProxyEndpoint(
    92|                 self._reactor,
    93|                 self.https_proxy_endpoint,
    94|                 parsed_uri.host,
    95|                 parsed_uri.port,
    96|             )
    97|         else:
    98|             endpoint = HostnameEndpoint(


# ====================================================================
# FILE: synapse/logging/context.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 10-51 ---
    10| import threading
    11| import types
    12| import warnings
    13| from typing import TYPE_CHECKING, Optional, Tuple, TypeVar, Union
    14| from typing_extensions import Literal
    15| from twisted.internet import defer, threads
    16| if TYPE_CHECKING:
    17|     from synapse.logging.scopecontextmanager import _LogContextScope
    18| logger = logging.getLogger(__name__)
    19| try:
    20|     import resource
    21|     RUSAGE_THREAD = 1
    22|     resource.getrusage(RUSAGE_THREAD)
    23|     is_thread_resource_usage_supported = True
    24|     def get_thread_resource_usage() -> "Optional[resource._RUsage]":
    25|         return resource.getrusage(RUSAGE_THREAD)
    26| except Exception:
    27|     is_thread_resource_usage_supported = False
    28|     def get_thread_resource_usage() -> "Optional[resource._RUsage]":
    29|         return None
    30| def logcontext_error(msg: str):
    31|     logger.warning(msg)
    32| get_thread_id = threading.get_ident
    33| class ContextResourceUsage:
    34|     """Object for tracking the resources used by a log context
    35|     Attributes:
    36|         ru_utime (float): user CPU time (in seconds)
    37|         ru_stime (float): system CPU time (in seconds)
    38|         db_txn_count (int): number of database transactions done
    39|         db_sched_duration_sec (float): amount of time spent waiting for a
    40|             database connection
    41|         db_txn_duration_sec (float): amount of time spent doing database
    42|             transactions (excluding scheduling time)
    43|         evt_db_fetch_count (int): number of events requested from the database
    44|     """
    45|     __slots__ = [
    46|         "ru_stime",
    47|         "ru_utime",
    48|         "db_txn_count",
    49|         "db_txn_duration_sec",
    50|         "db_sched_duration_sec",
    51|         "evt_db_fetch_count",

# --- HUNK 2: Lines 125-166 ---
   125|         self.request = None
   126|         self.scope = None
   127|         self.tag = None
   128|     def __str__(self):
   129|         return "sentinel"
   130|     def copy_to(self, record):
   131|         pass
   132|     def copy_to_twisted_log_entry(self, record):
   133|         record["request"] = None
   134|         record["scope"] = None
   135|     def start(self, rusage: "Optional[resource._RUsage]"):
   136|         pass
   137|     def stop(self, rusage: "Optional[resource._RUsage]"):
   138|         pass
   139|     def add_database_transaction(self, duration_sec):
   140|         pass
   141|     def add_database_scheduled(self, sched_sec):
   142|         pass
   143|     def record_event_fetch(self, event_count):
   144|         pass
   145|     def __bool__(self):
   146|         return False
   147| SENTINEL_CONTEXT = _Sentinel()
   148| class LoggingContext:
   149|     """Additional context for log formatting. Contexts are scoped within a
   150|     "with" block.
   151|     If a parent is given when creating a new context, then:
   152|         - logging fields are copied from the parent to the new context on entry
   153|         - when the new context exits, the cpu usage stats are copied from the
   154|           child to the parent
   155|     Args:
   156|         name (str): Name for the context for debugging.
   157|         parent_context (LoggingContext|None): The parent of the new context
   158|     """
   159|     __slots__ = [
   160|         "previous_context",
   161|         "name",
   162|         "parent_context",
   163|         "_resource_usage",
   164|         "usage_start",
   165|         "main_thread",
   166|         "finished",

# --- HUNK 3: Lines 208-316 ---
   208|     ) -> LoggingContextOrSentinel:
   209|         """Set the current logging context in thread local storage
   210|         This exists for backwards compatibility. ``set_current_context()`` should be
   211|         called directly.
   212|         Args:
   213|             context(LoggingContext): The context to activate.
   214|         Returns:
   215|             The context that was previously active
   216|         """
   217|         warnings.warn(
   218|             "synapse.logging.context.LoggingContext.set_current_context() is deprecated "
   219|             "in favor of synapse.logging.context.set_current_context().",
   220|             DeprecationWarning,
   221|             stacklevel=2,
   222|         )
   223|         return set_current_context(context)
   224|     def __enter__(self) -> "LoggingContext":
   225|         """Enters this logging context into thread local storage"""
   226|         old_context = set_current_context(self)
   227|         if self.previous_context != old_context:
   228|             logcontext_error(
   229|                 "Expected previous context %r, found %r"
   230|                 % (self.previous_context, old_context,)
   231|             )
   232|         return self
   233|     def __exit__(self, type, value, traceback) -> None:
   234|         """Restore the logging context in thread local storage to the state it
   235|         was before this context was entered.
   236|         Returns:
   237|             None to avoid suppressing any exceptions that were thrown.
   238|         """
   239|         current = set_current_context(self.previous_context)
   240|         if current is not self:
   241|             if current is SENTINEL_CONTEXT:
   242|                 logcontext_error("Expected logging context %s was lost" % (self,))
   243|             else:
   244|                 logcontext_error(
   245|                     "Expected logging context %s but found %s" % (self, current)
   246|                 )
   247|         self.finished = True
   248|     def copy_to(self, record) -> None:
   249|         """Copy logging fields from this context to a log record or
   250|         another LoggingContext
   251|         """
   252|         record.request = self.request
   253|         record.scope = self.scope
   254|     def copy_to_twisted_log_entry(self, record) -> None:
   255|         """
   256|         Copy logging fields from this context to a Twisted log record.
   257|         """
   258|         record["request"] = self.request
   259|         record["scope"] = self.scope
   260|     def start(self, rusage: "Optional[resource._RUsage]") -> None:
   261|         """
   262|         Record that this logcontext is currently running.
   263|         This should not be called directly: use set_current_context
   264|         Args:
   265|             rusage: the resources used by the current thread, at the point of
   266|                 switching to this logcontext. May be None if this platform doesn't
   267|                 support getrusuage.
   268|         """
   269|         if get_thread_id() != self.main_thread:
   270|             logcontext_error("Started logcontext %s on different thread" % (self,))
   271|             return
   272|         if self.finished:
   273|             logcontext_error("Re-starting finished log context %s" % (self,))
   274|         if self.usage_start:
   275|             logcontext_error("Re-starting already-active log context %s" % (self,))
   276|         else:
   277|             self.usage_start = rusage
   278|     def stop(self, rusage: "Optional[resource._RUsage]") -> None:
   279|         """
   280|         Record that this logcontext is no longer running.
   281|         This should not be called directly: use set_current_context
   282|         Args:
   283|             rusage: the resources used by the current thread, at the point of
   284|                 switching away from this logcontext. May be None if this platform
   285|                 doesn't support getrusuage.
   286|         """
   287|         try:
   288|             if get_thread_id() != self.main_thread:
   289|                 logcontext_error("Stopped logcontext %s on different thread" % (self,))
   290|                 return
   291|             if not rusage:
   292|                 return
   293|             if not self.usage_start:
   294|                 logcontext_error(
   295|                     "Called stop on logcontext %s without recording a start rusage"
   296|                     % (self,)
   297|                 )
   298|                 return
   299|             utime_delta, stime_delta = self._get_cputime(rusage)
   300|             self.add_cputime(utime_delta, stime_delta)
   301|         finally:
   302|             self.usage_start = None
   303|     def get_resource_usage(self) -> ContextResourceUsage:
   304|         """Get resources used by this logcontext so far.
   305|         Returns:
   306|             ContextResourceUsage: a *copy* of the object tracking resource
   307|                 usage so far
   308|         """
   309|         res = self._resource_usage.copy()
   310|         is_main_thread = get_thread_id() == self.main_thread
   311|         if self.usage_start and is_main_thread:
   312|             rusage = get_thread_resource_usage()
   313|             assert rusage is not None
   314|             utime_delta, stime_delta = self._get_cputime(rusage)
   315|             res.ru_utime += utime_delta
   316|             res.ru_stime += stime_delta

# --- HUNK 4: Lines 395-441 ---
   395|         context = current_context()
   396|         for key, value in self.defaults.items():
   397|             setattr(record, key, value)
   398|         if context is not None:
   399|             context.copy_to(record)
   400|         return True
   401| class PreserveLoggingContext:
   402|     """Context manager which replaces the logging context
   403|      The previous logging context is restored on exit."""
   404|     __slots__ = ["_old_context", "_new_context"]
   405|     def __init__(
   406|         self, new_context: LoggingContextOrSentinel = SENTINEL_CONTEXT
   407|     ) -> None:
   408|         self._new_context = new_context
   409|     def __enter__(self) -> None:
   410|         self._old_context = set_current_context(self._new_context)
   411|     def __exit__(self, type, value, traceback) -> None:
   412|         context = set_current_context(self._old_context)
   413|         if context != self._new_context:
   414|             if not context:
   415|                 logcontext_error(
   416|                     "Expected logging context %s was lost" % (self._new_context,)
   417|                 )
   418|             else:
   419|                 logcontext_error(
   420|                     "Expected logging context %s but found %s"
   421|                     % (self._new_context, context,)
   422|                 )
   423| _thread_local = threading.local()
   424| _thread_local.current_context = SENTINEL_CONTEXT
   425| def current_context() -> LoggingContextOrSentinel:
   426|     """Get the current logging context from thread local storage"""
   427|     return getattr(_thread_local, "current_context", SENTINEL_CONTEXT)
   428| def set_current_context(context: LoggingContextOrSentinel) -> LoggingContextOrSentinel:
   429|     """Set the current logging context in thread local storage
   430|     Args:
   431|         context(LoggingContext): The context to activate.
   432|     Returns:
   433|         The context that was previously active
   434|     """
   435|     if context is None:
   436|         raise TypeError("'context' argument may not be None")
   437|     current = current_context()
   438|     if current is not context:
   439|         rusage = get_thread_resource_usage()
   440|         current.stop(rusage)
   441|         _thread_local.current_context = context


# ====================================================================
# FILE: synapse/logging/formatter.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| import logging
     2| import traceback
     3| from io import StringIO
     4| class LogFormatter(logging.Formatter):
     5|     """Log formatter which gives more detail for exceptions
     6|     This is the same as the standard log formatter, except that when logging
     7|     exceptions [typically via log.foo("msg", exc_info=1)], it prints the
     8|     sequence that led up to the point at which the exception was caught.
     9|     (Normally only stack frames between the point the exception was raised and
    10|     where it was caught are logged).
    11|     """
    12|     def __init__(self, *args, **kwargs):
    13|         super().__init__(*args, **kwargs)
    14|     def formatException(self, ei):
    15|         sio = StringIO()
    16|         (typ, val, tb) = ei
    17|         if tb and hasattr(tb.tb_frame, "f_back"):
    18|             sio.write("Capture point (most recent call last):\n")
    19|             traceback.print_stack(tb.tb_frame.f_back, None, sio)
    20|         traceback.print_exception(typ, val, tb, None, sio)
    21|         s = sio.getvalue()
    22|         sio.close()
    23|         if s[-1:] == "\n":
    24|             s = s[:-1]
    25|         return s


# ====================================================================
# FILE: synapse/logging/scopecontextmanager.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 55-97 ---
    55|     A custom opentracing scope. The only significant difference is that it will
    56|     close the log context it's related to if the logcontext was created specifically
    57|     for this scope.
    58|     """
    59|     def __init__(self, manager, span, logcontext, enter_logcontext, finish_on_close):
    60|         """
    61|         Args:
    62|             manager (LogContextScopeManager):
    63|                 the manager that is responsible for this scope.
    64|             span (Span):
    65|                 the opentracing span which this scope represents the local
    66|                 lifetime for.
    67|             logcontext (LogContext):
    68|                 the logcontext to which this scope is attached.
    69|             enter_logcontext (Boolean):
    70|                 if True the logcontext will be entered and exited when the scope
    71|                 is entered and exited respectively
    72|             finish_on_close (Boolean):
    73|                 if True finish the span when the scope is closed
    74|         """
    75|         super().__init__(manager, span)
    76|         self.logcontext = logcontext
    77|         self._finish_on_close = finish_on_close
    78|         self._enter_logcontext = enter_logcontext
    79|     def __enter__(self):
    80|         if self._enter_logcontext:
    81|             self.logcontext.__enter__()
    82|         return self
    83|     def __exit__(self, type, value, traceback):
    84|         if type == twisted.internet.defer._DefGen_Return:
    85|             super().__exit__(None, None, None)
    86|         else:
    87|             super().__exit__(type, value, traceback)
    88|         if self._enter_logcontext:
    89|             self.logcontext.__exit__(type, value, traceback)
    90|         else:  # the logcontext existed before the creation of the scope
    91|             self.logcontext.scope = None
    92|     def close(self):
    93|         if self.manager.active is not self:
    94|             logger.error("Tried to close a non-active scope!")
    95|             return
    96|         if self._finish_on_close:
    97|             self.span.finish()


# ====================================================================
# FILE: synapse/logging/utils.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-35 ---
     1| import logging
     2| from functools import wraps
     3| from inspect import getcallargs
     4| _TIME_FUNC_ID = 0
     5| def _log_debug_as_f(f, msg, msg_args):
     6|     name = f.__module__
     7|     logger = logging.getLogger(name)
     8|     if logger.isEnabledFor(logging.DEBUG):
     9|         lineno = f.__code__.co_firstlineno
    10|         pathname = f.__code__.co_filename
    11|         record = logger.makeRecord(
    12|             name=name,
    13|             level=logging.DEBUG,
    14|             fn=pathname,
    15|             lno=lineno,
    16|             msg=msg,
    17|             args=msg_args,
    18|             exc_info=None,
    19|         )
    20|         logger.handle(record)
    21| def log_function(f):
    22|     """ Function decorator that logs every call to that function.
    23|     """
    24|     func_name = f.__name__
    25|     @wraps(f)
    26|     def wrapped(*args, **kwargs):
    27|         name = f.__module__
    28|         logger = logging.getLogger(name)
    29|         level = logging.DEBUG
    30|         if logger.isEnabledFor(level):
    31|             bound_args = getcallargs(f, *args, **kwargs)
    32|             def format(value):
    33|                 r = str(value)
    34|                 if len(r) > 50:
    35|                     r = r[:50] + "..."


# ====================================================================
# FILE: synapse/metrics/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-57 ---
     1| import functools
     2| import gc
     3| import itertools
     4| import logging
     5| import os
     6| import platform
     7| import threading
     8| import time
     9| from typing import Callable, Dict, Iterable, Optional, Tuple, Union
    10| import attr
    11| from prometheus_client import Counter, Gauge, Histogram
    12| from prometheus_client.core import (
    13|     REGISTRY,
    14|     CounterMetricFamily,
    15|     GaugeHistogramMetricFamily,
    16|     GaugeMetricFamily,
    17| )
    18| from twisted.internet import reactor
    19| import synapse
    20| from synapse.metrics._exposition import (
    21|     MetricsResource,
    22|     generate_latest,
    23|     start_http_server,
    24| )
    25| from synapse.util.versionstring import get_version_string
    26| logger = logging.getLogger(__name__)
    27| METRICS_PREFIX = "/_synapse/metrics"
    28| running_on_pypy = platform.python_implementation() == "PyPy"
    29| all_gauges = {}  # type: Dict[str, Union[LaterGauge, InFlightGauge]]
    30| HAVE_PROC_SELF_STAT = os.path.exists("/proc/self/stat")
    31| class RegistryProxy:
    32|     @staticmethod
    33|     def collect():
    34|         for metric in REGISTRY.collect():
    35|             if not metric.name.startswith("__"):
    36|                 yield metric
    37| @attr.s(slots=True, hash=True)
    38| class LaterGauge:
    39|     name = attr.ib(type=str)
    40|     desc = attr.ib(type=str)
    41|     labels = attr.ib(hash=False, type=Optional[Iterable[str]])
    42|     caller = attr.ib(type=Callable[[], Union[Dict[Tuple[str, ...], float], float]])
    43|     def collect(self):
    44|         g = GaugeMetricFamily(self.name, self.desc, labels=self.labels)
    45|         try:
    46|             calls = self.caller()
    47|         except Exception:
    48|             logger.exception("Exception running callback for LaterGauge(%s)", self.name)
    49|             yield g
    50|             return
    51|         if isinstance(calls, dict):
    52|             for k, v in calls.items():
    53|                 g.add_metric(k, v)
    54|         else:
    55|             g.add_metric([], calls)
    56|         yield g
    57|     def __attrs_post_init__(self):

# --- HUNK 2: Lines 119-215 ---
   119|                 callbacks = set(self._registrations[key])
   120|             in_flight.add_metric(key, len(callbacks))
   121|             metrics = self._metrics_class()
   122|             metrics_by_key[key] = metrics
   123|             for callback in callbacks:
   124|                 callback(metrics)
   125|         yield in_flight
   126|         for name in self.sub_metrics:
   127|             gauge = GaugeMetricFamily(
   128|                 "_".join([self.name, name]), "", labels=self.labels
   129|             )
   130|             for key, metrics in metrics_by_key.items():
   131|                 gauge.add_metric(key, getattr(metrics, name))
   132|             yield gauge
   133|     def _register_with_collector(self):
   134|         if self.name in all_gauges.keys():
   135|             logger.warning("%s already registered, reregistering" % (self.name,))
   136|             REGISTRY.unregister(all_gauges.pop(self.name))
   137|         REGISTRY.register(self)
   138|         all_gauges[self.name] = self
   139| class GaugeBucketCollector:
   140|     """Like a Histogram, but the buckets are Gauges which are updated atomically.
   141|     The data is updated by calling `update_data` with an iterable of measurements.
   142|     We assume that the data is updated less frequently than it is reported to
   143|     Prometheus, and optimise for that case.
   144|     """
   145|     __slots__ = ("_name", "_documentation", "_bucket_bounds", "_metric")
   146|     def __init__(
   147|         self,
   148|         name: str,
   149|         documentation: str,
   150|         buckets: Iterable[float],
   151|         registry=REGISTRY,
   152|     ):
   153|         """
   154|         Args:
   155|             name: base name of metric to be exported to Prometheus. (a _bucket suffix
   156|                will be added.)
   157|             documentation: help text for the metric
   158|             buckets: The top bounds of the buckets to report
   159|             registry: metric registry to register with
   160|         """
   161|         self._name = name
   162|         self._documentation = documentation
   163|         self._bucket_bounds = [float(b) for b in buckets]
   164|         if self._bucket_bounds != sorted(self._bucket_bounds):
   165|             raise ValueError("Buckets not in sorted order")
   166|         if self._bucket_bounds[-1] != float("inf"):
   167|             self._bucket_bounds.append(float("inf"))
   168|         self._metric = self._values_to_metric([])
   169|         registry.register(self)
   170|     def collect(self):
   171|         yield self._metric
   172|     def update_data(self, values: Iterable[float]):
   173|         """Update the data to be reported by the metric
   174|         The existing data is cleared, and each measurement in the input is assigned
   175|         to the relevant bucket.
   176|         """
   177|         self._metric = self._values_to_metric(values)
   178|     def _values_to_metric(self, values: Iterable[float]) -> GaugeHistogramMetricFamily:
   179|         total = 0.0
   180|         bucket_values = [0 for _ in self._bucket_bounds]
   181|         for v in values:
   182|             for i, bound in enumerate(self._bucket_bounds):
   183|                 if v <= bound:
   184|                     bucket_values[i] += 1
   185|                     break
   186|             total += v
   187|         accumulated_values = itertools.accumulate(bucket_values)
   188|         return GaugeHistogramMetricFamily(
   189|             self._name,
   190|             self._documentation,
   191|             buckets=list(
   192|                 zip((str(b) for b in self._bucket_bounds), accumulated_values)
   193|             ),
   194|             gsum_value=total,
   195|         )
   196| class CPUMetrics:
   197|     def __init__(self):
   198|         ticks_per_sec = 100
   199|         try:
   200|             ticks_per_sec = os.sysconf("SC_CLK_TCK")
   201|         except (ValueError, TypeError, AttributeError):
   202|             pass
   203|         self.ticks_per_sec = ticks_per_sec
   204|     def collect(self):
   205|         if not HAVE_PROC_SELF_STAT:
   206|             return
   207|         with open("/proc/self/stat") as s:
   208|             line = s.read()
   209|             raw_stats = line.split(") ", 1)[1].split(" ")
   210|             user = GaugeMetricFamily("process_cpu_user_seconds_total", "")
   211|             user.add_metric([], float(raw_stats[11]) / self.ticks_per_sec)
   212|             yield user
   213|             sys = GaugeMetricFamily("process_cpu_system_seconds_total", "")
   214|             sys.add_metric([], float(raw_stats[12]) / self.ticks_per_sec)
   215|             yield sys


# ====================================================================
# FILE: synapse/metrics/_exposition.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-145 ---
     1| """
     2| This code is based off `prometheus_client/exposition.py` from version 0.7.1.
     3| Due to the renaming of metrics in prometheus_client 0.4.0, this customised
     4| vendoring of the code will emit both the old versions that Synapse dashboards
     5| expect, and the newer "best practice" version of the up-to-date official client.
     6| """
     7| import math
     8| import threading
     9| from http.server import BaseHTTPRequestHandler, HTTPServer
    10| from socketserver import ThreadingMixIn
    11| from typing import Dict, List
    12| from urllib.parse import parse_qs, urlparse
    13| from prometheus_client import REGISTRY
    14| from twisted.web.resource import Resource
    15| from synapse.util import caches
    16| CONTENT_TYPE_LATEST = str("text/plain; version=0.0.4; charset=utf-8")
    17| INF = float("inf")
    18| MINUS_INF = float("-inf")
    19| def floatToGoString(d):
    20|     d = float(d)
    21|     if d == INF:
    22|         return "+Inf"
    23|     elif d == MINUS_INF:
    24|         return "-Inf"
    25|     elif math.isnan(d):
    26|         return "NaN"
    27|     else:
    28|         s = repr(d)
    29|         dot = s.find(".")
    30|         if d > 0 and dot > 6:
    31|             mantissa = "{0}.{1}{2}".format(s[0], s[1:dot], s[dot + 1 :]).rstrip("0.")
    32|             return "{0}e+0{1}".format(mantissa, dot - 1)
    33|         return s
    34| def sample_line(line, name):
    35|     if line.labels:
    36|         labelstr = "{{{0}}}".format(
    37|             ",".join(
    38|                 [
    39|                     '{0}="{1}"'.format(
    40|                         k,
    41|                         v.replace("\\", r"\\").replace("\n", r"\n").replace('"', r"\""),
    42|                     )
    43|                     for k, v in sorted(line.labels.items())
    44|                 ]
    45|             )
    46|         )
    47|     else:
    48|         labelstr = ""
    49|     timestamp = ""
    50|     if line.timestamp is not None:
    51|         timestamp = " {0:d}".format(int(float(line.timestamp) * 1000))
    52|     return "{0}{1} {2}{3}\n".format(
    53|         name, labelstr, floatToGoString(line.value), timestamp
    54|     )
    55| def generate_latest(registry, emit_help=False):
    56|     for collector in caches.collectors_by_name.values():
    57|         collector.collect()
    58|     output = []
    59|     for metric in registry.collect():
    60|         if not metric.samples:
    61|             continue
    62|         mname = metric.name
    63|         mnewname = metric.name
    64|         mtype = metric.type
    65|         if mtype == "counter":
    66|             mnewname = mnewname + "_total"
    67|         elif mtype == "info":
    68|             mtype = "gauge"
    69|             mnewname = mnewname + "_info"
    70|         elif mtype == "stateset":
    71|             mtype = "gauge"
    72|         elif mtype == "gaugehistogram":
    73|             mtype = "histogram"
    74|         elif mtype == "unknown":
    75|             mtype = "untyped"
    76|         if emit_help:
    77|             output.append(
    78|                 "# HELP {0} {1}\n".format(
    79|                     mname,
    80|                     metric.documentation.replace("\\", r"\\").replace("\n", r"\n"),
    81|                 )
    82|             )
    83|         output.append("# TYPE {0} {1}\n".format(mname, mtype))
    84|         om_samples = {}  # type: Dict[str, List[str]]
    85|         for s in metric.samples:
    86|             for suffix in ["_created", "_gsum", "_gcount"]:
    87|                 if s.name == metric.name + suffix:
    88|                     om_samples.setdefault(suffix, []).append(sample_line(s, s.name))
    89|                     break
    90|             else:
    91|                 newname = s.name.replace(mnewname, mname)
    92|                 if ":" in newname and newname.endswith("_total"):
    93|                     newname = newname[: -len("_total")]
    94|                 output.append(sample_line(s, newname))
    95|         for suffix, lines in sorted(om_samples.items()):
    96|             if emit_help:
    97|                 output.append(
    98|                     "# HELP {0}{1} {2}\n".format(
    99|                         metric.name,
   100|                         suffix,
   101|                         metric.documentation.replace("\\", r"\\").replace("\n", r"\n"),
   102|                     )
   103|                 )
   104|             output.append("# TYPE {0}{1} gauge\n".format(metric.name, suffix))
   105|             output.extend(lines)
   106|         if mtype == "counter":
   107|             mnewname = mnewname.replace(":total", "")
   108|         mnewname = mnewname.replace(":", "_")
   109|         if mname == mnewname:
   110|             continue
   111|         if emit_help:
   112|             output.append(
   113|                 "# HELP {0} {1}\n".format(
   114|                     mnewname,
   115|                     metric.documentation.replace("\\", r"\\").replace("\n", r"\n"),
   116|                 )
   117|             )
   118|         output.append("# TYPE {0} {1}\n".format(mnewname, mtype))
   119|         for s in metric.samples:
   120|             for suffix in ["_created", "_gsum", "_gcount"]:
   121|                 if s.name == metric.name + suffix:
   122|                     break
   123|             else:
   124|                 output.append(
   125|                     sample_line(s, s.name.replace(":total", "").replace(":", "_"))
   126|                 )
   127|     return "".join(output).encode("utf-8")
   128| class MetricsHandler(BaseHTTPRequestHandler):
   129|     """HTTP handler that gives metrics from ``REGISTRY``."""
   130|     registry = REGISTRY
   131|     def do_GET(self):
   132|         registry = self.registry
   133|         params = parse_qs(urlparse(self.path).query)
   134|         if "help" in params:
   135|             emit_help = True
   136|         else:
   137|             emit_help = False
   138|         try:
   139|             output = generate_latest(registry, emit_help=emit_help)
   140|         except Exception:
   141|             self.send_error(500, "error generating metric output")
   142|             raise
   143|         self.send_response(200)
   144|         self.send_header("Content-Type", CONTENT_TYPE_LATEST)
   145|         self.send_header("Content-Length", str(len(output)))


# ====================================================================
# FILE: synapse/notifier.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 7-53 ---
     7|     Iterable,
     8|     List,
     9|     Optional,
    10|     Set,
    11|     Tuple,
    12|     TypeVar,
    13|     Union,
    14| )
    15| from prometheus_client import Counter
    16| from twisted.internet import defer
    17| import synapse.server
    18| from synapse.api.constants import EventTypes, Membership
    19| from synapse.api.errors import AuthError
    20| from synapse.events import EventBase
    21| from synapse.handlers.presence import format_user_presence_state
    22| from synapse.logging.context import PreserveLoggingContext
    23| from synapse.logging.utils import log_function
    24| from synapse.metrics import LaterGauge
    25| from synapse.metrics.background_process_metrics import run_as_background_process
    26| from synapse.streams.config import PaginationConfig
    27| from synapse.types import (
    28|     Collection,
    29|     PersistedEventPosition,
    30|     RoomStreamToken,
    31|     StreamToken,
    32|     UserID,
    33| )
    34| from synapse.util.async_helpers import ObservableDeferred, timeout_deferred
    35| from synapse.util.metrics import Measure
    36| from synapse.visibility import filter_events_for_client
    37| logger = logging.getLogger(__name__)
    38| notified_events_counter = Counter("synapse_notifier_notified_events", "")
    39| users_woken_by_stream_counter = Counter(
    40|     "synapse_notifier_users_woken_by_stream", "", ["stream"]
    41| )
    42| T = TypeVar("T")
    43| def count(func: Callable[[T], bool], it: Iterable[T]) -> int:
    44|     """Return the number of items in it for which func returns true."""
    45|     n = 0
    46|     for x in it:
    47|         if func(x):
    48|             n += 1
    49|     return n
    50| class _NotificationListener:
    51|     """ This represents a single client connection to the events stream.
    52|     The events stream handler will have yielded to the deferred, so to
    53|     notify the handler it is sufficient to resolve the deferred.

# --- HUNK 2: Lines 60-268 ---
    60|     It tracks the most recent stream token for that user.
    61|     At a given point a user may have a number of streams listening for
    62|     events.
    63|     This listener will also keep track of which rooms it is listening in
    64|     so that it can remove itself from the indexes in the Notifier class.
    65|     """
    66|     def __init__(
    67|         self,
    68|         user_id: str,
    69|         rooms: Collection[str],
    70|         current_token: StreamToken,
    71|         time_now_ms: int,
    72|     ):
    73|         self.user_id = user_id
    74|         self.rooms = set(rooms)
    75|         self.current_token = current_token
    76|         self.last_notified_token = current_token
    77|         self.last_notified_ms = time_now_ms
    78|         with PreserveLoggingContext():
    79|             self.notify_deferred = ObservableDeferred(defer.Deferred())
    80|     def notify(
    81|         self, stream_key: str, stream_id: Union[int, RoomStreamToken], time_now_ms: int,
    82|     ):
    83|         """Notify any listeners for this user of a new event from an
    84|         event source.
    85|         Args:
    86|             stream_key: The stream the event came from.
    87|             stream_id: The new id for the stream the event came from.
    88|             time_now_ms: The current time in milliseconds.
    89|         """
    90|         self.current_token = self.current_token.copy_and_advance(stream_key, stream_id)
    91|         self.last_notified_token = self.current_token
    92|         self.last_notified_ms = time_now_ms
    93|         noify_deferred = self.notify_deferred
    94|         users_woken_by_stream_counter.labels(stream_key).inc()
    95|         with PreserveLoggingContext():
    96|             self.notify_deferred = ObservableDeferred(defer.Deferred())
    97|             noify_deferred.callback(self.current_token)
    98|     def remove(self, notifier: "Notifier"):
    99|         """ Remove this listener from all the indexes in the Notifier
   100|         it knows about.
   101|         """
   102|         for room in self.rooms:
   103|             lst = notifier.room_to_user_streams.get(room, set())
   104|             lst.discard(self)
   105|         notifier.user_to_user_stream.pop(self.user_id)
   106|     def count_listeners(self) -> int:
   107|         return len(self.notify_deferred.observers())
   108|     def new_listener(self, token: StreamToken) -> _NotificationListener:
   109|         """Returns a deferred that is resolved when there is a new token
   110|         greater than the given token.
   111|         Args:
   112|             token: The token from which we are streaming from, i.e. we shouldn't
   113|                 notify for things that happened before this.
   114|         """
   115|         if self.last_notified_token != token:
   116|             return _NotificationListener(defer.succeed(self.current_token))
   117|         else:
   118|             return _NotificationListener(self.notify_deferred.observe())
   119| class EventStreamResult(namedtuple("EventStreamResult", ("events", "tokens"))):
   120|     def __bool__(self):
   121|         return bool(self.events)
   122| class Notifier:
   123|     """ This class is responsible for notifying any listeners when there are
   124|     new events available for it.
   125|     Primarily used from the /events stream.
   126|     """
   127|     UNUSED_STREAM_EXPIRY_MS = 10 * 60 * 1000
   128|     def __init__(self, hs: "synapse.server.HomeServer"):
   129|         self.user_to_user_stream = {}  # type: Dict[str, _NotifierUserStream]
   130|         self.room_to_user_streams = {}  # type: Dict[str, Set[_NotifierUserStream]]
   131|         self.hs = hs
   132|         self.storage = hs.get_storage()
   133|         self.event_sources = hs.get_event_sources()
   134|         self.store = hs.get_datastore()
   135|         self.pending_new_room_events = (
   136|             []
   137|         )  # type: List[Tuple[PersistedEventPosition, EventBase, Collection[UserID]]]
   138|         self.replication_callbacks = []  # type: List[Callable[[], None]]
   139|         self.remote_server_up_callbacks = []  # type: List[Callable[[str], None]]
   140|         self.clock = hs.get_clock()
   141|         self.appservice_handler = hs.get_application_service_handler()
   142|         self._pusher_pool = hs.get_pusherpool()
   143|         self.federation_sender = None
   144|         if hs.should_send_federation():
   145|             self.federation_sender = hs.get_federation_sender()
   146|         self.state_handler = hs.get_state_handler()
   147|         self.clock.looping_call(
   148|             self.remove_expired_streams, self.UNUSED_STREAM_EXPIRY_MS
   149|         )
   150|         def count_listeners():
   151|             all_user_streams = set()  # type: Set[_NotifierUserStream]
   152|             for streams in list(self.room_to_user_streams.values()):
   153|                 all_user_streams |= streams
   154|             for stream in list(self.user_to_user_stream.values()):
   155|                 all_user_streams.add(stream)
   156|             return sum(stream.count_listeners() for stream in all_user_streams)
   157|         LaterGauge("synapse_notifier_listeners", "", [], count_listeners)
   158|         LaterGauge(
   159|             "synapse_notifier_rooms",
   160|             "",
   161|             [],
   162|             lambda: count(bool, list(self.room_to_user_streams.values())),
   163|         )
   164|         LaterGauge(
   165|             "synapse_notifier_users", "", [], lambda: len(self.user_to_user_stream)
   166|         )
   167|     def add_replication_callback(self, cb: Callable[[], None]):
   168|         """Add a callback that will be called when some new data is available.
   169|         Callback is not given any arguments. It should *not* return a Deferred - if
   170|         it needs to do any asynchronous work, a background thread should be started and
   171|         wrapped with run_as_background_process.
   172|         """
   173|         self.replication_callbacks.append(cb)
   174|     def on_new_room_event(
   175|         self,
   176|         event: EventBase,
   177|         event_pos: PersistedEventPosition,
   178|         max_room_stream_token: RoomStreamToken,
   179|         extra_users: Collection[UserID] = [],
   180|     ):
   181|         """ Used by handlers to inform the notifier something has happened
   182|         in the room, room event wise.
   183|         This triggers the notifier to wake up any listeners that are
   184|         listening to the room, and any listeners for the users in the
   185|         `extra_users` param.
   186|         The events can be peristed out of order. The notifier will wait
   187|         until all previous events have been persisted before notifying
   188|         the client streams.
   189|         """
   190|         self.pending_new_room_events.append((event_pos, event, extra_users))
   191|         self._notify_pending_new_room_events(max_room_stream_token)
   192|         self.notify_replication()
   193|     def _notify_pending_new_room_events(self, max_room_stream_token: RoomStreamToken):
   194|         """Notify for the room events that were queued waiting for a previous
   195|         event to be persisted.
   196|         Args:
   197|             max_room_stream_token: The highest stream_id below which all
   198|                 events have been persisted.
   199|         """
   200|         pending = self.pending_new_room_events
   201|         self.pending_new_room_events = []
   202|         users = set()  # type: Set[UserID]
   203|         rooms = set()  # type: Set[str]
   204|         for event_pos, event, extra_users in pending:
   205|             if event_pos.persisted_after(max_room_stream_token):
   206|                 self.pending_new_room_events.append((event_pos, event, extra_users))
   207|             else:
   208|                 if (
   209|                     event.type == EventTypes.Member
   210|                     and event.membership == Membership.JOIN
   211|                 ):
   212|                     self._user_joined_room(event.state_key, event.room_id)
   213|                 users.update(extra_users)
   214|                 rooms.add(event.room_id)
   215|         if users or rooms:
   216|             self.on_new_event(
   217|                 "room_key", max_room_stream_token, users=users, rooms=rooms,
   218|             )
   219|             self._on_updated_room_token(max_room_stream_token)
   220|     def _on_updated_room_token(self, max_room_stream_token: RoomStreamToken):
   221|         """Poke services that might care that the room position has been
   222|         updated.
   223|         """
   224|         run_as_background_process(
   225|             "_notify_app_services", self._notify_app_services, max_room_stream_token
   226|         )
   227|         run_as_background_process(
   228|             "_notify_pusher_pool", self._notify_pusher_pool, max_room_stream_token
   229|         )
   230|         if self.federation_sender:
   231|             self.federation_sender.notify_new_events(max_room_stream_token.stream)
   232|     async def _notify_app_services(self, max_room_stream_token: RoomStreamToken):
   233|         try:
   234|             await self.appservice_handler.notify_interested_services(
   235|                 max_room_stream_token.stream
   236|             )
   237|         except Exception:
   238|             logger.exception("Error notifying application services of event")
   239|     async def _notify_pusher_pool(self, max_room_stream_token: RoomStreamToken):
   240|         try:
   241|             await self._pusher_pool.on_new_notifications(max_room_stream_token.stream)
   242|         except Exception:
   243|             logger.exception("Error pusher pool of event")
   244|     def on_new_event(
   245|         self,
   246|         stream_key: str,
   247|         new_token: Union[int, RoomStreamToken],
   248|         users: Collection[UserID] = [],
   249|         rooms: Collection[str] = [],
   250|     ):
   251|         """ Used to inform listeners that something has happened event wise.
   252|         Will wake up all listeners for the given users and rooms.
   253|         """
   254|         with PreserveLoggingContext():
   255|             with Measure(self.clock, "on_new_event"):
   256|                 user_streams = set()
   257|                 for user in users:
   258|                     user_stream = self.user_to_user_stream.get(str(user))
   259|                     if user_stream is not None:
   260|                         user_streams.add(user_stream)
   261|                 for room in rooms:
   262|                     user_streams |= self.room_to_user_streams.get(room, set())
   263|                 time_now_ms = self.clock.time_msec()
   264|                 for user_stream in user_streams:
   265|                     try:
   266|                         user_stream.notify(stream_key, new_token, time_now_ms)
   267|                     except Exception:
   268|                         logger.exception("Failed to notify listener")

# --- HUNK 3: Lines 323-373 ---
   323|         if result is None:
   324|             current_token = user_stream.current_token
   325|             result = await callback(prev_token, current_token)
   326|         return result
   327|     async def get_events_for(
   328|         self,
   329|         user: UserID,
   330|         pagination_config: PaginationConfig,
   331|         timeout: int,
   332|         is_guest: bool = False,
   333|         explicit_room_id: str = None,
   334|     ) -> EventStreamResult:
   335|         """ For the given user and rooms, return any new events for them. If
   336|         there are no new events wait for up to `timeout` milliseconds for any
   337|         new events to happen before returning.
   338|         If explicit_room_id is not set, the user's joined rooms will be polled
   339|         for events.
   340|         If explicit_room_id is set, that room will be polled for events only if
   341|         it is world readable or the user has joined the room.
   342|         """
   343|         if pagination_config.from_token:
   344|             from_token = pagination_config.from_token
   345|         else:
   346|             from_token = self.event_sources.get_current_token()
   347|         limit = pagination_config.limit
   348|         room_ids, is_joined = await self._get_room_ids(user, explicit_room_id)
   349|         is_peeking = not is_joined
   350|         async def check_for_updates(
   351|             before_token: StreamToken, after_token: StreamToken
   352|         ) -> EventStreamResult:
   353|             if after_token == before_token:
   354|                 return EventStreamResult([], (from_token, from_token))
   355|             events = []  # type: List[EventBase]
   356|             end_token = from_token
   357|             for name, source in self.event_sources.sources.items():
   358|                 keyname = "%s_key" % name
   359|                 before_id = getattr(before_token, keyname)
   360|                 after_id = getattr(after_token, keyname)
   361|                 if before_id == after_id:
   362|                     continue
   363|                 new_events, new_key = await source.get_new_events(
   364|                     user=user,
   365|                     from_key=getattr(from_token, keyname),
   366|                     limit=limit,
   367|                     is_guest=is_peeking,
   368|                     room_ids=room_ids,
   369|                     explicit_room_id=explicit_room_id,
   370|                 )
   371|                 if name == "room":
   372|                     new_events = await filter_events_for_client(
   373|                         self.storage,


# ====================================================================
# FILE: synapse/push/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-3 ---
     1| class PusherConfigException(Exception):
     2|     def __init__(self, msg):
     3|         super().__init__(msg)


# ====================================================================
# FILE: synapse/push/emailpusher.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 28-68 ---
    28|         self.timed_call = None
    29|         self.throttle_params = None
    30|         self.max_stream_ordering = None
    31|         self._is_processing = False
    32|     def on_started(self, should_check_for_notifs):
    33|         """Called when this pusher has been started.
    34|         Args:
    35|             should_check_for_notifs (bool): Whether we should immediately
    36|                 check for push to send. Set to False only if it's known there
    37|                 is nothing to send
    38|         """
    39|         if should_check_for_notifs and self.mailer is not None:
    40|             self._start_processing()
    41|     def on_stop(self):
    42|         if self.timed_call:
    43|             try:
    44|                 self.timed_call.cancel()
    45|             except (AlreadyCalled, AlreadyCancelled):
    46|                 pass
    47|             self.timed_call = None
    48|     def on_new_notifications(self, max_stream_ordering):
    49|         if self.max_stream_ordering:
    50|             self.max_stream_ordering = max(
    51|                 max_stream_ordering, self.max_stream_ordering
    52|             )
    53|         else:
    54|             self.max_stream_ordering = max_stream_ordering
    55|         self._start_processing()
    56|     def on_new_receipts(self, min_stream_id, max_stream_id):
    57|         pass
    58|     def on_timer(self):
    59|         self.timed_call = None
    60|         self._start_processing()
    61|     def _start_processing(self):
    62|         if self._is_processing:
    63|             return
    64|         run_as_background_process("emailpush.process", self._process)
    65|     def _pause_processing(self):
    66|         """Used by tests to temporarily pause processing of events.
    67|         Asserts that its not currently processing.
    68|         """


# ====================================================================
# FILE: synapse/push/httppusher.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 55-95 ---
    55|             pusherdict["pushkey"],
    56|         )
    57|         if self.data is None:
    58|             raise PusherConfigException("data can not be null for HTTP pusher")
    59|         if "url" not in self.data:
    60|             raise PusherConfigException("'url' required in data for HTTP pusher")
    61|         self.url = self.data["url"]
    62|         self.http_client = hs.get_proxied_http_client()
    63|         self.data_minus_url = {}
    64|         self.data_minus_url.update(self.data)
    65|         del self.data_minus_url["url"]
    66|     def on_started(self, should_check_for_notifs):
    67|         """Called when this pusher has been started.
    68|         Args:
    69|             should_check_for_notifs (bool): Whether we should immediately
    70|                 check for push to send. Set to False only if it's known there
    71|                 is nothing to send
    72|         """
    73|         if should_check_for_notifs:
    74|             self._start_processing()
    75|     def on_new_notifications(self, max_stream_ordering):
    76|         self.max_stream_ordering = max(
    77|             max_stream_ordering, self.max_stream_ordering or 0
    78|         )
    79|         self._start_processing()
    80|     def on_new_receipts(self, min_stream_id, max_stream_id):
    81|         run_as_background_process("http_pusher.on_new_receipts", self._update_badge)
    82|     async def _update_badge(self):
    83|         badge = await push_tools.get_badge_count(self.hs.get_datastore(), self.user_id)
    84|         await self._send_badge(badge)
    85|     def on_timer(self):
    86|         self._start_processing()
    87|     def on_stop(self):
    88|         if self.timed_call:
    89|             try:
    90|                 self.timed_call.cancel()
    91|             except (AlreadyCalled, AlreadyCancelled):
    92|                 pass
    93|             self.timed_call = None
    94|     def _start_processing(self):
    95|         if self._is_processing:


# ====================================================================
# FILE: synapse/push/mailer.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 71-111 ---
    71|         self.macaroon_gen = self.hs.get_macaroon_generator()
    72|         self.state_handler = self.hs.get_state_handler()
    73|         self.storage = hs.get_storage()
    74|         self.app_name = app_name
    75|         self.email_subjects = hs.config.email_subjects  # type: EmailSubjectConfig
    76|         logger.info("Created Mailer for app_name %s" % app_name)
    77|     async def send_password_reset_mail(self, email_address, token, client_secret, sid):
    78|         """Send an email with a password reset link to a user
    79|         Args:
    80|             email_address (str): Email address we're sending the password
    81|                 reset to
    82|             token (str): Unique token generated by the server to verify
    83|                 the email was received
    84|             client_secret (str): Unique token generated by the client to
    85|                 group together multiple email sending attempts
    86|             sid (str): The generated session ID
    87|         """
    88|         params = {"token": token, "client_secret": client_secret, "sid": sid}
    89|         link = (
    90|             self.hs.config.public_baseurl
    91|             + "_synapse/client/password_reset/email/submit_token?%s"
    92|             % urllib.parse.urlencode(params)
    93|         )
    94|         template_vars = {"link": link}
    95|         await self.send_email(
    96|             email_address,
    97|             self.email_subjects.password_reset
    98|             % {"server_name": self.hs.config.server_name},
    99|             template_vars,
   100|         )
   101|     async def send_registration_mail(self, email_address, token, client_secret, sid):
   102|         """Send an email with a registration confirmation link to a user
   103|         Args:
   104|             email_address (str): Email address we're sending the registration
   105|                 link to
   106|             token (str): Unique token generated by the server to verify
   107|                 the email was received
   108|             client_secret (str): Unique token generated by the client to
   109|                 group together multiple email sending attempts
   110|             sid (str): The generated session ID
   111|         """


# ====================================================================
# FILE: synapse/push/pusherpool.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 14-57 ---
    14|     "synapse_pushers", "Number of active synapse pushers", ["kind", "app_id"]
    15| )
    16| class PusherPool:
    17|     """
    18|     The pusher pool. This is responsible for dispatching notifications of new events to
    19|     the http and email pushers.
    20|     It provides three methods which are designed to be called by the rest of the
    21|     application: `start`, `on_new_notifications`, and `on_new_receipts`: each of these
    22|     delegates to each of the relevant pushers.
    23|     Note that it is expected that each pusher will have its own 'processing' loop which
    24|     will send out the notifications in the background, rather than blocking until the
    25|     notifications are sent; accordingly Pusher.on_started, Pusher.on_new_notifications and
    26|     Pusher.on_new_receipts are not expected to return awaitables.
    27|     """
    28|     def __init__(self, hs: "HomeServer"):
    29|         self.hs = hs
    30|         self.pusher_factory = PusherFactory(hs)
    31|         self._should_start_pushers = hs.config.start_pushers
    32|         self.store = self.hs.get_datastore()
    33|         self.clock = self.hs.get_clock()
    34|         self._account_validity = hs.config.account_validity
    35|         self._pusher_shard_config = hs.config.push.pusher_shard_config
    36|         self._instance_name = hs.get_instance_name()
    37|         self._last_room_stream_id_seen = self.store.get_room_max_stream_ordering()
    38|         self.pushers = {}  # type: Dict[str, Dict[str, Union[HttpPusher, EmailPusher]]]
    39|     def start(self):
    40|         """Starts the pushers off in a background process.
    41|         """
    42|         if not self._should_start_pushers:
    43|             logger.info("Not starting pushers because they are disabled in the config")
    44|             return
    45|         run_as_background_process("start_pushers", self._start_pushers)
    46|     async def add_pusher(
    47|         self,
    48|         user_id,
    49|         access_token,
    50|         kind,
    51|         app_id,
    52|         app_display_name,
    53|         device_display_name,
    54|         pushkey,
    55|         lang,
    56|         data,
    57|         profile_tag="",

# --- HUNK 2: Lines 111-187 ---
   111|     async def remove_pushers_by_access_token(self, user_id, access_tokens):
   112|         """Remove the pushers for a given user corresponding to a set of
   113|         access_tokens.
   114|         Args:
   115|             user_id (str): user to remove pushers for
   116|             access_tokens (Iterable[int]): access token *ids* to remove pushers
   117|                 for
   118|         """
   119|         if not self._pusher_shard_config.should_handle(self._instance_name, user_id):
   120|             return
   121|         tokens = set(access_tokens)
   122|         for p in await self.store.get_pushers_by_user_id(user_id):
   123|             if p["access_token"] in tokens:
   124|                 logger.info(
   125|                     "Removing pusher for app id %s, pushkey %s, user %s",
   126|                     p["app_id"],
   127|                     p["pushkey"],
   128|                     p["user_name"],
   129|                 )
   130|                 await self.remove_pusher(p["app_id"], p["pushkey"], p["user_name"])
   131|     async def on_new_notifications(self, max_stream_id: int):
   132|         if not self.pushers:
   133|             return
   134|         if max_stream_id < self._last_room_stream_id_seen:
   135|             return
   136|         prev_stream_id = self._last_room_stream_id_seen
   137|         self._last_room_stream_id_seen = max_stream_id
   138|         try:
   139|             users_affected = await self.store.get_push_action_users_in_range(
   140|                 prev_stream_id, max_stream_id
   141|             )
   142|             for u in users_affected:
   143|                 if self._account_validity.enabled:
   144|                     expired = await self.store.is_account_expired(
   145|                         u, self.clock.time_msec()
   146|                     )
   147|                     if expired:
   148|                         continue
   149|                 if u in self.pushers:
   150|                     for p in self.pushers[u].values():
   151|                         p.on_new_notifications(max_stream_id)
   152|         except Exception:
   153|             logger.exception("Exception in pusher on_new_notifications")
   154|     async def on_new_receipts(self, min_stream_id, max_stream_id, affected_room_ids):
   155|         if not self.pushers:
   156|             return
   157|         try:
   158|             users_affected = await self.store.get_users_sent_receipts_between(
   159|                 min_stream_id - 1, max_stream_id
   160|             )
   161|             for u in users_affected:
   162|                 if self._account_validity.enabled:
   163|                     expired = await self.store.is_account_expired(
   164|                         u, self.clock.time_msec()
   165|                     )
   166|                     if expired:
   167|                         continue
   168|                 if u in self.pushers:
   169|                     for p in self.pushers[u].values():
   170|                         p.on_new_receipts(min_stream_id, max_stream_id)
   171|         except Exception:
   172|             logger.exception("Exception in pusher on_new_receipts")
   173|     async def start_pusher_by_id(self, app_id, pushkey, user_id):
   174|         """Look up the details for the given pusher, and start it
   175|         Returns:
   176|             EmailPusher|HttpPusher|None: The pusher started, if any
   177|         """
   178|         if not self._should_start_pushers:
   179|             return
   180|         if not self._pusher_shard_config.should_handle(self._instance_name, user_id):
   181|             return
   182|         resultlist = await self.store.get_pushers_by_app_id_and_pushkey(app_id, pushkey)
   183|         pusher_dict = None
   184|         for r in resultlist:
   185|             if r["user_name"] == user_id:
   186|                 pusher_dict = r
   187|         pusher = None


# ====================================================================
# FILE: synapse/python_dependencies.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-68 ---
     1| import logging
     2| from typing import List, Set
     3| from pkg_resources import (
     4|     DistributionNotFound,
     5|     Requirement,
     6|     VersionConflict,
     7|     get_provider,
     8| )
     9| logger = logging.getLogger(__name__)
    10| REQUIREMENTS = [
    11|     "jsonschema>=2.5.1",
    12|     "frozendict>=1",
    13|     "unpaddedbase64>=1.1.0",
    14|     "canonicaljson>=1.4.0",
    15|     "signedjson>=1.1.0",
    16|     "pynacl>=1.2.1",
    17|     "idna>=2.5",
    18|     "service_identity>=18.1.0",
    19|     "Twisted>=18.9.0",
    20|     "treq>=15.1",
    21|     "pyopenssl>=16.0.0",
    22|     "pyyaml>=3.11",
    23|     "pyasn1>=0.1.9",
    24|     "pyasn1-modules>=0.0.7",
    25|     "bcrypt>=3.1.0",
    26|     "pillow>=4.3.0",
    27|     "sortedcontainers>=1.4.4",
    28|     "pymacaroons>=0.13.0",
    29|     "msgpack>=0.5.2",
    30|     "phonenumbers>=8.2.0",
    31|     "prometheus_client>=0.4.0,<0.9.0",
    32|     "attrs>=19.1.0",
    33|     "netaddr>=0.7.18",
    34|     "Jinja2>=2.9",
    35|     "bleach>=1.4.3",
    36|     "typing-extensions>=3.7.4",
    37| ]
    38| CONDITIONAL_REQUIREMENTS = {
    39|     "matrix-synapse-ldap3": ["matrix-synapse-ldap3>=0.1"],
    40|     "postgres": ["psycopg2>=2.7"],
    41|     "acme": [
    42|         "txacme>=0.9.2",
    43|         'eliot<1.8.0;python_version<"3.5.3"',
    44|     ],
    45|     "saml2": ["pysaml2>=4.5.0"],
    46|     "oidc": ["authlib>=0.14.0"],
    47|     "systemd": ["systemd-python>=231"],
    48|     "url_preview": ["lxml>=3.5.0"],
    49|     "sentry": ["sentry-sdk>=0.7.2"],
    50|     "opentracing": ["jaeger-client>=4.0.0", "opentracing>=2.2.0"],
    51|     "jwt": ["pyjwt>=1.6.4"],
    52|     "redis": ["txredisapi>=1.4.7", "hiredis"],
    53| }
    54| ALL_OPTIONAL_REQUIREMENTS = set()  # type: Set[str]
    55| for name, optional_deps in CONDITIONAL_REQUIREMENTS.items():
    56|     if name not in ["systemd"]:
    57|         ALL_OPTIONAL_REQUIREMENTS = set(optional_deps) | ALL_OPTIONAL_REQUIREMENTS
    58| def list_requirements():
    59|     return list(set(REQUIREMENTS) | ALL_OPTIONAL_REQUIREMENTS)
    60| class DependencyException(Exception):
    61|     @property
    62|     def message(self):
    63|         return "\n".join(
    64|             [
    65|                 "Missing Requirements: %s" % (", ".join(self.dependencies),),
    66|                 "To install run:",
    67|                 "    pip install --upgrade --force %s" % (" ".join(self.dependencies),),
    68|                 "",


# ====================================================================
# FILE: synapse/replication/http/_base.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-74 ---
     1| import abc
     2| import logging
     3| import re
     4| import urllib
     5| from inspect import signature
     6| from typing import Dict, List, Tuple
     7| from prometheus_client import Counter, Gauge
     8| from synapse.api.errors import HttpResponseException, SynapseError
     9| from synapse.http import RequestTimedOutError
    10| from synapse.logging.opentracing import inject_active_span_byte_dict, trace
    11| from synapse.util.caches.response_cache import ResponseCache
    12| from synapse.util.stringutils import random_string
    13| logger = logging.getLogger(__name__)
    14| _pending_outgoing_requests = Gauge(
    15|     "synapse_pending_outgoing_replication_requests",
    16|     "Number of active outgoing replication requests, by replication method name",
    17|     ["name"],
    18| )
    19| _outgoing_request_counter = Counter(
    20|     "synapse_outgoing_replication_requests",
    21|     "Number of outgoing replication requests, by replication method name and result",
    22|     ["name", "code"],
    23| )
    24| class ReplicationEndpoint(metaclass=abc.ABCMeta):
    25|     """Helper base class for defining new replication HTTP endpoints.
    26|     This creates an endpoint under `/_synapse/replication/:NAME/:PATH_ARGS..`
    27|     (with a `/:txn_id` suffix for cached requests), where NAME is a name,
    28|     PATH_ARGS are a tuple of parameters to be encoded in the URL.
    29|     For example, if `NAME` is "send_event" and `PATH_ARGS` is `("event_id",)`,
    30|     with `CACHE` set to true then this generates an endpoint:
    31|         /_synapse/replication/send_event/:event_id/:txn_id
    32|     For POST/PUT requests the payload is serialized to json and sent as the
    33|     body, while for GET requests the payload is added as query parameters. See
    34|     `_serialize_payload` for details.
    35|     Incoming requests are handled by overriding `_handle_request`. Servers
    36|     must call `register` to register the path with the HTTP server.
    37|     Requests can be sent by calling the client returned by `make_client`.
    38|     Requests are sent to master process by default, but can be sent to other
    39|     named processes by specifying an `instance_name` keyword argument.
    40|     Attributes:
    41|         NAME (str): A name for the endpoint, added to the path as well as used
    42|             in logging and metrics.
    43|         PATH_ARGS (tuple[str]): A list of parameters to be added to the path.
    44|             Adding parameters to the path (rather than payload) can make it
    45|             easier to follow along in the log files.
    46|         METHOD (str): The method of the HTTP request, defaults to POST. Can be
    47|             one of POST, PUT or GET. If GET then the payload is sent as query
    48|             parameters rather than a JSON body.
    49|         CACHE (bool): Whether server should cache the result of the request/
    50|             If true then transparently adds a txn_id to all requests, and
    51|             `_handle_request` must return a Deferred.
    52|         RETRY_ON_TIMEOUT(bool): Whether or not to retry the request when a 504
    53|             is received.
    54|     """
    55|     NAME = abc.abstractproperty()  # type: str  # type: ignore
    56|     PATH_ARGS = abc.abstractproperty()  # type: Tuple[str, ...]  # type: ignore
    57|     METHOD = "POST"
    58|     CACHE = True
    59|     RETRY_ON_TIMEOUT = True
    60|     def __init__(self, hs):
    61|         if self.CACHE:
    62|             self.response_cache = ResponseCache(
    63|                 hs, "repl." + self.NAME, timeout_ms=30 * 60 * 1000
    64|             )
    65|         assert (
    66|             "instance_name" not in self.PATH_ARGS
    67|         ), "`instance_name` is a reserved parameter name"
    68|         assert (
    69|             "instance_name"
    70|             not in signature(self.__class__._serialize_payload).parameters
    71|         ), "`instance_name` is a reserved parameter name"
    72|         assert self.METHOD in ("PUT", "POST", "GET")
    73|     @abc.abstractmethod
    74|     async def _serialize_payload(**kwargs):

# --- HUNK 2: Lines 85-127 ---
    85|     @abc.abstractmethod
    86|     async def _handle_request(self, request, **kwargs):
    87|         """Handle incoming request.
    88|         This is called with the request object and PATH_ARGS.
    89|         Returns:
    90|             tuple[int, dict]: HTTP status code and a JSON serialisable dict
    91|             to be used as response body of request.
    92|         """
    93|         pass
    94|     @classmethod
    95|     def make_client(cls, hs):
    96|         """Create a client that makes requests.
    97|         Returns a callable that accepts the same parameters as `_serialize_payload`.
    98|         """
    99|         clock = hs.get_clock()
   100|         client = hs.get_simple_http_client()
   101|         local_instance_name = hs.get_instance_name()
   102|         master_host = hs.config.worker_replication_host
   103|         master_port = hs.config.worker_replication_http_port
   104|         instance_map = hs.config.worker.instance_map
   105|         outgoing_gauge = _pending_outgoing_requests.labels(cls.NAME)
   106|         @trace(opname="outgoing_replication_request")
   107|         @outgoing_gauge.track_inprogress()
   108|         async def send_request(instance_name="master", **kwargs):
   109|             if instance_name == local_instance_name:
   110|                 raise Exception("Trying to send HTTP request to self")
   111|             if instance_name == "master":
   112|                 host = master_host
   113|                 port = master_port
   114|             elif instance_name in instance_map:
   115|                 host = instance_map[instance_name].host
   116|                 port = instance_map[instance_name].port
   117|             else:
   118|                 raise Exception(
   119|                     "Instance %r not in 'instance_map' config" % (instance_name,)
   120|                 )
   121|             data = await cls._serialize_payload(**kwargs)
   122|             url_args = [
   123|                 urllib.parse.quote(kwargs[name], safe="") for name in cls.PATH_ARGS
   124|             ]
   125|             if cls.CACHE:
   126|                 txn_id = random_string(10)
   127|                 url_args.append(txn_id)

# --- HUNK 3: Lines 131-182 ---
   131|                 request_func = client.put_json
   132|             elif cls.METHOD == "GET":
   133|                 request_func = client.get_json
   134|             else:
   135|                 raise Exception(
   136|                     "Unknown METHOD on %s replication endpoint" % (cls.NAME,)
   137|                 )
   138|             uri = "http://%s:%s/_synapse/replication/%s/%s" % (
   139|                 host,
   140|                 port,
   141|                 cls.NAME,
   142|                 "/".join(url_args),
   143|             )
   144|             try:
   145|                 while True:
   146|                     headers = {}  # type: Dict[bytes, List[bytes]]
   147|                     inject_active_span_byte_dict(headers, None, check_destination=False)
   148|                     try:
   149|                         result = await request_func(uri, data, headers=headers)
   150|                         break
   151|                     except RequestTimedOutError:
   152|                         if not cls.RETRY_ON_TIMEOUT:
   153|                             raise
   154|                     logger.warning("%s request timed out; retrying", cls.NAME)
   155|                     await clock.sleep(1)
   156|             except HttpResponseException as e:
   157|                 _outgoing_request_counter.labels(cls.NAME, e.code).inc()
   158|                 raise e.to_synapse_error()
   159|             except Exception as e:
   160|                 _outgoing_request_counter.labels(cls.NAME, "ERR").inc()
   161|                 raise SynapseError(502, "Failed to talk to main process") from e
   162|             _outgoing_request_counter.labels(cls.NAME, 200).inc()
   163|             return result
   164|         return send_request
   165|     def register(self, http_server):
   166|         """Called by the server to register this as a handler to the
   167|         appropriate path.
   168|         """
   169|         url_args = list(self.PATH_ARGS)
   170|         handler = self._handle_request
   171|         method = self.METHOD
   172|         if self.CACHE:
   173|             handler = self._cached_handler  # type: ignore
   174|             url_args.append("txn_id")
   175|         args = "/".join("(?P<%s>[^/]+)" % (arg,) for arg in url_args)
   176|         pattern = re.compile("^/_synapse/replication/%s/%s$" % (self.NAME, args))
   177|         http_server.register_paths(
   178|             method, [pattern], handler, self.__class__.__name__,
   179|         )
   180|     def _cached_handler(self, request, txn_id, **kwargs):
   181|         """Called on new incoming requests when caching is enabled. Checks
   182|         if there is a cached response for the request and returns that,


# ====================================================================
# FILE: synapse/replication/http/devices.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 9-40 ---
     9|     Request format:
    10|         POST /_synapse/replication/user_device_resync/:user_id
    11|         {}
    12|     Response is equivalent to ` /_matrix/federation/v1/user/devices/:user_id`
    13|     response, e.g.:
    14|         {
    15|             "user_id": "@alice:example.org",
    16|             "devices": [
    17|                 {
    18|                     "device_id": "JLAFKJWSCS",
    19|                     "keys": { ... },
    20|                     "device_display_name": "Alice's Mobile Phone"
    21|                 }
    22|             ]
    23|         }
    24|     """
    25|     NAME = "user_device_resync"
    26|     PATH_ARGS = ("user_id",)
    27|     CACHE = False
    28|     def __init__(self, hs):
    29|         super().__init__(hs)
    30|         self.device_list_updater = hs.get_device_handler().device_list_updater
    31|         self.store = hs.get_datastore()
    32|         self.clock = hs.get_clock()
    33|     @staticmethod
    34|     async def _serialize_payload(user_id):
    35|         return {}
    36|     async def _handle_request(self, request, user_id):
    37|         user_devices = await self.device_list_updater.user_device_resync(user_id)
    38|         return 200, user_devices
    39| def register_servlets(hs, http_server):
    40|     ReplicationUserDevicesResyncRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/replication/http/federation.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 14-181 ---
    14|         {
    15|             "events": [{
    16|                 "event": { .. serialized event .. },
    17|                 "room_version": .., // "1", "2", "3", etc: the version of the room
    18|                                     // containing the event
    19|                 "event_format_version": .., // 1,2,3 etc: the event format version
    20|                 "internal_metadata": { .. serialized internal_metadata .. },
    21|                 "rejected_reason": ..,   // The event.rejected_reason field
    22|                 "context": { .. serialized event context .. },
    23|             }],
    24|             "backfilled": false
    25|         }
    26|         200 OK
    27|         {
    28|             "max_stream_id": 32443,
    29|         }
    30|     """
    31|     NAME = "fed_send_events"
    32|     PATH_ARGS = ()
    33|     def __init__(self, hs):
    34|         super().__init__(hs)
    35|         self.store = hs.get_datastore()
    36|         self.storage = hs.get_storage()
    37|         self.clock = hs.get_clock()
    38|         self.federation_handler = hs.get_handlers().federation_handler
    39|     @staticmethod
    40|     async def _serialize_payload(store, room_id, event_and_contexts, backfilled):
    41|         """
    42|         Args:
    43|             store
    44|             room_id (str)
    45|             event_and_contexts (list[tuple[FrozenEvent, EventContext]])
    46|             backfilled (bool): Whether or not the events are the result of
    47|                 backfilling
    48|         """
    49|         event_payloads = []
    50|         for event, context in event_and_contexts:
    51|             serialized_context = await context.serialize(event, store)
    52|             event_payloads.append(
    53|                 {
    54|                     "event": event.get_pdu_json(),
    55|                     "room_version": event.room_version.identifier,
    56|                     "event_format_version": event.format_version,
    57|                     "internal_metadata": event.internal_metadata.get_dict(),
    58|                     "rejected_reason": event.rejected_reason,
    59|                     "context": serialized_context,
    60|                 }
    61|             )
    62|         payload = {
    63|             "events": event_payloads,
    64|             "backfilled": backfilled,
    65|             "room_id": room_id,
    66|         }
    67|         return payload
    68|     async def _handle_request(self, request):
    69|         with Measure(self.clock, "repl_fed_send_events_parse"):
    70|             content = parse_json_object_from_request(request)
    71|             room_id = content["room_id"]
    72|             backfilled = content["backfilled"]
    73|             event_payloads = content["events"]
    74|             event_and_contexts = []
    75|             for event_payload in event_payloads:
    76|                 event_dict = event_payload["event"]
    77|                 room_ver = KNOWN_ROOM_VERSIONS[event_payload["room_version"]]
    78|                 internal_metadata = event_payload["internal_metadata"]
    79|                 rejected_reason = event_payload["rejected_reason"]
    80|                 event = make_event_from_dict(
    81|                     event_dict, room_ver, internal_metadata, rejected_reason
    82|                 )
    83|                 context = EventContext.deserialize(
    84|                     self.storage, event_payload["context"]
    85|                 )
    86|                 event_and_contexts.append((event, context))
    87|         logger.info("Got %d events from federation", len(event_and_contexts))
    88|         max_stream_id = await self.federation_handler.persist_events_and_notify(
    89|             room_id, event_and_contexts, backfilled
    90|         )
    91|         return 200, {"max_stream_id": max_stream_id}
    92| class ReplicationFederationSendEduRestServlet(ReplicationEndpoint):
    93|     """Handles EDUs newly received from federation, including persisting and
    94|     notifying.
    95|     Request format:
    96|         POST /_synapse/replication/fed_send_edu/:edu_type/:txn_id
    97|         {
    98|             "origin": ...,
    99|             "content: { ... }
   100|         }
   101|     """
   102|     NAME = "fed_send_edu"
   103|     PATH_ARGS = ("edu_type",)
   104|     def __init__(self, hs):
   105|         super().__init__(hs)
   106|         self.store = hs.get_datastore()
   107|         self.clock = hs.get_clock()
   108|         self.registry = hs.get_federation_registry()
   109|     @staticmethod
   110|     async def _serialize_payload(edu_type, origin, content):
   111|         return {"origin": origin, "content": content}
   112|     async def _handle_request(self, request, edu_type):
   113|         with Measure(self.clock, "repl_fed_send_edu_parse"):
   114|             content = parse_json_object_from_request(request)
   115|             origin = content["origin"]
   116|             edu_content = content["content"]
   117|         logger.info("Got %r edu from %s", edu_type, origin)
   118|         result = await self.registry.on_edu(edu_type, origin, edu_content)
   119|         return 200, result
   120| class ReplicationGetQueryRestServlet(ReplicationEndpoint):
   121|     """Handle responding to queries from federation.
   122|     Request format:
   123|         POST /_synapse/replication/fed_query/:query_type
   124|         {
   125|             "args": { ... }
   126|         }
   127|     """
   128|     NAME = "fed_query"
   129|     PATH_ARGS = ("query_type",)
   130|     CACHE = False
   131|     def __init__(self, hs):
   132|         super().__init__(hs)
   133|         self.store = hs.get_datastore()
   134|         self.clock = hs.get_clock()
   135|         self.registry = hs.get_federation_registry()
   136|     @staticmethod
   137|     async def _serialize_payload(query_type, args):
   138|         """
   139|         Args:
   140|             query_type (str)
   141|             args (dict): The arguments received for the given query type
   142|         """
   143|         return {"args": args}
   144|     async def _handle_request(self, request, query_type):
   145|         with Measure(self.clock, "repl_fed_query_parse"):
   146|             content = parse_json_object_from_request(request)
   147|             args = content["args"]
   148|         logger.info("Got %r query", query_type)
   149|         result = await self.registry.on_query(query_type, args)
   150|         return 200, result
   151| class ReplicationCleanRoomRestServlet(ReplicationEndpoint):
   152|     """Called to clean up any data in DB for a given room, ready for the
   153|     server to join the room.
   154|     Request format:
   155|         POST /_synapse/replication/fed_cleanup_room/:room_id/:txn_id
   156|         {}
   157|     """
   158|     NAME = "fed_cleanup_room"
   159|     PATH_ARGS = ("room_id",)
   160|     def __init__(self, hs):
   161|         super().__init__(hs)
   162|         self.store = hs.get_datastore()
   163|     @staticmethod
   164|     async def _serialize_payload(room_id, args):
   165|         """
   166|         Args:
   167|             room_id (str)
   168|         """
   169|         return {}
   170|     async def _handle_request(self, request, room_id):
   171|         await self.store.clean_room_for_join(room_id)
   172|         return 200, {}
   173| class ReplicationStoreRoomOnInviteRestServlet(ReplicationEndpoint):
   174|     """Called to clean up any data in DB for a given room, ready for the
   175|     server to join the room.
   176|     Request format:
   177|         POST /_synapse/replication/store_room_on_invite/:room_id/:txn_id
   178|         {
   179|             "room_version": "1",
   180|         }
   181|     """


# ====================================================================
# FILE: synapse/replication/http/login.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-33 ---
     1| import logging
     2| from synapse.http.servlet import parse_json_object_from_request
     3| from synapse.replication.http._base import ReplicationEndpoint
     4| logger = logging.getLogger(__name__)
     5| class RegisterDeviceReplicationServlet(ReplicationEndpoint):
     6|     """Ensure a device is registered, generating a new access token for the
     7|     device.
     8|     Used during registration and login.
     9|     """
    10|     NAME = "device_check_registered"
    11|     PATH_ARGS = ("user_id",)
    12|     def __init__(self, hs):
    13|         super().__init__(hs)
    14|         self.registration_handler = hs.get_registration_handler()
    15|     @staticmethod
    16|     async def _serialize_payload(user_id, device_id, initial_display_name, is_guest):
    17|         """
    18|         Args:
    19|             device_id (str|None): Device ID to use, if None a new one is
    20|                 generated.
    21|             initial_display_name (str|None)
    22|             is_guest (bool)
    23|         """
    24|         return {
    25|             "device_id": device_id,
    26|             "initial_display_name": initial_display_name,
    27|             "is_guest": is_guest,
    28|         }
    29|     async def _handle_request(self, request, user_id):
    30|         content = parse_json_object_from_request(request)
    31|         device_id = content["device_id"]
    32|         initial_display_name = content["initial_display_name"]
    33|         is_guest = content["is_guest"]


# ====================================================================
# FILE: synapse/replication/http/membership.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-43 ---
     1| import logging
     2| from typing import TYPE_CHECKING, Optional
     3| from synapse.http.servlet import parse_json_object_from_request
     4| from synapse.replication.http._base import ReplicationEndpoint
     5| from synapse.types import JsonDict, Requester, UserID
     6| from synapse.util.distributor import user_left_room
     7| if TYPE_CHECKING:
     8|     from synapse.server import HomeServer
     9| logger = logging.getLogger(__name__)
    10| class ReplicationRemoteJoinRestServlet(ReplicationEndpoint):
    11|     """Does a remote join for the given user to the given room
    12|     Request format:
    13|         POST /_synapse/replication/remote_join/:room_id/:user_id
    14|         {
    15|             "requester": ...,
    16|             "remote_room_hosts": [...],
    17|             "content": { ... }
    18|         }
    19|     """
    20|     NAME = "remote_join"
    21|     PATH_ARGS = ("room_id", "user_id")
    22|     def __init__(self, hs):
    23|         super().__init__(hs)
    24|         self.federation_handler = hs.get_handlers().federation_handler
    25|         self.store = hs.get_datastore()
    26|         self.clock = hs.get_clock()
    27|     @staticmethod
    28|     async def _serialize_payload(
    29|         requester, room_id, user_id, remote_room_hosts, content
    30|     ):
    31|         """
    32|         Args:
    33|             requester(Requester)
    34|             room_id (str)
    35|             user_id (str)
    36|             remote_room_hosts (list[str]): Servers to try and join via
    37|             content(dict): The event content to use for the join event
    38|         """
    39|         return {
    40|             "requester": requester.serialize(),
    41|             "remote_room_hosts": remote_room_hosts,
    42|             "content": content,
    43|         }

# --- HUNK 2: Lines 49-89 ---
    49|         if requester.user:
    50|             request.authenticated_entity = requester.user.to_string()
    51|         logger.info("remote_join: %s into room: %s", user_id, room_id)
    52|         event_id, stream_id = await self.federation_handler.do_invite_join(
    53|             remote_room_hosts, room_id, user_id, event_content
    54|         )
    55|         return 200, {"event_id": event_id, "stream_id": stream_id}
    56| class ReplicationRemoteRejectInviteRestServlet(ReplicationEndpoint):
    57|     """Rejects an out-of-band invite we have received from a remote server
    58|     Request format:
    59|         POST /_synapse/replication/remote_reject_invite/:event_id
    60|         {
    61|             "txn_id": ...,
    62|             "requester": ...,
    63|             "content": { ... }
    64|         }
    65|     """
    66|     NAME = "remote_reject_invite"
    67|     PATH_ARGS = ("invite_event_id",)
    68|     def __init__(self, hs: "HomeServer"):
    69|         super().__init__(hs)
    70|         self.store = hs.get_datastore()
    71|         self.clock = hs.get_clock()
    72|         self.member_handler = hs.get_room_member_handler()
    73|     @staticmethod
    74|     async def _serialize_payload(  # type: ignore
    75|         invite_event_id: str,
    76|         txn_id: Optional[str],
    77|         requester: Requester,
    78|         content: JsonDict,
    79|     ):
    80|         """
    81|         Args:
    82|             invite_event_id: ID of the invite to be rejected
    83|             txn_id: optional transaction ID supplied by the client
    84|             requester: user making the rejection request, according to the access token
    85|             content: additional content to include in the rejection event.
    86|                Normally an empty dict.
    87|         """
    88|         return {
    89|             "txn_id": txn_id,

# --- HUNK 3: Lines 94-140 ---
    94|         content = parse_json_object_from_request(request)
    95|         txn_id = content["txn_id"]
    96|         event_content = content["content"]
    97|         requester = Requester.deserialize(self.store, content["requester"])
    98|         if requester.user:
    99|             request.authenticated_entity = requester.user.to_string()
   100|         event_id, stream_id = await self.member_handler.remote_reject_invite(
   101|             invite_event_id, txn_id, requester, event_content,
   102|         )
   103|         return 200, {"event_id": event_id, "stream_id": stream_id}
   104| class ReplicationUserJoinedLeftRoomRestServlet(ReplicationEndpoint):
   105|     """Notifies that a user has joined or left the room
   106|     Request format:
   107|         POST /_synapse/replication/membership_change/:room_id/:user_id/:change
   108|         {}
   109|     """
   110|     NAME = "membership_change"
   111|     PATH_ARGS = ("room_id", "user_id", "change")
   112|     CACHE = False  # No point caching as should return instantly.
   113|     def __init__(self, hs):
   114|         super().__init__(hs)
   115|         self.registeration_handler = hs.get_registration_handler()
   116|         self.store = hs.get_datastore()
   117|         self.clock = hs.get_clock()
   118|         self.distributor = hs.get_distributor()
   119|     @staticmethod
   120|     async def _serialize_payload(room_id, user_id, change):
   121|         """
   122|         Args:
   123|             room_id (str)
   124|             user_id (str)
   125|             change (str): "left"
   126|         """
   127|         assert change == "left"
   128|         return {}
   129|     def _handle_request(self, request, room_id, user_id, change):
   130|         logger.info("user membership change: %s in %s", user_id, room_id)
   131|         user = UserID.from_string(user_id)
   132|         if change == "left":
   133|             user_left_room(self.distributor, user, room_id)
   134|         else:
   135|             raise Exception("Unrecognized change: %r", change)
   136|         return 200, {}
   137| def register_servlets(hs, http_server):
   138|     ReplicationRemoteJoinRestServlet(hs).register(http_server)
   139|     ReplicationRemoteRejectInviteRestServlet(hs).register(http_server)
   140|     ReplicationUserJoinedLeftRoomRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/replication/http/register.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| import logging
     2| from synapse.http.servlet import parse_json_object_from_request
     3| from synapse.replication.http._base import ReplicationEndpoint
     4| logger = logging.getLogger(__name__)
     5| class ReplicationRegisterServlet(ReplicationEndpoint):
     6|     """Register a new user
     7|     """
     8|     NAME = "register_user"
     9|     PATH_ARGS = ("user_id",)
    10|     def __init__(self, hs):
    11|         super().__init__(hs)
    12|         self.store = hs.get_datastore()
    13|         self.registration_handler = hs.get_registration_handler()
    14|     @staticmethod
    15|     async def _serialize_payload(
    16|         user_id,
    17|         password_hash,
    18|         was_guest,
    19|         make_guest,
    20|         appservice_id,
    21|         create_profile_with_displayname,
    22|         admin,
    23|         user_type,
    24|         address,
    25|         shadow_banned,
    26|     ):
    27|         """
    28|         Args:
    29|             user_id (str): The desired user ID to register.
    30|             password_hash (str|None): Optional. The password hash for this user.
    31|             was_guest (bool): Optional. Whether this is a guest account being

# --- HUNK 2: Lines 57-97 ---
    57|         self.registration_handler.check_registration_ratelimit(content["address"])
    58|         await self.registration_handler.register_with_store(
    59|             user_id=user_id,
    60|             password_hash=content["password_hash"],
    61|             was_guest=content["was_guest"],
    62|             make_guest=content["make_guest"],
    63|             appservice_id=content["appservice_id"],
    64|             create_profile_with_displayname=content["create_profile_with_displayname"],
    65|             admin=content["admin"],
    66|             user_type=content["user_type"],
    67|             address=content["address"],
    68|             shadow_banned=content["shadow_banned"],
    69|         )
    70|         return 200, {}
    71| class ReplicationPostRegisterActionsServlet(ReplicationEndpoint):
    72|     """Run any post registration actions
    73|     """
    74|     NAME = "post_register"
    75|     PATH_ARGS = ("user_id",)
    76|     def __init__(self, hs):
    77|         super().__init__(hs)
    78|         self.store = hs.get_datastore()
    79|         self.registration_handler = hs.get_registration_handler()
    80|     @staticmethod
    81|     async def _serialize_payload(user_id, auth_result, access_token):
    82|         """
    83|         Args:
    84|             user_id (str): The user ID that consented
    85|             auth_result (dict): The authenticated credentials of the newly
    86|                 registered user.
    87|             access_token (str|None): The access token of the newly logged in
    88|                 device, or None if `inhibit_login` enabled.
    89|         """
    90|         return {"auth_result": auth_result, "access_token": access_token}
    91|     async def _handle_request(self, request, user_id):
    92|         content = parse_json_object_from_request(request)
    93|         auth_result = content["auth_result"]
    94|         access_token = content["access_token"]
    95|         await self.registration_handler.post_registration_actions(
    96|             user_id=user_id, auth_result=auth_result, access_token=access_token
    97|         )


# ====================================================================
# FILE: synapse/replication/http/send_event.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 11-51 ---
    11|     """Handles events newly created on workers, including persisting and
    12|     notifying.
    13|     The API looks like:
    14|         POST /_synapse/replication/send_event/:event_id/:txn_id
    15|         {
    16|             "event": { .. serialized event .. },
    17|             "room_version": .., // "1", "2", "3", etc: the version of the room
    18|                                 // containing the event
    19|             "event_format_version": .., // 1,2,3 etc: the event format version
    20|             "internal_metadata": { .. serialized internal_metadata .. },
    21|             "rejected_reason": ..,   // The event.rejected_reason field
    22|             "context": { .. serialized event context .. },
    23|             "requester": { .. serialized requester .. },
    24|             "ratelimit": true,
    25|             "extra_users": [],
    26|         }
    27|     """
    28|     NAME = "send_event"
    29|     PATH_ARGS = ("event_id",)
    30|     def __init__(self, hs):
    31|         super().__init__(hs)
    32|         self.event_creation_handler = hs.get_event_creation_handler()
    33|         self.store = hs.get_datastore()
    34|         self.storage = hs.get_storage()
    35|         self.clock = hs.get_clock()
    36|     @staticmethod
    37|     async def _serialize_payload(
    38|         event_id, store, event, context, requester, ratelimit, extra_users
    39|     ):
    40|         """
    41|         Args:
    42|             event_id (str)
    43|             store (DataStore)
    44|             requester (Requester)
    45|             event (FrozenEvent)
    46|             context (EventContext)
    47|             ratelimit (bool)
    48|             extra_users (list(UserID)): Any extra users to notify about event
    49|         """
    50|         serialized_context = await context.serialize(event, store)
    51|         payload = {


# ====================================================================
# FILE: synapse/replication/slave/storage/_base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| import logging
     2| from typing import Optional
     3| from synapse.storage.database import DatabasePool
     4| from synapse.storage.databases.main.cache import CacheInvalidationWorkerStore
     5| from synapse.storage.engines import PostgresEngine
     6| from synapse.storage.util.id_generators import MultiWriterIdGenerator
     7| logger = logging.getLogger(__name__)
     8| class BaseSlavedStore(CacheInvalidationWorkerStore):
     9|     def __init__(self, database: DatabasePool, db_conn, hs):
    10|         super().__init__(database, db_conn, hs)
    11|         if isinstance(self.database_engine, PostgresEngine):
    12|             self._cache_id_gen = MultiWriterIdGenerator(
    13|                 db_conn,
    14|                 database,
    15|                 stream_name="caches",
    16|                 instance_name=hs.get_instance_name(),
    17|                 table="cache_invalidation_stream_by_instance",
    18|                 instance_column="instance_name",
    19|                 id_column="stream_id",
    20|                 sequence_name="cache_invalidation_stream_seq",
    21|                 writers=[],
    22|             )  # type: Optional[MultiWriterIdGenerator]
    23|         else:
    24|             self._cache_id_gen = None
    25|         self.hs = hs


# ====================================================================
# FILE: synapse/replication/slave/storage/account_data.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-38 ---
     1| from synapse.replication.slave.storage._base import BaseSlavedStore
     2| from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
     3| from synapse.replication.tcp.streams import AccountDataStream, TagAccountDataStream
     4| from synapse.storage.database import DatabasePool
     5| from synapse.storage.databases.main.account_data import AccountDataWorkerStore
     6| from synapse.storage.databases.main.tags import TagsWorkerStore
     7| class SlavedAccountDataStore(TagsWorkerStore, AccountDataWorkerStore, BaseSlavedStore):
     8|     def __init__(self, database: DatabasePool, db_conn, hs):
     9|         self._account_data_id_gen = SlavedIdTracker(
    10|             db_conn,
    11|             "account_data",
    12|             "stream_id",
    13|             extra_tables=[
    14|                 ("room_account_data", "stream_id"),
    15|                 ("room_tags_revisions", "stream_id"),
    16|             ],
    17|         )
    18|         super().__init__(database, db_conn, hs)
    19|     def get_max_account_data_stream_id(self):
    20|         return self._account_data_id_gen.get_current_token()
    21|     def process_replication_rows(self, stream_name, instance_name, token, rows):
    22|         if stream_name == TagAccountDataStream.NAME:
    23|             self._account_data_id_gen.advance(instance_name, token)
    24|             for row in rows:
    25|                 self.get_tags_for_user.invalidate((row.user_id,))
    26|                 self._account_data_stream_cache.entity_has_changed(row.user_id, token)
    27|         elif stream_name == AccountDataStream.NAME:
    28|             self._account_data_id_gen.advance(instance_name, token)
    29|             for row in rows:
    30|                 if not row.room_id:
    31|                     self.get_global_account_data_by_type_for_user.invalidate(
    32|                         (row.data_type, row.user_id)
    33|                     )
    34|                 self.get_account_data_for_user.invalidate((row.user_id,))
    35|                 self.get_account_data_for_room.invalidate((row.user_id, row.room_id))
    36|                 self.get_account_data_for_room_and_type.invalidate(
    37|                     (row.user_id, row.room_id, row.data_type)
    38|                 )


# ====================================================================
# FILE: synapse/replication/slave/storage/client_ips.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-23 ---
     1| from synapse.storage.database import DatabasePool
     2| from synapse.storage.databases.main.client_ips import LAST_SEEN_GRANULARITY
     3| from synapse.util.caches.descriptors import Cache
     4| from ._base import BaseSlavedStore
     5| class SlavedClientIpStore(BaseSlavedStore):
     6|     def __init__(self, database: DatabasePool, db_conn, hs):
     7|         super().__init__(database, db_conn, hs)
     8|         self.client_ip_last_seen = Cache(
     9|             name="client_ip_last_seen", keylen=4, max_entries=50000
    10|         )
    11|     async def insert_client_ip(self, user_id, access_token, ip, user_agent, device_id):
    12|         now = int(self._clock.time_msec())
    13|         key = (user_id, access_token, ip)
    14|         try:
    15|             last_seen = self.client_ip_last_seen.get(key)
    16|         except KeyError:
    17|             last_seen = None
    18|         if last_seen is not None and (now - last_seen) < LAST_SEEN_GRANULARITY:
    19|             return
    20|         self.client_ip_last_seen.prefill(key, now)
    21|         self.hs.get_tcp_replication().send_user_ip(
    22|             user_id, access_token, ip, user_agent, device_id, now
    23|         )


# ====================================================================
# FILE: synapse/replication/slave/storage/deviceinbox.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-30 ---
     1| from synapse.replication.slave.storage._base import BaseSlavedStore
     2| from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
     3| from synapse.replication.tcp.streams import ToDeviceStream
     4| from synapse.storage.database import DatabasePool
     5| from synapse.storage.databases.main.deviceinbox import DeviceInboxWorkerStore
     6| from synapse.util.caches.expiringcache import ExpiringCache
     7| from synapse.util.caches.stream_change_cache import StreamChangeCache
     8| class SlavedDeviceInboxStore(DeviceInboxWorkerStore, BaseSlavedStore):
     9|     def __init__(self, database: DatabasePool, db_conn, hs):
    10|         super().__init__(database, db_conn, hs)
    11|         self._device_inbox_id_gen = SlavedIdTracker(
    12|             db_conn, "device_inbox", "stream_id"
    13|         )
    14|         self._device_inbox_stream_cache = StreamChangeCache(
    15|             "DeviceInboxStreamChangeCache",
    16|             self._device_inbox_id_gen.get_current_token(),
    17|         )
    18|         self._device_federation_outbox_stream_cache = StreamChangeCache(
    19|             "DeviceFederationOutboxStreamChangeCache",
    20|             self._device_inbox_id_gen.get_current_token(),
    21|         )
    22|         self._last_device_delete_cache = ExpiringCache(
    23|             cache_name="last_device_delete_cache",
    24|             clock=self._clock,
    25|             max_len=10000,
    26|             expiry_ms=30 * 60 * 1000,
    27|         )
    28|     def process_replication_rows(self, stream_name, instance_name, token, rows):
    29|         if stream_name == ToDeviceStream.NAME:
    30|             self._device_inbox_id_gen.advance(instance_name, token)


# ====================================================================
# FILE: synapse/replication/slave/storage/devices.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-30 ---
     1| from synapse.replication.slave.storage._base import BaseSlavedStore
     2| from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
     3| from synapse.replication.tcp.streams._base import DeviceListsStream, UserSignatureStream
     4| from synapse.storage.database import DatabasePool
     5| from synapse.storage.databases.main.devices import DeviceWorkerStore
     6| from synapse.storage.databases.main.end_to_end_keys import EndToEndKeyWorkerStore
     7| from synapse.util.caches.stream_change_cache import StreamChangeCache
     8| class SlavedDeviceStore(EndToEndKeyWorkerStore, DeviceWorkerStore, BaseSlavedStore):
     9|     def __init__(self, database: DatabasePool, db_conn, hs):
    10|         super().__init__(database, db_conn, hs)
    11|         self.hs = hs
    12|         self._device_list_id_gen = SlavedIdTracker(
    13|             db_conn,
    14|             "device_lists_stream",
    15|             "stream_id",
    16|             extra_tables=[
    17|                 ("user_signature_stream", "stream_id"),
    18|                 ("device_lists_outbound_pokes", "stream_id"),
    19|             ],
    20|         )
    21|         device_list_max = self._device_list_id_gen.get_current_token()
    22|         self._device_list_stream_cache = StreamChangeCache(
    23|             "DeviceListStreamChangeCache", device_list_max
    24|         )
    25|         self._user_signature_stream_cache = StreamChangeCache(
    26|             "UserSignatureStreamChangeCache", device_list_max
    27|         )
    28|         self._device_list_federation_stream_cache = StreamChangeCache(
    29|             "DeviceListFederationStreamChangeCache", device_list_max
    30|         )


# ====================================================================
# FILE: synapse/replication/slave/storage/events.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 10-48 ---
    10| from synapse.storage.databases.main.signatures import SignatureWorkerStore
    11| from synapse.storage.databases.main.state import StateGroupWorkerStore
    12| from synapse.storage.databases.main.stream import StreamWorkerStore
    13| from synapse.storage.databases.main.user_erasure_store import UserErasureWorkerStore
    14| from synapse.util.caches.stream_change_cache import StreamChangeCache
    15| from ._base import BaseSlavedStore
    16| logger = logging.getLogger(__name__)
    17| class SlavedEventStore(
    18|     EventFederationWorkerStore,
    19|     RoomMemberWorkerStore,
    20|     EventPushActionsWorkerStore,
    21|     StreamWorkerStore,
    22|     StateGroupWorkerStore,
    23|     EventsWorkerStore,
    24|     SignatureWorkerStore,
    25|     UserErasureWorkerStore,
    26|     RelationsWorkerStore,
    27|     BaseSlavedStore,
    28| ):
    29|     def __init__(self, database: DatabasePool, db_conn, hs):
    30|         super().__init__(database, db_conn, hs)
    31|         events_max = self._stream_id_gen.get_current_token()
    32|         curr_state_delta_prefill, min_curr_state_delta_id = self.db_pool.get_cache_dict(
    33|             db_conn,
    34|             "current_state_delta_stream",
    35|             entity_column="room_id",
    36|             stream_column="stream_id",
    37|             max_value=events_max,  # As we share the stream id with events token
    38|             limit=1000,
    39|         )
    40|         self._curr_state_delta_stream_cache = StreamChangeCache(
    41|             "_curr_state_delta_stream_cache",
    42|             min_curr_state_delta_id,
    43|             prefilled_cache=curr_state_delta_prefill,
    44|         )
    45|     def get_room_max_stream_ordering(self):
    46|         return self._stream_id_gen.get_current_token()
    47|     def get_room_min_stream_ordering(self):
    48|         return self._backfill_id_gen.get_current_token()


# ====================================================================
# FILE: synapse/replication/slave/storage/filtering.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-7 ---
     1| from synapse.storage.database import DatabasePool
     2| from synapse.storage.databases.main.filtering import FilteringStore
     3| from ._base import BaseSlavedStore
     4| class SlavedFilteringStore(BaseSlavedStore):
     5|     def __init__(self, database: DatabasePool, db_conn, hs):
     6|         super().__init__(database, db_conn, hs)
     7|     get_user_filter = FilteringStore.__dict__["get_user_filter"]


# ====================================================================
# FILE: synapse/replication/slave/storage/groups.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| from synapse.replication.slave.storage._base import BaseSlavedStore
     2| from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
     3| from synapse.replication.tcp.streams import GroupServerStream
     4| from synapse.storage.database import DatabasePool
     5| from synapse.storage.databases.main.group_server import GroupServerWorkerStore
     6| from synapse.util.caches.stream_change_cache import StreamChangeCache
     7| class SlavedGroupServerStore(GroupServerWorkerStore, BaseSlavedStore):
     8|     def __init__(self, database: DatabasePool, db_conn, hs):
     9|         super().__init__(database, db_conn, hs)
    10|         self.hs = hs
    11|         self._group_updates_id_gen = SlavedIdTracker(
    12|             db_conn, "local_group_updates", "stream_id"
    13|         )
    14|         self._group_updates_stream_cache = StreamChangeCache(
    15|             "_group_updates_stream_cache",
    16|             self._group_updates_id_gen.get_current_token(),
    17|         )
    18|     def get_group_stream_token(self):
    19|         return self._group_updates_id_gen.get_current_token()
    20|     def process_replication_rows(self, stream_name, instance_name, token, rows):
    21|         if stream_name == GroupServerStream.NAME:
    22|             self._group_updates_id_gen.advance(instance_name, token)
    23|             for row in rows:
    24|                 self._group_updates_stream_cache.entity_has_changed(row.user_id, token)
    25|         return super().process_replication_rows(stream_name, instance_name, token, rows)


# ====================================================================
# FILE: synapse/replication/slave/storage/presence.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| from synapse.replication.tcp.streams import PresenceStream
     2| from synapse.storage import DataStore
     3| from synapse.storage.database import DatabasePool
     4| from synapse.storage.databases.main.presence import PresenceStore
     5| from synapse.util.caches.stream_change_cache import StreamChangeCache
     6| from ._base import BaseSlavedStore
     7| from ._slaved_id_tracker import SlavedIdTracker
     8| class SlavedPresenceStore(BaseSlavedStore):
     9|     def __init__(self, database: DatabasePool, db_conn, hs):
    10|         super().__init__(database, db_conn, hs)
    11|         self._presence_id_gen = SlavedIdTracker(db_conn, "presence_stream", "stream_id")
    12|         self._presence_on_startup = self._get_active_presence(db_conn)  # type: ignore
    13|         self.presence_stream_cache = StreamChangeCache(
    14|             "PresenceStreamChangeCache", self._presence_id_gen.get_current_token()
    15|         )
    16|     _get_active_presence = DataStore._get_active_presence
    17|     take_presence_startup_info = DataStore.take_presence_startup_info
    18|     _get_presence_for_user = PresenceStore.__dict__["_get_presence_for_user"]
    19|     get_presence_for_users = PresenceStore.__dict__["get_presence_for_users"]
    20|     def get_current_presence_token(self):
    21|         return self._presence_id_gen.get_current_token()
    22|     def process_replication_rows(self, stream_name, instance_name, token, rows):
    23|         if stream_name == PresenceStream.NAME:
    24|             self._presence_id_gen.advance(instance_name, token)
    25|             for row in rows:
    26|                 self.presence_stream_cache.entity_has_changed(row.user_id, token)
    27|                 self._get_presence_for_user.invalidate((row.user_id,))
    28|         return super().process_replication_rows(stream_name, instance_name, token, rows)


# ====================================================================
# FILE: synapse/replication/slave/storage/pushers.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-17 ---
     1| from synapse.replication.tcp.streams import PushersStream
     2| from synapse.storage.database import DatabasePool
     3| from synapse.storage.databases.main.pusher import PusherWorkerStore
     4| from ._base import BaseSlavedStore
     5| from ._slaved_id_tracker import SlavedIdTracker
     6| class SlavedPusherStore(PusherWorkerStore, BaseSlavedStore):
     7|     def __init__(self, database: DatabasePool, db_conn, hs):
     8|         super().__init__(database, db_conn, hs)
     9|         self._pushers_id_gen = SlavedIdTracker(
    10|             db_conn, "pushers", "id", extra_tables=[("deleted_pushers", "stream_id")]
    11|         )
    12|     def get_pushers_stream_token(self):
    13|         return self._pushers_id_gen.get_current_token()
    14|     def process_replication_rows(self, stream_name, instance_name, token, rows):
    15|         if stream_name == PushersStream.NAME:
    16|             self._pushers_id_gen.advance(instance_name, token)
    17|         return super().process_replication_rows(stream_name, instance_name, token, rows)


# ====================================================================
# FILE: synapse/replication/slave/storage/receipts.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-30 ---
     1| from synapse.replication.tcp.streams import ReceiptsStream
     2| from synapse.storage.database import DatabasePool
     3| from synapse.storage.databases.main.receipts import ReceiptsWorkerStore
     4| from ._base import BaseSlavedStore
     5| from ._slaved_id_tracker import SlavedIdTracker
     6| class SlavedReceiptsStore(ReceiptsWorkerStore, BaseSlavedStore):
     7|     def __init__(self, database: DatabasePool, db_conn, hs):
     8|         self._receipts_id_gen = SlavedIdTracker(
     9|             db_conn, "receipts_linearized", "stream_id"
    10|         )
    11|         super().__init__(database, db_conn, hs)
    12|     def get_max_receipt_stream_id(self):
    13|         return self._receipts_id_gen.get_current_token()
    14|     def invalidate_caches_for_receipt(self, room_id, receipt_type, user_id):
    15|         self.get_receipts_for_user.invalidate((user_id, receipt_type))
    16|         self._get_linearized_receipts_for_room.invalidate_many((room_id,))
    17|         self.get_last_receipt_event_id_for_user.invalidate(
    18|             (user_id, room_id, receipt_type)
    19|         )
    20|         self._invalidate_get_users_with_receipts_in_room(room_id, receipt_type, user_id)
    21|         self.get_receipts_for_room.invalidate((room_id, receipt_type))
    22|     def process_replication_rows(self, stream_name, instance_name, token, rows):
    23|         if stream_name == ReceiptsStream.NAME:
    24|             self._receipts_id_gen.advance(instance_name, token)
    25|             for row in rows:
    26|                 self.invalidate_caches_for_receipt(
    27|                     row.room_id, row.receipt_type, row.user_id
    28|                 )
    29|                 self._receipts_stream_cache.entity_has_changed(row.room_id, token)
    30|         return super().process_replication_rows(stream_name, instance_name, token, rows)


# ====================================================================
# FILE: synapse/replication/slave/storage/room.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-17 ---
     1| from synapse.replication.tcp.streams import PublicRoomsStream
     2| from synapse.storage.database import DatabasePool
     3| from synapse.storage.databases.main.room import RoomWorkerStore
     4| from ._base import BaseSlavedStore
     5| from ._slaved_id_tracker import SlavedIdTracker
     6| class RoomStore(RoomWorkerStore, BaseSlavedStore):
     7|     def __init__(self, database: DatabasePool, db_conn, hs):
     8|         super().__init__(database, db_conn, hs)
     9|         self._public_room_id_gen = SlavedIdTracker(
    10|             db_conn, "public_room_list_stream", "stream_id"
    11|         )
    12|     def get_current_public_room_stream_id(self):
    13|         return self._public_room_id_gen.get_current_token()
    14|     def process_replication_rows(self, stream_name, instance_name, token, rows):
    15|         if stream_name == PublicRoomsStream.NAME:
    16|             self._public_room_id_gen.advance(instance_name, token)
    17|         return super().process_replication_rows(stream_name, instance_name, token, rows)


# ====================================================================
# FILE: synapse/replication/tcp/client.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-36 ---
     1| """A replication client for use by synapse workers.
     2| """
     3| import logging
     4| from typing import TYPE_CHECKING, Dict, List, Tuple
     5| from twisted.internet.defer import Deferred
     6| from twisted.internet.protocol import ReconnectingClientFactory
     7| from synapse.api.constants import EventTypes
     8| from synapse.logging.context import PreserveLoggingContext, make_deferred_yieldable
     9| from synapse.replication.tcp.protocol import ClientReplicationStreamProtocol
    10| from synapse.replication.tcp.streams import TypingStream
    11| from synapse.replication.tcp.streams.events import (
    12|     EventsStream,
    13|     EventsStreamEventRow,
    14|     EventsStreamRow,
    15| )
    16| from synapse.types import PersistedEventPosition, UserID
    17| from synapse.util.async_helpers import timeout_deferred
    18| from synapse.util.metrics import Measure
    19| if TYPE_CHECKING:
    20|     from synapse.replication.tcp.handler import ReplicationCommandHandler
    21|     from synapse.server import HomeServer
    22| logger = logging.getLogger(__name__)
    23| _WAIT_FOR_REPLICATION_TIMEOUT_SECONDS = 30
    24| class DirectTcpReplicationClientFactory(ReconnectingClientFactory):
    25|     """Factory for building connections to the master. Will reconnect if the
    26|     connection is lost.
    27|     Accepts a handler that is passed to `ClientReplicationStreamProtocol`.
    28|     """
    29|     initialDelay = 0.1
    30|     maxDelay = 1  # Try at least once every N seconds
    31|     def __init__(
    32|         self,
    33|         hs: "HomeServer",
    34|         client_name: str,
    35|         command_handler: "ReplicationCommandHandler",
    36|     ):

# --- HUNK 2: Lines 47-131 ---
    47|         return ClientReplicationStreamProtocol(
    48|             self.hs,
    49|             self.client_name,
    50|             self.server_name,
    51|             self._clock,
    52|             self.command_handler,
    53|         )
    54|     def clientConnectionLost(self, connector, reason):
    55|         logger.error("Lost replication conn: %r", reason)
    56|         ReconnectingClientFactory.clientConnectionLost(self, connector, reason)
    57|     def clientConnectionFailed(self, connector, reason):
    58|         logger.error("Failed to connect to replication: %r", reason)
    59|         ReconnectingClientFactory.clientConnectionFailed(self, connector, reason)
    60| class ReplicationDataHandler:
    61|     """Handles incoming stream updates from replication.
    62|     This instance notifies the slave data store about updates. Can be subclassed
    63|     to handle updates in additional ways.
    64|     """
    65|     def __init__(self, hs: "HomeServer"):
    66|         self.store = hs.get_datastore()
    67|         self.notifier = hs.get_notifier()
    68|         self._reactor = hs.get_reactor()
    69|         self._clock = hs.get_clock()
    70|         self._streams = hs.get_replication_streams()
    71|         self._instance_name = hs.get_instance_name()
    72|         self._typing_handler = hs.get_typing_handler()
    73|         self._streams_to_waiters = (
    74|             {}
    75|         )  # type: Dict[str, List[Tuple[int, Deferred[None]]]]
    76|     async def on_rdata(
    77|         self, stream_name: str, instance_name: str, token: int, rows: list
    78|     ):
    79|         """Called to handle a batch of replication data with a given stream token.
    80|         By default this just pokes the slave store. Can be overridden in subclasses to
    81|         handle more.
    82|         Args:
    83|             stream_name: name of the replication stream for this batch of rows
    84|             instance_name: the instance that wrote the rows.
    85|             token: stream token for this batch of rows
    86|             rows: a list of Stream.ROW_TYPE objects as returned by Stream.parse_row.
    87|         """
    88|         self.store.process_replication_rows(stream_name, instance_name, token, rows)
    89|         if stream_name == TypingStream.NAME:
    90|             self._typing_handler.process_replication_rows(token, rows)
    91|             self.notifier.on_new_event(
    92|                 "typing_key", token, rooms=[row.room_id for row in rows]
    93|             )
    94|         if stream_name == EventsStream.NAME:
    95|             for row in rows:
    96|                 if row.type != EventsStreamEventRow.TypeId:
    97|                     continue
    98|                 assert isinstance(row, EventsStreamRow)
    99|                 event = await self.store.get_event(
   100|                     row.data.event_id, allow_rejected=True
   101|                 )
   102|                 if event.rejected_reason:
   103|                     continue
   104|                 extra_users = ()  # type: Tuple[UserID, ...]
   105|                 if event.type == EventTypes.Member:
   106|                     extra_users = (UserID.from_string(event.state_key),)
   107|                 max_token = self.store.get_room_max_token()
   108|                 event_pos = PersistedEventPosition(instance_name, token)
   109|                 self.notifier.on_new_room_event(
   110|                     event, event_pos, max_token, extra_users
   111|                 )
   112|         waiting_list = self._streams_to_waiters.get(stream_name, [])
   113|         index_of_first_deferred_not_called = len(waiting_list)
   114|         for idx, (position, deferred) in enumerate(waiting_list):
   115|             if position <= token:
   116|                 try:
   117|                     with PreserveLoggingContext():
   118|                         deferred.callback(None)
   119|                 except Exception:
   120|                     pass
   121|             else:
   122|                 index_of_first_deferred_not_called = idx
   123|                 break
   124|         waiting_list[:] = waiting_list[index_of_first_deferred_not_called:]
   125|     async def on_position(self, stream_name: str, instance_name: str, token: int):
   126|         self.store.process_replication_rows(stream_name, instance_name, token, [])
   127|     def on_remote_server_up(self, server: str):
   128|         """Called when get a new REMOTE_SERVER_UP command."""
   129|     async def wait_for_stream_position(
   130|         self, instance_name: str, stream_name: str, position: int
   131|     ):


# ====================================================================
# FILE: synapse/replication/tcp/handler.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 55-95 ---
    55|     """Handles incoming commands from replication as well as sending commands
    56|     back out to connections.
    57|     """
    58|     def __init__(self, hs):
    59|         self._replication_data_handler = hs.get_replication_data_handler()
    60|         self._presence_handler = hs.get_presence_handler()
    61|         self._store = hs.get_datastore()
    62|         self._notifier = hs.get_notifier()
    63|         self._clock = hs.get_clock()
    64|         self._instance_id = hs.get_instance_id()
    65|         self._instance_name = hs.get_instance_name()
    66|         self._streams = {
    67|             stream.NAME: stream(hs) for stream in STREAMS_MAP.values()
    68|         }  # type: Dict[str, Stream]
    69|         self._streams_to_replicate = []  # type: List[Stream]
    70|         for stream in self._streams.values():
    71|             if stream.NAME == CachesStream.NAME:
    72|                 self._streams_to_replicate.append(stream)
    73|                 continue
    74|             if isinstance(stream, (EventsStream, BackfillStream)):
    75|                 if hs.get_instance_name() in hs.config.worker.writers.events:
    76|                     self._streams_to_replicate.append(stream)
    77|                 continue
    78|             if isinstance(stream, TypingStream):
    79|                 if hs.config.worker.writers.typing == hs.get_instance_name():
    80|                     self._streams_to_replicate.append(stream)
    81|                 continue
    82|             if hs.config.worker_app is not None:
    83|                 continue
    84|             if stream.NAME == FederationStream.NAME and hs.config.send_federation:
    85|                 continue
    86|             self._streams_to_replicate.append(stream)
    87|         self._pending_batches = {}  # type: Dict[str, List[Any]]
    88|         self._factory = None  # type: Optional[ReconnectingClientFactory]
    89|         self._connections = []  # type: List[AbstractConnection]
    90|         LaterGauge(
    91|             "synapse_replication_tcp_resource_total_connections",
    92|             "",
    93|             [],
    94|             lambda: len(self._connections),
    95|         )


# ====================================================================
# FILE: synapse/replication/tcp/streams/_base.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 209-277 ---
   209|             "data",  # dict
   210|         ),
   211|     )
   212|     NAME = "receipts"
   213|     ROW_TYPE = ReceiptsStreamRow
   214|     def __init__(self, hs):
   215|         store = hs.get_datastore()
   216|         super().__init__(
   217|             hs.get_instance_name(),
   218|             current_token_without_instance(store.get_max_receipt_stream_id),
   219|             store.get_all_updated_receipts,
   220|         )
   221| class PushRulesStream(Stream):
   222|     """A user has changed their push rules
   223|     """
   224|     PushRulesStreamRow = namedtuple("PushRulesStreamRow", ("user_id",))  # str
   225|     NAME = "push_rules"
   226|     ROW_TYPE = PushRulesStreamRow
   227|     def __init__(self, hs):
   228|         self.store = hs.get_datastore()
   229|         super().__init__(
   230|             hs.get_instance_name(),
   231|             self._current_token,
   232|             self.store.get_all_push_rule_updates,
   233|         )
   234|     def _current_token(self, instance_name: str) -> int:
   235|         push_rules_token = self.store.get_max_push_rules_stream_id()
   236|         return push_rules_token
   237| class PushersStream(Stream):
   238|     """A user has added/changed/removed a pusher
   239|     """
   240|     PushersStreamRow = namedtuple(
   241|         "PushersStreamRow",
   242|         ("user_id", "app_id", "pushkey", "deleted"),  # str  # str  # str  # bool
   243|     )
   244|     NAME = "pushers"
   245|     ROW_TYPE = PushersStreamRow
   246|     def __init__(self, hs):
   247|         store = hs.get_datastore()
   248|         super().__init__(
   249|             hs.get_instance_name(),
   250|             current_token_without_instance(store.get_pushers_stream_token),
   251|             store.get_all_updated_pushers_rows,
   252|         )
   253| class CachesStream(Stream):
   254|     """A cache was invalidated on the master and no other stream would invalidate
   255|     the cache on the workers
   256|     """
   257|     @attr.s(slots=True)
   258|     class CachesStreamRow:
   259|         """Stream to inform workers they should invalidate their cache.
   260|         Attributes:
   261|             cache_func: Name of the cached function.
   262|             keys: The entry in the cache to invalidate. If None then will
   263|                 invalidate all.
   264|             invalidation_ts: Timestamp of when the invalidation took place.
   265|         """
   266|         cache_func = attr.ib(type=str)
   267|         keys = attr.ib(type=Optional[List[Any]])
   268|         invalidation_ts = attr.ib(type=int)
   269|     NAME = "caches"
   270|     ROW_TYPE = CachesStreamRow
   271|     def __init__(self, hs):
   272|         store = hs.get_datastore()
   273|         super().__init__(
   274|             hs.get_instance_name(),
   275|             store.get_cache_stream_token_for_writer,
   276|             store.get_all_updated_caches,
   277|         )

# --- HUNK 2: Lines 283-323 ---
   283|         (
   284|             "room_id",  # str
   285|             "visibility",  # str
   286|             "appservice_id",  # str, optional
   287|             "network_id",  # str, optional
   288|         ),
   289|     )
   290|     NAME = "public_rooms"
   291|     ROW_TYPE = PublicRoomsStreamRow
   292|     def __init__(self, hs):
   293|         store = hs.get_datastore()
   294|         super().__init__(
   295|             hs.get_instance_name(),
   296|             current_token_without_instance(store.get_current_public_room_stream_id),
   297|             store.get_all_new_public_rooms,
   298|         )
   299| class DeviceListsStream(Stream):
   300|     """Either a user has updated their devices or a remote server needs to be
   301|     told about a device update.
   302|     """
   303|     @attr.s(slots=True)
   304|     class DeviceListsStreamRow:
   305|         entity = attr.ib(type=str)
   306|     NAME = "device_lists"
   307|     ROW_TYPE = DeviceListsStreamRow
   308|     def __init__(self, hs):
   309|         store = hs.get_datastore()
   310|         super().__init__(
   311|             hs.get_instance_name(),
   312|             current_token_without_instance(store.get_device_stream_token),
   313|             store.get_all_device_list_changes_for_remotes,
   314|         )
   315| class ToDeviceStream(Stream):
   316|     """New to_device messages for a client
   317|     """
   318|     ToDeviceStreamRow = namedtuple("ToDeviceStreamRow", ("entity",))  # str
   319|     NAME = "to_device"
   320|     ROW_TYPE = ToDeviceStreamRow
   321|     def __init__(self, hs):
   322|         store = hs.get_datastore()
   323|         super().__init__(


# ====================================================================
# FILE: synapse/replication/tcp/streams/events.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| import heapq
     2| from collections.abc import Iterable
     3| from typing import List, Tuple, Type
     4| import attr
     5| from ._base import Stream, StreamUpdateResult, Token
     6| """Handling of the 'events' replication stream
     7| This stream contains rows of various types. Each row therefore contains a 'type'
     8| identifier before the real data. For example::
     9|     RDATA events batch ["state", ["!room:id", "m.type", "", "$event:id"]]
    10|     RDATA events 12345 ["ev", ["$event:id", "!room:id", "m.type", null, null]]
    11| An "ev" row is sent for each new event. The fields in the data part are:
    12|  * The new event id
    13|  * The room id for the event
    14|  * The type of the new event
    15|  * The state key of the event, for state events
    16|  * The event id of an event which is redacted by this event.
    17| A "state" row is sent whenever the "current state" in a room changes. The fields in the
    18| data part are:
    19|  * The room id for the state change
    20|  * The event type of the state which has changed
    21|  * The state_key of the state which has changed
    22|  * The event id of the new state
    23| """
    24| @attr.s(slots=True, frozen=True)
    25| class EventsStreamRow:

# --- HUNK 2: Lines 51-91 ---
    51| @attr.s(slots=True, frozen=True)
    52| class EventsStreamCurrentStateRow(BaseEventsStreamRow):
    53|     TypeId = "state"
    54|     room_id = attr.ib()  # str
    55|     type = attr.ib()  # str
    56|     state_key = attr.ib()  # str
    57|     event_id = attr.ib()  # str, optional
    58| _EventRows = (
    59|     EventsStreamEventRow,
    60|     EventsStreamCurrentStateRow,
    61| )  # type: Tuple[Type[BaseEventsStreamRow], ...]
    62| TypeToRow = {Row.TypeId: Row for Row in _EventRows}
    63| class EventsStream(Stream):
    64|     """We received a new event, or an event went from being an outlier to not
    65|     """
    66|     NAME = "events"
    67|     def __init__(self, hs):
    68|         self._store = hs.get_datastore()
    69|         super().__init__(
    70|             hs.get_instance_name(),
    71|             self._store._stream_id_gen.get_current_token_for_writer,
    72|             self._update_function,
    73|         )
    74|     async def _update_function(
    75|         self,
    76|         instance_name: str,
    77|         from_token: Token,
    78|         current_token: Token,
    79|         target_row_count: int,
    80|     ) -> StreamUpdateResult:
    81|         target_row_count //= 2
    82|         event_rows = await self._store.get_all_new_forward_event_rows(
    83|             from_token, current_token, target_row_count
    84|         )  # type: List[Tuple]
    85|         assert (
    86|             len(event_rows) <= target_row_count
    87|         ), "get_all_new_forward_event_rows did not honour row limit"
    88|         if len(event_rows) == target_row_count:
    89|             limited = True
    90|             upper_limit = event_rows[-1][0]  # type: int
    91|         else:


# ====================================================================
# FILE: synapse/rest/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-22 ---
     1| from synapse.http.server import JsonResource
     2| from synapse.rest import admin
     3| from synapse.rest.client import versions
     4| from synapse.rest.client.v1 import (
     5|     directory,
     6|     events,
     7|     initial_sync,
     8|     login as v1_login,
     9|     logout,
    10|     presence,
    11|     profile,
    12|     push_rule,
    13|     pusher,
    14|     room,
    15|     voip,
    16| )
    17| from synapse.rest.client.v2_alpha import (
    18|     account,
    19|     account_data,
    20|     account_validity,
    21|     auth,
    22|     capabilities,

# --- HUNK 2: Lines 76-97 ---
    76|         receipts.register_servlets(hs, client_resource)
    77|         read_marker.register_servlets(hs, client_resource)
    78|         room_keys.register_servlets(hs, client_resource)
    79|         keys.register_servlets(hs, client_resource)
    80|         tokenrefresh.register_servlets(hs, client_resource)
    81|         tags.register_servlets(hs, client_resource)
    82|         account_data.register_servlets(hs, client_resource)
    83|         report_event.register_servlets(hs, client_resource)
    84|         openid.register_servlets(hs, client_resource)
    85|         notifications.register_servlets(hs, client_resource)
    86|         devices.register_servlets(hs, client_resource)
    87|         thirdparty.register_servlets(hs, client_resource)
    88|         sendtodevice.register_servlets(hs, client_resource)
    89|         user_directory.register_servlets(hs, client_resource)
    90|         groups.register_servlets(hs, client_resource)
    91|         room_upgrade_rest_servlet.register_servlets(hs, client_resource)
    92|         capabilities.register_servlets(hs, client_resource)
    93|         account_validity.register_servlets(hs, client_resource)
    94|         relations.register_servlets(hs, client_resource)
    95|         password_policy.register_servlets(hs, client_resource)
    96|         admin.register_servlets_for_client_rest_resource(hs, client_resource)
    97|         shared_rooms.register_servlets(hs, client_resource)


# ====================================================================
# FILE: synapse/rest/admin/__init__.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-97 ---
     1| import logging
     2| import platform
     3| import synapse
     4| from synapse.api.errors import Codes, NotFoundError, SynapseError
     5| from synapse.http.server import JsonResource
     6| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     7| from synapse.rest.admin._base import (
     8|     admin_patterns,
     9|     assert_requester_is_admin,
    10|     historical_admin_path_patterns,
    11| )
    12| from synapse.rest.admin.devices import (
    13|     DeleteDevicesRestServlet,
    14|     DeviceRestServlet,
    15|     DevicesRestServlet,
    16| )
    17| from synapse.rest.admin.event_reports import EventReportsRestServlet
    18| from synapse.rest.admin.groups import DeleteGroupAdminRestServlet
    19| from synapse.rest.admin.media import ListMediaInRoom, register_servlets_for_media_repo
    20| from synapse.rest.admin.purge_room_servlet import PurgeRoomServlet
    21| from synapse.rest.admin.rooms import (
    22|     DeleteRoomRestServlet,
    23|     JoinRoomAliasServlet,
    24|     ListRoomRestServlet,
    25|     RoomMembersRestServlet,
    26|     RoomRestServlet,
    27|     ShutdownRoomRestServlet,
    28| )
    29| from synapse.rest.admin.server_notice_servlet import SendServerNoticeServlet
    30| from synapse.rest.admin.users import (
    31|     AccountValidityRenewServlet,
    32|     DeactivateAccountRestServlet,
    33|     ResetPasswordRestServlet,
    34|     SearchUsersRestServlet,
    35|     UserAdminServlet,
    36|     UserMembershipRestServlet,
    37|     UserRegisterServlet,
    38|     UserRestServletV2,
    39|     UsersRestServlet,
    40|     UsersRestServletV2,
    41|     WhoisRestServlet,
    42| )
    43| from synapse.util.versionstring import get_version_string
    44| logger = logging.getLogger(__name__)
    45| class VersionServlet(RestServlet):
    46|     PATTERNS = admin_patterns("/server_version$")
    47|     def __init__(self, hs):
    48|         self.res = {
    49|             "server_version": get_version_string(synapse),
    50|             "python_version": platform.python_version(),
    51|         }
    52|     def on_GET(self, request):
    53|         return 200, self.res
    54| class PurgeHistoryRestServlet(RestServlet):
    55|     PATTERNS = historical_admin_path_patterns(
    56|         "/purge_history/(?P<room_id>[^/]*)(/(?P<event_id>[^/]+))?"
    57|     )
    58|     def __init__(self, hs):
    59|         """
    60|         Args:
    61|             hs (synapse.server.HomeServer)
    62|         """
    63|         self.pagination_handler = hs.get_pagination_handler()
    64|         self.store = hs.get_datastore()
    65|         self.auth = hs.get_auth()
    66|     async def on_POST(self, request, room_id, event_id):
    67|         await assert_requester_is_admin(self.auth, request)
    68|         body = parse_json_object_from_request(request, allow_empty_body=True)
    69|         delete_local_events = bool(body.get("delete_local_events", False))
    70|         if event_id is None:
    71|             event_id = body.get("purge_up_to_event_id")
    72|         if event_id is not None:
    73|             event = await self.store.get_event(event_id)
    74|             if event.room_id != room_id:
    75|                 raise SynapseError(400, "Event is for wrong room.")
    76|             room_token = await self.store.get_topological_token_for_event(event_id)
    77|             token = await room_token.to_string(self.store)
    78|             logger.info("[purge] purging up to token %s (event_id %s)", token, event_id)
    79|         elif "purge_up_to_ts" in body:
    80|             ts = body["purge_up_to_ts"]
    81|             if not isinstance(ts, int):
    82|                 raise SynapseError(
    83|                     400, "purge_up_to_ts must be an int", errcode=Codes.BAD_JSON
    84|                 )
    85|             stream_ordering = await self.store.find_first_stream_ordering_after_ts(ts)
    86|             r = await self.store.get_room_event_before_stream_ordering(
    87|                 room_id, stream_ordering
    88|             )
    89|             if not r:
    90|                 logger.warning(
    91|                     "[purge] purging events not possible: No event found "
    92|                     "(received_ts %i => stream_ordering %i)",
    93|                     ts,
    94|                     stream_ordering,
    95|                 )
    96|                 raise SynapseError(
    97|                     404, "there is no event to be purged", errcode=Codes.NOT_FOUND

# --- HUNK 2: Lines 134-177 ---
   134|         return 200, purge_status.asdict()
   135| class AdminRestResource(JsonResource):
   136|     """The REST resource which gets mounted at /_synapse/admin"""
   137|     def __init__(self, hs):
   138|         JsonResource.__init__(self, hs, canonical_json=False)
   139|         register_servlets(hs, self)
   140| def register_servlets(hs, http_server):
   141|     """
   142|     Register all the admin servlets.
   143|     """
   144|     register_servlets_for_client_rest_resource(hs, http_server)
   145|     ListRoomRestServlet(hs).register(http_server)
   146|     RoomRestServlet(hs).register(http_server)
   147|     RoomMembersRestServlet(hs).register(http_server)
   148|     DeleteRoomRestServlet(hs).register(http_server)
   149|     JoinRoomAliasServlet(hs).register(http_server)
   150|     PurgeRoomServlet(hs).register(http_server)
   151|     SendServerNoticeServlet(hs).register(http_server)
   152|     VersionServlet(hs).register(http_server)
   153|     UserAdminServlet(hs).register(http_server)
   154|     UserMembershipRestServlet(hs).register(http_server)
   155|     UserRestServletV2(hs).register(http_server)
   156|     UsersRestServletV2(hs).register(http_server)
   157|     DeviceRestServlet(hs).register(http_server)
   158|     DevicesRestServlet(hs).register(http_server)
   159|     DeleteDevicesRestServlet(hs).register(http_server)
   160|     EventReportsRestServlet(hs).register(http_server)
   161| def register_servlets_for_client_rest_resource(hs, http_server):
   162|     """Register only the servlets which need to be exposed on /_matrix/client/xxx"""
   163|     WhoisRestServlet(hs).register(http_server)
   164|     PurgeHistoryStatusRestServlet(hs).register(http_server)
   165|     DeactivateAccountRestServlet(hs).register(http_server)
   166|     PurgeHistoryRestServlet(hs).register(http_server)
   167|     UsersRestServlet(hs).register(http_server)
   168|     ResetPasswordRestServlet(hs).register(http_server)
   169|     SearchUsersRestServlet(hs).register(http_server)
   170|     ShutdownRoomRestServlet(hs).register(http_server)
   171|     UserRegisterServlet(hs).register(http_server)
   172|     DeleteGroupAdminRestServlet(hs).register(http_server)
   173|     AccountValidityRenewServlet(hs).register(http_server)
   174|     if hs.config.can_load_media_repo:
   175|         register_servlets_for_media_repo(hs, http_server)
   176|     else:
   177|         ListMediaInRoom(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/admin/_base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 4-52 ---
     4| from synapse.api.errors import AuthError
     5| from synapse.types import UserID
     6| def historical_admin_path_patterns(path_regex):
     7|     """Returns the list of patterns for an admin endpoint, including historical ones
     8|     This is a backwards-compatibility hack. Previously, the Admin API was exposed at
     9|     various paths under /_matrix/client. This function returns a list of patterns
    10|     matching those paths (as well as the new one), so that existing scripts which rely
    11|     on the endpoints being available there are not broken.
    12|     Note that this should only be used for existing endpoints: new ones should just
    13|     register for the /_synapse/admin path.
    14|     """
    15|     return [
    16|         re.compile(prefix + path_regex)
    17|         for prefix in (
    18|             "^/_synapse/admin/v1",
    19|             "^/_matrix/client/api/v1/admin",
    20|             "^/_matrix/client/unstable/admin",
    21|             "^/_matrix/client/r0/admin",
    22|         )
    23|     ]
    24| def admin_patterns(path_regex: str, version: str = "v1"):
    25|     """Returns the list of patterns for an admin endpoint
    26|     Args:
    27|         path_regex: The regex string to match. This should NOT have a ^
    28|             as this will be prefixed.
    29|     Returns:
    30|         A list of regex patterns.
    31|     """
    32|     admin_prefix = "^/_synapse/admin/" + version
    33|     patterns = [re.compile(admin_prefix + path_regex)]
    34|     return patterns
    35| async def assert_requester_is_admin(
    36|     auth: synapse.api.auth.Auth, request: twisted.web.server.Request
    37| ) -> None:
    38|     """Verify that the requester is an admin user
    39|     Args:
    40|         auth: api.auth.Auth singleton
    41|         request: incoming request
    42|     Raises:
    43|         AuthError if the requester is not a server admin
    44|     """
    45|     requester = await auth.get_user_by_req(request)
    46|     await assert_user_is_admin(auth, requester.user)
    47| async def assert_user_is_admin(auth: synapse.api.auth.Auth, user_id: UserID) -> None:
    48|     """Verify that the given user is an admin user
    49|     Args:
    50|         auth: api.auth.Auth singleton
    51|         user_id: user to check
    52|     Raises:


# ====================================================================
# FILE: synapse/rest/admin/devices.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-39 ---
     1| import logging
     2| from synapse.api.errors import NotFoundError, SynapseError
     3| from synapse.http.servlet import (
     4|     RestServlet,
     5|     assert_params_in_dict,
     6|     parse_json_object_from_request,
     7| )
     8| from synapse.rest.admin._base import admin_patterns, assert_requester_is_admin
     9| from synapse.types import UserID
    10| logger = logging.getLogger(__name__)
    11| class DeviceRestServlet(RestServlet):
    12|     """
    13|     Get, update or delete the given user's device
    14|     """
    15|     PATTERNS = admin_patterns(
    16|         "/users/(?P<user_id>[^/]*)/devices/(?P<device_id>[^/]*)$", "v2"
    17|     )
    18|     def __init__(self, hs):
    19|         super().__init__()
    20|         self.hs = hs
    21|         self.auth = hs.get_auth()
    22|         self.device_handler = hs.get_device_handler()
    23|         self.store = hs.get_datastore()
    24|     async def on_GET(self, request, user_id, device_id):
    25|         await assert_requester_is_admin(self.auth, request)
    26|         target_user = UserID.from_string(user_id)
    27|         if not self.hs.is_mine(target_user):
    28|             raise SynapseError(400, "Can only lookup local users")
    29|         u = await self.store.get_user_by_id(target_user.to_string())
    30|         if u is None:
    31|             raise NotFoundError("Unknown user")
    32|         device = await self.device_handler.get_device(
    33|             target_user.to_string(), device_id
    34|         )
    35|         return 200, device
    36|     async def on_DELETE(self, request, user_id, device_id):
    37|         await assert_requester_is_admin(self.auth, request)
    38|         target_user = UserID.from_string(user_id)
    39|         if not self.hs.is_mine(target_user):

# --- HUNK 2: Lines 43-107 ---
    43|             raise NotFoundError("Unknown user")
    44|         await self.device_handler.delete_device(target_user.to_string(), device_id)
    45|         return 200, {}
    46|     async def on_PUT(self, request, user_id, device_id):
    47|         await assert_requester_is_admin(self.auth, request)
    48|         target_user = UserID.from_string(user_id)
    49|         if not self.hs.is_mine(target_user):
    50|             raise SynapseError(400, "Can only lookup local users")
    51|         u = await self.store.get_user_by_id(target_user.to_string())
    52|         if u is None:
    53|             raise NotFoundError("Unknown user")
    54|         body = parse_json_object_from_request(request, allow_empty_body=True)
    55|         await self.device_handler.update_device(
    56|             target_user.to_string(), device_id, body
    57|         )
    58|         return 200, {}
    59| class DevicesRestServlet(RestServlet):
    60|     """
    61|     Retrieve the given user's devices
    62|     """
    63|     PATTERNS = admin_patterns("/users/(?P<user_id>[^/]*)/devices$", "v2")
    64|     def __init__(self, hs):
    65|         """
    66|         Args:
    67|             hs (synapse.server.HomeServer): server
    68|         """
    69|         self.hs = hs
    70|         self.auth = hs.get_auth()
    71|         self.device_handler = hs.get_device_handler()
    72|         self.store = hs.get_datastore()
    73|     async def on_GET(self, request, user_id):
    74|         await assert_requester_is_admin(self.auth, request)
    75|         target_user = UserID.from_string(user_id)
    76|         if not self.hs.is_mine(target_user):
    77|             raise SynapseError(400, "Can only lookup local users")
    78|         u = await self.store.get_user_by_id(target_user.to_string())
    79|         if u is None:
    80|             raise NotFoundError("Unknown user")
    81|         devices = await self.device_handler.get_devices_by_user(target_user.to_string())
    82|         return 200, {"devices": devices}
    83| class DeleteDevicesRestServlet(RestServlet):
    84|     """
    85|     API for bulk deletion of devices. Accepts a JSON object with a devices
    86|     key which lists the device_ids to delete.
    87|     """
    88|     PATTERNS = admin_patterns("/users/(?P<user_id>[^/]*)/delete_devices$", "v2")
    89|     def __init__(self, hs):
    90|         self.hs = hs
    91|         self.auth = hs.get_auth()
    92|         self.device_handler = hs.get_device_handler()
    93|         self.store = hs.get_datastore()
    94|     async def on_POST(self, request, user_id):
    95|         await assert_requester_is_admin(self.auth, request)
    96|         target_user = UserID.from_string(user_id)
    97|         if not self.hs.is_mine(target_user):
    98|             raise SynapseError(400, "Can only lookup local users")
    99|         u = await self.store.get_user_by_id(target_user.to_string())
   100|         if u is None:
   101|             raise NotFoundError("Unknown user")
   102|         body = parse_json_object_from_request(request, allow_empty_body=False)
   103|         assert_params_in_dict(body, ["devices"])
   104|         await self.device_handler.delete_devices(
   105|             target_user.to_string(), body["devices"]
   106|         )
   107|         return 200, {}


# ====================================================================
# FILE: synapse/rest/admin/event_reports.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-58 ---
     1| import logging
     2| from synapse.api.errors import Codes, SynapseError
     3| from synapse.http.servlet import RestServlet, parse_integer, parse_string
     4| from synapse.rest.admin._base import admin_patterns, assert_requester_is_admin
     5| logger = logging.getLogger(__name__)
     6| class EventReportsRestServlet(RestServlet):
     7|     """
     8|     List all reported events that are known to the homeserver. Results are returned
     9|     in a dictionary containing report information. Supports pagination.
    10|     The requester must have administrator access in Synapse.
    11|     GET /_synapse/admin/v1/event_reports
    12|     returns:
    13|         200 OK with list of reports if success otherwise an error.
    14|     Args:
    15|         The parameters `from` and `limit` are required only for pagination.
    16|         By default, a `limit` of 100 is used.
    17|         The parameter `dir` can be used to define the order of results.
    18|         The parameter `user_id` can be used to filter by user id.
    19|         The parameter `room_id` can be used to filter by room id.
    20|     Returns:
    21|         A list of reported events and an integer representing the total number of
    22|         reported events that exist given this query
    23|     """
    24|     PATTERNS = admin_patterns("/event_reports$")
    25|     def __init__(self, hs):
    26|         self.hs = hs
    27|         self.auth = hs.get_auth()
    28|         self.store = hs.get_datastore()
    29|     async def on_GET(self, request):
    30|         await assert_requester_is_admin(self.auth, request)
    31|         start = parse_integer(request, "from", default=0)
    32|         limit = parse_integer(request, "limit", default=100)
    33|         direction = parse_string(request, "dir", default="b")
    34|         user_id = parse_string(request, "user_id")
    35|         room_id = parse_string(request, "room_id")
    36|         if start < 0:
    37|             raise SynapseError(
    38|                 400,
    39|                 "The start parameter must be a positive integer.",
    40|                 errcode=Codes.INVALID_PARAM,
    41|             )
    42|         if limit < 0:
    43|             raise SynapseError(
    44|                 400,
    45|                 "The limit parameter must be a positive integer.",
    46|                 errcode=Codes.INVALID_PARAM,
    47|             )
    48|         if direction not in ("f", "b"):
    49|             raise SynapseError(
    50|                 400, "Unknown direction: %s" % (direction,), errcode=Codes.INVALID_PARAM
    51|             )
    52|         event_reports, total = await self.store.get_event_reports_paginate(
    53|             start, limit, direction, user_id, room_id
    54|         )
    55|         ret = {"event_reports": event_reports, "total": total}
    56|         if (start + limit) < total:
    57|             ret["next_token"] = start + len(event_reports)
    58|         return 200, ret


# ====================================================================
# FILE: synapse/rest/admin/purge_room_servlet.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| from synapse.http.servlet import (
     2|     RestServlet,
     3|     assert_params_in_dict,
     4|     parse_json_object_from_request,
     5| )
     6| from synapse.rest.admin import assert_requester_is_admin
     7| from synapse.rest.admin._base import admin_patterns
     8| class PurgeRoomServlet(RestServlet):
     9|     """Servlet which will remove all trace of a room from the database
    10|     POST /_synapse/admin/v1/purge_room
    11|     {
    12|         "room_id": "!room:id"
    13|     }
    14|     returns:
    15|     {}
    16|     """
    17|     PATTERNS = admin_patterns("/purge_room$")
    18|     def __init__(self, hs):
    19|         """
    20|         Args:
    21|             hs (synapse.server.HomeServer): server
    22|         """
    23|         self.hs = hs
    24|         self.auth = hs.get_auth()
    25|         self.pagination_handler = hs.get_pagination_handler()
    26|     async def on_POST(self, request):
    27|         await assert_requester_is_admin(self.auth, request)
    28|         body = parse_json_object_from_request(request)
    29|         assert_params_in_dict(body, ("room_id",))
    30|         await self.pagination_handler.purge_room(body["room_id"])
    31|         return 200, {}


# ====================================================================
# FILE: synapse/rest/admin/server_notice_servlet.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-63 ---
     1| from synapse.api.constants import EventTypes
     2| from synapse.api.errors import SynapseError
     3| from synapse.http.servlet import (
     4|     RestServlet,
     5|     assert_params_in_dict,
     6|     parse_json_object_from_request,
     7| )
     8| from synapse.rest.admin import assert_requester_is_admin
     9| from synapse.rest.admin._base import admin_patterns
    10| from synapse.rest.client.transactions import HttpTransactionCache
    11| from synapse.types import UserID
    12| class SendServerNoticeServlet(RestServlet):
    13|     """Servlet which will send a server notice to a given user
    14|     POST /_synapse/admin/v1/send_server_notice
    15|     {
    16|         "user_id": "@target_user:server_name",
    17|         "content": {
    18|             "msgtype": "m.text",
    19|             "body": "This is my message"
    20|         }
    21|     }
    22|     returns:
    23|     {
    24|         "event_id": "$1895723857jgskldgujpious"
    25|     }
    26|     """
    27|     def __init__(self, hs):
    28|         """
    29|         Args:
    30|             hs (synapse.server.HomeServer): server
    31|         """
    32|         self.hs = hs
    33|         self.auth = hs.get_auth()
    34|         self.txns = HttpTransactionCache(hs)
    35|         self.snm = hs.get_server_notices_manager()
    36|     def register(self, json_resource):
    37|         PATTERN = "/send_server_notice"
    38|         json_resource.register_paths(
    39|             "POST", admin_patterns(PATTERN + "$"), self.on_POST, self.__class__.__name__
    40|         )
    41|         json_resource.register_paths(
    42|             "PUT",
    43|             admin_patterns(PATTERN + "/(?P<txn_id>[^/]*)$"),
    44|             self.on_PUT,
    45|             self.__class__.__name__,
    46|         )
    47|     async def on_POST(self, request, txn_id=None):
    48|         await assert_requester_is_admin(self.auth, request)
    49|         body = parse_json_object_from_request(request)
    50|         assert_params_in_dict(body, ("user_id", "content"))
    51|         event_type = body.get("type", EventTypes.Message)
    52|         state_key = body.get("state_key")
    53|         if not self.snm.is_enabled():
    54|             raise SynapseError(400, "Server notices are not enabled on this server")
    55|         user_id = body["user_id"]
    56|         UserID.from_string(user_id)
    57|         if not self.hs.is_mine_id(user_id):
    58|             raise SynapseError(400, "Server notices can only be sent to local users")
    59|         event = await self.snm.send_notice(
    60|             user_id=body["user_id"],
    61|             type=event_type,
    62|             state_key=state_key,
    63|             event_content=body["content"],


# ====================================================================
# FILE: synapse/rest/admin/users.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 1-92 ---
     1| import hashlib
     2| import hmac
     3| import logging
     4| from http import HTTPStatus
     5| from synapse.api.constants import UserTypes
     6| from synapse.api.errors import Codes, NotFoundError, SynapseError
     7| from synapse.http.servlet import (
     8|     RestServlet,
     9|     assert_params_in_dict,
    10|     parse_boolean,
    11|     parse_integer,
    12|     parse_json_object_from_request,
    13|     parse_string,
    14| )
    15| from synapse.rest.admin._base import (
    16|     admin_patterns,
    17|     assert_requester_is_admin,
    18|     assert_user_is_admin,
    19|     historical_admin_path_patterns,
    20| )
    21| from synapse.types import UserID
    22| logger = logging.getLogger(__name__)
    23| class UsersRestServlet(RestServlet):
    24|     PATTERNS = historical_admin_path_patterns("/users/(?P<user_id>[^/]*)$")
    25|     def __init__(self, hs):
    26|         self.hs = hs
    27|         self.store = hs.get_datastore()
    28|         self.auth = hs.get_auth()
    29|         self.admin_handler = hs.get_handlers().admin_handler
    30|     async def on_GET(self, request, user_id):
    31|         target_user = UserID.from_string(user_id)
    32|         await assert_requester_is_admin(self.auth, request)
    33|         if not self.hs.is_mine(target_user):
    34|             raise SynapseError(400, "Can only users a local user")
    35|         ret = await self.store.get_users()
    36|         return 200, ret
    37| class UsersRestServletV2(RestServlet):
    38|     PATTERNS = admin_patterns("/users$", "v2")
    39|     """Get request to list all local users.
    40|     This needs user to have administrator access in Synapse.
    41|     GET /_synapse/admin/v2/users?from=0&limit=10&guests=false
    42|     returns:
    43|         200 OK with list of users if success otherwise an error.
    44|     The parameters `from` and `limit` are required only for pagination.
    45|     By default, a `limit` of 100 is used.
    46|     The parameter `user_id` can be used to filter by user id.
    47|     The parameter `name` can be used to filter by user id or display name.
    48|     The parameter `guests` can be used to exclude guest users.
    49|     The parameter `deactivated` can be used to include deactivated users.
    50|     """
    51|     def __init__(self, hs):
    52|         self.hs = hs
    53|         self.store = hs.get_datastore()
    54|         self.auth = hs.get_auth()
    55|         self.admin_handler = hs.get_handlers().admin_handler
    56|     async def on_GET(self, request):
    57|         await assert_requester_is_admin(self.auth, request)
    58|         start = parse_integer(request, "from", default=0)
    59|         limit = parse_integer(request, "limit", default=100)
    60|         user_id = parse_string(request, "user_id", default=None)
    61|         name = parse_string(request, "name", default=None)
    62|         guests = parse_boolean(request, "guests", default=True)
    63|         deactivated = parse_boolean(request, "deactivated", default=False)
    64|         users, total = await self.store.get_users_paginate(
    65|             start, limit, user_id, name, guests, deactivated
    66|         )
    67|         ret = {"users": users, "total": total}
    68|         if len(users) >= limit:
    69|             ret["next_token"] = str(start + len(users))
    70|         return 200, ret
    71| class UserRestServletV2(RestServlet):
    72|     PATTERNS = admin_patterns("/users/(?P<user_id>[^/]+)$", "v2")
    73|     """Get request to list user details.
    74|     This needs user to have administrator access in Synapse.
    75|     GET /_synapse/admin/v2/users/<user_id>
    76|     returns:
    77|         200 OK with user details if success otherwise an error.
    78|     Put request to allow an administrator to add or modify a user.
    79|     This needs user to have administrator access in Synapse.
    80|     We use PUT instead of POST since we already know the id of the user
    81|     object to create. POST could be used to create guests.
    82|     PUT /_synapse/admin/v2/users/<user_id>
    83|     {
    84|         "password": "secret",
    85|         "displayname": "User"
    86|     }
    87|     returns:
    88|         201 OK with new user object if user was created or
    89|         200 OK with modified user object if user was modified
    90|         otherwise an error.
    91|     """
    92|     def __init__(self, hs):

# --- HUNK 2: Lines 463-526 ---
   463|     Get or set whether or not a user is a server administrator.
   464|     Note that only local users can be server administrators, and that an
   465|     administrator may not demote themselves.
   466|     Only server administrators can use this API.
   467|     Examples:
   468|         * Get
   469|             GET /_synapse/admin/v1/users/@nonadmin:example.com/admin
   470|             response on success:
   471|                 {
   472|                     "admin": false
   473|                 }
   474|         * Set
   475|             PUT /_synapse/admin/v1/users/@reivilibre:librepush.net/admin
   476|             request body:
   477|                 {
   478|                     "admin": true
   479|                 }
   480|             response on success:
   481|                 {}
   482|     """
   483|     PATTERNS = admin_patterns("/users/(?P<user_id>[^/]*)/admin$")
   484|     def __init__(self, hs):
   485|         self.hs = hs
   486|         self.store = hs.get_datastore()
   487|         self.auth = hs.get_auth()
   488|     async def on_GET(self, request, user_id):
   489|         await assert_requester_is_admin(self.auth, request)
   490|         target_user = UserID.from_string(user_id)
   491|         if not self.hs.is_mine(target_user):
   492|             raise SynapseError(400, "Only local users can be admins of this homeserver")
   493|         is_admin = await self.store.is_server_admin(target_user)
   494|         return 200, {"admin": is_admin}
   495|     async def on_PUT(self, request, user_id):
   496|         requester = await self.auth.get_user_by_req(request)
   497|         await assert_user_is_admin(self.auth, requester.user)
   498|         auth_user = requester.user
   499|         target_user = UserID.from_string(user_id)
   500|         body = parse_json_object_from_request(request)
   501|         assert_params_in_dict(body, ["admin"])
   502|         if not self.hs.is_mine(target_user):
   503|             raise SynapseError(400, "Only local users can be admins of this homeserver")
   504|         set_admin_to = bool(body["admin"])
   505|         if target_user == auth_user and not set_admin_to:
   506|             raise SynapseError(400, "You may not demote yourself.")
   507|         await self.store.set_server_admin(target_user, set_admin_to)
   508|         return 200, {}
   509| class UserMembershipRestServlet(RestServlet):
   510|     """
   511|     Get room list of an user.
   512|     """
   513|     PATTERNS = admin_patterns("/users/(?P<user_id>[^/]+)/joined_rooms$")
   514|     def __init__(self, hs):
   515|         self.is_mine = hs.is_mine
   516|         self.auth = hs.get_auth()
   517|         self.store = hs.get_datastore()
   518|     async def on_GET(self, request, user_id):
   519|         await assert_requester_is_admin(self.auth, request)
   520|         if not self.is_mine(UserID.from_string(user_id)):
   521|             raise SynapseError(400, "Can only lookup local users")
   522|         room_ids = await self.store.get_rooms_for_user(user_id)
   523|         if not room_ids:
   524|             raise NotFoundError("User not found")
   525|         ret = {"joined_rooms": list(room_ids), "total": len(room_ids)}
   526|         return 200, ret


# ====================================================================
# FILE: synapse/rest/client/v1/directory.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-40 ---
     1| import logging
     2| from synapse.api.errors import (
     3|     AuthError,
     4|     Codes,
     5|     InvalidClientCredentialsError,
     6|     NotFoundError,
     7|     SynapseError,
     8| )
     9| from synapse.http.servlet import RestServlet, parse_json_object_from_request
    10| from synapse.rest.client.v2_alpha._base import client_patterns
    11| from synapse.types import RoomAlias
    12| logger = logging.getLogger(__name__)
    13| def register_servlets(hs, http_server):
    14|     ClientDirectoryServer(hs).register(http_server)
    15|     ClientDirectoryListServer(hs).register(http_server)
    16|     ClientAppserviceDirectoryListServer(hs).register(http_server)
    17| class ClientDirectoryServer(RestServlet):
    18|     PATTERNS = client_patterns("/directory/room/(?P<room_alias>[^/]*)$", v1=True)
    19|     def __init__(self, hs):
    20|         super().__init__()
    21|         self.store = hs.get_datastore()
    22|         self.handlers = hs.get_handlers()
    23|         self.auth = hs.get_auth()
    24|     async def on_GET(self, request, room_alias):
    25|         room_alias = RoomAlias.from_string(room_alias)
    26|         dir_handler = self.handlers.directory_handler
    27|         res = await dir_handler.get_association(room_alias)
    28|         return 200, res
    29|     async def on_PUT(self, request, room_alias):
    30|         room_alias = RoomAlias.from_string(room_alias)
    31|         content = parse_json_object_from_request(request)
    32|         if "room_id" not in content:
    33|             raise SynapseError(
    34|                 400, 'Missing params: ["room_id"]', errcode=Codes.BAD_JSON
    35|             )
    36|         logger.debug("Got content: %s", content)
    37|         logger.debug("Got room name: %s", room_alias.to_string())
    38|         room_id = content["room_id"]
    39|         servers = content["servers"] if "servers" in content else None
    40|         logger.debug("Got room_id: %s", room_id)

# --- HUNK 2: Lines 55-122 ---
    55|             await dir_handler.delete_appservice_association(service, room_alias)
    56|             logger.info(
    57|                 "Application service at %s deleted alias %s",
    58|                 service.url,
    59|                 room_alias.to_string(),
    60|             )
    61|             return 200, {}
    62|         except InvalidClientCredentialsError:
    63|             pass
    64|         requester = await self.auth.get_user_by_req(request)
    65|         user = requester.user
    66|         room_alias = RoomAlias.from_string(room_alias)
    67|         await dir_handler.delete_association(requester, room_alias)
    68|         logger.info(
    69|             "User %s deleted alias %s", user.to_string(), room_alias.to_string()
    70|         )
    71|         return 200, {}
    72| class ClientDirectoryListServer(RestServlet):
    73|     PATTERNS = client_patterns("/directory/list/room/(?P<room_id>[^/]*)$", v1=True)
    74|     def __init__(self, hs):
    75|         super().__init__()
    76|         self.store = hs.get_datastore()
    77|         self.handlers = hs.get_handlers()
    78|         self.auth = hs.get_auth()
    79|     async def on_GET(self, request, room_id):
    80|         room = await self.store.get_room(room_id)
    81|         if room is None:
    82|             raise NotFoundError("Unknown room")
    83|         return 200, {"visibility": "public" if room["is_public"] else "private"}
    84|     async def on_PUT(self, request, room_id):
    85|         requester = await self.auth.get_user_by_req(request)
    86|         content = parse_json_object_from_request(request)
    87|         visibility = content.get("visibility", "public")
    88|         await self.handlers.directory_handler.edit_published_room_list(
    89|             requester, room_id, visibility
    90|         )
    91|         return 200, {}
    92|     async def on_DELETE(self, request, room_id):
    93|         requester = await self.auth.get_user_by_req(request)
    94|         await self.handlers.directory_handler.edit_published_room_list(
    95|             requester, room_id, "private"
    96|         )
    97|         return 200, {}
    98| class ClientAppserviceDirectoryListServer(RestServlet):
    99|     PATTERNS = client_patterns(
   100|         "/directory/list/appservice/(?P<network_id>[^/]*)/(?P<room_id>[^/]*)$", v1=True
   101|     )
   102|     def __init__(self, hs):
   103|         super().__init__()
   104|         self.store = hs.get_datastore()
   105|         self.handlers = hs.get_handlers()
   106|         self.auth = hs.get_auth()
   107|     def on_PUT(self, request, network_id, room_id):
   108|         content = parse_json_object_from_request(request)
   109|         visibility = content.get("visibility", "public")
   110|         return self._edit(request, network_id, room_id, visibility)
   111|     def on_DELETE(self, request, network_id, room_id):
   112|         return self._edit(request, network_id, room_id, "private")
   113|     async def _edit(self, request, network_id, room_id, visibility):
   114|         requester = await self.auth.get_user_by_req(request)
   115|         if not requester.app_service:
   116|             raise AuthError(
   117|                 403, "Only appservices can edit the appservice published room list"
   118|             )
   119|         await self.handlers.directory_handler.edit_published_appservice_room_list(
   120|             requester.app_service.id, network_id, room_id, visibility
   121|         )
   122|         return 200, {}


# ====================================================================
# FILE: synapse/rest/client/v1/events.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-64 ---
     1| """This module contains REST servlets to do with event streaming, /events."""
     2| import logging
     3| from synapse.api.errors import SynapseError
     4| from synapse.http.servlet import RestServlet
     5| from synapse.rest.client.v2_alpha._base import client_patterns
     6| from synapse.streams.config import PaginationConfig
     7| logger = logging.getLogger(__name__)
     8| class EventStreamRestServlet(RestServlet):
     9|     PATTERNS = client_patterns("/events$", v1=True)
    10|     DEFAULT_LONGPOLL_TIME_MS = 30000
    11|     def __init__(self, hs):
    12|         super().__init__()
    13|         self.event_stream_handler = hs.get_event_stream_handler()
    14|         self.auth = hs.get_auth()
    15|         self.store = hs.get_datastore()
    16|     async def on_GET(self, request):
    17|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    18|         is_guest = requester.is_guest
    19|         room_id = None
    20|         if is_guest:
    21|             if b"room_id" not in request.args:
    22|                 raise SynapseError(400, "Guest users must specify room_id param")
    23|         if b"room_id" in request.args:
    24|             room_id = request.args[b"room_id"][0].decode("ascii")
    25|         pagin_config = await PaginationConfig.from_request(self.store, request)
    26|         timeout = EventStreamRestServlet.DEFAULT_LONGPOLL_TIME_MS
    27|         if b"timeout" in request.args:
    28|             try:
    29|                 timeout = int(request.args[b"timeout"][0])
    30|             except ValueError:
    31|                 raise SynapseError(400, "timeout must be in milliseconds.")
    32|         as_client_event = b"raw" not in request.args
    33|         chunk = await self.event_stream_handler.get_stream(
    34|             requester.user.to_string(),
    35|             pagin_config,
    36|             timeout=timeout,
    37|             as_client_event=as_client_event,
    38|             affect_presence=(not is_guest),
    39|             room_id=room_id,
    40|             is_guest=is_guest,
    41|         )
    42|         return 200, chunk
    43|     def on_OPTIONS(self, request):
    44|         return 200, {}
    45| class EventRestServlet(RestServlet):
    46|     PATTERNS = client_patterns("/events/(?P<event_id>[^/]*)$", v1=True)
    47|     def __init__(self, hs):
    48|         super().__init__()
    49|         self.clock = hs.get_clock()
    50|         self.event_handler = hs.get_event_handler()
    51|         self.auth = hs.get_auth()
    52|         self._event_serializer = hs.get_event_client_serializer()
    53|     async def on_GET(self, request, event_id):
    54|         requester = await self.auth.get_user_by_req(request)
    55|         event = await self.event_handler.get_event(requester.user, None, event_id)
    56|         time_now = self.clock.time_msec()
    57|         if event:
    58|             event = await self._event_serializer.serialize_event(event, time_now)
    59|             return 200, event
    60|         else:
    61|             return 404, "Event not found."
    62| def register_servlets(hs, http_server):
    63|     EventStreamRestServlet(hs).register(http_server)
    64|     EventRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v1/initial_sync.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-24 ---
     1| from synapse.http.servlet import RestServlet, parse_boolean
     2| from synapse.rest.client.v2_alpha._base import client_patterns
     3| from synapse.streams.config import PaginationConfig
     4| class InitialSyncRestServlet(RestServlet):
     5|     PATTERNS = client_patterns("/initialSync$", v1=True)
     6|     def __init__(self, hs):
     7|         super().__init__()
     8|         self.initial_sync_handler = hs.get_initial_sync_handler()
     9|         self.auth = hs.get_auth()
    10|         self.store = hs.get_datastore()
    11|     async def on_GET(self, request):
    12|         requester = await self.auth.get_user_by_req(request)
    13|         as_client_event = b"raw" not in request.args
    14|         pagination_config = await PaginationConfig.from_request(self.store, request)
    15|         include_archived = parse_boolean(request, "archived", default=False)
    16|         content = await self.initial_sync_handler.snapshot_all_rooms(
    17|             user_id=requester.user.to_string(),
    18|             pagin_config=pagination_config,
    19|             as_client_event=as_client_event,
    20|             include_archived=include_archived,
    21|         )
    22|         return 200, content
    23| def register_servlets(hs, http_server):
    24|     InitialSyncRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v1/login.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 1-139 ---
     1| import logging
     2| from typing import Awaitable, Callable, Dict, Optional
     3| from synapse.api.errors import Codes, LoginError, SynapseError
     4| from synapse.api.ratelimiting import Ratelimiter
     5| from synapse.appservice import ApplicationService
     6| from synapse.handlers.auth import (
     7|     convert_client_dict_legacy_fields_to_identifier,
     8|     login_id_phone_to_thirdparty,
     9| )
    10| from synapse.http.server import finish_request
    11| from synapse.http.servlet import (
    12|     RestServlet,
    13|     parse_json_object_from_request,
    14|     parse_string,
    15| )
    16| from synapse.http.site import SynapseRequest
    17| from synapse.rest.client.v2_alpha._base import client_patterns
    18| from synapse.rest.well_known import WellKnownBuilder
    19| from synapse.types import JsonDict, UserID
    20| from synapse.util.threepids import canonicalise_email
    21| logger = logging.getLogger(__name__)
    22| class LoginRestServlet(RestServlet):
    23|     PATTERNS = client_patterns("/login$", v1=True)
    24|     CAS_TYPE = "m.login.cas"
    25|     SSO_TYPE = "m.login.sso"
    26|     TOKEN_TYPE = "m.login.token"
    27|     JWT_TYPE = "org.matrix.login.jwt"
    28|     JWT_TYPE_DEPRECATED = "m.login.jwt"
    29|     APPSERVICE_TYPE = "uk.half-shot.msc2778.login.application_service"
    30|     def __init__(self, hs):
    31|         super().__init__()
    32|         self.hs = hs
    33|         self.jwt_enabled = hs.config.jwt_enabled
    34|         self.jwt_secret = hs.config.jwt_secret
    35|         self.jwt_algorithm = hs.config.jwt_algorithm
    36|         self.jwt_issuer = hs.config.jwt_issuer
    37|         self.jwt_audiences = hs.config.jwt_audiences
    38|         self.saml2_enabled = hs.config.saml2_enabled
    39|         self.cas_enabled = hs.config.cas_enabled
    40|         self.oidc_enabled = hs.config.oidc_enabled
    41|         self.auth = hs.get_auth()
    42|         self.auth_handler = self.hs.get_auth_handler()
    43|         self.registration_handler = hs.get_registration_handler()
    44|         self.handlers = hs.get_handlers()
    45|         self._well_known_builder = WellKnownBuilder(hs)
    46|         self._address_ratelimiter = Ratelimiter(
    47|             clock=hs.get_clock(),
    48|             rate_hz=self.hs.config.rc_login_address.per_second,
    49|             burst_count=self.hs.config.rc_login_address.burst_count,
    50|         )
    51|         self._account_ratelimiter = Ratelimiter(
    52|             clock=hs.get_clock(),
    53|             rate_hz=self.hs.config.rc_login_account.per_second,
    54|             burst_count=self.hs.config.rc_login_account.burst_count,
    55|         )
    56|         self._failed_attempts_ratelimiter = Ratelimiter(
    57|             clock=hs.get_clock(),
    58|             rate_hz=self.hs.config.rc_login_failed_attempts.per_second,
    59|             burst_count=self.hs.config.rc_login_failed_attempts.burst_count,
    60|         )
    61|     def on_GET(self, request: SynapseRequest):
    62|         flows = []
    63|         if self.jwt_enabled:
    64|             flows.append({"type": LoginRestServlet.JWT_TYPE})
    65|             flows.append({"type": LoginRestServlet.JWT_TYPE_DEPRECATED})
    66|         if self.cas_enabled:
    67|             flows.append({"type": LoginRestServlet.CAS_TYPE})
    68|         if self.cas_enabled or self.saml2_enabled or self.oidc_enabled:
    69|             flows.append({"type": LoginRestServlet.SSO_TYPE})
    70|             flows.append({"type": LoginRestServlet.TOKEN_TYPE})
    71|         flows.extend(
    72|             ({"type": t} for t in self.auth_handler.get_supported_login_types())
    73|         )
    74|         return 200, {"flows": flows}
    75|     def on_OPTIONS(self, request: SynapseRequest):
    76|         return 200, {}
    77|     async def on_POST(self, request: SynapseRequest):
    78|         self._address_ratelimiter.ratelimit(request.getClientIP())
    79|         login_submission = parse_json_object_from_request(request)
    80|         try:
    81|             if login_submission["type"] == LoginRestServlet.APPSERVICE_TYPE:
    82|                 appservice = self.auth.get_appservice_by_req(request)
    83|                 result = await self._do_appservice_login(login_submission, appservice)
    84|             elif self.jwt_enabled and (
    85|                 login_submission["type"] == LoginRestServlet.JWT_TYPE
    86|                 or login_submission["type"] == LoginRestServlet.JWT_TYPE_DEPRECATED
    87|             ):
    88|                 result = await self._do_jwt_login(login_submission)
    89|             elif login_submission["type"] == LoginRestServlet.TOKEN_TYPE:
    90|                 result = await self._do_token_login(login_submission)
    91|             else:
    92|                 result = await self._do_other_login(login_submission)
    93|         except KeyError:
    94|             raise SynapseError(400, "Missing JSON keys.")
    95|         well_known_data = self._well_known_builder.get_well_known()
    96|         if well_known_data:
    97|             result["well_known"] = well_known_data
    98|         return 200, result
    99|     def _get_qualified_user_id(self, identifier):
   100|         if identifier["type"] != "m.id.user":
   101|             raise SynapseError(400, "Unknown login identifier type")
   102|         if "user" not in identifier:
   103|             raise SynapseError(400, "User identifier is missing 'user' key")
   104|         if identifier["user"].startswith("@"):
   105|             return identifier["user"]
   106|         else:
   107|             return UserID(identifier["user"], self.hs.hostname).to_string()
   108|     async def _do_appservice_login(
   109|         self, login_submission: JsonDict, appservice: ApplicationService
   110|     ):
   111|         logger.info(
   112|             "Got appservice login request with identifier: %r",
   113|             login_submission.get("identifier"),
   114|         )
   115|         identifier = convert_client_dict_legacy_fields_to_identifier(login_submission)
   116|         qualified_user_id = self._get_qualified_user_id(identifier)
   117|         if not appservice.is_interested_in_user(qualified_user_id):
   118|             raise LoginError(403, "Invalid access_token", errcode=Codes.FORBIDDEN)
   119|         return await self._complete_login(qualified_user_id, login_submission)
   120|     async def _do_other_login(self, login_submission: JsonDict) -> Dict[str, str]:
   121|         """Handle non-token/saml/jwt logins
   122|         Args:
   123|             login_submission:
   124|         Returns:
   125|             HTTP response
   126|         """
   127|         logger.info(
   128|             "Got login request with identifier: %r, medium: %r, address: %r, user: %r",
   129|             login_submission.get("identifier"),
   130|             login_submission.get("medium"),
   131|             login_submission.get("address"),
   132|             login_submission.get("user"),
   133|         )
   134|         identifier = convert_client_dict_legacy_fields_to_identifier(login_submission)
   135|         if identifier["type"] == "m.id.phone":
   136|             identifier = login_id_phone_to_thirdparty(identifier)
   137|         if identifier["type"] == "m.id.thirdparty":
   138|             address = identifier.get("address")
   139|             medium = identifier.get("medium")

# --- HUNK 2: Lines 149-262 ---
   149|                 canonical_user_id,
   150|                 callback_3pid,
   151|             ) = await self.auth_handler.check_password_provider_3pid(
   152|                 medium, address, login_submission["password"]
   153|             )
   154|             if canonical_user_id:
   155|                 result = await self._complete_login(
   156|                     canonical_user_id, login_submission, callback_3pid
   157|                 )
   158|                 return result
   159|             user_id = await self.hs.get_datastore().get_user_id_by_threepid(
   160|                 medium, address
   161|             )
   162|             if not user_id:
   163|                 logger.warning(
   164|                     "unknown 3pid identifier medium %s, address %r", medium, address
   165|                 )
   166|                 self._failed_attempts_ratelimiter.can_do_action((medium, address))
   167|                 raise LoginError(403, "", errcode=Codes.FORBIDDEN)
   168|             identifier = {"type": "m.id.user", "user": user_id}
   169|         qualified_user_id = self._get_qualified_user_id(identifier)
   170|         self._failed_attempts_ratelimiter.ratelimit(
   171|             qualified_user_id.lower(), update=False
   172|         )
   173|         try:
   174|             canonical_user_id, callback = await self.auth_handler.validate_login(
   175|                 identifier["user"], login_submission
   176|             )
   177|         except LoginError:
   178|             self._failed_attempts_ratelimiter.can_do_action(qualified_user_id.lower())
   179|             raise
   180|         result = await self._complete_login(
   181|             canonical_user_id, login_submission, callback
   182|         )
   183|         return result
   184|     async def _complete_login(
   185|         self,
   186|         user_id: str,
   187|         login_submission: JsonDict,
   188|         callback: Optional[Callable[[Dict[str, str]], Awaitable[None]]] = None,
   189|         create_non_existent_users: bool = False,
   190|     ) -> Dict[str, str]:
   191|         """Called when we've successfully authed the user and now need to
   192|         actually login them in (e.g. create devices). This gets called on
   193|         all successful logins.
   194|         Applies the ratelimiting for successful login attempts against an
   195|         account.
   196|         Args:
   197|             user_id: ID of the user to register.
   198|             login_submission: Dictionary of login information.
   199|             callback: Callback function to run after login.
   200|             create_non_existent_users: Whether to create the user if they don't
   201|                 exist. Defaults to False.
   202|         Returns:
   203|             result: Dictionary of account information after successful login.
   204|         """
   205|         self._account_ratelimiter.ratelimit(user_id.lower())
   206|         if create_non_existent_users:
   207|             canonical_uid = await self.auth_handler.check_user_exists(user_id)
   208|             if not canonical_uid:
   209|                 canonical_uid = await self.registration_handler.register_user(
   210|                     localpart=UserID.from_string(user_id).localpart
   211|                 )
   212|             user_id = canonical_uid
   213|         device_id = login_submission.get("device_id")
   214|         initial_display_name = login_submission.get("initial_device_display_name")
   215|         device_id, access_token = await self.registration_handler.register_device(
   216|             user_id, device_id, initial_display_name
   217|         )
   218|         result = {
   219|             "user_id": user_id,
   220|             "access_token": access_token,
   221|             "home_server": self.hs.hostname,
   222|             "device_id": device_id,
   223|         }
   224|         if callback is not None:
   225|             await callback(result)
   226|         return result
   227|     async def _do_token_login(self, login_submission: JsonDict) -> Dict[str, str]:
   228|         """
   229|         Handle the final stage of SSO login.
   230|         Args:
   231|              login_submission: The JSON request body.
   232|         Returns:
   233|             The body of the JSON response.
   234|         """
   235|         token = login_submission["token"]
   236|         auth_handler = self.auth_handler
   237|         user_id = await auth_handler.validate_short_term_login_token_and_get_user_id(
   238|             token
   239|         )
   240|         return await self._complete_login(
   241|             user_id, login_submission, self.auth_handler._sso_login_callback
   242|         )
   243|     async def _do_jwt_login(self, login_submission: JsonDict) -> Dict[str, str]:
   244|         token = login_submission.get("token", None)
   245|         if token is None:
   246|             raise LoginError(
   247|                 403, "Token field for JWT is missing", errcode=Codes.FORBIDDEN
   248|             )
   249|         import jwt
   250|         try:
   251|             payload = jwt.decode(
   252|                 token,
   253|                 self.jwt_secret,
   254|                 algorithms=[self.jwt_algorithm],
   255|                 issuer=self.jwt_issuer,
   256|                 audience=self.jwt_audiences,
   257|             )
   258|         except jwt.PyJWTError as e:
   259|             raise LoginError(
   260|                 403, "JWT validation failed: %s" % (str(e),), errcode=Codes.FORBIDDEN,
   261|             )
   262|         user = payload.get("sub", None)

# --- HUNK 3: Lines 285-325 ---
   285|         Args:
   286|             request: The client request to redirect.
   287|             client_redirect_url: the URL that we should redirect the
   288|                 client to when everything is done
   289|         Returns:
   290|             URL to redirect to
   291|         """
   292|         raise NotImplementedError()
   293| class CasRedirectServlet(BaseSSORedirectServlet):
   294|     def __init__(self, hs):
   295|         self._cas_handler = hs.get_cas_handler()
   296|     async def get_sso_url(
   297|         self, request: SynapseRequest, client_redirect_url: bytes
   298|     ) -> bytes:
   299|         return self._cas_handler.get_redirect_url(
   300|             {"redirectUrl": client_redirect_url}
   301|         ).encode("ascii")
   302| class CasTicketServlet(RestServlet):
   303|     PATTERNS = client_patterns("/login/cas/ticket", v1=True)
   304|     def __init__(self, hs):
   305|         super().__init__()
   306|         self._cas_handler = hs.get_cas_handler()
   307|     async def on_GET(self, request: SynapseRequest) -> None:
   308|         client_redirect_url = parse_string(request, "redirectUrl")
   309|         ticket = parse_string(request, "ticket", required=True)
   310|         session = parse_string(request, "session")
   311|         if not client_redirect_url and not session:
   312|             message = "Missing string query parameter redirectUrl or session"
   313|             raise SynapseError(400, message, errcode=Codes.MISSING_PARAM)
   314|         await self._cas_handler.handle_ticket(
   315|             request, ticket, client_redirect_url, session
   316|         )
   317| class SAMLRedirectServlet(BaseSSORedirectServlet):
   318|     PATTERNS = client_patterns("/login/sso/redirect", v1=True)
   319|     def __init__(self, hs):
   320|         self._saml_handler = hs.get_saml_handler()
   321|     async def get_sso_url(
   322|         self, request: SynapseRequest, client_redirect_url: bytes
   323|     ) -> bytes:
   324|         return self._saml_handler.handle_redirect_request(client_redirect_url)
   325| class OIDCRedirectServlet(BaseSSORedirectServlet):


# ====================================================================
# FILE: synapse/rest/client/v1/logout.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-41 ---
     1| import logging
     2| from synapse.http.servlet import RestServlet
     3| from synapse.rest.client.v2_alpha._base import client_patterns
     4| logger = logging.getLogger(__name__)
     5| class LogoutRestServlet(RestServlet):
     6|     PATTERNS = client_patterns("/logout$", v1=True)
     7|     def __init__(self, hs):
     8|         super().__init__()
     9|         self.auth = hs.get_auth()
    10|         self._auth_handler = hs.get_auth_handler()
    11|         self._device_handler = hs.get_device_handler()
    12|     def on_OPTIONS(self, request):
    13|         return 200, {}
    14|     async def on_POST(self, request):
    15|         requester = await self.auth.get_user_by_req(request, allow_expired=True)
    16|         if requester.device_id is None:
    17|             access_token = self.auth.get_access_token_from_request(request)
    18|             await self._auth_handler.delete_access_token(access_token)
    19|         else:
    20|             await self._device_handler.delete_device(
    21|                 requester.user.to_string(), requester.device_id
    22|             )
    23|         return 200, {}
    24| class LogoutAllRestServlet(RestServlet):
    25|     PATTERNS = client_patterns("/logout/all$", v1=True)
    26|     def __init__(self, hs):
    27|         super().__init__()
    28|         self.auth = hs.get_auth()
    29|         self._auth_handler = hs.get_auth_handler()
    30|         self._device_handler = hs.get_device_handler()
    31|     def on_OPTIONS(self, request):
    32|         return 200, {}
    33|     async def on_POST(self, request):
    34|         requester = await self.auth.get_user_by_req(request, allow_expired=True)
    35|         user_id = requester.user.to_string()
    36|         await self._device_handler.delete_all_devices_for_user(user_id)
    37|         await self._auth_handler.delete_access_tokens_for_user(user_id)
    38|         return 200, {}
    39| def register_servlets(hs, http_server):
    40|     LogoutRestServlet(hs).register(http_server)
    41|     LogoutAllRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v1/presence.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-33 ---
     1| """ This module contains REST servlets to do with presence: /presence/<paths>
     2| """
     3| import logging
     4| from synapse.api.errors import AuthError, SynapseError
     5| from synapse.handlers.presence import format_user_presence_state
     6| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     7| from synapse.rest.client.v2_alpha._base import client_patterns
     8| from synapse.types import UserID
     9| logger = logging.getLogger(__name__)
    10| class PresenceStatusRestServlet(RestServlet):
    11|     PATTERNS = client_patterns("/presence/(?P<user_id>[^/]*)/status", v1=True)
    12|     def __init__(self, hs):
    13|         super().__init__()
    14|         self.hs = hs
    15|         self.presence_handler = hs.get_presence_handler()
    16|         self.clock = hs.get_clock()
    17|         self.auth = hs.get_auth()
    18|     async def on_GET(self, request, user_id):
    19|         requester = await self.auth.get_user_by_req(request)
    20|         user = UserID.from_string(user_id)
    21|         if requester.user != user:
    22|             allowed = await self.presence_handler.is_visible(
    23|                 observed_user=user, observer_user=requester.user
    24|             )
    25|             if not allowed:
    26|                 raise AuthError(403, "You are not allowed to see their presence.")
    27|         state = await self.presence_handler.get_state(target_user=user)
    28|         state = format_user_presence_state(
    29|             state, self.clock.time_msec(), include_user_id=False
    30|         )
    31|         return 200, state
    32|     async def on_PUT(self, request, user_id):
    33|         requester = await self.auth.get_user_by_req(request)


# ====================================================================
# FILE: synapse/rest/client/v1/profile.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-97 ---
     1| """ This module contains REST servlets to do with profile: /profile/<paths> """
     2| from synapse.api.errors import Codes, SynapseError
     3| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     4| from synapse.rest.client.v2_alpha._base import client_patterns
     5| from synapse.types import UserID
     6| class ProfileDisplaynameRestServlet(RestServlet):
     7|     PATTERNS = client_patterns("/profile/(?P<user_id>[^/]*)/displayname", v1=True)
     8|     def __init__(self, hs):
     9|         super().__init__()
    10|         self.hs = hs
    11|         self.profile_handler = hs.get_profile_handler()
    12|         self.auth = hs.get_auth()
    13|     async def on_GET(self, request, user_id):
    14|         requester_user = None
    15|         if self.hs.config.require_auth_for_profile_requests:
    16|             requester = await self.auth.get_user_by_req(request)
    17|             requester_user = requester.user
    18|         user = UserID.from_string(user_id)
    19|         await self.profile_handler.check_profile_query_allowed(user, requester_user)
    20|         displayname = await self.profile_handler.get_displayname(user)
    21|         ret = {}
    22|         if displayname is not None:
    23|             ret["displayname"] = displayname
    24|         return 200, ret
    25|     async def on_PUT(self, request, user_id):
    26|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    27|         user = UserID.from_string(user_id)
    28|         is_admin = await self.auth.is_server_admin(requester.user)
    29|         content = parse_json_object_from_request(request)
    30|         try:
    31|             new_name = content["displayname"]
    32|         except Exception:
    33|             return 400, "Unable to parse name"
    34|         await self.profile_handler.set_displayname(user, requester, new_name, is_admin)
    35|         return 200, {}
    36|     def on_OPTIONS(self, request, user_id):
    37|         return 200, {}
    38| class ProfileAvatarURLRestServlet(RestServlet):
    39|     PATTERNS = client_patterns("/profile/(?P<user_id>[^/]*)/avatar_url", v1=True)
    40|     def __init__(self, hs):
    41|         super().__init__()
    42|         self.hs = hs
    43|         self.profile_handler = hs.get_profile_handler()
    44|         self.auth = hs.get_auth()
    45|     async def on_GET(self, request, user_id):
    46|         requester_user = None
    47|         if self.hs.config.require_auth_for_profile_requests:
    48|             requester = await self.auth.get_user_by_req(request)
    49|             requester_user = requester.user
    50|         user = UserID.from_string(user_id)
    51|         await self.profile_handler.check_profile_query_allowed(user, requester_user)
    52|         avatar_url = await self.profile_handler.get_avatar_url(user)
    53|         ret = {}
    54|         if avatar_url is not None:
    55|             ret["avatar_url"] = avatar_url
    56|         return 200, ret
    57|     async def on_PUT(self, request, user_id):
    58|         requester = await self.auth.get_user_by_req(request)
    59|         user = UserID.from_string(user_id)
    60|         is_admin = await self.auth.is_server_admin(requester.user)
    61|         content = parse_json_object_from_request(request)
    62|         try:
    63|             new_avatar_url = content["avatar_url"]
    64|         except KeyError:
    65|             raise SynapseError(
    66|                 400, "Missing key 'avatar_url'", errcode=Codes.MISSING_PARAM
    67|             )
    68|         await self.profile_handler.set_avatar_url(
    69|             user, requester, new_avatar_url, is_admin
    70|         )
    71|         return 200, {}
    72|     def on_OPTIONS(self, request, user_id):
    73|         return 200, {}
    74| class ProfileRestServlet(RestServlet):
    75|     PATTERNS = client_patterns("/profile/(?P<user_id>[^/]*)", v1=True)
    76|     def __init__(self, hs):
    77|         super().__init__()
    78|         self.hs = hs
    79|         self.profile_handler = hs.get_profile_handler()
    80|         self.auth = hs.get_auth()
    81|     async def on_GET(self, request, user_id):
    82|         requester_user = None
    83|         if self.hs.config.require_auth_for_profile_requests:
    84|             requester = await self.auth.get_user_by_req(request)
    85|             requester_user = requester.user
    86|         user = UserID.from_string(user_id)
    87|         await self.profile_handler.check_profile_query_allowed(user, requester_user)
    88|         displayname = await self.profile_handler.get_displayname(user)
    89|         avatar_url = await self.profile_handler.get_avatar_url(user)
    90|         ret = {}
    91|         if displayname is not None:
    92|             ret["displayname"] = displayname
    93|         if avatar_url is not None:
    94|             ret["avatar_url"] = avatar_url
    95|         return 200, ret
    96| def register_servlets(hs, http_server):
    97|     ProfileDisplaynameRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v1/push_rule.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 3-43 ---
     3|     StoreError,
     4|     SynapseError,
     5|     UnrecognizedRequestError,
     6| )
     7| from synapse.http.servlet import (
     8|     RestServlet,
     9|     parse_json_value_from_request,
    10|     parse_string,
    11| )
    12| from synapse.push.baserules import BASE_RULE_IDS, NEW_RULE_IDS
    13| from synapse.push.clientformat import format_push_rules_for_user
    14| from synapse.push.rulekinds import PRIORITY_CLASS_MAP
    15| from synapse.rest.client.v2_alpha._base import client_patterns
    16| from synapse.storage.push_rule import InconsistentRuleException, RuleNotFoundException
    17| class PushRuleRestServlet(RestServlet):
    18|     PATTERNS = client_patterns("/(?P<path>pushrules/.*)$", v1=True)
    19|     SLIGHTLY_PEDANTIC_TRAILING_SLASH_ERROR = (
    20|         "Unrecognised request: You probably wanted a trailing slash"
    21|     )
    22|     def __init__(self, hs):
    23|         super().__init__()
    24|         self.auth = hs.get_auth()
    25|         self.store = hs.get_datastore()
    26|         self.notifier = hs.get_notifier()
    27|         self._is_worker = hs.config.worker_app is not None
    28|         self._users_new_default_push_rules = hs.config.users_new_default_push_rules
    29|     async def on_PUT(self, request, path):
    30|         if self._is_worker:
    31|             raise Exception("Cannot handle PUT /push_rules on worker")
    32|         spec = _rule_spec_from_path(path.split("/"))
    33|         try:
    34|             priority_class = _priority_class_from_spec(spec)
    35|         except InvalidRuleException as e:
    36|             raise SynapseError(400, str(e))
    37|         requester = await self.auth.get_user_by_req(request)
    38|         if "/" in spec["rule_id"] or "\\" in spec["rule_id"]:
    39|             raise SynapseError(400, "rule_id may not contain slashes")
    40|         content = parse_json_value_from_request(request)
    41|         user_id = requester.user.to_string()
    42|         if "attr" in spec:
    43|             await self.set_rule_attr(user_id, spec, content)

# --- HUNK 2: Lines 95-149 ---
    95|         rules = await self.store.get_push_rules_for_user(user_id)
    96|         rules = format_push_rules_for_user(requester.user, rules)
    97|         path = path.split("/")[1:]
    98|         if path == []:
    99|             raise UnrecognizedRequestError(
   100|                 PushRuleRestServlet.SLIGHTLY_PEDANTIC_TRAILING_SLASH_ERROR
   101|             )
   102|         if path[0] == "":
   103|             return 200, rules
   104|         elif path[0] == "global":
   105|             result = _filter_ruleset_with_path(rules["global"], path[1:])
   106|             return 200, result
   107|         else:
   108|             raise UnrecognizedRequestError()
   109|     def on_OPTIONS(self, request, path):
   110|         return 200, {}
   111|     def notify_user(self, user_id):
   112|         stream_id = self.store.get_max_push_rules_stream_id()
   113|         self.notifier.on_new_event("push_rules_key", stream_id, users=[user_id])
   114|     async def set_rule_attr(self, user_id, spec, val):
   115|         if spec["attr"] not in ("enabled", "actions"):
   116|             raise UnrecognizedRequestError()
   117|         namespaced_rule_id = _namespaced_rule_id_from_spec(spec)
   118|         rule_id = spec["rule_id"]
   119|         is_default_rule = rule_id.startswith(".")
   120|         if is_default_rule:
   121|             if namespaced_rule_id not in BASE_RULE_IDS:
   122|                 raise NotFoundError("Unknown rule %s" % (namespaced_rule_id,))
   123|         if spec["attr"] == "enabled":
   124|             if isinstance(val, dict) and "enabled" in val:
   125|                 val = val["enabled"]
   126|             if not isinstance(val, bool):
   127|                 raise SynapseError(400, "Value for 'enabled' must be boolean")
   128|             return await self.store.set_push_rule_enabled(
   129|                 user_id, namespaced_rule_id, val, is_default_rule
   130|             )
   131|         elif spec["attr"] == "actions":
   132|             actions = val.get("actions")
   133|             _check_actions(actions)
   134|             namespaced_rule_id = _namespaced_rule_id_from_spec(spec)
   135|             rule_id = spec["rule_id"]
   136|             is_default_rule = rule_id.startswith(".")
   137|             if is_default_rule:
   138|                 if user_id in self._users_new_default_push_rules:
   139|                     rule_ids = NEW_RULE_IDS
   140|                 else:
   141|                     rule_ids = BASE_RULE_IDS
   142|                 if namespaced_rule_id not in rule_ids:
   143|                     raise SynapseError(404, "Unknown rule %r" % (namespaced_rule_id,))
   144|             return await self.store.set_push_rule_actions(
   145|                 user_id, namespaced_rule_id, actions, is_default_rule
   146|             )
   147|         else:
   148|             raise UnrecognizedRequestError()
   149| def _rule_spec_from_path(path):


# ====================================================================
# FILE: synapse/rest/client/v1/pusher.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 6-62 ---
     6|     assert_params_in_dict,
     7|     parse_json_object_from_request,
     8|     parse_string,
     9| )
    10| from synapse.push import PusherConfigException
    11| from synapse.rest.client.v2_alpha._base import client_patterns
    12| logger = logging.getLogger(__name__)
    13| ALLOWED_KEYS = {
    14|     "app_display_name",
    15|     "app_id",
    16|     "data",
    17|     "device_display_name",
    18|     "kind",
    19|     "lang",
    20|     "profile_tag",
    21|     "pushkey",
    22| }
    23| class PushersRestServlet(RestServlet):
    24|     PATTERNS = client_patterns("/pushers$", v1=True)
    25|     def __init__(self, hs):
    26|         super().__init__()
    27|         self.hs = hs
    28|         self.auth = hs.get_auth()
    29|     async def on_GET(self, request):
    30|         requester = await self.auth.get_user_by_req(request)
    31|         user = requester.user
    32|         pushers = await self.hs.get_datastore().get_pushers_by_user_id(user.to_string())
    33|         filtered_pushers = [
    34|             {k: v for k, v in p.items() if k in ALLOWED_KEYS} for p in pushers
    35|         ]
    36|         return 200, {"pushers": filtered_pushers}
    37|     def on_OPTIONS(self, _):
    38|         return 200, {}
    39| class PushersSetRestServlet(RestServlet):
    40|     PATTERNS = client_patterns("/pushers/set$", v1=True)
    41|     def __init__(self, hs):
    42|         super().__init__()
    43|         self.hs = hs
    44|         self.auth = hs.get_auth()
    45|         self.notifier = hs.get_notifier()
    46|         self.pusher_pool = self.hs.get_pusherpool()
    47|     async def on_POST(self, request):
    48|         requester = await self.auth.get_user_by_req(request)
    49|         user = requester.user
    50|         content = parse_json_object_from_request(request)
    51|         if (
    52|             "pushkey" in content
    53|             and "app_id" in content
    54|             and "kind" in content
    55|             and content["kind"] is None
    56|         ):
    57|             await self.pusher_pool.remove_pusher(
    58|                 content["app_id"], content["pushkey"], user_id=user.to_string()
    59|             )
    60|             return 200, {}
    61|         assert_params_in_dict(
    62|             content,

# --- HUNK 2: Lines 92-132 ---
    92|                 pushkey=content["pushkey"],
    93|                 lang=content["lang"],
    94|                 data=content["data"],
    95|                 profile_tag=content.get("profile_tag", ""),
    96|             )
    97|         except PusherConfigException as pce:
    98|             raise SynapseError(
    99|                 400, "Config Error: " + str(pce), errcode=Codes.MISSING_PARAM
   100|             )
   101|         self.notifier.on_new_replication_data()
   102|         return 200, {}
   103|     def on_OPTIONS(self, _):
   104|         return 200, {}
   105| class PushersRemoveRestServlet(RestServlet):
   106|     """
   107|     To allow pusher to be delete by clicking a link (ie. GET request)
   108|     """
   109|     PATTERNS = client_patterns("/pushers/remove$", v1=True)
   110|     SUCCESS_HTML = b"<html><body>You have been unsubscribed</body><html>"
   111|     def __init__(self, hs):
   112|         super().__init__()
   113|         self.hs = hs
   114|         self.notifier = hs.get_notifier()
   115|         self.auth = hs.get_auth()
   116|         self.pusher_pool = self.hs.get_pusherpool()
   117|     async def on_GET(self, request):
   118|         requester = await self.auth.get_user_by_req(request, rights="delete_pusher")
   119|         user = requester.user
   120|         app_id = parse_string(request, "app_id", required=True)
   121|         pushkey = parse_string(request, "pushkey", required=True)
   122|         try:
   123|             await self.pusher_pool.remove_pusher(
   124|                 app_id=app_id, pushkey=pushkey, user_id=user.to_string()
   125|             )
   126|         except StoreError as se:
   127|             if se.code != 404:
   128|                 raise
   129|         self.notifier.on_new_replication_data()
   130|         respond_with_html_bytes(
   131|             request, 200, PushersRemoveRestServlet.SUCCESS_HTML,
   132|         )


# ====================================================================
# FILE: synapse/rest/client/v1/room.py
# Total hunks: 12
# ====================================================================
# --- HUNK 1: Lines 18-96 ---
    18|     RestServlet,
    19|     assert_params_in_dict,
    20|     parse_integer,
    21|     parse_json_object_from_request,
    22|     parse_string,
    23| )
    24| from synapse.logging.opentracing import set_tag
    25| from synapse.rest.client.transactions import HttpTransactionCache
    26| from synapse.rest.client.v2_alpha._base import client_patterns
    27| from synapse.storage.state import StateFilter
    28| from synapse.streams.config import PaginationConfig
    29| from synapse.types import RoomAlias, RoomID, StreamToken, ThirdPartyInstanceID, UserID
    30| from synapse.util import json_decoder
    31| from synapse.util.stringutils import random_string
    32| MYPY = False
    33| if MYPY:
    34|     import synapse.server
    35| logger = logging.getLogger(__name__)
    36| class TransactionRestServlet(RestServlet):
    37|     def __init__(self, hs):
    38|         super().__init__()
    39|         self.txns = HttpTransactionCache(hs)
    40| class RoomCreateRestServlet(TransactionRestServlet):
    41|     def __init__(self, hs):
    42|         super().__init__(hs)
    43|         self._room_creation_handler = hs.get_room_creation_handler()
    44|         self.auth = hs.get_auth()
    45|     def register(self, http_server):
    46|         PATTERNS = "/createRoom"
    47|         register_txn_path(self, PATTERNS, http_server)
    48|         http_server.register_paths(
    49|             "OPTIONS",
    50|             client_patterns("/rooms(?:/.*)?$", v1=True),
    51|             self.on_OPTIONS,
    52|             self.__class__.__name__,
    53|         )
    54|         http_server.register_paths(
    55|             "OPTIONS",
    56|             client_patterns("/createRoom(?:/.*)?$", v1=True),
    57|             self.on_OPTIONS,
    58|             self.__class__.__name__,
    59|         )
    60|     def on_PUT(self, request, txn_id):
    61|         set_tag("txn_id", txn_id)
    62|         return self.txns.fetch_or_execute_request(request, self.on_POST, request)
    63|     async def on_POST(self, request):
    64|         requester = await self.auth.get_user_by_req(request)
    65|         info, _ = await self._room_creation_handler.create_room(
    66|             requester, self.get_room_config(request)
    67|         )
    68|         return 200, info
    69|     def get_room_config(self, request):
    70|         user_supplied_config = parse_json_object_from_request(request)
    71|         return user_supplied_config
    72|     def on_OPTIONS(self, request):
    73|         return 200, {}
    74| class RoomStateEventRestServlet(TransactionRestServlet):
    75|     def __init__(self, hs):
    76|         super().__init__(hs)
    77|         self.handlers = hs.get_handlers()
    78|         self.event_creation_handler = hs.get_event_creation_handler()
    79|         self.room_member_handler = hs.get_room_member_handler()
    80|         self.message_handler = hs.get_message_handler()
    81|         self.auth = hs.get_auth()
    82|     def register(self, http_server):
    83|         no_state_key = "/rooms/(?P<room_id>[^/]*)/state/(?P<event_type>[^/]*)$"
    84|         state_key = (
    85|             "/rooms/(?P<room_id>[^/]*)/state/"
    86|             "(?P<event_type>[^/]*)/(?P<state_key>[^/]*)$"
    87|         )
    88|         http_server.register_paths(
    89|             "GET",
    90|             client_patterns(state_key, v1=True),
    91|             self.on_GET,
    92|             self.__class__.__name__,
    93|         )
    94|         http_server.register_paths(
    95|             "PUT",
    96|             client_patterns(state_key, v1=True),

# --- HUNK 2: Lines 153-231 ---
   153|                     target=UserID.from_string(state_key),
   154|                     room_id=room_id,
   155|                     action=membership,
   156|                     content=content,
   157|                 )
   158|             else:
   159|                 (
   160|                     event,
   161|                     _,
   162|                 ) = await self.event_creation_handler.create_and_send_nonmember_event(
   163|                     requester, event_dict, txn_id=txn_id
   164|                 )
   165|                 event_id = event.event_id
   166|         except ShadowBanError:
   167|             event_id = "$" + random_string(43)
   168|         set_tag("event_id", event_id)
   169|         ret = {"event_id": event_id}
   170|         return 200, ret
   171| class RoomSendEventRestServlet(TransactionRestServlet):
   172|     def __init__(self, hs):
   173|         super().__init__(hs)
   174|         self.event_creation_handler = hs.get_event_creation_handler()
   175|         self.auth = hs.get_auth()
   176|     def register(self, http_server):
   177|         PATTERNS = "/rooms/(?P<room_id>[^/]*)/send/(?P<event_type>[^/]*)"
   178|         register_txn_path(self, PATTERNS, http_server, with_get=True)
   179|     async def on_POST(self, request, room_id, event_type, txn_id=None):
   180|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   181|         content = parse_json_object_from_request(request)
   182|         event_dict = {
   183|             "type": event_type,
   184|             "content": content,
   185|             "room_id": room_id,
   186|             "sender": requester.user.to_string(),
   187|         }
   188|         if b"ts" in request.args and requester.app_service:
   189|             event_dict["origin_server_ts"] = parse_integer(request, "ts", 0)
   190|         try:
   191|             (
   192|                 event,
   193|                 _,
   194|             ) = await self.event_creation_handler.create_and_send_nonmember_event(
   195|                 requester, event_dict, txn_id=txn_id
   196|             )
   197|             event_id = event.event_id
   198|         except ShadowBanError:
   199|             event_id = "$" + random_string(43)
   200|         set_tag("event_id", event_id)
   201|         return 200, {"event_id": event_id}
   202|     def on_GET(self, request, room_id, event_type, txn_id):
   203|         return 200, "Not implemented"
   204|     def on_PUT(self, request, room_id, event_type, txn_id):
   205|         set_tag("txn_id", txn_id)
   206|         return self.txns.fetch_or_execute_request(
   207|             request, self.on_POST, request, room_id, event_type, txn_id
   208|         )
   209| class JoinRoomAliasServlet(TransactionRestServlet):
   210|     def __init__(self, hs):
   211|         super().__init__(hs)
   212|         self.room_member_handler = hs.get_room_member_handler()
   213|         self.auth = hs.get_auth()
   214|     def register(self, http_server):
   215|         PATTERNS = "/join/(?P<room_identifier>[^/]*)"
   216|         register_txn_path(self, PATTERNS, http_server)
   217|     async def on_POST(self, request, room_identifier, txn_id=None):
   218|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   219|         try:
   220|             content = parse_json_object_from_request(request)
   221|         except Exception:
   222|             content = {}
   223|         if RoomID.is_valid(room_identifier):
   224|             room_id = room_identifier
   225|             try:
   226|                 remote_room_hosts = [
   227|                     x.decode("ascii") for x in request.args[b"server_name"]
   228|                 ]  # type: Optional[List[str]]
   229|             except Exception:
   230|                 remote_room_hosts = None
   231|         elif RoomAlias.is_valid(room_identifier):

# --- HUNK 3: Lines 239-279 ---
   239|             )
   240|         await self.room_member_handler.update_membership(
   241|             requester=requester,
   242|             target=requester.user,
   243|             room_id=room_id,
   244|             action="join",
   245|             txn_id=txn_id,
   246|             remote_room_hosts=remote_room_hosts,
   247|             content=content,
   248|             third_party_signed=content.get("third_party_signed", None),
   249|         )
   250|         return 200, {"room_id": room_id}
   251|     def on_PUT(self, request, room_identifier, txn_id):
   252|         set_tag("txn_id", txn_id)
   253|         return self.txns.fetch_or_execute_request(
   254|             request, self.on_POST, request, room_identifier, txn_id
   255|         )
   256| class PublicRoomListRestServlet(TransactionRestServlet):
   257|     PATTERNS = client_patterns("/publicRooms$", v1=True)
   258|     def __init__(self, hs):
   259|         super().__init__(hs)
   260|         self.hs = hs
   261|         self.auth = hs.get_auth()
   262|     async def on_GET(self, request):
   263|         server = parse_string(request, "server", default=None)
   264|         try:
   265|             await self.auth.get_user_by_req(request, allow_guest=True)
   266|         except InvalidClientCredentialsError as e:
   267|             if not self.hs.config.allow_public_rooms_without_auth:
   268|                 raise
   269|             if server:
   270|                 raise e
   271|             else:
   272|                 pass
   273|         limit = parse_integer(request, "limit", 0)
   274|         since_token = parse_string(request, "since", None)
   275|         if limit == 0:
   276|             limit = None
   277|         handler = self.hs.get_room_list_handler()
   278|         if server and server != self.hs.config.server_name:
   279|             try:

# --- HUNK 4: Lines 315-540 ---
   315|                     server,
   316|                     limit=limit,
   317|                     since_token=since_token,
   318|                     search_filter=search_filter,
   319|                     include_all_networks=include_all_networks,
   320|                     third_party_instance_id=third_party_instance_id,
   321|                 )
   322|             except HttpResponseException as e:
   323|                 raise e.to_synapse_error()
   324|         else:
   325|             data = await handler.get_local_public_room_list(
   326|                 limit=limit,
   327|                 since_token=since_token,
   328|                 search_filter=search_filter,
   329|                 network_tuple=network_tuple,
   330|             )
   331|         return 200, data
   332| class RoomMemberListRestServlet(RestServlet):
   333|     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/members$", v1=True)
   334|     def __init__(self, hs):
   335|         super().__init__()
   336|         self.message_handler = hs.get_message_handler()
   337|         self.auth = hs.get_auth()
   338|         self.store = hs.get_datastore()
   339|     async def on_GET(self, request, room_id):
   340|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   341|         handler = self.message_handler
   342|         at_token_string = parse_string(request, "at")
   343|         if at_token_string is None:
   344|             at_token = None
   345|         else:
   346|             at_token = await StreamToken.from_string(self.store, at_token_string)
   347|         membership = parse_string(request, "membership")
   348|         not_membership = parse_string(request, "not_membership")
   349|         events = await handler.get_state_events(
   350|             room_id=room_id,
   351|             user_id=requester.user.to_string(),
   352|             at_token=at_token,
   353|             state_filter=StateFilter.from_types([(EventTypes.Member, None)]),
   354|         )
   355|         chunk = []
   356|         for event in events:
   357|             if (membership and event["content"].get("membership") != membership) or (
   358|                 not_membership and event["content"].get("membership") == not_membership
   359|             ):
   360|                 continue
   361|             chunk.append(event)
   362|         return 200, {"chunk": chunk}
   363| class JoinedRoomMemberListRestServlet(RestServlet):
   364|     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/joined_members$", v1=True)
   365|     def __init__(self, hs):
   366|         super().__init__()
   367|         self.message_handler = hs.get_message_handler()
   368|         self.auth = hs.get_auth()
   369|     async def on_GET(self, request, room_id):
   370|         requester = await self.auth.get_user_by_req(request)
   371|         users_with_profile = await self.message_handler.get_joined_members(
   372|             requester, room_id
   373|         )
   374|         return 200, {"joined": users_with_profile}
   375| class RoomMessageListRestServlet(RestServlet):
   376|     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/messages$", v1=True)
   377|     def __init__(self, hs):
   378|         super().__init__()
   379|         self.pagination_handler = hs.get_pagination_handler()
   380|         self.auth = hs.get_auth()
   381|         self.store = hs.get_datastore()
   382|     async def on_GET(self, request, room_id):
   383|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   384|         pagination_config = await PaginationConfig.from_request(
   385|             self.store, request, default_limit=10
   386|         )
   387|         as_client_event = b"raw" not in request.args
   388|         filter_str = parse_string(request, b"filter", encoding="utf-8")
   389|         if filter_str:
   390|             filter_json = urlparse.unquote(filter_str)
   391|             event_filter = Filter(
   392|                 json_decoder.decode(filter_json)
   393|             )  # type: Optional[Filter]
   394|             if (
   395|                 event_filter
   396|                 and event_filter.filter_json.get("event_format", "client")
   397|                 == "federation"
   398|             ):
   399|                 as_client_event = False
   400|         else:
   401|             event_filter = None
   402|         msgs = await self.pagination_handler.get_messages(
   403|             room_id=room_id,
   404|             requester=requester,
   405|             pagin_config=pagination_config,
   406|             as_client_event=as_client_event,
   407|             event_filter=event_filter,
   408|         )
   409|         return 200, msgs
   410| class RoomStateRestServlet(RestServlet):
   411|     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/state$", v1=True)
   412|     def __init__(self, hs):
   413|         super().__init__()
   414|         self.message_handler = hs.get_message_handler()
   415|         self.auth = hs.get_auth()
   416|     async def on_GET(self, request, room_id):
   417|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   418|         events = await self.message_handler.get_state_events(
   419|             room_id=room_id,
   420|             user_id=requester.user.to_string(),
   421|             is_guest=requester.is_guest,
   422|         )
   423|         return 200, events
   424| class RoomInitialSyncRestServlet(RestServlet):
   425|     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/initialSync$", v1=True)
   426|     def __init__(self, hs):
   427|         super().__init__()
   428|         self.initial_sync_handler = hs.get_initial_sync_handler()
   429|         self.auth = hs.get_auth()
   430|         self.store = hs.get_datastore()
   431|     async def on_GET(self, request, room_id):
   432|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   433|         pagination_config = await PaginationConfig.from_request(self.store, request)
   434|         content = await self.initial_sync_handler.room_initial_sync(
   435|             room_id=room_id, requester=requester, pagin_config=pagination_config
   436|         )
   437|         return 200, content
   438| class RoomEventServlet(RestServlet):
   439|     PATTERNS = client_patterns(
   440|         "/rooms/(?P<room_id>[^/]*)/event/(?P<event_id>[^/]*)$", v1=True
   441|     )
   442|     def __init__(self, hs):
   443|         super().__init__()
   444|         self.clock = hs.get_clock()
   445|         self.event_handler = hs.get_event_handler()
   446|         self._event_serializer = hs.get_event_client_serializer()
   447|         self.auth = hs.get_auth()
   448|     async def on_GET(self, request, room_id, event_id):
   449|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   450|         try:
   451|             event = await self.event_handler.get_event(
   452|                 requester.user, room_id, event_id
   453|             )
   454|         except AuthError:
   455|             raise SynapseError(404, "Event not found.", errcode=Codes.NOT_FOUND)
   456|         time_now = self.clock.time_msec()
   457|         if event:
   458|             event = await self._event_serializer.serialize_event(event, time_now)
   459|             return 200, event
   460|         return SynapseError(404, "Event not found.", errcode=Codes.NOT_FOUND)
   461| class RoomEventContextServlet(RestServlet):
   462|     PATTERNS = client_patterns(
   463|         "/rooms/(?P<room_id>[^/]*)/context/(?P<event_id>[^/]*)$", v1=True
   464|     )
   465|     def __init__(self, hs):
   466|         super().__init__()
   467|         self.clock = hs.get_clock()
   468|         self.room_context_handler = hs.get_room_context_handler()
   469|         self._event_serializer = hs.get_event_client_serializer()
   470|         self.auth = hs.get_auth()
   471|     async def on_GET(self, request, room_id, event_id):
   472|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   473|         limit = parse_integer(request, "limit", default=10)
   474|         filter_str = parse_string(request, b"filter", encoding="utf-8")
   475|         if filter_str:
   476|             filter_json = urlparse.unquote(filter_str)
   477|             event_filter = Filter(
   478|                 json_decoder.decode(filter_json)
   479|             )  # type: Optional[Filter]
   480|         else:
   481|             event_filter = None
   482|         results = await self.room_context_handler.get_event_context(
   483|             requester.user, room_id, event_id, limit, event_filter
   484|         )
   485|         if not results:
   486|             raise SynapseError(404, "Event not found.", errcode=Codes.NOT_FOUND)
   487|         time_now = self.clock.time_msec()
   488|         results["events_before"] = await self._event_serializer.serialize_events(
   489|             results["events_before"], time_now
   490|         )
   491|         results["event"] = await self._event_serializer.serialize_event(
   492|             results["event"], time_now
   493|         )
   494|         results["events_after"] = await self._event_serializer.serialize_events(
   495|             results["events_after"], time_now
   496|         )
   497|         results["state"] = await self._event_serializer.serialize_events(
   498|             results["state"], time_now
   499|         )
   500|         return 200, results
   501| class RoomForgetRestServlet(TransactionRestServlet):
   502|     def __init__(self, hs):
   503|         super().__init__(hs)
   504|         self.room_member_handler = hs.get_room_member_handler()
   505|         self.auth = hs.get_auth()
   506|     def register(self, http_server):
   507|         PATTERNS = "/rooms/(?P<room_id>[^/]*)/forget"
   508|         register_txn_path(self, PATTERNS, http_server)
   509|     async def on_POST(self, request, room_id, txn_id=None):
   510|         requester = await self.auth.get_user_by_req(request, allow_guest=False)
   511|         await self.room_member_handler.forget(user=requester.user, room_id=room_id)
   512|         return 200, {}
   513|     def on_PUT(self, request, room_id, txn_id):
   514|         set_tag("txn_id", txn_id)
   515|         return self.txns.fetch_or_execute_request(
   516|             request, self.on_POST, request, room_id, txn_id
   517|         )
   518| class RoomMembershipRestServlet(TransactionRestServlet):
   519|     def __init__(self, hs):
   520|         super().__init__(hs)
   521|         self.room_member_handler = hs.get_room_member_handler()
   522|         self.auth = hs.get_auth()
   523|     def register(self, http_server):
   524|         PATTERNS = (
   525|             "/rooms/(?P<room_id>[^/]*)/"
   526|             "(?P<membership_action>join|invite|leave|ban|unban|kick)"
   527|         )
   528|         register_txn_path(self, PATTERNS, http_server)
   529|     async def on_POST(self, request, room_id, membership_action, txn_id=None):
   530|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   531|         if requester.is_guest and membership_action not in {
   532|             Membership.JOIN,
   533|             Membership.LEAVE,
   534|         }:
   535|             raise AuthError(403, "Guest access not allowed")
   536|         try:
   537|             content = parse_json_object_from_request(request)
   538|         except Exception:
   539|             content = {}
   540|         if membership_action == "invite" and self._has_3pid_invite_keys(content):

# --- HUNK 5: Lines 570-650 ---
   570|                 content=event_content,
   571|             )
   572|         except ShadowBanError:
   573|             pass
   574|         return_value = {}
   575|         if membership_action == "join":
   576|             return_value["room_id"] = room_id
   577|         return 200, return_value
   578|     def _has_3pid_invite_keys(self, content):
   579|         for key in {"id_server", "medium", "address"}:
   580|             if key not in content:
   581|                 return False
   582|         return True
   583|     def on_PUT(self, request, room_id, membership_action, txn_id):
   584|         set_tag("txn_id", txn_id)
   585|         return self.txns.fetch_or_execute_request(
   586|             request, self.on_POST, request, room_id, membership_action, txn_id
   587|         )
   588| class RoomRedactEventRestServlet(TransactionRestServlet):
   589|     def __init__(self, hs):
   590|         super().__init__(hs)
   591|         self.handlers = hs.get_handlers()
   592|         self.event_creation_handler = hs.get_event_creation_handler()
   593|         self.auth = hs.get_auth()
   594|     def register(self, http_server):
   595|         PATTERNS = "/rooms/(?P<room_id>[^/]*)/redact/(?P<event_id>[^/]*)"
   596|         register_txn_path(self, PATTERNS, http_server)
   597|     async def on_POST(self, request, room_id, event_id, txn_id=None):
   598|         requester = await self.auth.get_user_by_req(request)
   599|         content = parse_json_object_from_request(request)
   600|         try:
   601|             (
   602|                 event,
   603|                 _,
   604|             ) = await self.event_creation_handler.create_and_send_nonmember_event(
   605|                 requester,
   606|                 {
   607|                     "type": EventTypes.Redaction,
   608|                     "content": content,
   609|                     "room_id": room_id,
   610|                     "sender": requester.user.to_string(),
   611|                     "redacts": event_id,
   612|                 },
   613|                 txn_id=txn_id,
   614|             )
   615|             event_id = event.event_id
   616|         except ShadowBanError:
   617|             event_id = "$" + random_string(43)
   618|         set_tag("event_id", event_id)
   619|         return 200, {"event_id": event_id}
   620|     def on_PUT(self, request, room_id, event_id, txn_id):
   621|         set_tag("txn_id", txn_id)
   622|         return self.txns.fetch_or_execute_request(
   623|             request, self.on_POST, request, room_id, event_id, txn_id
   624|         )
   625| class RoomTypingRestServlet(RestServlet):
   626|     PATTERNS = client_patterns(
   627|         "/rooms/(?P<room_id>[^/]*)/typing/(?P<user_id>[^/]*)$", v1=True
   628|     )
   629|     def __init__(self, hs):
   630|         super().__init__()
   631|         self.presence_handler = hs.get_presence_handler()
   632|         self.typing_handler = hs.get_typing_handler()
   633|         self.auth = hs.get_auth()
   634|         self._is_typing_writer = (
   635|             hs.config.worker.writers.typing == hs.get_instance_name()
   636|         )
   637|     async def on_PUT(self, request, room_id, user_id):
   638|         requester = await self.auth.get_user_by_req(request)
   639|         if not self._is_typing_writer:
   640|             raise Exception("Got /typing request on instance that is not typing writer")
   641|         room_id = urlparse.unquote(room_id)
   642|         target_user = UserID.from_string(urlparse.unquote(user_id))
   643|         content = parse_json_object_from_request(request)
   644|         await self.presence_handler.bump_presence_active_time(requester.user)
   645|         timeout = min(content.get("timeout", 30000), 120000)
   646|         try:
   647|             if content["typing"]:
   648|                 await self.typing_handler.started_typing(
   649|                     target_user=target_user,
   650|                     requester=requester,

# --- HUNK 6: Lines 661-715 ---
   661| class RoomAliasListServlet(RestServlet):
   662|     PATTERNS = [
   663|         re.compile(
   664|             r"^/_matrix/client/unstable/org\.matrix\.msc2432"
   665|             r"/rooms/(?P<room_id>[^/]*)/aliases"
   666|         ),
   667|     ]
   668|     def __init__(self, hs: "synapse.server.HomeServer"):
   669|         super().__init__()
   670|         self.auth = hs.get_auth()
   671|         self.directory_handler = hs.get_handlers().directory_handler
   672|     async def on_GET(self, request, room_id):
   673|         requester = await self.auth.get_user_by_req(request)
   674|         alias_list = await self.directory_handler.get_aliases_for_room(
   675|             requester, room_id
   676|         )
   677|         return 200, {"aliases": alias_list}
   678| class SearchRestServlet(RestServlet):
   679|     PATTERNS = client_patterns("/search$", v1=True)
   680|     def __init__(self, hs):
   681|         super().__init__()
   682|         self.handlers = hs.get_handlers()
   683|         self.auth = hs.get_auth()
   684|     async def on_POST(self, request):
   685|         requester = await self.auth.get_user_by_req(request)
   686|         content = parse_json_object_from_request(request)
   687|         batch = parse_string(request, "next_batch")
   688|         results = await self.handlers.search_handler.search(
   689|             requester.user, content, batch
   690|         )
   691|         return 200, results
   692| class JoinedRoomsRestServlet(RestServlet):
   693|     PATTERNS = client_patterns("/joined_rooms$", v1=True)
   694|     def __init__(self, hs):
   695|         super().__init__()
   696|         self.store = hs.get_datastore()
   697|         self.auth = hs.get_auth()
   698|     async def on_GET(self, request):
   699|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   700|         room_ids = await self.store.get_rooms_for_user(requester.user.to_string())
   701|         return 200, {"joined_rooms": list(room_ids)}
   702| def register_txn_path(servlet, regex_string, http_server, with_get=False):
   703|     """Registers a transaction-based path.
   704|     This registers two paths:
   705|         PUT regex_string/$txnid
   706|         POST regex_string
   707|     Args:
   708|         regex_string (str): The regex string to register. Must NOT have a
   709|         trailing $ as this string will be appended to.
   710|         http_server : The http_server to register paths with.
   711|         with_get: True to also register respective GET paths for the PUTs.
   712|     """
   713|     http_server.register_paths(
   714|         "POST",
   715|         client_patterns(regex_string + "$", v1=True),


# ====================================================================
# FILE: synapse/rest/client/v1/voip.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-29 ---
     1| import base64
     2| import hashlib
     3| import hmac
     4| from synapse.http.servlet import RestServlet
     5| from synapse.rest.client.v2_alpha._base import client_patterns
     6| class VoipRestServlet(RestServlet):
     7|     PATTERNS = client_patterns("/voip/turnServer$", v1=True)
     8|     def __init__(self, hs):
     9|         super().__init__()
    10|         self.hs = hs
    11|         self.auth = hs.get_auth()
    12|     async def on_GET(self, request):
    13|         requester = await self.auth.get_user_by_req(
    14|             request, self.hs.config.turn_allow_guests
    15|         )
    16|         turnUris = self.hs.config.turn_uris
    17|         turnSecret = self.hs.config.turn_shared_secret
    18|         turnUsername = self.hs.config.turn_username
    19|         turnPassword = self.hs.config.turn_password
    20|         userLifetime = self.hs.config.turn_user_lifetime
    21|         if turnUris and turnSecret and userLifetime:
    22|             expiry = (self.hs.get_clock().time_msec() + userLifetime) / 1000
    23|             username = "%d:%s" % (expiry, requester.user.to_string())
    24|             mac = hmac.new(
    25|                 turnSecret.encode(), msg=username.encode(), digestmod=hashlib.sha1
    26|             )
    27|             password = base64.b64encode(mac.digest()).decode("ascii")
    28|         elif turnUris and turnUsername and turnPassword and userLifetime:
    29|             username = turnUsername


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/account.py
# Total hunks: 14
# ====================================================================
# --- HUNK 1: Lines 1-115 ---
     1| import logging
     2| import random
     3| from http import HTTPStatus
     4| from typing import TYPE_CHECKING
     5| from urllib.parse import urlparse
     6| if TYPE_CHECKING:
     7|     from synapse.app.homeserver import HomeServer
     8| from synapse.api.constants import LoginType
     9| from synapse.api.errors import (
    10|     Codes,
    11|     InteractiveAuthIncompleteError,
    12|     SynapseError,
    13|     ThreepidValidationError,
    14| )
    15| from synapse.config.emailconfig import ThreepidBehaviour
    16| from synapse.http.server import finish_request, respond_with_html
    17| from synapse.http.servlet import (
    18|     RestServlet,
    19|     assert_params_in_dict,
    20|     parse_json_object_from_request,
    21|     parse_string,
    22| )
    23| from synapse.push.mailer import Mailer
    24| from synapse.util.msisdn import phone_number_to_msisdn
    25| from synapse.util.stringutils import assert_valid_client_secret, random_string
    26| from synapse.util.threepids import canonicalise_email, check_3pid_allowed
    27| from ._base import client_patterns, interactive_auth_handler
    28| logger = logging.getLogger(__name__)
    29| class EmailPasswordRequestTokenRestServlet(RestServlet):
    30|     PATTERNS = client_patterns("/account/password/email/requestToken$")
    31|     def __init__(self, hs):
    32|         super().__init__()
    33|         self.hs = hs
    34|         self.datastore = hs.get_datastore()
    35|         self.config = hs.config
    36|         self.identity_handler = hs.get_handlers().identity_handler
    37|         if self.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:
    38|             self.mailer = Mailer(
    39|                 hs=self.hs,
    40|                 app_name=self.config.email_app_name,
    41|                 template_html=self.config.email_password_reset_template_html,
    42|                 template_text=self.config.email_password_reset_template_text,
    43|             )
    44|     async def on_POST(self, request):
    45|         if self.config.threepid_behaviour_email == ThreepidBehaviour.OFF:
    46|             if self.config.local_threepid_handling_disabled_due_to_email_config:
    47|                 logger.warning(
    48|                     "User password resets have been disabled due to lack of email config"
    49|                 )
    50|             raise SynapseError(
    51|                 400, "Email-based password resets have been disabled on this server"
    52|             )
    53|         body = parse_json_object_from_request(request)
    54|         assert_params_in_dict(body, ["client_secret", "email", "send_attempt"])
    55|         client_secret = body["client_secret"]
    56|         assert_valid_client_secret(client_secret)
    57|         try:
    58|             email = canonicalise_email(body["email"])
    59|         except ValueError as e:
    60|             raise SynapseError(400, str(e))
    61|         send_attempt = body["send_attempt"]
    62|         next_link = body.get("next_link")  # Optional param
    63|         if next_link:
    64|             assert_valid_next_link(self.hs, next_link)
    65|         existing_user_id = await self.hs.get_datastore().get_user_id_by_threepid(
    66|             "email", email
    67|         )
    68|         if existing_user_id is None:
    69|             if self.config.request_token_inhibit_3pid_errors:
    70|                 await self.hs.clock.sleep(random.randint(1, 10) / 10)
    71|                 return 200, {"sid": random_string(16)}
    72|             raise SynapseError(400, "Email not found", Codes.THREEPID_NOT_FOUND)
    73|         if self.config.threepid_behaviour_email == ThreepidBehaviour.REMOTE:
    74|             assert self.hs.config.account_threepid_delegate_email
    75|             ret = await self.identity_handler.requestEmailToken(
    76|                 self.hs.config.account_threepid_delegate_email,
    77|                 email,
    78|                 client_secret,
    79|                 send_attempt,
    80|                 next_link,
    81|             )
    82|         else:
    83|             sid = await self.identity_handler.send_threepid_validation(
    84|                 email,
    85|                 client_secret,
    86|                 send_attempt,
    87|                 self.mailer.send_password_reset_mail,
    88|                 next_link,
    89|             )
    90|             ret = {"sid": sid}
    91|         return 200, ret
    92| class PasswordRestServlet(RestServlet):
    93|     PATTERNS = client_patterns("/account/password$")
    94|     def __init__(self, hs):
    95|         super().__init__()
    96|         self.hs = hs
    97|         self.auth = hs.get_auth()
    98|         self.auth_handler = hs.get_auth_handler()
    99|         self.datastore = self.hs.get_datastore()
   100|         self.password_policy_handler = hs.get_password_policy_handler()
   101|         self._set_password_handler = hs.get_set_password_handler()
   102|     @interactive_auth_handler
   103|     async def on_POST(self, request):
   104|         body = parse_json_object_from_request(request)
   105|         new_password = body.pop("new_password", None)
   106|         if new_password is not None:
   107|             if not isinstance(new_password, str) or len(new_password) > 512:
   108|                 raise SynapseError(400, "Invalid password")
   109|             self.password_policy_handler.validate_password(new_password)
   110|         if self.auth.has_access_token(request):
   111|             requester = await self.auth.get_user_by_req(request)
   112|             try:
   113|                 params, session_id = await self.auth_handler.validate_user_via_ui_auth(
   114|                     requester,
   115|                     request,

# --- HUNK 2: Lines 161-330 ---
   161|                 logger.error("Auth succeeded but no known type! %r", result.keys())
   162|                 raise SynapseError(500, "", Codes.UNKNOWN)
   163|         if new_password:
   164|             password_hash = await self.auth_handler.hash(new_password)
   165|         else:
   166|             password_hash = await self.auth_handler.get_session_data(
   167|                 session_id, "password_hash", None
   168|             )
   169|         if not password_hash:
   170|             raise SynapseError(400, "Missing params: password", Codes.MISSING_PARAM)
   171|         logout_devices = params.get("logout_devices", True)
   172|         await self._set_password_handler.set_password(
   173|             user_id, password_hash, logout_devices, requester
   174|         )
   175|         return 200, {}
   176|     def on_OPTIONS(self, _):
   177|         return 200, {}
   178| class DeactivateAccountRestServlet(RestServlet):
   179|     PATTERNS = client_patterns("/account/deactivate$")
   180|     def __init__(self, hs):
   181|         super().__init__()
   182|         self.hs = hs
   183|         self.auth = hs.get_auth()
   184|         self.auth_handler = hs.get_auth_handler()
   185|         self._deactivate_account_handler = hs.get_deactivate_account_handler()
   186|     @interactive_auth_handler
   187|     async def on_POST(self, request):
   188|         body = parse_json_object_from_request(request)
   189|         erase = body.get("erase", False)
   190|         if not isinstance(erase, bool):
   191|             raise SynapseError(
   192|                 HTTPStatus.BAD_REQUEST,
   193|                 "Param 'erase' must be a boolean, if given",
   194|                 Codes.BAD_JSON,
   195|             )
   196|         requester = await self.auth.get_user_by_req(request)
   197|         if requester.app_service:
   198|             await self._deactivate_account_handler.deactivate_account(
   199|                 requester.user.to_string(), erase
   200|             )
   201|             return 200, {}
   202|         await self.auth_handler.validate_user_via_ui_auth(
   203|             requester,
   204|             request,
   205|             body,
   206|             self.hs.get_ip_from_request(request),
   207|             "deactivate your account",
   208|         )
   209|         result = await self._deactivate_account_handler.deactivate_account(
   210|             requester.user.to_string(), erase, id_server=body.get("id_server")
   211|         )
   212|         if result:
   213|             id_server_unbind_result = "success"
   214|         else:
   215|             id_server_unbind_result = "no-support"
   216|         return 200, {"id_server_unbind_result": id_server_unbind_result}
   217| class EmailThreepidRequestTokenRestServlet(RestServlet):
   218|     PATTERNS = client_patterns("/account/3pid/email/requestToken$")
   219|     def __init__(self, hs):
   220|         super().__init__()
   221|         self.hs = hs
   222|         self.config = hs.config
   223|         self.identity_handler = hs.get_handlers().identity_handler
   224|         self.store = self.hs.get_datastore()
   225|         if self.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:
   226|             self.mailer = Mailer(
   227|                 hs=self.hs,
   228|                 app_name=self.config.email_app_name,
   229|                 template_html=self.config.email_add_threepid_template_html,
   230|                 template_text=self.config.email_add_threepid_template_text,
   231|             )
   232|     async def on_POST(self, request):
   233|         if self.config.threepid_behaviour_email == ThreepidBehaviour.OFF:
   234|             if self.config.local_threepid_handling_disabled_due_to_email_config:
   235|                 logger.warning(
   236|                     "Adding emails have been disabled due to lack of an email config"
   237|                 )
   238|             raise SynapseError(
   239|                 400, "Adding an email to your account is disabled on this server"
   240|             )
   241|         body = parse_json_object_from_request(request)
   242|         assert_params_in_dict(body, ["client_secret", "email", "send_attempt"])
   243|         client_secret = body["client_secret"]
   244|         assert_valid_client_secret(client_secret)
   245|         try:
   246|             email = canonicalise_email(body["email"])
   247|         except ValueError as e:
   248|             raise SynapseError(400, str(e))
   249|         send_attempt = body["send_attempt"]
   250|         next_link = body.get("next_link")  # Optional param
   251|         if not check_3pid_allowed(self.hs, "email", email):
   252|             raise SynapseError(
   253|                 403,
   254|                 "Your email domain is not authorized on this server",
   255|                 Codes.THREEPID_DENIED,
   256|             )
   257|         if next_link:
   258|             assert_valid_next_link(self.hs, next_link)
   259|         existing_user_id = await self.store.get_user_id_by_threepid("email", email)
   260|         if existing_user_id is not None:
   261|             if self.config.request_token_inhibit_3pid_errors:
   262|                 await self.hs.clock.sleep(random.randint(1, 10) / 10)
   263|                 return 200, {"sid": random_string(16)}
   264|             raise SynapseError(400, "Email is already in use", Codes.THREEPID_IN_USE)
   265|         if self.config.threepid_behaviour_email == ThreepidBehaviour.REMOTE:
   266|             assert self.hs.config.account_threepid_delegate_email
   267|             ret = await self.identity_handler.requestEmailToken(
   268|                 self.hs.config.account_threepid_delegate_email,
   269|                 email,
   270|                 client_secret,
   271|                 send_attempt,
   272|                 next_link,
   273|             )
   274|         else:
   275|             sid = await self.identity_handler.send_threepid_validation(
   276|                 email,
   277|                 client_secret,
   278|                 send_attempt,
   279|                 self.mailer.send_add_threepid_mail,
   280|                 next_link,
   281|             )
   282|             ret = {"sid": sid}
   283|         return 200, ret
   284| class MsisdnThreepidRequestTokenRestServlet(RestServlet):
   285|     PATTERNS = client_patterns("/account/3pid/msisdn/requestToken$")
   286|     def __init__(self, hs):
   287|         self.hs = hs
   288|         super().__init__()
   289|         self.store = self.hs.get_datastore()
   290|         self.identity_handler = hs.get_handlers().identity_handler
   291|     async def on_POST(self, request):
   292|         body = parse_json_object_from_request(request)
   293|         assert_params_in_dict(
   294|             body, ["client_secret", "country", "phone_number", "send_attempt"]
   295|         )
   296|         client_secret = body["client_secret"]
   297|         assert_valid_client_secret(client_secret)
   298|         country = body["country"]
   299|         phone_number = body["phone_number"]
   300|         send_attempt = body["send_attempt"]
   301|         next_link = body.get("next_link")  # Optional param
   302|         msisdn = phone_number_to_msisdn(country, phone_number)
   303|         if not check_3pid_allowed(self.hs, "msisdn", msisdn):
   304|             raise SynapseError(
   305|                 403,
   306|                 "Account phone numbers are not authorized on this server",
   307|                 Codes.THREEPID_DENIED,
   308|             )
   309|         if next_link:
   310|             assert_valid_next_link(self.hs, next_link)
   311|         existing_user_id = await self.store.get_user_id_by_threepid("msisdn", msisdn)
   312|         if existing_user_id is not None:
   313|             if self.hs.config.request_token_inhibit_3pid_errors:
   314|                 await self.hs.clock.sleep(random.randint(1, 10) / 10)
   315|                 return 200, {"sid": random_string(16)}
   316|             raise SynapseError(400, "MSISDN is already in use", Codes.THREEPID_IN_USE)
   317|         if not self.hs.config.account_threepid_delegate_msisdn:
   318|             logger.warning(
   319|                 "No upstream msisdn account_threepid_delegate configured on the server to "
   320|                 "handle this request"
   321|             )
   322|             raise SynapseError(
   323|                 400,
   324|                 "Adding phone numbers to user account is not supported by this homeserver",
   325|             )
   326|         ret = await self.identity_handler.requestMsisdnToken(
   327|             self.hs.config.account_threepid_delegate_msisdn,
   328|             country,
   329|             phone_number,
   330|             client_secret,

# --- HUNK 3: Lines 357-400 ---
   357|                     "Adding emails have been disabled due to lack of an email config"
   358|                 )
   359|             raise SynapseError(
   360|                 400, "Adding an email to your account is disabled on this server"
   361|             )
   362|         elif self.config.threepid_behaviour_email == ThreepidBehaviour.REMOTE:
   363|             raise SynapseError(
   364|                 400,
   365|                 "This homeserver is not validating threepids. Use an identity server "
   366|                 "instead.",
   367|             )
   368|         sid = parse_string(request, "sid", required=True)
   369|         token = parse_string(request, "token", required=True)
   370|         client_secret = parse_string(request, "client_secret", required=True)
   371|         assert_valid_client_secret(client_secret)
   372|         try:
   373|             next_link = await self.store.validate_threepid_session(
   374|                 sid, client_secret, token, self.clock.time_msec()
   375|             )
   376|             if next_link:
   377|                 request.setResponseCode(302)
   378|                 request.setHeader("Location", next_link)
   379|                 finish_request(request)
   380|                 return None
   381|             html = self.config.email_add_threepid_template_success_html_content
   382|             status_code = 200
   383|         except ThreepidValidationError as e:
   384|             status_code = e.code
   385|             template_vars = {"failure_reason": e.msg}
   386|             html = self._failure_email_template.render(**template_vars)
   387|         respond_with_html(request, status_code, html)
   388| class AddThreepidMsisdnSubmitTokenServlet(RestServlet):
   389|     """Handles 3PID validation token submission for adding a phone number to a user's
   390|     account
   391|     """
   392|     PATTERNS = client_patterns(
   393|         "/add_threepid/msisdn/submit_token$", releases=(), unstable=True
   394|     )
   395|     def __init__(self, hs):
   396|         """
   397|         Args:
   398|             hs (synapse.server.HomeServer): server
   399|         """
   400|         super().__init__()

# --- HUNK 4: Lines 405-445 ---
   405|     async def on_POST(self, request):
   406|         if not self.config.account_threepid_delegate_msisdn:
   407|             raise SynapseError(
   408|                 400,
   409|                 "This homeserver is not validating phone numbers. Use an identity server "
   410|                 "instead.",
   411|             )
   412|         body = parse_json_object_from_request(request)
   413|         assert_params_in_dict(body, ["client_secret", "sid", "token"])
   414|         assert_valid_client_secret(body["client_secret"])
   415|         response = await self.identity_handler.proxy_msisdn_submit_token(
   416|             self.config.account_threepid_delegate_msisdn,
   417|             body["client_secret"],
   418|             body["sid"],
   419|             body["token"],
   420|         )
   421|         return 200, response
   422| class ThreepidRestServlet(RestServlet):
   423|     PATTERNS = client_patterns("/account/3pid$")
   424|     def __init__(self, hs):
   425|         super().__init__()
   426|         self.hs = hs
   427|         self.identity_handler = hs.get_handlers().identity_handler
   428|         self.auth = hs.get_auth()
   429|         self.auth_handler = hs.get_auth_handler()
   430|         self.datastore = self.hs.get_datastore()
   431|     async def on_GET(self, request):
   432|         requester = await self.auth.get_user_by_req(request)
   433|         threepids = await self.datastore.user_get_threepids(requester.user.to_string())
   434|         return 200, {"threepids": threepids}
   435|     async def on_POST(self, request):
   436|         if not self.hs.config.enable_3pid_changes:
   437|             raise SynapseError(
   438|                 400, "3PID changes are disabled on this server", Codes.FORBIDDEN
   439|             )
   440|         requester = await self.auth.get_user_by_req(request)
   441|         user_id = requester.user.to_string()
   442|         body = parse_json_object_from_request(request)
   443|         threepid_creds = body.get("threePidCreds") or body.get("three_pid_creds")
   444|         if threepid_creds is None:
   445|             raise SynapseError(

# --- HUNK 5: Lines 449-489 ---
   449|         sid = threepid_creds["sid"]
   450|         client_secret = threepid_creds["client_secret"]
   451|         assert_valid_client_secret(client_secret)
   452|         validation_session = await self.identity_handler.validate_threepid_session(
   453|             client_secret, sid
   454|         )
   455|         if validation_session:
   456|             await self.auth_handler.add_threepid(
   457|                 user_id,
   458|                 validation_session["medium"],
   459|                 validation_session["address"],
   460|                 validation_session["validated_at"],
   461|             )
   462|             return 200, {}
   463|         raise SynapseError(
   464|             400, "No validated 3pid session found", Codes.THREEPID_AUTH_FAILED
   465|         )
   466| class ThreepidAddRestServlet(RestServlet):
   467|     PATTERNS = client_patterns("/account/3pid/add$")
   468|     def __init__(self, hs):
   469|         super().__init__()
   470|         self.hs = hs
   471|         self.identity_handler = hs.get_handlers().identity_handler
   472|         self.auth = hs.get_auth()
   473|         self.auth_handler = hs.get_auth_handler()
   474|     @interactive_auth_handler
   475|     async def on_POST(self, request):
   476|         if not self.hs.config.enable_3pid_changes:
   477|             raise SynapseError(
   478|                 400, "3PID changes are disabled on this server", Codes.FORBIDDEN
   479|             )
   480|         requester = await self.auth.get_user_by_req(request)
   481|         user_id = requester.user.to_string()
   482|         body = parse_json_object_from_request(request)
   483|         assert_params_in_dict(body, ["client_secret", "sid"])
   484|         sid = body["sid"]
   485|         client_secret = body["client_secret"]
   486|         assert_valid_client_secret(client_secret)
   487|         await self.auth_handler.validate_user_via_ui_auth(
   488|             requester,
   489|             request,

# --- HUNK 6: Lines 491-628 ---
   491|             self.hs.get_ip_from_request(request),
   492|             "add a third-party identifier to your account",
   493|         )
   494|         validation_session = await self.identity_handler.validate_threepid_session(
   495|             client_secret, sid
   496|         )
   497|         if validation_session:
   498|             await self.auth_handler.add_threepid(
   499|                 user_id,
   500|                 validation_session["medium"],
   501|                 validation_session["address"],
   502|                 validation_session["validated_at"],
   503|             )
   504|             return 200, {}
   505|         raise SynapseError(
   506|             400, "No validated 3pid session found", Codes.THREEPID_AUTH_FAILED
   507|         )
   508| class ThreepidBindRestServlet(RestServlet):
   509|     PATTERNS = client_patterns("/account/3pid/bind$")
   510|     def __init__(self, hs):
   511|         super().__init__()
   512|         self.hs = hs
   513|         self.identity_handler = hs.get_handlers().identity_handler
   514|         self.auth = hs.get_auth()
   515|     async def on_POST(self, request):
   516|         body = parse_json_object_from_request(request)
   517|         assert_params_in_dict(body, ["id_server", "sid", "client_secret"])
   518|         id_server = body["id_server"]
   519|         sid = body["sid"]
   520|         id_access_token = body.get("id_access_token")  # optional
   521|         client_secret = body["client_secret"]
   522|         assert_valid_client_secret(client_secret)
   523|         requester = await self.auth.get_user_by_req(request)
   524|         user_id = requester.user.to_string()
   525|         await self.identity_handler.bind_threepid(
   526|             client_secret, sid, user_id, id_server, id_access_token
   527|         )
   528|         return 200, {}
   529| class ThreepidUnbindRestServlet(RestServlet):
   530|     PATTERNS = client_patterns("/account/3pid/unbind$")
   531|     def __init__(self, hs):
   532|         super().__init__()
   533|         self.hs = hs
   534|         self.identity_handler = hs.get_handlers().identity_handler
   535|         self.auth = hs.get_auth()
   536|         self.datastore = self.hs.get_datastore()
   537|     async def on_POST(self, request):
   538|         """Unbind the given 3pid from a specific identity server, or identity servers that are
   539|         known to have this 3pid bound
   540|         """
   541|         requester = await self.auth.get_user_by_req(request)
   542|         body = parse_json_object_from_request(request)
   543|         assert_params_in_dict(body, ["medium", "address"])
   544|         medium = body.get("medium")
   545|         address = body.get("address")
   546|         id_server = body.get("id_server")
   547|         result = await self.identity_handler.try_unbind_threepid(
   548|             requester.user.to_string(),
   549|             {"address": address, "medium": medium, "id_server": id_server},
   550|         )
   551|         return 200, {"id_server_unbind_result": "success" if result else "no-support"}
   552| class ThreepidDeleteRestServlet(RestServlet):
   553|     PATTERNS = client_patterns("/account/3pid/delete$")
   554|     def __init__(self, hs):
   555|         super().__init__()
   556|         self.hs = hs
   557|         self.auth = hs.get_auth()
   558|         self.auth_handler = hs.get_auth_handler()
   559|     async def on_POST(self, request):
   560|         if not self.hs.config.enable_3pid_changes:
   561|             raise SynapseError(
   562|                 400, "3PID changes are disabled on this server", Codes.FORBIDDEN
   563|             )
   564|         body = parse_json_object_from_request(request)
   565|         assert_params_in_dict(body, ["medium", "address"])
   566|         requester = await self.auth.get_user_by_req(request)
   567|         user_id = requester.user.to_string()
   568|         try:
   569|             ret = await self.auth_handler.delete_threepid(
   570|                 user_id, body["medium"], body["address"], body.get("id_server")
   571|             )
   572|         except Exception:
   573|             logger.exception("Failed to remove threepid")
   574|             raise SynapseError(500, "Failed to remove threepid")
   575|         if ret:
   576|             id_server_unbind_result = "success"
   577|         else:
   578|             id_server_unbind_result = "no-support"
   579|         return 200, {"id_server_unbind_result": id_server_unbind_result}
   580| def assert_valid_next_link(hs: "HomeServer", next_link: str):
   581|     """
   582|     Raises a SynapseError if a given next_link value is invalid
   583|     next_link is valid if the scheme is http(s) and the next_link.domain_whitelist config
   584|     option is either empty or contains a domain that matches the one in the given next_link
   585|     Args:
   586|         hs: The homeserver object
   587|         next_link: The next_link value given by the client
   588|     Raises:
   589|         SynapseError: If the next_link is invalid
   590|     """
   591|     valid = True
   592|     next_link_parsed = urlparse(next_link)
   593|     if next_link_parsed.scheme == "file":
   594|         valid = False
   595|     if (
   596|         valid
   597|         and hs.config.next_link_domain_whitelist is not None
   598|         and next_link_parsed.hostname not in hs.config.next_link_domain_whitelist
   599|     ):
   600|         valid = False
   601|     if not valid:
   602|         raise SynapseError(
   603|             400,
   604|             "'next_link' domain not included in whitelist, or not http(s)",
   605|             errcode=Codes.INVALID_PARAM,
   606|         )
   607| class WhoamiRestServlet(RestServlet):
   608|     PATTERNS = client_patterns("/account/whoami$")
   609|     def __init__(self, hs):
   610|         super().__init__()
   611|         self.auth = hs.get_auth()
   612|     async def on_GET(self, request):
   613|         requester = await self.auth.get_user_by_req(request)
   614|         return 200, {"user_id": requester.user.to_string()}
   615| def register_servlets(hs, http_server):
   616|     EmailPasswordRequestTokenRestServlet(hs).register(http_server)
   617|     PasswordRestServlet(hs).register(http_server)
   618|     DeactivateAccountRestServlet(hs).register(http_server)
   619|     EmailThreepidRequestTokenRestServlet(hs).register(http_server)
   620|     MsisdnThreepidRequestTokenRestServlet(hs).register(http_server)
   621|     AddThreepidEmailSubmitTokenServlet(hs).register(http_server)
   622|     AddThreepidMsisdnSubmitTokenServlet(hs).register(http_server)
   623|     ThreepidRestServlet(hs).register(http_server)
   624|     ThreepidAddRestServlet(hs).register(http_server)
   625|     ThreepidBindRestServlet(hs).register(http_server)
   626|     ThreepidUnbindRestServlet(hs).register(http_server)
   627|     ThreepidDeleteRestServlet(hs).register(http_server)
   628|     WhoamiRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/account_data.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-73 ---
     1| import logging
     2| from synapse.api.errors import AuthError, NotFoundError, SynapseError
     3| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     4| from ._base import client_patterns
     5| logger = logging.getLogger(__name__)
     6| class AccountDataServlet(RestServlet):
     7|     """
     8|     PUT /user/{user_id}/account_data/{account_dataType} HTTP/1.1
     9|     GET /user/{user_id}/account_data/{account_dataType} HTTP/1.1
    10|     """
    11|     PATTERNS = client_patterns(
    12|         "/user/(?P<user_id>[^/]*)/account_data/(?P<account_data_type>[^/]*)"
    13|     )
    14|     def __init__(self, hs):
    15|         super().__init__()
    16|         self.auth = hs.get_auth()
    17|         self.store = hs.get_datastore()
    18|         self.notifier = hs.get_notifier()
    19|         self._is_worker = hs.config.worker_app is not None
    20|     async def on_PUT(self, request, user_id, account_data_type):
    21|         if self._is_worker:
    22|             raise Exception("Cannot handle PUT /account_data on worker")
    23|         requester = await self.auth.get_user_by_req(request)
    24|         if user_id != requester.user.to_string():
    25|             raise AuthError(403, "Cannot add account data for other users.")
    26|         body = parse_json_object_from_request(request)
    27|         max_id = await self.store.add_account_data_for_user(
    28|             user_id, account_data_type, body
    29|         )
    30|         self.notifier.on_new_event("account_data_key", max_id, users=[user_id])
    31|         return 200, {}
    32|     async def on_GET(self, request, user_id, account_data_type):
    33|         requester = await self.auth.get_user_by_req(request)
    34|         if user_id != requester.user.to_string():
    35|             raise AuthError(403, "Cannot get account data for other users.")
    36|         event = await self.store.get_global_account_data_by_type_for_user(
    37|             account_data_type, user_id
    38|         )
    39|         if event is None:
    40|             raise NotFoundError("Account data not found")
    41|         return 200, event
    42| class RoomAccountDataServlet(RestServlet):
    43|     """
    44|     PUT /user/{user_id}/rooms/{room_id}/account_data/{account_dataType} HTTP/1.1
    45|     GET /user/{user_id}/rooms/{room_id}/account_data/{account_dataType} HTTP/1.1
    46|     """
    47|     PATTERNS = client_patterns(
    48|         "/user/(?P<user_id>[^/]*)"
    49|         "/rooms/(?P<room_id>[^/]*)"
    50|         "/account_data/(?P<account_data_type>[^/]*)"
    51|     )
    52|     def __init__(self, hs):
    53|         super().__init__()
    54|         self.auth = hs.get_auth()
    55|         self.store = hs.get_datastore()
    56|         self.notifier = hs.get_notifier()
    57|         self._is_worker = hs.config.worker_app is not None
    58|     async def on_PUT(self, request, user_id, room_id, account_data_type):
    59|         if self._is_worker:
    60|             raise Exception("Cannot handle PUT /account_data on worker")
    61|         requester = await self.auth.get_user_by_req(request)
    62|         if user_id != requester.user.to_string():
    63|             raise AuthError(403, "Cannot add account data for other users.")
    64|         body = parse_json_object_from_request(request)
    65|         if account_data_type == "m.fully_read":
    66|             raise SynapseError(
    67|                 405,
    68|                 "Cannot set m.fully_read through this API."
    69|                 " Use /rooms/!roomId:server.name/read_markers",
    70|             )
    71|         max_id = await self.store.add_account_data_to_room(
    72|             user_id, room_id, account_data_type, body
    73|         )


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/account_validity.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-57 ---
     1| import logging
     2| from synapse.api.errors import AuthError, SynapseError
     3| from synapse.http.server import respond_with_html
     4| from synapse.http.servlet import RestServlet
     5| from ._base import client_patterns
     6| logger = logging.getLogger(__name__)
     7| class AccountValidityRenewServlet(RestServlet):
     8|     PATTERNS = client_patterns("/account_validity/renew$")
     9|     def __init__(self, hs):
    10|         """
    11|         Args:
    12|             hs (synapse.server.HomeServer): server
    13|         """
    14|         super().__init__()
    15|         self.hs = hs
    16|         self.account_activity_handler = hs.get_account_validity_handler()
    17|         self.auth = hs.get_auth()
    18|         self.success_html = hs.config.account_validity.account_renewed_html_content
    19|         self.failure_html = hs.config.account_validity.invalid_token_html_content
    20|     async def on_GET(self, request):
    21|         if b"token" not in request.args:
    22|             raise SynapseError(400, "Missing renewal token")
    23|         renewal_token = request.args[b"token"][0]
    24|         token_valid = await self.account_activity_handler.renew_account(
    25|             renewal_token.decode("utf8")
    26|         )
    27|         if token_valid:
    28|             status_code = 200
    29|             response = self.success_html
    30|         else:
    31|             status_code = 404
    32|             response = self.failure_html
    33|         respond_with_html(request, status_code, response)
    34| class AccountValiditySendMailServlet(RestServlet):
    35|     PATTERNS = client_patterns("/account_validity/send_mail$")
    36|     def __init__(self, hs):
    37|         """
    38|         Args:
    39|             hs (synapse.server.HomeServer): server
    40|         """
    41|         super().__init__()
    42|         self.hs = hs
    43|         self.account_activity_handler = hs.get_account_validity_handler()
    44|         self.auth = hs.get_auth()
    45|         self.account_validity = self.hs.config.account_validity
    46|     async def on_POST(self, request):
    47|         if not self.account_validity.renew_by_email_enabled:
    48|             raise AuthError(
    49|                 403, "Account renewal via email is disabled on this server."
    50|             )
    51|         requester = await self.auth.get_user_by_req(request, allow_expired=True)
    52|         user_id = requester.user.to_string()
    53|         await self.account_activity_handler.send_renewal_email_to_user(user_id)
    54|         return 200, {}
    55| def register_servlets(hs, http_server):
    56|     AccountValidityRenewServlet(hs).register(http_server)
    57|     AccountValiditySendMailServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/auth.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-127 ---
     1| import logging
     2| from synapse.api.constants import LoginType
     3| from synapse.api.errors import SynapseError
     4| from synapse.api.urls import CLIENT_API_PREFIX
     5| from synapse.http.server import respond_with_html
     6| from synapse.http.servlet import RestServlet, parse_string
     7| from ._base import client_patterns
     8| logger = logging.getLogger(__name__)
     9| class AuthRestServlet(RestServlet):
    10|     """
    11|     Handles Client / Server API authentication in any situations where it
    12|     cannot be handled in the normal flow (with requests to the same endpoint).
    13|     Current use is for web fallback auth.
    14|     """
    15|     PATTERNS = client_patterns(r"/auth/(?P<stagetype>[\w\.]*)/fallback/web")
    16|     def __init__(self, hs):
    17|         super().__init__()
    18|         self.hs = hs
    19|         self.auth = hs.get_auth()
    20|         self.auth_handler = hs.get_auth_handler()
    21|         self.registration_handler = hs.get_registration_handler()
    22|         self._cas_enabled = hs.config.cas_enabled
    23|         if self._cas_enabled:
    24|             self._cas_handler = hs.get_cas_handler()
    25|             self._cas_server_url = hs.config.cas_server_url
    26|             self._cas_service_url = hs.config.cas_service_url
    27|         self._saml_enabled = hs.config.saml2_enabled
    28|         if self._saml_enabled:
    29|             self._saml_handler = hs.get_saml_handler()
    30|         self._oidc_enabled = hs.config.oidc_enabled
    31|         if self._oidc_enabled:
    32|             self._oidc_handler = hs.get_oidc_handler()
    33|             self._cas_server_url = hs.config.cas_server_url
    34|             self._cas_service_url = hs.config.cas_service_url
    35|         self.recaptcha_template = hs.config.recaptcha_template
    36|         self.terms_template = hs.config.terms_template
    37|         self.success_template = hs.config.fallback_success_template
    38|     async def on_GET(self, request, stagetype):
    39|         session = parse_string(request, "session")
    40|         if not session:
    41|             raise SynapseError(400, "No session supplied")
    42|         if stagetype == LoginType.RECAPTCHA:
    43|             html = self.recaptcha_template.render(
    44|                 session=session,
    45|                 myurl="%s/r0/auth/%s/fallback/web"
    46|                 % (CLIENT_API_PREFIX, LoginType.RECAPTCHA),
    47|                 sitekey=self.hs.config.recaptcha_public_key,
    48|             )
    49|         elif stagetype == LoginType.TERMS:
    50|             html = self.terms_template.render(
    51|                 session=session,
    52|                 terms_url="%s_matrix/consent?v=%s"
    53|                 % (self.hs.config.public_baseurl, self.hs.config.user_consent_version),
    54|                 myurl="%s/r0/auth/%s/fallback/web"
    55|                 % (CLIENT_API_PREFIX, LoginType.TERMS),
    56|             )
    57|         elif stagetype == LoginType.SSO:
    58|             if self._cas_enabled:
    59|                 sso_redirect_url = self._cas_handler.get_redirect_url(
    60|                     {"session": session},
    61|                 )
    62|             elif self._saml_enabled:
    63|                 client_redirect_url = b"unused"
    64|                 sso_redirect_url = self._saml_handler.handle_redirect_request(
    65|                     client_redirect_url, session
    66|                 )
    67|             elif self._oidc_enabled:
    68|                 client_redirect_url = b""
    69|                 sso_redirect_url = await self._oidc_handler.handle_redirect_request(
    70|                     request, client_redirect_url, session
    71|                 )
    72|             else:
    73|                 raise SynapseError(400, "Homeserver not configured for SSO.")
    74|             html = await self.auth_handler.start_sso_ui_auth(sso_redirect_url, session)
    75|         else:
    76|             raise SynapseError(404, "Unknown auth stage type")
    77|         respond_with_html(request, 200, html)
    78|         return None
    79|     async def on_POST(self, request, stagetype):
    80|         session = parse_string(request, "session")
    81|         if not session:
    82|             raise SynapseError(400, "No session supplied")
    83|         if stagetype == LoginType.RECAPTCHA:
    84|             response = parse_string(request, "g-recaptcha-response")
    85|             if not response:
    86|                 raise SynapseError(400, "No captcha response supplied")
    87|             authdict = {"response": response, "session": session}
    88|             success = await self.auth_handler.add_oob_auth(
    89|                 LoginType.RECAPTCHA, authdict, self.hs.get_ip_from_request(request)
    90|             )
    91|             if success:
    92|                 html = self.success_template.render()
    93|             else:
    94|                 html = self.recaptcha_template.render(
    95|                     session=session,
    96|                     myurl="%s/r0/auth/%s/fallback/web"
    97|                     % (CLIENT_API_PREFIX, LoginType.RECAPTCHA),
    98|                     sitekey=self.hs.config.recaptcha_public_key,
    99|                 )
   100|         elif stagetype == LoginType.TERMS:
   101|             authdict = {"session": session}
   102|             success = await self.auth_handler.add_oob_auth(
   103|                 LoginType.TERMS, authdict, self.hs.get_ip_from_request(request)
   104|             )
   105|             if success:
   106|                 html = self.success_template.render()
   107|             else:
   108|                 html = self.terms_template.render(
   109|                     session=session,
   110|                     terms_url="%s_matrix/consent?v=%s"
   111|                     % (
   112|                         self.hs.config.public_baseurl,
   113|                         self.hs.config.user_consent_version,
   114|                     ),
   115|                     myurl="%s/r0/auth/%s/fallback/web"
   116|                     % (CLIENT_API_PREFIX, LoginType.TERMS),
   117|                 )
   118|         elif stagetype == LoginType.SSO:
   119|             raise SynapseError(404, "Fallback SSO auth does not support POST requests.")
   120|         else:
   121|             raise SynapseError(404, "Unknown auth stage type")
   122|         respond_with_html(request, 200, html)
   123|         return None
   124|     def on_OPTIONS(self, _):
   125|         return 200, {}
   126| def register_servlets(hs, http_server):
   127|     AuthRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/capabilities.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-34 ---
     1| import logging
     2| from synapse.api.room_versions import KNOWN_ROOM_VERSIONS
     3| from synapse.http.servlet import RestServlet
     4| from ._base import client_patterns
     5| logger = logging.getLogger(__name__)
     6| class CapabilitiesRestServlet(RestServlet):
     7|     """End point to expose the capabilities of the server."""
     8|     PATTERNS = client_patterns("/capabilities$")
     9|     def __init__(self, hs):
    10|         """
    11|         Args:
    12|             hs (synapse.server.HomeServer): server
    13|         """
    14|         super().__init__()
    15|         self.hs = hs
    16|         self.config = hs.config
    17|         self.auth = hs.get_auth()
    18|         self.store = hs.get_datastore()
    19|     async def on_GET(self, request):
    20|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    21|         user = await self.store.get_user_by_id(requester.user.to_string())
    22|         change_password = bool(user["password_hash"])
    23|         response = {
    24|             "capabilities": {
    25|                 "m.room_versions": {
    26|                     "default": self.config.default_room_version.identifier,
    27|                     "available": {
    28|                         v.identifier: v.disposition
    29|                         for v in KNOWN_ROOM_VERSIONS.values()
    30|                     },
    31|                 },
    32|                 "m.change_password": {"enabled": change_password},
    33|             }
    34|         }


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/devices.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-88 ---
     1| import logging
     2| from synapse.api import errors
     3| from synapse.http.servlet import (
     4|     RestServlet,
     5|     assert_params_in_dict,
     6|     parse_json_object_from_request,
     7| )
     8| from ._base import client_patterns, interactive_auth_handler
     9| logger = logging.getLogger(__name__)
    10| class DevicesRestServlet(RestServlet):
    11|     PATTERNS = client_patterns("/devices$")
    12|     def __init__(self, hs):
    13|         """
    14|         Args:
    15|             hs (synapse.server.HomeServer): server
    16|         """
    17|         super().__init__()
    18|         self.hs = hs
    19|         self.auth = hs.get_auth()
    20|         self.device_handler = hs.get_device_handler()
    21|     async def on_GET(self, request):
    22|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    23|         devices = await self.device_handler.get_devices_by_user(
    24|             requester.user.to_string()
    25|         )
    26|         return 200, {"devices": devices}
    27| class DeleteDevicesRestServlet(RestServlet):
    28|     """
    29|     API for bulk deletion of devices. Accepts a JSON object with a devices
    30|     key which lists the device_ids to delete. Requires user interactive auth.
    31|     """
    32|     PATTERNS = client_patterns("/delete_devices")
    33|     def __init__(self, hs):
    34|         super().__init__()
    35|         self.hs = hs
    36|         self.auth = hs.get_auth()
    37|         self.device_handler = hs.get_device_handler()
    38|         self.auth_handler = hs.get_auth_handler()
    39|     @interactive_auth_handler
    40|     async def on_POST(self, request):
    41|         requester = await self.auth.get_user_by_req(request)
    42|         try:
    43|             body = parse_json_object_from_request(request)
    44|         except errors.SynapseError as e:
    45|             if e.errcode == errors.Codes.NOT_JSON:
    46|                 body = {}
    47|             else:
    48|                 raise e
    49|         assert_params_in_dict(body, ["devices"])
    50|         await self.auth_handler.validate_user_via_ui_auth(
    51|             requester,
    52|             request,
    53|             body,
    54|             self.hs.get_ip_from_request(request),
    55|             "remove device(s) from your account",
    56|         )
    57|         await self.device_handler.delete_devices(
    58|             requester.user.to_string(), body["devices"]
    59|         )
    60|         return 200, {}
    61| class DeviceRestServlet(RestServlet):
    62|     PATTERNS = client_patterns("/devices/(?P<device_id>[^/]*)$")
    63|     def __init__(self, hs):
    64|         """
    65|         Args:
    66|             hs (synapse.server.HomeServer): server
    67|         """
    68|         super().__init__()
    69|         self.hs = hs
    70|         self.auth = hs.get_auth()
    71|         self.device_handler = hs.get_device_handler()
    72|         self.auth_handler = hs.get_auth_handler()
    73|     async def on_GET(self, request, device_id):
    74|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    75|         device = await self.device_handler.get_device(
    76|             requester.user.to_string(), device_id
    77|         )
    78|         return 200, device
    79|     @interactive_auth_handler
    80|     async def on_DELETE(self, request, device_id):
    81|         requester = await self.auth.get_user_by_req(request)
    82|         try:
    83|             body = parse_json_object_from_request(request)
    84|         except errors.SynapseError as e:
    85|             if e.errcode == errors.Codes.NOT_JSON:
    86|                 body = {}
    87|             else:
    88|                 raise


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/filter.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-56 ---
     1| import logging
     2| from synapse.api.errors import AuthError, NotFoundError, StoreError, SynapseError
     3| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     4| from synapse.types import UserID
     5| from ._base import client_patterns, set_timeline_upper_limit
     6| logger = logging.getLogger(__name__)
     7| class GetFilterRestServlet(RestServlet):
     8|     PATTERNS = client_patterns("/user/(?P<user_id>[^/]*)/filter/(?P<filter_id>[^/]*)")
     9|     def __init__(self, hs):
    10|         super().__init__()
    11|         self.hs = hs
    12|         self.auth = hs.get_auth()
    13|         self.filtering = hs.get_filtering()
    14|     async def on_GET(self, request, user_id, filter_id):
    15|         target_user = UserID.from_string(user_id)
    16|         requester = await self.auth.get_user_by_req(request)
    17|         if target_user != requester.user:
    18|             raise AuthError(403, "Cannot get filters for other users")
    19|         if not self.hs.is_mine(target_user):
    20|             raise AuthError(403, "Can only get filters for local users")
    21|         try:
    22|             filter_id = int(filter_id)
    23|         except Exception:
    24|             raise SynapseError(400, "Invalid filter_id")
    25|         try:
    26|             filter_collection = await self.filtering.get_user_filter(
    27|                 user_localpart=target_user.localpart, filter_id=filter_id
    28|             )
    29|         except StoreError as e:
    30|             if e.code != 404:
    31|                 raise
    32|             raise NotFoundError("No such filter")
    33|         return 200, filter_collection.get_filter_json()
    34| class CreateFilterRestServlet(RestServlet):
    35|     PATTERNS = client_patterns("/user/(?P<user_id>[^/]*)/filter")
    36|     def __init__(self, hs):
    37|         super().__init__()
    38|         self.hs = hs
    39|         self.auth = hs.get_auth()
    40|         self.filtering = hs.get_filtering()
    41|     async def on_POST(self, request, user_id):
    42|         target_user = UserID.from_string(user_id)
    43|         requester = await self.auth.get_user_by_req(request)
    44|         if target_user != requester.user:
    45|             raise AuthError(403, "Cannot create filters for other users")
    46|         if not self.hs.is_mine(target_user):
    47|             raise AuthError(403, "Can only create filters for local users")
    48|         content = parse_json_object_from_request(request)
    49|         set_timeline_upper_limit(content, self.hs.config.filter_timeline_limit)
    50|         filter_id = await self.filtering.add_user_filter(
    51|             user_localpart=target_user.localpart, user_filter=content
    52|         )
    53|         return 200, {"filter_id": str(filter_id)}
    54| def register_servlets(hs, http_server):
    55|     GetFilterRestServlet(hs).register(http_server)
    56|     CreateFilterRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/groups.py
# Total hunks: 11
# ====================================================================
# --- HUNK 1: Lines 1-508 ---
     1| import logging
     2| from synapse.api.errors import SynapseError
     3| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     4| from synapse.types import GroupID
     5| from ._base import client_patterns
     6| logger = logging.getLogger(__name__)
     7| class GroupServlet(RestServlet):
     8|     """Get the group profile
     9|     """
    10|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/profile$")
    11|     def __init__(self, hs):
    12|         super().__init__()
    13|         self.auth = hs.get_auth()
    14|         self.clock = hs.get_clock()
    15|         self.groups_handler = hs.get_groups_local_handler()
    16|     async def on_GET(self, request, group_id):
    17|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    18|         requester_user_id = requester.user.to_string()
    19|         group_description = await self.groups_handler.get_group_profile(
    20|             group_id, requester_user_id
    21|         )
    22|         return 200, group_description
    23|     async def on_POST(self, request, group_id):
    24|         requester = await self.auth.get_user_by_req(request)
    25|         requester_user_id = requester.user.to_string()
    26|         content = parse_json_object_from_request(request)
    27|         await self.groups_handler.update_group_profile(
    28|             group_id, requester_user_id, content
    29|         )
    30|         return 200, {}
    31| class GroupSummaryServlet(RestServlet):
    32|     """Get the full group summary
    33|     """
    34|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/summary$")
    35|     def __init__(self, hs):
    36|         super().__init__()
    37|         self.auth = hs.get_auth()
    38|         self.clock = hs.get_clock()
    39|         self.groups_handler = hs.get_groups_local_handler()
    40|     async def on_GET(self, request, group_id):
    41|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    42|         requester_user_id = requester.user.to_string()
    43|         get_group_summary = await self.groups_handler.get_group_summary(
    44|             group_id, requester_user_id
    45|         )
    46|         return 200, get_group_summary
    47| class GroupSummaryRoomsCatServlet(RestServlet):
    48|     """Update/delete a rooms entry in the summary.
    49|     Matches both:
    50|         - /groups/:group/summary/rooms/:room_id
    51|         - /groups/:group/summary/categories/:category/rooms/:room_id
    52|     """
    53|     PATTERNS = client_patterns(
    54|         "/groups/(?P<group_id>[^/]*)/summary"
    55|         "(/categories/(?P<category_id>[^/]+))?"
    56|         "/rooms/(?P<room_id>[^/]*)$"
    57|     )
    58|     def __init__(self, hs):
    59|         super().__init__()
    60|         self.auth = hs.get_auth()
    61|         self.clock = hs.get_clock()
    62|         self.groups_handler = hs.get_groups_local_handler()
    63|     async def on_PUT(self, request, group_id, category_id, room_id):
    64|         requester = await self.auth.get_user_by_req(request)
    65|         requester_user_id = requester.user.to_string()
    66|         content = parse_json_object_from_request(request)
    67|         resp = await self.groups_handler.update_group_summary_room(
    68|             group_id,
    69|             requester_user_id,
    70|             room_id=room_id,
    71|             category_id=category_id,
    72|             content=content,
    73|         )
    74|         return 200, resp
    75|     async def on_DELETE(self, request, group_id, category_id, room_id):
    76|         requester = await self.auth.get_user_by_req(request)
    77|         requester_user_id = requester.user.to_string()
    78|         resp = await self.groups_handler.delete_group_summary_room(
    79|             group_id, requester_user_id, room_id=room_id, category_id=category_id
    80|         )
    81|         return 200, resp
    82| class GroupCategoryServlet(RestServlet):
    83|     """Get/add/update/delete a group category
    84|     """
    85|     PATTERNS = client_patterns(
    86|         "/groups/(?P<group_id>[^/]*)/categories/(?P<category_id>[^/]+)$"
    87|     )
    88|     def __init__(self, hs):
    89|         super().__init__()
    90|         self.auth = hs.get_auth()
    91|         self.clock = hs.get_clock()
    92|         self.groups_handler = hs.get_groups_local_handler()
    93|     async def on_GET(self, request, group_id, category_id):
    94|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    95|         requester_user_id = requester.user.to_string()
    96|         category = await self.groups_handler.get_group_category(
    97|             group_id, requester_user_id, category_id=category_id
    98|         )
    99|         return 200, category
   100|     async def on_PUT(self, request, group_id, category_id):
   101|         requester = await self.auth.get_user_by_req(request)
   102|         requester_user_id = requester.user.to_string()
   103|         content = parse_json_object_from_request(request)
   104|         resp = await self.groups_handler.update_group_category(
   105|             group_id, requester_user_id, category_id=category_id, content=content
   106|         )
   107|         return 200, resp
   108|     async def on_DELETE(self, request, group_id, category_id):
   109|         requester = await self.auth.get_user_by_req(request)
   110|         requester_user_id = requester.user.to_string()
   111|         resp = await self.groups_handler.delete_group_category(
   112|             group_id, requester_user_id, category_id=category_id
   113|         )
   114|         return 200, resp
   115| class GroupCategoriesServlet(RestServlet):
   116|     """Get all group categories
   117|     """
   118|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/categories/$")
   119|     def __init__(self, hs):
   120|         super().__init__()
   121|         self.auth = hs.get_auth()
   122|         self.clock = hs.get_clock()
   123|         self.groups_handler = hs.get_groups_local_handler()
   124|     async def on_GET(self, request, group_id):
   125|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   126|         requester_user_id = requester.user.to_string()
   127|         category = await self.groups_handler.get_group_categories(
   128|             group_id, requester_user_id
   129|         )
   130|         return 200, category
   131| class GroupRoleServlet(RestServlet):
   132|     """Get/add/update/delete a group role
   133|     """
   134|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/roles/(?P<role_id>[^/]+)$")
   135|     def __init__(self, hs):
   136|         super().__init__()
   137|         self.auth = hs.get_auth()
   138|         self.clock = hs.get_clock()
   139|         self.groups_handler = hs.get_groups_local_handler()
   140|     async def on_GET(self, request, group_id, role_id):
   141|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   142|         requester_user_id = requester.user.to_string()
   143|         category = await self.groups_handler.get_group_role(
   144|             group_id, requester_user_id, role_id=role_id
   145|         )
   146|         return 200, category
   147|     async def on_PUT(self, request, group_id, role_id):
   148|         requester = await self.auth.get_user_by_req(request)
   149|         requester_user_id = requester.user.to_string()
   150|         content = parse_json_object_from_request(request)
   151|         resp = await self.groups_handler.update_group_role(
   152|             group_id, requester_user_id, role_id=role_id, content=content
   153|         )
   154|         return 200, resp
   155|     async def on_DELETE(self, request, group_id, role_id):
   156|         requester = await self.auth.get_user_by_req(request)
   157|         requester_user_id = requester.user.to_string()
   158|         resp = await self.groups_handler.delete_group_role(
   159|             group_id, requester_user_id, role_id=role_id
   160|         )
   161|         return 200, resp
   162| class GroupRolesServlet(RestServlet):
   163|     """Get all group roles
   164|     """
   165|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/roles/$")
   166|     def __init__(self, hs):
   167|         super().__init__()
   168|         self.auth = hs.get_auth()
   169|         self.clock = hs.get_clock()
   170|         self.groups_handler = hs.get_groups_local_handler()
   171|     async def on_GET(self, request, group_id):
   172|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   173|         requester_user_id = requester.user.to_string()
   174|         category = await self.groups_handler.get_group_roles(
   175|             group_id, requester_user_id
   176|         )
   177|         return 200, category
   178| class GroupSummaryUsersRoleServlet(RestServlet):
   179|     """Update/delete a user's entry in the summary.
   180|     Matches both:
   181|         - /groups/:group/summary/users/:room_id
   182|         - /groups/:group/summary/roles/:role/users/:user_id
   183|     """
   184|     PATTERNS = client_patterns(
   185|         "/groups/(?P<group_id>[^/]*)/summary"
   186|         "(/roles/(?P<role_id>[^/]+))?"
   187|         "/users/(?P<user_id>[^/]*)$"
   188|     )
   189|     def __init__(self, hs):
   190|         super().__init__()
   191|         self.auth = hs.get_auth()
   192|         self.clock = hs.get_clock()
   193|         self.groups_handler = hs.get_groups_local_handler()
   194|     async def on_PUT(self, request, group_id, role_id, user_id):
   195|         requester = await self.auth.get_user_by_req(request)
   196|         requester_user_id = requester.user.to_string()
   197|         content = parse_json_object_from_request(request)
   198|         resp = await self.groups_handler.update_group_summary_user(
   199|             group_id,
   200|             requester_user_id,
   201|             user_id=user_id,
   202|             role_id=role_id,
   203|             content=content,
   204|         )
   205|         return 200, resp
   206|     async def on_DELETE(self, request, group_id, role_id, user_id):
   207|         requester = await self.auth.get_user_by_req(request)
   208|         requester_user_id = requester.user.to_string()
   209|         resp = await self.groups_handler.delete_group_summary_user(
   210|             group_id, requester_user_id, user_id=user_id, role_id=role_id
   211|         )
   212|         return 200, resp
   213| class GroupRoomServlet(RestServlet):
   214|     """Get all rooms in a group
   215|     """
   216|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/rooms$")
   217|     def __init__(self, hs):
   218|         super().__init__()
   219|         self.auth = hs.get_auth()
   220|         self.clock = hs.get_clock()
   221|         self.groups_handler = hs.get_groups_local_handler()
   222|     async def on_GET(self, request, group_id):
   223|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   224|         requester_user_id = requester.user.to_string()
   225|         if not GroupID.is_valid(group_id):
   226|             raise SynapseError(400, "%s was not legal group ID" % (group_id,))
   227|         result = await self.groups_handler.get_rooms_in_group(
   228|             group_id, requester_user_id
   229|         )
   230|         return 200, result
   231| class GroupUsersServlet(RestServlet):
   232|     """Get all users in a group
   233|     """
   234|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/users$")
   235|     def __init__(self, hs):
   236|         super().__init__()
   237|         self.auth = hs.get_auth()
   238|         self.clock = hs.get_clock()
   239|         self.groups_handler = hs.get_groups_local_handler()
   240|     async def on_GET(self, request, group_id):
   241|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   242|         requester_user_id = requester.user.to_string()
   243|         result = await self.groups_handler.get_users_in_group(
   244|             group_id, requester_user_id
   245|         )
   246|         return 200, result
   247| class GroupInvitedUsersServlet(RestServlet):
   248|     """Get users invited to a group
   249|     """
   250|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/invited_users$")
   251|     def __init__(self, hs):
   252|         super().__init__()
   253|         self.auth = hs.get_auth()
   254|         self.clock = hs.get_clock()
   255|         self.groups_handler = hs.get_groups_local_handler()
   256|     async def on_GET(self, request, group_id):
   257|         requester = await self.auth.get_user_by_req(request)
   258|         requester_user_id = requester.user.to_string()
   259|         result = await self.groups_handler.get_invited_users_in_group(
   260|             group_id, requester_user_id
   261|         )
   262|         return 200, result
   263| class GroupSettingJoinPolicyServlet(RestServlet):
   264|     """Set group join policy
   265|     """
   266|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/settings/m.join_policy$")
   267|     def __init__(self, hs):
   268|         super().__init__()
   269|         self.auth = hs.get_auth()
   270|         self.groups_handler = hs.get_groups_local_handler()
   271|     async def on_PUT(self, request, group_id):
   272|         requester = await self.auth.get_user_by_req(request)
   273|         requester_user_id = requester.user.to_string()
   274|         content = parse_json_object_from_request(request)
   275|         result = await self.groups_handler.set_group_join_policy(
   276|             group_id, requester_user_id, content
   277|         )
   278|         return 200, result
   279| class GroupCreateServlet(RestServlet):
   280|     """Create a group
   281|     """
   282|     PATTERNS = client_patterns("/create_group$")
   283|     def __init__(self, hs):
   284|         super().__init__()
   285|         self.auth = hs.get_auth()
   286|         self.clock = hs.get_clock()
   287|         self.groups_handler = hs.get_groups_local_handler()
   288|         self.server_name = hs.hostname
   289|     async def on_POST(self, request):
   290|         requester = await self.auth.get_user_by_req(request)
   291|         requester_user_id = requester.user.to_string()
   292|         content = parse_json_object_from_request(request)
   293|         localpart = content.pop("localpart")
   294|         group_id = GroupID(localpart, self.server_name).to_string()
   295|         result = await self.groups_handler.create_group(
   296|             group_id, requester_user_id, content
   297|         )
   298|         return 200, result
   299| class GroupAdminRoomsServlet(RestServlet):
   300|     """Add a room to the group
   301|     """
   302|     PATTERNS = client_patterns(
   303|         "/groups/(?P<group_id>[^/]*)/admin/rooms/(?P<room_id>[^/]*)$"
   304|     )
   305|     def __init__(self, hs):
   306|         super().__init__()
   307|         self.auth = hs.get_auth()
   308|         self.clock = hs.get_clock()
   309|         self.groups_handler = hs.get_groups_local_handler()
   310|     async def on_PUT(self, request, group_id, room_id):
   311|         requester = await self.auth.get_user_by_req(request)
   312|         requester_user_id = requester.user.to_string()
   313|         content = parse_json_object_from_request(request)
   314|         result = await self.groups_handler.add_room_to_group(
   315|             group_id, requester_user_id, room_id, content
   316|         )
   317|         return 200, result
   318|     async def on_DELETE(self, request, group_id, room_id):
   319|         requester = await self.auth.get_user_by_req(request)
   320|         requester_user_id = requester.user.to_string()
   321|         result = await self.groups_handler.remove_room_from_group(
   322|             group_id, requester_user_id, room_id
   323|         )
   324|         return 200, result
   325| class GroupAdminRoomsConfigServlet(RestServlet):
   326|     """Update the config of a room in a group
   327|     """
   328|     PATTERNS = client_patterns(
   329|         "/groups/(?P<group_id>[^/]*)/admin/rooms/(?P<room_id>[^/]*)"
   330|         "/config/(?P<config_key>[^/]*)$"
   331|     )
   332|     def __init__(self, hs):
   333|         super().__init__()
   334|         self.auth = hs.get_auth()
   335|         self.clock = hs.get_clock()
   336|         self.groups_handler = hs.get_groups_local_handler()
   337|     async def on_PUT(self, request, group_id, room_id, config_key):
   338|         requester = await self.auth.get_user_by_req(request)
   339|         requester_user_id = requester.user.to_string()
   340|         content = parse_json_object_from_request(request)
   341|         result = await self.groups_handler.update_room_in_group(
   342|             group_id, requester_user_id, room_id, config_key, content
   343|         )
   344|         return 200, result
   345| class GroupAdminUsersInviteServlet(RestServlet):
   346|     """Invite a user to the group
   347|     """
   348|     PATTERNS = client_patterns(
   349|         "/groups/(?P<group_id>[^/]*)/admin/users/invite/(?P<user_id>[^/]*)$"
   350|     )
   351|     def __init__(self, hs):
   352|         super().__init__()
   353|         self.auth = hs.get_auth()
   354|         self.clock = hs.get_clock()
   355|         self.groups_handler = hs.get_groups_local_handler()
   356|         self.store = hs.get_datastore()
   357|         self.is_mine_id = hs.is_mine_id
   358|     async def on_PUT(self, request, group_id, user_id):
   359|         requester = await self.auth.get_user_by_req(request)
   360|         requester_user_id = requester.user.to_string()
   361|         content = parse_json_object_from_request(request)
   362|         config = content.get("config", {})
   363|         result = await self.groups_handler.invite(
   364|             group_id, user_id, requester_user_id, config
   365|         )
   366|         return 200, result
   367| class GroupAdminUsersKickServlet(RestServlet):
   368|     """Kick a user from the group
   369|     """
   370|     PATTERNS = client_patterns(
   371|         "/groups/(?P<group_id>[^/]*)/admin/users/remove/(?P<user_id>[^/]*)$"
   372|     )
   373|     def __init__(self, hs):
   374|         super().__init__()
   375|         self.auth = hs.get_auth()
   376|         self.clock = hs.get_clock()
   377|         self.groups_handler = hs.get_groups_local_handler()
   378|     async def on_PUT(self, request, group_id, user_id):
   379|         requester = await self.auth.get_user_by_req(request)
   380|         requester_user_id = requester.user.to_string()
   381|         content = parse_json_object_from_request(request)
   382|         result = await self.groups_handler.remove_user_from_group(
   383|             group_id, user_id, requester_user_id, content
   384|         )
   385|         return 200, result
   386| class GroupSelfLeaveServlet(RestServlet):
   387|     """Leave a joined group
   388|     """
   389|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/self/leave$")
   390|     def __init__(self, hs):
   391|         super().__init__()
   392|         self.auth = hs.get_auth()
   393|         self.clock = hs.get_clock()
   394|         self.groups_handler = hs.get_groups_local_handler()
   395|     async def on_PUT(self, request, group_id):
   396|         requester = await self.auth.get_user_by_req(request)
   397|         requester_user_id = requester.user.to_string()
   398|         content = parse_json_object_from_request(request)
   399|         result = await self.groups_handler.remove_user_from_group(
   400|             group_id, requester_user_id, requester_user_id, content
   401|         )
   402|         return 200, result
   403| class GroupSelfJoinServlet(RestServlet):
   404|     """Attempt to join a group, or knock
   405|     """
   406|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/self/join$")
   407|     def __init__(self, hs):
   408|         super().__init__()
   409|         self.auth = hs.get_auth()
   410|         self.clock = hs.get_clock()
   411|         self.groups_handler = hs.get_groups_local_handler()
   412|     async def on_PUT(self, request, group_id):
   413|         requester = await self.auth.get_user_by_req(request)
   414|         requester_user_id = requester.user.to_string()
   415|         content = parse_json_object_from_request(request)
   416|         result = await self.groups_handler.join_group(
   417|             group_id, requester_user_id, content
   418|         )
   419|         return 200, result
   420| class GroupSelfAcceptInviteServlet(RestServlet):
   421|     """Accept a group invite
   422|     """
   423|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/self/accept_invite$")
   424|     def __init__(self, hs):
   425|         super().__init__()
   426|         self.auth = hs.get_auth()
   427|         self.clock = hs.get_clock()
   428|         self.groups_handler = hs.get_groups_local_handler()
   429|     async def on_PUT(self, request, group_id):
   430|         requester = await self.auth.get_user_by_req(request)
   431|         requester_user_id = requester.user.to_string()
   432|         content = parse_json_object_from_request(request)
   433|         result = await self.groups_handler.accept_invite(
   434|             group_id, requester_user_id, content
   435|         )
   436|         return 200, result
   437| class GroupSelfUpdatePublicityServlet(RestServlet):
   438|     """Update whether we publicise a users membership of a group
   439|     """
   440|     PATTERNS = client_patterns("/groups/(?P<group_id>[^/]*)/self/update_publicity$")
   441|     def __init__(self, hs):
   442|         super().__init__()
   443|         self.auth = hs.get_auth()
   444|         self.clock = hs.get_clock()
   445|         self.store = hs.get_datastore()
   446|     async def on_PUT(self, request, group_id):
   447|         requester = await self.auth.get_user_by_req(request)
   448|         requester_user_id = requester.user.to_string()
   449|         content = parse_json_object_from_request(request)
   450|         publicise = content["publicise"]
   451|         await self.store.update_group_publicity(group_id, requester_user_id, publicise)
   452|         return 200, {}
   453| class PublicisedGroupsForUserServlet(RestServlet):
   454|     """Get the list of groups a user is advertising
   455|     """
   456|     PATTERNS = client_patterns("/publicised_groups/(?P<user_id>[^/]*)$")
   457|     def __init__(self, hs):
   458|         super().__init__()
   459|         self.auth = hs.get_auth()
   460|         self.clock = hs.get_clock()
   461|         self.store = hs.get_datastore()
   462|         self.groups_handler = hs.get_groups_local_handler()
   463|     async def on_GET(self, request, user_id):
   464|         await self.auth.get_user_by_req(request, allow_guest=True)
   465|         result = await self.groups_handler.get_publicised_groups_for_user(user_id)
   466|         return 200, result
   467| class PublicisedGroupsForUsersServlet(RestServlet):
   468|     """Get the list of groups a user is advertising
   469|     """
   470|     PATTERNS = client_patterns("/publicised_groups$")
   471|     def __init__(self, hs):
   472|         super().__init__()
   473|         self.auth = hs.get_auth()
   474|         self.clock = hs.get_clock()
   475|         self.store = hs.get_datastore()
   476|         self.groups_handler = hs.get_groups_local_handler()
   477|     async def on_POST(self, request):
   478|         await self.auth.get_user_by_req(request, allow_guest=True)
   479|         content = parse_json_object_from_request(request)
   480|         user_ids = content["user_ids"]
   481|         result = await self.groups_handler.bulk_get_publicised_groups(user_ids)
   482|         return 200, result
   483| class GroupsForUserServlet(RestServlet):
   484|     """Get all groups the logged in user is joined to
   485|     """
   486|     PATTERNS = client_patterns("/joined_groups$")
   487|     def __init__(self, hs):
   488|         super().__init__()
   489|         self.auth = hs.get_auth()
   490|         self.clock = hs.get_clock()
   491|         self.groups_handler = hs.get_groups_local_handler()
   492|     async def on_GET(self, request):
   493|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   494|         requester_user_id = requester.user.to_string()
   495|         result = await self.groups_handler.get_joined_groups(requester_user_id)
   496|         return 200, result
   497| def register_servlets(hs, http_server):
   498|     GroupServlet(hs).register(http_server)
   499|     GroupSummaryServlet(hs).register(http_server)
   500|     GroupInvitedUsersServlet(hs).register(http_server)
   501|     GroupUsersServlet(hs).register(http_server)
   502|     GroupRoomServlet(hs).register(http_server)
   503|     GroupSettingJoinPolicyServlet(hs).register(http_server)
   504|     GroupCreateServlet(hs).register(http_server)
   505|     GroupAdminRoomsServlet(hs).register(http_server)
   506|     GroupAdminRoomsConfigServlet(hs).register(http_server)
   507|     GroupAdminUsersInviteServlet(hs).register(http_server)
   508|     GroupAdminUsersKickServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/keys.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 23-63 ---
    23|           "m.olm.curve25519-aes-sha2",
    24|         ]
    25|         "keys": {
    26|           "<algorithm>:<device_id>": "<key_base64>",
    27|         },
    28|         "signatures:" {
    29|           "<user_id>" {
    30|             "<algorithm>:<device_id>": "<signature_base64>"
    31|       } } },
    32|       "one_time_keys": {
    33|         "<algorithm>:<key_id>": "<key_base64>"
    34|       },
    35|     }
    36|     """
    37|     PATTERNS = client_patterns("/keys/upload(/(?P<device_id>[^/]+))?$")
    38|     def __init__(self, hs):
    39|         """
    40|         Args:
    41|             hs (synapse.server.HomeServer): server
    42|         """
    43|         super().__init__()
    44|         self.auth = hs.get_auth()
    45|         self.e2e_keys_handler = hs.get_e2e_keys_handler()
    46|     @trace(opname="upload_keys")
    47|     async def on_POST(self, request, device_id):
    48|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    49|         user_id = requester.user.to_string()
    50|         body = parse_json_object_from_request(request)
    51|         if device_id is not None:
    52|             if requester.device_id is not None and device_id != requester.device_id:
    53|                 set_tag("error", True)
    54|                 log_kv(
    55|                     {
    56|                         "message": "Client uploading keys for a different device",
    57|                         "logged_in_id": requester.device_id,
    58|                         "key_being_uploaded": device_id,
    59|                     }
    60|                 )
    61|                 logger.warning(
    62|                     "Client uploading keys for a different device "
    63|                     "(logged in as %s, uploading for %s)",

# --- HUNK 2: Lines 95-211 ---
    95|             ],
    96|             "keys": { // Must include a ed25519 signing key
    97|               "<algorithm>:<key_id>": "<key_base64>",
    98|             },
    99|             "signatures:" {
   100|               // Must be signed with device's ed25519 key
   101|               "<user_id>/<device_id>": {
   102|                 "<algorithm>:<key_id>": "<signature_base64>"
   103|               }
   104|               // Must be signed by this server.
   105|               "<server_name>": {
   106|                 "<algorithm>:<key_id>": "<signature_base64>"
   107|     } } } } } }
   108|     """
   109|     PATTERNS = client_patterns("/keys/query$")
   110|     def __init__(self, hs):
   111|         """
   112|         Args:
   113|             hs (synapse.server.HomeServer):
   114|         """
   115|         super().__init__()
   116|         self.auth = hs.get_auth()
   117|         self.e2e_keys_handler = hs.get_e2e_keys_handler()
   118|     async def on_POST(self, request):
   119|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   120|         user_id = requester.user.to_string()
   121|         timeout = parse_integer(request, "timeout", 10 * 1000)
   122|         body = parse_json_object_from_request(request)
   123|         result = await self.e2e_keys_handler.query_devices(body, timeout, user_id)
   124|         return 200, result
   125| class KeyChangesServlet(RestServlet):
   126|     """Returns the list of changes of keys between two stream tokens (may return
   127|     spurious extra results, since we currently ignore the `to` param).
   128|         GET /keys/changes?from=...&to=...
   129|         200 OK
   130|         { "changed": ["@foo:example.com"] }
   131|     """
   132|     PATTERNS = client_patterns("/keys/changes$")
   133|     def __init__(self, hs):
   134|         """
   135|         Args:
   136|             hs (synapse.server.HomeServer):
   137|         """
   138|         super().__init__()
   139|         self.auth = hs.get_auth()
   140|         self.device_handler = hs.get_device_handler()
   141|         self.store = hs.get_datastore()
   142|     async def on_GET(self, request):
   143|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   144|         from_token_string = parse_string(request, "from")
   145|         set_tag("from", from_token_string)
   146|         set_tag("to", parse_string(request, "to"))
   147|         from_token = await StreamToken.from_string(self.store, from_token_string)
   148|         user_id = requester.user.to_string()
   149|         results = await self.device_handler.get_user_ids_changed(user_id, from_token)
   150|         return 200, results
   151| class OneTimeKeyServlet(RestServlet):
   152|     """
   153|     POST /keys/claim HTTP/1.1
   154|     {
   155|       "one_time_keys": {
   156|         "<user_id>": {
   157|           "<device_id>": "<algorithm>"
   158|     } } }
   159|     HTTP/1.1 200 OK
   160|     {
   161|       "one_time_keys": {
   162|         "<user_id>": {
   163|           "<device_id>": {
   164|             "<algorithm>:<key_id>": "<key_base64>"
   165|     } } } }
   166|     """
   167|     PATTERNS = client_patterns("/keys/claim$")
   168|     def __init__(self, hs):
   169|         super().__init__()
   170|         self.auth = hs.get_auth()
   171|         self.e2e_keys_handler = hs.get_e2e_keys_handler()
   172|     async def on_POST(self, request):
   173|         await self.auth.get_user_by_req(request, allow_guest=True)
   174|         timeout = parse_integer(request, "timeout", 10 * 1000)
   175|         body = parse_json_object_from_request(request)
   176|         result = await self.e2e_keys_handler.claim_one_time_keys(body, timeout)
   177|         return 200, result
   178| class SigningKeyUploadServlet(RestServlet):
   179|     """
   180|     POST /keys/device_signing/upload HTTP/1.1
   181|     Content-Type: application/json
   182|     {
   183|     }
   184|     """
   185|     PATTERNS = client_patterns("/keys/device_signing/upload$", releases=())
   186|     def __init__(self, hs):
   187|         """
   188|         Args:
   189|             hs (synapse.server.HomeServer): server
   190|         """
   191|         super().__init__()
   192|         self.hs = hs
   193|         self.auth = hs.get_auth()
   194|         self.e2e_keys_handler = hs.get_e2e_keys_handler()
   195|         self.auth_handler = hs.get_auth_handler()
   196|     @interactive_auth_handler
   197|     async def on_POST(self, request):
   198|         requester = await self.auth.get_user_by_req(request)
   199|         user_id = requester.user.to_string()
   200|         body = parse_json_object_from_request(request)
   201|         await self.auth_handler.validate_user_via_ui_auth(
   202|             requester,
   203|             request,
   204|             body,
   205|             self.hs.get_ip_from_request(request),
   206|             "add a device signing key to your account",
   207|         )
   208|         result = await self.e2e_keys_handler.upload_signing_keys_for_user(user_id, body)
   209|         return 200, result
   210| class SignaturesUploadServlet(RestServlet):
   211|     """

# --- HUNK 3: Lines 221-258 ---
   221|             "m.megolm.v1.aes-sha2"
   222|           ],
   223|           "keys": {
   224|             "<algorithm>:<device_id>": "<key_base64>",
   225|           },
   226|           "signatures": {
   227|             "<signing_user_id>": {
   228|               "<algorithm>:<signing_key_base64>": "<signature_base64>>"
   229|             }
   230|           }
   231|         }
   232|       }
   233|     }
   234|     """
   235|     PATTERNS = client_patterns("/keys/signatures/upload$")
   236|     def __init__(self, hs):
   237|         """
   238|         Args:
   239|             hs (synapse.server.HomeServer): server
   240|         """
   241|         super().__init__()
   242|         self.auth = hs.get_auth()
   243|         self.e2e_keys_handler = hs.get_e2e_keys_handler()
   244|     async def on_POST(self, request):
   245|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   246|         user_id = requester.user.to_string()
   247|         body = parse_json_object_from_request(request)
   248|         result = await self.e2e_keys_handler.upload_signatures_for_device_keys(
   249|             user_id, body
   250|         )
   251|         return 200, result
   252| def register_servlets(hs, http_server):
   253|     KeyUploadServlet(hs).register(http_server)
   254|     KeyQueryServlet(hs).register(http_server)
   255|     KeyChangesServlet(hs).register(http_server)
   256|     OneTimeKeyServlet(hs).register(http_server)
   257|     SigningKeyUploadServlet(hs).register(http_server)
   258|     SignaturesUploadServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/notifications.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-29 ---
     1| import logging
     2| from synapse.events.utils import format_event_for_client_v2_without_room_id
     3| from synapse.http.servlet import RestServlet, parse_integer, parse_string
     4| from ._base import client_patterns
     5| logger = logging.getLogger(__name__)
     6| class NotificationsServlet(RestServlet):
     7|     PATTERNS = client_patterns("/notifications$")
     8|     def __init__(self, hs):
     9|         super().__init__()
    10|         self.store = hs.get_datastore()
    11|         self.auth = hs.get_auth()
    12|         self.clock = hs.get_clock()
    13|         self._event_serializer = hs.get_event_client_serializer()
    14|     async def on_GET(self, request):
    15|         requester = await self.auth.get_user_by_req(request)
    16|         user_id = requester.user.to_string()
    17|         from_token = parse_string(request, "from", required=False)
    18|         limit = parse_integer(request, "limit", default=50)
    19|         only = parse_string(request, "only", required=False)
    20|         limit = min(limit, 500)
    21|         push_actions = await self.store.get_push_actions_for_user(
    22|             user_id, from_token, limit, only_highlight=(only == "highlight")
    23|         )
    24|         receipts_by_room = await self.store.get_receipts_for_user_with_orderings(
    25|             user_id, "m.read"
    26|         )
    27|         notif_event_ids = [pa["event_id"] for pa in push_actions]
    28|         notif_events = await self.store.get_events(notif_event_ids)
    29|         returned_push_actions = []


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/openid.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 12-52 ---
    12|     in http://openid.net/specs/openid-connect-core-1_0.html#TokenResponse
    13|     But instead of returning a signed "id_token" the response contains the
    14|     name of the issuing matrix homeserver. This means that for now the third
    15|     party will need to check the validity of the "id_token" against the
    16|     federation /openid/userinfo endpoint of the homeserver.
    17|     Request:
    18|     POST /user/{user_id}/openid/request_token?access_token=... HTTP/1.1
    19|     {}
    20|     Response:
    21|     HTTP/1.1 200 OK
    22|     {
    23|         "access_token": "ABDEFGH",
    24|         "token_type": "Bearer",
    25|         "matrix_server_name": "example.com",
    26|         "expires_in": 3600,
    27|     }
    28|     """
    29|     PATTERNS = client_patterns("/user/(?P<user_id>[^/]*)/openid/request_token")
    30|     EXPIRES_MS = 3600 * 1000
    31|     def __init__(self, hs):
    32|         super().__init__()
    33|         self.auth = hs.get_auth()
    34|         self.store = hs.get_datastore()
    35|         self.clock = hs.get_clock()
    36|         self.server_name = hs.config.server_name
    37|     async def on_POST(self, request, user_id):
    38|         requester = await self.auth.get_user_by_req(request)
    39|         if user_id != requester.user.to_string():
    40|             raise AuthError(403, "Cannot request tokens for other users.")
    41|         parse_json_object_from_request(request)
    42|         token = random_string(24)
    43|         ts_valid_until_ms = self.clock.time_msec() + self.EXPIRES_MS
    44|         await self.store.insert_open_id_token(token, ts_valid_until_ms, user_id)
    45|         return (
    46|             200,
    47|             {
    48|                 "access_token": token,
    49|                 "token_type": "Bearer",
    50|                 "matrix_server_name": self.server_name,
    51|                 "expires_in": self.EXPIRES_MS / 1000,
    52|             },


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/password_policy.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-30 ---
     1| import logging
     2| from synapse.http.servlet import RestServlet
     3| from ._base import client_patterns
     4| logger = logging.getLogger(__name__)
     5| class PasswordPolicyServlet(RestServlet):
     6|     PATTERNS = client_patterns("/password_policy$")
     7|     def __init__(self, hs):
     8|         """
     9|         Args:
    10|             hs (synapse.server.HomeServer): server
    11|         """
    12|         super().__init__()
    13|         self.policy = hs.config.password_policy
    14|         self.enabled = hs.config.password_policy_enabled
    15|     def on_GET(self, request):
    16|         if not self.enabled or not self.policy:
    17|             return (200, {})
    18|         policy = {}
    19|         for param in [
    20|             "minimum_length",
    21|             "require_digit",
    22|             "require_symbol",
    23|             "require_lowercase",
    24|             "require_uppercase",
    25|         ]:
    26|             if param in self.policy:
    27|                 policy["m.%s" % param] = self.policy[param]
    28|         return (200, policy)
    29| def register_servlets(hs, http_server):
    30|     PasswordPolicyServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/read_marker.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| import logging
     2| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     3| from ._base import client_patterns
     4| logger = logging.getLogger(__name__)
     5| class ReadMarkerRestServlet(RestServlet):
     6|     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/read_markers$")
     7|     def __init__(self, hs):
     8|         super().__init__()
     9|         self.auth = hs.get_auth()
    10|         self.receipts_handler = hs.get_receipts_handler()
    11|         self.read_marker_handler = hs.get_read_marker_handler()
    12|         self.presence_handler = hs.get_presence_handler()
    13|     async def on_POST(self, request, room_id):
    14|         requester = await self.auth.get_user_by_req(request)
    15|         await self.presence_handler.bump_presence_active_time(requester.user)
    16|         body = parse_json_object_from_request(request)
    17|         read_event_id = body.get("m.read", None)
    18|         if read_event_id:
    19|             await self.receipts_handler.received_client_receipt(
    20|                 room_id,
    21|                 "m.read",
    22|                 user_id=requester.user.to_string(),
    23|                 event_id=read_event_id,
    24|             )
    25|         read_marker_event_id = body.get("m.fully_read", None)
    26|         if read_marker_event_id:
    27|             await self.read_marker_handler.received_client_read_marker(
    28|                 room_id,


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/receipts.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| import logging
     2| from synapse.api.errors import SynapseError
     3| from synapse.http.servlet import RestServlet
     4| from ._base import client_patterns
     5| logger = logging.getLogger(__name__)
     6| class ReceiptRestServlet(RestServlet):
     7|     PATTERNS = client_patterns(
     8|         "/rooms/(?P<room_id>[^/]*)"
     9|         "/receipt/(?P<receipt_type>[^/]*)"
    10|         "/(?P<event_id>[^/]*)$"
    11|     )
    12|     def __init__(self, hs):
    13|         super().__init__()
    14|         self.hs = hs
    15|         self.auth = hs.get_auth()
    16|         self.receipts_handler = hs.get_receipts_handler()
    17|         self.presence_handler = hs.get_presence_handler()
    18|     async def on_POST(self, request, room_id, receipt_type, event_id):
    19|         requester = await self.auth.get_user_by_req(request)
    20|         if receipt_type != "m.read":
    21|             raise SynapseError(400, "Receipt type must be 'm.read'")
    22|         await self.presence_handler.bump_presence_active_time(requester.user)
    23|         await self.receipts_handler.received_client_receipt(
    24|             room_id, receipt_type, user_id=requester.user.to_string(), event_id=event_id
    25|         )
    26|         return 200, {}
    27| def register_servlets(hs, http_server):
    28|     ReceiptRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/register.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 30-70 ---
    30| )
    31| from synapse.push.mailer import Mailer
    32| from synapse.util.msisdn import phone_number_to_msisdn
    33| from synapse.util.ratelimitutils import FederationRateLimiter
    34| from synapse.util.stringutils import assert_valid_client_secret, random_string
    35| from synapse.util.threepids import canonicalise_email, check_3pid_allowed
    36| from ._base import client_patterns, interactive_auth_handler
    37| if hasattr(hmac, "compare_digest"):
    38|     compare_digest = hmac.compare_digest
    39| else:
    40|     def compare_digest(a, b):
    41|         return a == b
    42| logger = logging.getLogger(__name__)
    43| class EmailRegisterRequestTokenRestServlet(RestServlet):
    44|     PATTERNS = client_patterns("/register/email/requestToken$")
    45|     def __init__(self, hs):
    46|         """
    47|         Args:
    48|             hs (synapse.server.HomeServer): server
    49|         """
    50|         super().__init__()
    51|         self.hs = hs
    52|         self.identity_handler = hs.get_handlers().identity_handler
    53|         self.config = hs.config
    54|         if self.hs.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:
    55|             self.mailer = Mailer(
    56|                 hs=self.hs,
    57|                 app_name=self.config.email_app_name,
    58|                 template_html=self.config.email_registration_template_html,
    59|                 template_text=self.config.email_registration_template_text,
    60|             )
    61|     async def on_POST(self, request):
    62|         if self.hs.config.threepid_behaviour_email == ThreepidBehaviour.OFF:
    63|             if self.hs.config.local_threepid_handling_disabled_due_to_email_config:
    64|                 logger.warning(
    65|                     "Email registration has been disabled due to lack of email config"
    66|                 )
    67|             raise SynapseError(
    68|                 400, "Email-based registration has been disabled on this server"
    69|             )
    70|         body = parse_json_object_from_request(request)

# --- HUNK 2: Lines 100-140 ---
   100|                 send_attempt,
   101|                 next_link,
   102|             )
   103|         else:
   104|             sid = await self.identity_handler.send_threepid_validation(
   105|                 email,
   106|                 client_secret,
   107|                 send_attempt,
   108|                 self.mailer.send_registration_mail,
   109|                 next_link,
   110|             )
   111|             ret = {"sid": sid}
   112|         return 200, ret
   113| class MsisdnRegisterRequestTokenRestServlet(RestServlet):
   114|     PATTERNS = client_patterns("/register/msisdn/requestToken$")
   115|     def __init__(self, hs):
   116|         """
   117|         Args:
   118|             hs (synapse.server.HomeServer): server
   119|         """
   120|         super().__init__()
   121|         self.hs = hs
   122|         self.identity_handler = hs.get_handlers().identity_handler
   123|     async def on_POST(self, request):
   124|         body = parse_json_object_from_request(request)
   125|         assert_params_in_dict(
   126|             body, ["client_secret", "country", "phone_number", "send_attempt"]
   127|         )
   128|         client_secret = body["client_secret"]
   129|         country = body["country"]
   130|         phone_number = body["phone_number"]
   131|         send_attempt = body["send_attempt"]
   132|         next_link = body.get("next_link")  # Optional param
   133|         msisdn = phone_number_to_msisdn(country, phone_number)
   134|         if not check_3pid_allowed(self.hs, "msisdn", msisdn):
   135|             raise SynapseError(
   136|                 403,
   137|                 "Phone numbers are not authorized to register on this server",
   138|                 Codes.THREEPID_DENIED,
   139|             )
   140|         existing_user_id = await self.hs.get_datastore().get_user_id_by_threepid(

# --- HUNK 3: Lines 157-197 ---
   157|             )
   158|         ret = await self.identity_handler.requestMsisdnToken(
   159|             self.hs.config.account_threepid_delegate_msisdn,
   160|             country,
   161|             phone_number,
   162|             client_secret,
   163|             send_attempt,
   164|             next_link,
   165|         )
   166|         return 200, ret
   167| class RegistrationSubmitTokenServlet(RestServlet):
   168|     """Handles registration 3PID validation token submission"""
   169|     PATTERNS = client_patterns(
   170|         "/registration/(?P<medium>[^/]*)/submit_token$", releases=(), unstable=True
   171|     )
   172|     def __init__(self, hs):
   173|         """
   174|         Args:
   175|             hs (synapse.server.HomeServer): server
   176|         """
   177|         super().__init__()
   178|         self.hs = hs
   179|         self.auth = hs.get_auth()
   180|         self.config = hs.config
   181|         self.clock = hs.get_clock()
   182|         self.store = hs.get_datastore()
   183|         if self.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL:
   184|             self._failure_email_template = (
   185|                 self.config.email_registration_template_failure_html
   186|             )
   187|     async def on_GET(self, request, medium):
   188|         if medium != "email":
   189|             raise SynapseError(
   190|                 400, "This medium is currently not supported for registration"
   191|             )
   192|         if self.config.threepid_behaviour_email == ThreepidBehaviour.OFF:
   193|             if self.config.local_threepid_handling_disabled_due_to_email_config:
   194|                 logger.warning(
   195|                     "User registration via email has been disabled due to lack of email config"
   196|                 )
   197|             raise SynapseError(

# --- HUNK 4: Lines 211-282 ---
   211|                     )
   212|                 else:
   213|                     request.setResponseCode(302)
   214|                     request.setHeader("Location", next_link)
   215|                     finish_request(request)
   216|                     return None
   217|             html = self.config.email_registration_template_success_html_content
   218|             status_code = 200
   219|         except ThreepidValidationError as e:
   220|             status_code = e.code
   221|             template_vars = {"failure_reason": e.msg}
   222|             html = self._failure_email_template.render(**template_vars)
   223|         respond_with_html(request, status_code, html)
   224| class UsernameAvailabilityRestServlet(RestServlet):
   225|     PATTERNS = client_patterns("/register/available")
   226|     def __init__(self, hs):
   227|         """
   228|         Args:
   229|             hs (synapse.server.HomeServer): server
   230|         """
   231|         super().__init__()
   232|         self.hs = hs
   233|         self.registration_handler = hs.get_registration_handler()
   234|         self.ratelimiter = FederationRateLimiter(
   235|             hs.get_clock(),
   236|             FederationRateLimitConfig(
   237|                 window_size=2000,
   238|                 sleep_limit=1,
   239|                 sleep_msec=1000,
   240|                 reject_limit=1,
   241|                 concurrent_requests=1,
   242|             ),
   243|         )
   244|     async def on_GET(self, request):
   245|         if not self.hs.config.enable_registration:
   246|             raise SynapseError(
   247|                 403, "Registration has been disabled", errcode=Codes.FORBIDDEN
   248|             )
   249|         ip = self.hs.get_ip_from_request(request)
   250|         with self.ratelimiter.ratelimit(ip) as wait_deferred:
   251|             await wait_deferred
   252|             username = parse_string(request, "username", required=True)
   253|             await self.registration_handler.check_username(username)
   254|             return 200, {"available": True}
   255| class RegisterRestServlet(RestServlet):
   256|     PATTERNS = client_patterns("/register$")
   257|     def __init__(self, hs):
   258|         """
   259|         Args:
   260|             hs (synapse.server.HomeServer): server
   261|         """
   262|         super().__init__()
   263|         self.hs = hs
   264|         self.auth = hs.get_auth()
   265|         self.store = hs.get_datastore()
   266|         self.auth_handler = hs.get_auth_handler()
   267|         self.registration_handler = hs.get_registration_handler()
   268|         self.identity_handler = hs.get_handlers().identity_handler
   269|         self.room_member_handler = hs.get_room_member_handler()
   270|         self.macaroon_gen = hs.get_macaroon_generator()
   271|         self.ratelimiter = hs.get_registration_ratelimiter()
   272|         self.password_policy_handler = hs.get_password_policy_handler()
   273|         self.clock = hs.get_clock()
   274|         self._registration_enabled = self.hs.config.enable_registration
   275|         self._registration_flows = _calculate_registration_flows(
   276|             hs.config, self.auth_handler
   277|         )
   278|     @interactive_auth_handler
   279|     async def on_POST(self, request):
   280|         body = parse_json_object_from_request(request)
   281|         client_addr = request.getClientIP()
   282|         self.ratelimiter.ratelimit(client_addr, update=False)

# --- HUNK 5: Lines 284-329 ---
   284|         if b"kind" in request.args:
   285|             kind = request.args[b"kind"][0]
   286|         if kind == b"guest":
   287|             ret = await self._do_guest_registration(body, address=client_addr)
   288|             return ret
   289|         elif kind != b"user":
   290|             raise UnrecognizedRequestError(
   291|                 "Do not understand membership kind: %s" % (kind.decode("utf8"),)
   292|             )
   293|         desired_username = None
   294|         if "username" in body:
   295|             if not isinstance(body["username"], str) or len(body["username"]) > 512:
   296|                 raise SynapseError(400, "Invalid username")
   297|             desired_username = body["username"]
   298|         appservice = None
   299|         if self.auth.has_access_token(request):
   300|             appservice = self.auth.get_appservice_by_req(request)
   301|         if appservice:
   302|             desired_username = body.get("user", desired_username)
   303|             access_token = self.auth.get_access_token_from_request(request)
   304|             if not isinstance(desired_username, str):
   305|                 raise SynapseError(400, "Desired Username is missing or not a string")
   306|             result = await self._do_appservice_registration(
   307|                 desired_username, access_token, body
   308|             )
   309|             return 200, result
   310|         if not self._registration_enabled:
   311|             raise SynapseError(403, "Registration has been disabled")
   312|         if desired_username is not None:
   313|             desired_username = desired_username.lower()
   314|         guest_access_token = body.get("guest_access_token", None)
   315|         password = body.pop("password", None)
   316|         if password is not None:
   317|             if not isinstance(password, str) or len(password) > 512:
   318|                 raise SynapseError(400, "Invalid password")
   319|             self.password_policy_handler.validate_password(password)
   320|         if "initial_device_display_name" in body and password is None:
   321|             logger.warning("Ignoring initial_device_display_name without password")
   322|             del body["initial_device_display_name"]
   323|         session_id = self.auth_handler.get_session_id(body)
   324|         registered_user_id = None
   325|         password_hash = None
   326|         if session_id:
   327|             registered_user_id = await self.auth_handler.get_session_data(
   328|                 session_id, "registered_user_id", None
   329|             )


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/relations.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 17-57 ---
    17|     PaginationChunk,
    18|     RelationPaginationToken,
    19| )
    20| from synapse.util.stringutils import random_string
    21| from ._base import client_patterns
    22| logger = logging.getLogger(__name__)
    23| class RelationSendServlet(RestServlet):
    24|     """Helper API for sending events that have relation data.
    25|     Example API shape to send a  reaction to a room:
    26|         POST /rooms/!foo/send_relation/$bar/m.annotation/m.reaction?key=%F0%9F%91%8D
    27|         {}
    28|         {
    29|             "event_id": "$foobar"
    30|         }
    31|     """
    32|     PATTERN = (
    33|         "/rooms/(?P<room_id>[^/]*)/send_relation"
    34|         "/(?P<parent_id>[^/]*)/(?P<relation_type>[^/]*)/(?P<event_type>[^/]*)"
    35|     )
    36|     def __init__(self, hs):
    37|         super().__init__()
    38|         self.auth = hs.get_auth()
    39|         self.event_creation_handler = hs.get_event_creation_handler()
    40|         self.txns = HttpTransactionCache(hs)
    41|     def register(self, http_server):
    42|         http_server.register_paths(
    43|             "POST",
    44|             client_patterns(self.PATTERN + "$", releases=()),
    45|             self.on_PUT_or_POST,
    46|             self.__class__.__name__,
    47|         )
    48|         http_server.register_paths(
    49|             "PUT",
    50|             client_patterns(self.PATTERN + "/(?P<txn_id>[^/]*)$", releases=()),
    51|             self.on_PUT,
    52|             self.__class__.__name__,
    53|         )
    54|     def on_PUT(self, request, *args, **kwargs):
    55|         return self.txns.fetch_or_execute_request(
    56|             request, self.on_PUT_or_POST, request, *args, **kwargs
    57|         )

# --- HUNK 2: Lines 78-118 ---
    78|             (
    79|                 event,
    80|                 _,
    81|             ) = await self.event_creation_handler.create_and_send_nonmember_event(
    82|                 requester, event_dict=event_dict, txn_id=txn_id
    83|             )
    84|             event_id = event.event_id
    85|         except ShadowBanError:
    86|             event_id = "$" + random_string(43)
    87|         return 200, {"event_id": event_id}
    88| class RelationPaginationServlet(RestServlet):
    89|     """API to paginate relations on an event by topological ordering, optionally
    90|     filtered by relation type and event type.
    91|     """
    92|     PATTERNS = client_patterns(
    93|         "/rooms/(?P<room_id>[^/]*)/relations/(?P<parent_id>[^/]*)"
    94|         "(/(?P<relation_type>[^/]*)(/(?P<event_type>[^/]*))?)?$",
    95|         releases=(),
    96|     )
    97|     def __init__(self, hs):
    98|         super().__init__()
    99|         self.auth = hs.get_auth()
   100|         self.store = hs.get_datastore()
   101|         self.clock = hs.get_clock()
   102|         self._event_serializer = hs.get_event_client_serializer()
   103|         self.event_handler = hs.get_event_handler()
   104|     async def on_GET(
   105|         self, request, room_id, parent_id, relation_type=None, event_type=None
   106|     ):
   107|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   108|         await self.auth.check_user_in_room_or_world_readable(
   109|             room_id, requester.user.to_string(), allow_departed_users=True
   110|         )
   111|         event = await self.event_handler.get_event(requester.user, room_id, parent_id)
   112|         limit = parse_integer(request, "limit", default=5)
   113|         from_token = parse_string(request, "from")
   114|         to_token = parse_string(request, "to")
   115|         if event.internal_metadata.is_redacted():
   116|             pagination_chunk = PaginationChunk(chunk=[])
   117|         else:
   118|             if from_token:

# --- HUNK 3: Lines 145-185 ---
   145|     """API to paginate aggregation groups of relations, e.g. paginate the
   146|     types and counts of the reactions on the events.
   147|     Example request and response:
   148|         GET /rooms/{room_id}/aggregations/{parent_id}
   149|         {
   150|             chunk: [
   151|                 {
   152|                     "type": "m.reaction",
   153|                     "key": "",
   154|                     "count": 3
   155|                 }
   156|             ]
   157|         }
   158|     """
   159|     PATTERNS = client_patterns(
   160|         "/rooms/(?P<room_id>[^/]*)/aggregations/(?P<parent_id>[^/]*)"
   161|         "(/(?P<relation_type>[^/]*)(/(?P<event_type>[^/]*))?)?$",
   162|         releases=(),
   163|     )
   164|     def __init__(self, hs):
   165|         super().__init__()
   166|         self.auth = hs.get_auth()
   167|         self.store = hs.get_datastore()
   168|         self.event_handler = hs.get_event_handler()
   169|     async def on_GET(
   170|         self, request, room_id, parent_id, relation_type=None, event_type=None
   171|     ):
   172|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   173|         await self.auth.check_user_in_room_or_world_readable(
   174|             room_id, requester.user.to_string(), allow_departed_users=True,
   175|         )
   176|         event = await self.event_handler.get_event(requester.user, room_id, parent_id)
   177|         if relation_type not in (RelationTypes.ANNOTATION, None):
   178|             raise SynapseError(400, "Relation type must be 'annotation'")
   179|         limit = parse_integer(request, "limit", default=5)
   180|         from_token = parse_string(request, "from")
   181|         to_token = parse_string(request, "to")
   182|         if event.internal_metadata.is_redacted():
   183|             pagination_chunk = PaginationChunk(chunk=[])
   184|         else:
   185|             if from_token:

# --- HUNK 4: Lines 203-243 ---
   203|             chunk: [
   204|                 {
   205|                     "type": "m.reaction",
   206|                     "content": {
   207|                         "m.relates_to": {
   208|                             "rel_type": "m.annotation",
   209|                             "key": ""
   210|                         }
   211|                     }
   212|                 },
   213|                 ...
   214|             ]
   215|         }
   216|     """
   217|     PATTERNS = client_patterns(
   218|         "/rooms/(?P<room_id>[^/]*)/aggregations/(?P<parent_id>[^/]*)"
   219|         "/(?P<relation_type>[^/]*)/(?P<event_type>[^/]*)/(?P<key>[^/]*)$",
   220|         releases=(),
   221|     )
   222|     def __init__(self, hs):
   223|         super().__init__()
   224|         self.auth = hs.get_auth()
   225|         self.store = hs.get_datastore()
   226|         self.clock = hs.get_clock()
   227|         self._event_serializer = hs.get_event_client_serializer()
   228|         self.event_handler = hs.get_event_handler()
   229|     async def on_GET(self, request, room_id, parent_id, relation_type, event_type, key):
   230|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
   231|         await self.auth.check_user_in_room_or_world_readable(
   232|             room_id, requester.user.to_string(), allow_departed_users=True,
   233|         )
   234|         await self.event_handler.get_event(requester.user, room_id, parent_id)
   235|         if relation_type != RelationTypes.ANNOTATION:
   236|             raise SynapseError(400, "Relation type must be 'annotation'")
   237|         limit = parse_integer(request, "limit", default=5)
   238|         from_token = parse_string(request, "from")
   239|         to_token = parse_string(request, "to")
   240|         if from_token:
   241|             from_token = RelationPaginationToken.from_string(from_token)
   242|         if to_token:
   243|             to_token = RelationPaginationToken.from_string(to_token)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/report_event.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-34 ---
     1| import logging
     2| from http import HTTPStatus
     3| from synapse.api.errors import Codes, SynapseError
     4| from synapse.http.servlet import (
     5|     RestServlet,
     6|     assert_params_in_dict,
     7|     parse_json_object_from_request,
     8| )
     9| from ._base import client_patterns
    10| logger = logging.getLogger(__name__)
    11| class ReportEventRestServlet(RestServlet):
    12|     PATTERNS = client_patterns("/rooms/(?P<room_id>[^/]*)/report/(?P<event_id>[^/]*)$")
    13|     def __init__(self, hs):
    14|         super().__init__()
    15|         self.hs = hs
    16|         self.auth = hs.get_auth()
    17|         self.clock = hs.get_clock()
    18|         self.store = hs.get_datastore()
    19|     async def on_POST(self, request, room_id, event_id):
    20|         requester = await self.auth.get_user_by_req(request)
    21|         user_id = requester.user.to_string()
    22|         body = parse_json_object_from_request(request)
    23|         assert_params_in_dict(body, ("reason", "score"))
    24|         if not isinstance(body["reason"], str):
    25|             raise SynapseError(
    26|                 HTTPStatus.BAD_REQUEST,
    27|                 "Param 'reason' must be a string",
    28|                 Codes.BAD_JSON,
    29|             )
    30|         if not isinstance(body["score"], int):
    31|             raise SynapseError(
    32|                 HTTPStatus.BAD_REQUEST,
    33|                 "Param 'score' must be an integer",
    34|                 Codes.BAD_JSON,


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/room_keys.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-39 ---
     1| import logging
     2| from synapse.api.errors import Codes, NotFoundError, SynapseError
     3| from synapse.http.servlet import (
     4|     RestServlet,
     5|     parse_json_object_from_request,
     6|     parse_string,
     7| )
     8| from ._base import client_patterns
     9| logger = logging.getLogger(__name__)
    10| class RoomKeysServlet(RestServlet):
    11|     PATTERNS = client_patterns(
    12|         "/room_keys/keys(/(?P<room_id>[^/]+))?(/(?P<session_id>[^/]+))?$"
    13|     )
    14|     def __init__(self, hs):
    15|         """
    16|         Args:
    17|             hs (synapse.server.HomeServer): server
    18|         """
    19|         super().__init__()
    20|         self.auth = hs.get_auth()
    21|         self.e2e_room_keys_handler = hs.get_e2e_room_keys_handler()
    22|     async def on_PUT(self, request, room_id, session_id):
    23|         """
    24|         Uploads one or more encrypted E2E room keys for backup purposes.
    25|         room_id: the ID of the room the keys are for (optional)
    26|         session_id: the ID for the E2E room keys for the room (optional)
    27|         version: the version of the user's backup which this data is for.
    28|         the version must already have been created via the /room_keys/version API.
    29|         Each session has:
    30|          * first_message_index: a numeric index indicating the oldest message
    31|            encrypted by this session.
    32|          * forwarded_count: how many times the uploading client claims this key
    33|            has been shared (forwarded)
    34|          * is_verified: whether the client that uploaded the keys claims they
    35|            were sent by a device which they've verified
    36|          * session_data: base64-encrypted data describing the session.
    37|         Returns 200 OK on success with body {}
    38|         Returns 403 Forbidden if the version in question is not the most recently
    39|         created version (i.e. if this is an old client trying to write to a stale backup)

# --- HUNK 2: Lines 166-246 ---
   166|         {}
   167|         room_id: the ID of the room whose keys to delete (optional)
   168|         session_id: the ID for the E2E session to delete (optional)
   169|         version: the version of the user's backup which this data is for.
   170|         the version must already have been created via the /change_secret API.
   171|         """
   172|         requester = await self.auth.get_user_by_req(request, allow_guest=False)
   173|         user_id = requester.user.to_string()
   174|         version = parse_string(request, "version")
   175|         ret = await self.e2e_room_keys_handler.delete_room_keys(
   176|             user_id, version, room_id, session_id
   177|         )
   178|         return 200, ret
   179| class RoomKeysNewVersionServlet(RestServlet):
   180|     PATTERNS = client_patterns("/room_keys/version$")
   181|     def __init__(self, hs):
   182|         """
   183|         Args:
   184|             hs (synapse.server.HomeServer): server
   185|         """
   186|         super().__init__()
   187|         self.auth = hs.get_auth()
   188|         self.e2e_room_keys_handler = hs.get_e2e_room_keys_handler()
   189|     async def on_POST(self, request):
   190|         """
   191|         Create a new backup version for this user's room_keys with the given
   192|         info.  The version is allocated by the server and returned to the user
   193|         in the response.  This API is intended to be used whenever the user
   194|         changes the encryption key for their backups, ensuring that backups
   195|         encrypted with different keys don't collide.
   196|         It takes out an exclusive lock on this user's room_key backups, to ensure
   197|         clients only upload to the current backup.
   198|         The algorithm passed in the version info is a reverse-DNS namespaced
   199|         identifier to describe the format of the encrypted backupped keys.
   200|         The auth_data is { user_id: "user_id", nonce: <random string> }
   201|         encrypted using the algorithm and current encryption key described above.
   202|         POST /room_keys/version
   203|         Content-Type: application/json
   204|         {
   205|             "algorithm": "m.megolm_backup.v1",
   206|             "auth_data": "dGhpcyBzaG91bGQgYWN0dWFsbHkgYmUgZW5jcnlwdGVkIGpzb24K"
   207|         }
   208|         HTTP/1.1 200 OK
   209|         Content-Type: application/json
   210|         {
   211|             "version": 12345
   212|         }
   213|         """
   214|         requester = await self.auth.get_user_by_req(request, allow_guest=False)
   215|         user_id = requester.user.to_string()
   216|         info = parse_json_object_from_request(request)
   217|         new_version = await self.e2e_room_keys_handler.create_version(user_id, info)
   218|         return 200, {"version": new_version}
   219| class RoomKeysVersionServlet(RestServlet):
   220|     PATTERNS = client_patterns("/room_keys/version(/(?P<version>[^/]+))?$")
   221|     def __init__(self, hs):
   222|         """
   223|         Args:
   224|             hs (synapse.server.HomeServer): server
   225|         """
   226|         super().__init__()
   227|         self.auth = hs.get_auth()
   228|         self.e2e_room_keys_handler = hs.get_e2e_room_keys_handler()
   229|     async def on_GET(self, request, version):
   230|         """
   231|         Retrieve the version information about a given version of the user's
   232|         room_keys backup.  If the version part is missing, returns info about the
   233|         most current backup version (if any)
   234|         It takes out an exclusive lock on this user's room_key backups, to ensure
   235|         clients only upload to the current backup.
   236|         Returns 404 if the given version does not exist.
   237|         GET /room_keys/version/12345 HTTP/1.1
   238|         {
   239|             "version": "12345",
   240|             "algorithm": "m.megolm_backup.v1",
   241|             "auth_data": "dGhpcyBzaG91bGQgYWN0dWFsbHkgYmUgZW5jcnlwdGVkIGpzb24K"
   242|         }
   243|         """
   244|         requester = await self.auth.get_user_by_req(request, allow_guest=False)
   245|         user_id = requester.user.to_string()
   246|         try:


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/room_upgrade_rest_servlet.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 8-48 ---
     8| )
     9| from synapse.util import stringutils
    10| from ._base import client_patterns
    11| logger = logging.getLogger(__name__)
    12| class RoomUpgradeRestServlet(RestServlet):
    13|     """Handler for room uprade requests.
    14|     Handles requests of the form:
    15|         POST /_matrix/client/r0/rooms/$roomid/upgrade HTTP/1.1
    16|         Content-Type: application/json
    17|         {
    18|             "new_version": "2",
    19|         }
    20|     Creates a new room and shuts down the old one. Returns the ID of the new room.
    21|     Args:
    22|         hs (synapse.server.HomeServer):
    23|     """
    24|     PATTERNS = client_patterns(
    25|         "/rooms/(?P<room_id>[^/]*)/upgrade$"
    26|     )
    27|     def __init__(self, hs):
    28|         super().__init__()
    29|         self._hs = hs
    30|         self._room_creation_handler = hs.get_room_creation_handler()
    31|         self._auth = hs.get_auth()
    32|     async def on_POST(self, request, room_id):
    33|         requester = await self._auth.get_user_by_req(request)
    34|         content = parse_json_object_from_request(request)
    35|         assert_params_in_dict(content, ("new_version",))
    36|         new_version = KNOWN_ROOM_VERSIONS.get(content["new_version"])
    37|         if new_version is None:
    38|             raise SynapseError(
    39|                 400,
    40|                 "Your homeserver does not support this room version",
    41|                 Codes.UNSUPPORTED_ROOM_VERSION,
    42|             )
    43|         try:
    44|             new_room_id = await self._room_creation_handler.upgrade_room(
    45|                 requester, room_id, new_version
    46|             )
    47|         except ShadowBanError:
    48|             new_room_id = stringutils.random_string(18)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/sendtodevice.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-38 ---
     1| import logging
     2| from typing import Tuple
     3| from synapse.http import servlet
     4| from synapse.http.servlet import parse_json_object_from_request
     5| from synapse.logging.opentracing import set_tag, trace
     6| from synapse.rest.client.transactions import HttpTransactionCache
     7| from ._base import client_patterns
     8| logger = logging.getLogger(__name__)
     9| class SendToDeviceRestServlet(servlet.RestServlet):
    10|     PATTERNS = client_patterns(
    11|         "/sendToDevice/(?P<message_type>[^/]*)/(?P<txn_id>[^/]*)$"
    12|     )
    13|     def __init__(self, hs):
    14|         """
    15|         Args:
    16|             hs (synapse.server.HomeServer): server
    17|         """
    18|         super().__init__()
    19|         self.hs = hs
    20|         self.auth = hs.get_auth()
    21|         self.txns = HttpTransactionCache(hs)
    22|         self.device_message_handler = hs.get_device_message_handler()
    23|     @trace(opname="sendToDevice")
    24|     def on_PUT(self, request, message_type, txn_id):
    25|         set_tag("message_type", message_type)
    26|         set_tag("txn_id", txn_id)
    27|         return self.txns.fetch_or_execute_request(
    28|             request, self._put, request, message_type, txn_id
    29|         )
    30|     async def _put(self, request, message_type, txn_id):
    31|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    32|         content = parse_json_object_from_request(request)
    33|         sender_user_id = requester.user.to_string()
    34|         await self.device_message_handler.send_device_message(
    35|             sender_user_id, message_type, content["messages"]
    36|         )
    37|         response = (200, {})  # type: Tuple[int, dict]
    38|         return response


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/shared_rooms.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-36 ---
     1| import logging
     2| from synapse.api.errors import Codes, SynapseError
     3| from synapse.http.servlet import RestServlet
     4| from synapse.types import UserID
     5| from ._base import client_patterns
     6| logger = logging.getLogger(__name__)
     7| class UserSharedRoomsServlet(RestServlet):
     8|     """
     9|     GET /uk.half-shot.msc2666/user/shared_rooms/{user_id} HTTP/1.1
    10|     """
    11|     PATTERNS = client_patterns(
    12|         "/uk.half-shot.msc2666/user/shared_rooms/(?P<user_id>[^/]*)",
    13|         releases=(),  # This is an unstable feature
    14|     )
    15|     def __init__(self, hs):
    16|         super().__init__()
    17|         self.auth = hs.get_auth()
    18|         self.store = hs.get_datastore()
    19|         self.user_directory_active = hs.config.update_user_directory
    20|     async def on_GET(self, request, user_id):
    21|         if not self.user_directory_active:
    22|             raise SynapseError(
    23|                 code=400,
    24|                 msg="The user directory is disabled on this server. Cannot determine shared rooms.",
    25|                 errcode=Codes.FORBIDDEN,
    26|             )
    27|         UserID.from_string(user_id)
    28|         requester = await self.auth.get_user_by_req(request)
    29|         if user_id == requester.user.to_string():
    30|             raise SynapseError(
    31|                 code=400,
    32|                 msg="You cannot request a list of shared rooms with yourself",
    33|                 errcode=Codes.FORBIDDEN,
    34|             )
    35|         rooms = await self.store.get_shared_rooms_for_users(
    36|             requester.user.to_string(), user_id


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/sync.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 33-76 ---
    33|                 "timeline": { // The recent events in the room if gap is "true"
    34|                   "limited": // Was the per-room event limit exceeded?
    35|                              // otherwise the next events in the room.
    36|                   "events": [] // list of EventIDs in the "event_map".
    37|                   "prev_batch": // back token for getting previous events.
    38|                 }
    39|                 "state": {"events": []} // list of EventIDs updating the
    40|                                         // current state to be what it should
    41|                                         // be at the end of the batch.
    42|                 "ephemeral": {"events": []} // list of event objects
    43|               }
    44|             },
    45|             "invite": {}, // Invited rooms being updated.
    46|             "leave": {} // Archived rooms being updated.
    47|           }
    48|         }
    49|     """
    50|     PATTERNS = client_patterns("/sync$")
    51|     ALLOWED_PRESENCE = {"online", "offline", "unavailable"}
    52|     def __init__(self, hs):
    53|         super().__init__()
    54|         self.hs = hs
    55|         self.auth = hs.get_auth()
    56|         self.store = hs.get_datastore()
    57|         self.sync_handler = hs.get_sync_handler()
    58|         self.clock = hs.get_clock()
    59|         self.filtering = hs.get_filtering()
    60|         self.presence_handler = hs.get_presence_handler()
    61|         self._server_notices_sender = hs.get_server_notices_sender()
    62|         self._event_serializer = hs.get_event_client_serializer()
    63|     async def on_GET(self, request):
    64|         if b"from" in request.args:
    65|             raise SynapseError(
    66|                 400, "'from' is not a valid query parameter. Did you mean 'since'?"
    67|             )
    68|         requester = await self.auth.get_user_by_req(request, allow_guest=True)
    69|         user = requester.user
    70|         device_id = requester.device_id
    71|         timeout = parse_integer(request, "timeout", default=0)
    72|         since = parse_string(request, "since")
    73|         set_presence = parse_string(
    74|             request,
    75|             "set_presence",
    76|             default="online",

# --- HUNK 2: Lines 100-142 ---
   100|             except Exception:
   101|                 raise SynapseError(400, "Invalid filter JSON")
   102|             self.filtering.check_valid_filter(filter_object)
   103|             filter_collection = FilterCollection(filter_object)
   104|         else:
   105|             try:
   106|                 filter_collection = await self.filtering.get_user_filter(
   107|                     user.localpart, filter_id
   108|                 )
   109|             except StoreError as err:
   110|                 if err.code != 404:
   111|                     raise
   112|                 raise SynapseError(400, "No such filter", errcode=Codes.INVALID_PARAM)
   113|         sync_config = SyncConfig(
   114|             user=user,
   115|             filter_collection=filter_collection,
   116|             is_guest=requester.is_guest,
   117|             request_key=request_key,
   118|             device_id=device_id,
   119|         )
   120|         since_token = None
   121|         if since is not None:
   122|             since_token = await StreamToken.from_string(self.store, since)
   123|         await self._server_notices_sender.on_user_syncing(user.to_string())
   124|         affect_presence = set_presence != PresenceState.OFFLINE
   125|         if affect_presence:
   126|             await self.presence_handler.set_state(
   127|                 user, {"presence": set_presence}, True
   128|             )
   129|         context = await self.presence_handler.user_syncing(
   130|             user.to_string(), affect_presence=affect_presence
   131|         )
   132|         with context:
   133|             sync_result = await self.sync_handler.wait_for_sync_for_user(
   134|                 sync_config,
   135|                 since_token=since_token,
   136|                 timeout=timeout,
   137|                 full_state=full_state,
   138|             )
   139|         if request._disconnected:
   140|             logger.info("Client has disconnected; not serializing response.")
   141|             return 200, {}
   142|         time_now = self.clock.time_msec()

# --- HUNK 3: Lines 169-209 ---
   169|             access_token_id,
   170|             filter.event_fields,
   171|             event_formatter,
   172|         )
   173|         logger.debug("building sync response dict")
   174|         return {
   175|             "account_data": {"events": sync_result.account_data},
   176|             "to_device": {"events": sync_result.to_device},
   177|             "device_lists": {
   178|                 "changed": list(sync_result.device_lists.changed),
   179|                 "left": list(sync_result.device_lists.left),
   180|             },
   181|             "presence": SyncRestServlet.encode_presence(sync_result.presence, time_now),
   182|             "rooms": {"join": joined, "invite": invited, "leave": archived},
   183|             "groups": {
   184|                 "join": sync_result.groups.join,
   185|                 "invite": sync_result.groups.invite,
   186|                 "leave": sync_result.groups.leave,
   187|             },
   188|             "device_one_time_keys_count": sync_result.device_one_time_keys_count,
   189|             "next_batch": await sync_result.next_batch.to_string(self.store),
   190|         }
   191|     @staticmethod
   192|     def encode_presence(events, time_now):
   193|         return {
   194|             "events": [
   195|                 {
   196|                     "type": "m.presence",
   197|                     "sender": event.user_id,
   198|                     "content": format_user_presence_state(
   199|                         event, time_now, include_user_id=False
   200|                     ),
   201|                 }
   202|                 for event in events
   203|             ]
   204|         }
   205|     async def encode_joined(
   206|         self, rooms, time_now, token_id, event_fields, event_formatter
   207|     ):
   208|         """
   209|         Encode the joined rooms in a sync result

# --- HUNK 4: Lines 323-357 ---
   323|                 event_format=event_formatter,
   324|                 only_event_fields=only_fields,
   325|             )
   326|         state_dict = room.state
   327|         timeline_events = room.timeline.events
   328|         state_events = state_dict.values()
   329|         for event in itertools.chain(state_events, timeline_events):
   330|             if event.room_id != room.room_id:
   331|                 logger.warning(
   332|                     "Event %r is under room %r instead of %r",
   333|                     event.event_id,
   334|                     room.room_id,
   335|                     event.room_id,
   336|                 )
   337|         serialized_state = await serialize(state_events)
   338|         serialized_timeline = await serialize(timeline_events)
   339|         account_data = room.account_data
   340|         result = {
   341|             "timeline": {
   342|                 "events": serialized_timeline,
   343|                 "prev_batch": await room.timeline.prev_batch.to_string(self.store),
   344|                 "limited": room.timeline.limited,
   345|             },
   346|             "state": {"events": serialized_state},
   347|             "account_data": {"events": account_data},
   348|         }
   349|         if joined:
   350|             ephemeral_events = room.ephemeral
   351|             result["ephemeral"] = {"events": ephemeral_events}
   352|             result["unread_notifications"] = room.unread_notifications
   353|             result["summary"] = room.summary
   354|             result["org.matrix.msc2654.unread_count"] = room.unread_count
   355|         return result
   356| def register_servlets(hs, http_server):
   357|     SyncRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/tags.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-50 ---
     1| import logging
     2| from synapse.api.errors import AuthError
     3| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     4| from ._base import client_patterns
     5| logger = logging.getLogger(__name__)
     6| class TagListServlet(RestServlet):
     7|     """
     8|     GET /user/{user_id}/rooms/{room_id}/tags HTTP/1.1
     9|     """
    10|     PATTERNS = client_patterns("/user/(?P<user_id>[^/]*)/rooms/(?P<room_id>[^/]*)/tags")
    11|     def __init__(self, hs):
    12|         super().__init__()
    13|         self.auth = hs.get_auth()
    14|         self.store = hs.get_datastore()
    15|     async def on_GET(self, request, user_id, room_id):
    16|         requester = await self.auth.get_user_by_req(request)
    17|         if user_id != requester.user.to_string():
    18|             raise AuthError(403, "Cannot get tags for other users.")
    19|         tags = await self.store.get_tags_for_room(user_id, room_id)
    20|         return 200, {"tags": tags}
    21| class TagServlet(RestServlet):
    22|     """
    23|     PUT /user/{user_id}/rooms/{room_id}/tags/{tag} HTTP/1.1
    24|     DELETE /user/{user_id}/rooms/{room_id}/tags/{tag} HTTP/1.1
    25|     """
    26|     PATTERNS = client_patterns(
    27|         "/user/(?P<user_id>[^/]*)/rooms/(?P<room_id>[^/]*)/tags/(?P<tag>[^/]*)"
    28|     )
    29|     def __init__(self, hs):
    30|         super().__init__()
    31|         self.auth = hs.get_auth()
    32|         self.store = hs.get_datastore()
    33|         self.notifier = hs.get_notifier()
    34|     async def on_PUT(self, request, user_id, room_id, tag):
    35|         requester = await self.auth.get_user_by_req(request)
    36|         if user_id != requester.user.to_string():
    37|             raise AuthError(403, "Cannot add tags for other users.")
    38|         body = parse_json_object_from_request(request)
    39|         max_id = await self.store.add_tag_to_room(user_id, room_id, tag, body)
    40|         self.notifier.on_new_event("account_data_key", max_id, users=[user_id])
    41|         return 200, {}
    42|     async def on_DELETE(self, request, user_id, room_id, tag):
    43|         requester = await self.auth.get_user_by_req(request)
    44|         if user_id != requester.user.to_string():
    45|             raise AuthError(403, "Cannot add tags for other users.")
    46|         max_id = await self.store.remove_tag_from_room(user_id, room_id, tag)
    47|         self.notifier.on_new_event("account_data_key", max_id, users=[user_id])
    48|         return 200, {}
    49| def register_servlets(hs, http_server):
    50|     TagListServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/thirdparty.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-63 ---
     1| import logging
     2| from synapse.api.constants import ThirdPartyEntityKind
     3| from synapse.http.servlet import RestServlet
     4| from ._base import client_patterns
     5| logger = logging.getLogger(__name__)
     6| class ThirdPartyProtocolsServlet(RestServlet):
     7|     PATTERNS = client_patterns("/thirdparty/protocols")
     8|     def __init__(self, hs):
     9|         super().__init__()
    10|         self.auth = hs.get_auth()
    11|         self.appservice_handler = hs.get_application_service_handler()
    12|     async def on_GET(self, request):
    13|         await self.auth.get_user_by_req(request, allow_guest=True)
    14|         protocols = await self.appservice_handler.get_3pe_protocols()
    15|         return 200, protocols
    16| class ThirdPartyProtocolServlet(RestServlet):
    17|     PATTERNS = client_patterns("/thirdparty/protocol/(?P<protocol>[^/]+)$")
    18|     def __init__(self, hs):
    19|         super().__init__()
    20|         self.auth = hs.get_auth()
    21|         self.appservice_handler = hs.get_application_service_handler()
    22|     async def on_GET(self, request, protocol):
    23|         await self.auth.get_user_by_req(request, allow_guest=True)
    24|         protocols = await self.appservice_handler.get_3pe_protocols(
    25|             only_protocol=protocol
    26|         )
    27|         if protocol in protocols:
    28|             return 200, protocols[protocol]
    29|         else:
    30|             return 404, {"error": "Unknown protocol"}
    31| class ThirdPartyUserServlet(RestServlet):
    32|     PATTERNS = client_patterns("/thirdparty/user(/(?P<protocol>[^/]+))?$")
    33|     def __init__(self, hs):
    34|         super().__init__()
    35|         self.auth = hs.get_auth()
    36|         self.appservice_handler = hs.get_application_service_handler()
    37|     async def on_GET(self, request, protocol):
    38|         await self.auth.get_user_by_req(request, allow_guest=True)
    39|         fields = request.args
    40|         fields.pop(b"access_token", None)
    41|         results = await self.appservice_handler.query_3pe(
    42|             ThirdPartyEntityKind.USER, protocol, fields
    43|         )
    44|         return 200, results
    45| class ThirdPartyLocationServlet(RestServlet):
    46|     PATTERNS = client_patterns("/thirdparty/location(/(?P<protocol>[^/]+))?$")
    47|     def __init__(self, hs):
    48|         super().__init__()
    49|         self.auth = hs.get_auth()
    50|         self.appservice_handler = hs.get_application_service_handler()
    51|     async def on_GET(self, request, protocol):
    52|         await self.auth.get_user_by_req(request, allow_guest=True)
    53|         fields = request.args
    54|         fields.pop(b"access_token", None)
    55|         results = await self.appservice_handler.query_3pe(
    56|             ThirdPartyEntityKind.LOCATION, protocol, fields
    57|         )
    58|         return 200, results
    59| def register_servlets(hs, http_server):
    60|     ThirdPartyProtocolsServlet(hs).register(http_server)
    61|     ThirdPartyProtocolServlet(hs).register(http_server)
    62|     ThirdPartyUserServlet(hs).register(http_server)
    63|     ThirdPartyLocationServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/tokenrefresh.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-15 ---
     1| from synapse.api.errors import AuthError
     2| from synapse.http.servlet import RestServlet
     3| from ._base import client_patterns
     4| class TokenRefreshRestServlet(RestServlet):
     5|     """
     6|     Exchanges refresh tokens for a pair of an access token and a new refresh
     7|     token.
     8|     """
     9|     PATTERNS = client_patterns("/tokenrefresh")
    10|     def __init__(self, hs):
    11|         super().__init__()
    12|     async def on_POST(self, request):
    13|         raise AuthError(403, "tokenrefresh is no longer supported.")
    14| def register_servlets(hs, http_server):
    15|     TokenRefreshRestServlet(hs).register(http_server)


# ====================================================================
# FILE: synapse/rest/client/v2_alpha/user_directory.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-33 ---
     1| import logging
     2| from synapse.api.errors import SynapseError
     3| from synapse.http.servlet import RestServlet, parse_json_object_from_request
     4| from ._base import client_patterns
     5| logger = logging.getLogger(__name__)
     6| class UserDirectorySearchRestServlet(RestServlet):
     7|     PATTERNS = client_patterns("/user_directory/search$")
     8|     def __init__(self, hs):
     9|         """
    10|         Args:
    11|             hs (synapse.server.HomeServer): server
    12|         """
    13|         super().__init__()
    14|         self.hs = hs
    15|         self.auth = hs.get_auth()
    16|         self.user_directory_handler = hs.get_user_directory_handler()
    17|     async def on_POST(self, request):
    18|         """Searches for users in directory
    19|         Returns:
    20|             dict of the form::
    21|                 {
    22|                     "limited": <bool>,  # whether there were more results or not
    23|                     "results": [  # Ordered by best match first
    24|                         {
    25|                             "user_id": <user_id>,
    26|                             "display_name": <display_name>,
    27|                             "avatar_url": <avatar_url>
    28|                         }
    29|                     ]
    30|                 }
    31|         """
    32|         requester = await self.auth.get_user_by_req(request, allow_guest=False)
    33|         user_id = requester.user.to_string()


# ====================================================================
# FILE: synapse/rest/client/versions.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-29 ---
     1| import logging
     2| import re
     3| from synapse.api.constants import RoomCreationPreset
     4| from synapse.http.servlet import RestServlet
     5| logger = logging.getLogger(__name__)
     6| class VersionsRestServlet(RestServlet):
     7|     PATTERNS = [re.compile("^/_matrix/client/versions$")]
     8|     def __init__(self, hs):
     9|         super().__init__()
    10|         self.config = hs.config
    11|         self.e2ee_forced_public = (
    12|             RoomCreationPreset.PUBLIC_CHAT
    13|             in self.config.encryption_enabled_by_default_for_room_presets
    14|         )
    15|         self.e2ee_forced_private = (
    16|             RoomCreationPreset.PRIVATE_CHAT
    17|             in self.config.encryption_enabled_by_default_for_room_presets
    18|         )
    19|         self.e2ee_forced_trusted_private = (
    20|             RoomCreationPreset.TRUSTED_PRIVATE_CHAT
    21|             in self.config.encryption_enabled_by_default_for_room_presets
    22|         )
    23|     def on_GET(self, request):
    24|         return (
    25|             200,
    26|             {
    27|                 "versions": [
    28|                     "r0.0.1",
    29|                     "r0.1.0",


# ====================================================================
# FILE: synapse/rest/key/v2/remote_key_resource.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| import logging
     2| from typing import Dict, Set
     3| from signedjson.sign import sign_json
     4| from synapse.api.errors import Codes, SynapseError
     5| from synapse.crypto.keyring import ServerKeyFetcher
     6| from synapse.http.server import DirectServeJsonResource, respond_with_json
     7| from synapse.http.servlet import parse_integer, parse_json_object_from_request
     8| from synapse.util import json_decoder
     9| logger = logging.getLogger(__name__)
    10| class RemoteKey(DirectServeJsonResource):
    11|     """HTTP resource for retrieving the TLS certificate and NACL signature
    12|     verification keys for a collection of servers. Checks that the reported
    13|     X.509 TLS certificate matches the one used in the HTTPS connection. Checks
    14|     that the NACL signature for the remote server is valid. Returns a dict of
    15|     JSON signed by both the remote server and by this server.
    16|     Supports individual GET APIs and a bulk query POST API.
    17|     Requests:
    18|     GET /_matrix/key/v2/query/remote.server.example.com HTTP/1.1
    19|     GET /_matrix/key/v2/query/remote.server.example.com/a.key.id HTTP/1.1
    20|     POST /_matrix/v2/query HTTP/1.1
    21|     Content-Type: application/json
    22|     {
    23|         "server_keys": {
    24|             "remote.server.example.com": {
    25|                 "a.key.id": {
    26|                     "minimum_valid_until_ts": 1234567890123
    27|                 }
    28|             }
    29|         }
    30|     }
    31|     Response:
    32|     HTTP/1.1 200 OK
    33|     Content-Type: application/json
    34|     {
    35|         "server_keys": [
    36|             {
    37|                 "server_name": "remote.server.example.com"


# ====================================================================
# FILE: synapse/rest/media/v1/filepath.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 30-92 ---
    30|     default_thumbnail = _wrap_in_base_path(default_thumbnail_rel)
    31|     def local_media_filepath_rel(self, media_id):
    32|         return os.path.join("local_content", media_id[0:2], media_id[2:4], media_id[4:])
    33|     local_media_filepath = _wrap_in_base_path(local_media_filepath_rel)
    34|     def local_media_thumbnail_rel(self, media_id, width, height, content_type, method):
    35|         top_level_type, sub_type = content_type.split("/")
    36|         file_name = "%i-%i-%s-%s-%s" % (width, height, top_level_type, sub_type, method)
    37|         return os.path.join(
    38|             "local_thumbnails", media_id[0:2], media_id[2:4], media_id[4:], file_name
    39|         )
    40|     local_media_thumbnail = _wrap_in_base_path(local_media_thumbnail_rel)
    41|     def remote_media_filepath_rel(self, server_name, file_id):
    42|         return os.path.join(
    43|             "remote_content", server_name, file_id[0:2], file_id[2:4], file_id[4:]
    44|         )
    45|     remote_media_filepath = _wrap_in_base_path(remote_media_filepath_rel)
    46|     def remote_media_thumbnail_rel(
    47|         self, server_name, file_id, width, height, content_type, method
    48|     ):
    49|         top_level_type, sub_type = content_type.split("/")
    50|         file_name = "%i-%i-%s-%s-%s" % (width, height, top_level_type, sub_type, method)
    51|         return os.path.join(
    52|             "remote_thumbnail",
    53|             server_name,
    54|             file_id[0:2],
    55|             file_id[2:4],
    56|             file_id[4:],
    57|             file_name,
    58|         )
    59|     remote_media_thumbnail = _wrap_in_base_path(remote_media_thumbnail_rel)
    60|     def remote_media_thumbnail_rel_legacy(
    61|         self, server_name, file_id, width, height, content_type
    62|     ):
    63|         top_level_type, sub_type = content_type.split("/")
    64|         file_name = "%i-%i-%s-%s" % (width, height, top_level_type, sub_type)
    65|         return os.path.join(
    66|             "remote_thumbnail",
    67|             server_name,
    68|             file_id[0:2],
    69|             file_id[2:4],
    70|             file_id[4:],
    71|             file_name,
    72|         )
    73|     def remote_media_thumbnail_dir(self, server_name, file_id):
    74|         return os.path.join(
    75|             self.base_path,
    76|             "remote_thumbnail",
    77|             server_name,
    78|             file_id[0:2],
    79|             file_id[2:4],
    80|             file_id[4:],
    81|         )
    82|     def url_cache_filepath_rel(self, media_id):
    83|         if NEW_FORMAT_ID_RE.match(media_id):
    84|             return os.path.join("url_cache", media_id[:10], media_id[11:])
    85|         else:
    86|             return os.path.join("url_cache", media_id[0:2], media_id[2:4], media_id[4:])
    87|     url_cache_filepath = _wrap_in_base_path(url_cache_filepath_rel)
    88|     def url_cache_filepath_dirs_to_delete(self, media_id):
    89|         "The dirs to try and remove if we delete the media_id file"
    90|         if NEW_FORMAT_ID_RE.match(media_id):
    91|             return [os.path.join(self.base_path, "url_cache", media_id[:10])]
    92|         else:


# ====================================================================
# FILE: synapse/rest/media/v1/media_repository.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 17-57 ---
    17| from synapse.config._base import ConfigError
    18| from synapse.logging.context import defer_to_thread
    19| from synapse.metrics.background_process_metrics import run_as_background_process
    20| from synapse.util.async_helpers import Linearizer
    21| from synapse.util.retryutils import NotRetryingDestination
    22| from synapse.util.stringutils import random_string
    23| from ._base import (
    24|     FileInfo,
    25|     Responder,
    26|     get_filename_from_headers,
    27|     respond_404,
    28|     respond_with_responder,
    29| )
    30| from .config_resource import MediaConfigResource
    31| from .download_resource import DownloadResource
    32| from .filepath import MediaFilePaths
    33| from .media_storage import MediaStorage
    34| from .preview_url_resource import PreviewUrlResource
    35| from .storage_provider import StorageProviderWrapper
    36| from .thumbnail_resource import ThumbnailResource
    37| from .thumbnailer import Thumbnailer, ThumbnailError
    38| from .upload_resource import UploadResource
    39| logger = logging.getLogger(__name__)
    40| UPDATE_RECENTLY_ACCESSED_TS = 60 * 1000
    41| class MediaRepository:
    42|     def __init__(self, hs):
    43|         self.hs = hs
    44|         self.auth = hs.get_auth()
    45|         self.client = hs.get_http_client()
    46|         self.clock = hs.get_clock()
    47|         self.server_name = hs.hostname
    48|         self.store = hs.get_datastore()
    49|         self.max_upload_size = hs.config.max_upload_size
    50|         self.max_image_pixels = hs.config.max_image_pixels
    51|         self.primary_base_path = hs.config.media_store_path
    52|         self.filepaths = MediaFilePaths(self.primary_base_path)
    53|         self.dynamic_thumbnails = hs.config.dynamic_thumbnails
    54|         self.thumbnail_requirements = hs.config.thumbnail_requirements
    55|         self.remote_media_linearizer = Linearizer(name="media_remote")
    56|         self.recently_accessed_remotes = set()
    57|         self.recently_accessed_locals = set()

# --- HUNK 2: Lines 80-128 ---
    80|         remote_media = self.recently_accessed_remotes
    81|         self.recently_accessed_remotes = set()
    82|         local_media = self.recently_accessed_locals
    83|         self.recently_accessed_locals = set()
    84|         await self.store.update_cached_last_access_time(
    85|             local_media, remote_media, self.clock.time_msec()
    86|         )
    87|     def mark_recently_accessed(self, server_name, media_id):
    88|         """Mark the given media as recently accessed.
    89|         Args:
    90|             server_name (str|None): Origin server of media, or None if local
    91|             media_id (str): The media ID of the content
    92|         """
    93|         if server_name:
    94|             self.recently_accessed_remotes.add((server_name, media_id))
    95|         else:
    96|             self.recently_accessed_locals.add(media_id)
    97|     async def create_content(
    98|         self,
    99|         media_type: str,
   100|         upload_name: Optional[str],
   101|         content: IO,
   102|         content_length: int,
   103|         auth_user: str,
   104|     ) -> str:
   105|         """Store uploaded content for a local user and return the mxc URL
   106|         Args:
   107|             media_type: The content type of the file.
   108|             upload_name: The name of the file, if provided.
   109|             content: A file like object that is the content to store
   110|             content_length: The length of the content
   111|             auth_user: The user_id of the uploader
   112|         Returns:
   113|             The mxc url of the stored content
   114|         """
   115|         media_id = random_string(24)
   116|         file_info = FileInfo(server_name=None, file_id=media_id)
   117|         fname = await self.media_storage.store_file(content, file_info)
   118|         logger.info("Stored local media in file %r", fname)
   119|         await self.store.store_local_media(
   120|             media_id=media_id,
   121|             media_type=media_type,
   122|             time_now_ms=self.clock.time_msec(),
   123|             upload_name=upload_name,
   124|             media_length=content_length,
   125|             user_id=auth_user,
   126|         )
   127|         await self._generate_thumbnails(None, media_id, media_id, media_type)
   128|         return "mxc://%s/%s" % (self.server_name, media_id)

# --- HUNK 3: Lines 329-525 ---
   329|             logger.info(
   330|                 "Image too large to thumbnail %r x %r > %r",
   331|                 m_width,
   332|                 m_height,
   333|                 self.max_image_pixels,
   334|             )
   335|             return
   336|         if thumbnailer.transpose_method is not None:
   337|             m_width, m_height = thumbnailer.transpose()
   338|         if t_method == "crop":
   339|             t_byte_source = thumbnailer.crop(t_width, t_height, t_type)
   340|         elif t_method == "scale":
   341|             t_width, t_height = thumbnailer.aspect(t_width, t_height)
   342|             t_width = min(m_width, t_width)
   343|             t_height = min(m_height, t_height)
   344|             t_byte_source = thumbnailer.scale(t_width, t_height, t_type)
   345|         else:
   346|             t_byte_source = None
   347|         return t_byte_source
   348|     async def generate_local_exact_thumbnail(
   349|         self,
   350|         media_id: str,
   351|         t_width: int,
   352|         t_height: int,
   353|         t_method: str,
   354|         t_type: str,
   355|         url_cache: str,
   356|     ) -> Optional[str]:
   357|         input_path = await self.media_storage.ensure_media_is_in_local_cache(
   358|             FileInfo(None, media_id, url_cache=url_cache)
   359|         )
   360|         try:
   361|             thumbnailer = Thumbnailer(input_path)
   362|         except ThumbnailError as e:
   363|             logger.warning(
   364|                 "Unable to generate a thumbnail for local media %s using a method of %s and type of %s: %s",
   365|                 media_id,
   366|                 t_method,
   367|                 t_type,
   368|                 e,
   369|             )
   370|             return None
   371|         t_byte_source = await defer_to_thread(
   372|             self.hs.get_reactor(),
   373|             self._generate_thumbnail,
   374|             thumbnailer,
   375|             t_width,
   376|             t_height,
   377|             t_method,
   378|             t_type,
   379|         )
   380|         if t_byte_source:
   381|             try:
   382|                 file_info = FileInfo(
   383|                     server_name=None,
   384|                     file_id=media_id,
   385|                     url_cache=url_cache,
   386|                     thumbnail=True,
   387|                     thumbnail_width=t_width,
   388|                     thumbnail_height=t_height,
   389|                     thumbnail_method=t_method,
   390|                     thumbnail_type=t_type,
   391|                 )
   392|                 output_path = await self.media_storage.store_file(
   393|                     t_byte_source, file_info
   394|                 )
   395|             finally:
   396|                 t_byte_source.close()
   397|             logger.info("Stored thumbnail in file %r", output_path)
   398|             t_len = os.path.getsize(output_path)
   399|             await self.store.store_local_thumbnail(
   400|                 media_id, t_width, t_height, t_type, t_method, t_len
   401|             )
   402|             return output_path
   403|         return None
   404|     async def generate_remote_exact_thumbnail(
   405|         self,
   406|         server_name: str,
   407|         file_id: str,
   408|         media_id: str,
   409|         t_width: int,
   410|         t_height: int,
   411|         t_method: str,
   412|         t_type: str,
   413|     ) -> Optional[str]:
   414|         input_path = await self.media_storage.ensure_media_is_in_local_cache(
   415|             FileInfo(server_name, file_id, url_cache=False)
   416|         )
   417|         try:
   418|             thumbnailer = Thumbnailer(input_path)
   419|         except ThumbnailError as e:
   420|             logger.warning(
   421|                 "Unable to generate a thumbnail for remote media %s from %s using a method of %s and type of %s: %s",
   422|                 media_id,
   423|                 server_name,
   424|                 t_method,
   425|                 t_type,
   426|                 e,
   427|             )
   428|             return None
   429|         t_byte_source = await defer_to_thread(
   430|             self.hs.get_reactor(),
   431|             self._generate_thumbnail,
   432|             thumbnailer,
   433|             t_width,
   434|             t_height,
   435|             t_method,
   436|             t_type,
   437|         )
   438|         if t_byte_source:
   439|             try:
   440|                 file_info = FileInfo(
   441|                     server_name=server_name,
   442|                     file_id=file_id,
   443|                     thumbnail=True,
   444|                     thumbnail_width=t_width,
   445|                     thumbnail_height=t_height,
   446|                     thumbnail_method=t_method,
   447|                     thumbnail_type=t_type,
   448|                 )
   449|                 output_path = await self.media_storage.store_file(
   450|                     t_byte_source, file_info
   451|                 )
   452|             finally:
   453|                 t_byte_source.close()
   454|             logger.info("Stored thumbnail in file %r", output_path)
   455|             t_len = os.path.getsize(output_path)
   456|             await self.store.store_remote_media_thumbnail(
   457|                 server_name,
   458|                 media_id,
   459|                 file_id,
   460|                 t_width,
   461|                 t_height,
   462|                 t_type,
   463|                 t_method,
   464|                 t_len,
   465|             )
   466|             return output_path
   467|         return None
   468|     async def _generate_thumbnails(
   469|         self,
   470|         server_name: Optional[str],
   471|         media_id: str,
   472|         file_id: str,
   473|         media_type: str,
   474|         url_cache: bool = False,
   475|     ) -> Optional[dict]:
   476|         """Generate and store thumbnails for an image.
   477|         Args:
   478|             server_name: The server name if remote media, else None if local
   479|             media_id: The media ID of the content. (This is the same as
   480|                 the file_id for local content)
   481|             file_id: Local file ID
   482|             media_type: The content type of the file
   483|             url_cache: If we are thumbnailing images downloaded for the URL cache,
   484|                 used exclusively by the url previewer
   485|         Returns:
   486|             Dict with "width" and "height" keys of original image or None if the
   487|             media cannot be thumbnailed.
   488|         """
   489|         requirements = self._get_thumbnail_requirements(media_type)
   490|         if not requirements:
   491|             return None
   492|         input_path = await self.media_storage.ensure_media_is_in_local_cache(
   493|             FileInfo(server_name, file_id, url_cache=url_cache)
   494|         )
   495|         try:
   496|             thumbnailer = Thumbnailer(input_path)
   497|         except ThumbnailError as e:
   498|             logger.warning(
   499|                 "Unable to generate thumbnails for remote media %s from %s of type %s: %s",
   500|                 media_id,
   501|                 server_name,
   502|                 media_type,
   503|                 e,
   504|             )
   505|             return None
   506|         m_width = thumbnailer.width
   507|         m_height = thumbnailer.height
   508|         if m_width * m_height >= self.max_image_pixels:
   509|             logger.info(
   510|                 "Image too large to thumbnail %r x %r > %r",
   511|                 m_width,
   512|                 m_height,
   513|                 self.max_image_pixels,
   514|             )
   515|             return None
   516|         if thumbnailer.transpose_method is not None:
   517|             m_width, m_height = await defer_to_thread(
   518|                 self.hs.get_reactor(), thumbnailer.transpose
   519|             )
   520|         thumbnails = {}  # type: Dict[Tuple[int, int, str], str]
   521|         for r_width, r_height, r_method, r_type in requirements:
   522|             if r_method == "crop":
   523|                 thumbnails.setdefault((r_width, r_height, r_type), r_method)
   524|             elif r_method == "scale":
   525|                 t_width, t_height = thumbnailer.aspect(r_width, r_height)


# ====================================================================
# FILE: synapse/rest/media/v1/media_storage.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 76-163 ---
    76|             finished_called[0] = True
    77|         try:
    78|             with open(fname, "wb") as f:
    79|                 yield f, fname, finish
    80|         except Exception:
    81|             try:
    82|                 os.remove(fname)
    83|             except Exception:
    84|                 pass
    85|             raise
    86|         if not finished_called:
    87|             raise Exception("Finished callback not called")
    88|     async def fetch_media(self, file_info: FileInfo) -> Optional[Responder]:
    89|         """Attempts to fetch media described by file_info from the local cache
    90|         and configured storage providers.
    91|         Args:
    92|             file_info
    93|         Returns:
    94|             Returns a Responder if the file was found, otherwise None.
    95|         """
    96|         paths = [self._file_info_to_path(file_info)]
    97|         if file_info.thumbnail and file_info.server_name:
    98|             paths.append(
    99|                 self.filepaths.remote_media_thumbnail_rel_legacy(
   100|                     server_name=file_info.server_name,
   101|                     file_id=file_info.file_id,
   102|                     width=file_info.thumbnail_width,
   103|                     height=file_info.thumbnail_height,
   104|                     content_type=file_info.thumbnail_type,
   105|                 )
   106|             )
   107|         for path in paths:
   108|             local_path = os.path.join(self.local_media_directory, path)
   109|             if os.path.exists(local_path):
   110|                 logger.debug("responding with local file %s", local_path)
   111|                 return FileResponder(open(local_path, "rb"))
   112|             logger.debug("local file %s did not exist", local_path)
   113|         for provider in self.storage_providers:
   114|             for path in paths:
   115|                 res = await provider.fetch(path, file_info)  # type: Any
   116|                 if res:
   117|                     logger.debug("Streaming %s from %s", path, provider)
   118|                     return res
   119|                 logger.debug("%s not found on %s", path, provider)
   120|         return None
   121|     async def ensure_media_is_in_local_cache(self, file_info: FileInfo) -> str:
   122|         """Ensures that the given file is in the local cache. Attempts to
   123|         download it from storage providers if it isn't.
   124|         Args:
   125|             file_info
   126|         Returns:
   127|             Full path to local file
   128|         """
   129|         path = self._file_info_to_path(file_info)
   130|         local_path = os.path.join(self.local_media_directory, path)
   131|         if os.path.exists(local_path):
   132|             return local_path
   133|         if file_info.thumbnail and file_info.server_name:
   134|             legacy_path = self.filepaths.remote_media_thumbnail_rel_legacy(
   135|                 server_name=file_info.server_name,
   136|                 file_id=file_info.file_id,
   137|                 width=file_info.thumbnail_width,
   138|                 height=file_info.thumbnail_height,
   139|                 content_type=file_info.thumbnail_type,
   140|             )
   141|             legacy_local_path = os.path.join(self.local_media_directory, legacy_path)
   142|             if os.path.exists(legacy_local_path):
   143|                 return legacy_local_path
   144|         dirname = os.path.dirname(local_path)
   145|         if not os.path.exists(dirname):
   146|             os.makedirs(dirname)
   147|         for provider in self.storage_providers:
   148|             res = await provider.fetch(path, file_info)  # type: Any
   149|             if res:
   150|                 with res:
   151|                     consumer = BackgroundFileConsumer(
   152|                         open(local_path, "wb"), self.hs.get_reactor()
   153|                     )
   154|                     await res.write_to_consumer(consumer)
   155|                     await consumer.wait()
   156|                 return local_path
   157|         raise Exception("file could not be found")
   158|     def _file_info_to_path(self, file_info: FileInfo) -> str:
   159|         """Converts file_info into a relative path.
   160|         The path is suitable for storing files under a directory, e.g. used to
   161|         store files on local FS under the base media repository directory.
   162|         """
   163|         if file_info.url_cache:


# ====================================================================
# FILE: synapse/rest/media/v1/preview_url_resource.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 43-83 ---
    43|         "http://twitter.com/*/status/*",
    44|         "http://*.twitter.com/*/status/*",
    45|         "http://twitter.com/*/moments/*",
    46|         "http://*.twitter.com/*/moments/*",
    47|     ],
    48| }
    49| _oembed_patterns = {}
    50| for endpoint, globs in _oembed_globs.items():
    51|     for glob in globs:
    52|         results = urlparse.urlparse(glob)
    53|         if results.scheme not in {"http", "https"}:
    54|             raise ValueError("Insecure oEmbed glob scheme: %s" % (results.scheme,))
    55|         pattern = urlparse.urlunparse(
    56|             [
    57|                 results.scheme,
    58|                 re.escape(results.netloc).replace("\\*", "[a-zA-Z0-9_-]+"),
    59|             ]
    60|             + [re.escape(part).replace("\\*", ".+") for part in results[2:]]
    61|         )
    62|         _oembed_patterns[re.compile(pattern)] = endpoint
    63| @attr.s(slots=True)
    64| class OEmbedResult:
    65|     html = attr.ib(type=Optional[str])
    66|     url = attr.ib(type=Optional[str])
    67|     title = attr.ib(type=Optional[str])
    68|     cache_age = attr.ib(type=int)
    69| class OEmbedError(Exception):
    70|     """An error occurred processing the oEmbed object."""
    71| class PreviewUrlResource(DirectServeJsonResource):
    72|     isLeaf = True
    73|     def __init__(self, hs, media_repo, media_storage):
    74|         super().__init__()
    75|         self.auth = hs.get_auth()
    76|         self.clock = hs.get_clock()
    77|         self.filepaths = media_repo.filepaths
    78|         self.max_spider_size = hs.config.max_spider_size
    79|         self.server_name = hs.hostname
    80|         self.store = hs.get_datastore()
    81|         self.client = SimpleHttpClient(
    82|             hs,
    83|             treq_args={"browser_like_redirects": True},

# --- HUNK 2: Lines 282-325 ---
   282|             cache_age = result.get("cache_age")
   283|             if cache_age:
   284|                 cache_age = int(cache_age)
   285|             oembed_result = OEmbedResult(None, None, result.get("title"), cache_age)
   286|             if oembed_type == "rich":
   287|                 oembed_result.html = result.get("html")
   288|                 return oembed_result
   289|             if oembed_type == "photo":
   290|                 oembed_result.url = result.get("url")
   291|                 return oembed_result
   292|             if "thumbnail_url" in result:
   293|                 oembed_result.url = result.get("thumbnail_url")
   294|                 return oembed_result
   295|             raise OEmbedError("Incompatible oEmbed information.")
   296|         except OEmbedError as e:
   297|             logger.warning("Error parsing oEmbed metadata from %s: %r", url, e)
   298|             raise
   299|         except Exception as e:
   300|             logger.warning("Error downloading oEmbed metadata from %s: %r", url, e)
   301|             raise OEmbedError() from e
   302|     async def _download_url(self, url: str, user):
   303|         file_id = datetime.date.today().isoformat() + "_" + random_string(16)
   304|         file_info = FileInfo(server_name=None, file_id=file_id, url_cache=True)
   305|         url_to_download = url  # type: Optional[str]
   306|         oembed_url = self._get_oembed_url(url)
   307|         if oembed_url:
   308|             try:
   309|                 oembed_result = await self._get_oembed_content(oembed_url, url)
   310|                 if oembed_result.url:
   311|                     url_to_download = oembed_result.url
   312|                 elif oembed_result.html:
   313|                     url_to_download = None
   314|             except OEmbedError:
   315|                 pass
   316|         if url_to_download:
   317|             with self.media_storage.store_into_file(file_info) as (f, fname, finish):
   318|                 try:
   319|                     logger.debug("Trying to get preview for url '%s'", url_to_download)
   320|                     length, headers, uri, code = await self.client.get_file(
   321|                         url_to_download,
   322|                         output_stream=f,
   323|                         max_size=self.max_spider_size,
   324|                         headers={"Accept-Language": self.url_preview_accept_language},
   325|                     )

# --- HUNK 3: Lines 329-375 ---
   329|                     raise SynapseError(
   330|                         502,
   331|                         "DNS resolution failure during URL preview generation",
   332|                         Codes.UNKNOWN,
   333|                     )
   334|                 except Exception as e:
   335|                     logger.warning("Error downloading %s: %r", url_to_download, e)
   336|                     raise SynapseError(
   337|                         500,
   338|                         "Failed to download content: %s"
   339|                         % (traceback.format_exception_only(sys.exc_info()[0], e),),
   340|                         Codes.UNKNOWN,
   341|                     )
   342|                 await finish()
   343|                 if b"Content-Type" in headers:
   344|                     media_type = headers[b"Content-Type"][0].decode("ascii")
   345|                 else:
   346|                     media_type = "application/octet-stream"
   347|                 download_name = get_filename_from_headers(headers)
   348|                 expires = ONE_HOUR
   349|                 etag = (
   350|                     headers[b"ETag"][0].decode("ascii") if b"ETag" in headers else None
   351|                 )
   352|         else:
   353|             assert oembed_result.html is not None
   354|             assert oembed_url is not None
   355|             html_bytes = oembed_result.html.encode("utf-8")
   356|             with self.media_storage.store_into_file(file_info) as (f, fname, finish):
   357|                 f.write(html_bytes)
   358|                 await finish()
   359|             media_type = "text/html"
   360|             download_name = oembed_result.title
   361|             length = len(html_bytes)
   362|             expires = oembed_result.cache_age or ONE_HOUR
   363|             uri = oembed_url
   364|             code = 200
   365|             etag = None
   366|         try:
   367|             time_now_ms = self.clock.time_msec()
   368|             await self.store.store_local_media(
   369|                 media_id=file_id,
   370|                 media_type=media_type,
   371|                 time_now_ms=time_now_ms,
   372|                 upload_name=download_name,
   373|                 media_length=length,
   374|                 user_id=user,
   375|                 url_cache=url,


# ====================================================================
# FILE: synapse/rest/media/v1/thumbnail_resource.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-22 ---
     1| import logging
     2| from synapse.api.errors import SynapseError
     3| from synapse.http.server import DirectServeJsonResource, set_cors_headers
     4| from synapse.http.servlet import parse_integer, parse_string
     5| from ._base import (
     6|     FileInfo,
     7|     parse_media_id,
     8|     respond_404,
     9|     respond_with_file,
    10|     respond_with_responder,
    11| )
    12| logger = logging.getLogger(__name__)
    13| class ThumbnailResource(DirectServeJsonResource):
    14|     isLeaf = True
    15|     def __init__(self, hs, media_repo, media_storage):
    16|         super().__init__()
    17|         self.store = hs.get_datastore()
    18|         self.media_repo = media_repo
    19|         self.media_storage = media_storage
    20|         self.dynamic_thumbnails = hs.config.dynamic_thumbnails
    21|         self.server_name = hs.hostname
    22|     async def _async_render_GET(self, request):

# --- HUNK 2: Lines 115-155 ---
   115|                 )
   116|                 t_type = file_info.thumbnail_type
   117|                 t_length = info["thumbnail_length"]
   118|                 responder = await self.media_storage.fetch_media(file_info)
   119|                 if responder:
   120|                     await respond_with_responder(request, responder, t_type, t_length)
   121|                     return
   122|         logger.debug("We don't have a thumbnail of that size. Generating")
   123|         file_path = await self.media_repo.generate_local_exact_thumbnail(
   124|             media_id,
   125|             desired_width,
   126|             desired_height,
   127|             desired_method,
   128|             desired_type,
   129|             url_cache=media_info["url_cache"],
   130|         )
   131|         if file_path:
   132|             await respond_with_file(request, desired_type, file_path)
   133|         else:
   134|             logger.warning("Failed to generate thumbnail")
   135|             raise SynapseError(400, "Failed to generate thumbnail.")
   136|     async def _select_or_generate_remote_thumbnail(
   137|         self,
   138|         request,
   139|         server_name,
   140|         media_id,
   141|         desired_width,
   142|         desired_height,
   143|         desired_method,
   144|         desired_type,
   145|     ):
   146|         media_info = await self.media_repo.get_remote_media_info(server_name, media_id)
   147|         thumbnail_infos = await self.store.get_remote_media_thumbnails(
   148|             server_name, media_id
   149|         )
   150|         file_id = media_info["filesystem_id"]
   151|         for info in thumbnail_infos:
   152|             t_w = info["thumbnail_width"] == desired_width
   153|             t_h = info["thumbnail_height"] == desired_height
   154|             t_method = info["thumbnail_method"] == desired_method
   155|             t_type = info["thumbnail_type"] == desired_type

# --- HUNK 3: Lines 166-206 ---
   166|                 t_type = file_info.thumbnail_type
   167|                 t_length = info["thumbnail_length"]
   168|                 responder = await self.media_storage.fetch_media(file_info)
   169|                 if responder:
   170|                     await respond_with_responder(request, responder, t_type, t_length)
   171|                     return
   172|         logger.debug("We don't have a thumbnail of that size. Generating")
   173|         file_path = await self.media_repo.generate_remote_exact_thumbnail(
   174|             server_name,
   175|             file_id,
   176|             media_id,
   177|             desired_width,
   178|             desired_height,
   179|             desired_method,
   180|             desired_type,
   181|         )
   182|         if file_path:
   183|             await respond_with_file(request, desired_type, file_path)
   184|         else:
   185|             logger.warning("Failed to generate thumbnail")
   186|             raise SynapseError(400, "Failed to generate thumbnail.")
   187|     async def _respond_remote_thumbnail(
   188|         self, request, server_name, media_id, width, height, method, m_type
   189|     ):
   190|         media_info = await self.media_repo.get_remote_media_info(server_name, media_id)
   191|         thumbnail_infos = await self.store.get_remote_media_thumbnails(
   192|             server_name, media_id
   193|         )
   194|         if thumbnail_infos:
   195|             thumbnail_info = self._select_thumbnail(
   196|                 width, height, method, m_type, thumbnail_infos
   197|             )
   198|             file_info = FileInfo(
   199|                 server_name=server_name,
   200|                 file_id=media_info["filesystem_id"],
   201|                 thumbnail=True,
   202|                 thumbnail_width=thumbnail_info["thumbnail_width"],
   203|                 thumbnail_height=thumbnail_info["thumbnail_height"],
   204|                 thumbnail_type=thumbnail_info["thumbnail_type"],
   205|                 thumbnail_method=thumbnail_info["thumbnail_method"],
   206|             )


# ====================================================================
# FILE: synapse/rest/media/v1/thumbnailer.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-97 ---
     1| import logging
     2| from io import BytesIO
     3| from PIL import Image
     4| logger = logging.getLogger(__name__)
     5| EXIF_ORIENTATION_TAG = 0x0112
     6| EXIF_TRANSPOSE_MAPPINGS = {
     7|     2: Image.FLIP_LEFT_RIGHT,
     8|     3: Image.ROTATE_180,
     9|     4: Image.FLIP_TOP_BOTTOM,
    10|     5: Image.TRANSPOSE,
    11|     6: Image.ROTATE_270,
    12|     7: Image.TRANSVERSE,
    13|     8: Image.ROTATE_90,
    14| }
    15| class ThumbnailError(Exception):
    16|     """An error occurred generating a thumbnail."""
    17| class Thumbnailer:
    18|     FORMATS = {"image/jpeg": "JPEG", "image/png": "PNG"}
    19|     def __init__(self, input_path):
    20|         try:
    21|             self.image = Image.open(input_path)
    22|         except OSError as e:
    23|             raise ThumbnailError from e
    24|         self.width, self.height = self.image.size
    25|         self.transpose_method = None
    26|         try:
    27|             image_exif = self.image._getexif()
    28|             if image_exif is not None:
    29|                 image_orientation = image_exif.get(EXIF_ORIENTATION_TAG)
    30|                 self.transpose_method = EXIF_TRANSPOSE_MAPPINGS.get(image_orientation)
    31|         except Exception as e:
    32|             logger.info("Error parsing image EXIF information: %s", e)
    33|     def transpose(self):
    34|         """Transpose the image using its EXIF Orientation tag
    35|         Returns:
    36|             Tuple[int, int]: (width, height) containing the new image size in pixels.
    37|         """
    38|         if self.transpose_method is not None:
    39|             self.image = self.image.transpose(self.transpose_method)
    40|             self.width, self.height = self.image.size
    41|             self.transpose_method = None
    42|             self.image.info["exif"] = None
    43|         return self.image.size
    44|     def aspect(self, max_width, max_height):
    45|         """Calculate the largest size that preserves aspect ratio which
    46|         fits within the given rectangle::
    47|             (w_in / h_in) = (w_out / h_out)
    48|             w_out = min(w_max, h_max * (w_in / h_in))
    49|             h_out = min(h_max, w_max * (h_in / w_in))
    50|         Args:
    51|             max_width: The largest possible width.
    52|             max_height: The largest possible height.
    53|         """
    54|         if max_width * self.height < max_height * self.width:
    55|             return max_width, (max_width * self.height) // self.width
    56|         else:
    57|             return (max_height * self.width) // self.height, max_height
    58|     def _resize(self, width, height):
    59|         if self.image.mode in ["1", "P"]:
    60|             self.image = self.image.convert("RGB")
    61|         return self.image.resize((width, height), Image.ANTIALIAS)
    62|     def scale(self, width, height, output_type):
    63|         """Rescales the image to the given dimensions.
    64|         Returns:
    65|             BytesIO: the bytes of the encoded image ready to be written to disk
    66|         """
    67|         scaled = self._resize(width, height)
    68|         return self._encode_image(scaled, output_type)
    69|     def crop(self, width, height, output_type):
    70|         """Rescales and crops the image to the given dimensions preserving
    71|         aspect::
    72|             (w_in / h_in) = (w_scaled / h_scaled)
    73|             w_scaled = max(w_out, h_out * (w_in / h_in))
    74|             h_scaled = max(h_out, w_out * (h_in / w_in))
    75|         Args:
    76|             max_width: The largest possible width.
    77|             max_height: The largest possible height.
    78|         Returns:
    79|             BytesIO: the bytes of the encoded image ready to be written to disk
    80|         """
    81|         if width * self.height > height * self.width:
    82|             scaled_height = (width * self.height) // self.width
    83|             scaled_image = self._resize(width, scaled_height)
    84|             crop_top = (scaled_height - height) // 2
    85|             crop_bottom = height + crop_top
    86|             cropped = scaled_image.crop((0, crop_top, width, crop_bottom))
    87|         else:
    88|             scaled_width = (height * self.width) // self.height
    89|             scaled_image = self._resize(scaled_width, height)
    90|             crop_left = (scaled_width - width) // 2
    91|             crop_right = width + crop_left
    92|             cropped = scaled_image.crop((crop_left, 0, crop_right, height))
    93|         return self._encode_image(cropped, output_type)
    94|     def _encode_image(self, output_image, output_type):
    95|         output_bytes_io = BytesIO()
    96|         fmt = self.FORMATS[output_type]
    97|         if fmt == "JPEG":


# ====================================================================
# FILE: synapse/rest/media/v1/upload_resource.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 19-50 ---
    19|         respond_with_json(request, 200, {}, send_cors=True)
    20|     async def _async_render_POST(self, request):
    21|         requester = await self.auth.get_user_by_req(request)
    22|         content_length = request.getHeader(b"Content-Length").decode("ascii")
    23|         if content_length is None:
    24|             raise SynapseError(msg="Request must specify a Content-Length", code=400)
    25|         if int(content_length) > self.max_upload_size:
    26|             raise SynapseError(
    27|                 msg="Upload request body is too large",
    28|                 code=413,
    29|                 errcode=Codes.TOO_LARGE,
    30|             )
    31|         upload_name = parse_string(request, b"filename", encoding=None)
    32|         if upload_name:
    33|             try:
    34|                 upload_name = upload_name.decode("utf8")
    35|             except UnicodeDecodeError:
    36|                 raise SynapseError(
    37|                     msg="Invalid UTF-8 filename parameter: %r" % (upload_name), code=400
    38|                 )
    39|         else:
    40|             upload_name = None
    41|         headers = request.requestHeaders
    42|         if headers.hasHeader(b"Content-Type"):
    43|             media_type = headers.getRawHeaders(b"Content-Type")[0].decode("ascii")
    44|         else:
    45|             raise SynapseError(msg="Upload request missing 'Content-Type'", code=400)
    46|         content_uri = await self.media_repo.create_content(
    47|             media_type, upload_name, request.content, content_length, requester.user
    48|         )
    49|         logger.info("Uploaded content with URI %r", content_uri)
    50|         respond_with_json(request, 200, {"content_uri": content_uri}, send_cors=True)


# ====================================================================
# FILE: synapse/rest/saml2/response_resource.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-13 ---
     1| from synapse.http.server import DirectServeHtmlResource
     2| class SAML2ResponseResource(DirectServeHtmlResource):
     3|     """A Twisted web resource which handles the SAML response"""
     4|     isLeaf = 1
     5|     def __init__(self, hs):
     6|         super().__init__()
     7|         self._saml_handler = hs.get_saml_handler()
     8|     async def _async_render_GET(self, request):
     9|         self._saml_handler._render_error(
    10|             request, "unexpected_get", "Unexpected GET request on /saml2/authn_response"
    11|         )
    12|     async def _async_render_POST(self, request):
    13|         await self._saml_handler.handle_saml_response(request)


# ====================================================================
# FILE: synapse/rest/synapse/client/password_reset.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-85 ---
     1| import logging
     2| from typing import TYPE_CHECKING, Tuple
     3| from twisted.web.http import Request
     4| from synapse.api.errors import ThreepidValidationError
     5| from synapse.config.emailconfig import ThreepidBehaviour
     6| from synapse.http.server import DirectServeHtmlResource
     7| from synapse.http.servlet import parse_string
     8| from synapse.util.stringutils import assert_valid_client_secret
     9| if TYPE_CHECKING:
    10|     from synapse.server import HomeServer
    11| logger = logging.getLogger(__name__)
    12| class PasswordResetSubmitTokenResource(DirectServeHtmlResource):
    13|     """Handles 3PID validation token submission
    14|     This resource gets mounted under /_synapse/client/password_reset/email/submit_token
    15|     """
    16|     isLeaf = 1
    17|     def __init__(self, hs: "HomeServer"):
    18|         """
    19|         Args:
    20|             hs: server
    21|         """
    22|         super().__init__()
    23|         self.clock = hs.get_clock()
    24|         self.store = hs.get_datastore()
    25|         self._local_threepid_handling_disabled_due_to_email_config = (
    26|             hs.config.local_threepid_handling_disabled_due_to_email_config
    27|         )
    28|         self._confirmation_email_template = (
    29|             hs.config.email_password_reset_template_confirmation_html
    30|         )
    31|         self._email_password_reset_template_success_html = (
    32|             hs.config.email_password_reset_template_success_html_content
    33|         )
    34|         self._failure_email_template = (
    35|             hs.config.email_password_reset_template_failure_html
    36|         )
    37|         assert hs.config.threepid_behaviour_email == ThreepidBehaviour.LOCAL
    38|     async def _async_render_GET(self, request: Request) -> Tuple[int, bytes]:
    39|         sid = parse_string(request, "sid", required=True)
    40|         token = parse_string(request, "token", required=True)
    41|         client_secret = parse_string(request, "client_secret", required=True)
    42|         assert_valid_client_secret(client_secret)
    43|         template_vars = {
    44|             "sid": sid,
    45|             "token": token,
    46|             "client_secret": client_secret,
    47|         }
    48|         return (
    49|             200,
    50|             self._confirmation_email_template.render(**template_vars).encode("utf-8"),
    51|         )
    52|     async def _async_render_POST(self, request: Request) -> Tuple[int, bytes]:
    53|         sid = parse_string(request, "sid", required=True)
    54|         token = parse_string(request, "token", required=True)
    55|         client_secret = parse_string(request, "client_secret", required=True)
    56|         try:
    57|             next_link = await self.store.validate_threepid_session(
    58|                 sid, client_secret, token, self.clock.time_msec()
    59|             )
    60|             if next_link:
    61|                 if next_link.startswith("file:///"):
    62|                     logger.warning(
    63|                         "Not redirecting to next_link as it is a local file: address"
    64|                     )
    65|                 else:
    66|                     next_link_bytes = next_link.encode("utf-8")
    67|                     request.setHeader("Location", next_link_bytes)
    68|                     return (
    69|                         302,
    70|                         (
    71|                             b'You are being redirected to <a src="%s">%s</a>.'
    72|                             % (next_link_bytes, next_link_bytes)
    73|                         ),
    74|                     )
    75|             html_bytes = self._email_password_reset_template_success_html.encode(
    76|                 "utf-8"
    77|             )
    78|             status_code = 200
    79|         except ThreepidValidationError as e:
    80|             status_code = e.code
    81|             template_vars = {"failure_reason": e.msg}
    82|             html_bytes = self._failure_email_template.render(**template_vars).encode(
    83|                 "utf-8"
    84|             )
    85|         return status_code, html_bytes


# ====================================================================
# FILE: synapse/state/__init__.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-57 ---
     1| import heapq
     2| import logging
     3| from collections import defaultdict, namedtuple
     4| from typing import (
     5|     Any,
     6|     Awaitable,
     7|     Callable,
     8|     DefaultDict,
     9|     Dict,
    10|     Iterable,
    11|     List,
    12|     Optional,
    13|     Sequence,
    14|     Set,
    15|     Tuple,
    16|     Union,
    17|     overload,
    18| )
    19| import attr
    20| from frozendict import frozendict
    21| from prometheus_client import Counter, Histogram
    22| from typing_extensions import Literal
    23| from synapse.api.constants import EventTypes
    24| from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, StateResolutionVersions
    25| from synapse.events import EventBase
    26| from synapse.events.snapshot import EventContext
    27| from synapse.logging.context import ContextResourceUsage
    28| from synapse.logging.utils import log_function
    29| from synapse.state import v1, v2
    30| from synapse.storage.databases.main.events_worker import EventRedactBehaviour
    31| from synapse.storage.roommember import ProfileInfo
    32| from synapse.types import Collection, StateMap
    33| from synapse.util.async_helpers import Linearizer
    34| from synapse.util.caches.expiringcache import ExpiringCache
    35| from synapse.util.metrics import Measure, measure_func
    36| logger = logging.getLogger(__name__)
    37| metrics_logger = logging.getLogger("synapse.state.metrics")
    38| state_groups_histogram = Histogram(
    39|     "synapse_state_number_state_groups_in_resolution",
    40|     "Number of state groups used when performing a state resolution",
    41|     buckets=(1, 2, 3, 5, 7, 10, 15, 20, 50, 100, 200, 500, "+Inf"),
    42| )
    43| KeyStateTuple = namedtuple("KeyStateTuple", ("context", "type", "state_key"))
    44| EVICTION_TIMEOUT_SECONDS = 60 * 60
    45| _NEXT_STATE_ID = 1
    46| POWER_KEY = (EventTypes.PowerLevels, "")
    47| def _gen_state_id():
    48|     global _NEXT_STATE_ID
    49|     s = "X%d" % (_NEXT_STATE_ID,)
    50|     _NEXT_STATE_ID += 1
    51|     return s
    52| class _StateCacheEntry:
    53|     __slots__ = ["state", "state_group", "state_id", "prev_group", "delta_ids"]
    54|     def __init__(
    55|         self,
    56|         state: StateMap[str],
    57|         state_group: Optional[int],

# --- HUNK 2: Lines 302-551 ---
   302|             room_id,
   303|             room_version,
   304|             state_groups_ids,
   305|             None,
   306|             state_res_store=StateResolutionStore(self.store),
   307|         )
   308|         return result
   309|     async def resolve_events(
   310|         self,
   311|         room_version: str,
   312|         state_sets: Collection[Iterable[EventBase]],
   313|         event: EventBase,
   314|     ) -> StateMap[EventBase]:
   315|         logger.info(
   316|             "Resolving state for %s with %d groups", event.room_id, len(state_sets)
   317|         )
   318|         state_set_ids = [
   319|             {(ev.type, ev.state_key): ev.event_id for ev in st} for st in state_sets
   320|         ]
   321|         state_map = {ev.event_id: ev for st in state_sets for ev in st}
   322|         new_state = await self._state_resolution_handler.resolve_events_with_store(
   323|             event.room_id,
   324|             room_version,
   325|             state_set_ids,
   326|             event_map=state_map,
   327|             state_res_store=StateResolutionStore(self.store),
   328|         )
   329|         return {key: state_map[ev_id] for key, ev_id in new_state.items()}
   330| @attr.s(slots=True)
   331| class _StateResMetrics:
   332|     """Keeps track of some usage metrics about state res."""
   333|     cpu_time = attr.ib(type=float, default=0.0)
   334|     db_time = attr.ib(type=float, default=0.0)
   335|     db_events = attr.ib(type=int, default=0)
   336| _biggest_room_by_cpu_counter = Counter(
   337|     "synapse_state_res_cpu_for_biggest_room_seconds",
   338|     "CPU time spent performing state resolution for the single most expensive "
   339|     "room for state resolution",
   340| )
   341| _biggest_room_by_db_counter = Counter(
   342|     "synapse_state_res_db_for_biggest_room_seconds",
   343|     "Database time spent performing state resolution for the single most "
   344|     "expensive room for state resolution",
   345| )
   346| class StateResolutionHandler:
   347|     """Responsible for doing state conflict resolution.
   348|     Note that the storage layer depends on this handler, so all functions must
   349|     be storage-independent.
   350|     """
   351|     def __init__(self, hs):
   352|         self.clock = hs.get_clock()
   353|         self.resolve_linearizer = Linearizer(name="state_resolve_lock")
   354|         self._state_cache = ExpiringCache(
   355|             cache_name="state_cache",
   356|             clock=self.clock,
   357|             max_len=100000,
   358|             expiry_ms=EVICTION_TIMEOUT_SECONDS * 1000,
   359|             iterable=True,
   360|             reset_expiry_on_get=True,
   361|         )
   362|         self._state_res_metrics = defaultdict(
   363|             _StateResMetrics
   364|         )  # type: DefaultDict[str, _StateResMetrics]
   365|         self.clock.looping_call(self._report_metrics, 120 * 1000)
   366|     @log_function
   367|     async def resolve_state_groups(
   368|         self,
   369|         room_id: str,
   370|         room_version: str,
   371|         state_groups_ids: Dict[int, StateMap[str]],
   372|         event_map: Optional[Dict[str, EventBase]],
   373|         state_res_store: "StateResolutionStore",
   374|     ):
   375|         """Resolves conflicts between a set of state groups
   376|         Always generates a new state group (unless we hit the cache), so should
   377|         not be called for a single state group
   378|         Args:
   379|             room_id: room we are resolving for (used for logging and sanity checks)
   380|             room_version: version of the room
   381|             state_groups_ids:
   382|                 A map from state group id to the state in that state group
   383|                 (where 'state' is a map from state key to event id)
   384|             event_map:
   385|                 a dict from event_id to event, for any events that we happen to
   386|                 have in flight (eg, those currently being persisted). This will be
   387|                 used as a starting point fof finding the state we need; any missing
   388|                 events will be requested via state_res_store.
   389|                 If None, all events will be fetched via state_res_store.
   390|             state_res_store
   391|         Returns:
   392|             The resolved state
   393|         """
   394|         group_names = frozenset(state_groups_ids.keys())
   395|         with (await self.resolve_linearizer.queue(group_names)):
   396|             cache = self._state_cache.get(group_names, None)
   397|             if cache:
   398|                 return cache
   399|             logger.info(
   400|                 "Resolving state for %s with groups %s", room_id, list(group_names),
   401|             )
   402|             state_groups_histogram.observe(len(state_groups_ids))
   403|             new_state = await self.resolve_events_with_store(
   404|                 room_id,
   405|                 room_version,
   406|                 list(state_groups_ids.values()),
   407|                 event_map=event_map,
   408|                 state_res_store=state_res_store,
   409|             )
   410|             with Measure(self.clock, "state.create_group_ids"):
   411|                 cache = _make_state_cache_entry(new_state, state_groups_ids)
   412|             self._state_cache[group_names] = cache
   413|             return cache
   414|     async def resolve_events_with_store(
   415|         self,
   416|         room_id: str,
   417|         room_version: str,
   418|         state_sets: Sequence[StateMap[str]],
   419|         event_map: Optional[Dict[str, EventBase]],
   420|         state_res_store: "StateResolutionStore",
   421|     ) -> StateMap[str]:
   422|         """
   423|         Args:
   424|             room_id: the room we are working in
   425|             room_version: Version of the room
   426|             state_sets: List of dicts of (type, state_key) -> event_id,
   427|                 which are the different state groups to resolve.
   428|             event_map:
   429|                 a dict from event_id to event, for any events that we happen to
   430|                 have in flight (eg, those currently being persisted). This will be
   431|                 used as a starting point fof finding the state we need; any missing
   432|                 events will be requested via state_map_factory.
   433|                 If None, all events will be fetched via state_res_store.
   434|             state_res_store: a place to fetch events from
   435|         Returns:
   436|             a map from (type, state_key) to event_id.
   437|         """
   438|         try:
   439|             with Measure(self.clock, "state._resolve_events") as m:
   440|                 v = KNOWN_ROOM_VERSIONS[room_version]
   441|                 if v.state_res == StateResolutionVersions.V1:
   442|                     return await v1.resolve_events_with_store(
   443|                         room_id, state_sets, event_map, state_res_store.get_events
   444|                     )
   445|                 else:
   446|                     return await v2.resolve_events_with_store(
   447|                         self.clock,
   448|                         room_id,
   449|                         room_version,
   450|                         state_sets,
   451|                         event_map,
   452|                         state_res_store,
   453|                     )
   454|         finally:
   455|             self._record_state_res_metrics(room_id, m.get_resource_usage())
   456|     def _record_state_res_metrics(self, room_id: str, rusage: ContextResourceUsage):
   457|         room_metrics = self._state_res_metrics[room_id]
   458|         room_metrics.cpu_time += rusage.ru_utime + rusage.ru_stime
   459|         room_metrics.db_time += rusage.db_txn_duration_sec
   460|         room_metrics.db_events += rusage.evt_db_fetch_count
   461|     def _report_metrics(self):
   462|         if not self._state_res_metrics:
   463|             return
   464|         self._report_biggest(
   465|             lambda i: i.cpu_time, "CPU time", _biggest_room_by_cpu_counter,
   466|         )
   467|         self._report_biggest(
   468|             lambda i: i.db_time, "DB time", _biggest_room_by_db_counter,
   469|         )
   470|         self._state_res_metrics.clear()
   471|     def _report_biggest(
   472|         self,
   473|         extract_key: Callable[[_StateResMetrics], Any],
   474|         metric_name: str,
   475|         prometheus_counter_metric: Counter,
   476|     ) -> None:
   477|         """Report metrics on the biggest rooms for state res
   478|         Args:
   479|             extract_key: a callable which, given a _StateResMetrics, extracts a single
   480|                 metric to sort by.
   481|             metric_name: the name of the metric we have extracted, for the log line
   482|             prometheus_counter_metric: a prometheus metric recording the sum of the
   483|                 the extracted metric
   484|         """
   485|         n_to_log = 10
   486|         if not metrics_logger.isEnabledFor(logging.DEBUG):
   487|             n_to_log = 1
   488|         items = self._state_res_metrics.items()
   489|         biggest = heapq.nlargest(
   490|             n_to_log, items, key=lambda i: extract_key(i[1])
   491|         )  # type: List[Tuple[str, _StateResMetrics]]
   492|         metrics_logger.debug(
   493|             "%i biggest rooms for state-res by %s: %s",
   494|             len(biggest),
   495|             metric_name,
   496|             ["%s (%gs)" % (r, extract_key(m)) for (r, m) in biggest],
   497|         )
   498|         _, biggest_metrics = biggest[0]
   499|         prometheus_counter_metric.inc(extract_key(biggest_metrics))
   500| def _make_state_cache_entry(
   501|     new_state: StateMap[str], state_groups_ids: Dict[int, StateMap[str]]
   502| ) -> _StateCacheEntry:
   503|     """Given a resolved state, and a set of input state groups, pick one to base
   504|     a new state group on (if any), and return an appropriately-constructed
   505|     _StateCacheEntry.
   506|     Args:
   507|         new_state: resolved state map (mapping from (type, state_key) to event_id)
   508|         state_groups_ids:
   509|             map from state group id to the state in that state group (where
   510|             'state' is a map from state key to event id)
   511|     Returns:
   512|         The cache entry.
   513|     """
   514|     new_state_event_ids = set(new_state.values())
   515|     for sg, state in state_groups_ids.items():
   516|         if len(new_state_event_ids) != len(state):
   517|             continue
   518|         old_state_event_ids = set(state.values())
   519|         if new_state_event_ids == old_state_event_ids:
   520|             return _StateCacheEntry(state=new_state, state_group=sg)
   521|     prev_group = None
   522|     delta_ids = None
   523|     for old_group, old_state in state_groups_ids.items():
   524|         n_delta_ids = {k: v for k, v in new_state.items() if old_state.get(k) != v}
   525|         if not delta_ids or len(n_delta_ids) < len(delta_ids):
   526|             prev_group = old_group
   527|             delta_ids = n_delta_ids
   528|     return _StateCacheEntry(
   529|         state=new_state, state_group=None, prev_group=prev_group, delta_ids=delta_ids
   530|     )
   531| @attr.s(slots=True)
   532| class StateResolutionStore:
   533|     """Interface that allows state resolution algorithms to access the database
   534|     in well defined way.
   535|     Args:
   536|         store (DataStore)
   537|     """
   538|     store = attr.ib()
   539|     def get_events(
   540|         self, event_ids: Iterable[str], allow_rejected: bool = False
   541|     ) -> Awaitable[Dict[str, EventBase]]:
   542|         """Get events from the database
   543|         Args:
   544|             event_ids: The event_ids of the events to fetch
   545|             allow_rejected: If True return rejected events.
   546|         Returns:
   547|             An awaitable which resolves to a dict from event_id to event.
   548|         """
   549|         return self.store.get_events(
   550|             event_ids,
   551|             redact_behaviour=EventRedactBehaviour.AS_IS,


# ====================================================================
# FILE: synapse/storage/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 4-28 ---
     4| databases). The `DatabasePool` class represents connections to a single physical
     5| database. The `databases` are classes that talk directly to a `DatabasePool`
     6| instance and have associated schemas, background updates, etc. On top of those
     7| there are classes that provide high level interfaces that combine calls to
     8| multiple `databases`.
     9| There are also schemas that get applied to every database, regardless of the
    10| data stores associated with them (e.g. the schema version tables), which are
    11| stored in `synapse.storage.schema`.
    12| """
    13| from synapse.storage.databases import Databases
    14| from synapse.storage.databases.main import DataStore
    15| from synapse.storage.persist_events import EventsPersistenceStorage
    16| from synapse.storage.purge_events import PurgeEventsStorage
    17| from synapse.storage.state import StateGroupStorage
    18| __all__ = ["DataStores", "DataStore"]
    19| class Storage:
    20|     """The high level interfaces for talking to various storage layers.
    21|     """
    22|     def __init__(self, hs, stores: Databases):
    23|         self.main = stores.main
    24|         self.purge_events = PurgeEventsStorage(hs, stores)
    25|         self.state = StateGroupStorage(hs, stores)
    26|         self.persistence = None
    27|         if stores.persist_events:
    28|             self.persistence = EventsPersistenceStorage(hs, stores)


# ====================================================================
# FILE: synapse/storage/database.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 262-316 ---
   262|             time_now = monotonic_time()
   263|             time_then = self._previous_loop_ts
   264|             self._previous_loop_ts = time_now
   265|             duration = time_now - time_then
   266|             ratio = (curr - prev) / duration
   267|             top_three_counters = self._txn_perf_counters.interval(duration, limit=3)
   268|             perf_logger.debug(
   269|                 "Total database time: %.3f%% {%s}", ratio * 100, top_three_counters
   270|             )
   271|         self._clock.looping_call(loop, 10000)
   272|     def new_transaction(
   273|         self,
   274|         conn: Connection,
   275|         desc: str,
   276|         after_callbacks: List[_CallbackListEntry],
   277|         exception_callbacks: List[_CallbackListEntry],
   278|         func: "Callable[..., R]",
   279|         *args: Any,
   280|         **kwargs: Any
   281|     ) -> R:
   282|         """Start a new database transaction with the given connection.
   283|         Note: The given func may be called multiple times under certain
   284|         failure modes. This is normally fine when in a standard transaction,
   285|         but care must be taken if the connection is in `autocommit` mode that
   286|         the function will correctly handle being aborted and retried half way
   287|         through its execution.
   288|         Args:
   289|             conn
   290|             desc
   291|             after_callbacks
   292|             exception_callbacks
   293|             func
   294|             *args
   295|             **kwargs
   296|         """
   297|         start = monotonic_time()
   298|         txn_id = self._TXN_ID
   299|         self._TXN_ID = (self._TXN_ID + 1) % (MAX_TXN_ID)
   300|         name = "%s-%x" % (desc, txn_id)
   301|         transaction_logger.debug("[TXN START] {%s}", name)
   302|         try:
   303|             i = 0
   304|             N = 5
   305|             while True:
   306|                 cursor = LoggingTransaction(
   307|                     conn.cursor(),
   308|                     name,
   309|                     self.engine,
   310|                     after_callbacks,
   311|                     exception_callbacks,
   312|                 )
   313|                 try:
   314|                     r = func(cursor, *args, **kwargs)
   315|                     conn.commit()
   316|                     return r

# --- HUNK 2: Lines 338-469 ---
   338|                             except self.engine.module.Error as e1:
   339|                                 transaction_logger.warning(
   340|                                     "[TXN EROLL] {%s} %s", name, e1,
   341|                                 )
   342|                             continue
   343|                     raise
   344|                 finally:
   345|                     cursor.close()
   346|         except Exception as e:
   347|             transaction_logger.debug("[TXN FAIL] {%s} %s", name, e)
   348|             raise
   349|         finally:
   350|             end = monotonic_time()
   351|             duration = end - start
   352|             current_context().add_database_transaction(duration)
   353|             transaction_logger.debug("[TXN END] {%s} %f sec", name, duration)
   354|             self._current_txn_total_time += duration
   355|             self._txn_perf_counters.update(desc, duration)
   356|             sql_txn_timer.labels(desc).observe(duration)
   357|     async def runInteraction(
   358|         self,
   359|         desc: str,
   360|         func: "Callable[..., R]",
   361|         *args: Any,
   362|         db_autocommit: bool = False,
   363|         **kwargs: Any
   364|     ) -> R:
   365|         """Starts a transaction on the database and runs a given function
   366|         Arguments:
   367|             desc: description of the transaction, for logging and metrics
   368|             func: callback function, which will be called with a
   369|                 database transaction (twisted.enterprise.adbapi.Transaction) as
   370|                 its first argument, followed by `args` and `kwargs`.
   371|             db_autocommit: Whether to run the function in "autocommit" mode,
   372|                 i.e. outside of a transaction. This is useful for transactions
   373|                 that are only a single query.
   374|                 Currently, this is only implemented for Postgres. SQLite will still
   375|                 run the function inside a transaction.
   376|                 WARNING: This means that if func fails half way through then
   377|                 the changes will *not* be rolled back. `func` may also get
   378|                 called multiple times if the transaction is retried, so must
   379|                 correctly handle that case.
   380|             args: positional args to pass to `func`
   381|             kwargs: named args to pass to `func`
   382|         Returns:
   383|             The result of func
   384|         """
   385|         after_callbacks = []  # type: List[_CallbackListEntry]
   386|         exception_callbacks = []  # type: List[_CallbackListEntry]
   387|         if not current_context():
   388|             logger.warning("Starting db txn '%s' from sentinel context", desc)
   389|         try:
   390|             result = await self.runWithConnection(
   391|                 self.new_transaction,
   392|                 desc,
   393|                 after_callbacks,
   394|                 exception_callbacks,
   395|                 func,
   396|                 *args,
   397|                 db_autocommit=db_autocommit,
   398|                 **kwargs
   399|             )
   400|             for after_callback, after_args, after_kwargs in after_callbacks:
   401|                 after_callback(*after_args, **after_kwargs)
   402|         except:  # noqa: E722, as we reraise the exception this is fine.
   403|             for after_callback, after_args, after_kwargs in exception_callbacks:
   404|                 after_callback(*after_args, **after_kwargs)
   405|             raise
   406|         return cast(R, result)
   407|     async def runWithConnection(
   408|         self,
   409|         func: "Callable[..., R]",
   410|         *args: Any,
   411|         db_autocommit: bool = False,
   412|         **kwargs: Any
   413|     ) -> R:
   414|         """Wraps the .runWithConnection() method on the underlying db_pool.
   415|         Arguments:
   416|             func: callback function, which will be called with a
   417|                 database connection (twisted.enterprise.adbapi.Connection) as
   418|                 its first argument, followed by `args` and `kwargs`.
   419|             args: positional args to pass to `func`
   420|             db_autocommit: Whether to run the function in "autocommit" mode,
   421|                 i.e. outside of a transaction. This is useful for transaction
   422|                 that are only a single query. Currently only affects postgres.
   423|             kwargs: named args to pass to `func`
   424|         Returns:
   425|             The result of func
   426|         """
   427|         parent_context = current_context()  # type: Optional[LoggingContextOrSentinel]
   428|         if not parent_context:
   429|             logger.warning(
   430|                 "Starting db connection from sentinel context: metrics will be lost"
   431|             )
   432|             parent_context = None
   433|         start_time = monotonic_time()
   434|         def inner_func(conn, *args, **kwargs):
   435|             assert not self.engine.in_transaction(conn)
   436|             with LoggingContext("runWithConnection", parent_context) as context:
   437|                 sched_duration_sec = monotonic_time() - start_time
   438|                 sql_scheduling_timer.observe(sched_duration_sec)
   439|                 context.add_database_scheduled(sched_duration_sec)
   440|                 if self.engine.is_connection_closed(conn):
   441|                     logger.debug("Reconnecting closed database connection")
   442|                     conn.reconnect()
   443|                 try:
   444|                     if db_autocommit:
   445|                         self.engine.attempt_to_set_autocommit(conn, True)
   446|                     return func(conn, *args, **kwargs)
   447|                 finally:
   448|                     if db_autocommit:
   449|                         self.engine.attempt_to_set_autocommit(conn, False)
   450|         return await make_deferred_yieldable(
   451|             self._db_pool.runWithConnection(inner_func, *args, **kwargs)
   452|         )
   453|     @staticmethod
   454|     def cursor_to_dict(cursor: Cursor) -> List[Dict[str, Any]]:
   455|         """Converts a SQL cursor into an list of dicts.
   456|         Args:
   457|             cursor: The DBAPI cursor which has executed a query.
   458|         Returns:
   459|             A list of dicts where the key is the column header.
   460|         """
   461|         col_headers = [intern(str(column[0])) for column in cursor.description]
   462|         results = [dict(zip(col_headers, row)) for row in cursor]
   463|         return results
   464|     @overload
   465|     async def execute(
   466|         self, desc: str, decoder: Literal[None], query: str, *args: Any
   467|     ) -> List[Tuple[Any, ...]]:
   468|         ...
   469|     @overload

# --- HUNK 3: Lines 728-795 ---
   728|         if not values:
   729|             latter = "NOTHING"
   730|         else:
   731|             allvalues.update(values)
   732|             latter = "UPDATE SET " + ", ".join(k + "=EXCLUDED." + k for k in values)
   733|         sql = ("INSERT INTO %s (%s) VALUES (%s) ON CONFLICT (%s) DO %s") % (
   734|             table,
   735|             ", ".join(k for k in allvalues),
   736|             ", ".join("?" for _ in allvalues),
   737|             ", ".join(k for k in keyvalues),
   738|             latter,
   739|         )
   740|         txn.execute(sql, list(allvalues.values()))
   741|     def simple_upsert_many_txn(
   742|         self,
   743|         txn: LoggingTransaction,
   744|         table: str,
   745|         key_names: Collection[str],
   746|         key_values: Collection[Iterable[Any]],
   747|         value_names: Collection[str],
   748|         value_values: Iterable[Iterable[Any]],
   749|     ) -> None:
   750|         """
   751|         Upsert, many times.
   752|         Args:
   753|             table: The table to upsert into
   754|             key_names: The key column names.
   755|             key_values: A list of each row's key column values.
   756|             value_names: The value column names
   757|             value_values: A list of each row's value column values.
   758|                 Ignored if value_names is empty.
   759|         """
   760|         if self.engine.can_native_upsert and table not in self._unsafe_to_upsert_tables:
   761|             return self.simple_upsert_many_txn_native_upsert(
   762|                 txn, table, key_names, key_values, value_names, value_values
   763|             )
   764|         else:
   765|             return self.simple_upsert_many_txn_emulated(
   766|                 txn, table, key_names, key_values, value_names, value_values
   767|             )
   768|     def simple_upsert_many_txn_emulated(
   769|         self,
   770|         txn: LoggingTransaction,
   771|         table: str,
   772|         key_names: Iterable[str],
   773|         key_values: Collection[Iterable[Any]],
   774|         value_names: Collection[str],
   775|         value_values: Iterable[Iterable[Any]],
   776|     ) -> None:
   777|         """
   778|         Upsert, many times, but without native UPSERT support or batching.
   779|         Args:
   780|             table: The table to upsert into
   781|             key_names: The key column names.
   782|             key_values: A list of each row's key column values.
   783|             value_names: The value column names
   784|             value_values: A list of each row's value column values.
   785|                 Ignored if value_names is empty.
   786|         """
   787|         if not value_names:
   788|             value_values = [() for x in range(len(key_values))]
   789|         for keyv, valv in zip(key_values, value_values):
   790|             _keys = {x: y for x, y in zip(key_names, keyv)}
   791|             _vals = {x: y for x, y in zip(value_names, valv)}
   792|             self.simple_upsert_txn_emulated(txn, table, _keys, _vals)
   793|     def simple_upsert_many_txn_native_upsert(
   794|         self,
   795|         txn: LoggingTransaction,


# ====================================================================
# FILE: synapse/storage/databases/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 21-60 ---
    21|             engine = create_engine(database_config.config)
    22|             with make_conn(database_config, engine) as db_conn:
    23|                 logger.info("[database config %r]: Checking database server", db_name)
    24|                 engine.check_database(db_conn)
    25|                 logger.info(
    26|                     "[database config %r]: Preparing for databases %r",
    27|                     db_name,
    28|                     database_config.databases,
    29|                 )
    30|                 prepare_database(
    31|                     db_conn, engine, hs.config, databases=database_config.databases,
    32|                 )
    33|                 database = DatabasePool(hs, database_config, engine)
    34|                 if "main" in database_config.databases:
    35|                     logger.info(
    36|                         "[database config %r]: Starting 'main' database", db_name
    37|                     )
    38|                     if main:
    39|                         raise Exception("'main' data store already configured")
    40|                     main = main_store_class(database, db_conn, hs)
    41|                     if hs.get_instance_name() in hs.config.worker.writers.events:
    42|                         persist_events = PersistEventsStore(hs, database, main)
    43|                 if "state" in database_config.databases:
    44|                     logger.info(
    45|                         "[database config %r]: Starting 'state' database", db_name
    46|                     )
    47|                     if state:
    48|                         raise Exception("'state' data store already configured")
    49|                     state = StateGroupDataStore(database, db_conn, hs)
    50|                 db_conn.commit()
    51|                 self.databases.append(database)
    52|                 logger.info("[database config %r]: prepared", db_name)
    53|             db_conn.close()
    54|         if not main:
    55|             raise Exception("No 'main' database configured")
    56|         if not state:
    57|             raise Exception("No 'state' database configured")
    58|         self.main = main
    59|         self.state = state
    60|         self.persist_events = persist_events


# ====================================================================
# FILE: synapse/storage/databases/main/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 121-171 ---
   121|                 ("device_lists_outbound_pokes", "stream_id"),
   122|             ],
   123|         )
   124|         self._cross_signing_id_gen = StreamIdGenerator(
   125|             db_conn, "e2e_cross_signing_keys", "stream_id"
   126|         )
   127|         self._access_tokens_id_gen = IdGenerator(db_conn, "access_tokens", "id")
   128|         self._event_reports_id_gen = IdGenerator(db_conn, "event_reports", "id")
   129|         self._push_rule_id_gen = IdGenerator(db_conn, "push_rules", "id")
   130|         self._push_rules_enable_id_gen = IdGenerator(db_conn, "push_rules_enable", "id")
   131|         self._pushers_id_gen = StreamIdGenerator(
   132|             db_conn, "pushers", "id", extra_tables=[("deleted_pushers", "stream_id")]
   133|         )
   134|         self._group_updates_id_gen = StreamIdGenerator(
   135|             db_conn, "local_group_updates", "stream_id"
   136|         )
   137|         if isinstance(self.database_engine, PostgresEngine):
   138|             self._cache_id_gen = MultiWriterIdGenerator(
   139|                 db_conn,
   140|                 database,
   141|                 stream_name="caches",
   142|                 instance_name=hs.get_instance_name(),
   143|                 table="cache_invalidation_stream_by_instance",
   144|                 instance_column="instance_name",
   145|                 id_column="stream_id",
   146|                 sequence_name="cache_invalidation_stream_seq",
   147|                 writers=[],
   148|             )
   149|         else:
   150|             self._cache_id_gen = None
   151|         super().__init__(database, db_conn, hs)
   152|         self._presence_on_startup = self._get_active_presence(db_conn)
   153|         presence_cache_prefill, min_presence_val = self.db_pool.get_cache_dict(
   154|             db_conn,
   155|             "presence_stream",
   156|             entity_column="user_id",
   157|             stream_column="stream_id",
   158|             max_value=self._presence_id_gen.get_current_token(),
   159|         )
   160|         self.presence_stream_cache = StreamChangeCache(
   161|             "PresenceStreamChangeCache",
   162|             min_presence_val,
   163|             prefilled_cache=presence_cache_prefill,
   164|         )
   165|         max_device_inbox_id = self._device_inbox_id_gen.get_current_token()
   166|         device_inbox_prefill, min_device_inbox_id = self.db_pool.get_cache_dict(
   167|             db_conn,
   168|             "device_inbox",
   169|             entity_column="user_id",
   170|             stream_column="stream_id",
   171|             max_value=max_device_inbox_id,


# ====================================================================
# FILE: synapse/storage/databases/main/account_data.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-41 ---
     1| import abc
     2| import logging
     3| from typing import Dict, List, Optional, Tuple
     4| from synapse.storage._base import SQLBaseStore, db_to_json
     5| from synapse.storage.database import DatabasePool
     6| from synapse.storage.util.id_generators import StreamIdGenerator
     7| from synapse.types import JsonDict
     8| from synapse.util import json_encoder
     9| from synapse.util.caches.descriptors import _CacheContext, cached
    10| from synapse.util.caches.stream_change_cache import StreamChangeCache
    11| logger = logging.getLogger(__name__)
    12| class AccountDataWorkerStore(SQLBaseStore, metaclass=abc.ABCMeta):
    13|     """This is an abstract base class where subclasses must implement
    14|     `get_max_account_data_stream_id` which can be called in the initializer.
    15|     """
    16|     def __init__(self, database: DatabasePool, db_conn, hs):
    17|         account_max = self.get_max_account_data_stream_id()
    18|         self._account_data_stream_cache = StreamChangeCache(
    19|             "AccountDataAndTagsChangeCache", account_max
    20|         )
    21|         super().__init__(database, db_conn, hs)
    22|     @abc.abstractmethod
    23|     def get_max_account_data_stream_id(self):
    24|         """Get the current max stream ID for account data stream
    25|         Returns:
    26|             int
    27|         """
    28|         raise NotImplementedError()
    29|     @cached()
    30|     async def get_account_data_for_user(
    31|         self, user_id: str
    32|     ) -> Tuple[Dict[str, JsonDict], Dict[str, Dict[str, JsonDict]]]:
    33|         """Get all the client account_data for a user.
    34|         Args:
    35|             user_id: The user to get the account_data for.
    36|         Returns:
    37|             A 2-tuple of a dict of global account_data and a dict mapping from
    38|             room_id string to per room account_data dicts.
    39|         """
    40|         def get_account_data_for_user_txn(txn):
    41|             rows = self.db_pool.simple_select_list_txn(

# --- HUNK 2: Lines 224-316 ---
   224|     ) -> bool:
   225|         ignored_account_data = await self.get_global_account_data_by_type_for_user(
   226|             "m.ignored_user_list",
   227|             ignorer_user_id,
   228|             on_invalidate=cache_context.invalidate,
   229|         )
   230|         if not ignored_account_data:
   231|             return False
   232|         return ignored_user_id in ignored_account_data.get("ignored_users", {})
   233| class AccountDataStore(AccountDataWorkerStore):
   234|     def __init__(self, database: DatabasePool, db_conn, hs):
   235|         self._account_data_id_gen = StreamIdGenerator(
   236|             db_conn,
   237|             "account_data_max_stream_id",
   238|             "stream_id",
   239|             extra_tables=[
   240|                 ("room_account_data", "stream_id"),
   241|                 ("room_tags_revisions", "stream_id"),
   242|             ],
   243|         )
   244|         super().__init__(database, db_conn, hs)
   245|     def get_max_account_data_stream_id(self) -> int:
   246|         """Get the current max stream id for the private user data stream
   247|         Returns:
   248|             The maximum stream ID.
   249|         """
   250|         return self._account_data_id_gen.get_current_token()
   251|     async def add_account_data_to_room(
   252|         self, user_id: str, room_id: str, account_data_type: str, content: JsonDict
   253|     ) -> int:
   254|         """Add some account_data to a room for a user.
   255|         Args:
   256|             user_id: The user to add a tag for.
   257|             room_id: The room to add a tag for.
   258|             account_data_type: The type of account_data to add.
   259|             content: A json object to associate with the tag.
   260|         Returns:
   261|             The maximum stream ID.
   262|         """
   263|         content_json = json_encoder.encode(content)
   264|         async with self._account_data_id_gen.get_next() as next_id:
   265|             await self.db_pool.simple_upsert(
   266|                 desc="add_room_account_data",
   267|                 table="room_account_data",
   268|                 keyvalues={
   269|                     "user_id": user_id,
   270|                     "room_id": room_id,
   271|                     "account_data_type": account_data_type,
   272|                 },
   273|                 values={"stream_id": next_id, "content": content_json},
   274|                 lock=False,
   275|             )
   276|             await self._update_max_stream_id(next_id)
   277|             self._account_data_stream_cache.entity_has_changed(user_id, next_id)
   278|             self.get_account_data_for_user.invalidate((user_id,))
   279|             self.get_account_data_for_room.invalidate((user_id, room_id))
   280|             self.get_account_data_for_room_and_type.prefill(
   281|                 (user_id, room_id, account_data_type), content
   282|             )
   283|         return self._account_data_id_gen.get_current_token()
   284|     async def add_account_data_for_user(
   285|         self, user_id: str, account_data_type: str, content: JsonDict
   286|     ) -> int:
   287|         """Add some account_data to a room for a user.
   288|         Args:
   289|             user_id: The user to add a tag for.
   290|             account_data_type: The type of account_data to add.
   291|             content: A json object to associate with the tag.
   292|         Returns:
   293|             The maximum stream ID.
   294|         """
   295|         content_json = json_encoder.encode(content)
   296|         async with self._account_data_id_gen.get_next() as next_id:
   297|             await self.db_pool.simple_upsert(
   298|                 desc="add_user_account_data",
   299|                 table="account_data",
   300|                 keyvalues={"user_id": user_id, "account_data_type": account_data_type},
   301|                 values={"stream_id": next_id, "content": content_json},
   302|                 lock=False,
   303|             )
   304|             await self._update_max_stream_id(next_id)
   305|             self._account_data_stream_cache.entity_has_changed(user_id, next_id)
   306|             self.get_account_data_for_user.invalidate((user_id,))
   307|             self.get_global_account_data_by_type_for_user.invalidate(
   308|                 (account_data_type, user_id)
   309|             )
   310|         return self._account_data_id_gen.get_current_token()
   311|     async def _update_max_stream_id(self, next_id: int) -> None:
   312|         """Update the max stream_id
   313|         Args:
   314|             next_id: The the revision to advance to.
   315|         """
   316|         def _update(txn):


# ====================================================================
# FILE: synapse/storage/databases/main/appservice.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 8-48 ---
     8| from synapse.util import json_encoder
     9| logger = logging.getLogger(__name__)
    10| def _make_exclusive_regex(services_cache):
    11|     exclusive_user_regexes = [
    12|         regex.pattern
    13|         for service in services_cache
    14|         for regex in service.get_exclusive_user_regexes()
    15|     ]
    16|     if exclusive_user_regexes:
    17|         exclusive_user_regex = "|".join("(" + r + ")" for r in exclusive_user_regexes)
    18|         exclusive_user_regex = re.compile(exclusive_user_regex)
    19|     else:
    20|         exclusive_user_regex = None
    21|     return exclusive_user_regex
    22| class ApplicationServiceWorkerStore(SQLBaseStore):
    23|     def __init__(self, database: DatabasePool, db_conn, hs):
    24|         self.services_cache = load_appservices(
    25|             hs.hostname, hs.config.app_service_config_files
    26|         )
    27|         self.exclusive_user_regex = _make_exclusive_regex(self.services_cache)
    28|         super().__init__(database, db_conn, hs)
    29|     def get_app_services(self):
    30|         return self.services_cache
    31|     def get_if_app_services_interested_in_user(self, user_id):
    32|         """Check if the user is one associated with an app service (exclusively)
    33|         """
    34|         if self.exclusive_user_regex:
    35|             return bool(self.exclusive_user_regex.match(user_id))
    36|         else:
    37|             return False
    38|     def get_app_service_by_user_id(self, user_id):
    39|         """Retrieve an application service from their user ID.
    40|         All application services have associated with them a particular user ID.
    41|         There is no distinguishing feature on the user ID which indicates it
    42|         represents an application service. This function allows you to map from
    43|         a user ID to an application service.
    44|         Args:
    45|             user_id(str): The user ID to see if it is an application service.
    46|         Returns:
    47|             synapse.appservice.ApplicationService or None.
    48|         """


# ====================================================================
# FILE: synapse/storage/databases/main/client_ips.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| import logging
     2| from typing import Dict, Optional, Tuple
     3| from synapse.metrics.background_process_metrics import wrap_as_background_process
     4| from synapse.storage._base import SQLBaseStore
     5| from synapse.storage.database import DatabasePool, make_tuple_comparison_clause
     6| from synapse.util.caches.descriptors import Cache
     7| logger = logging.getLogger(__name__)
     8| LAST_SEEN_GRANULARITY = 120 * 1000
     9| class ClientIpBackgroundUpdateStore(SQLBaseStore):
    10|     def __init__(self, database: DatabasePool, db_conn, hs):
    11|         super().__init__(database, db_conn, hs)
    12|         self.db_pool.updates.register_background_index_update(
    13|             "user_ips_device_index",
    14|             index_name="user_ips_device_id",
    15|             table="user_ips",
    16|             columns=["user_id", "device_id", "last_seen"],
    17|         )
    18|         self.db_pool.updates.register_background_index_update(
    19|             "user_ips_last_seen_index",
    20|             index_name="user_ips_last_seen",
    21|             table="user_ips",
    22|             columns=["user_id", "last_seen"],
    23|         )
    24|         self.db_pool.updates.register_background_index_update(
    25|             "user_ips_last_seen_only_index",
    26|             index_name="user_ips_last_seen_only",
    27|             table="user_ips",
    28|             columns=["last_seen"],
    29|         )
    30|         self.db_pool.updates.register_background_update_handler(
    31|             "user_ips_analyze", self._analyze_user_ip

# --- HUNK 2: Lines 190-230 ---
   190|             """
   191|             txn.execute_batch(sql, rows)
   192|             _, _, _, user_id, device_id = rows[-1]
   193|             self.db_pool.updates._background_update_progress_txn(
   194|                 txn,
   195|                 "devices_last_seen",
   196|                 {"last_user_id": user_id, "last_device_id": device_id},
   197|             )
   198|             return len(rows)
   199|         updated = await self.db_pool.runInteraction(
   200|             "_devices_last_seen_update", _devices_last_seen_update_txn
   201|         )
   202|         if not updated:
   203|             await self.db_pool.updates._end_background_update("devices_last_seen")
   204|         return updated
   205| class ClientIpStore(ClientIpBackgroundUpdateStore):
   206|     def __init__(self, database: DatabasePool, db_conn, hs):
   207|         self.client_ip_last_seen = Cache(
   208|             name="client_ip_last_seen", keylen=4, max_entries=50000
   209|         )
   210|         super().__init__(database, db_conn, hs)
   211|         self.user_ips_max_age = hs.config.user_ips_max_age
   212|         self._batch_row_update = {}
   213|         self._client_ip_looper = self._clock.looping_call(
   214|             self._update_client_ips_batch, 5 * 1000
   215|         )
   216|         self.hs.get_reactor().addSystemEventTrigger(
   217|             "before", "shutdown", self._update_client_ips_batch
   218|         )
   219|         if self.user_ips_max_age:
   220|             self._clock.looping_call(self._prune_old_user_ips, 5 * 1000)
   221|     async def insert_client_ip(
   222|         self, user_id, access_token, ip, user_agent, device_id, now=None
   223|     ):
   224|         if not now:
   225|             now = int(self._clock.time_msec())
   226|         key = (user_id, access_token, ip)
   227|         try:
   228|             last_seen = self.client_ip_last_seen.get(key)
   229|         except KeyError:
   230|             last_seen = None


# ====================================================================
# FILE: synapse/storage/databases/main/deviceinbox.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 201-335 ---
   201|                 "SELECT max(stream_id), destination"
   202|                 " FROM device_federation_outbox"
   203|                 " WHERE ? < stream_id AND stream_id <= ?"
   204|                 " GROUP BY destination"
   205|             )
   206|             txn.execute(sql, (last_id, upper_pos))
   207|             updates.extend((row[0], row[1:]) for row in txn)
   208|             updates.sort()
   209|             limited = False
   210|             upto_token = current_id
   211|             if len(updates) >= limit:
   212|                 upto_token = updates[-1][0]
   213|                 limited = True
   214|             return updates, upto_token, limited
   215|         return await self.db_pool.runInteraction(
   216|             "get_all_new_device_messages", get_all_new_device_messages_txn
   217|         )
   218| class DeviceInboxBackgroundUpdateStore(SQLBaseStore):
   219|     DEVICE_INBOX_STREAM_ID = "device_inbox_stream_drop"
   220|     def __init__(self, database: DatabasePool, db_conn, hs):
   221|         super().__init__(database, db_conn, hs)
   222|         self.db_pool.updates.register_background_index_update(
   223|             "device_inbox_stream_index",
   224|             index_name="device_inbox_stream_id_user_id",
   225|             table="device_inbox",
   226|             columns=["stream_id", "user_id"],
   227|         )
   228|         self.db_pool.updates.register_background_update_handler(
   229|             self.DEVICE_INBOX_STREAM_ID, self._background_drop_index_device_inbox
   230|         )
   231|     async def _background_drop_index_device_inbox(self, progress, batch_size):
   232|         def reindex_txn(conn):
   233|             txn = conn.cursor()
   234|             txn.execute("DROP INDEX IF EXISTS device_inbox_stream_id")
   235|             txn.close()
   236|         await self.db_pool.runWithConnection(reindex_txn)
   237|         await self.db_pool.updates._end_background_update(self.DEVICE_INBOX_STREAM_ID)
   238|         return 1
   239| class DeviceInboxStore(DeviceInboxWorkerStore, DeviceInboxBackgroundUpdateStore):
   240|     DEVICE_INBOX_STREAM_ID = "device_inbox_stream_drop"
   241|     def __init__(self, database: DatabasePool, db_conn, hs):
   242|         super().__init__(database, db_conn, hs)
   243|         self._last_device_delete_cache = ExpiringCache(
   244|             cache_name="last_device_delete_cache",
   245|             clock=self._clock,
   246|             max_len=10000,
   247|             expiry_ms=30 * 60 * 1000,
   248|         )
   249|     @trace
   250|     async def add_messages_to_device_inbox(
   251|         self,
   252|         local_messages_by_user_then_device: dict,
   253|         remote_messages_by_destination: dict,
   254|     ) -> int:
   255|         """Used to send messages from this server.
   256|         Args:
   257|             local_messages_by_user_and_device:
   258|                 Dictionary of user_id to device_id to message.
   259|             remote_messages_by_destination:
   260|                 Dictionary of destination server_name to the EDU JSON to send.
   261|         Returns:
   262|             The new stream_id.
   263|         """
   264|         def add_messages_txn(txn, now_ms, stream_id):
   265|             self._add_messages_to_local_device_inbox_txn(
   266|                 txn, stream_id, local_messages_by_user_then_device
   267|             )
   268|             sql = (
   269|                 "INSERT INTO device_federation_outbox"
   270|                 " (destination, stream_id, queued_ts, messages_json)"
   271|                 " VALUES (?,?,?,?)"
   272|             )
   273|             rows = []
   274|             for destination, edu in remote_messages_by_destination.items():
   275|                 edu_json = json_encoder.encode(edu)
   276|                 rows.append((destination, stream_id, now_ms, edu_json))
   277|             txn.executemany(sql, rows)
   278|         async with self._device_inbox_id_gen.get_next() as stream_id:
   279|             now_ms = self.clock.time_msec()
   280|             await self.db_pool.runInteraction(
   281|                 "add_messages_to_device_inbox", add_messages_txn, now_ms, stream_id
   282|             )
   283|             for user_id in local_messages_by_user_then_device.keys():
   284|                 self._device_inbox_stream_cache.entity_has_changed(user_id, stream_id)
   285|             for destination in remote_messages_by_destination.keys():
   286|                 self._device_federation_outbox_stream_cache.entity_has_changed(
   287|                     destination, stream_id
   288|                 )
   289|         return self._device_inbox_id_gen.get_current_token()
   290|     async def add_messages_from_remote_to_device_inbox(
   291|         self, origin: str, message_id: str, local_messages_by_user_then_device: dict
   292|     ) -> int:
   293|         def add_messages_txn(txn, now_ms, stream_id):
   294|             already_inserted = self.db_pool.simple_select_one_txn(
   295|                 txn,
   296|                 table="device_federation_inbox",
   297|                 keyvalues={"origin": origin, "message_id": message_id},
   298|                 retcols=("message_id",),
   299|                 allow_none=True,
   300|             )
   301|             if already_inserted is not None:
   302|                 return
   303|             self.db_pool.simple_insert_txn(
   304|                 txn,
   305|                 table="device_federation_inbox",
   306|                 values={
   307|                     "origin": origin,
   308|                     "message_id": message_id,
   309|                     "received_ts": now_ms,
   310|                 },
   311|             )
   312|             self._add_messages_to_local_device_inbox_txn(
   313|                 txn, stream_id, local_messages_by_user_then_device
   314|             )
   315|         async with self._device_inbox_id_gen.get_next() as stream_id:
   316|             now_ms = self.clock.time_msec()
   317|             await self.db_pool.runInteraction(
   318|                 "add_messages_from_remote_to_device_inbox",
   319|                 add_messages_txn,
   320|                 now_ms,
   321|                 stream_id,
   322|             )
   323|             for user_id in local_messages_by_user_then_device.keys():
   324|                 self._device_inbox_stream_cache.entity_has_changed(user_id, stream_id)
   325|         return stream_id
   326|     def _add_messages_to_local_device_inbox_txn(
   327|         self, txn, stream_id, messages_by_user_then_device
   328|     ):
   329|         local_by_user_then_device = {}
   330|         for user_id, messages_by_device in messages_by_user_then_device.items():
   331|             messages_json_for_user = {}
   332|             devices = list(messages_by_device.keys())
   333|             if len(devices) == 1 and devices[0] == "*":
   334|                 sql = "SELECT device_id FROM devices WHERE user_id = ?"
   335|                 txn.execute(sql, (user_id,))


# ====================================================================
# FILE: synapse/storage/databases/main/devices.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 266-306 ---
   266|             key_names=("destination", "user_id"),
   267|             key_values=((destination, user_id) for user_id, _ in rows),
   268|             value_names=("stream_id",),
   269|             value_values=((stream_id,) for _, stream_id in rows),
   270|         )
   271|         sql = """
   272|             DELETE FROM device_lists_outbound_pokes
   273|             WHERE destination = ? AND stream_id <= ?
   274|         """
   275|         txn.execute(sql, (destination, stream_id))
   276|     async def add_user_signature_change_to_streams(
   277|         self, from_user_id: str, user_ids: List[str]
   278|     ) -> int:
   279|         """Persist that a user has made new signatures
   280|         Args:
   281|             from_user_id: the user who made the signatures
   282|             user_ids: the users who were signed
   283|         Returns:
   284|             THe new stream ID.
   285|         """
   286|         async with self._device_list_id_gen.get_next() as stream_id:
   287|             await self.db_pool.runInteraction(
   288|                 "add_user_sig_change_to_streams",
   289|                 self._add_user_signature_change_txn,
   290|                 from_user_id,
   291|                 user_ids,
   292|                 stream_id,
   293|             )
   294|         return stream_id
   295|     def _add_user_signature_change_txn(
   296|         self,
   297|         txn: LoggingTransaction,
   298|         from_user_id: str,
   299|         user_ids: List[str],
   300|         stream_id: int,
   301|     ) -> None:
   302|         txn.call_after(
   303|             self._user_signature_stream_cache.entity_has_changed,
   304|             from_user_id,
   305|             stream_id,
   306|         )

# --- HUNK 2: Lines 355-437 ---
   355|     async def _get_cached_user_device(self, user_id: str, device_id: str) -> JsonDict:
   356|         content = await self.db_pool.simple_select_one_onecol(
   357|             table="device_lists_remote_cache",
   358|             keyvalues={"user_id": user_id, "device_id": device_id},
   359|             retcol="content",
   360|             desc="_get_cached_user_device",
   361|         )
   362|         return db_to_json(content)
   363|     @cached()
   364|     async def get_cached_devices_for_user(self, user_id: str) -> Dict[str, JsonDict]:
   365|         devices = await self.db_pool.simple_select_list(
   366|             table="device_lists_remote_cache",
   367|             keyvalues={"user_id": user_id},
   368|             retcols=("device_id", "content"),
   369|             desc="get_cached_devices_for_user",
   370|         )
   371|         return {
   372|             device["device_id"]: db_to_json(device["content"]) for device in devices
   373|         }
   374|     async def get_users_whose_devices_changed(
   375|         self, from_key: int, user_ids: Iterable[str]
   376|     ) -> Set[str]:
   377|         """Get set of users whose devices have changed since `from_key` that
   378|         are in the given list of user_ids.
   379|         Args:
   380|             from_key: The device lists stream token
   381|             user_ids: The user IDs to query for devices.
   382|         Returns:
   383|             The set of user_ids whose devices have changed since `from_key`
   384|         """
   385|         to_check = self._device_list_stream_cache.get_entities_changed(
   386|             user_ids, from_key
   387|         )
   388|         if not to_check:
   389|             return set()
   390|         def _get_users_whose_devices_changed_txn(txn):
   391|             changes = set()
   392|             sql = """
   393|                 SELECT DISTINCT user_id FROM device_lists_stream
   394|                 WHERE stream_id > ?
   395|                 AND
   396|             """
   397|             for chunk in batch_iter(to_check, 100):
   398|                 clause, args = make_in_list_sql_clause(
   399|                     txn.database_engine, "user_id", chunk
   400|                 )
   401|                 txn.execute(sql + clause, (from_key,) + tuple(args))
   402|                 changes.update(user_id for user_id, in txn)
   403|             return changes
   404|         return await self.db_pool.runInteraction(
   405|             "get_users_whose_devices_changed", _get_users_whose_devices_changed_txn
   406|         )
   407|     async def get_users_whose_signatures_changed(
   408|         self, user_id: str, from_key: int
   409|     ) -> Set[str]:
   410|         """Get the users who have new cross-signing signatures made by `user_id` since
   411|         `from_key`.
   412|         Args:
   413|             user_id: the user who made the signatures
   414|             from_key: The device lists stream token
   415|         Returns:
   416|             A set of user IDs with updated signatures.
   417|         """
   418|         if self._user_signature_stream_cache.has_entity_changed(user_id, from_key):
   419|             sql = """
   420|                 SELECT DISTINCT user_ids FROM user_signature_stream
   421|                 WHERE from_user_id = ? AND stream_id > ?
   422|             """
   423|             rows = await self.db_pool.execute(
   424|                 "get_users_whose_signatures_changed", None, sql, user_id, from_key
   425|             )
   426|             return {user for row in rows for user in db_to_json(row[0])}
   427|         else:
   428|             return set()
   429|     async def get_all_device_list_changes_for_remotes(
   430|         self, instance_name: str, last_id: int, current_id: int, limit: int
   431|     ) -> Tuple[List[Tuple[int, tuple]], int, bool]:
   432|         """Get updates for device lists replication stream.
   433|         Args:
   434|             instance_name: The writer we want to fetch updates from. Unused
   435|                 here since there is only ever one writer.
   436|             last_id: The token to fetch updates from. Exclusive.
   437|             current_id: The token to fetch updates up to. Inclusive.

# --- HUNK 3: Lines 535-575 ---
   535|             desc="make_remote_user_device_cache_as_stale",
   536|         )
   537|     async def mark_remote_user_device_list_as_unsubscribed(self, user_id: str) -> None:
   538|         """Mark that we no longer track device lists for remote user.
   539|         """
   540|         def _mark_remote_user_device_list_as_unsubscribed_txn(txn):
   541|             self.db_pool.simple_delete_txn(
   542|                 txn,
   543|                 table="device_lists_remote_extremeties",
   544|                 keyvalues={"user_id": user_id},
   545|             )
   546|             self._invalidate_cache_and_stream(
   547|                 txn, self.get_device_list_last_stream_id_for_remote, (user_id,)
   548|             )
   549|         await self.db_pool.runInteraction(
   550|             "mark_remote_user_device_list_as_unsubscribed",
   551|             _mark_remote_user_device_list_as_unsubscribed_txn,
   552|         )
   553| class DeviceBackgroundUpdateStore(SQLBaseStore):
   554|     def __init__(self, database: DatabasePool, db_conn, hs):
   555|         super().__init__(database, db_conn, hs)
   556|         self.db_pool.updates.register_background_index_update(
   557|             "device_lists_stream_idx",
   558|             index_name="device_lists_stream_user_id",
   559|             table="device_lists_stream",
   560|             columns=["user_id", "device_id"],
   561|         )
   562|         self.db_pool.updates.register_background_index_update(
   563|             "device_lists_remote_cache_unique_idx",
   564|             index_name="device_lists_remote_cache_unique_id",
   565|             table="device_lists_remote_cache",
   566|             columns=["user_id", "device_id"],
   567|             unique=True,
   568|         )
   569|         self.db_pool.updates.register_background_index_update(
   570|             "device_lists_remote_extremeties_unique_idx",
   571|             index_name="device_lists_remote_extremeties_unique_idx",
   572|             table="device_lists_remote_extremeties",
   573|             columns=["user_id"],
   574|             unique=True,
   575|         )

# --- HUNK 4: Lines 629-669 ---
   629|                 )
   630|                 row["sent"] = False
   631|                 self.db_pool.simple_insert_txn(
   632|                     txn, "device_lists_outbound_pokes", row,
   633|                 )
   634|             if row:
   635|                 self.db_pool.updates._background_update_progress_txn(
   636|                     txn, BG_UPDATE_REMOVE_DUP_OUTBOUND_POKES, {"last_row": row},
   637|                 )
   638|             return len(rows)
   639|         rows = await self.db_pool.runInteraction(
   640|             BG_UPDATE_REMOVE_DUP_OUTBOUND_POKES, _txn
   641|         )
   642|         if not rows:
   643|             await self.db_pool.updates._end_background_update(
   644|                 BG_UPDATE_REMOVE_DUP_OUTBOUND_POKES
   645|             )
   646|         return rows
   647| class DeviceStore(DeviceWorkerStore, DeviceBackgroundUpdateStore):
   648|     def __init__(self, database: DatabasePool, db_conn, hs):
   649|         super().__init__(database, db_conn, hs)
   650|         self.device_id_exists_cache = Cache(
   651|             name="device_id_exists", keylen=2, max_entries=10000
   652|         )
   653|         self._clock.looping_call(self._prune_old_outbound_device_pokes, 60 * 60 * 1000)
   654|     async def store_device(
   655|         self, user_id: str, device_id: str, initial_device_display_name: str
   656|     ) -> bool:
   657|         """Ensure the given device is known; add it to the store if not
   658|         Args:
   659|             user_id: id of user associated with the device
   660|             device_id: id of device
   661|             initial_device_display_name: initial displayname of the device.
   662|                 Ignored if device exists.
   663|         Returns:
   664|             Whether the device was inserted or an existing device existed with that ID.
   665|         Raises:
   666|             StoreError: if the device is already in use
   667|         """
   668|         key = (user_id, device_id)
   669|         if self.device_id_exists_cache.get(key, None):

# --- HUNK 5: Lines 852-905 ---
   852|             self.get_device_list_last_stream_id_for_remote.invalidate, (user_id,)
   853|         )
   854|         self.db_pool.simple_upsert_txn(
   855|             txn,
   856|             table="device_lists_remote_extremeties",
   857|             keyvalues={"user_id": user_id},
   858|             values={"stream_id": stream_id},
   859|             lock=False,
   860|         )
   861|         self.db_pool.simple_delete_txn(
   862|             txn, table="device_lists_remote_resync", keyvalues={"user_id": user_id},
   863|         )
   864|     async def add_device_change_to_streams(
   865|         self, user_id: str, device_ids: Collection[str], hosts: List[str]
   866|     ):
   867|         """Persist that a user's devices have been updated, and which hosts
   868|         (if any) should be poked.
   869|         """
   870|         if not device_ids:
   871|             return
   872|         async with self._device_list_id_gen.get_next_mult(
   873|             len(device_ids)
   874|         ) as stream_ids:
   875|             await self.db_pool.runInteraction(
   876|                 "add_device_change_to_stream",
   877|                 self._add_device_change_to_stream_txn,
   878|                 user_id,
   879|                 device_ids,
   880|                 stream_ids,
   881|             )
   882|         if not hosts:
   883|             return stream_ids[-1]
   884|         context = get_active_span_text_map()
   885|         async with self._device_list_id_gen.get_next_mult(
   886|             len(hosts) * len(device_ids)
   887|         ) as stream_ids:
   888|             await self.db_pool.runInteraction(
   889|                 "add_device_outbound_poke_to_stream",
   890|                 self._add_device_outbound_poke_to_stream_txn,
   891|                 user_id,
   892|                 device_ids,
   893|                 hosts,
   894|                 stream_ids,
   895|                 context,
   896|             )
   897|         return stream_ids[-1]
   898|     def _add_device_change_to_stream_txn(
   899|         self,
   900|         txn: LoggingTransaction,
   901|         user_id: str,
   902|         device_ids: Collection[str],
   903|         stream_ids: List[str],
   904|     ):
   905|         txn.call_after(


# ====================================================================
# FILE: synapse/storage/databases/main/end_to_end_keys.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-36 ---
     1| import abc
     2| from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Tuple
     3| import attr
     4| from canonicaljson import encode_canonical_json
     5| from twisted.enterprise.adbapi import Connection
     6| from synapse.logging.opentracing import log_kv, set_tag, trace
     7| from synapse.storage._base import SQLBaseStore, db_to_json
     8| from synapse.storage.database import make_in_list_sql_clause
     9| from synapse.storage.types import Cursor
    10| from synapse.types import JsonDict
    11| from synapse.util import json_encoder
    12| from synapse.util.caches.descriptors import cached, cachedList
    13| from synapse.util.iterutils import batch_iter
    14| if TYPE_CHECKING:
    15|     from synapse.handlers.e2e_keys import SignatureListItem
    16| @attr.s(slots=True)
    17| class DeviceKeyLookupResult:
    18|     """The type returned by get_e2e_device_keys_and_signatures"""
    19|     display_name = attr.ib(type=Optional[str])
    20|     keys = attr.ib(type=Optional[JsonDict])
    21| class EndToEndKeyWorkerStore(SQLBaseStore):
    22|     async def get_e2e_device_keys_for_federation_query(
    23|         self, user_id: str
    24|     ) -> Tuple[int, List[JsonDict]]:
    25|         """Get all devices (with any device keys) for a user
    26|         Returns:
    27|             (stream_id, devices)
    28|         """
    29|         now_stream_id = self.get_device_stream_token()
    30|         devices = await self.get_e2e_device_keys_and_signatures([(user_id, None)])
    31|         if devices:
    32|             user_devices = devices[user_id]
    33|             results = []
    34|             for device_id, device in user_devices.items():
    35|                 result = {"device_id": device_id}
    36|                 keys = device.keys

# --- HUNK 2: Lines 625-665 ---
   625|         self.db_pool.simple_insert_txn(
   626|             txn,
   627|             "e2e_cross_signing_keys",
   628|             values={
   629|                 "user_id": user_id,
   630|                 "keytype": key_type,
   631|                 "keydata": json_encoder.encode(key),
   632|                 "stream_id": stream_id,
   633|             },
   634|         )
   635|         self._invalidate_cache_and_stream(
   636|             txn, self._get_bare_e2e_cross_signing_keys, (user_id,)
   637|         )
   638|     async def set_e2e_cross_signing_key(self, user_id, key_type, key):
   639|         """Set a user's cross-signing key.
   640|         Args:
   641|             user_id (str): the user to set the user-signing key for
   642|             key_type (str): the type of cross-signing key to set
   643|             key (dict): the key data
   644|         """
   645|         async with self._cross_signing_id_gen.get_next() as stream_id:
   646|             return await self.db_pool.runInteraction(
   647|                 "add_e2e_cross_signing_key",
   648|                 self._set_e2e_cross_signing_key_txn,
   649|                 user_id,
   650|                 key_type,
   651|                 key,
   652|                 stream_id,
   653|             )
   654|     async def store_e2e_cross_signing_signatures(
   655|         self, user_id: str, signatures: "Iterable[SignatureListItem]"
   656|     ) -> None:
   657|         """Stores cross-signing signatures.
   658|         Args:
   659|             user_id: the user who made the signatures
   660|             signatures: signatures to add
   661|         """
   662|         await self.db_pool.simple_insert_many(
   663|             "e2e_cross_signing_signatures",
   664|             [
   665|                 {


# ====================================================================
# FILE: synapse/storage/databases/main/event_federation.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 260-300 ---
   260|         stream_orderings from that point.
   261|         Args:
   262|             room_id:
   263|             stream_ordering:
   264|         Returns:
   265|             A list of event_ids
   266|         """
   267|         last_change = self._events_stream_cache.get_max_pos_of_last_change(room_id)
   268|         last_change = max(self._stream_order_on_start, last_change)
   269|         if last_change > self.stream_ordering_month_ago:
   270|             stream_ordering = min(last_change, stream_ordering)
   271|         return await self._get_forward_extremeties_for_room(room_id, stream_ordering)
   272|     @cached(max_entries=5000, num_args=2)
   273|     async def _get_forward_extremeties_for_room(self, room_id, stream_ordering):
   274|         """For a given room_id and stream_ordering, return the forward
   275|         extremeties of the room at that point in "time".
   276|         Throws a StoreError if we have since purged the index for
   277|         stream_orderings from that point.
   278|         """
   279|         if stream_ordering <= self.stream_ordering_month_ago:
   280|             raise StoreError(400, "stream_ordering too old %s" % (stream_ordering,))
   281|         sql = """
   282|                 SELECT event_id FROM stream_ordering_to_exterm
   283|                 INNER JOIN (
   284|                     SELECT room_id, MAX(stream_ordering) AS stream_ordering
   285|                     FROM stream_ordering_to_exterm
   286|                     WHERE stream_ordering <= ? GROUP BY room_id
   287|                 ) AS rms USING (room_id, stream_ordering)
   288|                 WHERE room_id = ?
   289|         """
   290|         def get_forward_extremeties_for_room_txn(txn):
   291|             txn.execute(sql, (stream_ordering, room_id))
   292|             return [event_id for event_id, in txn]
   293|         return await self.db_pool.runInteraction(
   294|             "get_forward_extremeties_for_room", get_forward_extremeties_for_room_txn
   295|         )
   296|     async def get_backfill_events(self, room_id: str, event_list: list, limit: int):
   297|         """Get a list of Events for a given topic that occurred before (and
   298|         including) the events in event_list. Return a list of max size `limit`
   299|         Args:
   300|             room_id

# --- HUNK 2: Lines 383-423 ---
   383|             event_ids: The events to use as the previous events.
   384|         """
   385|         rows = await self.db_pool.simple_select_many_batch(
   386|             table="event_edges",
   387|             column="prev_event_id",
   388|             iterable=event_ids,
   389|             retcols=("event_id",),
   390|             desc="get_successor_events",
   391|         )
   392|         return [row["event_id"] for row in rows]
   393| class EventFederationStore(EventFederationWorkerStore):
   394|     """ Responsible for storing and serving up the various graphs associated
   395|     with an event. Including the main event graph and the auth chains for an
   396|     event.
   397|     Also has methods for getting the front (latest) and back (oldest) edges
   398|     of the event graphs. These are used to generate the parents for new events
   399|     and backfilling from another server respectively.
   400|     """
   401|     EVENT_AUTH_STATE_ONLY = "event_auth_state_only"
   402|     def __init__(self, database: DatabasePool, db_conn, hs):
   403|         super().__init__(database, db_conn, hs)
   404|         self.db_pool.updates.register_background_update_handler(
   405|             self.EVENT_AUTH_STATE_ONLY, self._background_delete_non_state_event_auth
   406|         )
   407|         hs.get_clock().looping_call(
   408|             self._delete_old_forward_extrem_cache, 60 * 60 * 1000
   409|         )
   410|     def _delete_old_forward_extrem_cache(self):
   411|         def _delete_old_forward_extrem_cache_txn(txn):
   412|             sql = """
   413|                 DELETE FROM stream_ordering_to_exterm
   414|                 WHERE
   415|                 room_id IN (
   416|                     SELECT room_id
   417|                     FROM stream_ordering_to_exterm
   418|                     WHERE stream_ordering > ?
   419|                 ) AND stream_ordering < ?
   420|             """
   421|             txn.execute(
   422|                 sql, (self.stream_ordering_month_ago, self.stream_ordering_month_ago)
   423|             )


# ====================================================================
# FILE: synapse/storage/databases/main/event_push_actions.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 22-62 ---
    22|     any real JSON actions
    23|     """
    24|     if is_highlight:
    25|         if actions == DEFAULT_HIGHLIGHT_ACTION:
    26|             return ""  # We use empty string as the column is non-NULL
    27|     else:
    28|         if actions == DEFAULT_NOTIF_ACTION:
    29|             return ""
    30|     return json_encoder.encode(actions)
    31| def _deserialize_action(actions, is_highlight):
    32|     """Custom deserializer for actions. This allows us to "compress" common actions
    33|     """
    34|     if actions:
    35|         return db_to_json(actions)
    36|     if is_highlight:
    37|         return DEFAULT_HIGHLIGHT_ACTION
    38|     else:
    39|         return DEFAULT_NOTIF_ACTION
    40| class EventPushActionsWorkerStore(SQLBaseStore):
    41|     def __init__(self, database: DatabasePool, db_conn, hs):
    42|         super().__init__(database, db_conn, hs)
    43|         self.stream_ordering_month_ago = None
    44|         self.stream_ordering_day_ago = None
    45|         cur = LoggingTransaction(
    46|             db_conn.cursor(),
    47|             name="_find_stream_orderings_for_times_txn",
    48|             database_engine=self.database_engine,
    49|         )
    50|         self._find_stream_orderings_for_times_txn(cur)
    51|         cur.close()
    52|         self.find_stream_orderings_looping_call = self._clock.looping_call(
    53|             self._find_stream_orderings_for_times, 10 * 60 * 1000
    54|         )
    55|         self._rotate_delay = 3
    56|         self._rotate_count = 10000
    57|     @cached(num_args=3, tree=True, max_entries=5000)
    58|     async def get_unread_event_push_actions_by_room_for_user(
    59|         self, room_id: str, user_id: str, last_read_event_id: Optional[str],
    60|     ) -> Dict[str, int]:
    61|         """Get the notification count, the highlight count and the unread message count
    62|         for a given user in a given room after the given read receipt.

# --- HUNK 2: Lines 476-516 ---
   476|         return range_end
   477|     async def get_time_of_last_push_action_before(self, stream_ordering):
   478|         def f(txn):
   479|             sql = (
   480|                 "SELECT e.received_ts"
   481|                 " FROM event_push_actions AS ep"
   482|                 " JOIN events e ON ep.room_id = e.room_id AND ep.event_id = e.event_id"
   483|                 " WHERE ep.stream_ordering > ? AND notif = 1"
   484|                 " ORDER BY ep.stream_ordering ASC"
   485|                 " LIMIT 1"
   486|             )
   487|             txn.execute(sql, (stream_ordering,))
   488|             return txn.fetchone()
   489|         result = await self.db_pool.runInteraction(
   490|             "get_time_of_last_push_action_before", f
   491|         )
   492|         return result[0] if result else None
   493| class EventPushActionsStore(EventPushActionsWorkerStore):
   494|     EPA_HIGHLIGHT_INDEX = "epa_highlight_index"
   495|     def __init__(self, database: DatabasePool, db_conn, hs):
   496|         super().__init__(database, db_conn, hs)
   497|         self.db_pool.updates.register_background_index_update(
   498|             self.EPA_HIGHLIGHT_INDEX,
   499|             index_name="event_push_actions_u_highlight",
   500|             table="event_push_actions",
   501|             columns=["user_id", "stream_ordering"],
   502|         )
   503|         self.db_pool.updates.register_background_index_update(
   504|             "event_push_actions_highlights_index",
   505|             index_name="event_push_actions_highlights_index",
   506|             table="event_push_actions",
   507|             columns=["user_id", "room_id", "topological_ordering", "stream_ordering"],
   508|             where_clause="highlight=1",
   509|         )
   510|         self._doing_notif_rotation = False
   511|         self._rotate_notif_loop = self._clock.looping_call(
   512|             self._start_rotate_notifs, 30 * 60 * 1000
   513|         )
   514|     async def get_push_actions_for_user(
   515|         self, user_id, before=None, limit=50, only_highlight=False
   516|     ):

# --- HUNK 3: Lines 715-743 ---
   715|             ),
   716|         )
   717|         txn.execute(
   718|             "DELETE FROM event_push_actions"
   719|             " WHERE ? <= stream_ordering AND stream_ordering < ? AND highlight = 0",
   720|             (old_rotate_stream_ordering, rotate_to_stream_ordering),
   721|         )
   722|         logger.info("Rotating notifications, deleted %s push actions", txn.rowcount)
   723|         txn.execute(
   724|             "UPDATE event_push_summary_stream_ordering SET stream_ordering = ?",
   725|             (rotate_to_stream_ordering,),
   726|         )
   727| def _action_has_highlight(actions):
   728|     for action in actions:
   729|         try:
   730|             if action.get("set_tweak", None) == "highlight":
   731|                 return action.get("value", True)
   732|         except AttributeError:
   733|             pass
   734|     return False
   735| @attr.s(slots=True)
   736| class _EventPushSummary:
   737|     """Summary of pending event push actions for a given user in a given room.
   738|     Used in _rotate_notifs_before_txn to manipulate results from event_push_actions.
   739|     """
   740|     unread_count = attr.ib(type=int)
   741|     stream_ordering = attr.ib(type=int)
   742|     old_user_id = attr.ib(type=str)
   743|     notif_count = attr.ib(type=int)


# ====================================================================
# FILE: synapse/storage/databases/main/events.py
# Total hunks: 8
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| import itertools
     2| import logging
     3| from collections import OrderedDict, namedtuple
     4| from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Set, Tuple
     5| import attr
     6| from prometheus_client import Counter
     7| import synapse.metrics
     8| from synapse.api.constants import EventContentFields, EventTypes, RelationTypes
     9| from synapse.api.room_versions import RoomVersions
    10| from synapse.crypto.event_signing import compute_event_reference_hash
    11| from synapse.events import EventBase  # noqa: F401
    12| from synapse.events.snapshot import EventContext  # noqa: F401
    13| from synapse.logging.utils import log_function
    14| from synapse.storage._base import db_to_json, make_in_list_sql_clause
    15| from synapse.storage.database import DatabasePool, LoggingTransaction
    16| from synapse.storage.databases.main.search import SearchEntry
    17| from synapse.storage.util.id_generators import MultiWriterIdGenerator
    18| from synapse.types import StateMap, get_domain_from_id
    19| from synapse.util.frozenutils import frozendict_json_encoder
    20| from synapse.util.iterutils import batch_iter
    21| if TYPE_CHECKING:
    22|     from synapse.server import HomeServer
    23|     from synapse.storage.databases.main import DataStore
    24| logger = logging.getLogger(__name__)
    25| persist_event_counter = Counter("synapse_storage_events_persisted_events", "")
    26| event_counter = Counter(
    27|     "synapse_storage_events_persisted_events_sep",
    28|     "",
    29|     ["type", "origin_type", "origin_entity"],
    30| )
    31| def encode_json(json_object):
    32|     """
    33|     Encode a Python object as JSON and return it in a Unicode string.
    34|     """
    35|     out = frozendict_json_encoder.encode(json_object)
    36|     if isinstance(out, bytes):
    37|         out = out.decode("utf8")

# --- HUNK 2: Lines 45-166 ---
    45|         to_insert: Map of state to upsert into current state
    46|         no_longer_in_room: The server is not longer in the room, so the room
    47|             should e.g. be removed from `current_state_events` table.
    48|     """
    49|     to_delete = attr.ib(type=List[Tuple[str, str]])
    50|     to_insert = attr.ib(type=StateMap[str])
    51|     no_longer_in_room = attr.ib(type=bool, default=False)
    52| class PersistEventsStore:
    53|     """Contains all the functions for writing events to the database.
    54|     Should only be instantiated on one process (when using a worker mode setup).
    55|     Note: This is not part of the `DataStore` mixin.
    56|     """
    57|     def __init__(
    58|         self, hs: "HomeServer", db: DatabasePool, main_data_store: "DataStore"
    59|     ):
    60|         self.hs = hs
    61|         self.db_pool = db
    62|         self.store = main_data_store
    63|         self.database_engine = db.engine
    64|         self._clock = hs.get_clock()
    65|         self._instance_name = hs.get_instance_name()
    66|         self._ephemeral_messages_enabled = hs.config.enable_ephemeral_messages
    67|         self.is_mine_id = hs.is_mine_id
    68|         self._backfill_id_gen = (
    69|             self.store._backfill_id_gen
    70|         )  # type: MultiWriterIdGenerator
    71|         self._stream_id_gen = self.store._stream_id_gen  # type: MultiWriterIdGenerator
    72|         assert (
    73|             hs.get_instance_name() in hs.config.worker.writers.events
    74|         ), "Can only instantiate EventsStore on master"
    75|     async def _persist_events_and_state_updates(
    76|         self,
    77|         events_and_contexts: List[Tuple[EventBase, EventContext]],
    78|         current_state_for_room: Dict[str, StateMap[str]],
    79|         state_delta_for_room: Dict[str, DeltaState],
    80|         new_forward_extremeties: Dict[str, List[str]],
    81|         backfilled: bool = False,
    82|     ) -> None:
    83|         """Persist a set of events alongside updates to the current state and
    84|         forward extremities tables.
    85|         Args:
    86|             events_and_contexts:
    87|             current_state_for_room: Map from room_id to the current state of
    88|                 the room based on forward extremities
    89|             state_delta_for_room: Map from room_id to the delta to apply to
    90|                 room state
    91|             new_forward_extremities: Map from room_id to list of event IDs
    92|                 that are the new forward extremities of the room.
    93|             backfilled
    94|         Returns:
    95|             Resolves when the events have been persisted
    96|         """
    97|         if backfilled:
    98|             stream_ordering_manager = self._backfill_id_gen.get_next_mult(
    99|                 len(events_and_contexts)
   100|             )
   101|         else:
   102|             stream_ordering_manager = self._stream_id_gen.get_next_mult(
   103|                 len(events_and_contexts)
   104|             )
   105|         async with stream_ordering_manager as stream_orderings:
   106|             for (event, context), stream in zip(events_and_contexts, stream_orderings):
   107|                 event.internal_metadata.stream_ordering = stream
   108|             await self.db_pool.runInteraction(
   109|                 "persist_events",
   110|                 self._persist_events_txn,
   111|                 events_and_contexts=events_and_contexts,
   112|                 backfilled=backfilled,
   113|                 state_delta_for_room=state_delta_for_room,
   114|                 new_forward_extremeties=new_forward_extremeties,
   115|             )
   116|             persist_event_counter.inc(len(events_and_contexts))
   117|             if not backfilled:
   118|                 synapse.metrics.event_persisted_position.set(
   119|                     events_and_contexts[-1][0].internal_metadata.stream_ordering
   120|                 )
   121|             for event, context in events_and_contexts:
   122|                 if context.app_service:
   123|                     origin_type = "local"
   124|                     origin_entity = context.app_service.id
   125|                 elif self.hs.is_mine_id(event.sender):
   126|                     origin_type = "local"
   127|                     origin_entity = "*client*"
   128|                 else:
   129|                     origin_type = "remote"
   130|                     origin_entity = get_domain_from_id(event.sender)
   131|                 event_counter.labels(event.type, origin_type, origin_entity).inc()
   132|             for room_id, new_state in current_state_for_room.items():
   133|                 self.store.get_current_state_ids.prefill((room_id,), new_state)
   134|             for room_id, latest_event_ids in new_forward_extremeties.items():
   135|                 self.store.get_latest_event_ids_in_room.prefill(
   136|                     (room_id,), list(latest_event_ids)
   137|                 )
   138|     async def _get_events_which_are_prevs(self, event_ids: Iterable[str]) -> List[str]:
   139|         """Filter the supplied list of event_ids to get those which are prev_events of
   140|         existing (non-outlier/rejected) events.
   141|         Args:
   142|             event_ids: event ids to filter
   143|         Returns:
   144|             Filtered event ids
   145|         """
   146|         results = []  # type: List[str]
   147|         def _get_events_which_are_prevs_txn(txn, batch):
   148|             sql = """
   149|             SELECT prev_event_id, internal_metadata
   150|             FROM event_edges
   151|                 INNER JOIN events USING (event_id)
   152|                 LEFT JOIN rejections USING (event_id)
   153|                 LEFT JOIN event_json USING (event_id)
   154|             WHERE
   155|                 NOT events.outlier
   156|                 AND rejections.event_id IS NULL
   157|                 AND
   158|             """
   159|             clause, args = make_in_list_sql_clause(
   160|                 self.database_engine, "prev_event_id", batch
   161|             )
   162|             txn.execute(sql + clause, args)
   163|             results.extend(r[0] for r in txn if not db_to_json(r[1]).get("soft_failed"))
   164|         for chunk in batch_iter(event_ids, 100):
   165|             await self.db_pool.runInteraction(
   166|                 "_get_events_which_are_prevs", _get_events_which_are_prevs_txn, chunk

# --- HUNK 3: Lines 427-503 ---
   427|             values=[
   428|                 {"event_id": ev_id, "room_id": room_id}
   429|                 for room_id, new_extrem in new_forward_extremities.items()
   430|                 for ev_id in new_extrem
   431|             ],
   432|         )
   433|         self.db_pool.simple_insert_many_txn(
   434|             txn,
   435|             table="stream_ordering_to_exterm",
   436|             values=[
   437|                 {
   438|                     "room_id": room_id,
   439|                     "event_id": event_id,
   440|                     "stream_ordering": max_stream_order,
   441|                 }
   442|                 for room_id, new_extrem in new_forward_extremities.items()
   443|                 for event_id in new_extrem
   444|             ],
   445|         )
   446|     @classmethod
   447|     def _filter_events_and_contexts_for_duplicates(
   448|         cls, events_and_contexts: List[Tuple[EventBase, EventContext]]
   449|     ) -> List[Tuple[EventBase, EventContext]]:
   450|         """Ensure that we don't have the same event twice.
   451|         Pick the earliest non-outlier if there is one, else the earliest one.
   452|         Args:
   453|             events_and_contexts (list[(EventBase, EventContext)]):
   454|         Returns:
   455|             list[(EventBase, EventContext)]: filtered list
   456|         """
   457|         new_events_and_contexts = (
   458|             OrderedDict()
   459|         )  # type: OrderedDict[str, Tuple[EventBase, EventContext]]
   460|         for event, context in events_and_contexts:
   461|             prev_event_context = new_events_and_contexts.get(event.event_id)
   462|             if prev_event_context:
   463|                 if not event.internal_metadata.is_outlier():
   464|                     if prev_event_context[0].internal_metadata.is_outlier():
   465|                         new_events_and_contexts.pop(event.event_id, None)
   466|                         new_events_and_contexts[event.event_id] = (event, context)
   467|             else:
   468|                 new_events_and_contexts[event.event_id] = (event, context)
   469|         return list(new_events_and_contexts.values())
   470|     def _update_room_depths_txn(
   471|         self,
   472|         txn,
   473|         events_and_contexts: List[Tuple[EventBase, EventContext]],
   474|         backfilled: bool,
   475|     ):
   476|         """Update min_depth for each room
   477|         Args:
   478|             txn (twisted.enterprise.adbapi.Connection): db connection
   479|             events_and_contexts (list[(EventBase, EventContext)]): events
   480|                 we are persisting
   481|             backfilled (bool): True if the events were backfilled
   482|         """
   483|         depth_updates = {}  # type: Dict[str, int]
   484|         for event, context in events_and_contexts:
   485|             txn.call_after(self.store._invalidate_get_event_cache, event.event_id)
   486|             if not backfilled:
   487|                 txn.call_after(
   488|                     self.store._events_stream_cache.entity_has_changed,
   489|                     event.room_id,
   490|                     event.internal_metadata.stream_ordering,
   491|                 )
   492|             if not event.internal_metadata.is_outlier() and not context.rejected:
   493|                 depth_updates[event.room_id] = max(
   494|                     event.depth, depth_updates.get(event.room_id, event.depth)
   495|                 )
   496|         for room_id, depth in depth_updates.items():
   497|             self._update_min_depth_for_room_txn(txn, room_id, depth)
   498|     def _update_outliers_txn(self, txn, events_and_contexts):
   499|         """Update any outliers with new event info.
   500|         This turns outliers into ex-outliers (unless the new event was
   501|         rejected).
   502|         Args:
   503|             txn (twisted.enterprise.adbapi.Connection): db connection

# --- HUNK 4: Lines 563-603 ---
   563|             txn,
   564|             table="event_json",
   565|             values=[
   566|                 {
   567|                     "event_id": event.event_id,
   568|                     "room_id": event.room_id,
   569|                     "internal_metadata": encode_json(
   570|                         event.internal_metadata.get_dict()
   571|                     ),
   572|                     "json": encode_json(event_dict(event)),
   573|                     "format_version": event.format_version,
   574|                 }
   575|                 for event, _ in events_and_contexts
   576|             ],
   577|         )
   578|         self.db_pool.simple_insert_many_txn(
   579|             txn,
   580|             table="events",
   581|             values=[
   582|                 {
   583|                     "instance_name": self._instance_name,
   584|                     "stream_ordering": event.internal_metadata.stream_ordering,
   585|                     "topological_ordering": event.depth,
   586|                     "depth": event.depth,
   587|                     "event_id": event.event_id,
   588|                     "room_id": event.room_id,
   589|                     "type": event.type,
   590|                     "processed": True,
   591|                     "outlier": event.internal_metadata.is_outlier(),
   592|                     "origin_server_ts": int(event.origin_server_ts),
   593|                     "received_ts": self._clock.time_msec(),
   594|                     "sender": event.sender,
   595|                     "contains_url": (
   596|                         "url" in event.content and isinstance(event.content["url"], str)
   597|                     ),
   598|                 }
   599|                 for event, _ in events_and_contexts
   600|             ],
   601|         )
   602|         for event, _ in events_and_contexts:
   603|             if not event.internal_metadata.is_redacted():

# --- HUNK 5: Lines 793-846 ---
   793|         Args:
   794|             txn (cursor):
   795|             events (list): list of Events.
   796|         """
   797|         vals = []
   798|         for event in events:
   799|             ref_alg, ref_hash_bytes = compute_event_reference_hash(event)
   800|             vals.append(
   801|                 {
   802|                     "event_id": event.event_id,
   803|                     "algorithm": ref_alg,
   804|                     "hash": memoryview(ref_hash_bytes),
   805|                 }
   806|             )
   807|         self.db_pool.simple_insert_many_txn(
   808|             txn, table="event_reference_hashes", values=vals
   809|         )
   810|     def _store_room_members_txn(self, txn, events, backfilled):
   811|         """Store a room member in the database.
   812|         """
   813|         def str_or_none(val: Any) -> Optional[str]:
   814|             return val if isinstance(val, str) else None
   815|         self.db_pool.simple_insert_many_txn(
   816|             txn,
   817|             table="room_memberships",
   818|             values=[
   819|                 {
   820|                     "event_id": event.event_id,
   821|                     "user_id": event.state_key,
   822|                     "sender": event.user_id,
   823|                     "room_id": event.room_id,
   824|                     "membership": event.membership,
   825|                     "display_name": str_or_none(event.content.get("displayname")),
   826|                     "avatar_url": str_or_none(event.content.get("avatar_url")),
   827|                 }
   828|                 for event in events
   829|             ],
   830|         )
   831|         for event in events:
   832|             txn.call_after(
   833|                 self.store._membership_stream_cache.entity_has_changed,
   834|                 event.state_key,
   835|                 event.internal_metadata.stream_ordering,
   836|             )
   837|             txn.call_after(
   838|                 self.store.get_invited_rooms_for_local_user.invalidate,
   839|                 (event.state_key,),
   840|             )
   841|             if (
   842|                 self.is_mine_id(event.state_key)
   843|                 and not backfilled
   844|                 and event.internal_metadata.is_outlier()
   845|                 and event.internal_metadata.is_out_of_band_membership()
   846|             ):

# --- HUNK 6: Lines 1077-1117 ---
  1077|             table="event_edges",
  1078|             values=[
  1079|                 {
  1080|                     "event_id": ev.event_id,
  1081|                     "prev_event_id": e_id,
  1082|                     "room_id": ev.room_id,
  1083|                     "is_state": False,
  1084|                 }
  1085|                 for ev in events
  1086|                 for e_id in ev.prev_event_ids()
  1087|             ],
  1088|         )
  1089|         self._update_backward_extremeties(txn, events)
  1090|     def _update_backward_extremeties(self, txn, events):
  1091|         """Updates the event_backward_extremities tables based on the new/updated
  1092|         events being persisted.
  1093|         This is called for new events *and* for events that were outliers, but
  1094|         are now being persisted as non-outliers.
  1095|         Forward extremities are handled when we first start persisting the events.
  1096|         """
  1097|         events_by_room = {}  # type: Dict[str, List[EventBase]]
  1098|         for ev in events:
  1099|             events_by_room.setdefault(ev.room_id, []).append(ev)
  1100|         query = (
  1101|             "INSERT INTO event_backward_extremities (event_id, room_id)"
  1102|             " SELECT ?, ? WHERE NOT EXISTS ("
  1103|             " SELECT 1 FROM event_backward_extremities"
  1104|             " WHERE event_id = ? AND room_id = ?"
  1105|             " )"
  1106|             " AND NOT EXISTS ("
  1107|             " SELECT 1 FROM events WHERE event_id = ? AND room_id = ? "
  1108|             " AND outlier = ?"
  1109|             " )"
  1110|         )
  1111|         txn.executemany(
  1112|             query,
  1113|             [
  1114|                 (e_id, ev.room_id, e_id, ev.room_id, e_id, ev.room_id, False)
  1115|                 for ev in events
  1116|                 for e_id in ev.prev_event_ids()
  1117|                 if not ev.internal_metadata.is_outlier()


# ====================================================================
# FILE: synapse/storage/databases/main/events_bg_updates.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| import logging
     2| from synapse.api.constants import EventContentFields
     3| from synapse.storage._base import SQLBaseStore, db_to_json, make_in_list_sql_clause
     4| from synapse.storage.database import DatabasePool
     5| logger = logging.getLogger(__name__)
     6| class EventsBackgroundUpdatesStore(SQLBaseStore):
     7|     EVENT_ORIGIN_SERVER_TS_NAME = "event_origin_server_ts"
     8|     EVENT_FIELDS_SENDER_URL_UPDATE_NAME = "event_fields_sender_url"
     9|     DELETE_SOFT_FAILED_EXTREMITIES = "delete_soft_failed_extremities"
    10|     def __init__(self, database: DatabasePool, db_conn, hs):
    11|         super().__init__(database, db_conn, hs)
    12|         self.db_pool.updates.register_background_update_handler(
    13|             self.EVENT_ORIGIN_SERVER_TS_NAME, self._background_reindex_origin_server_ts
    14|         )
    15|         self.db_pool.updates.register_background_update_handler(
    16|             self.EVENT_FIELDS_SENDER_URL_UPDATE_NAME,
    17|             self._background_reindex_fields_sender,
    18|         )
    19|         self.db_pool.updates.register_background_index_update(
    20|             "event_contains_url_index",
    21|             index_name="event_contains_url_index",
    22|             table="events",
    23|             columns=["room_id", "topological_ordering", "stream_ordering"],
    24|             where_clause="contains_url = true AND outlier = false",
    25|         )
    26|         self.db_pool.updates.register_background_index_update(
    27|             "event_search_event_id_idx",
    28|             index_name="event_search_event_id_idx",
    29|             table="event_search",
    30|             columns=["event_id"],
    31|             unique=True,


# ====================================================================
# FILE: synapse/storage/databases/main/events_worker.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-108 ---
     1| import itertools
     2| import logging
     3| import threading
     4| from collections import namedtuple
     5| from typing import Dict, Iterable, List, Optional, Tuple, overload
     6| from constantly import NamedConstant, Names
     7| from typing_extensions import Literal
     8| from twisted.internet import defer
     9| from synapse.api.constants import EventTypes
    10| from synapse.api.errors import NotFoundError, SynapseError
    11| from synapse.api.room_versions import (
    12|     KNOWN_ROOM_VERSIONS,
    13|     EventFormatVersions,
    14|     RoomVersions,
    15| )
    16| from synapse.events import EventBase, make_event_from_dict
    17| from synapse.events.utils import prune_event
    18| from synapse.logging.context import PreserveLoggingContext, current_context
    19| from synapse.metrics.background_process_metrics import run_as_background_process
    20| from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
    21| from synapse.replication.tcp.streams import BackfillStream
    22| from synapse.replication.tcp.streams.events import EventsStream
    23| from synapse.storage._base import SQLBaseStore, db_to_json, make_in_list_sql_clause
    24| from synapse.storage.database import DatabasePool
    25| from synapse.storage.engines import PostgresEngine
    26| from synapse.storage.util.id_generators import MultiWriterIdGenerator, StreamIdGenerator
    27| from synapse.types import Collection, get_domain_from_id
    28| from synapse.util.caches.descriptors import Cache, cached
    29| from synapse.util.iterutils import batch_iter
    30| from synapse.util.metrics import Measure
    31| logger = logging.getLogger(__name__)
    32| EVENT_QUEUE_THREADS = 3  # Max number of threads that will fetch events
    33| EVENT_QUEUE_ITERATIONS = 3  # No. times we block waiting for requests for events
    34| EVENT_QUEUE_TIMEOUT_S = 0.1  # Timeout when waiting for requests for events
    35| _EventCacheEntry = namedtuple("_EventCacheEntry", ("event", "redacted_event"))
    36| class EventRedactBehaviour(Names):
    37|     """
    38|     What to do when retrieving a redacted event from the database.
    39|     """
    40|     AS_IS = NamedConstant()
    41|     REDACT = NamedConstant()
    42|     BLOCK = NamedConstant()
    43| class EventsWorkerStore(SQLBaseStore):
    44|     def __init__(self, database: DatabasePool, db_conn, hs):
    45|         super().__init__(database, db_conn, hs)
    46|         if isinstance(database.engine, PostgresEngine):
    47|             self._stream_id_gen = MultiWriterIdGenerator(
    48|                 db_conn=db_conn,
    49|                 db=database,
    50|                 stream_name="events",
    51|                 instance_name=hs.get_instance_name(),
    52|                 table="events",
    53|                 instance_column="instance_name",
    54|                 id_column="stream_ordering",
    55|                 sequence_name="events_stream_seq",
    56|                 writers=hs.config.worker.writers.events,
    57|             )
    58|             self._backfill_id_gen = MultiWriterIdGenerator(
    59|                 db_conn=db_conn,
    60|                 db=database,
    61|                 stream_name="backfill",
    62|                 instance_name=hs.get_instance_name(),
    63|                 table="events",
    64|                 instance_column="instance_name",
    65|                 id_column="stream_ordering",
    66|                 sequence_name="events_backfill_stream_seq",
    67|                 positive=False,
    68|                 writers=hs.config.worker.writers.events,
    69|             )
    70|         else:
    71|             if hs.get_instance_name() in hs.config.worker.writers.events:
    72|                 self._stream_id_gen = StreamIdGenerator(
    73|                     db_conn, "events", "stream_ordering",
    74|                 )
    75|                 self._backfill_id_gen = StreamIdGenerator(
    76|                     db_conn,
    77|                     "events",
    78|                     "stream_ordering",
    79|                     step=-1,
    80|                     extra_tables=[("ex_outlier_stream", "event_stream_ordering")],
    81|                 )
    82|             else:
    83|                 self._stream_id_gen = SlavedIdTracker(
    84|                     db_conn, "events", "stream_ordering"
    85|                 )
    86|                 self._backfill_id_gen = SlavedIdTracker(
    87|                     db_conn, "events", "stream_ordering", step=-1
    88|                 )
    89|         self._get_event_cache = Cache(
    90|             "*getEvent*",
    91|             keylen=3,
    92|             max_entries=hs.config.caches.event_cache_size,
    93|             apply_cache_factor_from_config=False,
    94|         )
    95|         self._event_fetch_lock = threading.Condition()
    96|         self._event_fetch_list = []
    97|         self._event_fetch_ongoing = 0
    98|     def process_replication_rows(self, stream_name, instance_name, token, rows):
    99|         if stream_name == EventsStream.NAME:
   100|             self._stream_id_gen.advance(instance_name, token)
   101|         elif stream_name == BackfillStream.NAME:
   102|             self._backfill_id_gen.advance(instance_name, -token)
   103|         super().process_replication_rows(stream_name, instance_name, token, rows)
   104|     async def get_received_ts(self, event_id: str) -> Optional[int]:
   105|         """Get received_ts (when it was persisted) for the event.
   106|         Raises an exception for unknown events.
   107|         Args:
   108|             event_id: The event ID to query.


# ====================================================================
# FILE: synapse/storage/databases/main/group_server.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1068-1108 ---
  1068|                         table="group_attestations_remote",
  1069|                         values={
  1070|                             "group_id": group_id,
  1071|                             "user_id": user_id,
  1072|                             "valid_until_ms": remote_attestation["valid_until_ms"],
  1073|                             "attestation_json": json_encoder.encode(remote_attestation),
  1074|                         },
  1075|                     )
  1076|             else:
  1077|                 self.db_pool.simple_delete_txn(
  1078|                     txn,
  1079|                     table="group_attestations_renewals",
  1080|                     keyvalues={"group_id": group_id, "user_id": user_id},
  1081|                 )
  1082|                 self.db_pool.simple_delete_txn(
  1083|                     txn,
  1084|                     table="group_attestations_remote",
  1085|                     keyvalues={"group_id": group_id, "user_id": user_id},
  1086|                 )
  1087|             return next_id
  1088|         async with self._group_updates_id_gen.get_next() as next_id:
  1089|             res = await self.db_pool.runInteraction(
  1090|                 "register_user_group_membership",
  1091|                 _register_user_group_membership_txn,
  1092|                 next_id,
  1093|             )
  1094|         return res
  1095|     async def create_group(
  1096|         self, group_id, user_id, name, avatar_url, short_description, long_description
  1097|     ) -> None:
  1098|         await self.db_pool.simple_insert(
  1099|             table="groups",
  1100|             values={
  1101|                 "group_id": group_id,
  1102|                 "name": name,
  1103|                 "avatar_url": avatar_url,
  1104|                 "short_description": short_description,
  1105|                 "long_description": long_description,
  1106|                 "is_public": True,
  1107|             },
  1108|             desc="create_group",


# ====================================================================
# FILE: synapse/storage/databases/main/media_repository.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-84 ---
     1| from typing import Any, Dict, Iterable, List, Optional, Tuple
     2| from synapse.storage._base import SQLBaseStore
     3| from synapse.storage.database import DatabasePool
     4| BG_UPDATE_REMOVE_MEDIA_REPO_INDEX_WITHOUT_METHOD = (
     5|     "media_repository_drop_index_wo_method"
     6| )
     7| class MediaRepositoryBackgroundUpdateStore(SQLBaseStore):
     8|     def __init__(self, database: DatabasePool, db_conn, hs):
     9|         super().__init__(database, db_conn, hs)
    10|         self.db_pool.updates.register_background_index_update(
    11|             update_name="local_media_repository_url_idx",
    12|             index_name="local_media_repository_url_idx",
    13|             table="local_media_repository",
    14|             columns=["created_ts"],
    15|             where_clause="url_cache IS NOT NULL",
    16|         )
    17|         self.db_pool.updates.register_background_index_update(
    18|             update_name="local_media_repository_thumbnails_method_idx",
    19|             index_name="local_media_repository_thumbn_media_id_width_height_method_key",
    20|             table="local_media_repository_thumbnails",
    21|             columns=[
    22|                 "media_id",
    23|                 "thumbnail_width",
    24|                 "thumbnail_height",
    25|                 "thumbnail_type",
    26|                 "thumbnail_method",
    27|             ],
    28|             unique=True,
    29|         )
    30|         self.db_pool.updates.register_background_index_update(
    31|             update_name="remote_media_repository_thumbnails_method_idx",
    32|             index_name="remote_media_repository_thumbn_media_origin_id_width_height_method_key",
    33|             table="remote_media_cache_thumbnails",
    34|             columns=[
    35|                 "media_origin",
    36|                 "media_id",
    37|                 "thumbnail_width",
    38|                 "thumbnail_height",
    39|                 "thumbnail_type",
    40|                 "thumbnail_method",
    41|             ],
    42|             unique=True,
    43|         )
    44|         self.db_pool.updates.register_background_update_handler(
    45|             BG_UPDATE_REMOVE_MEDIA_REPO_INDEX_WITHOUT_METHOD,
    46|             self._drop_media_index_without_method,
    47|         )
    48|     async def _drop_media_index_without_method(self, progress, batch_size):
    49|         def f(txn):
    50|             txn.execute(
    51|                 "ALTER TABLE local_media_repository_thumbnails DROP CONSTRAINT IF EXISTS local_media_repository_thumbn_media_id_thumbnail_width_thum_key"
    52|             )
    53|             txn.execute(
    54|                 "ALTER TABLE remote_media_cache_thumbnails DROP CONSTRAINT IF EXISTS remote_media_repository_thumbn_media_id_thumbnail_width_thum_key"
    55|             )
    56|         await self.db_pool.runInteraction("drop_media_indices_without_method", f)
    57|         await self.db_pool.updates._end_background_update(
    58|             BG_UPDATE_REMOVE_MEDIA_REPO_INDEX_WITHOUT_METHOD
    59|         )
    60|         return 1
    61| class MediaRepositoryStore(MediaRepositoryBackgroundUpdateStore):
    62|     """Persistence for attachments and avatars"""
    63|     def __init__(self, database: DatabasePool, db_conn, hs):
    64|         super().__init__(database, db_conn, hs)
    65|     async def get_local_media(self, media_id: str) -> Optional[Dict[str, Any]]:
    66|         """Get the metadata for a local piece of media
    67|         Returns:
    68|             None if the media_id doesn't exist.
    69|         """
    70|         return await self.db_pool.simple_select_one(
    71|             "local_media_repository",
    72|             {"media_id": media_id},
    73|             (
    74|                 "media_type",
    75|                 "media_length",
    76|                 "upload_name",
    77|                 "created_ts",
    78|                 "quarantined_by",
    79|                 "url_cache",
    80|             ),
    81|             allow_none=True,
    82|             desc="get_local_media",
    83|         )
    84|     async def store_local_media(


# ====================================================================
# FILE: synapse/storage/databases/main/metrics.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-70 ---
     1| from synapse.metrics import GaugeBucketCollector
     2| from synapse.metrics.background_process_metrics import run_as_background_process
     3| from synapse.storage._base import SQLBaseStore
     4| from synapse.storage.database import DatabasePool
     5| from synapse.storage.databases.main.event_push_actions import (
     6|     EventPushActionsWorkerStore,
     7| )
     8| _extremities_collecter = GaugeBucketCollector(
     9|     "synapse_forward_extremities",
    10|     "Number of rooms on the server with the given number of forward extremities"
    11|     " or fewer",
    12|     buckets=[1, 2, 3, 5, 7, 10, 15, 20, 50, 100, 200, 500],
    13| )
    14| _excess_state_events_collecter = GaugeBucketCollector(
    15|     "synapse_excess_extremity_events",
    16|     "Number of rooms on the server with the given number of excess extremity "
    17|     "events, or fewer",
    18|     buckets=[0] + [1 << n for n in range(12)],
    19| )
    20| class ServerMetricsStore(EventPushActionsWorkerStore, SQLBaseStore):
    21|     """Functions to pull various metrics from the DB, for e.g. phone home
    22|     stats and prometheus metrics.
    23|     """
    24|     def __init__(self, database: DatabasePool, db_conn, hs):
    25|         super().__init__(database, db_conn, hs)
    26|         def read_forward_extremities():
    27|             return run_as_background_process(
    28|                 "read_forward_extremities", self._read_forward_extremities
    29|             )
    30|         hs.get_clock().looping_call(read_forward_extremities, 60 * 60 * 1000)
    31|     async def _read_forward_extremities(self):
    32|         def fetch(txn):
    33|             txn.execute(
    34|                 """
    35|                 SELECT t1.c, t2.c
    36|                 FROM (
    37|                     SELECT room_id, COUNT(*) c FROM event_forward_extremities
    38|                     GROUP BY room_id
    39|                 ) t1 LEFT JOIN (
    40|                     SELECT room_id, COUNT(*) c FROM current_state_events
    41|                     GROUP BY room_id
    42|                 ) t2 ON t1.room_id = t2.room_id
    43|                 """
    44|             )
    45|             return txn.fetchall()
    46|         res = await self.db_pool.runInteraction("read_forward_extremities", fetch)
    47|         _extremities_collecter.update_data(x[0] for x in res)
    48|         _excess_state_events_collecter.update_data(
    49|             (x[0] - 1) * x[1] for x in res if x[1]
    50|         )
    51|     async def count_daily_messages(self):
    52|         """
    53|         Returns an estimate of the number of messages sent in the last day.
    54|         If it has been significantly less or more than one day since the last
    55|         call to this function, it will return None.
    56|         """
    57|         def _count_messages(txn):
    58|             sql = """
    59|                 SELECT COALESCE(COUNT(*), 0) FROM events
    60|                 WHERE type = 'm.room.message'
    61|                 AND stream_ordering > ?
    62|             """
    63|             txn.execute(sql, (self.stream_ordering_day_ago,))
    64|             (count,) = txn.fetchone()
    65|             return count
    66|         return await self.db_pool.runInteraction("count_messages", _count_messages)
    67|     async def count_daily_sent_messages(self):
    68|         def _count_messages(txn):
    69|             like_clause = "%:" + self.hs.hostname
    70|             sql = """


# ====================================================================
# FILE: synapse/storage/databases/main/monthly_active_users.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-46 ---
     1| import logging
     2| from typing import Dict, List
     3| from synapse.storage._base import SQLBaseStore
     4| from synapse.storage.database import DatabasePool, make_in_list_sql_clause
     5| from synapse.util.caches.descriptors import cached
     6| logger = logging.getLogger(__name__)
     7| LAST_SEEN_GRANULARITY = 60 * 60 * 1000
     8| class MonthlyActiveUsersWorkerStore(SQLBaseStore):
     9|     def __init__(self, database: DatabasePool, db_conn, hs):
    10|         super().__init__(database, db_conn, hs)
    11|         self._clock = hs.get_clock()
    12|         self.hs = hs
    13|     @cached(num_args=0)
    14|     async def get_monthly_active_count(self) -> int:
    15|         """Generates current count of monthly active users
    16|         Returns:
    17|             Number of current monthly active users
    18|         """
    19|         def _count_users(txn):
    20|             sql = """
    21|                 SELECT COALESCE(count(*), 0)
    22|                 FROM monthly_active_users
    23|                     LEFT JOIN users
    24|                     ON monthly_active_users.user_id=users.name
    25|                 WHERE (users.appservice_id IS NULL OR users.appservice_id = '');
    26|             """
    27|             txn.execute(sql)
    28|             (count,) = txn.fetchone()
    29|             return count
    30|         return await self.db_pool.runInteraction("count_users", _count_users)
    31|     @cached(num_args=0)
    32|     async def get_monthly_active_count_by_service(self) -> Dict[str, int]:
    33|         """Generates current count of monthly active users broken down by service.
    34|         A service is typically an appservice but also includes native matrix users.
    35|         Since the `monthly_active_users` table is populated from the `user_ips` table
    36|         `config.track_appservice_user_ips` must be set to `true` for this
    37|         method to return anything other than native matrix users.
    38|         Returns:
    39|             A mapping between app_service_id and the number of occurrences.
    40|         """
    41|         def _count_users_by_service(txn):
    42|             sql = """
    43|                 SELECT COALESCE(appservice_id, 'native'), COALESCE(count(*), 0)
    44|                 FROM monthly_active_users
    45|                 LEFT JOIN users ON monthly_active_users.user_id=users.name
    46|                 GROUP BY appservice_id;

# --- HUNK 2: Lines 68-108 ---
    68|                 users.append(user_id)
    69|         return users
    70|     @cached(num_args=1)
    71|     async def user_last_seen_monthly_active(self, user_id: str) -> int:
    72|         """
    73|         Checks if a given user is part of the monthly active user group
    74|         Arguments:
    75|             user_id: user to add/update
    76|         Return:
    77|             Timestamp since last seen, None if never seen
    78|         """
    79|         return await self.db_pool.simple_select_one_onecol(
    80|             table="monthly_active_users",
    81|             keyvalues={"user_id": user_id},
    82|             retcol="timestamp",
    83|             allow_none=True,
    84|             desc="user_last_seen_monthly_active",
    85|         )
    86| class MonthlyActiveUsersStore(MonthlyActiveUsersWorkerStore):
    87|     def __init__(self, database: DatabasePool, db_conn, hs):
    88|         super().__init__(database, db_conn, hs)
    89|         self._limit_usage_by_mau = hs.config.limit_usage_by_mau
    90|         self._mau_stats_only = hs.config.mau_stats_only
    91|         self._max_mau_value = hs.config.max_mau_value
    92|         self.db_pool.new_transaction(
    93|             db_conn,
    94|             "initialise_mau_threepids",
    95|             [],
    96|             [],
    97|             self._initialise_reserved_users,
    98|             hs.config.mau_limits_reserved_threepids[: self._max_mau_value],
    99|         )
   100|     def _initialise_reserved_users(self, txn, threepids):
   101|         """Ensures that reserved threepids are accounted for in the MAU table, should
   102|         be called on start up.
   103|         Args:
   104|             txn (cursor):
   105|             threepids (list[dict]): List of threepid dicts to reserve
   106|         """
   107|         for tp in threepids:
   108|             user_id = self.get_user_id_by_threepid_txn(txn, tp["medium"], tp["address"])


# ====================================================================
# FILE: synapse/storage/databases/main/presence.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| from typing import List, Tuple
     2| from synapse.api.presence import UserPresenceState
     3| from synapse.storage._base import SQLBaseStore, make_in_list_sql_clause
     4| from synapse.util.caches.descriptors import cached, cachedList
     5| from synapse.util.iterutils import batch_iter
     6| class PresenceStore(SQLBaseStore):
     7|     async def update_presence(self, presence_states):
     8|         stream_ordering_manager = self._presence_id_gen.get_next_mult(
     9|             len(presence_states)
    10|         )
    11|         async with stream_ordering_manager as stream_orderings:
    12|             await self.db_pool.runInteraction(
    13|                 "update_presence",
    14|                 self._update_presence_txn,
    15|                 stream_orderings,
    16|                 presence_states,
    17|             )
    18|         return stream_orderings[-1], self._presence_id_gen.get_current_token()
    19|     def _update_presence_txn(self, txn, stream_orderings, presence_states):
    20|         for stream_id, state in zip(stream_orderings, presence_states):
    21|             txn.call_after(
    22|                 self.presence_stream_cache.entity_has_changed, state.user_id, stream_id
    23|             )
    24|             txn.call_after(self._get_presence_for_user.invalidate, (state.user_id,))
    25|         self.db_pool.simple_insert_many_txn(
    26|             txn,
    27|             table="presence_stream",
    28|             values=[
    29|                 {
    30|                     "stream_id": stream_id,
    31|                     "user_id": state.user_id,


# ====================================================================
# FILE: synapse/storage/databases/main/purge_events.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 3-51 ---
     3| from synapse.api.errors import SynapseError
     4| from synapse.storage._base import SQLBaseStore
     5| from synapse.storage.databases.main.state import StateGroupWorkerStore
     6| from synapse.types import RoomStreamToken
     7| logger = logging.getLogger(__name__)
     8| class PurgeEventsStore(StateGroupWorkerStore, SQLBaseStore):
     9|     async def purge_history(
    10|         self, room_id: str, token: str, delete_local_events: bool
    11|     ) -> Set[int]:
    12|         """Deletes room history before a certain point
    13|         Args:
    14|             room_id:
    15|             token: A topological token to delete events before
    16|             delete_local_events:
    17|                 if True, we will delete local events as well as remote ones
    18|                 (instead of just marking them as outliers and deleting their
    19|                 state groups).
    20|         Returns:
    21|             The set of state groups that are referenced by deleted events.
    22|         """
    23|         parsed_token = await RoomStreamToken.parse(self, token)
    24|         return await self.db_pool.runInteraction(
    25|             "purge_history",
    26|             self._purge_history_txn,
    27|             room_id,
    28|             parsed_token,
    29|             delete_local_events,
    30|         )
    31|     def _purge_history_txn(self, txn, room_id, token, delete_local_events):
    32|         txn.execute("DROP TABLE IF EXISTS events_to_purge")
    33|         txn.execute(
    34|             "CREATE TEMPORARY TABLE events_to_purge ("
    35|             "    event_id TEXT NOT NULL,"
    36|             "    should_delete BOOLEAN NOT NULL"
    37|             ")"
    38|         )
    39|         txn.execute(
    40|             "SELECT e.event_id, e.depth FROM events as e "
    41|             "INNER JOIN event_forward_extremities as f "
    42|             "ON e.event_id = f.event_id "
    43|             "AND e.room_id = f.room_id "
    44|             "WHERE f.room_id = ?",
    45|             (room_id,),
    46|         )
    47|         rows = txn.fetchall()
    48|         max_depth = max(row[1] for row in rows)
    49|         if max_depth < token.topological:
    50|             raise SynapseError(
    51|                 400, "topological_ordering is greater than forward extremeties"

# --- HUNK 2: Lines 191-231 ---
   191|             "event_push_actions_staging",
   192|             "event_reference_hashes",
   193|             "event_relations",
   194|             "event_to_state_groups",
   195|             "redactions",
   196|             "rejections",
   197|             "state_events",
   198|         ):
   199|             logger.info("[purge] removing %s from %s", room_id, table)
   200|             txn.execute(
   201|                 """
   202|                 DELETE FROM %s WHERE event_id IN (
   203|                   SELECT event_id FROM events WHERE room_id=?
   204|                 )
   205|                 """
   206|                 % (table,),
   207|                 (room_id,),
   208|             )
   209|         for table in (
   210|             "current_state_events",
   211|             "destination_rooms",
   212|             "event_backward_extremities",
   213|             "event_forward_extremities",
   214|             "event_json",
   215|             "event_push_actions",
   216|             "event_search",
   217|             "events",
   218|             "group_rooms",
   219|             "public_room_list_stream",
   220|             "receipts_graph",
   221|             "receipts_linearized",
   222|             "room_aliases",
   223|             "room_depth",
   224|             "room_memberships",
   225|             "room_stats_state",
   226|             "room_stats_current",
   227|             "room_stats_historical",
   228|             "room_stats_earliest_token",
   229|             "rooms",
   230|             "stream_ordering_to_exterm",
   231|             "users_in_public_rooms",


# ====================================================================
# FILE: synapse/storage/databases/main/push_rule.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-71 ---
     1| import abc
     2| import logging
     3| from typing import List, Tuple, Union
     4| from synapse.api.errors import NotFoundError, StoreError
     5| from synapse.push.baserules import list_with_base_rules
     6| from synapse.replication.slave.storage._slaved_id_tracker import SlavedIdTracker
     7| from synapse.storage._base import SQLBaseStore, db_to_json
     8| from synapse.storage.database import DatabasePool
     9| from synapse.storage.databases.main.appservice import ApplicationServiceWorkerStore
    10| from synapse.storage.databases.main.events_worker import EventsWorkerStore
    11| from synapse.storage.databases.main.pusher import PusherWorkerStore
    12| from synapse.storage.databases.main.receipts import ReceiptsWorkerStore
    13| from synapse.storage.databases.main.roommember import RoomMemberWorkerStore
    14| from synapse.storage.engines import PostgresEngine, Sqlite3Engine
    15| from synapse.storage.push_rule import InconsistentRuleException, RuleNotFoundException
    16| from synapse.storage.util.id_generators import StreamIdGenerator
    17| from synapse.util import json_encoder
    18| from synapse.util.caches.descriptors import cached, cachedList
    19| from synapse.util.caches.stream_change_cache import StreamChangeCache
    20| logger = logging.getLogger(__name__)
    21| def _load_rules(rawrules, enabled_map, use_new_defaults=False):
    22|     ruleslist = []
    23|     for rawrule in rawrules:
    24|         rule = dict(rawrule)
    25|         rule["conditions"] = db_to_json(rawrule["conditions"])
    26|         rule["actions"] = db_to_json(rawrule["actions"])
    27|         rule["default"] = False
    28|         ruleslist.append(rule)
    29|     rules = list(list_with_base_rules(ruleslist, use_new_defaults))
    30|     for i, rule in enumerate(rules):
    31|         rule_id = rule["rule_id"]
    32|         if rule_id in enabled_map:
    33|             if rule.get("enabled", True) != bool(enabled_map[rule_id]):
    34|                 rule = dict(rule)
    35|                 rule["enabled"] = bool(enabled_map[rule_id])
    36|                 rules[i] = rule
    37|     return rules
    38| class PushRulesWorkerStore(
    39|     ApplicationServiceWorkerStore,
    40|     ReceiptsWorkerStore,
    41|     PusherWorkerStore,
    42|     RoomMemberWorkerStore,
    43|     EventsWorkerStore,
    44|     SQLBaseStore,
    45|     metaclass=abc.ABCMeta,
    46| ):
    47|     """This is an abstract base class where subclasses must implement
    48|     `get_max_push_rules_stream_id` which can be called in the initializer.
    49|     """
    50|     def __init__(self, database: DatabasePool, db_conn, hs):
    51|         super().__init__(database, db_conn, hs)
    52|         if hs.config.worker.worker_app is None:
    53|             self._push_rules_stream_id_gen = StreamIdGenerator(
    54|                 db_conn, "push_rules_stream", "stream_id"
    55|             )  # type: Union[StreamIdGenerator, SlavedIdTracker]
    56|         else:
    57|             self._push_rules_stream_id_gen = SlavedIdTracker(
    58|                 db_conn, "push_rules_stream", "stream_id"
    59|             )
    60|         push_rules_prefill, push_rules_id = self.db_pool.get_cache_dict(
    61|             db_conn,
    62|             "push_rules_stream",
    63|             entity_column="user_id",
    64|             stream_column="stream_id",
    65|             max_value=self.get_max_push_rules_stream_id(),
    66|         )
    67|         self.push_rules_stream_cache = StreamChangeCache(
    68|             "PushRulesStreamChangeCache",
    69|             push_rules_id,
    70|             prefilled_cache=push_rules_prefill,
    71|         )

# --- HUNK 2: Lines 241-281 ---
   241|             if len(updates) == limit:
   242|                 limited = True
   243|                 upper_bound = updates[-1][0]
   244|             return updates, upper_bound, limited
   245|         return await self.db_pool.runInteraction(
   246|             "get_all_push_rule_updates", get_all_push_rule_updates_txn
   247|         )
   248| class PushRuleStore(PushRulesWorkerStore):
   249|     async def add_push_rule(
   250|         self,
   251|         user_id,
   252|         rule_id,
   253|         priority_class,
   254|         conditions,
   255|         actions,
   256|         before=None,
   257|         after=None,
   258|     ) -> None:
   259|         conditions_json = json_encoder.encode(conditions)
   260|         actions_json = json_encoder.encode(actions)
   261|         async with self._push_rules_stream_id_gen.get_next() as stream_id:
   262|             event_stream_ordering = self._stream_id_gen.get_current_token()
   263|             if before or after:
   264|                 await self.db_pool.runInteraction(
   265|                     "_add_push_rule_relative_txn",
   266|                     self._add_push_rule_relative_txn,
   267|                     stream_id,
   268|                     event_stream_ordering,
   269|                     user_id,
   270|                     rule_id,
   271|                     priority_class,
   272|                     conditions_json,
   273|                     actions_json,
   274|                     before,
   275|                     after,
   276|                 )
   277|             else:
   278|                 await self.db_pool.runInteraction(
   279|                     "_add_push_rule_highest_priority_txn",
   280|                     self._add_push_rule_highest_priority_txn,
   281|                     stream_id,

# --- HUNK 3: Lines 409-625 ---
   409|                     "priority": priority,
   410|                     "conditions": conditions_json,
   411|                     "actions": actions_json,
   412|                 },
   413|             )
   414|         if update_stream:
   415|             self._insert_push_rules_update_txn(
   416|                 txn,
   417|                 stream_id,
   418|                 event_stream_ordering,
   419|                 user_id,
   420|                 rule_id,
   421|                 op="ADD",
   422|                 data={
   423|                     "priority_class": priority_class,
   424|                     "priority": priority,
   425|                     "conditions": conditions_json,
   426|                     "actions": actions_json,
   427|                 },
   428|             )
   429|         if isinstance(self.database_engine, PostgresEngine):
   430|             sql = """
   431|                 INSERT INTO push_rules_enable (id, user_name, rule_id, enabled)
   432|                 VALUES (?, ?, ?, ?)
   433|                 ON CONFLICT DO NOTHING
   434|             """
   435|         elif isinstance(self.database_engine, Sqlite3Engine):
   436|             sql = """
   437|                 INSERT OR IGNORE INTO push_rules_enable (id, user_name, rule_id, enabled)
   438|                 VALUES (?, ?, ?, ?)
   439|             """
   440|         else:
   441|             raise RuntimeError("Unknown database engine")
   442|         new_enable_id = self._push_rules_enable_id_gen.get_next()
   443|         txn.execute(sql, (new_enable_id, user_id, rule_id, 1))
   444|     async def delete_push_rule(self, user_id: str, rule_id: str) -> None:
   445|         """
   446|         Delete a push rule. Args specify the row to be deleted and can be
   447|         any of the columns in the push_rule table, but below are the
   448|         standard ones
   449|         Args:
   450|             user_id: The matrix ID of the push rule owner
   451|             rule_id: The rule_id of the rule to be deleted
   452|         """
   453|         def delete_push_rule_txn(txn, stream_id, event_stream_ordering):
   454|             self.db_pool.simple_delete_txn(
   455|                 txn, "push_rules_enable", {"user_name": user_id, "rule_id": rule_id}
   456|             )
   457|             self.db_pool.simple_delete_one_txn(
   458|                 txn, "push_rules", {"user_name": user_id, "rule_id": rule_id}
   459|             )
   460|             self._insert_push_rules_update_txn(
   461|                 txn, stream_id, event_stream_ordering, user_id, rule_id, op="DELETE"
   462|             )
   463|         async with self._push_rules_stream_id_gen.get_next() as stream_id:
   464|             event_stream_ordering = self._stream_id_gen.get_current_token()
   465|             await self.db_pool.runInteraction(
   466|                 "delete_push_rule",
   467|                 delete_push_rule_txn,
   468|                 stream_id,
   469|                 event_stream_ordering,
   470|             )
   471|     async def set_push_rule_enabled(
   472|         self, user_id: str, rule_id: str, enabled: bool, is_default_rule: bool
   473|     ) -> None:
   474|         """
   475|         Sets the `enabled` state of a push rule.
   476|         Args:
   477|             user_id: the user ID of the user who wishes to enable/disable the rule
   478|                 e.g. '@tina:example.org'
   479|             rule_id: the full rule ID of the rule to be enabled/disabled
   480|                 e.g. 'global/override/.m.rule.roomnotif'
   481|                   or 'global/override/myCustomRule'
   482|             enabled: True if the rule is to be enabled, False if it is to be
   483|                 disabled
   484|             is_default_rule: True if and only if this is a server-default rule.
   485|                 This skips the check for existence (as only user-created rules
   486|                 are always stored in the database `push_rules` table).
   487|         Raises:
   488|             NotFoundError if the rule does not exist.
   489|         """
   490|         async with self._push_rules_stream_id_gen.get_next() as stream_id:
   491|             event_stream_ordering = self._stream_id_gen.get_current_token()
   492|             await self.db_pool.runInteraction(
   493|                 "_set_push_rule_enabled_txn",
   494|                 self._set_push_rule_enabled_txn,
   495|                 stream_id,
   496|                 event_stream_ordering,
   497|                 user_id,
   498|                 rule_id,
   499|                 enabled,
   500|                 is_default_rule,
   501|             )
   502|     def _set_push_rule_enabled_txn(
   503|         self,
   504|         txn,
   505|         stream_id,
   506|         event_stream_ordering,
   507|         user_id,
   508|         rule_id,
   509|         enabled,
   510|         is_default_rule,
   511|     ):
   512|         new_id = self._push_rules_enable_id_gen.get_next()
   513|         if not is_default_rule:
   514|             for_key_share = "FOR KEY SHARE"
   515|             if not isinstance(self.database_engine, PostgresEngine):
   516|                 for_key_share = ""
   517|             sql = (
   518|                 """
   519|                 SELECT 1 FROM push_rules
   520|                 WHERE user_name = ? AND rule_id = ?
   521|                 %s
   522|             """
   523|                 % for_key_share
   524|             )
   525|             txn.execute(sql, (user_id, rule_id))
   526|             if txn.fetchone() is None:
   527|                 raise NotFoundError("Push rule does not exist.")
   528|         self.db_pool.simple_upsert_txn(
   529|             txn,
   530|             "push_rules_enable",
   531|             {"user_name": user_id, "rule_id": rule_id},
   532|             {"enabled": 1 if enabled else 0},
   533|             {"id": new_id},
   534|         )
   535|         self._insert_push_rules_update_txn(
   536|             txn,
   537|             stream_id,
   538|             event_stream_ordering,
   539|             user_id,
   540|             rule_id,
   541|             op="ENABLE" if enabled else "DISABLE",
   542|         )
   543|     async def set_push_rule_actions(
   544|         self,
   545|         user_id: str,
   546|         rule_id: str,
   547|         actions: List[Union[dict, str]],
   548|         is_default_rule: bool,
   549|     ) -> None:
   550|         """
   551|         Sets the `actions` state of a push rule.
   552|         Will throw NotFoundError if the rule does not exist; the Code for this
   553|         is NOT_FOUND.
   554|         Args:
   555|             user_id: the user ID of the user who wishes to enable/disable the rule
   556|                 e.g. '@tina:example.org'
   557|             rule_id: the full rule ID of the rule to be enabled/disabled
   558|                 e.g. 'global/override/.m.rule.roomnotif'
   559|                   or 'global/override/myCustomRule'
   560|             actions: A list of actions (each action being a dict or string),
   561|                 e.g. ["notify", {"set_tweak": "highlight", "value": false}]
   562|             is_default_rule: True if and only if this is a server-default rule.
   563|                 This skips the check for existence (as only user-created rules
   564|                 are always stored in the database `push_rules` table).
   565|         """
   566|         actions_json = json_encoder.encode(actions)
   567|         def set_push_rule_actions_txn(txn, stream_id, event_stream_ordering):
   568|             if is_default_rule:
   569|                 priority_class = -1
   570|                 priority = 1
   571|                 self._upsert_push_rule_txn(
   572|                     txn,
   573|                     stream_id,
   574|                     event_stream_ordering,
   575|                     user_id,
   576|                     rule_id,
   577|                     priority_class,
   578|                     priority,
   579|                     "[]",
   580|                     actions_json,
   581|                     update_stream=False,
   582|                 )
   583|             else:
   584|                 try:
   585|                     self.db_pool.simple_update_one_txn(
   586|                         txn,
   587|                         "push_rules",
   588|                         {"user_name": user_id, "rule_id": rule_id},
   589|                         {"actions": actions_json},
   590|                     )
   591|                 except StoreError as serr:
   592|                     if serr.code == 404:
   593|                         raise NotFoundError("Push rule does not exist")
   594|                     else:
   595|                         raise
   596|             self._insert_push_rules_update_txn(
   597|                 txn,
   598|                 stream_id,
   599|                 event_stream_ordering,
   600|                 user_id,
   601|                 rule_id,
   602|                 op="ACTIONS",
   603|                 data={"actions": actions_json},
   604|             )
   605|         async with self._push_rules_stream_id_gen.get_next() as stream_id:
   606|             event_stream_ordering = self._stream_id_gen.get_current_token()
   607|             await self.db_pool.runInteraction(
   608|                 "set_push_rule_actions",
   609|                 set_push_rule_actions_txn,
   610|                 stream_id,
   611|                 event_stream_ordering,
   612|             )
   613|     def _insert_push_rules_update_txn(
   614|         self, txn, stream_id, event_stream_ordering, user_id, rule_id, op, data=None
   615|     ):
   616|         values = {
   617|             "stream_id": stream_id,
   618|             "event_stream_ordering": event_stream_ordering,
   619|             "user_id": user_id,
   620|             "rule_id": rule_id,
   621|             "op": op,
   622|         }
   623|         if data is not None:
   624|             values.update(data)
   625|         self.db_pool.simple_insert_txn(txn, "push_rules_stream", values=values)


# ====================================================================
# FILE: synapse/storage/databases/main/pusher.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 202-242 ---
   202|             lock=False,
   203|         )
   204| class PusherStore(PusherWorkerStore):
   205|     def get_pushers_stream_token(self):
   206|         return self._pushers_id_gen.get_current_token()
   207|     async def add_pusher(
   208|         self,
   209|         user_id,
   210|         access_token,
   211|         kind,
   212|         app_id,
   213|         app_display_name,
   214|         device_display_name,
   215|         pushkey,
   216|         pushkey_ts,
   217|         lang,
   218|         data,
   219|         last_stream_ordering,
   220|         profile_tag="",
   221|     ) -> None:
   222|         async with self._pushers_id_gen.get_next() as stream_id:
   223|             await self.db_pool.simple_upsert(
   224|                 table="pushers",
   225|                 keyvalues={"app_id": app_id, "pushkey": pushkey, "user_name": user_id},
   226|                 values={
   227|                     "access_token": access_token,
   228|                     "kind": kind,
   229|                     "app_display_name": app_display_name,
   230|                     "device_display_name": device_display_name,
   231|                     "ts": pushkey_ts,
   232|                     "lang": lang,
   233|                     "data": bytearray(encode_canonical_json(data)),
   234|                     "last_stream_ordering": last_stream_ordering,
   235|                     "profile_tag": profile_tag,
   236|                     "id": stream_id,
   237|                 },
   238|                 desc="add_pusher",
   239|                 lock=False,
   240|             )
   241|             user_has_pusher = self.get_if_user_has_pusher.cache.get(
   242|                 (user_id,), None, update_metrics=False

# --- HUNK 2: Lines 253-276 ---
   253|     ) -> None:
   254|         def delete_pusher_txn(txn, stream_id):
   255|             self._invalidate_cache_and_stream(
   256|                 txn, self.get_if_user_has_pusher, (user_id,)
   257|             )
   258|             self.db_pool.simple_delete_one_txn(
   259|                 txn,
   260|                 "pushers",
   261|                 {"app_id": app_id, "pushkey": pushkey, "user_name": user_id},
   262|             )
   263|             self.db_pool.simple_insert_txn(
   264|                 txn,
   265|                 table="deleted_pushers",
   266|                 values={
   267|                     "stream_id": stream_id,
   268|                     "app_id": app_id,
   269|                     "pushkey": pushkey,
   270|                     "user_id": user_id,
   271|                 },
   272|             )
   273|         async with self._pushers_id_gen.get_next() as stream_id:
   274|             await self.db_pool.runInteraction(
   275|                 "delete_pusher", delete_pusher_txn, stream_id
   276|             )


# ====================================================================
# FILE: synapse/storage/databases/main/receipts.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-38 ---
     1| import abc
     2| import logging
     3| from typing import Any, Dict, List, Optional, Tuple
     4| from twisted.internet import defer
     5| from synapse.storage._base import SQLBaseStore, db_to_json, make_in_list_sql_clause
     6| from synapse.storage.database import DatabasePool
     7| from synapse.storage.util.id_generators import StreamIdGenerator
     8| from synapse.util import json_encoder
     9| from synapse.util.async_helpers import ObservableDeferred
    10| from synapse.util.caches.descriptors import cached, cachedList
    11| from synapse.util.caches.stream_change_cache import StreamChangeCache
    12| logger = logging.getLogger(__name__)
    13| class ReceiptsWorkerStore(SQLBaseStore, metaclass=abc.ABCMeta):
    14|     """This is an abstract base class where subclasses must implement
    15|     `get_max_receipt_stream_id` which can be called in the initializer.
    16|     """
    17|     def __init__(self, database: DatabasePool, db_conn, hs):
    18|         super().__init__(database, db_conn, hs)
    19|         self._receipts_stream_cache = StreamChangeCache(
    20|             "ReceiptsRoomChangeCache", self.get_max_receipt_stream_id()
    21|         )
    22|     @abc.abstractmethod
    23|     def get_max_receipt_stream_id(self):
    24|         """Get the current max stream ID for receipts stream
    25|         Returns:
    26|             int
    27|         """
    28|         raise NotImplementedError()
    29|     @cached()
    30|     async def get_users_with_read_receipts_in_room(self, room_id):
    31|         receipts = await self.get_receipts_for_room(room_id, "m.read")
    32|         return {r["user_id"] for r in receipts}
    33|     @cached(num_args=2)
    34|     async def get_receipts_for_room(
    35|         self, room_id: str, receipt_type: str
    36|     ) -> List[Dict[str, Any]]:
    37|         return await self.db_pool.simple_select_list(
    38|             table="receipts_linearized",

# --- HUNK 2: Lines 264-304 ---
   264|         self, room_id: str, receipt_type: str, user_id: str
   265|     ):
   266|         if receipt_type != "m.read":
   267|             return
   268|         res = self.get_users_with_read_receipts_in_room.cache.get(
   269|             room_id, None, update_metrics=False
   270|         )
   271|         if isinstance(res, ObservableDeferred):
   272|             if res.has_called():
   273|                 res = res.get_result()
   274|             else:
   275|                 res = None
   276|         if res and user_id in res:
   277|             return
   278|         self.get_users_with_read_receipts_in_room.invalidate((room_id,))
   279| class ReceiptsStore(ReceiptsWorkerStore):
   280|     def __init__(self, database: DatabasePool, db_conn, hs):
   281|         self._receipts_id_gen = StreamIdGenerator(
   282|             db_conn, "receipts_linearized", "stream_id"
   283|         )
   284|         super().__init__(database, db_conn, hs)
   285|     def get_max_receipt_stream_id(self):
   286|         return self._receipts_id_gen.get_current_token()
   287|     def insert_linearized_receipt_txn(
   288|         self, txn, room_id, receipt_type, user_id, event_id, data, stream_id
   289|     ):
   290|         """Inserts a read-receipt into the database if it's newer than the current RR
   291|         Returns: int|None
   292|             None if the RR is older than the current RR
   293|             otherwise, the rx timestamp of the event that the RR corresponds to
   294|                 (or 0 if the event is unknown)
   295|         """
   296|         res = self.db_pool.simple_select_one_txn(
   297|             txn,
   298|             table="events",
   299|             retcols=["stream_ordering", "received_ts"],
   300|             keyvalues={"event_id": event_id},
   301|             allow_none=True,
   302|         )
   303|         stream_ordering = int(res["stream_ordering"]) if res else None
   304|         rx_ts = res["received_ts"] if res else 0

# --- HUNK 3: Lines 376-416 ---
   376|             def graph_to_linear(txn):
   377|                 clause, args = make_in_list_sql_clause(
   378|                     self.database_engine, "event_id", event_ids
   379|                 )
   380|                 sql = """
   381|                     SELECT event_id WHERE room_id = ? AND stream_ordering IN (
   382|                         SELECT max(stream_ordering) WHERE %s
   383|                     )
   384|                 """ % (
   385|                     clause,
   386|                 )
   387|                 txn.execute(sql, [room_id] + list(args))
   388|                 rows = txn.fetchall()
   389|                 if rows:
   390|                     return rows[0][0]
   391|                 else:
   392|                     raise RuntimeError("Unrecognized event_ids: %r" % (event_ids,))
   393|             linearized_event_id = await self.db_pool.runInteraction(
   394|                 "insert_receipt_conv", graph_to_linear
   395|             )
   396|         async with self._receipts_id_gen.get_next() as stream_id:
   397|             event_ts = await self.db_pool.runInteraction(
   398|                 "insert_linearized_receipt",
   399|                 self.insert_linearized_receipt_txn,
   400|                 room_id,
   401|                 receipt_type,
   402|                 user_id,
   403|                 linearized_event_id,
   404|                 data,
   405|                 stream_id=stream_id,
   406|             )
   407|         if event_ts is None:
   408|             return None
   409|         now = self._clock.time_msec()
   410|         logger.debug(
   411|             "RR for event %s in %s (%i ms old)",
   412|             linearized_event_id,
   413|             room_id,
   414|             now - event_ts,
   415|         )
   416|         await self.insert_graph_receipt(room_id, receipt_type, user_id, event_ids, data)


# ====================================================================
# FILE: synapse/storage/databases/main/registration.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| import logging
     2| import re
     3| from typing import Any, Dict, List, Optional, Tuple
     4| from synapse.api.constants import UserTypes
     5| from synapse.api.errors import Codes, StoreError, SynapseError, ThreepidValidationError
     6| from synapse.metrics.background_process_metrics import run_as_background_process
     7| from synapse.storage._base import SQLBaseStore
     8| from synapse.storage.database import DatabasePool
     9| from synapse.storage.types import Cursor
    10| from synapse.storage.util.sequence import build_sequence_generator
    11| from synapse.types import UserID
    12| from synapse.util.caches.descriptors import cached
    13| THIRTY_MINUTES_IN_MS = 30 * 60 * 1000
    14| logger = logging.getLogger(__name__)
    15| class RegistrationWorkerStore(SQLBaseStore):
    16|     def __init__(self, database: DatabasePool, db_conn, hs):
    17|         super().__init__(database, db_conn, hs)
    18|         self.config = hs.config
    19|         self.clock = hs.get_clock()
    20|         self._user_id_seq = build_sequence_generator(
    21|             database.engine, find_max_generated_user_id_localpart, "user_id_seq",
    22|         )
    23|     @cached()
    24|     async def get_user_by_id(self, user_id: str) -> Optional[Dict[str, Any]]:
    25|         return await self.db_pool.simple_select_one(
    26|             table="users",
    27|             keyvalues={"name": user_id},
    28|             retcols=[
    29|                 "name",
    30|                 "password_hash",
    31|                 "is_guest",
    32|                 "admin",
    33|                 "consent_version",
    34|                 "consent_server_notice_sent",
    35|                 "appservice_id",
    36|                 "creation_ts",
    37|                 "user_type",

# --- HUNK 2: Lines 65-115 ---
    65|         """
    66|         return await self.db_pool.runInteraction(
    67|             "get_user_by_access_token", self._query_for_auth, token
    68|         )
    69|     @cached()
    70|     async def get_expiration_ts_for_user(self, user_id: str) -> Optional[int]:
    71|         """Get the expiration timestamp for the account bearing a given user ID.
    72|         Args:
    73|             user_id: The ID of the user.
    74|         Returns:
    75|             None, if the account has no expiration timestamp, otherwise int
    76|             representation of the timestamp (as a number of milliseconds since epoch).
    77|         """
    78|         return await self.db_pool.simple_select_one_onecol(
    79|             table="account_validity",
    80|             keyvalues={"user_id": user_id},
    81|             retcol="expiration_ts_ms",
    82|             allow_none=True,
    83|             desc="get_expiration_ts_for_user",
    84|         )
    85|     async def is_account_expired(self, user_id: str, current_ts: int) -> bool:
    86|         """
    87|         Returns whether an user account is expired.
    88|         Args:
    89|             user_id: The user's ID
    90|             current_ts: The current timestamp
    91|         Returns:
    92|             Whether the user account has expired
    93|         """
    94|         expiration_ts = await self.get_expiration_ts_for_user(user_id)
    95|         return expiration_ts is not None and current_ts >= expiration_ts
    96|     async def set_account_validity_for_user(
    97|         self,
    98|         user_id: str,
    99|         expiration_ts: int,
   100|         email_sent: bool,
   101|         renewal_token: Optional[str] = None,
   102|     ) -> None:
   103|         """Updates the account validity properties of the given account, with the
   104|         given values.
   105|         Args:
   106|             user_id: ID of the account to update properties for.
   107|             expiration_ts: New expiration date, as a timestamp in milliseconds
   108|                 since epoch.
   109|             email_sent: True means a renewal email has been sent for this account
   110|                 and there's no need to send another one for the current validity
   111|                 period.
   112|             renewal_token: Renewal token the user can use to extend the validity
   113|                 of their account. Defaults to no token.
   114|         """
   115|         def set_account_validity_for_user_txn(txn):

# --- HUNK 3: Lines 295-341 ---
   295|         res = self.db_pool.simple_select_one_onecol_txn(
   296|             txn=txn,
   297|             table="users",
   298|             keyvalues={"name": user_id},
   299|             retcol="user_type",
   300|             allow_none=True,
   301|         )
   302|         return True if res == UserTypes.SUPPORT else False
   303|     async def get_users_by_id_case_insensitive(self, user_id: str) -> Dict[str, str]:
   304|         """Gets users that match user_id case insensitively.
   305|         Returns:
   306|              A mapping of user_id -> password_hash.
   307|         """
   308|         def f(txn):
   309|             sql = "SELECT name, password_hash FROM users WHERE lower(name) = lower(?)"
   310|             txn.execute(sql, (user_id,))
   311|             return dict(txn)
   312|         return await self.db_pool.runInteraction("get_users_by_id_case_insensitive", f)
   313|     async def get_user_by_external_id(
   314|         self, auth_provider: str, external_id: str
   315|     ) -> Optional[str]:
   316|         """Look up a user by their external auth id
   317|         Args:
   318|             auth_provider: identifier for the remote auth provider
   319|             external_id: id on that system
   320|         Returns:
   321|             the mxid of the user, or None if they are not known
   322|         """
   323|         return await self.db_pool.simple_select_one_onecol(
   324|             table="user_external_ids",
   325|             keyvalues={"auth_provider": auth_provider, "external_id": external_id},
   326|             retcol="user_id",
   327|             allow_none=True,
   328|             desc="get_user_by_external_id",
   329|         )
   330|     async def count_all_users(self):
   331|         """Counts all users registered on the homeserver."""
   332|         def _count_users(txn):
   333|             txn.execute("SELECT COUNT(*) AS users FROM users")
   334|             rows = self.db_pool.cursor_to_dict(txn)
   335|             if rows:
   336|                 return rows[0]["users"]
   337|             return 0
   338|         return await self.db_pool.runInteraction("count_users", _count_users)
   339|     async def count_daily_user_type(self) -> Dict[str, int]:
   340|         """
   341|         Counts 1) native non guest users

# --- HUNK 4: Lines 615-655 ---
   615|         waiting on it has been carried out
   616|         Args:
   617|             session_id: The ID of the session to delete
   618|         """
   619|         def delete_threepid_session_txn(txn):
   620|             self.db_pool.simple_delete_txn(
   621|                 txn,
   622|                 table="threepid_validation_token",
   623|                 keyvalues={"session_id": session_id},
   624|             )
   625|             self.db_pool.simple_delete_txn(
   626|                 txn,
   627|                 table="threepid_validation_session",
   628|                 keyvalues={"session_id": session_id},
   629|             )
   630|         await self.db_pool.runInteraction(
   631|             "delete_threepid_session", delete_threepid_session_txn
   632|         )
   633| class RegistrationBackgroundUpdateStore(RegistrationWorkerStore):
   634|     def __init__(self, database: DatabasePool, db_conn, hs):
   635|         super().__init__(database, db_conn, hs)
   636|         self.clock = hs.get_clock()
   637|         self.config = hs.config
   638|         self.db_pool.updates.register_background_index_update(
   639|             "access_tokens_device_index",
   640|             index_name="access_tokens_device_id",
   641|             table="access_tokens",
   642|             columns=["user_id", "device_id"],
   643|         )
   644|         self.db_pool.updates.register_background_index_update(
   645|             "users_creation_ts",
   646|             index_name="users_creation_ts",
   647|             table="users",
   648|             columns=["creation_ts"],
   649|         )
   650|         self.db_pool.updates.register_noop_background_update(
   651|             "refresh_tokens_device_index"
   652|         )
   653|         self.db_pool.updates.register_background_update_handler(
   654|             "user_threepids_grandfather", self._bg_user_threepids_grandfather
   655|         )

# --- HUNK 5: Lines 712-752 ---
   712|         We do this by grandfathering in existing user threepids assuming that
   713|         they used one of the server configured trusted identity servers.
   714|         """
   715|         id_servers = set(self.config.trusted_third_party_id_servers)
   716|         def _bg_user_threepids_grandfather_txn(txn):
   717|             sql = """
   718|                 INSERT INTO user_threepid_id_server
   719|                     (user_id, medium, address, id_server)
   720|                 SELECT user_id, medium, address, ?
   721|                 FROM user_threepids
   722|             """
   723|             txn.executemany(sql, [(id_server,) for id_server in id_servers])
   724|         if id_servers:
   725|             await self.db_pool.runInteraction(
   726|                 "_bg_user_threepids_grandfather", _bg_user_threepids_grandfather_txn
   727|             )
   728|         await self.db_pool.updates._end_background_update("user_threepids_grandfather")
   729|         return 1
   730| class RegistrationStore(RegistrationBackgroundUpdateStore):
   731|     def __init__(self, database: DatabasePool, db_conn, hs):
   732|         super().__init__(database, db_conn, hs)
   733|         self._account_validity = hs.config.account_validity
   734|         self._ignore_unknown_session_error = hs.config.request_token_inhibit_3pid_errors
   735|         if self._account_validity.enabled:
   736|             self._clock.call_later(
   737|                 0.0,
   738|                 run_as_background_process,
   739|                 "account_validity_set_expiration_dates",
   740|                 self._set_expiration_date_when_missing,
   741|             )
   742|         def start_cull():
   743|             return run_as_background_process(
   744|                 "cull_expired_threepid_validation_tokens",
   745|                 self.cull_expired_threepid_validation_tokens,
   746|             )
   747|         hs.get_clock().looping_call(start_cull, THIRTY_MINUTES_IN_MS)
   748|     async def add_access_token_to_user(
   749|         self,
   750|         user_id: str,
   751|         token: str,
   752|         device_id: Optional[str],


# ====================================================================
# FILE: synapse/storage/databases/main/room.py
# Total hunks: 8
# ====================================================================
# --- HUNK 1: Lines 23-93 ---
    23|     NAME = sort rooms alphabetically by name
    24|     JOINED_MEMBERS = sort rooms by membership size, highest to lowest
    25|     """
    26|     ALPHABETICAL = "alphabetical"
    27|     SIZE = "size"
    28|     NAME = "name"
    29|     CANONICAL_ALIAS = "canonical_alias"
    30|     JOINED_MEMBERS = "joined_members"
    31|     JOINED_LOCAL_MEMBERS = "joined_local_members"
    32|     VERSION = "version"
    33|     CREATOR = "creator"
    34|     ENCRYPTION = "encryption"
    35|     FEDERATABLE = "federatable"
    36|     PUBLIC = "public"
    37|     JOIN_RULES = "join_rules"
    38|     GUEST_ACCESS = "guest_access"
    39|     HISTORY_VISIBILITY = "history_visibility"
    40|     STATE_EVENTS = "state_events"
    41| class RoomWorkerStore(SQLBaseStore):
    42|     def __init__(self, database: DatabasePool, db_conn, hs):
    43|         super().__init__(database, db_conn, hs)
    44|         self.config = hs.config
    45|     async def get_room(self, room_id: str) -> dict:
    46|         """Retrieve a room.
    47|         Args:
    48|             room_id: The ID of the room to retrieve.
    49|         Returns:
    50|             A dict containing the room information, or None if the room is unknown.
    51|         """
    52|         return await self.db_pool.simple_select_one(
    53|             table="rooms",
    54|             keyvalues={"room_id": room_id},
    55|             retcols=("room_id", "is_public", "creator"),
    56|             desc="get_room",
    57|             allow_none=True,
    58|         )
    59|     async def get_room_with_stats(self, room_id: str) -> Optional[Dict[str, Any]]:
    60|         """Retrieve room with statistics.
    61|         Args:
    62|             room_id: The ID of the room to retrieve.
    63|         Returns:
    64|             A dict containing the room information, or None if the room is unknown.
    65|         """
    66|         def get_room_with_stats_txn(txn, room_id):
    67|             sql = """
    68|                 SELECT room_id, state.name, state.canonical_alias, curr.joined_members,
    69|                   curr.local_users_in_room AS joined_local_members, rooms.room_version AS version,
    70|                   rooms.creator, state.encryption, state.is_federatable AS federatable,
    71|                   rooms.is_public AS public, state.join_rules, state.guest_access,
    72|                   state.history_visibility, curr.current_state_events AS state_events,
    73|                   state.avatar, state.topic
    74|                 FROM rooms
    75|                 LEFT JOIN room_stats_state state USING (room_id)
    76|                 LEFT JOIN room_stats_current curr USING (room_id)
    77|                 WHERE room_id = ?
    78|                 """
    79|             txn.execute(sql, [room_id])
    80|             try:
    81|                 res = self.db_pool.cursor_to_dict(txn)[0]
    82|             except IndexError:
    83|                 return None
    84|             res["federatable"] = bool(res["federatable"])
    85|             res["public"] = bool(res["public"])
    86|             return res
    87|         return await self.db_pool.runInteraction(
    88|             "get_room_with_stats", get_room_with_stats_txn, room_id
    89|         )
    90|     async def get_public_room_ids(self) -> List[str]:
    91|         return await self.db_pool.simple_select_onecol(
    92|             table="rooms",
    93|             keyvalues={"is_public": True},

# --- HUNK 2: Lines 663-703 ---
   663|                 FROM public_room_list_stream
   664|                 WHERE stream_id > ? AND stream_id <= ?
   665|                 ORDER BY stream_id ASC
   666|                 LIMIT ?
   667|             """
   668|             txn.execute(sql, (last_id, current_id, limit))
   669|             updates = [(row[0], row[1:]) for row in txn]
   670|             limited = False
   671|             upto_token = current_id
   672|             if len(updates) >= limit:
   673|                 upto_token = updates[-1][0]
   674|                 limited = True
   675|             return updates, upto_token, limited
   676|         return await self.db_pool.runInteraction(
   677|             "get_all_new_public_rooms", get_all_new_public_rooms
   678|         )
   679| class RoomBackgroundUpdateStore(SQLBaseStore):
   680|     REMOVE_TOMESTONED_ROOMS_BG_UPDATE = "remove_tombstoned_rooms_from_directory"
   681|     ADD_ROOMS_ROOM_VERSION_COLUMN = "add_rooms_room_version_column"
   682|     def __init__(self, database: DatabasePool, db_conn, hs):
   683|         super().__init__(database, db_conn, hs)
   684|         self.config = hs.config
   685|         self.db_pool.updates.register_background_update_handler(
   686|             "insert_room_retention", self._background_insert_retention,
   687|         )
   688|         self.db_pool.updates.register_background_update_handler(
   689|             self.REMOVE_TOMESTONED_ROOMS_BG_UPDATE,
   690|             self._remove_tombstoned_rooms_from_directory,
   691|         )
   692|         self.db_pool.updates.register_background_update_handler(
   693|             self.ADD_ROOMS_ROOM_VERSION_COLUMN,
   694|             self._background_add_rooms_room_version_column,
   695|         )
   696|     async def _background_insert_retention(self, progress, batch_size):
   697|         """Retrieves a list of all rooms within a range and inserts an entry for each of
   698|         them into the room_retention table.
   699|         NULLs the property's columns if missing from the retention event in the room's
   700|         state (or NULLs all of them if there's no retention event in the room's state),
   701|         so that we fall back to the server's retention policy.
   702|         """
   703|         last_room = progress.get("room_id", "")

# --- HUNK 3: Lines 822-862 ---
   822|         rooms = await self.db_pool.runInteraction(
   823|             "get_tombstoned_directory_rooms", _get_rooms
   824|         )
   825|         if not rooms:
   826|             await self.db_pool.updates._end_background_update(
   827|                 self.REMOVE_TOMESTONED_ROOMS_BG_UPDATE
   828|             )
   829|             return 0
   830|         for room_id in rooms:
   831|             logger.info("Removing tombstoned room %s from the directory", room_id)
   832|             await self.set_room_is_public(room_id, False)
   833|         await self.db_pool.updates._background_update_progress(
   834|             self.REMOVE_TOMESTONED_ROOMS_BG_UPDATE, {"room_id": rooms[-1]}
   835|         )
   836|         return len(rooms)
   837|     @abstractmethod
   838|     def set_room_is_public(self, room_id, is_public):
   839|         raise NotImplementedError()
   840| class RoomStore(RoomBackgroundUpdateStore, RoomWorkerStore, SearchStore):
   841|     def __init__(self, database: DatabasePool, db_conn, hs):
   842|         super().__init__(database, db_conn, hs)
   843|         self.config = hs.config
   844|     async def upsert_room_on_join(self, room_id: str, room_version: RoomVersion):
   845|         """Ensure that the room is stored in the table
   846|         Called when we join a room over federation, and overwrites any room version
   847|         currently in the table.
   848|         """
   849|         await self.db_pool.simple_upsert(
   850|             desc="upsert_room_on_join",
   851|             table="rooms",
   852|             keyvalues={"room_id": room_id},
   853|             values={"room_version": room_version.identifier},
   854|             insertion_values={"is_public": False, "creator": ""},
   855|             lock=False,
   856|         )
   857|     async def store_room(
   858|         self,
   859|         room_id: str,
   860|         room_creator_user_id: str,
   861|         is_public: bool,
   862|         room_version: RoomVersion,

# --- HUNK 4: Lines 876-916 ---
   876|                 self.db_pool.simple_insert_txn(
   877|                     txn,
   878|                     "rooms",
   879|                     {
   880|                         "room_id": room_id,
   881|                         "creator": room_creator_user_id,
   882|                         "is_public": is_public,
   883|                         "room_version": room_version.identifier,
   884|                     },
   885|                 )
   886|                 if is_public:
   887|                     self.db_pool.simple_insert_txn(
   888|                         txn,
   889|                         table="public_room_list_stream",
   890|                         values={
   891|                             "stream_id": next_id,
   892|                             "room_id": room_id,
   893|                             "visibility": is_public,
   894|                         },
   895|                     )
   896|             async with self._public_room_id_gen.get_next() as next_id:
   897|                 await self.db_pool.runInteraction(
   898|                     "store_room_txn", store_room_txn, next_id
   899|                 )
   900|         except Exception as e:
   901|             logger.error("store_room with room_id=%s failed: %s", room_id, e)
   902|             raise StoreError(500, "Problem creating room.")
   903|     async def maybe_store_room_on_invite(self, room_id: str, room_version: RoomVersion):
   904|         """
   905|         When we receive an invite over federation, store the version of the room if we
   906|         don't already know the room version.
   907|         """
   908|         await self.db_pool.simple_upsert(
   909|             desc="maybe_store_room_on_invite",
   910|             table="rooms",
   911|             keyvalues={"room_id": room_id},
   912|             values={},
   913|             insertion_values={
   914|                 "room_version": room_version.identifier,
   915|                 "is_public": False,
   916|                 "creator": "",

# --- HUNK 5: Lines 934-974 ---
   934|                     "network_id": None,
   935|                 },
   936|                 retcols=("stream_id", "visibility"),
   937|             )
   938|             entries.sort(key=lambda r: r["stream_id"])
   939|             add_to_stream = True
   940|             if entries:
   941|                 add_to_stream = bool(entries[-1]["visibility"]) != is_public
   942|             if add_to_stream:
   943|                 self.db_pool.simple_insert_txn(
   944|                     txn,
   945|                     table="public_room_list_stream",
   946|                     values={
   947|                         "stream_id": next_id,
   948|                         "room_id": room_id,
   949|                         "visibility": is_public,
   950|                         "appservice_id": None,
   951|                         "network_id": None,
   952|                     },
   953|                 )
   954|         async with self._public_room_id_gen.get_next() as next_id:
   955|             await self.db_pool.runInteraction(
   956|                 "set_room_is_public", set_room_is_public_txn, next_id
   957|             )
   958|         self.hs.get_notifier().on_new_replication_data()
   959|     async def set_room_is_public_appservice(
   960|         self, room_id, appservice_id, network_id, is_public
   961|     ):
   962|         """Edit the appservice/network specific public room list.
   963|         Each appservice can have a number of published room lists associated
   964|         with them, keyed off of an appservice defined `network_id`, which
   965|         basically represents a single instance of a bridge to a third party
   966|         network.
   967|         Args:
   968|             room_id (str)
   969|             appservice_id (str)
   970|             network_id (str)
   971|             is_public (bool): Whether to publish or unpublish the room from the
   972|                 list.
   973|         """
   974|         def set_room_is_public_appservice_txn(txn, next_id):

# --- HUNK 6: Lines 1004-1164 ---
  1004|                     "network_id": network_id,
  1005|                 },
  1006|                 retcols=("stream_id", "visibility"),
  1007|             )
  1008|             entries.sort(key=lambda r: r["stream_id"])
  1009|             add_to_stream = True
  1010|             if entries:
  1011|                 add_to_stream = bool(entries[-1]["visibility"]) != is_public
  1012|             if add_to_stream:
  1013|                 self.db_pool.simple_insert_txn(
  1014|                     txn,
  1015|                     table="public_room_list_stream",
  1016|                     values={
  1017|                         "stream_id": next_id,
  1018|                         "room_id": room_id,
  1019|                         "visibility": is_public,
  1020|                         "appservice_id": appservice_id,
  1021|                         "network_id": network_id,
  1022|                     },
  1023|                 )
  1024|         async with self._public_room_id_gen.get_next() as next_id:
  1025|             await self.db_pool.runInteraction(
  1026|                 "set_room_is_public_appservice",
  1027|                 set_room_is_public_appservice_txn,
  1028|                 next_id,
  1029|             )
  1030|         self.hs.get_notifier().on_new_replication_data()
  1031|     async def get_room_count(self) -> int:
  1032|         """Retrieve the total number of rooms.
  1033|         """
  1034|         def f(txn):
  1035|             sql = "SELECT count(*)  FROM rooms"
  1036|             txn.execute(sql)
  1037|             row = txn.fetchone()
  1038|             return row[0] or 0
  1039|         return await self.db_pool.runInteraction("get_rooms", f)
  1040|     async def add_event_report(
  1041|         self,
  1042|         room_id: str,
  1043|         event_id: str,
  1044|         user_id: str,
  1045|         reason: str,
  1046|         content: JsonDict,
  1047|         received_ts: int,
  1048|     ) -> None:
  1049|         next_id = self._event_reports_id_gen.get_next()
  1050|         await self.db_pool.simple_insert(
  1051|             table="event_reports",
  1052|             values={
  1053|                 "id": next_id,
  1054|                 "received_ts": received_ts,
  1055|                 "room_id": room_id,
  1056|                 "event_id": event_id,
  1057|                 "user_id": user_id,
  1058|                 "reason": reason,
  1059|                 "content": json_encoder.encode(content),
  1060|             },
  1061|             desc="add_event_report",
  1062|         )
  1063|     async def get_event_reports_paginate(
  1064|         self,
  1065|         start: int,
  1066|         limit: int,
  1067|         direction: str = "b",
  1068|         user_id: Optional[str] = None,
  1069|         room_id: Optional[str] = None,
  1070|     ) -> Tuple[List[Dict[str, Any]], int]:
  1071|         """Retrieve a paginated list of event reports
  1072|         Args:
  1073|             start: event offset to begin the query from
  1074|             limit: number of rows to retrieve
  1075|             direction: Whether to fetch the most recent first (`"b"`) or the
  1076|                 oldest first (`"f"`)
  1077|             user_id: search for user_id. Ignored if user_id is None
  1078|             room_id: search for room_id. Ignored if room_id is None
  1079|         Returns:
  1080|             event_reports: json list of event reports
  1081|             count: total number of event reports matching the filter criteria
  1082|         """
  1083|         def _get_event_reports_paginate_txn(txn):
  1084|             filters = []
  1085|             args = []
  1086|             if user_id:
  1087|                 filters.append("er.user_id LIKE ?")
  1088|                 args.extend(["%" + user_id + "%"])
  1089|             if room_id:
  1090|                 filters.append("er.room_id LIKE ?")
  1091|                 args.extend(["%" + room_id + "%"])
  1092|             if direction == "b":
  1093|                 order = "DESC"
  1094|             else:
  1095|                 order = "ASC"
  1096|             where_clause = "WHERE " + " AND ".join(filters) if len(filters) > 0 else ""
  1097|             sql = """
  1098|                 SELECT COUNT(*) as total_event_reports
  1099|                 FROM event_reports AS er
  1100|                 {}
  1101|                 """.format(
  1102|                 where_clause
  1103|             )
  1104|             txn.execute(sql, args)
  1105|             count = txn.fetchone()[0]
  1106|             sql = """
  1107|                 SELECT
  1108|                     er.id,
  1109|                     er.received_ts,
  1110|                     er.room_id,
  1111|                     er.event_id,
  1112|                     er.user_id,
  1113|                     er.reason,
  1114|                     er.content,
  1115|                     events.sender,
  1116|                     room_aliases.room_alias,
  1117|                     event_json.json AS event_json
  1118|                 FROM event_reports AS er
  1119|                 LEFT JOIN room_aliases
  1120|                     ON room_aliases.room_id = er.room_id
  1121|                 JOIN events
  1122|                     ON events.event_id = er.event_id
  1123|                 JOIN event_json
  1124|                     ON event_json.event_id = er.event_id
  1125|                 {where_clause}
  1126|                 ORDER BY er.received_ts {order}
  1127|                 LIMIT ?
  1128|                 OFFSET ?
  1129|             """.format(
  1130|                 where_clause=where_clause, order=order,
  1131|             )
  1132|             args += [limit, start]
  1133|             txn.execute(sql, args)
  1134|             event_reports = self.db_pool.cursor_to_dict(txn)
  1135|             if count > 0:
  1136|                 for row in event_reports:
  1137|                     try:
  1138|                         row["content"] = db_to_json(row["content"])
  1139|                         row["event_json"] = db_to_json(row["event_json"])
  1140|                     except Exception:
  1141|                         continue
  1142|             return event_reports, count
  1143|         return await self.db_pool.runInteraction(
  1144|             "get_event_reports_paginate", _get_event_reports_paginate_txn
  1145|         )
  1146|     def get_current_public_room_stream_id(self):
  1147|         return self._public_room_id_gen.get_current_token()
  1148|     async def block_room(self, room_id: str, user_id: str) -> None:
  1149|         """Marks the room as blocked. Can be called multiple times.
  1150|         Args:
  1151|             room_id: Room to block
  1152|             user_id: Who blocked it
  1153|         """
  1154|         await self.db_pool.simple_upsert(
  1155|             table="blocked_rooms",
  1156|             keyvalues={"room_id": room_id},
  1157|             values={},
  1158|             insertion_values={"user_id": user_id},
  1159|             desc="block_room",
  1160|         )
  1161|         await self.db_pool.runInteraction(
  1162|             "block_room_invalidation",
  1163|             self._invalidate_cache_and_stream,
  1164|             self.is_room_blocked,


# ====================================================================
# FILE: synapse/storage/databases/main/roommember.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 3-55 ---
     3| from synapse.api.constants import EventTypes, Membership
     4| from synapse.events import EventBase
     5| from synapse.events.snapshot import EventContext
     6| from synapse.metrics import LaterGauge
     7| from synapse.metrics.background_process_metrics import run_as_background_process
     8| from synapse.storage._base import (
     9|     LoggingTransaction,
    10|     SQLBaseStore,
    11|     db_to_json,
    12|     make_in_list_sql_clause,
    13| )
    14| from synapse.storage.database import DatabasePool
    15| from synapse.storage.databases.main.events_worker import EventsWorkerStore
    16| from synapse.storage.engines import Sqlite3Engine
    17| from synapse.storage.roommember import (
    18|     GetRoomsForUserWithStreamOrdering,
    19|     MemberSummary,
    20|     ProfileInfo,
    21|     RoomsForUser,
    22| )
    23| from synapse.types import Collection, PersistedEventPosition, get_domain_from_id
    24| from synapse.util.async_helpers import Linearizer
    25| from synapse.util.caches import intern_string
    26| from synapse.util.caches.descriptors import _CacheContext, cached, cachedList
    27| from synapse.util.metrics import Measure
    28| if TYPE_CHECKING:
    29|     from synapse.state import _StateCacheEntry
    30| logger = logging.getLogger(__name__)
    31| _MEMBERSHIP_PROFILE_UPDATE_NAME = "room_membership_profile_update"
    32| _CURRENT_STATE_MEMBERSHIP_UPDATE_NAME = "current_state_events_membership"
    33| class RoomMemberWorkerStore(EventsWorkerStore):
    34|     def __init__(self, database: DatabasePool, db_conn, hs):
    35|         super().__init__(database, db_conn, hs)
    36|         self._current_state_events_membership_up_to_date = False
    37|         txn = LoggingTransaction(
    38|             db_conn.cursor(),
    39|             name="_check_safe_current_state_events_membership_updated",
    40|             database_engine=self.database_engine,
    41|         )
    42|         self._check_safe_current_state_events_membership_updated_txn(txn)
    43|         txn.close()
    44|         if self.hs.config.metrics_flags.known_servers:
    45|             self._known_servers_count = 1
    46|             self.hs.get_clock().looping_call(
    47|                 run_as_background_process,
    48|                 60 * 1000,
    49|                 "_count_known_servers",
    50|                 self._count_known_servers,
    51|             )
    52|             self.hs.get_clock().call_later(
    53|                 1000,
    54|                 run_as_background_process,
    55|                 "_count_known_servers",

# --- HUNK 2: Lines 273-338 ---
   273|     ) -> FrozenSet[GetRoomsForUserWithStreamOrdering]:
   274|         """Returns a set of room_ids the user is currently joined to.
   275|         If a remote user only returns rooms this server is currently
   276|         participating in.
   277|         Args:
   278|             user_id
   279|         Returns:
   280|             Returns the rooms the user is in currently, along with the stream
   281|             ordering of the most recent join for that user and room.
   282|         """
   283|         return await self.db_pool.runInteraction(
   284|             "get_rooms_for_user_with_stream_ordering",
   285|             self._get_rooms_for_user_with_stream_ordering_txn,
   286|             user_id,
   287|         )
   288|     def _get_rooms_for_user_with_stream_ordering_txn(
   289|         self, txn, user_id: str
   290|     ) -> FrozenSet[GetRoomsForUserWithStreamOrdering]:
   291|         if self._current_state_events_membership_up_to_date:
   292|             sql = """
   293|                 SELECT room_id, e.instance_name, e.stream_ordering
   294|                 FROM current_state_events AS c
   295|                 INNER JOIN events AS e USING (room_id, event_id)
   296|                 WHERE
   297|                     c.type = 'm.room.member'
   298|                     AND state_key = ?
   299|                     AND c.membership = ?
   300|             """
   301|         else:
   302|             sql = """
   303|                 SELECT room_id, e.instance_name, e.stream_ordering
   304|                 FROM current_state_events AS c
   305|                 INNER JOIN room_memberships AS m USING (room_id, event_id)
   306|                 INNER JOIN events AS e USING (room_id, event_id)
   307|                 WHERE
   308|                     c.type = 'm.room.member'
   309|                     AND state_key = ?
   310|                     AND m.membership = ?
   311|             """
   312|         txn.execute(sql, (user_id, Membership.JOIN))
   313|         return frozenset(
   314|             GetRoomsForUserWithStreamOrdering(
   315|                 room_id, PersistedEventPosition(instance, stream_id)
   316|             )
   317|             for room_id, instance, stream_id in txn
   318|         )
   319|     async def get_users_server_still_shares_room_with(
   320|         self, user_ids: Collection[str]
   321|     ) -> Set[str]:
   322|         """Given a list of users return the set that the server still share a
   323|         room with.
   324|         """
   325|         if not user_ids:
   326|             return set()
   327|         def _get_users_server_still_shares_room_with_txn(txn):
   328|             sql = """
   329|                 SELECT state_key FROM current_state_events
   330|                 WHERE
   331|                     type = 'm.room.member'
   332|                     AND membership = 'join'
   333|                     AND %s
   334|                 GROUP BY state_key
   335|             """
   336|             clause, args = make_in_list_sql_clause(
   337|                 self.database_engine, "state_key", user_ids
   338|             )

# --- HUNK 3: Lines 601-641 ---
   601|             self.database_engine, "user_id", ignore_users
   602|         )
   603|         sql = """
   604|             SELECT 1 FROM local_current_membership
   605|             WHERE
   606|                 room_id = ? AND membership = ?
   607|                 AND NOT (%s)
   608|                 LIMIT 1
   609|         """ % (
   610|             clause,
   611|         )
   612|         def _is_local_host_in_room_ignoring_users_txn(txn):
   613|             txn.execute(sql, (room_id, Membership.JOIN, *args))
   614|             return bool(txn.fetchone())
   615|         return await self.db_pool.runInteraction(
   616|             "is_local_host_in_room_ignoring_users",
   617|             _is_local_host_in_room_ignoring_users_txn,
   618|         )
   619| class RoomMemberBackgroundUpdateStore(SQLBaseStore):
   620|     def __init__(self, database: DatabasePool, db_conn, hs):
   621|         super().__init__(database, db_conn, hs)
   622|         self.db_pool.updates.register_background_update_handler(
   623|             _MEMBERSHIP_PROFILE_UPDATE_NAME, self._background_add_membership_profile
   624|         )
   625|         self.db_pool.updates.register_background_update_handler(
   626|             _CURRENT_STATE_MEMBERSHIP_UPDATE_NAME,
   627|             self._background_current_state_membership,
   628|         )
   629|         self.db_pool.updates.register_background_index_update(
   630|             "room_membership_forgotten_idx",
   631|             index_name="room_memberships_user_room_forgotten",
   632|             table="room_memberships",
   633|             columns=["user_id", "room_id"],
   634|             where_clause="forgotten = 1",
   635|         )
   636|     async def _background_add_membership_profile(self, progress, batch_size):
   637|         target_min_stream_id = progress.get(
   638|             "target_min_stream_id_inclusive", self._min_stream_order_on_start
   639|         )
   640|         max_stream_id = progress.get(
   641|             "max_stream_id_exclusive", self._stream_order_on_start + 1

# --- HUNK 4: Lines 723-763 ---
   723|                 last_processed_room = next_room
   724|             self.db_pool.updates._background_update_progress_txn(
   725|                 txn,
   726|                 _CURRENT_STATE_MEMBERSHIP_UPDATE_NAME,
   727|                 {"last_processed_room": last_processed_room},
   728|             )
   729|             return processed, False
   730|         last_processed_room = progress.get("last_processed_room", "")
   731|         row_count, finished = await self.db_pool.runInteraction(
   732|             "_background_current_state_membership_update",
   733|             _background_current_state_membership_txn,
   734|             last_processed_room,
   735|         )
   736|         if finished:
   737|             await self.db_pool.updates._end_background_update(
   738|                 _CURRENT_STATE_MEMBERSHIP_UPDATE_NAME
   739|             )
   740|         return row_count
   741| class RoomMemberStore(RoomMemberWorkerStore, RoomMemberBackgroundUpdateStore):
   742|     def __init__(self, database: DatabasePool, db_conn, hs):
   743|         super().__init__(database, db_conn, hs)
   744|     async def forget(self, user_id: str, room_id: str) -> None:
   745|         """Indicate that user_id wishes to discard history for room_id."""
   746|         def f(txn):
   747|             sql = (
   748|                 "UPDATE"
   749|                 "  room_memberships"
   750|                 " SET"
   751|                 "  forgotten = 1"
   752|                 " WHERE"
   753|                 "  user_id = ?"
   754|                 " AND"
   755|                 "  room_id = ?"
   756|             )
   757|             txn.execute(sql, (user_id, room_id))
   758|             self._invalidate_cache_and_stream(txn, self.did_forget, (user_id, room_id))
   759|             self._invalidate_cache_and_stream(
   760|                 txn, self.get_forgotten_rooms_for_user, (user_id,)
   761|             )
   762|         await self.db_pool.runInteraction("forget_membership", f)
   763| class _JoinedHostsCache:


# ====================================================================
# FILE: synapse/storage/databases/main/search.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 42-82 ---
    42|             )
    43|             txn.executemany(sql, args)
    44|         elif isinstance(self.database_engine, Sqlite3Engine):
    45|             sql = (
    46|                 "INSERT INTO event_search (event_id, room_id, key, value)"
    47|                 " VALUES (?,?,?,?)"
    48|             )
    49|             args = (
    50|                 (entry.event_id, entry.room_id, entry.key, entry.value)
    51|                 for entry in entries
    52|             )
    53|             txn.executemany(sql, args)
    54|         else:
    55|             raise Exception("Unrecognized database engine")
    56| class SearchBackgroundUpdateStore(SearchWorkerStore):
    57|     EVENT_SEARCH_UPDATE_NAME = "event_search"
    58|     EVENT_SEARCH_ORDER_UPDATE_NAME = "event_search_order"
    59|     EVENT_SEARCH_USE_GIST_POSTGRES_NAME = "event_search_postgres_gist"
    60|     EVENT_SEARCH_USE_GIN_POSTGRES_NAME = "event_search_postgres_gin"
    61|     def __init__(self, database: DatabasePool, db_conn, hs):
    62|         super().__init__(database, db_conn, hs)
    63|         if not hs.config.enable_search:
    64|             return
    65|         self.db_pool.updates.register_background_update_handler(
    66|             self.EVENT_SEARCH_UPDATE_NAME, self._background_reindex_search
    67|         )
    68|         self.db_pool.updates.register_background_update_handler(
    69|             self.EVENT_SEARCH_ORDER_UPDATE_NAME, self._background_reindex_search_order
    70|         )
    71|         self.db_pool.updates.register_noop_background_update(
    72|             self.EVENT_SEARCH_USE_GIST_POSTGRES_NAME
    73|         )
    74|         self.db_pool.updates.register_background_update_handler(
    75|             self.EVENT_SEARCH_USE_GIN_POSTGRES_NAME, self._background_reindex_gin_search
    76|         )
    77|     async def _background_reindex_search(self, progress, batch_size):
    78|         target_min_stream_id = progress["target_min_stream_id_inclusive"]
    79|         max_stream_id = progress["max_stream_id_exclusive"]
    80|         rows_inserted = progress.get("rows_inserted", 0)
    81|         TYPES = ["m.room.name", "m.room.message", "m.room.topic"]
    82|         def reindex_search_txn(txn):

# --- HUNK 2: Lines 223-263 ---
   223|             progress = {
   224|                 "target_min_stream_id_inclusive": target_min_stream_id,
   225|                 "max_stream_id_exclusive": min_stream_id,
   226|                 "rows_inserted": rows_inserted + len(rows),
   227|                 "have_added_indexes": True,
   228|             }
   229|             self.db_pool.updates._background_update_progress_txn(
   230|                 txn, self.EVENT_SEARCH_ORDER_UPDATE_NAME, progress
   231|             )
   232|             return len(rows), True
   233|         num_rows, finished = await self.db_pool.runInteraction(
   234|             self.EVENT_SEARCH_ORDER_UPDATE_NAME, reindex_search_txn
   235|         )
   236|         if not finished:
   237|             await self.db_pool.updates._end_background_update(
   238|                 self.EVENT_SEARCH_ORDER_UPDATE_NAME
   239|             )
   240|         return num_rows
   241| class SearchStore(SearchBackgroundUpdateStore):
   242|     def __init__(self, database: DatabasePool, db_conn, hs):
   243|         super().__init__(database, db_conn, hs)
   244|     async def search_msgs(self, room_ids, search_term, keys):
   245|         """Performs a full text search over events with given keys.
   246|         Args:
   247|             room_ids (list): List of room ids to search in
   248|             search_term (str): Search term to search for
   249|             keys (list): List of keys to search in, currently supports
   250|                 "content.body", "content.name", "content.topic"
   251|         Returns:
   252|             list of dicts
   253|         """
   254|         clauses = []
   255|         search_query = _parse_query(self.database_engine, search_term)
   256|         args = []
   257|         if len(room_ids) < 500:
   258|             clause, args = make_in_list_sql_clause(
   259|                 self.database_engine, "room_id", room_ids
   260|             )
   261|             clauses = [clause]
   262|         local_clauses = []
   263|         for key in keys:


# ====================================================================
# FILE: synapse/storage/databases/main/state.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 12-52 ---
    12| from synapse.storage.databases.main.roommember import RoomMemberWorkerStore
    13| from synapse.storage.state import StateFilter
    14| from synapse.types import StateMap
    15| from synapse.util.caches import intern_string
    16| from synapse.util.caches.descriptors import cached, cachedList
    17| logger = logging.getLogger(__name__)
    18| MAX_STATE_DELTA_HOPS = 100
    19| class _GetStateGroupDelta(
    20|     namedtuple("_GetStateGroupDelta", ("prev_group", "delta_ids"))
    21| ):
    22|     """Return type of get_state_group_delta that implements __len__, which lets
    23|     us use the itrable flag when caching
    24|     """
    25|     __slots__ = []
    26|     def __len__(self):
    27|         return len(self.delta_ids) if self.delta_ids else 0
    28| class StateGroupWorkerStore(EventsWorkerStore, SQLBaseStore):
    29|     """The parts of StateGroupStore that can be called from workers.
    30|     """
    31|     def __init__(self, database: DatabasePool, db_conn, hs):
    32|         super().__init__(database, db_conn, hs)
    33|     async def get_room_version(self, room_id: str) -> RoomVersion:
    34|         """Get the room_version of a given room
    35|         Raises:
    36|             NotFoundError: if the room is unknown
    37|             UnsupportedRoomVersionError: if the room uses an unknown room version.
    38|                 Typically this happens if support for the room's version has been
    39|                 removed from Synapse.
    40|         """
    41|         room_version_id = await self.get_room_version_id(room_id)
    42|         v = KNOWN_ROOM_VERSIONS.get(room_version_id)
    43|         if not v:
    44|             raise UnsupportedRoomVersionError(
    45|                 "Room %s uses a room version %s which is no longer supported"
    46|                 % (room_id, room_version_id)
    47|             )
    48|         return v
    49|     @cached(max_entries=10000)
    50|     async def get_room_version_id(self, room_id: str) -> str:
    51|         """Get the room_version of a given room
    52|         Raises:

# --- HUNK 2: Lines 202-242 ---
   202|         """Check if the state groups are referenced by events.
   203|         Args:
   204|             state_groups
   205|         Returns:
   206|             The subset of state groups that are referenced.
   207|         """
   208|         rows = await self.db_pool.simple_select_many_batch(
   209|             table="event_to_state_groups",
   210|             column="state_group",
   211|             iterable=state_groups,
   212|             keyvalues={},
   213|             retcols=("DISTINCT state_group",),
   214|             desc="get_referenced_state_groups",
   215|         )
   216|         return {row["state_group"] for row in rows}
   217| class MainStateBackgroundUpdateStore(RoomMemberWorkerStore):
   218|     CURRENT_STATE_INDEX_UPDATE_NAME = "current_state_members_idx"
   219|     EVENT_STATE_GROUP_INDEX_UPDATE_NAME = "event_to_state_groups_sg_index"
   220|     DELETE_CURRENT_STATE_UPDATE_NAME = "delete_old_current_state_events"
   221|     def __init__(self, database: DatabasePool, db_conn, hs):
   222|         super().__init__(database, db_conn, hs)
   223|         self.server_name = hs.hostname
   224|         self.db_pool.updates.register_background_index_update(
   225|             self.CURRENT_STATE_INDEX_UPDATE_NAME,
   226|             index_name="current_state_events_member_index",
   227|             table="current_state_events",
   228|             columns=["state_key"],
   229|             where_clause="type='m.room.member'",
   230|         )
   231|         self.db_pool.updates.register_background_index_update(
   232|             self.EVENT_STATE_GROUP_INDEX_UPDATE_NAME,
   233|             index_name="event_to_state_groups_sg_index",
   234|             table="event_to_state_groups",
   235|             columns=["state_group"],
   236|         )
   237|         self.db_pool.updates.register_background_update_handler(
   238|             self.DELETE_CURRENT_STATE_UPDATE_NAME, self._background_remove_left_rooms,
   239|         )
   240|     async def _background_remove_left_rooms(self, progress, batch_size):
   241|         """Background update to delete rows from `current_state_events` and
   242|         `event_forward_extremities` tables of rooms that the server is no

# --- HUNK 3: Lines 320-340 ---
   320|         )
   321|         for user_id in potentially_left_users - joined_users:
   322|             await self.mark_remote_user_device_list_as_unsubscribed(user_id)
   323|         return batch_size
   324| class StateStore(StateGroupWorkerStore, MainStateBackgroundUpdateStore):
   325|     """ Keeps track of the state at a given event.
   326|     This is done by the concept of `state groups`. Every event is a assigned
   327|     a state group (identified by an arbitrary string), which references a
   328|     collection of state events. The current state of an event is then the
   329|     collection of state events referenced by the event's state group.
   330|     Hence, every change in the current state causes a new state group to be
   331|     generated. However, if no change happens (e.g., if we get a message event
   332|     with only one parent it inherits the state group from its parent.)
   333|     There are three tables:
   334|       * `state_groups`: Stores group name, first event with in the group and
   335|         room id.
   336|       * `event_to_state_groups`: Maps events to state groups.
   337|       * `state_groups_state`: Maps state group to state events.
   338|     """
   339|     def __init__(self, database: DatabasePool, db_conn, hs):
   340|         super().__init__(database, db_conn, hs)


# ====================================================================
# FILE: synapse/storage/databases/main/stats.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 11-58 ---
    11| logger = logging.getLogger(__name__)
    12| ABSOLUTE_STATS_FIELDS = {
    13|     "room": (
    14|         "current_state_events",
    15|         "joined_members",
    16|         "invited_members",
    17|         "left_members",
    18|         "banned_members",
    19|         "local_users_in_room",
    20|     ),
    21|     "user": ("joined_rooms",),
    22| }
    23| PER_SLICE_FIELDS = {
    24|     "room": ("total_events", "total_event_bytes"),
    25|     "user": ("invites_sent", "rooms_created", "total_events", "total_event_bytes"),
    26| }
    27| TYPE_TO_TABLE = {"room": ("room_stats", "room_id"), "user": ("user_stats", "user_id")}
    28| TYPE_TO_ORIGIN_TABLE = {"room": ("rooms", "room_id"), "user": ("users", "name")}
    29| class StatsStore(StateDeltasStore):
    30|     def __init__(self, database: DatabasePool, db_conn, hs):
    31|         super().__init__(database, db_conn, hs)
    32|         self.server_name = hs.hostname
    33|         self.clock = self.hs.get_clock()
    34|         self.stats_enabled = hs.config.stats_enabled
    35|         self.stats_bucket_size = hs.config.stats_bucket_size
    36|         self.stats_delta_processing_lock = DeferredLock()
    37|         self.db_pool.updates.register_background_update_handler(
    38|             "populate_stats_process_rooms", self._populate_stats_process_rooms
    39|         )
    40|         self.db_pool.updates.register_background_update_handler(
    41|             "populate_stats_process_users", self._populate_stats_process_users
    42|         )
    43|         self.db_pool.updates.register_noop_background_update("populate_stats_cleanup")
    44|         self.db_pool.updates.register_noop_background_update("populate_stats_prepare")
    45|     def quantise_stats_time(self, ts):
    46|         """
    47|         Quantises a timestamp to be a multiple of the bucket size.
    48|         Args:
    49|             ts (int): the timestamp to quantise, in milliseconds since the Unix
    50|                 Epoch
    51|         Returns:
    52|             int: a timestamp which
    53|               - is divisible by the bucket size;
    54|               - is no later than `ts`; and
    55|               - is the largest such timestamp.
    56|         """
    57|         return (ts // self.stats_bucket_size) * self.stats_bucket_size
    58|     async def _populate_stats_process_users(self, progress, batch_size):

# --- HUNK 2: Lines 76-186 ---
    76|             return [r for r, in txn]
    77|         users_to_work_on = await self.db_pool.runInteraction(
    78|             "_populate_stats_process_users", _get_next_batch
    79|         )
    80|         if not users_to_work_on:
    81|             await self.db_pool.updates._end_background_update(
    82|                 "populate_stats_process_users"
    83|             )
    84|             return 1
    85|         for user_id in users_to_work_on:
    86|             await self._calculate_and_set_initial_state_for_user(user_id)
    87|             progress["last_user_id"] = user_id
    88|         await self.db_pool.runInteraction(
    89|             "populate_stats_process_users",
    90|             self.db_pool.updates._background_update_progress_txn,
    91|             "populate_stats_process_users",
    92|             progress,
    93|         )
    94|         return len(users_to_work_on)
    95|     async def _populate_stats_process_rooms(self, progress, batch_size):
    96|         """This is a background update which regenerates statistics for rooms."""
    97|         if not self.stats_enabled:
    98|             await self.db_pool.updates._end_background_update(
    99|                 "populate_stats_process_rooms"
   100|             )
   101|             return 1
   102|         last_room_id = progress.get("last_room_id", "")
   103|         def _get_next_batch(txn):
   104|             sql = """
   105|                     SELECT DISTINCT room_id FROM current_state_events
   106|                     WHERE room_id > ?
   107|                     ORDER BY room_id ASC
   108|                     LIMIT ?
   109|                 """
   110|             txn.execute(sql, (last_room_id, batch_size))
   111|             return [r for r, in txn]
   112|         rooms_to_work_on = await self.db_pool.runInteraction(
   113|             "populate_stats_rooms_get_batch", _get_next_batch
   114|         )
   115|         if not rooms_to_work_on:
   116|             await self.db_pool.updates._end_background_update(
   117|                 "populate_stats_process_rooms"
   118|             )
   119|             return 1
   120|         for room_id in rooms_to_work_on:
   121|             await self._calculate_and_set_initial_state_for_room(room_id)
   122|             progress["last_room_id"] = room_id
   123|         await self.db_pool.runInteraction(
   124|             "_populate_stats_process_rooms",
   125|             self.db_pool.updates._background_update_progress_txn,
   126|             "populate_stats_process_rooms",
   127|             progress,
   128|         )
   129|         return len(rooms_to_work_on)
   130|     async def get_stats_positions(self) -> int:
   131|         """
   132|         Returns the stats processor positions.
   133|         """
   134|         return await self.db_pool.simple_select_one_onecol(
   135|             table="stats_incremental_position",
   136|             keyvalues={},
   137|             retcol="stream_id",
   138|             desc="stats_incremental_position",
   139|         )
   140|     async def update_room_state(self, room_id: str, fields: Dict[str, Any]) -> None:
   141|         """Update the state of a room.
   142|         fields can contain the following keys with string values:
   143|         * join_rules
   144|         * history_visibility
   145|         * encryption
   146|         * name
   147|         * topic
   148|         * avatar
   149|         * canonical_alias
   150|         * guest_access
   151|         A is_federatable key can also be included with a boolean value.
   152|         Args:
   153|             room_id: The room ID to update the state of.
   154|             fields: The fields to update. This can include a partial list of the
   155|                 above fields to only update some room information.
   156|         """
   157|         sentinel = object()
   158|         for col in (
   159|             "join_rules",
   160|             "history_visibility",
   161|             "encryption",
   162|             "name",
   163|             "topic",
   164|             "avatar",
   165|             "canonical_alias",
   166|             "guest_access",
   167|         ):
   168|             field = fields.get(col, sentinel)
   169|             if field is not sentinel and (not isinstance(field, str) or "\0" in field):
   170|                 fields[col] = None
   171|         await self.db_pool.simple_upsert(
   172|             table="room_stats_state",
   173|             keyvalues={"room_id": room_id},
   174|             values=fields,
   175|             desc="update_room_state",
   176|         )
   177|     async def get_statistics_for_subject(
   178|         self, stats_type: str, stats_id: str, start: str, size: int = 100
   179|     ) -> List[dict]:
   180|         """
   181|         Get statistics for a given subject.
   182|         Args:
   183|             stats_type: The type of subject
   184|             stats_id: The ID of the subject (e.g. room_id or user_id)
   185|             start: Pagination start. Number of entries, not timestamp.
   186|             size: How many entries to return.


# ====================================================================
# FILE: synapse/storage/databases/main/stream.py
# Total hunks: 11
# ====================================================================
# --- HUNK 1: Lines 13-68 ---
    13|     - stream tokens are of the form: "s%d", which maps directly to the column
    14|     - topological tokems: "t%d-%d", where the integers map to the topological
    15|       and stream ordering columns respectively.
    16| """
    17| import abc
    18| import logging
    19| from collections import namedtuple
    20| from typing import TYPE_CHECKING, Dict, List, Optional, Set, Tuple
    21| from twisted.internet import defer
    22| from synapse.api.filtering import Filter
    23| from synapse.events import EventBase
    24| from synapse.logging.context import make_deferred_yieldable, run_in_background
    25| from synapse.storage._base import SQLBaseStore
    26| from synapse.storage.database import (
    27|     DatabasePool,
    28|     LoggingTransaction,
    29|     make_in_list_sql_clause,
    30| )
    31| from synapse.storage.databases.main.events_worker import EventsWorkerStore
    32| from synapse.storage.engines import BaseDatabaseEngine, PostgresEngine
    33| from synapse.types import Collection, PersistedEventPosition, RoomStreamToken
    34| from synapse.util.caches.stream_change_cache import StreamChangeCache
    35| if TYPE_CHECKING:
    36|     from synapse.server import HomeServer
    37| logger = logging.getLogger(__name__)
    38| MAX_STREAM_SIZE = 1000
    39| _STREAM_TOKEN = "stream"
    40| _TOPOLOGICAL_TOKEN = "topological"
    41| _EventDictReturn = namedtuple(
    42|     "_EventDictReturn", ("event_id", "topological_ordering", "stream_ordering")
    43| )
    44| def generate_pagination_where_clause(
    45|     direction: str,
    46|     column_names: Tuple[str, str],
    47|     from_token: Optional[Tuple[Optional[int], int]],
    48|     to_token: Optional[Tuple[Optional[int], int]],
    49|     engine: BaseDatabaseEngine,
    50| ) -> str:
    51|     """Creates an SQL expression to bound the columns by the pagination
    52|     tokens.
    53|     For example creates an SQL expression like:
    54|         (6, 7) >= (topological_ordering, stream_ordering)
    55|         AND (5, 3) < (topological_ordering, stream_ordering)
    56|     would be generated for dir=b, from_token=(6, 7) and to_token=(5, 3).
    57|     Note that tokens are considered to be after the row they are in, e.g. if
    58|     a row A has a token T, then we consider A to be before T. This convention
    59|     is important when figuring out inequalities for the generated SQL, and
    60|     produces the following result:
    61|         - If paginating forwards then we exclude any rows matching the from
    62|           token, but include those that match the to token.
    63|         - If paginating backwards then we include any rows matching the from
    64|           token, but include those that match the to token.
    65|     Args:
    66|         direction: Whether we're paginating backwards("b") or forwards ("f").
    67|         column_names: The column names to bound. Must *not* be user defined as
    68|             these get inserted directly into the SQL statement without escapes.

# --- HUNK 2: Lines 155-410 ---
   155|         args.append(typ)
   156|     if event_filter.senders:
   157|         clauses.append("(%s)" % " OR ".join("sender = ?" for _ in event_filter.senders))
   158|         args.extend(event_filter.senders)
   159|     for sender in event_filter.not_senders:
   160|         clauses.append("sender != ?")
   161|         args.append(sender)
   162|     if event_filter.rooms:
   163|         clauses.append("(%s)" % " OR ".join("room_id = ?" for _ in event_filter.rooms))
   164|         args.extend(event_filter.rooms)
   165|     for room_id in event_filter.not_rooms:
   166|         clauses.append("room_id != ?")
   167|         args.append(room_id)
   168|     if event_filter.contains_url:
   169|         clauses.append("contains_url = ?")
   170|         args.append(event_filter.contains_url)
   171|     if event_filter.labels:
   172|         clauses.append("(%s)" % " OR ".join("label = ?" for _ in event_filter.labels))
   173|         args.extend(event_filter.labels)
   174|     return " AND ".join(clauses), args
   175| class StreamWorkerStore(EventsWorkerStore, SQLBaseStore, metaclass=abc.ABCMeta):
   176|     """This is an abstract base class where subclasses must implement
   177|     `get_room_max_stream_ordering` and `get_room_min_stream_ordering`
   178|     which can be called in the initializer.
   179|     """
   180|     def __init__(self, database: DatabasePool, db_conn, hs: "HomeServer"):
   181|         super().__init__(database, db_conn, hs)
   182|         self._instance_name = hs.get_instance_name()
   183|         self._send_federation = hs.should_send_federation()
   184|         self._federation_shard_config = hs.config.worker.federation_shard_config
   185|         self._need_to_reset_federation_stream_positions = self._send_federation
   186|         events_max = self.get_room_max_stream_ordering()
   187|         event_cache_prefill, min_event_val = self.db_pool.get_cache_dict(
   188|             db_conn,
   189|             "events",
   190|             entity_column="room_id",
   191|             stream_column="stream_ordering",
   192|             max_value=events_max,
   193|         )
   194|         self._events_stream_cache = StreamChangeCache(
   195|             "EventsRoomStreamChangeCache",
   196|             min_event_val,
   197|             prefilled_cache=event_cache_prefill,
   198|         )
   199|         self._membership_stream_cache = StreamChangeCache(
   200|             "MembershipStreamChangeCache", events_max
   201|         )
   202|         self._stream_order_on_start = self.get_room_max_stream_ordering()
   203|     @abc.abstractmethod
   204|     def get_room_max_stream_ordering(self) -> int:
   205|         raise NotImplementedError()
   206|     @abc.abstractmethod
   207|     def get_room_min_stream_ordering(self) -> int:
   208|         raise NotImplementedError()
   209|     def get_room_max_token(self) -> RoomStreamToken:
   210|         return RoomStreamToken(None, self.get_room_max_stream_ordering())
   211|     async def get_room_events_stream_for_rooms(
   212|         self,
   213|         room_ids: Collection[str],
   214|         from_key: RoomStreamToken,
   215|         to_key: RoomStreamToken,
   216|         limit: int = 0,
   217|         order: str = "DESC",
   218|     ) -> Dict[str, Tuple[List[EventBase], RoomStreamToken]]:
   219|         """Get new room events in stream ordering since `from_key`.
   220|         Args:
   221|             room_ids
   222|             from_key: Token from which no events are returned before
   223|             to_key: Token from which no events are returned after. (This
   224|                 is typically the current stream token)
   225|             limit: Maximum number of events to return
   226|             order: Either "DESC" or "ASC". Determines which events are
   227|                 returned when the result is limited. If "DESC" then the most
   228|                 recent `limit` events are returned, otherwise returns the
   229|                 oldest `limit` events.
   230|         Returns:
   231|             A map from room id to a tuple containing:
   232|                 - list of recent events in the room
   233|                 - stream ordering key for the start of the chunk of events returned.
   234|         """
   235|         room_ids = self._events_stream_cache.get_entities_changed(
   236|             room_ids, from_key.stream
   237|         )
   238|         if not room_ids:
   239|             return {}
   240|         results = {}
   241|         room_ids = list(room_ids)
   242|         for rm_ids in (room_ids[i : i + 20] for i in range(0, len(room_ids), 20)):
   243|             res = await make_deferred_yieldable(
   244|                 defer.gatherResults(
   245|                     [
   246|                         run_in_background(
   247|                             self.get_room_events_stream_for_room,
   248|                             room_id,
   249|                             from_key,
   250|                             to_key,
   251|                             limit,
   252|                             order=order,
   253|                         )
   254|                         for room_id in rm_ids
   255|                     ],
   256|                     consumeErrors=True,
   257|                 )
   258|             )
   259|             results.update(dict(zip(rm_ids, res)))
   260|         return results
   261|     def get_rooms_that_changed(
   262|         self, room_ids: Collection[str], from_key: RoomStreamToken
   263|     ) -> Set[str]:
   264|         """Given a list of rooms and a token, return rooms where there may have
   265|         been changes.
   266|         """
   267|         from_id = from_key.stream
   268|         return {
   269|             room_id
   270|             for room_id in room_ids
   271|             if self._events_stream_cache.has_entity_changed(room_id, from_id)
   272|         }
   273|     async def get_room_events_stream_for_room(
   274|         self,
   275|         room_id: str,
   276|         from_key: RoomStreamToken,
   277|         to_key: RoomStreamToken,
   278|         limit: int = 0,
   279|         order: str = "DESC",
   280|     ) -> Tuple[List[EventBase], RoomStreamToken]:
   281|         """Get new room events in stream ordering since `from_key`.
   282|         Args:
   283|             room_id
   284|             from_key: Token from which no events are returned before
   285|             to_key: Token from which no events are returned after. (This
   286|                 is typically the current stream token)
   287|             limit: Maximum number of events to return
   288|             order: Either "DESC" or "ASC". Determines which events are
   289|                 returned when the result is limited. If "DESC" then the most
   290|                 recent `limit` events are returned, otherwise returns the
   291|                 oldest `limit` events.
   292|         Returns:
   293|             The list of events (in ascending order) and the token from the start
   294|             of the chunk of events returned.
   295|         """
   296|         if from_key == to_key:
   297|             return [], from_key
   298|         from_id = from_key.stream
   299|         to_id = to_key.stream
   300|         has_changed = self._events_stream_cache.has_entity_changed(room_id, from_id)
   301|         if not has_changed:
   302|             return [], from_key
   303|         def f(txn):
   304|             sql = (
   305|                 "SELECT event_id, stream_ordering FROM events WHERE"
   306|                 " room_id = ?"
   307|                 " AND not outlier"
   308|                 " AND stream_ordering > ? AND stream_ordering <= ?"
   309|                 " ORDER BY stream_ordering %s LIMIT ?"
   310|             ) % (order,)
   311|             txn.execute(sql, (room_id, from_id, to_id, limit))
   312|             rows = [_EventDictReturn(row[0], None, row[1]) for row in txn]
   313|             return rows
   314|         rows = await self.db_pool.runInteraction("get_room_events_stream_for_room", f)
   315|         ret = await self.get_events_as_list(
   316|             [r.event_id for r in rows], get_prev_content=True
   317|         )
   318|         self._set_before_and_after(ret, rows, topo_order=from_id is None)
   319|         if order.lower() == "desc":
   320|             ret.reverse()
   321|         if rows:
   322|             key = RoomStreamToken(None, min(r.stream_ordering for r in rows))
   323|         else:
   324|             key = from_key
   325|         return ret, key
   326|     async def get_membership_changes_for_user(
   327|         self, user_id: str, from_key: RoomStreamToken, to_key: RoomStreamToken
   328|     ) -> List[EventBase]:
   329|         from_id = from_key.stream
   330|         to_id = to_key.stream
   331|         if from_key == to_key:
   332|             return []
   333|         if from_id:
   334|             has_changed = self._membership_stream_cache.has_entity_changed(
   335|                 user_id, int(from_id)
   336|             )
   337|             if not has_changed:
   338|                 return []
   339|         def f(txn):
   340|             sql = (
   341|                 "SELECT m.event_id, stream_ordering FROM events AS e,"
   342|                 " room_memberships AS m"
   343|                 " WHERE e.event_id = m.event_id"
   344|                 " AND m.user_id = ?"
   345|                 " AND e.stream_ordering > ? AND e.stream_ordering <= ?"
   346|                 " ORDER BY e.stream_ordering ASC"
   347|             )
   348|             txn.execute(sql, (user_id, from_id, to_id))
   349|             rows = [_EventDictReturn(row[0], None, row[1]) for row in txn]
   350|             return rows
   351|         rows = await self.db_pool.runInteraction("get_membership_changes_for_user", f)
   352|         ret = await self.get_events_as_list(
   353|             [r.event_id for r in rows], get_prev_content=True
   354|         )
   355|         self._set_before_and_after(ret, rows, topo_order=False)
   356|         return ret
   357|     async def get_recent_events_for_room(
   358|         self, room_id: str, limit: int, end_token: RoomStreamToken
   359|     ) -> Tuple[List[EventBase], RoomStreamToken]:
   360|         """Get the most recent events in the room in topological ordering.
   361|         Args:
   362|             room_id
   363|             limit
   364|             end_token: The stream token representing now.
   365|         Returns:
   366|             A list of events and a token pointing to the start of the returned
   367|             events. The events returned are in ascending order.
   368|         """
   369|         rows, token = await self.get_recent_event_ids_for_room(
   370|             room_id, limit, end_token
   371|         )
   372|         events = await self.get_events_as_list(
   373|             [r.event_id for r in rows], get_prev_content=True
   374|         )
   375|         self._set_before_and_after(events, rows)
   376|         return (events, token)
   377|     async def get_recent_event_ids_for_room(
   378|         self, room_id: str, limit: int, end_token: RoomStreamToken
   379|     ) -> Tuple[List[_EventDictReturn], RoomStreamToken]:
   380|         """Get the most recent events in the room in topological ordering.
   381|         Args:
   382|             room_id
   383|             limit
   384|             end_token: The stream token representing now.
   385|         Returns:
   386|             A list of _EventDictReturn and a token pointing to the start of the
   387|             returned events. The events returned are in ascending order.
   388|         """
   389|         if limit == 0:
   390|             return [], end_token
   391|         rows, token = await self.db_pool.runInteraction(
   392|             "get_recent_event_ids_for_room",
   393|             self._paginate_room_events_txn,
   394|             room_id,
   395|             from_token=end_token,
   396|             limit=limit,
   397|         )
   398|         rows.reverse()
   399|         return rows, token
   400|     async def get_room_event_before_stream_ordering(
   401|         self, room_id: str, stream_ordering: int
   402|     ) -> Tuple[int, int, str]:
   403|         """Gets details of the first event in a room at or before a stream ordering
   404|         Args:
   405|             room_id:
   406|             stream_ordering:
   407|         Returns:
   408|             A tuple of (stream ordering, topological ordering, event_id)
   409|         """
   410|         def _f(txn):

# --- HUNK 3: Lines 440-507 ---
   440|         Args:
   441|             event_id: The id of the event to look up a stream token for.
   442|         Raises:
   443|             StoreError if the event wasn't in the database.
   444|         Returns:
   445|             A stream ID.
   446|         """
   447|         return await self.db_pool.runInteraction(
   448|             "get_stream_id_for_event", self.get_stream_id_for_event_txn, event_id,
   449|         )
   450|     def get_stream_id_for_event_txn(
   451|         self, txn: LoggingTransaction, event_id: str, allow_none=False,
   452|     ) -> int:
   453|         return self.db_pool.simple_select_one_onecol_txn(
   454|             txn=txn,
   455|             table="events",
   456|             keyvalues={"event_id": event_id},
   457|             retcol="stream_ordering",
   458|             allow_none=allow_none,
   459|         )
   460|     async def get_position_for_event(self, event_id: str) -> PersistedEventPosition:
   461|         """Get the persisted position for an event
   462|         """
   463|         row = await self.db_pool.simple_select_one(
   464|             table="events",
   465|             keyvalues={"event_id": event_id},
   466|             retcols=("stream_ordering", "instance_name"),
   467|             desc="get_position_for_event",
   468|         )
   469|         return PersistedEventPosition(
   470|             row["instance_name"] or "master", row["stream_ordering"]
   471|         )
   472|     async def get_topological_token_for_event(self, event_id: str) -> RoomStreamToken:
   473|         """The stream token for an event
   474|         Args:
   475|             event_id: The id of the event to look up a stream token for.
   476|         Raises:
   477|             StoreError if the event wasn't in the database.
   478|         Returns:
   479|             A `RoomStreamToken` topological token.
   480|         """
   481|         row = await self.db_pool.simple_select_one(
   482|             table="events",
   483|             keyvalues={"event_id": event_id},
   484|             retcols=("stream_ordering", "topological_ordering"),
   485|             desc="get_topological_token_for_event",
   486|         )
   487|         return RoomStreamToken(row["topological_ordering"], row["stream_ordering"])
   488|     async def get_current_topological_token(self, room_id: str, stream_key: int) -> int:
   489|         """Gets the topological token in a room after or at the given stream
   490|         ordering.
   491|         Args:
   492|             room_id
   493|             stream_key
   494|         """
   495|         sql = (
   496|             "SELECT coalesce(MIN(topological_ordering), 0) FROM events"
   497|             " WHERE room_id = ? AND stream_ordering >= ?"
   498|         )
   499|         row = await self.db_pool.execute(
   500|             "get_current_topological_token", None, sql, room_id, stream_key
   501|         )
   502|         return row[0][0] if row else 0
   503|     def _get_max_topological_txn(self, txn: LoggingTransaction, room_id: str) -> int:
   504|         txn.execute(
   505|             "SELECT MAX(topological_ordering) FROM events WHERE room_id = ?",
   506|             (room_id,),
   507|         )

# --- HUNK 4: Lines 510-551 ---
   510|     @staticmethod
   511|     def _set_before_and_after(
   512|         events: List[EventBase], rows: List[_EventDictReturn], topo_order: bool = True
   513|     ):
   514|         """Inserts ordering information to events' internal metadata from
   515|         the DB rows.
   516|         Args:
   517|             events
   518|             rows
   519|             topo_order: Whether the events were ordered topologically or by stream
   520|                 ordering. If true then all rows should have a non null
   521|                 topological_ordering.
   522|         """
   523|         for event, row in zip(events, rows):
   524|             stream = row.stream_ordering
   525|             if topo_order and row.topological_ordering:
   526|                 topo = row.topological_ordering
   527|             else:
   528|                 topo = None
   529|             internal = event.internal_metadata
   530|             internal.before = RoomStreamToken(topo, stream - 1)
   531|             internal.after = RoomStreamToken(topo, stream)
   532|             internal.order = (int(topo) if topo else 0, int(stream))
   533|     async def get_events_around(
   534|         self,
   535|         room_id: str,
   536|         event_id: str,
   537|         before_limit: int,
   538|         after_limit: int,
   539|         event_filter: Optional[Filter] = None,
   540|     ) -> dict:
   541|         """Retrieve events and pagination tokens around a given event in a
   542|         room.
   543|         """
   544|         results = await self.db_pool.runInteraction(
   545|             "get_events_around",
   546|             self._get_events_around_txn,
   547|             room_id,
   548|             event_id,
   549|             before_limit,
   550|             after_limit,
   551|             event_filter,

# --- HUNK 5: Lines 711-843 ---
   711|         )
   712|         txn.execute(sql % (clause,), args)
   713|         for typ, stream_id in min_positions.items():
   714|             self.db_pool.simple_upsert_txn(
   715|                 txn,
   716|                 table="federation_stream_position",
   717|                 keyvalues={"type": typ, "instance_name": self._instance_name},
   718|                 values={"stream_id": stream_id},
   719|             )
   720|     def has_room_changed_since(self, room_id: str, stream_id: int) -> bool:
   721|         return self._events_stream_cache.has_entity_changed(room_id, stream_id)
   722|     def _paginate_room_events_txn(
   723|         self,
   724|         txn: LoggingTransaction,
   725|         room_id: str,
   726|         from_token: RoomStreamToken,
   727|         to_token: Optional[RoomStreamToken] = None,
   728|         direction: str = "b",
   729|         limit: int = -1,
   730|         event_filter: Optional[Filter] = None,
   731|     ) -> Tuple[List[_EventDictReturn], RoomStreamToken]:
   732|         """Returns list of events before or after a given token.
   733|         Args:
   734|             txn
   735|             room_id
   736|             from_token: The token used to stream from
   737|             to_token: A token which if given limits the results to only those before
   738|             direction: Either 'b' or 'f' to indicate whether we are paginating
   739|                 forwards or backwards from `from_key`.
   740|             limit: The maximum number of events to return.
   741|             event_filter: If provided filters the events to
   742|                 those that match the filter.
   743|         Returns:
   744|             A list of _EventDictReturn and a token that points to the end of the
   745|             result set. If no events are returned then the end of the stream has
   746|             been reached (i.e. there are no events between `from_token` and
   747|             `to_token`), or `limit` is zero.
   748|         """
   749|         assert int(limit) >= 0
   750|         args = [False, room_id]
   751|         if direction == "b":
   752|             order = "DESC"
   753|         else:
   754|             order = "ASC"
   755|         bounds = generate_pagination_where_clause(
   756|             direction=direction,
   757|             column_names=("topological_ordering", "stream_ordering"),
   758|             from_token=from_token.as_tuple(),
   759|             to_token=to_token.as_tuple() if to_token else None,
   760|             engine=self.database_engine,
   761|         )
   762|         filter_clause, filter_args = filter_to_clause(event_filter)
   763|         if filter_clause:
   764|             bounds += " AND " + filter_clause
   765|             args.extend(filter_args)
   766|         args.append(int(limit))
   767|         select_keywords = "SELECT"
   768|         join_clause = ""
   769|         if event_filter and event_filter.labels:
   770|             join_clause = """
   771|                 LEFT JOIN event_labels
   772|                 USING (event_id, room_id, topological_ordering)
   773|             """
   774|             if len(event_filter.labels) > 1:
   775|                 select_keywords += "DISTINCT"
   776|         sql = """
   777|             %(select_keywords)s event_id, topological_ordering, stream_ordering
   778|             FROM events
   779|             %(join_clause)s
   780|             WHERE outlier = ? AND room_id = ? AND %(bounds)s
   781|             ORDER BY topological_ordering %(order)s,
   782|             stream_ordering %(order)s LIMIT ?
   783|         """ % {
   784|             "select_keywords": select_keywords,
   785|             "join_clause": join_clause,
   786|             "bounds": bounds,
   787|             "order": order,
   788|         }
   789|         txn.execute(sql, args)
   790|         rows = [_EventDictReturn(row[0], row[1], row[2]) for row in txn]
   791|         if rows:
   792|             topo = rows[-1].topological_ordering
   793|             toke = rows[-1].stream_ordering
   794|             if direction == "b":
   795|                 toke -= 1
   796|             next_token = RoomStreamToken(topo, toke)
   797|         else:
   798|             next_token = to_token if to_token else from_token
   799|         return rows, next_token
   800|     async def paginate_room_events(
   801|         self,
   802|         room_id: str,
   803|         from_key: RoomStreamToken,
   804|         to_key: Optional[RoomStreamToken] = None,
   805|         direction: str = "b",
   806|         limit: int = -1,
   807|         event_filter: Optional[Filter] = None,
   808|     ) -> Tuple[List[EventBase], RoomStreamToken]:
   809|         """Returns list of events before or after a given token.
   810|         Args:
   811|             room_id
   812|             from_key: The token used to stream from
   813|             to_key: A token which if given limits the results to only those before
   814|             direction: Either 'b' or 'f' to indicate whether we are paginating
   815|                 forwards or backwards from `from_key`.
   816|             limit: The maximum number of events to return.
   817|             event_filter: If provided filters the events to those that match the filter.
   818|         Returns:
   819|             The results as a list of events and a token that points to the end
   820|             of the result set. If no events are returned then the end of the
   821|             stream has been reached (i.e. there are no events between `from_key`
   822|             and `to_key`).
   823|         """
   824|         rows, token = await self.db_pool.runInteraction(
   825|             "paginate_room_events",
   826|             self._paginate_room_events_txn,
   827|             room_id,
   828|             from_key,
   829|             to_key,
   830|             direction,
   831|             limit,
   832|             event_filter,
   833|         )
   834|         events = await self.get_events_as_list(
   835|             [r.event_id for r in rows], get_prev_content=True
   836|         )
   837|         self._set_before_and_after(events, rows)
   838|         return (events, token)
   839| class StreamStore(StreamWorkerStore):
   840|     def get_room_max_stream_ordering(self) -> int:
   841|         return self._stream_id_gen.get_current_token()
   842|     def get_room_min_stream_ordering(self) -> int:
   843|         return self._backfill_id_gen.get_current_token()


# ====================================================================
# FILE: synapse/storage/databases/main/tags.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 139-195 ---
   139|         self, user_id: str, room_id: str, tag: str, content: JsonDict
   140|     ) -> int:
   141|         """Add a tag to a room for a user.
   142|         Args:
   143|             user_id: The user to add a tag for.
   144|             room_id: The room to add a tag for.
   145|             tag: The tag name to add.
   146|             content: A json object to associate with the tag.
   147|         Returns:
   148|             The next account data ID.
   149|         """
   150|         content_json = json_encoder.encode(content)
   151|         def add_tag_txn(txn, next_id):
   152|             self.db_pool.simple_upsert_txn(
   153|                 txn,
   154|                 table="room_tags",
   155|                 keyvalues={"user_id": user_id, "room_id": room_id, "tag": tag},
   156|                 values={"content": content_json},
   157|             )
   158|             self._update_revision_txn(txn, user_id, room_id, next_id)
   159|         async with self._account_data_id_gen.get_next() as next_id:
   160|             await self.db_pool.runInteraction("add_tag", add_tag_txn, next_id)
   161|         self.get_tags_for_user.invalidate((user_id,))
   162|         return self._account_data_id_gen.get_current_token()
   163|     async def remove_tag_from_room(self, user_id: str, room_id: str, tag: str) -> int:
   164|         """Remove a tag from a room for a user.
   165|         Returns:
   166|             The next account data ID.
   167|         """
   168|         def remove_tag_txn(txn, next_id):
   169|             sql = (
   170|                 "DELETE FROM room_tags "
   171|                 " WHERE user_id = ? AND room_id = ? AND tag = ?"
   172|             )
   173|             txn.execute(sql, (user_id, room_id, tag))
   174|             self._update_revision_txn(txn, user_id, room_id, next_id)
   175|         async with self._account_data_id_gen.get_next() as next_id:
   176|             await self.db_pool.runInteraction("remove_tag", remove_tag_txn, next_id)
   177|         self.get_tags_for_user.invalidate((user_id,))
   178|         return self._account_data_id_gen.get_current_token()
   179|     def _update_revision_txn(
   180|         self, txn, user_id: str, room_id: str, next_id: int
   181|     ) -> None:
   182|         """Update the latest revision of the tags for the given user and room.
   183|         Args:
   184|             txn: The database cursor
   185|             user_id: The ID of the user.
   186|             room_id: The ID of the room.
   187|             next_id: The the revision to advance to.
   188|         """
   189|         txn.call_after(
   190|             self._account_data_stream_cache.entity_has_changed, user_id, next_id
   191|         )
   192|         update_max_id_sql = (
   193|             "UPDATE account_data_max_stream_id"
   194|             " SET stream_id = ?"
   195|             " WHERE stream_id < ?"


# ====================================================================
# FILE: synapse/storage/databases/main/transactions.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 1-45 ---
     1| import logging
     2| from collections import namedtuple
     3| from typing import Iterable, List, Optional, Tuple
     4| from canonicaljson import encode_canonical_json
     5| from synapse.metrics.background_process_metrics import run_as_background_process
     6| from synapse.storage._base import SQLBaseStore, db_to_json
     7| from synapse.storage.database import DatabasePool, LoggingTransaction
     8| from synapse.storage.engines import PostgresEngine, Sqlite3Engine
     9| from synapse.types import JsonDict
    10| from synapse.util.caches.expiringcache import ExpiringCache
    11| db_binary_type = memoryview
    12| logger = logging.getLogger(__name__)
    13| _TransactionRow = namedtuple(
    14|     "_TransactionRow",
    15|     ("id", "transaction_id", "destination", "ts", "response_code", "response_json"),
    16| )
    17| _UpdateTransactionRow = namedtuple(
    18|     "_TransactionRow", ("response_code", "response_json")
    19| )
    20| SENTINEL = object()
    21| class TransactionStore(SQLBaseStore):
    22|     """A collection of queries for handling PDUs.
    23|     """
    24|     def __init__(self, database: DatabasePool, db_conn, hs):
    25|         super().__init__(database, db_conn, hs)
    26|         self._clock.looping_call(self._start_cleanup_transactions, 30 * 60 * 1000)
    27|         self._destination_retry_cache = ExpiringCache(
    28|             cache_name="get_destination_retry_timings",
    29|             clock=self._clock,
    30|             expiry_ms=5 * 60 * 1000,
    31|         )
    32|     async def get_received_txn_response(
    33|         self, transaction_id: str, origin: str
    34|     ) -> Optional[Tuple[int, JsonDict]]:
    35|         """For an incoming transaction from a given origin, check if we have
    36|         already responded to it. If so, return the response code and response
    37|         body (as a dict).
    38|         Args:
    39|             transaction_id
    40|             origin
    41|         Returns:
    42|             None if we have not previously responded to this transaction or a
    43|             2-tuple of (int, dict)
    44|         """
    45|         return await self.db_pool.runInteraction(

# --- HUNK 2: Lines 100-140 ---
   100|             Otherwise a dict for the retry scheme
   101|         """
   102|         result = self._destination_retry_cache.get(destination, SENTINEL)
   103|         if result is not SENTINEL:
   104|             return result
   105|         result = await self.db_pool.runInteraction(
   106|             "get_destination_retry_timings",
   107|             self._get_destination_retry_timings,
   108|             destination,
   109|         )
   110|         self._destination_retry_cache[destination] = result
   111|         return result
   112|     def _get_destination_retry_timings(self, txn, destination):
   113|         result = self.db_pool.simple_select_one_txn(
   114|             txn,
   115|             table="destinations",
   116|             keyvalues={"destination": destination},
   117|             retcols=("destination", "failure_ts", "retry_last_ts", "retry_interval"),
   118|             allow_none=True,
   119|         )
   120|         if result and result["retry_last_ts"]:
   121|             return result
   122|         else:
   123|             return None
   124|     async def set_destination_retry_timings(
   125|         self,
   126|         destination: str,
   127|         failure_ts: Optional[int],
   128|         retry_last_ts: int,
   129|         retry_interval: int,
   130|     ) -> None:
   131|         """Sets the current retry timings for a given destination.
   132|         Both timings should be zero if retrying is no longer occuring.
   133|         Args:
   134|             destination
   135|             failure_ts: when the server started failing (ms since epoch)
   136|             retry_last_ts: time of last retry attempt in unix epoch ms
   137|             retry_interval: how long until next retry in ms
   138|         """
   139|         self._destination_retry_cache.pop(destination, None)
   140|         return await self.db_pool.runInteraction(

# --- HUNK 3: Lines 143-381 ---
   143|             destination,
   144|             failure_ts,
   145|             retry_last_ts,
   146|             retry_interval,
   147|         )
   148|     def _set_destination_retry_timings(
   149|         self, txn, destination, failure_ts, retry_last_ts, retry_interval
   150|     ):
   151|         if self.database_engine.can_native_upsert:
   152|             sql = """
   153|                 INSERT INTO destinations (
   154|                     destination, failure_ts, retry_last_ts, retry_interval
   155|                 )
   156|                     VALUES (?, ?, ?, ?)
   157|                 ON CONFLICT (destination) DO UPDATE SET
   158|                         failure_ts = EXCLUDED.failure_ts,
   159|                         retry_last_ts = EXCLUDED.retry_last_ts,
   160|                         retry_interval = EXCLUDED.retry_interval
   161|                     WHERE
   162|                         EXCLUDED.retry_interval = 0
   163|                         OR destinations.retry_interval IS NULL
   164|                         OR destinations.retry_interval < EXCLUDED.retry_interval
   165|             """
   166|             txn.execute(sql, (destination, failure_ts, retry_last_ts, retry_interval))
   167|             return
   168|         self.database_engine.lock_table(txn, "destinations")
   169|         prev_row = self.db_pool.simple_select_one_txn(
   170|             txn,
   171|             table="destinations",
   172|             keyvalues={"destination": destination},
   173|             retcols=("failure_ts", "retry_last_ts", "retry_interval"),
   174|             allow_none=True,
   175|         )
   176|         if not prev_row:
   177|             self.db_pool.simple_insert_txn(
   178|                 txn,
   179|                 table="destinations",
   180|                 values={
   181|                     "destination": destination,
   182|                     "failure_ts": failure_ts,
   183|                     "retry_last_ts": retry_last_ts,
   184|                     "retry_interval": retry_interval,
   185|                 },
   186|             )
   187|         elif (
   188|             retry_interval == 0
   189|             or prev_row["retry_interval"] is None
   190|             or prev_row["retry_interval"] < retry_interval
   191|         ):
   192|             self.db_pool.simple_update_one_txn(
   193|                 txn,
   194|                 "destinations",
   195|                 keyvalues={"destination": destination},
   196|                 updatevalues={
   197|                     "failure_ts": failure_ts,
   198|                     "retry_last_ts": retry_last_ts,
   199|                     "retry_interval": retry_interval,
   200|                 },
   201|             )
   202|     def _start_cleanup_transactions(self):
   203|         return run_as_background_process(
   204|             "cleanup_transactions", self._cleanup_transactions
   205|         )
   206|     async def _cleanup_transactions(self) -> None:
   207|         now = self._clock.time_msec()
   208|         month_ago = now - 30 * 24 * 60 * 60 * 1000
   209|         def _cleanup_transactions_txn(txn):
   210|             txn.execute("DELETE FROM received_transactions WHERE ts < ?", (month_ago,))
   211|         await self.db_pool.runInteraction(
   212|             "_cleanup_transactions", _cleanup_transactions_txn
   213|         )
   214|     async def store_destination_rooms_entries(
   215|         self, destinations: Iterable[str], room_id: str, stream_ordering: int,
   216|     ) -> None:
   217|         """
   218|         Updates or creates `destination_rooms` entries in batch for a single event.
   219|         Args:
   220|             destinations: list of destinations
   221|             room_id: the room_id of the event
   222|             stream_ordering: the stream_ordering of the event
   223|         """
   224|         return await self.db_pool.runInteraction(
   225|             "store_destination_rooms_entries",
   226|             self._store_destination_rooms_entries_txn,
   227|             destinations,
   228|             room_id,
   229|             stream_ordering,
   230|         )
   231|     def _store_destination_rooms_entries_txn(
   232|         self,
   233|         txn: LoggingTransaction,
   234|         destinations: Iterable[str],
   235|         room_id: str,
   236|         stream_ordering: int,
   237|     ) -> None:
   238|         if isinstance(self.database_engine, PostgresEngine):
   239|             q = """
   240|                 INSERT INTO destinations (destination)
   241|                     VALUES (?)
   242|                     ON CONFLICT DO NOTHING;
   243|             """
   244|         elif isinstance(self.database_engine, Sqlite3Engine):
   245|             q = """
   246|                 INSERT OR IGNORE INTO destinations (destination)
   247|                     VALUES (?);
   248|             """
   249|         else:
   250|             raise RuntimeError("Unknown database engine")
   251|         txn.execute_batch(q, ((destination,) for destination in destinations))
   252|         rows = [(destination, room_id) for destination in destinations]
   253|         self.db_pool.simple_upsert_many_txn(
   254|             txn,
   255|             "destination_rooms",
   256|             ["destination", "room_id"],
   257|             rows,
   258|             ["stream_ordering"],
   259|             [(stream_ordering,)] * len(rows),
   260|         )
   261|     async def get_destination_last_successful_stream_ordering(
   262|         self, destination: str
   263|     ) -> Optional[int]:
   264|         """
   265|         Gets the stream ordering of the PDU most-recently successfully sent
   266|         to the specified destination, or None if this information has not been
   267|         tracked yet.
   268|         Args:
   269|             destination: the destination to query
   270|         """
   271|         return await self.db_pool.simple_select_one_onecol(
   272|             "destinations",
   273|             {"destination": destination},
   274|             "last_successful_stream_ordering",
   275|             allow_none=True,
   276|             desc="get_last_successful_stream_ordering",
   277|         )
   278|     async def set_destination_last_successful_stream_ordering(
   279|         self, destination: str, last_successful_stream_ordering: int
   280|     ) -> None:
   281|         """
   282|         Marks that we have successfully sent the PDUs up to and including the
   283|         one specified.
   284|         Args:
   285|             destination: the destination we have successfully sent to
   286|             last_successful_stream_ordering: the stream_ordering of the most
   287|                 recent successfully-sent PDU
   288|         """
   289|         return await self.db_pool.simple_upsert(
   290|             "destinations",
   291|             keyvalues={"destination": destination},
   292|             values={"last_successful_stream_ordering": last_successful_stream_ordering},
   293|             desc="set_last_successful_stream_ordering",
   294|         )
   295|     async def get_catch_up_room_event_ids(
   296|         self, destination: str, last_successful_stream_ordering: int,
   297|     ) -> List[str]:
   298|         """
   299|         Returns at most 50 event IDs and their corresponding stream_orderings
   300|         that correspond to the oldest events that have not yet been sent to
   301|         the destination.
   302|         Args:
   303|             destination: the destination in question
   304|             last_successful_stream_ordering: the stream_ordering of the
   305|                 most-recently successfully-transmitted event to the destination
   306|         Returns:
   307|             list of event_ids
   308|         """
   309|         return await self.db_pool.runInteraction(
   310|             "get_catch_up_room_event_ids",
   311|             self._get_catch_up_room_event_ids_txn,
   312|             destination,
   313|             last_successful_stream_ordering,
   314|         )
   315|     @staticmethod
   316|     def _get_catch_up_room_event_ids_txn(
   317|         txn: LoggingTransaction, destination: str, last_successful_stream_ordering: int,
   318|     ) -> List[str]:
   319|         q = """
   320|                 SELECT event_id FROM destination_rooms
   321|                  JOIN events USING (stream_ordering)
   322|                 WHERE destination = ?
   323|                   AND stream_ordering > ?
   324|                 ORDER BY stream_ordering
   325|                 LIMIT 50
   326|             """
   327|         txn.execute(
   328|             q, (destination, last_successful_stream_ordering),
   329|         )
   330|         event_ids = [row[0] for row in txn]
   331|         return event_ids
   332|     async def get_catch_up_outstanding_destinations(
   333|         self, after_destination: Optional[str]
   334|     ) -> List[str]:
   335|         """
   336|         Gets at most 25 destinations which have outstanding PDUs to be caught up,
   337|         and are not being backed off from
   338|         Args:
   339|             after_destination:
   340|                 If provided, all destinations must be lexicographically greater
   341|                 than this one.
   342|         Returns:
   343|             list of up to 25 destinations with outstanding catch-up.
   344|                 These are the lexicographically first destinations which are
   345|                 lexicographically greater than after_destination (if provided).
   346|         """
   347|         time = self.hs.get_clock().time_msec()
   348|         return await self.db_pool.runInteraction(
   349|             "get_catch_up_outstanding_destinations",
   350|             self._get_catch_up_outstanding_destinations_txn,
   351|             time,
   352|             after_destination,
   353|         )
   354|     @staticmethod
   355|     def _get_catch_up_outstanding_destinations_txn(
   356|         txn: LoggingTransaction, now_time_ms: int, after_destination: Optional[str]
   357|     ) -> List[str]:
   358|         q = """
   359|             SELECT destination FROM destinations
   360|                 WHERE destination IN (
   361|                     SELECT destination FROM destination_rooms
   362|                         WHERE destination_rooms.stream_ordering >
   363|                             destinations.last_successful_stream_ordering
   364|                 )
   365|                 AND destination > ?
   366|                 AND (
   367|                     retry_last_ts IS NULL OR
   368|                     retry_last_ts + retry_interval < ?
   369|                 )
   370|                 ORDER BY destination
   371|                 LIMIT 25
   372|         """
   373|         txn.execute(
   374|             q,
   375|             (
   376|                 after_destination or "",
   377|                 now_time_ms,
   378|             ),
   379|         )
   380|         destinations = [row[0] for row in txn]
   381|         return destinations


# ====================================================================
# FILE: synapse/storage/databases/main/ui_auth.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| from typing import Any, Dict, List, Optional, Tuple, Union
     2| import attr
     3| from synapse.api.errors import StoreError
     4| from synapse.storage._base import SQLBaseStore, db_to_json
     5| from synapse.storage.database import LoggingTransaction
     6| from synapse.types import JsonDict
     7| from synapse.util import json_encoder, stringutils
     8| @attr.s(slots=True)
     9| class UIAuthSessionData:
    10|     session_id = attr.ib(type=str)
    11|     clientdict = attr.ib(type=JsonDict)
    12|     uri = attr.ib(type=str)
    13|     method = attr.ib(type=str)
    14|     description = attr.ib(type=str)
    15| class UIAuthWorkerStore(SQLBaseStore):
    16|     """
    17|     Manage user interactive authentication sessions.
    18|     """
    19|     async def create_ui_auth_session(
    20|         self, clientdict: JsonDict, uri: str, method: str, description: str,
    21|     ) -> UIAuthSessionData:
    22|         """
    23|         Creates a new user interactive authentication session.
    24|         The session can be used to track the stages necessary to authenticate a
    25|         user across multiple HTTP requests.
    26|         Args:
    27|             clientdict:
    28|                 The dictionary from the client root level, not the 'auth' key.


# ====================================================================
# FILE: synapse/storage/databases/main/user_directory.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-36 ---
     1| import logging
     2| import re
     3| from typing import Any, Dict, Iterable, Optional, Set, Tuple
     4| from synapse.api.constants import EventTypes, JoinRules
     5| from synapse.storage.database import DatabasePool
     6| from synapse.storage.databases.main.state import StateFilter
     7| from synapse.storage.databases.main.state_deltas import StateDeltasStore
     8| from synapse.storage.engines import PostgresEngine, Sqlite3Engine
     9| from synapse.types import get_domain_from_id, get_localpart_from_id
    10| from synapse.util.caches.descriptors import cached
    11| logger = logging.getLogger(__name__)
    12| TEMP_TABLE = "_temp_populate_user_directory"
    13| class UserDirectoryBackgroundUpdateStore(StateDeltasStore):
    14|     SHARE_PRIVATE_WORKING_SET = 500
    15|     def __init__(self, database: DatabasePool, db_conn, hs):
    16|         super().__init__(database, db_conn, hs)
    17|         self.server_name = hs.hostname
    18|         self.db_pool.updates.register_background_update_handler(
    19|             "populate_user_directory_createtables",
    20|             self._populate_user_directory_createtables,
    21|         )
    22|         self.db_pool.updates.register_background_update_handler(
    23|             "populate_user_directory_process_rooms",
    24|             self._populate_user_directory_process_rooms,
    25|         )
    26|         self.db_pool.updates.register_background_update_handler(
    27|             "populate_user_directory_process_users",
    28|             self._populate_user_directory_process_users,
    29|         )
    30|         self.db_pool.updates.register_background_update_handler(
    31|             "populate_user_directory_cleanup", self._populate_user_directory_cleanup
    32|         )
    33|     async def _populate_user_directory_createtables(self, progress, batch_size):
    34|         def _make_staging_area(txn):
    35|             sql = (
    36|                 "CREATE TABLE IF NOT EXISTS "

# --- HUNK 2: Lines 409-449 ---
   409|         )
   410|     @cached()
   411|     async def get_user_in_directory(self, user_id: str) -> Optional[Dict[str, Any]]:
   412|         return await self.db_pool.simple_select_one(
   413|             table="user_directory",
   414|             keyvalues={"user_id": user_id},
   415|             retcols=("display_name", "avatar_url"),
   416|             allow_none=True,
   417|             desc="get_user_in_directory",
   418|         )
   419|     async def update_user_directory_stream_pos(self, stream_id: str) -> None:
   420|         await self.db_pool.simple_update_one(
   421|             table="user_directory_stream_pos",
   422|             keyvalues={},
   423|             updatevalues={"stream_id": stream_id},
   424|             desc="update_user_directory_stream_pos",
   425|         )
   426| class UserDirectoryStore(UserDirectoryBackgroundUpdateStore):
   427|     SHARE_PRIVATE_WORKING_SET = 500
   428|     def __init__(self, database: DatabasePool, db_conn, hs):
   429|         super().__init__(database, db_conn, hs)
   430|     async def remove_from_user_dir(self, user_id: str) -> None:
   431|         def _remove_from_user_dir_txn(txn):
   432|             self.db_pool.simple_delete_txn(
   433|                 txn, table="user_directory", keyvalues={"user_id": user_id}
   434|             )
   435|             self.db_pool.simple_delete_txn(
   436|                 txn, table="user_directory_search", keyvalues={"user_id": user_id}
   437|             )
   438|             self.db_pool.simple_delete_txn(
   439|                 txn, table="users_in_public_rooms", keyvalues={"user_id": user_id}
   440|             )
   441|             self.db_pool.simple_delete_txn(
   442|                 txn,
   443|                 table="users_who_share_private_rooms",
   444|                 keyvalues={"user_id": user_id},
   445|             )
   446|             self.db_pool.simple_delete_txn(
   447|                 txn,
   448|                 table="users_who_share_private_rooms",
   449|                 keyvalues={"other_user_id": user_id},


# ====================================================================
# FILE: synapse/storage/databases/main/user_erasure_store.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 42-66 ---
    42|         """Indicate that user_id wishes their message history to be erased.
    43|         Args:
    44|             user_id: full user_id to be erased
    45|         """
    46|         def f(txn):
    47|             txn.execute("SELECT 1 FROM erased_users WHERE user_id = ?", (user_id,))
    48|             if txn.fetchone():
    49|                 return
    50|             txn.execute("INSERT INTO erased_users (user_id) VALUES (?)", (user_id,))
    51|             self._invalidate_cache_and_stream(txn, self.is_user_erased, (user_id,))
    52|         await self.db_pool.runInteraction("mark_user_erased", f)
    53|     async def mark_user_not_erased(self, user_id: str) -> None:
    54|         """Indicate that user_id is no longer erased.
    55|         Args:
    56|             user_id: full user_id to be un-erased
    57|         """
    58|         def f(txn):
    59|             txn.execute("SELECT 1 FROM erased_users WHERE user_id = ?", (user_id,))
    60|             if not txn.fetchone():
    61|                 return
    62|             self.db_pool.simple_delete_one_txn(
    63|                 txn, "erased_users", keyvalues={"user_id": user_id}
    64|             )
    65|             self._invalidate_cache_and_stream(txn, self.is_user_erased, (user_id,))
    66|         await self.db_pool.runInteraction("mark_user_not_erased", f)


# ====================================================================
# FILE: synapse/storage/databases/state/bg_updates.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 93-133 ---
    93|                         if (typ, state_key) not in results[group]
    94|                     )
    95|                     if (
    96|                         max_entries_returned is not None
    97|                         and len(results[group]) == max_entries_returned
    98|                     ):
    99|                         break
   100|                     next_group = self.db_pool.simple_select_one_onecol_txn(
   101|                         txn,
   102|                         table="state_group_edges",
   103|                         keyvalues={"state_group": next_group},
   104|                         retcol="prev_state_group",
   105|                         allow_none=True,
   106|                     )
   107|         return results
   108| class StateBackgroundUpdateStore(StateGroupBackgroundUpdateStore):
   109|     STATE_GROUP_DEDUPLICATION_UPDATE_NAME = "state_group_state_deduplication"
   110|     STATE_GROUP_INDEX_UPDATE_NAME = "state_group_state_type_index"
   111|     STATE_GROUPS_ROOM_INDEX_UPDATE_NAME = "state_groups_room_id_idx"
   112|     def __init__(self, database: DatabasePool, db_conn, hs):
   113|         super().__init__(database, db_conn, hs)
   114|         self.db_pool.updates.register_background_update_handler(
   115|             self.STATE_GROUP_DEDUPLICATION_UPDATE_NAME,
   116|             self._background_deduplicate_state,
   117|         )
   118|         self.db_pool.updates.register_background_update_handler(
   119|             self.STATE_GROUP_INDEX_UPDATE_NAME, self._background_index_state
   120|         )
   121|         self.db_pool.updates.register_background_index_update(
   122|             self.STATE_GROUPS_ROOM_INDEX_UPDATE_NAME,
   123|             index_name="state_groups_room_id_idx",
   124|             table="state_groups",
   125|             columns=["room_id"],
   126|         )
   127|     async def _background_deduplicate_state(self, progress, batch_size):
   128|         """This background update will slowly deduplicate state by reencoding
   129|         them as deltas.
   130|         """
   131|         last_state_group = progress.get("last_state_group", 0)
   132|         rows_inserted = progress.get("rows_inserted", 0)
   133|         max_group = progress.get("max_group", None)


# ====================================================================
# FILE: synapse/storage/databases/state/store.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-64 ---
     1| import logging
     2| from collections import namedtuple
     3| from typing import Dict, Iterable, List, Set, Tuple
     4| from synapse.api.constants import EventTypes
     5| from synapse.storage._base import SQLBaseStore
     6| from synapse.storage.database import DatabasePool
     7| from synapse.storage.databases.state.bg_updates import StateBackgroundUpdateStore
     8| from synapse.storage.state import StateFilter
     9| from synapse.storage.types import Cursor
    10| from synapse.storage.util.sequence import build_sequence_generator
    11| from synapse.types import MutableStateMap, StateMap
    12| from synapse.util.caches.descriptors import cached
    13| from synapse.util.caches.dictionary_cache import DictionaryCache
    14| logger = logging.getLogger(__name__)
    15| MAX_STATE_DELTA_HOPS = 100
    16| class _GetStateGroupDelta(
    17|     namedtuple("_GetStateGroupDelta", ("prev_group", "delta_ids"))
    18| ):
    19|     """Return type of get_state_group_delta that implements __len__, which lets
    20|     us use the itrable flag when caching
    21|     """
    22|     __slots__ = []
    23|     def __len__(self):
    24|         return len(self.delta_ids) if self.delta_ids else 0
    25| class StateGroupDataStore(StateBackgroundUpdateStore, SQLBaseStore):
    26|     """A data store for fetching/storing state groups.
    27|     """
    28|     def __init__(self, database: DatabasePool, db_conn, hs):
    29|         super().__init__(database, db_conn, hs)
    30|         self._state_group_cache = DictionaryCache(
    31|             "*stateGroupCache*",
    32|             50000,
    33|         )
    34|         self._state_group_members_cache = DictionaryCache(
    35|             "*stateGroupMembersCache*", 500000,
    36|         )
    37|         def get_max_state_group_txn(txn: Cursor):
    38|             txn.execute("SELECT COALESCE(max(id), 0) FROM state_groups")
    39|             return txn.fetchone()[0]
    40|         self._state_group_seq_gen = build_sequence_generator(
    41|             self.database_engine, get_max_state_group_txn, "state_group_id_seq"
    42|         )
    43|         self._state_group_seq_gen.check_consistency(
    44|             db_conn, table="state_groups", id_column="id"
    45|         )
    46|     @cached(max_entries=10000, iterable=True)
    47|     async def get_state_group_delta(self, state_group):
    48|         """Given a state group try to return a previous group and a delta between
    49|         the old and the new.
    50|         Returns:
    51|             (prev_group, delta_ids), where both may be None.
    52|         """
    53|         def _get_state_group_delta_txn(txn):
    54|             prev_group = self.db_pool.simple_select_one_onecol_txn(
    55|                 txn,
    56|                 table="state_group_edges",
    57|                 keyvalues={"state_group": state_group},
    58|                 retcol="prev_state_group",
    59|                 allow_none=True,
    60|             )
    61|             if not prev_group:
    62|                 return _GetStateGroupDelta(None, None)
    63|             delta_ids = self.db_pool.simple_select_list_txn(
    64|                 txn,

# --- HUNK 2: Lines 105-145 ---
   105|                 from the database.
   106|         Returns 2-tuple (`state_dict`, `got_all`).
   107|         `got_all` is a bool indicating if we successfully retrieved all
   108|         requests state from the cache, if False we need to query the DB for the
   109|         missing state.
   110|         """
   111|         is_all, known_absent, state_dict_ids = cache.get(group)
   112|         if is_all or state_filter.is_full():
   113|             return state_filter.filter_state(state_dict_ids), is_all
   114|         missing_types = False
   115|         if state_filter.has_wildcards():
   116|             missing_types = True
   117|         else:
   118|             for key in state_filter.concrete_types():
   119|                 if key not in state_dict_ids and key not in known_absent:
   120|                     missing_types = True
   121|                     break
   122|         return state_filter.filter_state(state_dict_ids), not missing_types
   123|     async def _get_state_for_groups(
   124|         self, groups: Iterable[int], state_filter: StateFilter = StateFilter.all()
   125|     ) -> Dict[int, MutableStateMap[str]]:
   126|         """Gets the state at each of a list of state groups, optionally
   127|         filtering by type/state_key
   128|         Args:
   129|             groups: list of state groups for which we want
   130|                 to get the state.
   131|             state_filter: The state filter used to fetch state
   132|                 from the database.
   133|         Returns:
   134|             Dict of state group to state map.
   135|         """
   136|         member_filter, non_member_filter = state_filter.get_member_split()
   137|         (
   138|             non_member_state,
   139|             incomplete_groups_nm,
   140|         ) = self._get_state_for_groups_using_cache(
   141|             groups, self._state_group_cache, state_filter=non_member_filter
   142|         )
   143|         (member_state, incomplete_groups_m,) = self._get_state_for_groups_using_cache(
   144|             groups, self._state_group_members_cache, state_filter=member_filter
   145|         )


# ====================================================================
# FILE: synapse/storage/engines/_base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 47-79 ---
    47|     def convert_param_style(self, sql: str) -> str:
    48|         ...
    49|     @abc.abstractmethod
    50|     def on_new_connection(self, db_conn: ConnectionType) -> None:
    51|         ...
    52|     @abc.abstractmethod
    53|     def is_deadlock(self, error: Exception) -> bool:
    54|         ...
    55|     @abc.abstractmethod
    56|     def is_connection_closed(self, conn: ConnectionType) -> bool:
    57|         ...
    58|     @abc.abstractmethod
    59|     def lock_table(self, txn, table: str) -> None:
    60|         ...
    61|     @property
    62|     @abc.abstractmethod
    63|     def server_version(self) -> str:
    64|         """Gets a string giving the server version. For example: '3.22.0'
    65|         """
    66|         ...
    67|     @abc.abstractmethod
    68|     def in_transaction(self, conn: Connection) -> bool:
    69|         """Whether the connection is currently in a transaction.
    70|         """
    71|         ...
    72|     @abc.abstractmethod
    73|     def attempt_to_set_autocommit(self, conn: Connection, autocommit: bool):
    74|         """Attempt to set the connections autocommit mode.
    75|         When True queries are run outside of transactions.
    76|         Note: This has no effect on SQLite3, so callers still need to
    77|         commit/rollback the connections.
    78|         """
    79|         ...


# ====================================================================
# FILE: synapse/storage/engines/postgres.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-23 ---
     1| import logging
     2| from synapse.storage.engines._base import BaseDatabaseEngine, IncorrectDatabaseSetup
     3| from synapse.storage.types import Connection
     4| logger = logging.getLogger(__name__)
     5| class PostgresEngine(BaseDatabaseEngine):
     6|     def __init__(self, database_module, database_config):
     7|         super().__init__(database_module, database_config)
     8|         self.module.extensions.register_type(self.module.extensions.UNICODE)
     9|         def _disable_bytes_adapter(_):
    10|             raise Exception("Passing bytes to DB is disabled.")
    11|         self.module.extensions.register_adapter(bytes, _disable_bytes_adapter)
    12|         self.synchronous_commit = database_config.get("synchronous_commit", True)
    13|         self._version = None  # unknown as yet
    14|     @property
    15|     def single_threaded(self) -> bool:
    16|         return False
    17|     def check_database(self, db_conn, allow_outdated_version: bool = False):
    18|         self._version = db_conn.server_version
    19|         if not allow_outdated_version and self._version < 90500:
    20|             raise RuntimeError("Synapse requires PostgreSQL 9.5+ or above.")
    21|         with db_conn.cursor() as txn:
    22|             txn.execute("SHOW SERVER_ENCODING")
    23|             rows = txn.fetchall()

# --- HUNK 2: Lines 54-115 ---
    54|         if collation != "C":
    55|             errors.append("    - 'COLLATE' is set to %r. Should be 'C'" % (collation,))
    56|         if ctype != "C":
    57|             errors.append("    - 'CTYPE' is set to %r. Should be 'C'" % (ctype,))
    58|         if errors:
    59|             raise IncorrectDatabaseSetup(
    60|                 "Database is incorrectly configured:\n\n%s\n\n"
    61|                 "See docs/postgres.md for more information." % ("\n".join(errors))
    62|             )
    63|     def convert_param_style(self, sql):
    64|         return sql.replace("?", "%s")
    65|     def on_new_connection(self, db_conn):
    66|         db_conn.set_isolation_level(
    67|             self.module.extensions.ISOLATION_LEVEL_REPEATABLE_READ
    68|         )
    69|         cursor = db_conn.cursor()
    70|         cursor.execute("SET bytea_output TO escape")
    71|         if not self.synchronous_commit:
    72|             cursor.execute("SET synchronous_commit TO OFF")
    73|         cursor.close()
    74|         db_conn.commit()
    75|     @property
    76|     def can_native_upsert(self):
    77|         """
    78|         Can we use native UPSERTs?
    79|         """
    80|         return True
    81|     @property
    82|     def supports_tuple_comparison(self):
    83|         """
    84|         Do we support comparing tuples, i.e. `(a, b) > (c, d)`?
    85|         """
    86|         return True
    87|     @property
    88|     def supports_using_any_list(self):
    89|         """Do we support using `a = ANY(?)` and passing a list
    90|         """
    91|         return True
    92|     def is_deadlock(self, error):
    93|         if isinstance(error, self.module.DatabaseError):
    94|             return error.pgcode in ["40001", "40P01"]
    95|         return False
    96|     def is_connection_closed(self, conn):
    97|         return bool(conn.closed)
    98|     def lock_table(self, txn, table):
    99|         txn.execute("LOCK TABLE %s in EXCLUSIVE MODE" % (table,))
   100|     @property
   101|     def server_version(self):
   102|         """Returns a string giving the server version. For example: '8.1.5'
   103|         Returns:
   104|             string
   105|         """
   106|         numver = self._version
   107|         assert numver is not None
   108|         if numver >= 100000:
   109|             return "%i.%i" % (numver / 10000, numver % 10000)
   110|         else:
   111|             return "%i.%i.%i" % (numver / 10000, (numver % 10000) / 100, numver % 100)
   112|     def in_transaction(self, conn: Connection) -> bool:
   113|         return conn.status != self.module.extensions.STATUS_READY  # type: ignore
   114|     def attempt_to_set_autocommit(self, conn: Connection, autocommit: bool):
   115|         return conn.set_session(autocommit=autocommit)  # type: ignore


# ====================================================================
# FILE: synapse/storage/engines/sqlite.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| import struct
     2| import threading
     3| import typing
     4| from synapse.storage.engines import BaseDatabaseEngine
     5| from synapse.storage.types import Connection
     6| if typing.TYPE_CHECKING:
     7|     import sqlite3  # noqa: F401
     8| class Sqlite3Engine(BaseDatabaseEngine["sqlite3.Connection"]):
     9|     def __init__(self, database_module, database_config):
    10|         super().__init__(database_module, database_config)
    11|         database = database_config.get("args", {}).get("database")
    12|         self._is_in_memory = database in (None, ":memory:",)
    13|         self._current_state_group_id = None
    14|         self._current_state_group_id_lock = threading.Lock()
    15|     @property
    16|     def single_threaded(self) -> bool:
    17|         return True
    18|     @property
    19|     def can_native_upsert(self):
    20|         """
    21|         Do we support native UPSERTs? This requires SQLite3 3.24+, plus some
    22|         more work we haven't done yet to tell what was inserted vs updated.
    23|         """
    24|         return self.module.sqlite_version_info >= (3, 24, 0)
    25|     @property

# --- HUNK 2: Lines 34-89 ---
    34|         """Do we support using `a = ANY(?)` and passing a list
    35|         """
    36|         return False
    37|     def check_database(self, db_conn, allow_outdated_version: bool = False):
    38|         if not allow_outdated_version:
    39|             version = self.module.sqlite_version_info
    40|             if version < (3, 11, 0):
    41|                 raise RuntimeError("Synapse requires sqlite 3.11 or above.")
    42|     def check_new_database(self, txn):
    43|         """Gets called when setting up a brand new database. This allows us to
    44|         apply stricter checks on new databases versus existing database.
    45|         """
    46|     def convert_param_style(self, sql):
    47|         return sql
    48|     def on_new_connection(self, db_conn):
    49|         from synapse.storage.prepare_database import prepare_database
    50|         if self._is_in_memory:
    51|             prepare_database(db_conn, self, config=None)
    52|         db_conn.create_function("rank", 1, _rank)
    53|         db_conn.execute("PRAGMA foreign_keys = ON;")
    54|         db_conn.commit()
    55|     def is_deadlock(self, error):
    56|         return False
    57|     def is_connection_closed(self, conn):
    58|         return False
    59|     def lock_table(self, txn, table):
    60|         return
    61|     @property
    62|     def server_version(self):
    63|         """Gets a string giving the server version. For example: '3.22.0'
    64|         Returns:
    65|             string
    66|         """
    67|         return "%i.%i.%i" % self.module.sqlite_version_info
    68|     def in_transaction(self, conn: Connection) -> bool:
    69|         return conn.in_transaction  # type: ignore
    70|     def attempt_to_set_autocommit(self, conn: Connection, autocommit: bool):
    71|         pass
    72| def _parse_match_info(buf):
    73|     bufsize = len(buf)
    74|     return [struct.unpack("@I", buf[i : i + 4])[0] for i in range(0, bufsize, 4)]
    75| def _rank(raw_match_info):
    76|     """Handle match_info called w/default args 'pcx' - based on the example rank
    77|     function http://sqlite.org/fts3.html#appendix_a
    78|     """
    79|     match_info = _parse_match_info(raw_match_info)
    80|     score = 0.0
    81|     p, c = match_info[:2]
    82|     for phrase_num in range(p):
    83|         phrase_info_idx = 2 + (phrase_num * c * 3)
    84|         for col_num in range(c):
    85|             col_idx = phrase_info_idx + (col_num * 3)
    86|             x1, x2 = match_info[col_idx : col_idx + 2]
    87|             if x1 > 0:
    88|                 score += float(x1) / x2
    89|     return score


# ====================================================================
# FILE: synapse/storage/persist_events.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-34 ---
     1| import itertools
     2| import logging
     3| from collections import deque, namedtuple
     4| from typing import Dict, Iterable, List, Optional, Set, Tuple
     5| from prometheus_client import Counter, Histogram
     6| from twisted.internet import defer
     7| from synapse.api.constants import EventTypes, Membership
     8| from synapse.events import EventBase
     9| from synapse.events.snapshot import EventContext
    10| from synapse.logging.context import PreserveLoggingContext, make_deferred_yieldable
    11| from synapse.metrics.background_process_metrics import run_as_background_process
    12| from synapse.storage.databases import Databases
    13| from synapse.storage.databases.main.events import DeltaState
    14| from synapse.types import Collection, PersistedEventPosition, RoomStreamToken, StateMap
    15| from synapse.util.async_helpers import ObservableDeferred
    16| from synapse.util.metrics import Measure
    17| logger = logging.getLogger(__name__)
    18| state_delta_counter = Counter("synapse_storage_events_state_delta", "")
    19| state_delta_single_event_counter = Counter(
    20|     "synapse_storage_events_state_delta_single_event", ""
    21| )
    22| state_delta_reuse_delta_counter = Counter(
    23|     "synapse_storage_events_state_delta_reuse_delta", ""
    24| )
    25| forward_extremities_counter = Histogram(
    26|     "synapse_storage_events_forward_extremities_persisted",
    27|     "Number of forward extremities for each new event",
    28|     buckets=(1, 2, 3, 5, 7, 10, 15, 20, 50, 100, 200, 500, "+Inf"),
    29| )
    30| stale_forward_extremities_counter = Histogram(
    31|     "synapse_storage_events_stale_forward_extremities_persisted",
    32|     "Number of unchanged forward extremities for each new event",
    33|     buckets=(0, 1, 2, 3, 5, 7, 10, 15, 20, 50, 100, 200, 500, "+Inf"),
    34| )

# --- HUNK 2: Lines 101-224 ---
   101|                 queue = self._event_persist_queues.pop(room_id, None)
   102|                 if queue:
   103|                     self._event_persist_queues[room_id] = queue
   104|                 self._currently_persisting_rooms.discard(room_id)
   105|         run_as_background_process("persist_events", handle_queue_loop)
   106|     def _get_drainining_queue(self, room_id):
   107|         queue = self._event_persist_queues.setdefault(room_id, deque())
   108|         try:
   109|             while True:
   110|                 yield queue.popleft()
   111|         except IndexError:
   112|             pass
   113| class EventsPersistenceStorage:
   114|     """High level interface for handling persisting newly received events.
   115|     Takes care of batching up events by room, and calculating the necessary
   116|     current state and forward extremity changes.
   117|     """
   118|     def __init__(self, hs, stores: Databases):
   119|         self.main_store = stores.main
   120|         self.state_store = stores.state
   121|         assert stores.persist_events
   122|         self.persist_events_store = stores.persist_events
   123|         self._clock = hs.get_clock()
   124|         self._instance_name = hs.get_instance_name()
   125|         self.is_mine_id = hs.is_mine_id
   126|         self._event_persist_queue = _EventPeristenceQueue()
   127|         self._state_resolution_handler = hs.get_state_resolution_handler()
   128|     async def persist_events(
   129|         self,
   130|         events_and_contexts: Iterable[Tuple[EventBase, EventContext]],
   131|         backfilled: bool = False,
   132|     ) -> RoomStreamToken:
   133|         """
   134|         Write events to the database
   135|         Args:
   136|             events_and_contexts: list of tuples of (event, context)
   137|             backfilled: Whether the results are retrieved from federation
   138|                 via backfill or not. Used to determine if they're "new" events
   139|                 which might update the current state etc.
   140|         Returns:
   141|             the stream ordering of the latest persisted event
   142|         """
   143|         partitioned = {}  # type: Dict[str, List[Tuple[EventBase, EventContext]]]
   144|         for event, ctx in events_and_contexts:
   145|             partitioned.setdefault(event.room_id, []).append((event, ctx))
   146|         deferreds = []
   147|         for room_id, evs_ctxs in partitioned.items():
   148|             d = self._event_persist_queue.add_to_queue(
   149|                 room_id, evs_ctxs, backfilled=backfilled
   150|             )
   151|             deferreds.append(d)
   152|         for room_id in partitioned:
   153|             self._maybe_start_persisting(room_id)
   154|         await make_deferred_yieldable(
   155|             defer.gatherResults(deferreds, consumeErrors=True)
   156|         )
   157|         return self.main_store.get_room_max_token()
   158|     async def persist_event(
   159|         self, event: EventBase, context: EventContext, backfilled: bool = False
   160|     ) -> Tuple[PersistedEventPosition, RoomStreamToken]:
   161|         """
   162|         Returns:
   163|             The stream ordering of `event`, and the stream ordering of the
   164|             latest persisted event
   165|         """
   166|         deferred = self._event_persist_queue.add_to_queue(
   167|             event.room_id, [(event, context)], backfilled=backfilled
   168|         )
   169|         self._maybe_start_persisting(event.room_id)
   170|         await make_deferred_yieldable(deferred)
   171|         event_stream_id = event.internal_metadata.stream_ordering
   172|         pos = PersistedEventPosition(self._instance_name, event_stream_id)
   173|         return pos, self.main_store.get_room_max_token()
   174|     def _maybe_start_persisting(self, room_id: str):
   175|         async def persisting_queue(item):
   176|             with Measure(self._clock, "persist_events"):
   177|                 await self._persist_events(
   178|                     item.events_and_contexts, backfilled=item.backfilled
   179|                 )
   180|         self._event_persist_queue.handle_queue(room_id, persisting_queue)
   181|     async def _persist_events(
   182|         self,
   183|         events_and_contexts: List[Tuple[EventBase, EventContext]],
   184|         backfilled: bool = False,
   185|     ):
   186|         """Calculates the change to current state and forward extremities, and
   187|         persists the given events and with those updates.
   188|         """
   189|         if not events_and_contexts:
   190|             return
   191|         chunks = [
   192|             events_and_contexts[x : x + 100]
   193|             for x in range(0, len(events_and_contexts), 100)
   194|         ]
   195|         for chunk in chunks:
   196|             new_forward_extremeties = {}
   197|             current_state_for_room = {}
   198|             state_delta_for_room = {}
   199|             potentially_left_users = set()  # type: Set[str]
   200|             if not backfilled:
   201|                 with Measure(self._clock, "_calculate_state_and_extrem"):
   202|                     events_by_room = (
   203|                         {}
   204|                     )  # type: Dict[str, List[Tuple[EventBase, EventContext]]]
   205|                     for event, context in chunk:
   206|                         events_by_room.setdefault(event.room_id, []).append(
   207|                             (event, context)
   208|                         )
   209|                     for room_id, ev_ctx_rm in events_by_room.items():
   210|                         latest_event_ids = await self.main_store.get_latest_event_ids_in_room(
   211|                             room_id
   212|                         )
   213|                         new_latest_event_ids = await self._calculate_new_extremities(
   214|                             room_id, ev_ctx_rm, latest_event_ids
   215|                         )
   216|                         latest_event_ids = set(latest_event_ids)
   217|                         if new_latest_event_ids == latest_event_ids:
   218|                             continue
   219|                         assert new_latest_event_ids, "No forward extremities left!"
   220|                         new_forward_extremeties[room_id] = new_latest_event_ids
   221|                         len_1 = (
   222|                             len(latest_event_ids) == 1
   223|                             and len(new_latest_event_ids) == 1
   224|                         )

# --- HUNK 3: Lines 270-331 ---
   270|                             if not is_still_joined:
   271|                                 logger.info("Server no longer in room %s", room_id)
   272|                                 latest_event_ids = []
   273|                                 current_state = {}
   274|                                 delta.no_longer_in_room = True
   275|                             state_delta_for_room[room_id] = delta
   276|                         if current_state is not None:
   277|                             current_state_for_room[room_id] = current_state
   278|             await self.persist_events_store._persist_events_and_state_updates(
   279|                 chunk,
   280|                 current_state_for_room=current_state_for_room,
   281|                 state_delta_for_room=state_delta_for_room,
   282|                 new_forward_extremeties=new_forward_extremeties,
   283|                 backfilled=backfilled,
   284|             )
   285|             await self._handle_potentially_left_users(potentially_left_users)
   286|     async def _calculate_new_extremities(
   287|         self,
   288|         room_id: str,
   289|         event_contexts: List[Tuple[EventBase, EventContext]],
   290|         latest_event_ids: Collection[str],
   291|     ):
   292|         """Calculates the new forward extremities for a room given events to
   293|         persist.
   294|         Assumes that we are only persisting events for one room at a time.
   295|         """
   296|         new_events = [
   297|             event
   298|             for event, ctx in event_contexts
   299|             if not event.internal_metadata.is_outlier()
   300|             and not ctx.rejected
   301|             and not event.internal_metadata.is_soft_failed()
   302|         ]
   303|         latest_event_ids = set(latest_event_ids)
   304|         result = set(latest_event_ids)
   305|         result.update(event.event_id for event in new_events)
   306|         result.difference_update(
   307|             e_id for event in new_events for e_id in event.prev_event_ids()
   308|         )
   309|         existing_prevs = await self.persist_events_store._get_events_which_are_prevs(
   310|             result
   311|         )  # type: Collection[str]
   312|         result.difference_update(existing_prevs)
   313|         existing_prevs = await self.persist_events_store._get_prevs_before_rejected(
   314|             e_id for event in new_events for e_id in event.prev_event_ids()
   315|         )
   316|         result.difference_update(existing_prevs)
   317|         if result != latest_event_ids:
   318|             forward_extremities_counter.observe(len(result))
   319|             stale = latest_event_ids & result
   320|             stale_forward_extremities_counter.observe(len(stale))
   321|         return result
   322|     async def _get_new_state_after_events(
   323|         self,
   324|         room_id: str,
   325|         events_context: List[Tuple[EventBase, EventContext]],
   326|         old_latest_event_ids: Iterable[str],
   327|         new_latest_event_ids: Iterable[str],
   328|     ) -> Tuple[Optional[StateMap[str]], Optional[StateMap[str]]]:
   329|         """Calculate the current state dict after adding some new events to
   330|         a room
   331|         Args:


# ====================================================================
# FILE: synapse/storage/prepare_database.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-73 ---
     1| import imp
     2| import logging
     3| import os
     4| import re
     5| from collections import Counter
     6| from typing import Optional, TextIO
     7| import attr
     8| from synapse.config.homeserver import HomeServerConfig
     9| from synapse.storage.engines import BaseDatabaseEngine
    10| from synapse.storage.engines.postgres import PostgresEngine
    11| from synapse.storage.types import Connection, Cursor
    12| from synapse.types import Collection
    13| logger = logging.getLogger(__name__)
    14| SCHEMA_VERSION = 58
    15| dir_path = os.path.abspath(os.path.dirname(__file__))
    16| class PrepareDatabaseException(Exception):
    17|     pass
    18| class UpgradeDatabaseException(PrepareDatabaseException):
    19|     pass
    20| OUTDATED_SCHEMA_ON_WORKER_ERROR = (
    21|     "Expected database schema version %i but got %i: run the main synapse process to "
    22|     "upgrade the database schema before starting worker processes."
    23| )
    24| EMPTY_DATABASE_ON_WORKER_ERROR = (
    25|     "Uninitialised database: run the main synapse process to prepare the database "
    26|     "schema before starting worker processes."
    27| )
    28| UNAPPLIED_DELTA_ON_WORKER_ERROR = (
    29|     "Database schema delta %s has not been applied: run the main synapse process to "
    30|     "upgrade the database schema before starting worker processes."
    31| )
    32| def prepare_database(
    33|     db_conn: Connection,
    34|     database_engine: BaseDatabaseEngine,
    35|     config: Optional[HomeServerConfig],
    36|     databases: Collection[str] = ["main", "state"],
    37| ):
    38|     """Prepares a physical database for usage. Will either create all necessary tables
    39|     or upgrade from an older schema version.
    40|     If `config` is None then prepare_database will assert that no upgrade is
    41|     necessary, *or* will create a fresh database if the database is empty.
    42|     Args:
    43|         db_conn:
    44|         database_engine:
    45|         config :
    46|             application config, or None if we are connecting to an existing
    47|             database which we expect to be configured already
    48|         databases: The name of the databases that will be used
    49|             with this physical database. Defaults to all databases.
    50|     """
    51|     try:
    52|         cur = db_conn.cursor()
    53|         cur.execute("BEGIN TRANSACTION")
    54|         logger.info("%r: Checking existing schema version", databases)
    55|         version_info = _get_or_create_schema_state(cur, database_engine)
    56|         if version_info:
    57|             user_version, delta_files, upgraded = version_info
    58|             logger.info(
    59|                 "%r: Existing schema is %i (+%i deltas)",
    60|                 databases,
    61|                 user_version,
    62|                 len(delta_files),
    63|             )
    64|             if config is None:
    65|                 raise ValueError(
    66|                     "config==None in prepare_database, but databse is not empty"
    67|                 )
    68|             if config.worker_app is not None and user_version != SCHEMA_VERSION:
    69|                 raise UpgradeDatabaseException(
    70|                     OUTDATED_SCHEMA_ON_WORKER_ERROR % (SCHEMA_VERSION, user_version)
    71|                 )
    72|             _upgrade_existing_database(
    73|                 cur,

# --- HUNK 2: Lines 432-460 ---
   432| def execute_statements_from_stream(cur: Cursor, f: TextIO):
   433|     for statement in get_statements(f):
   434|         cur.execute(statement)
   435| def _get_or_create_schema_state(txn, database_engine):
   436|     schema_path = os.path.join(dir_path, "schema", "schema_version.sql")
   437|     executescript(txn, schema_path)
   438|     txn.execute("SELECT version, upgraded FROM schema_version")
   439|     row = txn.fetchone()
   440|     current_version = int(row[0]) if row else None
   441|     upgraded = bool(row[1]) if row else None
   442|     if current_version:
   443|         txn.execute(
   444|             database_engine.convert_param_style(
   445|                 "SELECT file FROM applied_schema_deltas WHERE version >= ?"
   446|             ),
   447|             (current_version,),
   448|         )
   449|         applied_deltas = [d for d, in txn]
   450|         return current_version, applied_deltas, upgraded
   451|     return None
   452| @attr.s(slots=True)
   453| class _DirectoryListing:
   454|     """Helper class to store schema file name and the
   455|     absolute path to it.
   456|     These entries get sorted, so for consistency we want to ensure that
   457|     `file_name` attr is kept first.
   458|     """
   459|     file_name = attr.ib()
   460|     absolute_path = attr.ib()


# ====================================================================
# FILE: synapse/storage/relations.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| import logging
     2| import attr
     3| from synapse.api.errors import SynapseError
     4| logger = logging.getLogger(__name__)
     5| @attr.s(slots=True)
     6| class PaginationChunk:
     7|     """Returned by relation pagination APIs.
     8|     Attributes:
     9|         chunk (list): The rows returned by pagination
    10|         next_batch (Any|None): Token to fetch next set of results with, if
    11|             None then there are no more results.
    12|         prev_batch (Any|None): Token to fetch previous set of results with, if
    13|             None then there are no previous results.
    14|     """
    15|     chunk = attr.ib()
    16|     next_batch = attr.ib(default=None)
    17|     prev_batch = attr.ib(default=None)
    18|     def to_dict(self):
    19|         d = {"chunk": self.chunk}
    20|         if self.next_batch:
    21|             d["next_batch"] = self.next_batch.to_string()
    22|         if self.prev_batch:
    23|             d["prev_batch"] = self.prev_batch.to_string()
    24|         return d
    25| @attr.s(frozen=True, slots=True)


# ====================================================================
# FILE: synapse/storage/roommember.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-11 ---
     1| import logging
     2| from collections import namedtuple
     3| logger = logging.getLogger(__name__)
     4| RoomsForUser = namedtuple(
     5|     "RoomsForUser", ("room_id", "sender", "membership", "event_id", "stream_ordering")
     6| )
     7| GetRoomsForUserWithStreamOrdering = namedtuple(
     8|     "_GetRoomsForUserWithStreamOrdering", ("room_id", "event_pos")
     9| )
    10| ProfileInfo = namedtuple("ProfileInfo", ("avatar_url", "display_name"))
    11| MemberSummary = namedtuple("MemberSummary", ("members", "count"))


# ====================================================================
# FILE: synapse/storage/state.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-26 ---
     1| import logging
     2| from typing import Awaitable, Dict, Iterable, List, Optional, Set, Tuple, TypeVar
     3| import attr
     4| from synapse.api.constants import EventTypes
     5| from synapse.events import EventBase
     6| from synapse.types import MutableStateMap, StateMap
     7| logger = logging.getLogger(__name__)
     8| T = TypeVar("T")
     9| @attr.s(slots=True)
    10| class StateFilter:
    11|     """A filter used when querying for state.
    12|     Attributes:
    13|         types: Map from type to set of state keys (or None). This specifies
    14|             which state_keys for the given type to fetch from the DB. If None
    15|             then all events with that type are fetched. If the set is empty
    16|             then no events with that type are fetched.
    17|         include_others: Whether to fetch events with types that do not
    18|             appear in `types`.
    19|     """
    20|     types = attr.ib(type=Dict[str, Optional[Set[str]]])
    21|     include_others = attr.ib(default=False, type=bool)
    22|     def __attrs_post_init__(self):
    23|         if self.include_others:
    24|             self.types = {k: v for k, v in self.types.items() if v is not None}
    25|     @staticmethod
    26|     def all() -> "StateFilter":

# --- HUNK 2: Lines 225-265 ---
   225|             include_others=self.include_others,
   226|         )
   227|         return member_filter, non_member_filter
   228| class StateGroupStorage:
   229|     """High level interface to fetching state for event.
   230|     """
   231|     def __init__(self, hs, stores):
   232|         self.stores = stores
   233|     async def get_state_group_delta(self, state_group: int):
   234|         """Given a state group try to return a previous group and a delta between
   235|         the old and the new.
   236|         Args:
   237|             state_group: The state group used to retrieve state deltas.
   238|         Returns:
   239|             Tuple[Optional[int], Optional[StateMap[str]]]:
   240|                 (prev_group, delta_ids)
   241|         """
   242|         return await self.stores.state.get_state_group_delta(state_group)
   243|     async def get_state_groups_ids(
   244|         self, _room_id: str, event_ids: Iterable[str]
   245|     ) -> Dict[int, MutableStateMap[str]]:
   246|         """Get the event IDs of all the state for the state groups for the given events
   247|         Args:
   248|             _room_id: id of the room for these events
   249|             event_ids: ids of the events
   250|         Returns:
   251|             dict of state_group_id -> (dict of (type, state_key) -> event id)
   252|         """
   253|         if not event_ids:
   254|             return {}
   255|         event_to_groups = await self.stores.main._get_state_group_for_events(event_ids)
   256|         groups = set(event_to_groups.values())
   257|         group_to_state = await self.stores.state._get_state_for_groups(groups)
   258|         return group_to_state
   259|     async def get_state_ids_for_group(self, state_group: int) -> StateMap[str]:
   260|         """Get the event IDs of all the state in the given state group
   261|         Args:
   262|             state_group: A state group for which we want to get the state IDs.
   263|         Returns:
   264|             Resolves to a map of (type, state_key) -> event_id
   265|         """

# --- HUNK 3: Lines 369-409 ---
   369|         Returns:
   370|             A dict from (type, state_key) -> state_event
   371|         """
   372|         state_map = await self.get_state_for_events([event_id], state_filter)
   373|         return state_map[event_id]
   374|     async def get_state_ids_for_event(
   375|         self, event_id: str, state_filter: StateFilter = StateFilter.all()
   376|     ):
   377|         """
   378|         Get the state dict corresponding to a particular event
   379|         Args:
   380|             event_id: event whose state should be returned
   381|             state_filter: The state filter used to fetch state from the database.
   382|         Returns:
   383|             A dict from (type, state_key) -> state_event
   384|         """
   385|         state_map = await self.get_state_ids_for_events([event_id], state_filter)
   386|         return state_map[event_id]
   387|     def _get_state_for_groups(
   388|         self, groups: Iterable[int], state_filter: StateFilter = StateFilter.all()
   389|     ) -> Awaitable[Dict[int, MutableStateMap[str]]]:
   390|         """Gets the state at each of a list of state groups, optionally
   391|         filtering by type/state_key
   392|         Args:
   393|             groups: list of state groups for which we want to get the state.
   394|             state_filter: The state filter used to fetch state.
   395|                 from the database.
   396|         Returns:
   397|             Dict of state group to state map.
   398|         """
   399|         return self.stores.state._get_state_for_groups(groups, state_filter)
   400|     async def store_state_group(
   401|         self,
   402|         event_id: str,
   403|         room_id: str,
   404|         prev_group: Optional[int],
   405|         delta_ids: Optional[dict],
   406|         current_state_ids: dict,
   407|     ) -> int:
   408|         """Store a new set of state, returning a newly assigned state group.
   409|         Args:


# ====================================================================
# FILE: synapse/storage/util/id_generators.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| import heapq
     2| import logging
     3| import threading
     4| from collections import deque
     5| from contextlib import contextmanager
     6| from typing import Dict, List, Optional, Set, Union
     7| import attr
     8| from typing_extensions import Deque
     9| from synapse.metrics.background_process_metrics import run_as_background_process
    10| from synapse.storage.database import DatabasePool, LoggingTransaction
    11| from synapse.storage.types import Cursor
    12| from synapse.storage.util.sequence import PostgresSequenceGenerator
    13| logger = logging.getLogger(__name__)
    14| class IdGenerator:
    15|     def __init__(self, db_conn, table, column):
    16|         self._lock = threading.Lock()
    17|         self._next_id = _load_current_id(db_conn, table, column)
    18|     def get_next(self):
    19|         with self._lock:
    20|             self._next_id += 1
    21|             return self._next_id
    22| def _load_current_id(db_conn, table, column, step=1):
    23|     """
    24|     Args:
    25|         db_conn (object):
    26|         table (str):
    27|         column (str):
    28|         step (int):
    29|     Returns:
    30|         int
    31|     """

# --- HUNK 2: Lines 41-419 ---
    41|     return (max if step > 0 else min)(current_id, step)
    42| class StreamIdGenerator:
    43|     """Used to generate new stream ids when persisting events while keeping
    44|     track of which transactions have been completed.
    45|     This allows us to get the "current" stream id, i.e. the stream id such that
    46|     all ids less than or equal to it have completed. This handles the fact that
    47|     persistence of events can complete out of order.
    48|     Args:
    49|         db_conn(connection):  A database connection to use to fetch the
    50|             initial value of the generator from.
    51|         table(str): A database table to read the initial value of the id
    52|             generator from.
    53|         column(str): The column of the database table to read the initial
    54|             value from the id generator from.
    55|         extra_tables(list): List of pairs of database tables and columns to
    56|             use to source the initial value of the generator from. The value
    57|             with the largest magnitude is used.
    58|         step(int): which direction the stream ids grow in. +1 to grow
    59|             upwards, -1 to grow downwards.
    60|     Usage:
    61|         async with stream_id_gen.get_next() as stream_id:
    62|     """
    63|     def __init__(self, db_conn, table, column, extra_tables=[], step=1):
    64|         assert step != 0
    65|         self._lock = threading.Lock()
    66|         self._step = step
    67|         self._current = _load_current_id(db_conn, table, column, step)
    68|         for table, column in extra_tables:
    69|             self._current = (max if step > 0 else min)(
    70|                 self._current, _load_current_id(db_conn, table, column, step)
    71|             )
    72|         self._unfinished_ids = deque()  # type: Deque[int]
    73|     def get_next(self):
    74|         """
    75|         Usage:
    76|             async with stream_id_gen.get_next() as stream_id:
    77|         """
    78|         with self._lock:
    79|             self._current += self._step
    80|             next_id = self._current
    81|             self._unfinished_ids.append(next_id)
    82|         @contextmanager
    83|         def manager():
    84|             try:
    85|                 yield next_id
    86|             finally:
    87|                 with self._lock:
    88|                     self._unfinished_ids.remove(next_id)
    89|         return _AsyncCtxManagerWrapper(manager())
    90|     def get_next_mult(self, n):
    91|         """
    92|         Usage:
    93|             async with stream_id_gen.get_next(n) as stream_ids:
    94|         """
    95|         with self._lock:
    96|             next_ids = range(
    97|                 self._current + self._step,
    98|                 self._current + self._step * (n + 1),
    99|                 self._step,
   100|             )
   101|             self._current += n * self._step
   102|             for next_id in next_ids:
   103|                 self._unfinished_ids.append(next_id)
   104|         @contextmanager
   105|         def manager():
   106|             try:
   107|                 yield next_ids
   108|             finally:
   109|                 with self._lock:
   110|                     for next_id in next_ids:
   111|                         self._unfinished_ids.remove(next_id)
   112|         return _AsyncCtxManagerWrapper(manager())
   113|     def get_current_token(self):
   114|         """Returns the maximum stream id such that all stream ids less than or
   115|         equal to it have been successfully persisted.
   116|         Returns:
   117|             int
   118|         """
   119|         with self._lock:
   120|             if self._unfinished_ids:
   121|                 return self._unfinished_ids[0] - self._step
   122|             return self._current
   123|     def get_current_token_for_writer(self, instance_name: str) -> int:
   124|         """Returns the position of the given writer.
   125|         For streams with single writers this is equivalent to
   126|         `get_current_token`.
   127|         """
   128|         return self.get_current_token()
   129| class MultiWriterIdGenerator:
   130|     """An ID generator that tracks a stream that can have multiple writers.
   131|     Uses a Postgres sequence to coordinate ID assignment, but positions of other
   132|     writers will only get updated when `advance` is called (by replication).
   133|     Note: Only works with Postgres.
   134|     Args:
   135|         db_conn
   136|         db
   137|         stream_name: A name for the stream.
   138|         instance_name: The name of this instance.
   139|         table: Database table associated with stream.
   140|         instance_column: Column that stores the row's writer's instance name
   141|         id_column: Column that stores the stream ID.
   142|         sequence_name: The name of the postgres sequence used to generate new
   143|             IDs.
   144|         writers: A list of known writers to use to populate current positions
   145|             on startup. Can be empty if nothing uses `get_current_token` or
   146|             `get_positions` (e.g. caches stream).
   147|         positive: Whether the IDs are positive (true) or negative (false).
   148|             When using negative IDs we go backwards from -1 to -2, -3, etc.
   149|     """
   150|     def __init__(
   151|         self,
   152|         db_conn,
   153|         db: DatabasePool,
   154|         stream_name: str,
   155|         instance_name: str,
   156|         table: str,
   157|         instance_column: str,
   158|         id_column: str,
   159|         sequence_name: str,
   160|         writers: List[str],
   161|         positive: bool = True,
   162|     ):
   163|         self._db = db
   164|         self._stream_name = stream_name
   165|         self._instance_name = instance_name
   166|         self._positive = positive
   167|         self._writers = writers
   168|         self._return_factor = 1 if positive else -1
   169|         self._lock = threading.Lock()
   170|         self._current_positions = {}  # type: Dict[str, int]
   171|         self._unfinished_ids = set()  # type: Set[int]
   172|         self._finished_ids = set()  # type: Set[int]
   173|         self._persisted_upto_position = (
   174|             min(self._current_positions.values()) if self._current_positions else 1
   175|         )
   176|         self._known_persisted_positions = []  # type: List[int]
   177|         self._sequence_gen = PostgresSequenceGenerator(sequence_name)
   178|         self._sequence_gen.check_consistency(
   179|             db_conn, table=table, id_column=id_column, positive=positive
   180|         )
   181|         self._load_current_ids(db_conn, table, instance_column, id_column)
   182|     def _load_current_ids(
   183|         self, db_conn, table: str, instance_column: str, id_column: str
   184|     ):
   185|         cur = db_conn.cursor()
   186|         if self._writers:
   187|             sql = """
   188|                 DELETE FROM stream_positions
   189|                 WHERE
   190|                     stream_name = ?
   191|                     AND instance_name != ALL(?)
   192|             """
   193|             sql = self._db.engine.convert_param_style(sql)
   194|             cur.execute(sql, (self._stream_name, self._writers))
   195|             sql = """
   196|                 SELECT instance_name, stream_id FROM stream_positions
   197|                 WHERE stream_name = ?
   198|             """
   199|             sql = self._db.engine.convert_param_style(sql)
   200|             cur.execute(sql, (self._stream_name,))
   201|             self._current_positions = {
   202|                 instance: stream_id * self._return_factor
   203|                 for instance, stream_id in cur
   204|                 if instance in self._writers
   205|             }
   206|         min_stream_id = min(self._current_positions.values(), default=None)
   207|         if min_stream_id is None:
   208|             sql = """
   209|                 SELECT GREATEST(COALESCE(%(agg)s(%(id)s), 1), 1)
   210|                 FROM %(table)s
   211|             """ % {
   212|                 "id": id_column,
   213|                 "table": table,
   214|                 "agg": "MAX" if self._positive else "-MIN",
   215|             }
   216|             cur.execute(sql)
   217|             (stream_id,) = cur.fetchone()
   218|             self._persisted_upto_position = stream_id
   219|         else:
   220|             sql = """
   221|                 SELECT %(instance)s, %(id)s FROM %(table)s
   222|                 WHERE ? %(cmp)s %(id)s
   223|             """ % {
   224|                 "id": id_column,
   225|                 "table": table,
   226|                 "instance": instance_column,
   227|                 "cmp": "<=" if self._positive else ">=",
   228|             }
   229|             sql = self._db.engine.convert_param_style(sql)
   230|             cur.execute(sql, (min_stream_id * self._return_factor,))
   231|             self._persisted_upto_position = min_stream_id
   232|             with self._lock:
   233|                 for (instance, stream_id,) in cur:
   234|                     stream_id = self._return_factor * stream_id
   235|                     self._add_persisted_position(stream_id)
   236|                     if instance == self._instance_name:
   237|                         self._current_positions[instance] = stream_id
   238|         cur.close()
   239|     def _load_next_id_txn(self, txn) -> int:
   240|         return self._sequence_gen.get_next_id_txn(txn)
   241|     def _load_next_mult_id_txn(self, txn, n: int) -> List[int]:
   242|         return self._sequence_gen.get_next_mult_txn(txn, n)
   243|     def get_next(self):
   244|         """
   245|         Usage:
   246|             async with stream_id_gen.get_next() as stream_id:
   247|         """
   248|         return _MultiWriterCtxManager(self)
   249|     def get_next_mult(self, n: int):
   250|         """
   251|         Usage:
   252|             async with stream_id_gen.get_next_mult(5) as stream_ids:
   253|         """
   254|         return _MultiWriterCtxManager(self, n)
   255|     def get_next_txn(self, txn: LoggingTransaction):
   256|         """
   257|         Usage:
   258|             stream_id = stream_id_gen.get_next(txn)
   259|         """
   260|         next_id = self._load_next_id_txn(txn)
   261|         with self._lock:
   262|             self._unfinished_ids.add(next_id)
   263|         txn.call_after(self._mark_id_as_finished, next_id)
   264|         txn.call_on_exception(self._mark_id_as_finished, next_id)
   265|         if self._writers:
   266|             txn.call_after(
   267|                 run_as_background_process,
   268|                 "MultiWriterIdGenerator._update_table",
   269|                 self._db.runInteraction,
   270|                 "MultiWriterIdGenerator._update_table",
   271|                 self._update_stream_positions_table_txn,
   272|             )
   273|         return self._return_factor * next_id
   274|     def _mark_id_as_finished(self, next_id: int):
   275|         """The ID has finished being processed so we should advance the
   276|         current position if possible.
   277|         """
   278|         with self._lock:
   279|             self._unfinished_ids.discard(next_id)
   280|             self._finished_ids.add(next_id)
   281|             new_cur = None
   282|             if self._unfinished_ids:
   283|                 finished = set()
   284|                 min_unfinshed = min(self._unfinished_ids)
   285|                 for s in self._finished_ids:
   286|                     if s < min_unfinshed:
   287|                         if new_cur is None or new_cur < s:
   288|                             new_cur = s
   289|                     else:
   290|                         finished.add(s)
   291|                 self._finished_ids = finished
   292|             else:
   293|                 new_cur = max(self._finished_ids)
   294|                 self._finished_ids.clear()
   295|             if new_cur:
   296|                 curr = self._current_positions.get(self._instance_name, 0)
   297|                 self._current_positions[self._instance_name] = max(curr, new_cur)
   298|             self._add_persisted_position(next_id)
   299|     def get_current_token(self) -> int:
   300|         """Returns the maximum stream id such that all stream ids less than or
   301|         equal to it have been successfully persisted.
   302|         """
   303|         return self.get_persisted_upto_position()
   304|     def get_current_token_for_writer(self, instance_name: str) -> int:
   305|         """Returns the position of the given writer.
   306|         """
   307|         with self._lock:
   308|             return self._return_factor * self._current_positions.get(
   309|                 instance_name, self._persisted_upto_position
   310|             )
   311|     def get_positions(self) -> Dict[str, int]:
   312|         """Get a copy of the current positon map.
   313|         Note that this won't necessarily include all configured writers if some
   314|         writers haven't written anything yet.
   315|         """
   316|         with self._lock:
   317|             return {
   318|                 name: self._return_factor * i
   319|                 for name, i in self._current_positions.items()
   320|             }
   321|     def advance(self, instance_name: str, new_id: int):
   322|         """Advance the postion of the named writer to the given ID, if greater
   323|         than existing entry.
   324|         """
   325|         new_id *= self._return_factor
   326|         with self._lock:
   327|             self._current_positions[instance_name] = max(
   328|                 new_id, self._current_positions.get(instance_name, 0)
   329|             )
   330|             self._add_persisted_position(new_id)
   331|     def get_persisted_upto_position(self) -> int:
   332|         """Get the max position where all previous positions have been
   333|         persisted.
   334|         Note: In the worst case scenario this will be equal to the minimum
   335|         position across writers. This means that the returned position here can
   336|         lag if one writer doesn't write very often.
   337|         """
   338|         with self._lock:
   339|             return self._return_factor * self._persisted_upto_position
   340|     def _add_persisted_position(self, new_id: int):
   341|         """Record that we have persisted a position.
   342|         This is used to keep the `_current_positions` up to date.
   343|         """
   344|         assert self._lock.locked()
   345|         heapq.heappush(self._known_persisted_positions, new_id)
   346|         min_curr = min(self._current_positions.values(), default=0)
   347|         self._persisted_upto_position = max(min_curr, self._persisted_upto_position)
   348|         while self._known_persisted_positions:
   349|             if self._known_persisted_positions[0] <= self._persisted_upto_position:
   350|                 heapq.heappop(self._known_persisted_positions)
   351|             elif (
   352|                 self._known_persisted_positions[0] == self._persisted_upto_position + 1
   353|             ):
   354|                 heapq.heappop(self._known_persisted_positions)
   355|                 self._persisted_upto_position += 1
   356|             else:
   357|                 break
   358|     def _update_stream_positions_table_txn(self, txn: Cursor):
   359|         """Update the `stream_positions` table with newly persisted position.
   360|         """
   361|         if not self._writers:
   362|             return
   363|         sql = """
   364|             INSERT INTO stream_positions (stream_name, instance_name, stream_id)
   365|             VALUES (?, ?, ?)
   366|             ON CONFLICT (stream_name, instance_name)
   367|             DO UPDATE SET
   368|                 stream_id = %(agg)s(stream_positions.stream_id, EXCLUDED.stream_id)
   369|         """ % {
   370|             "agg": "GREATEST" if self._positive else "LEAST",
   371|         }
   372|         pos = (self.get_current_token_for_writer(self._instance_name),)
   373|         txn.execute(sql, (self._stream_name, self._instance_name, pos))
   374| @attr.s(slots=True)
   375| class _AsyncCtxManagerWrapper:
   376|     """Helper class to convert a plain context manager to an async one.
   377|     This is mainly useful if you have a plain context manager but the interface
   378|     requires an async one.
   379|     """
   380|     inner = attr.ib()
   381|     async def __aenter__(self):
   382|         return self.inner.__enter__()
   383|     async def __aexit__(self, exc_type, exc, tb):
   384|         return self.inner.__exit__(exc_type, exc, tb)
   385| @attr.s(slots=True)
   386| class _MultiWriterCtxManager:
   387|     """Async context manager returned by MultiWriterIdGenerator
   388|     """
   389|     id_gen = attr.ib(type=MultiWriterIdGenerator)
   390|     multiple_ids = attr.ib(type=Optional[int], default=None)
   391|     stream_ids = attr.ib(type=List[int], factory=list)
   392|     async def __aenter__(self) -> Union[int, List[int]]:
   393|         self.stream_ids = await self.id_gen._db.runInteraction(
   394|             "_load_next_mult_id",
   395|             self.id_gen._load_next_mult_id_txn,
   396|             self.multiple_ids or 1,
   397|             db_autocommit=True,
   398|         )
   399|         with self.id_gen._lock:
   400|             assert max(self.id_gen._current_positions.values(), default=0) < min(
   401|                 self.stream_ids
   402|             )
   403|             self.id_gen._unfinished_ids.update(self.stream_ids)
   404|         if self.multiple_ids is None:
   405|             return self.stream_ids[0] * self.id_gen._return_factor
   406|         else:
   407|             return [i * self.id_gen._return_factor for i in self.stream_ids]
   408|     async def __aexit__(self, exc_type, exc, tb):
   409|         for i in self.stream_ids:
   410|             self.id_gen._mark_id_as_finished(i)
   411|         if exc_type is not None:
   412|             return False
   413|         if self.id_gen._writers:
   414|             await self.id_gen._db.runInteraction(
   415|                 "MultiWriterIdGenerator._update_table",
   416|                 self.id_gen._update_stream_positions_table_txn,
   417|                 db_autocommit=True,
   418|             )
   419|         return False


# ====================================================================
# FILE: synapse/storage/util/sequence.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-128 ---
     1| import abc
     2| import logging
     3| import threading
     4| from typing import Callable, List, Optional
     5| from synapse.storage.engines import (
     6|     BaseDatabaseEngine,
     7|     IncorrectDatabaseSetup,
     8|     PostgresEngine,
     9| )
    10| from synapse.storage.types import Connection, Cursor
    11| logger = logging.getLogger(__name__)
    12| _INCONSISTENT_SEQUENCE_ERROR = """
    13| Postgres sequence '%(seq)s' is inconsistent with associated
    14| table '%(table)s'. This can happen if Synapse has been downgraded and
    15| then upgraded again, or due to a bad migration.
    16| To fix this error, shut down Synapse (including any and all workers)
    17| and run the following SQL:
    18|     SELECT setval('%(seq)s', (
    19|         %(max_id_sql)s
    20|     ));
    21| See docs/postgres.md for more information.
    22| """
    23| class SequenceGenerator(metaclass=abc.ABCMeta):
    24|     """A class which generates a unique sequence of integers"""
    25|     @abc.abstractmethod
    26|     def get_next_id_txn(self, txn: Cursor) -> int:
    27|         """Gets the next ID in the sequence"""
    28|         ...
    29|     @abc.abstractmethod
    30|     def check_consistency(
    31|         self, db_conn: Connection, table: str, id_column: str, positive: bool = True
    32|     ):
    33|         """Should be called during start up to test that the current value of
    34|         the sequence is greater than or equal to the maximum ID in the table.
    35|         This is to handle various cases where the sequence value can get out
    36|         of sync with the table, e.g. if Synapse gets rolled back to a previous
    37|         version and the rolled forwards again.
    38|         """
    39|         ...
    40| class PostgresSequenceGenerator(SequenceGenerator):
    41|     """An implementation of SequenceGenerator which uses a postgres sequence"""
    42|     def __init__(self, sequence_name: str):
    43|         self._sequence_name = sequence_name
    44|     def get_next_id_txn(self, txn: Cursor) -> int:
    45|         txn.execute("SELECT nextval(?)", (self._sequence_name,))
    46|         return txn.fetchone()[0]
    47|     def get_next_mult_txn(self, txn: Cursor, n: int) -> List[int]:
    48|         txn.execute(
    49|             "SELECT nextval(?) FROM generate_series(1, ?)", (self._sequence_name, n)
    50|         )
    51|         return [i for (i,) in txn]
    52|     def check_consistency(
    53|         self, db_conn: Connection, table: str, id_column: str, positive: bool = True
    54|     ):
    55|         txn = db_conn.cursor()
    56|         table_sql = "SELECT GREATEST(%(agg)s(%(id)s), 0) FROM %(table)s" % {
    57|             "id": id_column,
    58|             "table": table,
    59|             "agg": "MAX" if positive else "-MIN",
    60|         }
    61|         txn.execute(table_sql)
    62|         row = txn.fetchone()
    63|         if not row:
    64|             txn.close()
    65|             return
    66|         max_stream_id = row[0]
    67|         txn.execute(
    68|             "SELECT last_value, is_called FROM %(seq)s" % {"seq": self._sequence_name}
    69|         )
    70|         last_value, is_called = txn.fetchone()
    71|         txn.close()
    72|         if not is_called:
    73|             last_value -= 1
    74|         if max_stream_id > last_value:
    75|             logger.warning(
    76|                 "Postgres sequence %s is behind table %s: %d < %d",
    77|                 last_value,
    78|                 max_stream_id,
    79|             )
    80|             raise IncorrectDatabaseSetup(
    81|                 _INCONSISTENT_SEQUENCE_ERROR
    82|                 % {"seq": self._sequence_name, "table": table, "max_id_sql": table_sql}
    83|             )
    84| GetFirstCallbackType = Callable[[Cursor], int]
    85| class LocalSequenceGenerator(SequenceGenerator):
    86|     """An implementation of SequenceGenerator which uses local locking
    87|     This only works reliably if there are no other worker processes generating IDs at
    88|     the same time.
    89|     """
    90|     def __init__(self, get_first_callback: GetFirstCallbackType):
    91|         """
    92|         Args:
    93|             get_first_callback: a callback which is called on the first call to
    94|                  get_next_id_txn; should return the curreent maximum id
    95|         """
    96|         self._callback = get_first_callback  # type: Optional[GetFirstCallbackType]
    97|         self._current_max_id = None  # type: Optional[int]
    98|         self._lock = threading.Lock()
    99|     def get_next_id_txn(self, txn: Cursor) -> int:
   100|         with self._lock:
   101|             if self._current_max_id is None:
   102|                 assert self._callback is not None
   103|                 self._current_max_id = self._callback(txn)
   104|                 self._callback = None
   105|             self._current_max_id += 1
   106|             return self._current_max_id
   107|     def check_consistency(
   108|         self, db_conn: Connection, table: str, id_column: str, positive: bool = True
   109|     ):
   110|         pass
   111| def build_sequence_generator(
   112|     database_engine: BaseDatabaseEngine,
   113|     get_first_callback: GetFirstCallbackType,
   114|     sequence_name: str,
   115| ) -> SequenceGenerator:
   116|     """Get the best impl of SequenceGenerator available
   117|     This uses PostgresSequenceGenerator on postgres, and a locally-locked impl on
   118|     sqlite.
   119|     Args:
   120|         database_engine: the database engine we are connected to
   121|         get_first_callback: a callback which gets the next sequence ID. Used if
   122|             we're on sqlite.
   123|         sequence_name: the name of a postgres sequence to use.
   124|     """
   125|     if isinstance(database_engine, PostgresEngine):
   126|         return PostgresSequenceGenerator(sequence_name)
   127|     else:
   128|         return LocalSequenceGenerator(get_first_callback)


# ====================================================================
# FILE: synapse/streams/config.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-57 ---
     1| import logging
     2| from typing import Optional
     3| import attr
     4| from synapse.api.errors import SynapseError
     5| from synapse.http.servlet import parse_integer, parse_string
     6| from synapse.http.site import SynapseRequest
     7| from synapse.storage.databases.main import DataStore
     8| from synapse.types import StreamToken
     9| logger = logging.getLogger(__name__)
    10| MAX_LIMIT = 1000
    11| @attr.s(slots=True)
    12| class PaginationConfig:
    13|     """A configuration object which stores pagination parameters."""
    14|     from_token = attr.ib(type=Optional[StreamToken])
    15|     to_token = attr.ib(type=Optional[StreamToken])
    16|     direction = attr.ib(type=str)
    17|     limit = attr.ib(type=Optional[int])
    18|     @classmethod
    19|     async def from_request(
    20|         cls,
    21|         store: "DataStore",
    22|         request: SynapseRequest,
    23|         raise_invalid_params: bool = True,
    24|         default_limit: Optional[int] = None,
    25|     ) -> "PaginationConfig":
    26|         direction = parse_string(request, "dir", default="f", allowed_values=["f", "b"])
    27|         from_tok = parse_string(request, "from")
    28|         to_tok = parse_string(request, "to")
    29|         try:
    30|             if from_tok == "END":
    31|                 from_tok = None  # For backwards compat.
    32|             elif from_tok:
    33|                 from_tok = await StreamToken.from_string(store, from_tok)
    34|         except Exception:
    35|             raise SynapseError(400, "'from' parameter is invalid")
    36|         try:
    37|             if to_tok:
    38|                 to_tok = await StreamToken.from_string(store, to_tok)
    39|         except Exception:
    40|             raise SynapseError(400, "'to' parameter is invalid")
    41|         limit = parse_integer(request, "limit", default=default_limit)
    42|         if limit:
    43|             if limit < 0:
    44|                 raise SynapseError(400, "Limit must be 0 or above")
    45|             limit = min(int(limit), MAX_LIMIT)
    46|         try:
    47|             return PaginationConfig(from_tok, to_tok, direction, limit)
    48|         except Exception:
    49|             logger.exception("Failed to create pagination config")
    50|             raise SynapseError(400, "Invalid request.")
    51|     def __repr__(self) -> str:
    52|         return ("PaginationConfig(from_tok=%r, to_tok=%r, direction=%r, limit=%r)") % (
    53|             self.from_token,
    54|             self.to_token,
    55|             self.direction,
    56|             self.limit,
    57|         )


# ====================================================================
# FILE: synapse/types.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-42 ---
     1| import abc
     2| import re
     3| import string
     4| import sys
     5| from collections import namedtuple
     6| from typing import (
     7|     TYPE_CHECKING,
     8|     Any,
     9|     Dict,
    10|     Mapping,
    11|     MutableMapping,
    12|     Optional,
    13|     Tuple,
    14|     Type,
    15|     TypeVar,
    16| )
    17| import attr
    18| from signedjson.key import decode_verify_key_bytes
    19| from unpaddedbase64 import decode_base64
    20| from synapse.api.errors import Codes, SynapseError
    21| if TYPE_CHECKING:
    22|     from synapse.storage.databases.main import DataStore
    23| if sys.version_info[:3] >= (3, 6, 0):
    24|     from typing import Collection
    25| else:
    26|     from typing import Container, Iterable, Sized
    27|     T_co = TypeVar("T_co", covariant=True)
    28|     class Collection(Iterable[T_co], Container[T_co], Sized):  # type: ignore
    29|         __slots__ = ()
    30| T = TypeVar("T")
    31| StateKey = Tuple[str, str]
    32| StateMap = Mapping[StateKey, T]
    33| MutableStateMap = MutableMapping[StateKey, T]
    34| JsonDict = Dict[str, Any]
    35| class Requester(
    36|     namedtuple(
    37|         "Requester",
    38|         [
    39|             "user",
    40|             "access_token_id",
    41|             "is_guest",
    42|             "shadow_banned",

# --- HUNK 2: Lines 111-159 ---
   111|         app_service (ApplicationService|None):  the AS requesting on behalf of the user
   112|     Returns:
   113|         Requester
   114|     """
   115|     if not isinstance(user_id, UserID):
   116|         user_id = UserID.from_string(user_id)
   117|     return Requester(
   118|         user_id, access_token_id, is_guest, shadow_banned, device_id, app_service
   119|     )
   120| def get_domain_from_id(string):
   121|     idx = string.find(":")
   122|     if idx == -1:
   123|         raise SynapseError(400, "Invalid ID: %r" % (string,))
   124|     return string[idx + 1 :]
   125| def get_localpart_from_id(string):
   126|     idx = string.find(":")
   127|     if idx == -1:
   128|         raise SynapseError(400, "Invalid ID: %r" % (string,))
   129|     return string[1:idx]
   130| DS = TypeVar("DS", bound="DomainSpecificString")
   131| class DomainSpecificString(
   132|     namedtuple("DomainSpecificString", ("localpart", "domain")), metaclass=abc.ABCMeta
   133| ):
   134|     """Common base class among ID/name strings that have a local part and a
   135|     domain name, prefixed with a sigil.
   136|     Has the fields:
   137|         'localpart' : The local part of the name (without the leading sigil)
   138|         'domain' : The domain part of the name
   139|     """
   140|     SIGIL = abc.abstractproperty()  # type: str  # type: ignore
   141|     def __iter__(self):
   142|         raise ValueError("Attempted to iterate a %s" % (type(self).__name__,))
   143|     def __copy__(self):
   144|         return self
   145|     def __deepcopy__(self, memo):
   146|         return self
   147|     @classmethod
   148|     def from_string(cls: Type[DS], s: str) -> DS:
   149|         """Parse the string given by 's' into a structure object."""
   150|         if len(s) < 1 or s[0:1] != cls.SIGIL:
   151|             raise SynapseError(
   152|                 400,
   153|                 "Expected %s string to start with '%s'" % (cls.__name__, cls.SIGIL),
   154|                 Codes.INVALID_PARAM,
   155|             )
   156|         parts = s[1:].split(":", 1)
   157|         if len(parts) != 2:
   158|             raise SynapseError(
   159|                 400,

# --- HUNK 3: Lines 228-404 ---
   228|             onto different mxids
   229|     Returns:
   230|         unicode: string suitable for a mxid localpart
   231|     """
   232|     if not isinstance(username, bytes):
   233|         username = username.encode("utf-8")
   234|     if case_sensitive:
   235|         def f1(m):
   236|             return b"_" + m.group().lower()
   237|         username = UPPER_CASE_PATTERN.sub(f1, username)
   238|     else:
   239|         username = username.lower()
   240|     def f2(m):
   241|         g = m.group()[0]
   242|         if isinstance(g, str):
   243|             g = ord(g)
   244|         return b"=%02x" % (g,)
   245|     username = NON_MXID_CHARACTER_PATTERN.sub(f2, username)
   246|     username = re.sub(b"^_", b"=5f", username)
   247|     return username.decode("ascii")
   248| @attr.s(frozen=True, slots=True)
   249| class RoomStreamToken:
   250|     """Tokens are positions between events. The token "s1" comes after event 1.
   251|             s0    s1
   252|             |     |
   253|         [0] V [1] V [2]
   254|     Tokens can either be a point in the live event stream or a cursor going
   255|     through historic events.
   256|     When traversing the live event stream events are ordered by when they
   257|     arrived at the homeserver.
   258|     When traversing historic events the events are ordered by their depth in
   259|     the event graph "topological_ordering" and then by when they arrived at the
   260|     homeserver "stream_ordering".
   261|     Live tokens start with an "s" followed by the "stream_ordering" id of the
   262|     event it comes after. Historic tokens start with a "t" followed by the
   263|     "topological_ordering" id of the event it comes after, followed by "-",
   264|     followed by the "stream_ordering" id of the event it comes after.
   265|     """
   266|     topological = attr.ib(
   267|         type=Optional[int],
   268|         validator=attr.validators.optional(attr.validators.instance_of(int)),
   269|     )
   270|     stream = attr.ib(type=int, validator=attr.validators.instance_of(int))
   271|     @classmethod
   272|     async def parse(cls, store: "DataStore", string: str) -> "RoomStreamToken":
   273|         try:
   274|             if string[0] == "s":
   275|                 return cls(topological=None, stream=int(string[1:]))
   276|             if string[0] == "t":
   277|                 parts = string[1:].split("-", 1)
   278|                 return cls(topological=int(parts[0]), stream=int(parts[1]))
   279|         except Exception:
   280|             pass
   281|         raise SynapseError(400, "Invalid token %r" % (string,))
   282|     @classmethod
   283|     def parse_stream_token(cls, string: str) -> "RoomStreamToken":
   284|         try:
   285|             if string[0] == "s":
   286|                 return cls(topological=None, stream=int(string[1:]))
   287|         except Exception:
   288|             pass
   289|         raise SynapseError(400, "Invalid token %r" % (string,))
   290|     def copy_and_advance(self, other: "RoomStreamToken") -> "RoomStreamToken":
   291|         """Return a new token such that if an event is after both this token and
   292|         the other token, then its after the returned token too.
   293|         """
   294|         if self.topological or other.topological:
   295|             raise Exception("Can't advance topological tokens")
   296|         max_stream = max(self.stream, other.stream)
   297|         return RoomStreamToken(None, max_stream)
   298|     def as_tuple(self) -> Tuple[Optional[int], int]:
   299|         return (self.topological, self.stream)
   300|     async def to_string(self, store: "DataStore") -> str:
   301|         if self.topological is not None:
   302|             return "t%d-%d" % (self.topological, self.stream)
   303|         else:
   304|             return "s%d" % (self.stream,)
   305| @attr.s(slots=True, frozen=True)
   306| class StreamToken:
   307|     room_key = attr.ib(
   308|         type=RoomStreamToken, validator=attr.validators.instance_of(RoomStreamToken)
   309|     )
   310|     presence_key = attr.ib(type=int)
   311|     typing_key = attr.ib(type=int)
   312|     receipt_key = attr.ib(type=int)
   313|     account_data_key = attr.ib(type=int)
   314|     push_rules_key = attr.ib(type=int)
   315|     to_device_key = attr.ib(type=int)
   316|     device_list_key = attr.ib(type=int)
   317|     groups_key = attr.ib(type=int)
   318|     _SEPARATOR = "_"
   319|     START = None  # type: StreamToken
   320|     @classmethod
   321|     async def from_string(cls, store: "DataStore", string: str) -> "StreamToken":
   322|         try:
   323|             keys = string.split(cls._SEPARATOR)
   324|             while len(keys) < len(attr.fields(cls)):
   325|                 keys.append("0")
   326|             return cls(
   327|                 await RoomStreamToken.parse(store, keys[0]), *(int(k) for k in keys[1:])
   328|             )
   329|         except Exception:
   330|             raise SynapseError(400, "Invalid Token")
   331|     async def to_string(self, store: "DataStore") -> str:
   332|         return self._SEPARATOR.join(
   333|             [
   334|                 await self.room_key.to_string(store),
   335|                 str(self.presence_key),
   336|                 str(self.typing_key),
   337|                 str(self.receipt_key),
   338|                 str(self.account_data_key),
   339|                 str(self.push_rules_key),
   340|                 str(self.to_device_key),
   341|                 str(self.device_list_key),
   342|                 str(self.groups_key),
   343|             ]
   344|         )
   345|     @property
   346|     def room_stream_id(self):
   347|         return self.room_key.stream
   348|     def copy_and_advance(self, key, new_value) -> "StreamToken":
   349|         """Advance the given key in the token to a new value if and only if the
   350|         new value is after the old value.
   351|         """
   352|         if key == "room_key":
   353|             new_token = self.copy_and_replace(
   354|                 "room_key", self.room_key.copy_and_advance(new_value)
   355|             )
   356|             return new_token
   357|         new_token = self.copy_and_replace(key, new_value)
   358|         new_id = int(getattr(new_token, key))
   359|         old_id = int(getattr(self, key))
   360|         if old_id < new_id:
   361|             return new_token
   362|         else:
   363|             return self
   364|     def copy_and_replace(self, key, new_value) -> "StreamToken":
   365|         return attr.evolve(self, **{key: new_value})
   366| StreamToken.START = StreamToken(RoomStreamToken(None, 0), 0, 0, 0, 0, 0, 0, 0, 0)
   367| @attr.s(slots=True, frozen=True)
   368| class PersistedEventPosition:
   369|     """Position of a newly persisted event with instance that persisted it.
   370|     This can be used to test whether the event is persisted before or after a
   371|     RoomStreamToken.
   372|     """
   373|     instance_name = attr.ib(type=str)
   374|     stream = attr.ib(type=int)
   375|     def persisted_after(self, token: RoomStreamToken) -> bool:
   376|         return token.stream < self.stream
   377|     def to_room_stream_token(self) -> RoomStreamToken:
   378|         """Converts the position to a room stream token such that events
   379|         persisted in the same room after this position will be after the
   380|         returned `RoomStreamToken`.
   381|         Note: no guarentees are made about ordering w.r.t. events in other
   382|         rooms.
   383|         """
   384|         return RoomStreamToken(None, self.stream)
   385| class ThirdPartyInstanceID(
   386|     namedtuple("ThirdPartyInstanceID", ("appservice_id", "network_id"))
   387| ):
   388|     def __iter__(self):
   389|         raise ValueError("Attempted to iterate a %s" % (type(self).__name__,))
   390|     def __copy__(self):
   391|         return self
   392|     def __deepcopy__(self, memo):
   393|         return self
   394|     @classmethod
   395|     def from_string(cls, s):
   396|         bits = s.split("|", 2)
   397|         if len(bits) != 2:
   398|             raise SynapseError(400, "Invalid ID %r" % (s,))
   399|         return cls(appservice_id=bits[0], network_id=bits[1])
   400|     def to_string(self):
   401|         return "%s|%s" % (self.appservice_id, self.network_id)
   402|     __str__ = to_string
   403|     @classmethod
   404|     def create(cls, appservice_id, network_id):


# ====================================================================
# FILE: synapse/util/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-36 ---
     1| import json
     2| import logging
     3| import re
     4| import attr
     5| from twisted.internet import defer, task
     6| from synapse.logging import context
     7| logger = logging.getLogger(__name__)
     8| def _reject_invalid_json(val):
     9|     """Do not allow Infinity, -Infinity, or NaN values in JSON."""
    10|     raise ValueError("Invalid JSON value: '%s'" % val)
    11| json_encoder = json.JSONEncoder(allow_nan=False, separators=(",", ":"))
    12| json_decoder = json.JSONDecoder(parse_constant=_reject_invalid_json)
    13| def unwrapFirstError(failure):
    14|     failure.trap(defer.FirstError)
    15|     return failure.value.subFailure
    16| @attr.s(slots=True)
    17| class Clock:
    18|     """
    19|     A Clock wraps a Twisted reactor and provides utilities on top of it.
    20|     Args:
    21|         reactor: The Twisted reactor to use.
    22|     """
    23|     _reactor = attr.ib()
    24|     @defer.inlineCallbacks
    25|     def sleep(self, seconds):
    26|         d = defer.Deferred()
    27|         with context.PreserveLoggingContext():
    28|             self._reactor.callLater(seconds, d.callback, seconds)
    29|             res = yield d
    30|         return res
    31|     def time(self):
    32|         """Returns the current system time in seconds since epoch."""
    33|         return self._reactor.seconds()
    34|     def time_msec(self):
    35|         """Returns the current system time in milliseconds since epoch."""
    36|         return int(self.time() * 1000)


# ====================================================================
# FILE: synapse/util/async_helpers.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-62 ---
     1| import collections
     2| import logging
     3| from contextlib import contextmanager
     4| from typing import (
     5|     Any,
     6|     Callable,
     7|     Dict,
     8|     Hashable,
     9|     Iterable,
    10|     List,
    11|     Optional,
    12|     Set,
    13|     TypeVar,
    14|     Union,
    15| )
    16| import attr
    17| from typing_extensions import ContextManager
    18| from twisted.internet import defer
    19| from twisted.internet.defer import CancelledError
    20| from twisted.internet.interfaces import IReactorTime
    21| from twisted.python import failure
    22| from synapse.logging.context import (
    23|     PreserveLoggingContext,
    24|     make_deferred_yieldable,
    25|     run_in_background,
    26| )
    27| from synapse.util import Clock, unwrapFirstError
    28| logger = logging.getLogger(__name__)
    29| class ObservableDeferred:
    30|     """Wraps a deferred object so that we can add observer deferreds. These
    31|     observer deferreds do not affect the callback chain of the original
    32|     deferred.
    33|     If consumeErrors is true errors will be captured from the origin deferred.
    34|     Cancelling or otherwise resolving an observer will not affect the original
    35|     ObservableDeferred.
    36|     NB that it does not attempt to do anything with logcontexts; in general
    37|     you should probably make_deferred_yieldable the deferreds
    38|     returned by `observe`, and ensure that the original deferred runs its
    39|     callbacks in the sentinel logcontext.
    40|     """
    41|     __slots__ = ["_deferred", "_observers", "_result"]
    42|     def __init__(self, deferred: defer.Deferred, consumeErrors: bool = False):
    43|         object.__setattr__(self, "_deferred", deferred)
    44|         object.__setattr__(self, "_result", None)
    45|         object.__setattr__(self, "_observers", set())
    46|         def callback(r):
    47|             object.__setattr__(self, "_result", (True, r))
    48|             while self._observers:
    49|                 try:
    50|                     self._observers.pop().callback(r)
    51|                 except Exception:
    52|                     pass
    53|             return r
    54|         def errback(f):
    55|             object.__setattr__(self, "_result", (False, f))
    56|             while self._observers:
    57|                 f.value.__failure__ = f
    58|                 try:
    59|                     self._observers.pop().errback(f)
    60|                 except Exception:
    61|                     pass
    62|             if consumeErrors:

# --- HUNK 2: Lines 64-259 ---
    64|             else:
    65|                 return f
    66|         deferred.addCallbacks(callback, errback)
    67|     def observe(self) -> defer.Deferred:
    68|         """Observe the underlying deferred.
    69|         This returns a brand new deferred that is resolved when the underlying
    70|         deferred is resolved. Interacting with the returned deferred does not
    71|         effect the underlying deferred.
    72|         """
    73|         if not self._result:
    74|             d = defer.Deferred()
    75|             def remove(r):
    76|                 self._observers.discard(d)
    77|                 return r
    78|             d.addBoth(remove)
    79|             self._observers.add(d)
    80|             return d
    81|         else:
    82|             success, res = self._result
    83|             return defer.succeed(res) if success else defer.fail(res)
    84|     def observers(self) -> List[defer.Deferred]:
    85|         return self._observers
    86|     def has_called(self) -> bool:
    87|         return self._result is not None
    88|     def has_succeeded(self) -> bool:
    89|         return self._result is not None and self._result[0] is True
    90|     def get_result(self) -> Any:
    91|         return self._result[1]
    92|     def __getattr__(self, name: str) -> Any:
    93|         return getattr(self._deferred, name)
    94|     def __setattr__(self, name: str, value: Any) -> None:
    95|         setattr(self._deferred, name, value)
    96|     def __repr__(self) -> str:
    97|         return "<ObservableDeferred object at %s, result=%r, _deferred=%r>" % (
    98|             id(self),
    99|             self._result,
   100|             self._deferred,
   101|         )
   102| def concurrently_execute(
   103|     func: Callable, args: Iterable[Any], limit: int
   104| ) -> defer.Deferred:
   105|     """Executes the function with each argument concurrently while limiting
   106|     the number of concurrent executions.
   107|     Args:
   108|         func: Function to execute, should return a deferred or coroutine.
   109|         args: List of arguments to pass to func, each invocation of func
   110|             gets a single argument.
   111|         limit: Maximum number of conccurent executions.
   112|     Returns:
   113|         Deferred[list]: Resolved when all function invocations have finished.
   114|     """
   115|     it = iter(args)
   116|     async def _concurrently_execute_inner():
   117|         try:
   118|             while True:
   119|                 await maybe_awaitable(func(next(it)))
   120|         except StopIteration:
   121|             pass
   122|     return make_deferred_yieldable(
   123|         defer.gatherResults(
   124|             [run_in_background(_concurrently_execute_inner) for _ in range(limit)],
   125|             consumeErrors=True,
   126|         )
   127|     ).addErrback(unwrapFirstError)
   128| def yieldable_gather_results(
   129|     func: Callable, iter: Iterable, *args: Any, **kwargs: Any
   130| ) -> defer.Deferred:
   131|     """Executes the function with each argument concurrently.
   132|     Args:
   133|         func: Function to execute that returns a Deferred
   134|         iter: An iterable that yields items that get passed as the first
   135|             argument to the function
   136|         *args: Arguments to be passed to each call to func
   137|         **kwargs: Keyword arguments to be passed to each call to func
   138|     Returns
   139|         Deferred[list]: Resolved when all functions have been invoked, or errors if
   140|         one of the function calls fails.
   141|     """
   142|     return make_deferred_yieldable(
   143|         defer.gatherResults(
   144|             [run_in_background(func, item, *args, **kwargs) for item in iter],
   145|             consumeErrors=True,
   146|         )
   147|     ).addErrback(unwrapFirstError)
   148| @attr.s(slots=True)
   149| class _LinearizerEntry:
   150|     count = attr.ib(type=int)
   151|     deferreds = attr.ib(type=collections.OrderedDict)
   152| class Linearizer:
   153|     """Limits concurrent access to resources based on a key. Useful to ensure
   154|     only a few things happen at a time on a given resource.
   155|     Example:
   156|         with await limiter.queue("test_key"):
   157|     """
   158|     def __init__(
   159|         self,
   160|         name: Optional[str] = None,
   161|         max_count: int = 1,
   162|         clock: Optional[Clock] = None,
   163|     ):
   164|         """
   165|         Args:
   166|             max_count: The maximum number of concurrent accesses
   167|         """
   168|         if name is None:
   169|             self.name = id(self)  # type: Union[str, int]
   170|         else:
   171|             self.name = name
   172|         if not clock:
   173|             from twisted.internet import reactor
   174|             clock = Clock(reactor)
   175|         self._clock = clock
   176|         self.max_count = max_count
   177|         self.key_to_defer = {}  # type: Dict[Hashable, _LinearizerEntry]
   178|     def is_queued(self, key: Hashable) -> bool:
   179|         """Checks whether there is a process queued up waiting
   180|         """
   181|         entry = self.key_to_defer.get(key)
   182|         if not entry:
   183|             return False
   184|         return bool(entry.deferreds)
   185|     def queue(self, key: Hashable) -> defer.Deferred:
   186|         entry = self.key_to_defer.setdefault(
   187|             key, _LinearizerEntry(0, collections.OrderedDict())
   188|         )
   189|         if entry.count >= self.max_count:
   190|             res = self._await_lock(key)
   191|         else:
   192|             logger.debug(
   193|                 "Acquired uncontended linearizer lock %r for key %r", self.name, key
   194|             )
   195|             entry.count += 1
   196|             res = defer.succeed(None)
   197|         @contextmanager
   198|         def _ctx_manager(_):
   199|             try:
   200|                 yield
   201|             finally:
   202|                 logger.debug("Releasing linearizer lock %r for key %r", self.name, key)
   203|                 entry.count -= 1
   204|                 if entry.deferreds:
   205|                     (next_def, _) = entry.deferreds.popitem(last=False)
   206|                     with PreserveLoggingContext():
   207|                         next_def.callback(None)
   208|                 elif entry.count == 0:
   209|                     del self.key_to_defer[key]
   210|         res.addCallback(_ctx_manager)
   211|         return res
   212|     def _await_lock(self, key: Hashable) -> defer.Deferred:
   213|         """Helper for queue: adds a deferred to the queue
   214|         Assumes that we've already checked that we've reached the limit of the number
   215|         of lock-holders we allow. Creates a new deferred which is added to the list, and
   216|         adds some management around cancellations.
   217|         Returns the deferred, which will callback once we have secured the lock.
   218|         """
   219|         entry = self.key_to_defer[key]
   220|         logger.debug("Waiting to acquire linearizer lock %r for key %r", self.name, key)
   221|         new_defer = make_deferred_yieldable(defer.Deferred())
   222|         entry.deferreds[new_defer] = 1
   223|         def cb(_r):
   224|             logger.debug("Acquired linearizer lock %r for key %r", self.name, key)
   225|             entry.count += 1
   226|             return self._clock.sleep(0)
   227|         def eb(e):
   228|             logger.info("defer %r got err %r", new_defer, e)
   229|             if isinstance(e, CancelledError):
   230|                 logger.debug(
   231|                     "Cancelling wait for linearizer lock %r for key %r", self.name, key
   232|                 )
   233|             else:
   234|                 logger.warning(
   235|                     "Unexpected exception waiting for linearizer lock %r for key %r",
   236|                     self.name,
   237|                     key,
   238|                 )
   239|             del entry.deferreds[new_defer]
   240|             return e
   241|         new_defer.addCallbacks(cb, eb)
   242|         return new_defer
   243| class ReadWriteLock:
   244|     """An async read write lock.
   245|     Example:
   246|         with await read_write_lock.read("test_key"):
   247|     """
   248|     def __init__(self):
   249|         self.key_to_current_readers = {}  # type: Dict[str, Set[defer.Deferred]]
   250|         self.key_to_current_writer = {}  # type: Dict[str, defer.Deferred]
   251|     async def read(self, key: str) -> ContextManager:
   252|         new_defer = defer.Deferred()
   253|         curr_readers = self.key_to_current_readers.setdefault(key, set())
   254|         curr_writer = self.key_to_current_writer.get(key, None)
   255|         curr_readers.add(new_defer)
   256|         if curr_writer:
   257|             await make_deferred_yieldable(curr_writer)
   258|         @contextmanager
   259|         def _ctx_manager():

# --- HUNK 3: Lines 265-340 ---
   265|         return _ctx_manager()
   266|     async def write(self, key: str) -> ContextManager:
   267|         new_defer = defer.Deferred()
   268|         curr_readers = self.key_to_current_readers.get(key, set())
   269|         curr_writer = self.key_to_current_writer.get(key, None)
   270|         to_wait_on = list(curr_readers)
   271|         if curr_writer:
   272|             to_wait_on.append(curr_writer)
   273|         curr_readers.clear()
   274|         self.key_to_current_writer[key] = new_defer
   275|         await make_deferred_yieldable(defer.gatherResults(to_wait_on))
   276|         @contextmanager
   277|         def _ctx_manager():
   278|             try:
   279|                 yield
   280|             finally:
   281|                 new_defer.callback(None)
   282|                 if self.key_to_current_writer[key] == new_defer:
   283|                     self.key_to_current_writer.pop(key)
   284|         return _ctx_manager()
   285| R = TypeVar("R")
   286| def timeout_deferred(
   287|     deferred: defer.Deferred, timeout: float, reactor: IReactorTime,
   288| ) -> defer.Deferred:
   289|     """The in built twisted `Deferred.addTimeout` fails to time out deferreds
   290|     that have a canceller that throws exceptions. This method creates a new
   291|     deferred that wraps and times out the given deferred, correctly handling
   292|     the case where the given deferred's canceller throws.
   293|     (See https://twistedmatrix.com/trac/ticket/9534)
   294|     NOTE: Unlike `Deferred.addTimeout`, this function returns a new deferred.
   295|     NOTE: the TimeoutError raised by the resultant deferred is
   296|     twisted.internet.defer.TimeoutError, which is *different* to the built-in
   297|     TimeoutError, as well as various other TimeoutErrors you might have imported.
   298|     Args:
   299|         deferred: The Deferred to potentially timeout.
   300|         timeout: Timeout in seconds
   301|         reactor: The twisted reactor to use
   302|     Returns:
   303|         A new Deferred, which will errback with defer.TimeoutError on timeout.
   304|     """
   305|     new_d = defer.Deferred()
   306|     timed_out = [False]
   307|     def time_it_out():
   308|         timed_out[0] = True
   309|         try:
   310|             deferred.cancel()
   311|         except:  # noqa: E722, if we throw any exception it'll break time outs
   312|             logger.exception("Canceller failed during timeout")
   313|         if not new_d.called:
   314|             new_d.errback(defer.TimeoutError("Timed out after %gs" % (timeout,)))
   315|     delayed_call = reactor.callLater(timeout, time_it_out)
   316|     def convert_cancelled(value: failure.Failure):
   317|         if timed_out[0] and value.check(CancelledError):
   318|             raise defer.TimeoutError("Timed out after %gs" % (timeout,))
   319|         return value
   320|     deferred.addErrback(convert_cancelled)
   321|     def cancel_timeout(result):
   322|         if delayed_call.active():
   323|             delayed_call.cancel()
   324|         return result
   325|     deferred.addBoth(cancel_timeout)
   326|     def success_cb(val):
   327|         if not new_d.called:
   328|             new_d.callback(val)
   329|     def failure_cb(val):
   330|         if not new_d.called:
   331|             new_d.errback(val)
   332|     deferred.addCallbacks(success_cb, failure_cb)
   333|     return new_d
   334| @attr.s(slots=True, frozen=True)
   335| class DoneAwaitable:
   336|     """Simple awaitable that returns the provided value.
   337|     """
   338|     value = attr.ib()
   339|     def __await__(self):
   340|         return self


# ====================================================================
# FILE: synapse/util/caches/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-41 ---
     1| import logging
     2| from sys import intern
     3| from typing import Callable, Dict, Optional
     4| import attr
     5| from prometheus_client.core import Gauge
     6| from synapse.config.cache import add_resizable_cache
     7| logger = logging.getLogger(__name__)
     8| caches_by_name = {}
     9| collectors_by_name = {}  # type: Dict
    10| cache_size = Gauge("synapse_util_caches_cache:size", "", ["name"])
    11| cache_hits = Gauge("synapse_util_caches_cache:hits", "", ["name"])
    12| cache_evicted = Gauge("synapse_util_caches_cache:evicted_size", "", ["name"])
    13| cache_total = Gauge("synapse_util_caches_cache:total", "", ["name"])
    14| cache_max_size = Gauge("synapse_util_caches_cache_max_size", "", ["name"])
    15| response_cache_size = Gauge("synapse_util_caches_response_cache:size", "", ["name"])
    16| response_cache_hits = Gauge("synapse_util_caches_response_cache:hits", "", ["name"])
    17| response_cache_evicted = Gauge(
    18|     "synapse_util_caches_response_cache:evicted_size", "", ["name"]
    19| )
    20| response_cache_total = Gauge("synapse_util_caches_response_cache:total", "", ["name"])
    21| @attr.s(slots=True)
    22| class CacheMetric:
    23|     _cache = attr.ib()
    24|     _cache_type = attr.ib(type=str)
    25|     _cache_name = attr.ib(type=str)
    26|     _collect_callback = attr.ib(type=Optional[Callable])
    27|     hits = attr.ib(default=0)
    28|     misses = attr.ib(default=0)
    29|     evicted_size = attr.ib(default=0)
    30|     def inc_hits(self):
    31|         self.hits += 1
    32|     def inc_misses(self):
    33|         self.misses += 1
    34|     def inc_evictions(self, size=1):
    35|         self.evicted_size += size
    36|     def describe(self):
    37|         return []
    38|     def collect(self):
    39|         try:
    40|             if self._cache_type == "response_cache":
    41|                 response_cache_size.labels(self._cache_name).set(len(self._cache))


# ====================================================================
# FILE: synapse/util/distributor.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-79 ---
     1| import inspect
     2| import logging
     3| from twisted.internet import defer
     4| from synapse.logging.context import make_deferred_yieldable, run_in_background
     5| from synapse.metrics.background_process_metrics import run_as_background_process
     6| logger = logging.getLogger(__name__)
     7| def user_left_room(distributor, user, room_id):
     8|     distributor.fire("user_left_room", user=user, room_id=room_id)
     9| class Distributor:
    10|     """A central dispatch point for loosely-connected pieces of code to
    11|     register, observe, and fire signals.
    12|     Signals are named simply by strings.
    13|     TODO(paul): It would be nice to give signals stronger object identities,
    14|       so we can attach metadata, docstrings, detect typos, etc... But this
    15|       model will do for today.
    16|     """
    17|     def __init__(self):
    18|         self.signals = {}
    19|         self.pre_registration = {}
    20|     def declare(self, name):
    21|         if name in self.signals:
    22|             raise KeyError("%r already has a signal named %s" % (self, name))
    23|         self.signals[name] = Signal(name)
    24|         if name in self.pre_registration:
    25|             signal = self.signals[name]
    26|             for observer in self.pre_registration[name]:
    27|                 signal.observe(observer)
    28|     def observe(self, name, observer):
    29|         if name in self.signals:
    30|             self.signals[name].observe(observer)
    31|         else:
    32|             if name not in self.pre_registration:
    33|                 self.pre_registration[name] = []
    34|             self.pre_registration[name].append(observer)
    35|     def fire(self, name, *args, **kwargs):
    36|         """Dispatches the given signal to the registered observers.
    37|         Runs the observers as a background process. Does not return a deferred.
    38|         """
    39|         if name not in self.signals:
    40|             raise KeyError("%r does not have a signal named %s" % (self, name))
    41|         run_as_background_process(name, self.signals[name].fire, *args, **kwargs)
    42| class Signal:
    43|     """A Signal is a dispatch point that stores a list of callables as
    44|     observers of it.
    45|     Signals can be "fired", meaning that every callable observing it is
    46|     invoked. Firing a signal does not change its state; it can be fired again
    47|     at any later point. Firing a signal passes any arguments from the fire
    48|     method into all of the observers.
    49|     """
    50|     def __init__(self, name):
    51|         self.name = name
    52|         self.observers = []
    53|     def observe(self, observer):
    54|         """Adds a new callable to the observer list which will be invoked by
    55|         the 'fire' method.
    56|         Each observer callable may return a Deferred."""
    57|         self.observers.append(observer)
    58|     def fire(self, *args, **kwargs):
    59|         """Invokes every callable in the observer list, passing in the args and
    60|         kwargs. Exceptions thrown by observers are logged but ignored. It is
    61|         not an error to fire a signal with no observers.
    62|         Returns a Deferred that will complete when all the observers have
    63|         completed."""
    64|         async def do(observer):
    65|             try:
    66|                 result = observer(*args, **kwargs)
    67|                 if inspect.isawaitable(result):
    68|                     result = await result
    69|                 return result
    70|             except Exception as e:
    71|                 logger.warning(
    72|                     "%s signal observer %s failed: %r", self.name, observer, e,
    73|                 )
    74|         deferreds = [run_in_background(do, o) for o in self.observers]
    75|         return make_deferred_yieldable(
    76|             defer.gatherResults(deferreds, consumeErrors=True)
    77|         )
    78|     def __repr__(self):
    79|         return "<Signal name=%r>" % (self.name,)


# ====================================================================
# FILE: synapse/util/frozenutils.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-36 ---
     1| import json
     2| from frozendict import frozendict
     3| def freeze(o):
     4|     if isinstance(o, dict):
     5|         return frozendict({k: freeze(v) for k, v in o.items()})
     6|     if isinstance(o, frozendict):
     7|         return o
     8|     if isinstance(o, (bytes, str)):
     9|         return o
    10|     try:
    11|         return tuple(freeze(i) for i in o)
    12|     except TypeError:
    13|         pass
    14|     return o
    15| def unfreeze(o):
    16|     if isinstance(o, (dict, frozendict)):
    17|         return dict({k: unfreeze(v) for k, v in o.items()})
    18|     if isinstance(o, (bytes, str)):
    19|         return o
    20|     try:
    21|         return [unfreeze(i) for i in o]
    22|     except TypeError:
    23|         pass
    24|     return o
    25| def _handle_frozendict(obj):
    26|     """Helper for EventEncoder. Makes frozendicts serializable by returning
    27|     the underlying dict
    28|     """
    29|     if type(obj) is frozendict:
    30|         return obj._dict
    31|     raise TypeError(
    32|         "Object of type %s is not JSON serializable" % obj.__class__.__name__
    33|     )
    34| frozendict_json_encoder = json.JSONEncoder(
    35|     allow_nan=False, separators=(",", ":"), default=_handle_frozendict,
    36| )


# ====================================================================
# FILE: synapse/util/manhole.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 49-89 ---
    49|         username(str): The username ssh clients should auth with.
    50|         password(str): The password ssh clients should auth with.
    51|         globals(dict): The variables to expose in the shell.
    52|     Returns:
    53|         twisted.internet.protocol.Factory: A factory to pass to ``listenTCP``
    54|     """
    55|     if not isinstance(password, bytes):
    56|         password = password.encode("ascii")
    57|     checker = checkers.InMemoryUsernamePasswordDatabaseDontUse(**{username: password})
    58|     rlm = manhole_ssh.TerminalRealm()
    59|     rlm.chainedProtocolFactory = lambda: insults.ServerProtocol(
    60|         SynapseManhole, dict(globals, __name__="__console__")
    61|     )
    62|     factory = manhole_ssh.ConchFactory(portal.Portal(rlm, [checker]))
    63|     factory.publicKeys[b"ssh-rsa"] = Key.fromString(PUBLIC_KEY)
    64|     factory.privateKeys[b"ssh-rsa"] = Key.fromString(PRIVATE_KEY)
    65|     return factory
    66| class SynapseManhole(ColoredManhole):
    67|     """Overrides connectionMade to create our own ManholeInterpreter"""
    68|     def connectionMade(self):
    69|         super().connectionMade()
    70|         self.interpreter = SynapseManholeInterpreter(self, self.namespace)
    71| class SynapseManholeInterpreter(ManholeInterpreter):
    72|     def showsyntaxerror(self, filename=None):
    73|         """Display the syntax error that just occurred.
    74|         Overrides the base implementation, ignoring sys.excepthook. We always want
    75|         any syntax errors to be sent to the terminal, rather than sentry.
    76|         """
    77|         type, value, tb = sys.exc_info()
    78|         sys.last_type = type
    79|         sys.last_value = value
    80|         sys.last_traceback = tb
    81|         if filename and type is SyntaxError:
    82|             try:
    83|                 msg, (dummy_filename, lineno, offset, line) = value.args
    84|             except ValueError:
    85|                 pass
    86|             else:
    87|                 value = SyntaxError(msg, (filename, lineno, offset, line))
    88|                 sys.last_value = value
    89|         lines = traceback.format_exception_only(type, value)


# ====================================================================
# FILE: synapse/util/metrics.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-29 ---
     1| import logging
     2| from functools import wraps
     3| from typing import Any, Callable, Optional, TypeVar, cast
     4| from prometheus_client import Counter
     5| from synapse.logging.context import (
     6|     ContextResourceUsage,
     7|     LoggingContext,
     8|     current_context,
     9| )
    10| from synapse.metrics import InFlightGauge
    11| logger = logging.getLogger(__name__)
    12| block_counter = Counter("synapse_util_metrics_block_count", "", ["block_name"])
    13| block_timer = Counter("synapse_util_metrics_block_time_seconds", "", ["block_name"])
    14| block_ru_utime = Counter(
    15|     "synapse_util_metrics_block_ru_utime_seconds", "", ["block_name"]
    16| )
    17| block_ru_stime = Counter(
    18|     "synapse_util_metrics_block_ru_stime_seconds", "", ["block_name"]
    19| )
    20| block_db_txn_count = Counter(
    21|     "synapse_util_metrics_block_db_txn_count", "", ["block_name"]
    22| )
    23| block_db_txn_duration = Counter(
    24|     "synapse_util_metrics_block_db_txn_duration_seconds", "", ["block_name"]
    25| )
    26| block_db_sched_duration = Counter(
    27|     "synapse_util_metrics_block_db_sched_duration_seconds", "", ["block_name"]
    28| )
    29| in_flight = InFlightGauge(

# --- HUNK 2: Lines 47-106 ---
    47|     """
    48|     def wrapper(func: T) -> T:
    49|         block_name = func.__name__ if name is None else name
    50|         @wraps(func)
    51|         async def measured_func(self, *args, **kwargs):
    52|             with Measure(self.clock, block_name):
    53|                 r = await func(self, *args, **kwargs)
    54|             return r
    55|         return cast(T, measured_func)
    56|     return wrapper
    57| class Measure:
    58|     __slots__ = [
    59|         "clock",
    60|         "name",
    61|         "_logging_context",
    62|         "start",
    63|     ]
    64|     def __init__(self, clock, name):
    65|         self.clock = clock
    66|         self.name = name
    67|         parent_context = current_context()
    68|         self._logging_context = LoggingContext(
    69|             "Measure[%s]" % (self.name,), parent_context
    70|         )
    71|         self.start = None
    72|     def __enter__(self) -> "Measure":
    73|         if self.start is not None:
    74|             raise RuntimeError("Measure() objects cannot be re-used")
    75|         self.start = self.clock.time()
    76|         self._logging_context.__enter__()
    77|         in_flight.register((self.name,), self._update_in_flight)
    78|         return self
    79|     def __exit__(self, exc_type, exc_val, exc_tb):
    80|         if self.start is None:
    81|             raise RuntimeError("Measure() block exited without being entered")
    82|         duration = self.clock.time() - self.start
    83|         usage = self.get_resource_usage()
    84|         in_flight.unregister((self.name,), self._update_in_flight)
    85|         self._logging_context.__exit__(exc_type, exc_val, exc_tb)
    86|         try:
    87|             block_counter.labels(self.name).inc()
    88|             block_timer.labels(self.name).inc(duration)
    89|             block_ru_utime.labels(self.name).inc(usage.ru_utime)
    90|             block_ru_stime.labels(self.name).inc(usage.ru_stime)
    91|             block_db_txn_count.labels(self.name).inc(usage.db_txn_count)
    92|             block_db_txn_duration.labels(self.name).inc(usage.db_txn_duration_sec)
    93|             block_db_sched_duration.labels(self.name).inc(usage.db_sched_duration_sec)
    94|         except ValueError:
    95|             logger.warning("Failed to save metrics! Usage: %s", usage)
    96|     def get_resource_usage(self) -> ContextResourceUsage:
    97|         """Get the resources used within this Measure block
    98|         If the Measure block is still active, returns the resource usage so far.
    99|         """
   100|         return self._logging_context.get_resource_usage()
   101|     def _update_in_flight(self, metrics):
   102|         """Gets called when processing in flight metrics
   103|         """
   104|         duration = self.clock.time() - self.start
   105|         metrics.real_time_max = max(metrics.real_time_max, duration)
   106|         metrics.real_time_sum += duration


# ====================================================================
# FILE: synapse/util/patch_inline_callbacks.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-20 ---
     1| import functools
     2| import sys
     3| from typing import Any, Callable, List
     4| from twisted.internet import defer
     5| from twisted.internet.defer import Deferred
     6| from twisted.python.failure import Failure
     7| _already_patched = False
     8| def do_patch():
     9|     """
    10|     Patch defer.inlineCallbacks so that it checks the state of the logcontext on exit
    11|     """
    12|     from synapse.logging.context import current_context
    13|     global _already_patched
    14|     orig_inline_callbacks = defer.inlineCallbacks
    15|     if _already_patched:
    16|         return
    17|     def new_inline_callbacks(f):
    18|         @functools.wraps(f)
    19|         def wrapped(*args, **kwargs):
    20|             start_context = current_context()


# ====================================================================
# FILE: synapse/util/retryutils.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 2-42 ---
     2| import random
     3| import synapse.logging.context
     4| from synapse.api.errors import CodeMessageException
     5| logger = logging.getLogger(__name__)
     6| MIN_RETRY_INTERVAL = 10 * 60 * 1000
     7| RETRY_MULTIPLIER = 5
     8| MAX_RETRY_INTERVAL = 2 ** 62
     9| class NotRetryingDestination(Exception):
    10|     def __init__(self, retry_last_ts, retry_interval, destination):
    11|         """Raised by the limiter (and federation client) to indicate that we are
    12|         are deliberately not attempting to contact a given server.
    13|         Args:
    14|             retry_last_ts (int): the unix ts in milliseconds of our last attempt
    15|                 to contact the server.  0 indicates that the last attempt was
    16|                 successful or that we've never actually attempted to connect.
    17|             retry_interval (int): the time in milliseconds to wait until the next
    18|                 attempt.
    19|             destination (str): the domain in question
    20|         """
    21|         msg = "Not retrying server %s." % (destination,)
    22|         super().__init__(msg)
    23|         self.retry_last_ts = retry_last_ts
    24|         self.retry_interval = retry_interval
    25|         self.destination = destination
    26| async def get_retry_limiter(destination, clock, store, ignore_backoff=False, **kwargs):
    27|     """For a given destination check if we have previously failed to
    28|     send a request there and are waiting before retrying the destination.
    29|     If we are not ready to retry the destination, this will raise a
    30|     NotRetryingDestination exception. Otherwise, will return a Context Manager
    31|     that will mark the destination as down if an exception is thrown (excluding
    32|     CodeMessageException with code < 500)
    33|     Args:
    34|         destination (str): name of homeserver
    35|         clock (synapse.util.clock): timing source
    36|         store (synapse.storage.transactions.TransactionStore): datastore
    37|         ignore_backoff (bool): true to ignore the historical backoff data and
    38|             try the request anyway. We will still reset the retry_interval on success.
    39|     Example usage:
    40|         try:
    41|             limiter = await get_retry_limiter(destination, clock, store)
    42|             with limiter:

