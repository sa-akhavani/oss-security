--- a//dev/null
+++ b/alerts/migrations/0002_alter_messagingserviceconfig_project.py
@@ -0,0 +1,18 @@
+from django.db import migrations, models
+import django.db.models.deletion
+class Migration(migrations.Migration):
+    dependencies = [
+        ("projects", "0014_alter_projectmembership_project"),
+        ("alerts", "0001_initial"),
+    ]
+    operations = [
+        migrations.AlterField(
+            model_name="messagingserviceconfig",
+            name="project",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING,
+                related_name="service_configs",
+                to="projects.project",
+            ),
+        ),
+    ]

--- a//dev/null
+++ b/alerts/models.py
@@ -0,0 +1,11 @@
+from django.db import models
+from projects.models import Project
+from .service_backends.slack import SlackBackend
+class MessagingServiceConfig(models.Model):
+    project = models.ForeignKey(Project, on_delete=models.DO_NOTHING, related_name="service_configs")
+    display_name = models.CharField(max_length=100, blank=False,
+                                    help_text='For display in the UI, e.g. "#general on company Slack"')
+    kind = models.CharField(choices=[("slack", "Slack (or compatible)"), ], max_length=20, default="slack")
+    config = models.TextField(blank=False)
+    def get_backend(self):
+        return SlackBackend(self)

--- a//dev/null
+++ b/bugsink/context_processors.py
@@ -0,0 +1,73 @@
+from datetime import timedelta
+from collections import namedtuple
+from django.conf import settings
+from django.utils import timezone
+from django.utils.safestring import mark_safe
+from django.urls import reverse
+from django.contrib.auth.models import AnonymousUser
+from django.db.utils import OperationalError
+from django.db.models import Sum
+from bugsink.app_settings import get_settings, CB_ANYBODY
+from bugsink.transaction import durable_atomic
+from bugsink.timed_sqlite_backend.base import different_runtime_limit
+from snappea.settings import get_settings as get_snappea_settings
+from snappea.models import Task, Stat
+from phonehome.models import Installation
+SystemWarning = namedtuple('SystemWarning', ['message', 'ignore_url'])
+EMAIL_BACKEND_WARNING = mark_safe(
+    """Email is not set up, emails won't be sent. To get the most out of Bugsink, please
+    <a href="https://www.bugsink.com/docs/settings/#email" target="_blank" class="font-bold text-slate-800
+    dark:text-slate-100">set up email</a>.""")
+def get_snappea_warnings():
+    if get_snappea_settings().TASK_ALWAYS_EAGER:
+        return []
+    with durable_atomic(using="snappea"):
+        with different_runtime_limit(0.1, using="snappea"):
+            try:
+                task_count = Task.objects.all().count()
+            except OperationalError as e:
+                if e.args[0] != "interrupted":
+                    raise
+                task_count = "many"
+        if task_count == 0:
+            return []
+        ten_minutes_ago = timezone.now() - timedelta(minutes=10)
+        done = Stat.objects.filter(timestamp__gt=ten_minutes_ago).aggregate(done=Sum('done')).get('done', 0)
+        oldest_task_age = int(
+            (timezone.now() - Task.objects.all().order_by('created_at').first().created_at).total_seconds())
+    snappea_warning = f"Snappea has {task_count} tasks in the queue, the oldest being {oldest_task_age}s old. "
+    if done:
+        snappea_warning += f"It's processed approximately { done } tasks in the last 10 minutes. "
+    elif oldest_task_age > 70:
+        snappea_warning += "No tasks processed recently. Snappea may not be running, misconfigured or blocked."
+    else:
+        snappea_warning += "Snappea may be either backlogged, not running, misconfigured or blocked."
+    WARNING = SystemWarning((snappea_warning), None)
+    if task_count > 300:
+        return [WARNING]
+    if oldest_task_age > 5:
+        return [WARNING]
+    return []
+def useful_settings_processor(request):
+    """Adds useful settings (and more) to the context."""
+    def get_system_warnings():
+        installation = Installation.objects.get()
+        system_warnings = []
+        if settings.EMAIL_BACKEND in [
+                'bugsink.email_backends.QuietConsoleEmailBackend'] and not installation.silence_email_system_warning:
+            if getattr(request, "user", AnonymousUser()).is_superuser:
+                ignore_url = reverse("silence_email_system_warning")
+            else:
+                ignore_url = None
+            system_warnings.append(SystemWarning(EMAIL_BACKEND_WARNING, ignore_url))
+        return system_warnings + get_snappea_warnings()
+    return {
+        'site_title': get_settings().SITE_TITLE,
+        'registration_enabled': get_settings().USER_REGISTRATION == CB_ANYBODY,
+        'app_settings': get_settings(),
+        'system_warnings': get_system_warnings,
+    }
+def logged_in_user_processor(request):
+    return {
+        'logged_in_user': getattr(request, "user", AnonymousUser()),
+    }

--- a//dev/null
+++ b/bugsink/decorators.py
@@ -0,0 +1,79 @@
+from functools import wraps
+from django.shortcuts import get_object_or_404
+from django.core.exceptions import PermissionDenied
+from projects.models import Project
+from issues.models import Issue
+from events.models import Event
+from .transaction import durable_atomic, immediate_atomic
+def login_exempt(view):
+    view.login_exempt = True
+    return view
+def project_membership_required(function):
+    @wraps(function)
+    def wrapper(request, *args, **kwargs):
+        if "project_pk" not in kwargs:
+            raise TypeError("project_pk must be passed as a keyword argument")
+        project_pk = kwargs.pop("project_pk")
+        project = get_object_or_404(Project, pk=project_pk)
+        kwargs["project"] = project
+        if request.user.is_superuser:
+            return function(request, *args, **kwargs)
+        if project.users.filter(pk=request.user.pk).exists():
+            return function(request, *args, **kwargs)
+        raise PermissionDenied("You don't have permission to access this project")
+    return wrapper
+def issue_membership_required(function):
+    @wraps(function)
+    def wrapper(request, *args, **kwargs):
+        if "issue_pk" not in kwargs:
+            raise TypeError("issue_pk must be passed as a keyword argument")
+        issue_pk = kwargs.pop("issue_pk")
+        issue = get_object_or_404(Issue, pk=issue_pk, is_deleted=False)
+        kwargs["issue"] = issue
+        if request.user.is_superuser:
+            return function(request, *args, **kwargs)
+        if issue.project.users.filter(pk=request.user.pk).exists():
+            return function(request, *args, **kwargs)
+        raise PermissionDenied("You don't have permission to access this project")
+    return wrapper
+def event_membership_required(function):
+    @wraps(function)
+    def wrapper(request, *args, **kwargs):
+        if "event_pk" not in kwargs:
+            raise TypeError("event_pk must be passed as a keyword argument")
+        event_pk = kwargs.pop("event_pk")
+        event = get_object_or_404(Event, pk=event_pk)
+        kwargs["event"] = event
+        if request.user.is_superuser:
+            return function(request, *args, **kwargs)
+        if event.project.users.filter(pk=request.user.pk).exists():
+            return function(request, *args, **kwargs)
+        raise PermissionDenied("You don't have permission to access this project")
+    return wrapper
+def atomic_for_request_method(function, *decorator_args, **decorator_kwargs):
+    """
+    Wrap the request in the kind of atomic transaction matching its request method:
+    This is what immediate_atomic is for.
+    This might be surprising if you think about transactions as mainly a means of guaranteeing atomicity of writes (as
+    is directly implied by Django's naming). The thing we're going for is snapshot isolation (given by SQLite in WAL
+    mode (which we have turned on) in combination with use of transactions).
+    I want to have snapshot isolation because it's a mental model that I can understand. I'd argue it's the natural or
+    implicit mental model, and I'd rather have my program behave like so than spend _any_ time thinking about subtleties
+    such as "what if you select an event and an issue that are slightly out of sync" or to hunt down any hard to
+    reproduce bugs caused by such inconsistencies.
+    This is provided as a decorator; the expected use case is to wrap an entire view function. The reason is that, in
+    practice, reads which should be inside the transaction happen very early, namely when doing the various
+    membership_required checks. (the results of these reads are passed into the view function as event/issue/project)
+    (Path not taken: one could say that the membership_required tests are separate from the actual handling of the
+    request, whether that's a pure display request or an update. Instead of wrapping the transaction around everything,
+    you could re-select inside the view function. Potential advantage: shorter transactions (mostly relevant for writes,
+    since read-transactions are non-blocking). Disadvantage: one more query, and more complexity)
+    """
+    @wraps(function)
+    def wrapper(request, *args, **kwargs):
+        if request.method in ["POST", "PUT", "PATCH", "DELETE"]:
+            with immediate_atomic(*decorator_args, **decorator_kwargs):
+                return function(request, *args, **kwargs)
+        with durable_atomic(*decorator_args, **decorator_kwargs):
+            return function(request, *args, **kwargs)
+    return wrapper

--- a/bugsink/settings/development.py
+++ b//dev/null
@@ -1,85 +0,0 @@
-from .default import *  # noqa
-from .default import BASE_DIR, LOGGING, DATABASES, I_AM_RUNNING
-import os
-from sentry_sdk_extensions.transport import MoreLoudlyFailingTransport
-from bugsink.utils import deduce_allowed_hosts, eat_your_own_dogfood
-SECRET_KEY = 'django-insecure-$@clhhieazwnxnha-_zah&(bieq%yux7#^07&xsvhn58t)8@xw'
-DEBUG = True
-if os.getenv("DB", "sqlite") == "mysql":
-    DATABASES['default'] = {
-        'ENGINE': 'django.db.backends.mysql',
-        'NAME': 'bugsink',
-        'USER': os.environ["DB_USER"],
-        'PASSWORD': os.environ["DB_PASSWORD"],
-    }
-elif os.getenv("DB", "sqlite") == "postgres":
-    DATABASES['default'] = {
-        'ENGINE': 'django.db.backends.postgresql',
-        'NAME': 'bugsink',
-        'USER': os.environ["DB_USER"],
-        'PASSWORD': os.environ["DB_PASSWORD"],
-        'HOST': 'localhost',
-    }
-elif os.getenv("DB", "sqlite") == "sqlite":
-    DATABASES["default"]["NAME"] = BASE_DIR / 'db.sqlite3'
-    DATABASES["default"]["TEST"]["NAME"] = BASE_DIR / 'test.sqlite3'
-    DATABASES["default"]["OPTIONS"]["query_timeout"] = 0.11  # canary config: fail-fast in development.
-    DATABASES["snappea"]["NAME"] = BASE_DIR / 'snappea.sqlite3'
-    DATABASES["snappea"]["OPTIONS"]["query_timeout"] = 0.12
-else:
-    raise ValueError("Unknown DB", os.getenv("DB"))
-if not I_AM_RUNNING == "TEST":
-    SENTRY_DSN = os.getenv("SENTRY_DSN")
-    eat_your_own_dogfood(
-        SENTRY_DSN,
-        transport=MoreLoudlyFailingTransport,
-    )
-SNAPPEA = {
-    "TASK_ALWAYS_EAGER": True,  # at least for (unit) tests, this is a requirement
-    "NUM_WORKERS": 1,
-}
-EMAIL_HOST = os.getenv("EMAIL_HOST")
-EMAIL_HOST_USER = os.getenv("EMAIL_HOST_USER")
-EMAIL_HOST_PASSWORD = os.getenv("EMAIL_HOST_PASSWORD")
-EMAIL_PORT = 587
-EMAIL_USE_TLS = True
-SERVER_EMAIL = DEFAULT_FROM_EMAIL = 'Klaas van Schelven <klaas@bugsink.com>'
-BUGSINK = {
-    "DIGEST_IMMEDIATELY": False,
-    "BASE_URL": "http://bugsink:8000",  # no trailing slash
-    "SITE_TITLE": "Bugsink",  # you can customize this as e.g. "My Bugsink" or "Bugsink for My Company"
-    "USE_ADMIN": True,
-    "VALIDATE_ON_DIGEST": "warn",
-    "API_LOG_UNIMPLEMENTED_CALLS": True,
-    "MAX_EVENTS_PER_PROJECT_PER_5_MINUTES": 1_000_000,
-    "MAX_EVENTS_PER_PROJECT_PER_HOUR": 50_000_000,
-}
-if not I_AM_RUNNING == "TEST":
-    BUGSINK["EVENT_STORAGES"] = {
-        "local_flat_files": {
-            "STORAGE": "events.storage.FileEventStorage",
-            "OPTIONS": {
-                "basepath": os.path.join(BASE_DIR, "filestorage"),
-            },
-            "USE_FOR_WRITE": True,
-        },
-    }
-LOGGING["formatters"]["look_below"] = {
-    "format": "    {message} â†´",
-    "style": "{",
-}
-LOGGING["handlers"]["look_below_in_stream"] = {
-    "level": "INFO",
-    "class": "logging.StreamHandler",
-    "formatter": "look_below",
-    "filters": ['require_debug_true'],
-}
-if I_AM_RUNNING == "SNAPPEA":
-    LOGGING['loggers']['bugsink.performance']["handlers"] = ["snappea"]
-else:
-    LOGGING['loggers']['bugsink.performance']["handlers"] = ["look_below_in_stream"]
-LOGGING["handlers"]["snappea"]["level"] = "DEBUG"
-LOGGING["loggers"]["snappea"]["level"] = "DEBUG"
-LOGGING["formatters"]["snappea"]["format"] = "{asctime} - {threadName} - {levelname:7} - {message}"
-ALLOWED_HOSTS = deduce_allowed_hosts(BUGSINK["BASE_URL"])
-NPM_BIN_PATH = os.getenv("NPM_BIN_PATH", "npm")

--- a//dev/null
+++ b/bugsink/transaction.py
@@ -0,0 +1,128 @@
+from contextlib import contextmanager
+import logging
+import time
+from functools import partial
+import types
+import threading
+from django.db import transaction as django_db_transaction
+from django.db import DEFAULT_DB_ALIAS
+from snappea.settings import get_settings as get_snappea_settings
+performance_logger = logging.getLogger("bugsink.performance.db")
+local_storage = threading.local()
+immediate_semaphores = {
+    DEFAULT_DB_ALIAS: threading.Semaphore(1),
+    "snappea": threading.Semaphore(1),
+}
+class SemaphoreContext:
+    def __init__(self, using):
+        self.using = using
+    def __enter__(self):
+        t0 = time.time()
+        if not immediate_semaphores[self.using].acquire(timeout=10):
+            raise RuntimeError("Could not acquire immediate_semaphore")
+        took = time.time() - t0
+        inc_stat(self.using, "get_write_lock", took)
+        using_clause = f" ({ self.using })" if self.using != DEFAULT_DB_ALIAS else ""
+        performance_logger.info(f"{took * 1000:6.2f}ms BEGIN IMMEDIATE, A.K.A. get-write-lock{using_clause}")
+    def __exit__(self, exc_type, exc_value, traceback):
+        immediate_semaphores[self.using].release()
+class SuperDurableAtomic(django_db_transaction.Atomic):
+    """'super' durable because it is durable in tests as well"""
+    def __enter__(self):
+        connection = django_db_transaction.get_connection(self.using)
+        if (self.durable and connection.atomic_blocks):
+            if not connection.atomic_blocks[-1]._from_testcase:
+                raise RuntimeError("A durable atomic block cannot be nested within another atomic block.")
+            raise RuntimeError("A durable atomic block cannot be nested -- not even in tests.")
+        super(SuperDurableAtomic, self).__enter__()
+    def __exit__(self, exc_type, exc_value, traceback):
+        super(SuperDurableAtomic, self).__exit__(exc_type, exc_value, traceback)
+def durable_atomic(using=None, savepoint=True):
+    if callable(using):
+        return SuperDurableAtomic(DEFAULT_DB_ALIAS, savepoint, durable=True)(using)
+    return SuperDurableAtomic(using, savepoint, durable=True)
+def _start_transaction_under_autocommit_patched(self):
+    self.cursor().execute("BEGIN IMMEDIATE")
+class ImmediateAtomic(SuperDurableAtomic):
+    """
+    Sqlite specific (for other DBs this is simply ignored).
+    immediate_atomic allows us to begin atomic transactions using BEGIN IMMEDIATE instead of the default BEGIN. The
+    sqlite docs explain a context in which this is useful:
+    > Another example: X starts a read transaction using BEGIN and SELECT, then Y makes a changes to the database using
+    > UPDATE. Then X tries to make a change to the database using UPDATE. The attempt by X to escalate its transaction
+    > from a read transaction to a write transaction fails with an SQLITE_BUSY_SNAPSHOT error because the snapshot of
+    > the database being viewed by X is no longer the latest version of the database. If X were allowed to write, it
+    > would fork the history of the database file, which is something SQLite does not support. [..]
+    > **If X starts a transaction that will initially only read but X knows it will eventually want to write and does
+    > not want to be troubled with possible SQLITE_BUSY_SNAPSHOT errors that arise because another connection jumped
+    > ahead of it in line, then X can issue BEGIN IMMEDIATE to start its transaction instead of just an ordinary
+    > BEGIN.** The BEGIN IMMEDIATE command goes ahead and starts a write transaction, and thus blocks all other writers.
+    > If the BEGIN IMMEDIATE operation succeeds, then no subsequent operations in that transaction will ever fail with
+    > an SQLITE_BUSY error.
+    Django 5.1 will introduce the option to configure BEGIN IMMEDIATE, but only globally. This is not what we want,
+    because it unnecessarily escalates read-only-transactions into write-transactions.
+    https://github.com/django/django/pull/17760
+    PoC of the problem (which indeed goes away by converting to immediate); run in 2 separate shells to demonstrate it:
+    from time import sleep
+    from django.contrib.auth.models import User
+    from django.db import transaction
+    @transaction.atomic()
+    def read_sleep_write():
+        print("I am going to read")
+        user = User.objects.first()
+        print("I am going to sleep")
+        sleep(5)
+        print("I am going to write")
+        user.save() # no need to actually change the user, as long as we run a write query
+    One more note that I have trouble integrating into the story-line. Django says
+    > Atomicity is the defining property of database transactions.
+    But this is not true. But we also care about isolation (views on the data) and about serializability (the order of
+    transactions). In particular serializability (locking out other writers) is what we are after here.
+    """
+    def __enter__(self):
+        connection = django_db_transaction.get_connection(self.using)
+        if hasattr(connection, "_start_transaction_under_autocommit"):
+            self._start_transaction_under_autocommit_original = connection._start_transaction_under_autocommit
+            connection._start_transaction_under_autocommit = types.MethodType(
+                _start_transaction_under_autocommit_patched, connection)
+        super(ImmediateAtomic, self).__enter__()
+        if connection.vendor != 'sqlite':
+            from django.contrib.contenttypes.models import ContentType
+            ContentType.objects.select_for_update().order_by("pk").first()
+        self.t0 = time.time()
+    def __exit__(self, exc_type, exc_value, traceback):
+        super(ImmediateAtomic, self).__exit__(exc_type, exc_value, traceback)
+        took = time.time() - self.t0
+        inc_stat(self.using, "immediate_transaction", took)
+        using_clause = f" ({ self.using })" if self.using != DEFAULT_DB_ALIAS else ""
+        performance_logger.info(f"{took * 1000:6.2f}ms IMMEDIATE transaction{using_clause}")
+        connection = django_db_transaction.get_connection(self.using)
+        if hasattr(self, "_start_transaction_under_autocommit_original"):
+            connection._start_transaction_under_autocommit = self._start_transaction_under_autocommit_original
+            del self._start_transaction_under_autocommit_original
+@contextmanager
+def immediate_atomic(using=None, savepoint=True, durable=True):
+    assert durable, "immediate_atomic should always be used with durable=True"
+    using = DEFAULT_DB_ALIAS if using is None else using  # harmonize to "default" at the top for downstream lookups
+    if callable(using):
+        immediate_atomic = ImmediateAtomic(DEFAULT_DB_ALIAS, savepoint, durable)(using)
+    else:
+        immediate_atomic = ImmediateAtomic(using, savepoint, durable)
+    if get_snappea_settings().TASK_ALWAYS_EAGER:
+        with immediate_atomic:
+            yield
+    else:
+        with SemaphoreContext(using), immediate_atomic:
+            yield
+def delay_on_commit(function, *args, **kwargs):
+    django_db_transaction.on_commit(partial(function.delay, *args, **kwargs))
+def inc_stat(using, stat, took):
+    if using != "default":
+        return  # function signature ready for such stats; not actually collected though
+    if not hasattr(local_storage, "stats"):
+        local_storage.stats = {}
+    if stat not in local_storage.stats:
+        local_storage.stats[stat] = 0
+    local_storage.stats[stat] += took
+def get_stat(stat):
+    return getattr(local_storage, "stats", {}).get(stat, 0)

--- a/bugsink/urls.py
+++ b//dev/null
@@ -1,63 +0,0 @@
-from django.conf import settings
-from django.contrib import admin
-from django.urls import include, path
-from django.contrib.auth import views as auth_views
-from django.views.generic import RedirectView, TemplateView
-from alerts.views import debug_email as debug_alerts_email
-from users.views import debug_email as debug_users_email
-from teams.views import debug_email as debug_teams_email
-from bugsink.app_settings import get_settings
-from users.views import signup, confirm_email, resend_confirmation, request_reset_password, reset_password, preferences
-from ingest.views import download_envelope
-from files.views import chunk_upload, artifact_bundle_assemble, api_catch_all
-from bugsink.decorators import login_exempt
-from .views import home, trigger_error, favicon, settings_view, silence_email_system_warning, counts, health_check_ready
-from .debug_views import csrf_debug
-admin.site.site_header = get_settings().SITE_TITLE
-admin.site.site_title = get_settings().SITE_TITLE
-admin.site.index_title = "Admin"  # everyone calls this the "admin" anyway. Let's set the title accordingly.
-urlpatterns = [
-    path('', home, name='home'),
-    path("health/ready", health_check_ready, name="health_check_ready"),
-    path("accounts/signup/", signup, name="signup"),
-    path("accounts/resend-confirmation/", resend_confirmation, name="resend_confirmation"),
-    path("accounts/confirm-email/<str:token>/", confirm_email, name="confirm_email"),
-    path("accounts/request-reset-password/", request_reset_password, name="request_reset_password"),
-    path("accounts/reset-password/<str:token>/", reset_password, name="reset_password"),
-    path("accounts/login/", auth_views.LoginView.as_view(template_name="bugsink/login.html"), name="login"),
-    path("accounts/logout/", auth_views.LogoutView.as_view(template_name="users/logged_out.html"), name="logout"),
-    path("accounts/preferences/", preferences, name="preferences"),
-    path("users/", include("users.urls")),
-    path("api/0/organizations/<slug:organization_slug>/chunk-upload/", chunk_upload, name="chunk_upload"),
-    path("api/0/organizations/<slug:organization_slug>/artifactbundle/assemble/", artifact_bundle_assemble,
-         name="artifact_bundle_assemble"),
-    path('api/', include('ingest.urls')),
-    path('api/<path:subpath>', api_catch_all, name='api_catch_all'),
-    path('ingest/envelope/<str:envelope_id>/', download_envelope, name='download_envelope'),
-    path('projects/', include('projects.urls')),
-    path('teams/', include('teams.urls')),
-    path('events/', include('events.urls')),
-    path('issues/', include('issues.urls')),
-    path('files/', include('files.urls')),
-    path('orgredirect/organizations/:orgslug/settings/auth-tokens/',
-         RedirectView.as_view(url='/bsmain/auth_tokens/', permanent=False)),
-    path('bsmain/', include('bsmain.urls')),
-    path('admin/', admin.site.urls),
-    path('silence-email-system-warning/', silence_email_system_warning, name='silence_email_system_warning'),
-    path('settings/', settings_view, name='settings'),
-    path('counts/', counts, name='counts'),
-    path('debug/csrf/', csrf_debug, name='csrf_debug'),
-    path("favicon.ico", favicon),
-    path("robots.txt", login_exempt(TemplateView.as_view(template_name="robots.txt", content_type="text/plain"))),
-]
-if settings.DEBUG:
-    urlpatterns += [
-        path('debug-alerts-email/<str:template_name>/', debug_alerts_email),
-        path('debug-users-email/<str:template_name>/', debug_users_email),
-        path('debug-teams-email/<str:template_name>/', debug_teams_email),
-        path('trigger-error/', trigger_error),
-    ]
-handler400 = "bugsink.views.bad_request"
-handler403 = "bugsink.views.permission_denied"
-handler404 = "bugsink.views.page_not_found"
-handler500 = "bugsink.views.server_error"

--- a//dev/null
+++ b/bugsink/utils.py
@@ -0,0 +1,185 @@
+from collections import defaultdict
+from urllib.parse import urlparse
+from django.core.mail import EmailMultiAlternatives
+from django.template.loader import get_template
+from django.apps import apps
+from django.db.models import ForeignKey, F
+from .version import version
+def send_rendered_email(subject, base_template_name, recipient_list, context=None):
+    if context is None:
+        context = {}
+    html_content = get_template(base_template_name + ".html").render(context)
+    text_content = get_template(base_template_name + ".txt").render(context)
+    msg = EmailMultiAlternatives(
+        subject=subject,
+        body=text_content,
+        from_email=None,  # this is settings.DEFAULT_FROM_EMAIL
+        to=recipient_list,
+    )
+    msg.attach_alternative(html_content, "text/html")
+    msg.send()
+def deduce_allowed_hosts(base_url):
+    url = urlparse(base_url)
+    if url.hostname == "localhost" or url.hostname == "127.0.0.1":
+        return ["*"]
+    return [url.hostname] + ["localhost", "127.0.0.1"]
+def _name(type_):
+    try:
+        return type_.__module__ + "." + type_.__name__
+    except Exception:
+        try:
+            return type_.__name__
+        except Exception:
+            return "unknown"
+def fingerprint_exc(event, exc_info):
+    type_name = _name(exc_info[0])
+    if event["exception"]["values"][-1]["stacktrace"]["frames"][-1]["module"] == "bugsink.wsgi":
+        event['fingerprint'] = ['wsgi', type_name]
+    return event
+def fingerprint_log_record(event, log_record):
+    return event
+def fingerprint_before_send(event, hint):
+    if 'exc_info' in hint:
+        return fingerprint_exc(event, hint['exc_info'])
+    if 'log_record' in hint:
+        return fingerprint_log_record(event, hint['log_record'])
+    return event
+def eat_your_own_dogfood(sentry_dsn, **kwargs):
+    """
+    Configures your Bugsink installation to send messages to some Bugsink-compatible installation.
+    See https://www.bugsink.com/docs/dogfooding/
+    """
+    import sentry_sdk.serializer
+    sentry_sdk.serializer.MAX_DATABAG_DEPTH = float("inf")
+    sentry_sdk.serializer.MAX_DATABAG_BREADTH = float("inf")
+    if sentry_dsn is None:
+        return
+    default_kwargs = {
+        "dsn": sentry_dsn,
+        "traces_sample_rate": 0,
+        "send_default_pii": True,
+        "max_request_body_size": "always",
+        "in_app_include": [
+            "alerts",
+            "bsmain",
+            "bugsink",
+            "compat",
+            "events",
+            "ee",
+            "ingest",
+            "issues",
+            "performance",
+            "phonehome",
+            "projects",
+            "releases",
+            "sentry",
+            "sentry_sdk_extensions",
+            "snappea",
+            "tags",
+            "teams",
+            "theme",
+            "users",
+        ],
+        "release": version,
+        "before_send": fingerprint_before_send,
+    }
+    default_kwargs.update(kwargs)
+    sentry_sdk.init(
+        **default_kwargs,
+    )
+def get_model_topography():
+    """
+    Returns a dependency graph mapping:
+      referenced_model_key -> [
+          (referrer_model_class, fk_name),
+          ...
+      ]
+    """
+    dep_graph = defaultdict(list)
+    for model in apps.get_models():
+        for field in model._meta.get_fields(include_hidden=True):
+            if isinstance(field, ForeignKey):
+                referenced_model = field.related_model
+                referenced_key = f"{referenced_model._meta.app_label}.{referenced_model.__name__}"
+                dep_graph[referenced_key].append((model, field.name))
+    return dep_graph
+def fields_for_prune_orphans(model):
+    if model.__name__ == "IssueTag":
+        return ("value_id",)
+    return ()
+def prune_orphans(model, d_ids_to_check):
+    """For some model, does dangling-model-cleanup.
+    In a sense the oposite of delete_deps; delete_deps takes care of deleting the recursive closure of things that point
+    to some root. The present function cleans up things that are being pointed to (and, after some other thing is
+    deleted, potentially are no longer being pointed to, hence 'orphaned').
+    This is the hardcoded edition (IssueTag only); we _could_ try to think about doing this generically based on the
+    dependency graph, but it's quite questionably whether a combination of generic & performant is easy to arrive at and
+    worth it.
+    pruning of TagValue is done "inline" (as opposed to using a GC-like vacuum "later") because, whatever the exact
+    performance trade-offs may be, the following holds true:
+    1. the inline version is easier to reason about, it "just happens ASAP", and in the context of a given issue;
+       vacuum-based has to take into consideration the full DB including non-orphaned values.
+    2. repeated work is somewhat minimalized b/c of the IssueTag/EventTag relationship as described in prune_tagvalues.
+    """
+    from tags.models import prune_tagvalues  # avoid circular import
+    if model.__name__ != "IssueTag":
+        return  # we only prune IssueTag orphans
+    ids_to_check = [d["value_id"] for d in d_ids_to_check]  # d_ids_to_check: mirrors fields_for_prune_orphans(model)
+    prune_tagvalues(ids_to_check)
+def do_pre_delete(project_id, model, pks_to_delete, is_for_project):
+    "More model-specific cleanup, if needed; only for Event model at the moment."
+    if model.__name__ != "Event":
+        return  # we only do more cleanup for Event
+    from projects.models import Project
+    from events.models import Event
+    from events.retention import cleanup_events_on_storage
+    cleanup_events_on_storage(
+        Event.objects.filter(pk__in=pks_to_delete).exclude(storage_backend=None)
+        .values_list("id", "storage_backend")
+    )
+    if is_for_project:
+        return
+    Project.objects.filter(id=project_id).update(stored_event_count=F('stored_event_count') - len(pks_to_delete))
+def delete_deps_with_budget(project_id, referring_model, fk_name, referred_ids, budget, dep_graph, is_for_project):
+    r"""
+    Deletes all objects of type referring_model that refer to any of the referred_ids via fk_name.
+    Returns the number of deleted objects.
+    And does this recursively (i.e. if there are further dependencies, it will delete those as well).
+        Caller              This Func
+          |                     |
+          V                     V
+     <unspecified>        referring_model
+             ^                  /
+             \-------fk_name----
+        referred_ids        relevant_ids (deduced using a query)
+    """
+    num_deleted = 0
+    relevant_ids = list(
+       referring_model.objects.filter(**{f"{fk_name}__in": referred_ids}).order_by(f"{fk_name}_id", 'pk').values(
+           *(('pk',) + fields_for_prune_orphans(referring_model))
+        )[:budget]
+    )
+    if not relevant_ids:
+        return 0
+    for_recursion = dep_graph.get(f"{referring_model._meta.app_label}.{referring_model.__name__}", [])
+    for model_for_recursion, fk_name_for_recursion in for_recursion:
+        num_deleted += delete_deps_with_budget(
+            project_id,
+            model_for_recursion,
+            fk_name_for_recursion,
+            [d["pk"] for d in relevant_ids],
+            budget - num_deleted,
+            dep_graph,
+            is_for_project,
+        )
+        if num_deleted >= budget:
+            return num_deleted
+    relevant_ids_after_rec = relevant_ids[:budget - num_deleted]
+    do_pre_delete(project_id, referring_model, [d['pk'] for d in relevant_ids_after_rec], is_for_project)
+    my_num_deleted, del_d = referring_model.objects.filter(pk__in=[d['pk'] for d in relevant_ids_after_rec]).delete()
+    num_deleted += my_num_deleted
+    assert set(del_d.keys()) == {referring_model._meta.label}  # assert no-cascading (we do that ourselves)
+    if is_for_project:
+        return num_deleted
+    prune_orphans(referring_model, relevant_ids_after_rec)
+    return num_deleted

--- a//dev/null
+++ b/bugsink/wsgi.py
@@ -0,0 +1,59 @@
+"""
+WSGI config for bugsink project.
+It exposes the WSGI callable as a module-level variable named ``application``.
+For more information on this file, see
+https://docs.djangoproject.com/en/4.2/howto/deployment/wsgi/
+"""
+import os
+import django
+from django.core.handlers.wsgi import WSGIHandler, WSGIRequest
+from django.core.exceptions import DisallowedHost
+os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'bugsink_conf')
+class CustomWSGIRequest(WSGIRequest):
+    """
+    Custom WSQIRequest subclass with 3 fixes/changes:
+    * Chunked Transfer Encoding (Django's behavior is broken)
+    * Skip ALLOWED_HOSTS validation for /health/ endpoints (see #140)
+    * Better error message for disallowed hosts
+    Note: used in all servers (in gunicorn through wsgi.py; in Django's runserver through WSGI_APPLICATION)
+    """
+    def __init__(self, environ):
+        """
+        We override this method to fix Django's behavior in the context of Chunked Transfer Encoding (Django's
+        behavior, behind Gunicorn, is broken). Django's breakage is in the super() of this method, in the combination
+        [1] defaulting (through a set-on-catch) to 0 for CONTENT_LENGTH when not present and [2] settings self._stream
+        to a LimitedStream with that length. The lines below undo this behavior iff the HTTP_TRANSFER_ENCODING header
+        is present. See:
+        * https://code.djangoproject.com/ticket/35838   (The Django problem)
+        * https://github.com/bugsink/bugsink/issues/9   (Why we need a fix)
+        """
+        super().__init__(environ)
+        if "CONTENT_LENGTH" not in environ and "HTTP_TRANSFER_ENCODING" in environ:
+            self._stream = self.environ["wsgi.input"]
+    def get_host(self):
+        """
+        We override this method to provide a more informative error message when the host is disallowed, i.e. we include
+        the current value of ALLOWED_HOSTS in the error message. That this is useful for debugging is self-evident.
+        We're leaking a bit of information here, but I don't think it's too much TBH -- especially in the light of ssl
+        certificates being specifically tied to the domain name.
+        """
+        from django.conf import settings
+        try:
+            return super().get_host()
+        except DisallowedHost as e:
+            if self.path.startswith == "/health/":
+                return self._get_raw_host()
+            message = str(e)
+            if "ALLOWED_HOSTS" in message:
+                allowed_hosts = settings.ALLOWED_HOSTS
+                if settings.DEBUG and not allowed_hosts:
+                    allowed_hosts = [".localhost", "127.0.0.1", "[::1]"]
+                message = message[:-1 * len(".")]
+                message += ", which is currently set to %s." % repr(allowed_hosts)
+            raise DisallowedHost(message) from None
+class CustomWSGIHandler(WSGIHandler):
+    request_class = CustomWSGIRequest
+def custom_get_wsgi_application():
+    django.setup(set_prefix=False)
+    return CustomWSGIHandler()
+application = custom_get_wsgi_application()

--- a//dev/null
+++ b/events/admin.py
@@ -0,0 +1,87 @@
+import json
+from django.utils.html import escape, mark_safe
+from django.contrib import admin
+from django.views.decorators.csrf import csrf_protect
+from django.utils.decorators import method_decorator
+from bugsink.transaction import immediate_atomic
+from projects.admin import ProjectFilter
+from .models import Event
+csrf_protect_m = method_decorator(csrf_protect)
+@admin.register(Event)
+class EventAdmin(admin.ModelAdmin):
+    ordering = ['-timestamp']
+    search_fields = ['event_id', 'debug_info']
+    list_display = [
+        'timestamp',
+        'platform',
+        'level',
+        'sdk_name',
+        'sdk_version',
+        'debug_info',
+        'on_site',
+    ]
+    list_filter = [
+        ProjectFilter,
+        'platform',
+        'level',
+        'sdk_name',
+        'sdk_version',
+    ]
+    fields = [
+        'id',
+        'event_id',
+        'ingested_at',
+        'digested_at',
+        'calculated_type',
+        'calculated_value',
+        'issue',
+        'project',
+        'timestamp',
+        'platform',
+        'level',
+        'logger',
+        'transaction',
+        'server_name',
+        'release',
+        'dist',
+        'environment',
+        'sdk_name',
+        'sdk_version',
+        'debug_info',
+        'pretty_data',
+    ]
+    readonly_fields = [
+        'id',
+        'event_id',
+        'ingested_at',
+        'digested_at',
+        'calculated_type',
+        'calculated_value',
+        'issue',
+        'timestamp',
+        'project',
+        'pretty_data',
+    ]
+    def pretty_data(self, obj):
+        return mark_safe("<pre>" + escape(json.dumps(json.loads(obj.data), indent=2)) + "</pre>")
+    pretty_data.short_description = "Data"
+    def on_site(self, obj):
+        return mark_safe('<a href="' + escape(obj.get_absolute_url()) + '">View</a>')
+    def get_deleted_objects(self, objs, request):
+        to_delete = list(objs) + ["...all its related objects... (delayed)"]
+        model_count = {
+            Event: len(objs),
+        }
+        perms_needed = set()
+        protected = []
+        return to_delete, model_count, perms_needed, protected
+    def delete_queryset(self, request, queryset):
+        with immediate_atomic():
+            for obj in queryset:
+                obj.delete_deferred()
+    def delete_model(self, request, obj):
+        with immediate_atomic():
+            obj.delete_deferred()
+    @csrf_protect_m
+    def delete_view(self, request, object_id, extra_context=None):
+        return self._delete_view(request, object_id, extra_context)

--- a//dev/null
+++ b/events/factories.py
@@ -0,0 +1,47 @@
+import json
+import uuid
+from django.utils import timezone
+from django.db.models import Max
+from issues.factories import get_or_create_issue
+from .models import Event
+def create_event(project=None, issue=None, timestamp=None, event_data=None):
+    if issue is None:
+        issue, _ = get_or_create_issue(project, event_data)
+    if project is None:
+        project = issue.project
+    if timestamp is None:
+        timestamp = timezone.now()
+    if event_data is None:
+        event_data = create_event_data()
+    max_current = Event.objects.filter(project=project).aggregate(
+        Max("digest_order"))["digest_order__max"]
+    issue_digest_order = max_current + 1 if max_current is not None else 1
+    grouping = issue.grouping_set.first()
+    return Event.objects.create(
+        project=project,
+        issue=issue,
+        grouping=grouping,
+        ingested_at=timestamp,
+        digested_at=timestamp,
+        timestamp=timestamp,
+        event_id=uuid.uuid4().hex,
+        data=json.dumps(event_data),
+        digest_order=issue_digest_order,
+        irrelevance_for_retention=0,
+    )
+def create_event_data(exception_type=None):
+    result = {
+        "event_id": uuid.uuid4().hex,
+        "timestamp": timezone.now().isoformat(),
+        "platform": "python",
+    }
+    if exception_type is not None:
+        result["exception"] = {
+            "values": [
+                {
+                    "type": exception_type,
+                    "value": "This is a test exception",
+                }
+            ]
+        }
+    return result

--- a//dev/null
+++ b/events/management/commands/make_consistent.py
@@ -0,0 +1,105 @@
+from django.core.management.base import BaseCommand
+from django.db.models import Count
+from releases.models import Release
+from issues.models import Issue, Grouping, TurningPoint
+from events.models import Event
+from projects.models import Project
+from tags.models import TagKey, TagValue, EventTag, IssueTag
+from bugsink.transaction import immediate_atomic
+from bugsink.timed_sqlite_backend.base import allow_long_running_queries
+from bugsink.moreiterutils import batched
+from projects.tasks import delete_project_deps
+from issues.tasks import delete_issue_deps
+class DryRunException(Exception):
+    pass
+def _delete_for_missing_fk(clazz, field_name):
+    """
+    Delete all objects of class clazz of which the field field_name points to a non-existing object or null.
+    Non-existing objects may come into being when people muddle in the database directly with foreign key checks turned
+    off (note that fk checks are turned off by default in sqlite's CLI for backwards compatibility reasons).
+    In the future it's further possible that there will be pieces the actual Bugsink code where FK-checks are turned off
+    temporarily (e.g. when deleting a project with very many related objects). (In March 2025 there was no such code
+    yet)
+    To make make_consistent() do what it says on the can, we need to delete these dangling objects.
+    """
+    BATCH_SIZE = 1_000
+    dangling_fks = set()
+    field = clazz._meta.get_field(field_name)
+    related_model = field.related_model
+    available_keys = set(related_model.objects.values_list('pk', flat=True))
+    for batch in batched(clazz.objects.values_list(field.get_attname(), flat=True).distinct(), BATCH_SIZE):
+        for key in batch:
+            if key not in available_keys:
+                dangling_fks.add(key)
+    def _del(deletion_kwargs, msg_kind):
+        total_cnt, d_of_counts = clazz.objects.filter(**deletion_kwargs).delete()
+        count = d_of_counts.get(clazz._meta.label, 0)
+        if count == 0:
+            return
+        print("Deleted %d %ss, because their %s was %s" % (count, clazz.__name__, field_name, msg_kind))
+    _del({field.get_attname(): None}, "NULL")
+    for batch in batched(dangling_fks, BATCH_SIZE):
+        _del({field.get_attname() + '__in': batch}, "non-existing")
+def make_consistent():
+    for issue in Issue.objects.annotate(fresh_event_count=Count('event')).filter(fresh_event_count=0):
+        print("Deleting issue %s, because it has 0 events" % issue)
+        issue.delete()
+    _delete_for_missing_fk(Issue, 'project')
+    _delete_for_missing_fk(Grouping, 'project')
+    _delete_for_missing_fk(Grouping, 'issue')
+    _delete_for_missing_fk(TurningPoint, 'issue')
+    _delete_for_missing_fk(Release, 'project')
+    _delete_for_missing_fk(EventTag, 'issue')  # See #132 for the ordering of this statement
+    _delete_for_missing_fk(Event, 'project')
+    _delete_for_missing_fk(Event, 'issue')
+    _delete_for_missing_fk(IssueTag, 'project')
+    _delete_for_missing_fk(IssueTag, 'value')
+    _delete_for_missing_fk(IssueTag, 'issue')
+    _delete_for_missing_fk(EventTag, 'project')
+    _delete_for_missing_fk(EventTag, 'value')
+    _delete_for_missing_fk(EventTag, 'event')
+    _delete_for_missing_fk(TagValue, 'project')
+    _delete_for_missing_fk(TagValue, 'key')
+    _delete_for_missing_fk(TagKey, 'project')
+    for event in Event.objects.filter(turningpoint__isnull=False, never_evict=False).distinct():
+        print("Setting event %s to never_evict because it has a turningpoint" % event)
+        event.never_evict = True
+        event.save()
+    for issue in Issue.objects.all():
+        if issue.stored_event_count != issue.event_set.count():
+            print("Updating event count for issue %s from %d to %d" % (
+                issue, issue.stored_event_count, issue.event_set.count()))
+            issue.stored_event_count = issue.event_set.count()
+            issue.save()
+    for project in Project.objects.all():
+        if project.stored_event_count != project.event_set.count():
+            print("Updating event count for project %s from %d to %d" % (
+                project, project.stored_event_count, project.event_set.count()))
+            project.stored_event_count = project.event_set.count()
+            project.save()
+    for project in Project.objects.all():
+        if project.has_releases != project.release_set.exists():
+            print("Updating has_releases for project %s from %s to %s" % (
+                project, project.has_releases, project.release_set.exists()))
+            project.has_releases = project.release_set.exists()
+            project.save()
+class Command(BaseCommand):
+    help = """Make the database consistent by deleting dangling objects (issues, events, etc) and updating counters."""
+    def add_arguments(self, parser):
+        parser.add_argument('--dry-run', action='store_true', help="Roll back all changes after making them.")
+    def handle(self, *args, **options):
+        allow_long_running_queries()
+        try:
+            with immediate_atomic():
+                make_consistent()
+                if options['dry_run']:
+                    raise DryRunException("Dry run requested; rolling back changes.")
+            if not options['dry_run']:
+                for obj in Project.objects.filter(is_deleted=True):
+                    print("Enqueuing deletion of project dependencies for %s" % obj)
+                    delete_project_deps.delay(str(obj.pk))
+                for obj in Issue.objects.filter(is_deleted=True):
+                    print("Enqueuing deletion of issue dependencies for %s" % obj)
+                    delete_issue_deps.delay(str(obj.project_id), str(obj.pk))
+        except DryRunException:
+            print("Changes have been rolled back (dry-run)")

--- a//dev/null
+++ b/events/migrations/0020_remove_events_with_null_issue_or_grouping.py
@@ -0,0 +1,13 @@
+from django.db import migrations
+def remove_events_with_null_fks(apps, schema_editor):
+    Event = apps.get_model("events", "Event")
+    Event.objects.filter(issue__isnull=True).delete()
+    Event.objects.filter(grouping__isnull=True).delete()
+class Migration(migrations.Migration):
+    dependencies = [
+        ("events", "0019_event_storage_backend"),
+        ("issues", "0020_remove_objects_with_null_issue"),
+    ]
+    operations = [
+        migrations.RunPython(remove_events_with_null_fks, reverse_code=migrations.RunPython.noop),
+    ]

--- a//dev/null
+++ b/events/migrations/0021_alter_do_nothing.py
@@ -0,0 +1,23 @@
+from django.db import migrations, models
+import django.db.models.deletion
+class Migration(migrations.Migration):
+    dependencies = [
+        ("issues", "0021_alter_do_nothing"),
+        ("events", "0020_remove_events_with_null_issue_or_grouping"),
+    ]
+    operations = [
+        migrations.AlterField(
+            model_name="event",
+            name="grouping",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="issues.grouping"
+            ),
+        ),
+        migrations.AlterField(
+            model_name="event",
+            name="issue",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="issues.issue"
+            ),
+        ),
+    ]

--- a//dev/null
+++ b/events/migrations/0022_alter_event_project.py
@@ -0,0 +1,16 @@
+from django.db import migrations, models
+import django.db.models.deletion
+class Migration(migrations.Migration):
+    dependencies = [
+        ("projects", "0014_alter_projectmembership_project"),
+        ("events", "0021_alter_do_nothing"),
+    ]
+    operations = [
+        migrations.AlterField(
+            model_name="event",
+            name="project",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="projects.project"
+            ),
+        ),
+    ]

--- a//dev/null
+++ b/events/models.py
@@ -0,0 +1,158 @@
+import re
+import json
+import uuid
+from django.db import models
+from django.db.utils import IntegrityError
+from django.utils.functional import cached_property
+from projects.models import Project
+from compat.timestamp import parse_timestamp
+from bugsink.transaction import delay_on_commit
+from issues.utils import get_title_for_exception_type_and_value
+from .retention import get_random_irrelevance
+from .storage_registry import get_write_storage, get_storage
+from .tasks import delete_event_deps
+class Platform(models.TextChoices):
+    AS3 = "as3"
+    C = "c"
+    CFML = "cfml"
+    COCOA = "cocoa"
+    CSHARP = "csharp"
+    ELIXIR = "elixir"
+    HASKELL = "haskell"
+    GO = "go"
+    GROOVY = "groovy"
+    JAVA = "java"
+    JAVASCRIPT = "javascript"
+    NATIVE = "native"
+    NODE = "node"
+    OBJC = "objc"
+    OTHER = "other"
+    PERL = "perl"
+    PHP = "php"
+    PYTHON = "python"
+    RUBY = "ruby"
+class Level(models.TextChoices):
+    FATAL = "fatal"
+    ERROR = "error"
+    WARNING = "warning"
+    INFO = "info"
+    DEBUG = "debug"
+def maybe_empty(s):
+    return "" if not s else s
+def write_to_storage(event_id, parsed_data):
+    """
+    event_id means event.id, i.e. the internal one. This saves us from thinking about the security implications of
+    using an externally provided ID across storage backends.
+    """
+    with get_write_storage().open(event_id, "w") as f:
+        json.dump(parsed_data, f)
+class Event(models.Model):
+    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False, help_text="Bugsink-internal")
+    ingested_at = models.DateTimeField(blank=False, null=False)
+    digested_at = models.DateTimeField(db_index=True, blank=False, null=False)
+    issue = models.ForeignKey("issues.Issue", blank=False, null=False, on_delete=models.DO_NOTHING)
+    grouping = models.ForeignKey("issues.Grouping", blank=False, null=False, on_delete=models.DO_NOTHING)
+    event_id = models.UUIDField(primary_key=False, null=False, editable=False, help_text="As per the sent data")
+    project = models.ForeignKey(Project, blank=False, null=False, on_delete=models.DO_NOTHING)
+    data = models.TextField(blank=False, null=False)
+    timestamp = models.DateTimeField(db_index=True, blank=False, null=False)
+    platform = models.CharField(max_length=64, blank=False, null=False, choices=Platform.choices)
+    level = models.CharField(max_length=len("warning"), blank=True, null=False, choices=Level.choices)
+    logger = models.CharField(max_length=64, blank=True, null=False, default="")  # , db_index=True)
+    transaction = models.CharField(max_length=200, blank=True, null=False, default="")
+    server_name = models.CharField(max_length=255, blank=True, null=False, default="")
+    release = models.CharField(max_length=250, blank=True, null=False, default="")
+    dist = models.CharField(max_length=64, blank=True, null=False, default="")
+    environment = models.CharField(max_length=64, blank=True, null=False, default="")
+    sdk_name = models.CharField(max_length=255, blank=True, null=False, default="")
+    sdk_version = models.CharField(max_length=255, blank=True, null=False, default="")
+    debug_info = models.CharField(max_length=255, blank=True, null=False, default="")
+    calculated_type = models.CharField(max_length=128, blank=True, null=False, default="")
+    calculated_value = models.TextField(max_length=1024, blank=True, null=False, default="")
+    last_frame_filename = models.CharField(max_length=255, blank=True, null=False, default="")
+    last_frame_module = models.CharField(max_length=255, blank=True, null=False, default="")
+    last_frame_function = models.CharField(max_length=255, blank=True, null=False, default="")
+    digest_order = models.PositiveIntegerField(blank=False, null=False)
+    irrelevance_for_retention = models.PositiveIntegerField(blank=False, null=False)
+    never_evict = models.BooleanField(blank=False, null=False, default=False)
+    storage_backend = models.CharField(max_length=255, blank=True, null=True, default=None, editable=False)
+    class Meta:
+        unique_together = [
+            ("project", "event_id"),
+            ("issue", "digest_order"),
+        ]
+        indexes = [
+            models.Index(fields=["project", "never_evict", "digested_at", "irrelevance_for_retention"]),
+            models.Index(fields=["issue", "digested_at"]),
+        ]
+    def get_raw_data(self):
+        if self.storage_backend is None:
+            return self.data
+        storage = get_storage(self.storage_backend)
+        with storage.open(self.id, "r") as f:
+            return f.read()
+    def get_parsed_data(self):
+        if self.storage_backend is None:
+            return json.loads(self.data)
+        storage = get_storage(self.storage_backend)
+        with storage.open(self.id, "r") as f:
+            return json.load(f)
+    def get_absolute_url(self):
+        return f"/issues/issue/{ self.issue_id }/event/{ self.id }/"
+    def get_raw_link(self):
+        return "/events/event/%s/raw/" % self.id
+    def get_download_link(self):
+        return "/events/event/%s/download/" % self.id
+    def title(self):
+        return get_title_for_exception_type_and_value(self.calculated_type, self.calculated_value)
+    @classmethod
+    def from_ingested(cls, event_metadata, digested_at, digest_order, stored_event_count, issue, grouping, parsed_data,
+                      denormalized_fields):
+        irrelevance_for_retention = get_random_irrelevance(stored_event_count)
+        write_storage = get_write_storage()
+        try:
+            event = cls.objects.create(
+                event_id=event_metadata["event_id"],  # the metadata is the envelope's event_id, which takes precedence
+                project_id=event_metadata["project_id"],
+                issue=issue,
+                grouping=grouping,
+                ingested_at=event_metadata["ingested_at"],
+                digested_at=digested_at,
+                data=json.dumps(parsed_data) if write_storage is None else "",
+                storage_backend=None if write_storage is None else write_storage.name,
+                timestamp=parse_timestamp(parsed_data["timestamp"]),
+                platform=parsed_data["platform"][:64],
+                level=maybe_empty(parsed_data.get("level", "")),
+                logger=maybe_empty(parsed_data.get("logger", ""))[:64],
+                server_name=maybe_empty(parsed_data.get("server_name", ""))[:255],
+                release=maybe_empty(parsed_data.get("release", ""))[:250],
+                dist=maybe_empty(parsed_data.get("dist", ""))[:64],
+                environment=maybe_empty(parsed_data.get("environment", ""))[:64],
+                sdk_name=maybe_empty(parsed_data.get("", {}).get("name", ""))[:255],
+                sdk_version=maybe_empty(parsed_data.get("", {}).get("version", ""))[:255],
+                debug_info=event_metadata["debug_info"][:255],
+                digest_order=digest_order,
+                irrelevance_for_retention=irrelevance_for_retention,
+                **denormalized_fields,
+            )
+            created = True
+            if write_storage is not None:
+                write_to_storage(event.id, parsed_data)
+            return event, created
+        except IntegrityError as e:
+            ignore_patterns = [
+                r".*unique constraint failed.*events_event.*project_id.*events_event.*event_id",  # sqlite
+                r".*duplicate entry.*for key.*events_event.events_event_project_id_event_id.*",  # mysql
+                r".*duplicate key value violates unique constraint.*events_event_project_id_event_id.*",  # postgres
+            ]
+            if not any(re.match(p, str(e).lower()) for p in ignore_patterns):
+                raise
+            return None, False
+    @cached_property
+    def get_tags(self):
+        return list(
+            self.tags.all().select_related("value", "value__key").order_by("value__key__key")
+        )
+    def delete_deferred(self):
+        """Schedules deletion of all related objects"""
+        delay_on_commit(delete_event_deps, str(self.project_id), str(self.id))

--- a//dev/null
+++ b/events/retention.py
@@ -0,0 +1,172 @@
+import logging
+from django.db.models import Q, Min, Max, Count
+from random import random
+from datetime import timezone, datetime
+from bugsink.moreiterutils import pairwise, map_N_until
+from performance.context_managers import time_and_query_count
+from .storage_registry import get_storage
+bugsink_logger = logging.getLogger("bugsink")
+performance_logger = logging.getLogger("bugsink.performance.retention")
+class EvictionCounts:
+    def __init__(self, total, per_issue):
+        self.total = total
+        self.per_issue = per_issue
+    def __add__(self, other):
+        return EvictionCounts(
+            self.total + other.total,
+            {k: self.per_issue.get(k, 0) + other.per_issue.get(k, 0)
+             for k in set(self.per_issue) | set(other.per_issue)})
+    def __repr__(self):
+        return f"EvictionCounts(total={self.total})"
+def get_epoch(datetime_obj):
+    assert datetime_obj.tzinfo == timezone.utc
+    return int(datetime_obj.timestamp() / 3600)
+def datetime_for_epoch(epoch):
+    return datetime.fromtimestamp(epoch * 3600, timezone.utc)
+def get_epoch_bounds(lower, upper=None):
+    if lower is None and upper is None:
+        return Q()
+    if lower is None:
+        return Q(digested_at__lt=datetime_for_epoch(upper))
+    if upper is None:
+        return Q(digested_at__gte=datetime_for_epoch(lower))
+    return Q(digested_at__gte=datetime_for_epoch(lower), digested_at__lt=datetime_for_epoch(upper))
+def nonzero_leading_bits(n):
+    """
+    Return the non-roundness of a number when represented in binary, i.e. the number of leading bits until the last 1.
+    examples:
+    100000 -> 1
+    101000 -> 3
+    110001 -> 6
+    """
+    s = format(n, 'b')
+    return len(s.rstrip('0'))
+def get_random_irrelevance(stored_event_count):
+    """
+    gets a fixed-at-creation irrelevance-score for an Event; the basic idea is: the more events you have for a certain
+    issue, the less relevant any new event will be _on average_; but when you have many events you will on average still
+    have more relevant events than if you have few events.
+    irrelevance is basically determined by `nonzero_leading_bits`; we add some randomization to avoid repeated outcomes
+    if `cnt` "hovers" around a certain value (which is likely to happen when there's repeated eviction/fill-up). Ã—2 is
+    simply to correct for random() (which returns .5 on average).
+    """
+    assert stored_event_count >= 1
+    return nonzero_leading_bits(round(random() * stored_event_count * 2))
+def should_evict(project, timestamp, stored_event_count):
+    if stored_event_count > project.retention_max_event_count:  # > because: do something when _over_ the max
+        return True
+    return False
+def get_age_for_irrelevance(age_based_irrelevance):
+    return pow(4, age_based_irrelevance) - 1
+def get_epoch_bounds_with_irrelevance(project, current_timestamp, qs_kwargs={"never_evict": False}):
+    """Returns the epoch bounds for the project (newest first), with the age-based irrelevance for each epoch."""
+    from .models import Event
+    oldest = Event.objects.filter(project=project, **qs_kwargs).aggregate(val=Min('digested_at'))['val']
+    first_epoch = get_epoch(oldest) if oldest is not None else get_epoch(current_timestamp)
+    current_epoch = get_epoch(current_timestamp)
+    difference = current_epoch - first_epoch
+    ages = list(map_N_until(get_age_for_irrelevance, difference))  # e.g. [0, 3, 15]
+    epochs = [current_epoch - age for age in ages]  # e.g. [100, 97, 85]
+    swapped_bounds = pairwise([None] + epochs + [None])  # e.g. [(None, 100), (100, 97), (97, 85), (85, None)]
+    bounds = [(older, newer) for (newer, older) in swapped_bounds]  # e.g [(100, None), (97, 100), ...]
+    return [((lb, ub), age_based_irrelevance) for (age_based_irrelevance, (lb, ub)) in enumerate(bounds)]
+def get_irrelevance_pairs(project, epoch_bounds_with_irrelevance, qs_kwargs={"never_evict": False}):
+    """tuples of `age_based_irrelevance` and, per associated period, the max observed (evictable) event irrelevance"""
+    from .models import Event
+    for (lower_bound, upper_bound), age_based_irrelevance in epoch_bounds_with_irrelevance:
+        d = Event.objects.filter(
+            get_epoch_bounds(lower_bound, upper_bound),
+            project=project,
+            **qs_kwargs,
+        ).aggregate(Max('irrelevance_for_retention'))
+        max_event_irrelevance = d["irrelevance_for_retention__max"] or 0
+        yield (age_based_irrelevance, max_event_irrelevance)
+def filter_for_work(epoch_bounds_with_irrelevance, pairs, max_total_irrelevance):
+    for pair, ebwi in zip(pairs, epoch_bounds_with_irrelevance):
+        if sum(pair) > max_total_irrelevance:  # > because only if it is strictly greater will anything be evicted.
+            yield ebwi
+def eviction_target(max_event_count, stored_event_count):
+    return min(
+               max(
+                   int(max_event_count * 0.05),
+                   stored_event_count - max_event_count,
+               ),
+               500,
+           )
+def evict_for_max_events(project, timestamp, stored_event_count, include_never_evict=False):
+    qs_kwargs = {} if include_never_evict else {"never_evict": False}
+    with time_and_query_count() as phase0:
+        epoch_bounds_with_irrelevance = get_epoch_bounds_with_irrelevance(project, timestamp, qs_kwargs)
+        pairs = list(get_irrelevance_pairs(project, epoch_bounds_with_irrelevance, qs_kwargs))
+        max_total_irrelevance = orig_max_total_irrelevance = max(sum(pair) for pair in pairs)
+    with time_and_query_count() as phase1:
+        evicted = EvictionCounts(0, {})
+        target = eviction_target(project.retention_max_event_count, stored_event_count)
+        while evicted.total < target:
+            max_total_irrelevance -= 1
+            epoch_bounds_with_irrelevance_with_possible_work = list(
+                filter_for_work(epoch_bounds_with_irrelevance, pairs, max_total_irrelevance))
+            evicted += evict_for_irrelevance(
+                project,
+                max_total_irrelevance,
+                epoch_bounds_with_irrelevance_with_possible_work,
+                include_never_evict,
+                target - evicted.total,
+            )
+            if evicted.total < target and max_total_irrelevance <= -1:
+                if not include_never_evict:
+                    return evicted + evict_for_max_events(project, timestamp, stored_event_count - evicted.total, True)
+                bugsink_logger.error(
+                    "Failed to evict enough events; %d < %d (max %d, stored %d)", evicted.total, target,
+                    project.retention_max_event_count, stored_event_count)
+                break
+    performance_logger.info(
+        "%6.2fms EVICT; down to %d, max irr. from %d to %d in %dms+%dms and %d+%d queries",
+        phase0.took + phase1.took,
+        stored_event_count - evicted.total - 1,  # down to: -1, because the +1 happens post-eviction
+        orig_max_total_irrelevance, max_total_irrelevance, phase0.took, phase1.took, phase0.count, phase1.count)
+    return evicted
+def evict_for_irrelevance(
+        project, max_total_irrelevance, epoch_bounds_with_irrelevance, include_never_evict=False, max_event_count=0):
+    evicted = EvictionCounts(0, {})
+    for (_, epoch_ub_exclusive), irrelevance_for_age in epoch_bounds_with_irrelevance:
+        max_item_irrelevance = max_total_irrelevance - irrelevance_for_age
+        current_max = max_event_count - evicted.total
+        evicted += evict_for_epoch_and_irrelevance(
+            project, epoch_ub_exclusive, max_item_irrelevance, current_max, include_never_evict)
+        if max_item_irrelevance <= -1:
+            break
+        if evicted.total >= max_event_count:
+            break
+    return evicted
+def evict_for_epoch_and_irrelevance(project, max_epoch, max_irrelevance, max_event_count, include_never_evict):
+    from issues.models import TurningPoint
+    from .models import Event
+    from tags.models import EventTag
+    qs_kwargs = {} if include_never_evict else {"never_evict": False}
+    qs = Event.objects.filter(project=project, irrelevance_for_retention__gt=max_irrelevance, **qs_kwargs)
+    if max_epoch is not None:
+        qs = qs.filter(digested_at__lt=datetime_for_epoch(max_epoch))
+    if include_never_evict:
+        TurningPoint.objects.filter(triggering_event__in=qs).update(triggering_event=None)
+    pks_to_delete = list(qs.order_by("digest_order")[:max_event_count].values_list("pk", flat=True))
+    if len(pks_to_delete) > 0:
+        cleanup_events_on_storage(
+            Event.objects.filter(pk__in=pks_to_delete).exclude(storage_backend=None)
+            .values_list("id", "storage_backend")
+        )
+        deletions_per_issue = {
+            d['issue_id']: d['count'] for d in
+            Event.objects.filter(pk__in=pks_to_delete).values("issue_id").annotate(count=Count("issue_id"))}
+        EventTag.objects.filter(event_id__in=pks_to_delete).delete()
+        nr_of_deletions = Event.objects.filter(pk__in=pks_to_delete).delete()[1].get("events.Event", 0)
+    else:
+        nr_of_deletions = 0
+        deletions_per_issue = {}
+    return EvictionCounts(nr_of_deletions, deletions_per_issue)
+def cleanup_events_on_storage(todos):
+    for event_id, storage_backend in todos:
+        try:
+            get_storage(storage_backend).delete(event_id)
+        except Exception as e:
+            bugsink_logger.error("Error during cleanup of %s on %s: %s", event_id, storage_backend, e)

--- a//dev/null
+++ b/events/tasks.py
@@ -0,0 +1,31 @@
+from snappea.decorators import shared_task
+from bugsink.utils import get_model_topography, delete_deps_with_budget
+from bugsink.transaction import immediate_atomic, delay_on_commit
+@shared_task
+def delete_event_deps(project_id, event_id):
+    from .models import Event   # avoid circular import
+    with immediate_atomic():
+        budget = 500
+        num_deleted = 0
+        dep_graph = get_model_topography()
+        for model_for_recursion, fk_name_for_recursion in dep_graph["events.Event"]:
+            this_num_deleted = delete_deps_with_budget(
+                project_id,
+                model_for_recursion,
+                fk_name_for_recursion,
+                [event_id],
+                budget - num_deleted,
+                dep_graph,
+                is_for_project=False,
+            )
+            num_deleted += this_num_deleted
+            if num_deleted >= budget:
+                delay_on_commit(delete_event_deps, project_id, event_id)
+                return
+        if budget - num_deleted <= 0:
+            delay_on_commit(delete_event_deps, project_id, event_id)
+        else:
+            issue = Event.objects.get(pk=event_id).issue
+            Event.objects.filter(pk=event_id).delete()
+            issue.stored_event_count -= 1
+            issue.save(update_fields=["stored_event_count"])

--- a/ingest/filestore.py
+++ b//dev/null
@@ -1,6 +0,0 @@
-import uuid
-import os
-from bugsink.app_settings import get_settings
-def get_filename_for_event_id(event_id):
-    event_id_normalized = uuid.UUID(event_id).hex
-    return os.path.join(get_settings().INGEST_STORE_BASE_DIR, event_id_normalized)

--- a/ingest/views.py
+++ b/ingest/views.py
@@ -1,11 +1,10 @@
-import uuid
 import hashlib
 import os
 import logging
 import io
 from datetime import datetime, timezone
 import json
 import jsonschema
 import fastjsonschema
 from django.conf import settings
 from django.shortcuts import get_object_or_404
@@ -300,24 +299,20 @@
         else:
             project = self.get_project_for_request(project_pk, request)
         if project.quota_exceeded_until is not None and ingested_at < project.quota_exceeded_until:
             return HttpResponse(status=HTTP_429_TOO_MANY_REQUESTS)
         def factory(item_headers):
             if item_headers.get("type") == "event":
                 if get_settings().DIGEST_IMMEDIATELY:
                     return MaxDataWriter("MAX_EVENT_SIZE", io.BytesIO())
                 if "event_id" not in envelope_headers:
                     raise ParseError("event_id not found in envelope headers")
-                try:
-                    uuid.UUID(envelope_headers["event_id"])
-                except ValueError:
-                    raise ParseError("event_id in envelope headers is not a valid UUID")
                 filename = get_filename_for_event_id(envelope_headers["event_id"])
                 os.makedirs(os.path.dirname(filename), exist_ok=True)
                 return MaxDataWriter("MAX_EVENT_SIZE", open(filename, 'wb'))
             return NullWriter()
         for item_headers, event_output_stream in parser.get_items(factory):
             try:
                 if item_headers.get("type") != "event":
                     logger.info("skipping non-event item: %s", item_headers.get("type"))
                     if item_headers.get("type") == "transaction":
                         logger.info("discarding the rest of the envelope")

--- a//dev/null
+++ b/issues/admin.py
@@ -0,0 +1,91 @@
+from django.contrib import admin
+from bugsink.transaction import immediate_atomic
+from django.utils.decorators import method_decorator
+from django.views.decorators.csrf import csrf_protect
+from .models import Issue, Grouping, TurningPoint
+from .forms import IssueAdminForm
+csrf_protect_m = method_decorator(csrf_protect)
+class GroupingInline(admin.TabularInline):
+    model = Grouping
+    extra = 0
+    exclude = ['project']
+    readonly_fields = [
+        'grouping_key',
+    ]
+class TurningPointInline(admin.TabularInline):
+    model = TurningPoint
+    extra = 0
+    exclude = ['project']
+    fields = [
+        "kind",
+        "timestamp",
+        "user",
+        "triggering_event",
+        "metadata",
+        "comment",
+    ]
+    readonly_fields = [
+        "user",  # readonly because it avoid thinking about well-implemented select-boxes
+        "triggering_event",  # readonly because it avoid thinking about well-implemented select-boxes
+        "metadata",  # readonly to avoid a big textbox
+        "comment",  # readonly to avoid a big textbox
+    ]
+@admin.register(Issue)
+class IssueAdmin(admin.ModelAdmin):
+    form = IssueAdminForm
+    fields = [
+        'project',
+        'friendly_id',
+        'calculated_type',
+        'calculated_value',
+        'last_seen',
+        'first_seen',
+        'is_resolved',
+        'fixed_at',
+        'events_at',
+        'is_muted',
+        'unmute_on_volume_based_conditions',
+        'unmute_after',
+        'digested_event_count',
+        'stored_event_count',
+    ]
+    inlines = [
+        GroupingInline,
+        TurningPointInline,
+    ]
+    list_display = [
+        "title",
+        "project",
+        "digested_event_count",
+        "stored_event_count",
+    ]
+    list_filter = [
+        "project",
+    ]
+    exclude = ["events"]
+    readonly_fields = [
+        'project',
+        'friendly_id',
+        'calculated_type',
+        'calculated_value',
+        'digested_event_count',
+        'stored_event_count',
+    ]
+    def get_deleted_objects(self, objs, request):
+        to_delete = list(objs) + ["...all its related objects... (delayed)"]
+        model_count = {
+            Issue: len(objs),
+        }
+        perms_needed = set()
+        protected = []
+        return to_delete, model_count, perms_needed, protected
+    def delete_queryset(self, request, queryset):
+        with immediate_atomic():
+            for obj in queryset:
+                obj.delete_deferred()
+    def delete_model(self, request, obj):
+        with immediate_atomic():
+            obj.delete_deferred()
+    @csrf_protect_m
+    def delete_view(self, request, object_id, extra_context=None):
+        return self._delete_view(request, object_id, extra_context)

--- a//dev/null
+++ b/issues/factories.py
@@ -0,0 +1,37 @@
+import hashlib
+from django.utils import timezone
+from projects.models import Project
+from .models import Issue, Grouping
+from .utils import get_issue_grouper_for_data
+def get_or_create_issue(project=None, event_data=None):
+    """create issue for testing purposes (code basically stolen from ingest/views.py)"""
+    if event_data is None:
+        from events.factories import create_event_data
+        event_data = create_event_data()
+    if project is None:
+        project = Project.objects.create(name="Test project")
+    grouping_key = get_issue_grouper_for_data(event_data)
+    if not Grouping.objects.filter(project=project, grouping_key=grouping_key).exists():
+        issue = Issue.objects.create(
+            project=project,
+            **denormalized_issue_fields(),
+        )
+        issue_created = True
+        grouping = Grouping.objects.create(
+            project=project,
+            grouping_key=grouping_key,
+            grouping_key_hash=hashlib.sha256(grouping_key.encode()).hexdigest(),
+            issue=issue,
+        )
+    else:
+        grouping = Grouping.objects.get(project=project, grouping_key=grouping_key)
+        issue = grouping.issue
+        issue_created = False
+    return issue, issue_created
+def denormalized_issue_fields():
+    """placeholder values for the "denormalized" (cached, calculated) fields on Issue for which there is no default"""
+    return {
+        "first_seen": timezone.now(),
+        "last_seen": timezone.now(),
+        "digested_event_count": 1,
+    }

--- a//dev/null
+++ b/issues/migrations/0018_issue_is_deleted.py
@@ -0,0 +1,12 @@
+from django.db import migrations, models
+class Migration(migrations.Migration):
+    dependencies = [
+        ("issues", "0017_issue_list_indexes_must_start_with_project"),
+    ]
+    operations = [
+        migrations.AddField(
+            model_name="issue",
+            name="is_deleted",
+            field=models.BooleanField(default=False),
+        ),
+    ]

--- a//dev/null
+++ b/issues/migrations/0019_alter_grouping_grouping_key_hash.py
@@ -0,0 +1,12 @@
+from django.db import migrations, models
+class Migration(migrations.Migration):
+    dependencies = [
+        ("issues", "0018_issue_is_deleted"),
+    ]
+    operations = [
+        migrations.AlterField(
+            model_name="grouping",
+            name="grouping_key_hash",
+            field=models.CharField(max_length=64, null=True),
+        ),
+    ]

--- a//dev/null
+++ b/issues/migrations/0020_remove_objects_with_null_issue.py
@@ -0,0 +1,13 @@
+from django.db import migrations
+def remove_objects_with_null_issue(apps, schema_editor):
+    Grouping = apps.get_model("issues", "Grouping")
+    TurningPoint = apps.get_model("issues", "TurningPoint")
+    Grouping.objects.filter(issue__isnull=True).delete()
+    TurningPoint.objects.filter(issue__isnull=True).delete()
+class Migration(migrations.Migration):
+    dependencies = [
+        ("issues", "0019_alter_grouping_grouping_key_hash"),
+    ]
+    operations = [
+        migrations.RunPython(remove_objects_with_null_issue, reverse_code=migrations.RunPython.noop),
+    ]

--- a//dev/null
+++ b/issues/migrations/0021_alter_do_nothing.py
@@ -0,0 +1,22 @@
+from django.db import migrations, models
+import django.db.models.deletion
+class Migration(migrations.Migration):
+    dependencies = [
+        ("issues", "0020_remove_objects_with_null_issue"),
+    ]
+    operations = [
+        migrations.AlterField(
+            model_name="grouping",
+            name="issue",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="issues.issue"
+            ),
+        ),
+        migrations.AlterField(
+            model_name="turningpoint",
+            name="issue",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="issues.issue"
+            ),
+        ),
+    ]

--- a//dev/null
+++ b/issues/migrations/0022_turningpoint_project.py
@@ -0,0 +1,18 @@
+from django.db import migrations, models
+import django.db.models.deletion
+class Migration(migrations.Migration):
+    dependencies = [
+        ("projects", "0012_project_is_deleted"),
+        ("issues", "0021_alter_do_nothing"),
+    ]
+    operations = [
+        migrations.AddField(
+            model_name="turningpoint",
+            name="project",
+            field=models.ForeignKey(
+                null=True,
+                on_delete=django.db.models.deletion.DO_NOTHING,
+                to="projects.project",
+            ),
+        ),
+    ]

--- a//dev/null
+++ b/issues/migrations/0023_turningpoint_set_project.py
@@ -0,0 +1,13 @@
+from django.db import migrations
+def turningpoint_set_project(apps, schema_editor):
+    TurningPoint = apps.get_model("issues", "TurningPoint")
+    for turningpoint in TurningPoint.objects.all():
+        turningpoint.project = turningpoint.issue.project
+        turningpoint.save(update_fields=["project"])
+class Migration(migrations.Migration):
+    dependencies = [
+        ("issues", "0022_turningpoint_project"),
+    ]
+    operations = [
+        migrations.RunPython(turningpoint_set_project, migrations.RunPython.noop),
+    ]

--- a//dev/null
+++ b/issues/migrations/0024_turningpoint_project_alter_not_null.py
@@ -0,0 +1,16 @@
+from django.db import migrations, models
+import django.db.models.deletion
+class Migration(migrations.Migration):
+    dependencies = [
+        ("projects", "0012_project_is_deleted"),
+        ("issues", "0023_turningpoint_set_project"),
+    ]
+    operations = [
+        migrations.AlterField(
+            model_name="turningpoint",
+            name="project",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="projects.project"
+            ),
+        ),
+    ]

--- a//dev/null
+++ b/issues/migrations/0025_alter_grouping_project_alter_issue_project.py
@@ -0,0 +1,23 @@
+from django.db import migrations, models
+import django.db.models.deletion
+class Migration(migrations.Migration):
+    dependencies = [
+        ("projects", "0014_alter_projectmembership_project"),
+        ("issues", "0024_turningpoint_project_alter_not_null"),
+    ]
+    operations = [
+        migrations.AlterField(
+            model_name="grouping",
+            name="project",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="projects.project"
+            ),
+        ),
+        migrations.AlterField(
+            model_name="issue",
+            name="project",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="projects.project"
+            ),
+        ),
+    ]

--- a//dev/null
+++ b/issues/models.py
@@ -0,0 +1,324 @@
+import json
+import uuid
+from functools import partial
+from django.db import models, transaction
+from django.db.models import F, Value
+from django.db.models.functions import Concat
+from django.template.defaultfilters import date as default_date_filter
+from django.conf import settings
+from django.utils.functional import cached_property
+from bugsink.volume_based_condition import VolumeBasedCondition
+from bugsink.transaction import delay_on_commit
+from alerts.tasks import send_unmute_alert
+from compat.timestamp import parse_timestamp, format_timestamp
+from tags.models import IssueTag, TagValue
+from .utils import (
+    parse_lines, serialize_lines, filter_qs_for_fixed_at, exclude_qs_for_fixed_at,
+    get_title_for_exception_type_and_value)
+from .tasks import delete_issue_deps
+class IncongruentStateException(Exception):
+    pass
+class Issue(models.Model):
+    """
+    An Issue models a group of similar events. In particular: it models the result of both automatic (client-side and
+    server-side) and manual ("merge") grouping.
+    """
+    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
+    project = models.ForeignKey(
+        "projects.Project", blank=False, null=False, on_delete=models.DO_NOTHING)
+    is_deleted = models.BooleanField(default=False)
+    digest_order = models.PositiveIntegerField(blank=False, null=False)
+    last_seen = models.DateTimeField(blank=False, null=False)  # based on event.ingested_at
+    first_seen = models.DateTimeField(blank=False, null=False)  # based on event.ingested_at
+    digested_event_count = models.IntegerField(blank=False, null=False)
+    stored_event_count = models.IntegerField(blank=False, null=False, default=0, editable=False)
+    calculated_type = models.CharField(max_length=128, blank=True, null=False, default="")
+    calculated_value = models.TextField(max_length=1024, blank=True, null=False, default="")
+    transaction = models.CharField(max_length=200, blank=True, null=False, default="")
+    last_frame_filename = models.CharField(max_length=255, blank=True, null=False, default="")
+    last_frame_module = models.CharField(max_length=255, blank=True, null=False, default="")
+    last_frame_function = models.CharField(max_length=255, blank=True, null=False, default="")
+    is_resolved = models.BooleanField(default=False)
+    is_resolved_by_next_release = models.BooleanField(default=False)
+    fixed_at = models.TextField(blank=True, null=False, default='')  # line-separated list
+    events_at = models.TextField(blank=True, null=False, default='')  # line-separated list
+    is_muted = models.BooleanField(default=False)
+    unmute_on_volume_based_conditions = models.TextField(blank=False, null=False, default="[]")  # json string
+    unmute_after = models.DateTimeField(blank=True, null=True)
+    next_unmute_check = models.PositiveIntegerField(null=False, default=0)
+    def save(self, *args, **kwargs):
+        if self.digest_order is None:
+            max_current = self.digest_order = Issue.objects.filter(project=self.project).aggregate(
+                models.Max("digest_order"))["digest_order__max"]
+            self.digest_order = max_current + 1 if max_current is not None else 1
+        super().save(*args, **kwargs)
+    def delete_deferred(self):
+        """Marks the issue as deleted, and schedules deletion of all related objects"""
+        self.is_deleted = True
+        self.save(update_fields=["is_deleted"])
+        self.grouping_set.all().update(grouping_key_hash=None)
+        delay_on_commit(delete_issue_deps, str(self.project_id), str(self.id))
+    def friendly_id(self):
+        return f"{ self.project.slug.upper() }-{ self.digest_order }"
+    def get_absolute_url(self):
+        return f"/issues/issue/{ self.id }/event/last/"
+    def title(self):
+        return get_title_for_exception_type_and_value(self.calculated_type, self.calculated_value)
+    def get_fixed_at(self):
+        return parse_lines(self.fixed_at)
+    def get_events_at(self):
+        return parse_lines(self.events_at)
+    def get_events_at_2(self):
+        return [e for e in self.get_events_at() if e != ""]
+    def add_fixed_at(self, release_version):
+        fixed_at = self.get_fixed_at()
+        if release_version not in fixed_at:
+            fixed_at.append(release_version)
+            self.fixed_at = serialize_lines(fixed_at)
+    def get_unmute_on_volume_based_conditions(self):
+        return [
+            VolumeBasedCondition.from_dict(vbc_s)
+            for vbc_s in json.loads(self.unmute_on_volume_based_conditions)
+        ]
+    def occurs_in_last_release(self):
+        latest_release = self.project.get_latest_release()
+        return latest_release.version in self.events_at
+    def turningpoint_set_all(self):
+        return self.turningpoint_set.all().select_related("user")
+    @cached_property
+    def tags_summary(self):
+        return self._get_issue_tags(4, "...")
+    @cached_property
+    def tags_all(self):
+        return self._get_issue_tags(25, "Other...")
+    def _get_issue_tags(self, other_cutoff, other_label):
+        result = []
+        if self.digested_event_count > other_cutoff:
+            base_qs = self.tags.filter(key__mostly_unique=False)
+        else:
+            base_qs = self.tags
+        ds = base_qs.values("key")\
+            .annotate(count_sum=models.Sum("count"))\
+            .distinct()\
+            .order_by("key__key")
+        for d in ds:
+            issue_tags = [
+                issue_tag
+                for issue_tag in
+                (IssueTag.objects
+                 .filter(issue=self, key=d['key'])  # note: project is implied through issue
+                 .order_by("-count")
+                 .select_related("value", "key")[:other_cutoff + 1]  # +1 to see if we need to add "Other"
+                 )
+            ]
+            total_seen = d["count_sum"]
+            seen_till_now = 0
+            if len(issue_tags) > other_cutoff:
+                issue_tags = issue_tags[:other_cutoff - 1]  # cut off one more to make room for "Other"
+            for i, issue_tag in enumerate(issue_tags):
+                issue_tag.pct = int(issue_tag.count / total_seen * 100)
+                seen_till_now += issue_tag.count
+            if seen_till_now < total_seen:
+                issue_tags.append({
+                    "value": TagValue(value=other_label),
+                    "count": total_seen - seen_till_now,
+                    "pct": int((total_seen - seen_till_now) / total_seen * 100),
+                })
+            result.append(issue_tags)
+        return result
+    class Meta:
+        unique_together = [
+            ("project", "digest_order"),
+        ]
+        indexes = [
+            models.Index(fields=["project", "is_resolved", "is_muted", "last_seen"], name="issue_list_open"),
+            models.Index(fields=["project", "is_muted", "last_seen"], name="issue_list_muted"),
+            models.Index(fields=["project", "is_resolved", "last_seen"], name="issue_list_resolved"),  # and unresolved
+            models.Index(fields=["project", "last_seen"], name="issue_list_all"),  # all
+        ]
+class Grouping(models.Model):
+    """
+    Grouping models the automatic part of Events should be grouped into Issues. In particular: an automatically
+    calculated grouping key (from the event data, with a key role for the SDK-side fingerprint).
+    They are separated out into a separate model to allow for manually merging (after the fact) multiple such groupings
+    into a single issue. (such manual merging is not yet implemented, but the data-model is already prepared for it)
+    """
+    project = models.ForeignKey(
+        "projects.Project", blank=False, null=False, on_delete=models.DO_NOTHING)
+    grouping_key = models.TextField(blank=False, null=False)
+    grouping_key_hash = models.CharField(max_length=64, blank=False, null=True)
+    issue = models.ForeignKey("Issue", blank=False, null=False, on_delete=models.DO_NOTHING)
+    def __str__(self):
+        return self.grouping_key
+    class Meta:
+        unique_together = [
+            ("project", "grouping_key_hash"),
+        ]
+def format_unmute_reason(unmute_metadata):
+    if "mute_until" in unmute_metadata:
+        d = unmute_metadata["mute_until"]
+        plural_s = "" if d["nr_of_periods"] == 1 else "s"
+        return f"More than { d['volume'] } events per { d['nr_of_periods'] } { d['period'] }{ plural_s } occurred, "\
+               f"unmuting the issue."
+    d = unmute_metadata["mute_for"]
+    formatted_date = default_date_filter(d['unmute_after'], 'j M G:i')
+    return f"An event was observed after the mute-deadline of { formatted_date } and the issue was unmuted."
+class IssueStateManager(object):
+    """basically: a namespace; with static methods that combine field-setting in a single place"""
+    @staticmethod
+    def resolve(issue):
+        issue.is_resolved = True
+        issue.add_fixed_at("")  # i.e. fixed in the no-release-info-available release
+        IssueStateManager.unmute(issue)
+    @staticmethod
+    def resolve_by_latest(issue):
+        issue.is_resolved = True
+        issue.add_fixed_at(issue.project.get_latest_release().version)
+        IssueStateManager.unmute(issue)  # as in IssueStateManager.resolve()
+    @staticmethod
+    def resolve_by_release(issue, release_version):
+        issue.is_resolved = True
+        issue.add_fixed_at(release_version)
+        IssueStateManager.unmute(issue)  # as in IssueStateManager.resolve()
+    @staticmethod
+    def resolve_by_next(issue):
+        issue.is_resolved = True
+        issue.is_resolved_by_next_release = True
+        IssueStateManager.unmute(issue)  # as in IssueStateManager.resolve()
+    @staticmethod
+    def reopen(issue):
+        issue.is_resolved = False
+        IssueStateManager.unmute(issue)
+    @staticmethod
+    def mute(issue, unmute_on_volume_based_conditions="[]", unmute_after=None):
+        if issue.is_resolved:
+            raise IncongruentStateException("Cannot mute a resolved issue")
+        issue.is_muted = True
+        issue.unmute_on_volume_based_conditions = unmute_on_volume_based_conditions
+        issue.next_unmute_check = 0
+        if unmute_after is not None:
+            issue.unmute_after = unmute_after
+    @staticmethod
+    def unmute(issue, triggering_event=None, unmute_metadata=None):
+        if issue.is_muted:
+            issue.is_muted = False
+            issue.unmute_on_volume_based_conditions = "[]"
+            issue.unmute_after = None
+            if triggering_event is not None:
+                if issue.project.alert_on_unmute:
+                    transaction.on_commit(partial(
+                        send_unmute_alert.delay,
+                        str(issue.id), format_unmute_reason(unmute_metadata)))
+                if unmute_metadata is not None and "mute_for" in unmute_metadata:
+                    unmute_metadata["mute_for"]["unmute_after"] = \
+                        format_timestamp(unmute_metadata["mute_for"]["unmute_after"])
+                TurningPoint.objects.create(
+                    project_id=issue.project_id,
+                    issue=issue, triggering_event=triggering_event, timestamp=triggering_event.ingested_at,
+                    kind=TurningPointKind.UNMUTED, metadata=json.dumps(unmute_metadata))
+                triggering_event.never_evict = True  # .save() will be called by the caller of this function
+    @staticmethod
+    def delete(issue):
+        issue.delete_deferred()
+    @staticmethod
+    def get_unmute_thresholds(issue):
+        unmute_vbcs = [
+            VolumeBasedCondition.from_dict(vbc_s)
+            for vbc_s in json.loads(issue.unmute_on_volume_based_conditions)
+        ]
+        return [(vbc.period, vbc.nr_of_periods, vbc.volume) for vbc in unmute_vbcs]
+class IssueQuerysetStateManager(object):
+    """
+    This is exaclty the same as IssueStateManager, but it works on querysets instead of single objects.
+    The reason we do this as a copy/pasta (and not by just passing a queryset with a single element) is twofold:
+    * the qs-approach is harder to comprehend; understanding can be aided by referring back to the simple approach
+    * performance: the qs-approach may take a few queries to deal with a whole set; but when working on a single object
+        a single .save() is enough.
+    """
+    def _resolve_at(issue_qs, release_version):
+        filter_qs_for_fixed_at(issue_qs, release_version).update(
+            is_resolved=True,
+        )
+        exclude_qs_for_fixed_at(issue_qs, "").update(
+            is_resolved=True,
+            fixed_at=Concat(F("fixed_at"), Value(release_version + "\n")),
+        )
+        issue_qs.update(
+            fixed_at=Concat(F("fixed_at"), Value(release_version + "\n")),
+        )
+    @staticmethod
+    def resolve(issue_qs):
+        IssueQuerysetStateManager._resolve_at(issue_qs, "")  # i.e. fixed in the no-release-info-available release
+        IssueQuerysetStateManager.unmute(issue_qs)
+    @staticmethod
+    def resolve_by_latest(issue_qs):
+        raise NotImplementedError("resolve_by_latest is not implemented - see comments above")
+    @staticmethod
+    def resolve_by_release(issue_qs, release_version):
+        IssueQuerysetStateManager._resolve_at(issue_qs, release_version)
+        IssueQuerysetStateManager.unmute(issue_qs)  # as in IssueQuerysetStateManager.resolve()
+    @staticmethod
+    def resolve_by_next(issue_qs):
+        issue_qs.update(
+            is_resolved=True,
+            is_resolved_by_next_release=True,
+        )
+        IssueQuerysetStateManager.unmute(issue_qs)  # as in IssueQuerysetStateManager.resolve()
+    @staticmethod
+    def reopen(issue_qs):
+        raise NotImplementedError("reopen is not implemented - see comments above")
+    @staticmethod
+    def mute(issue_qs, unmute_on_volume_based_conditions="[]", unmute_after=None):
+        if issue_qs.filter(is_resolved=True).exists():
+            raise IncongruentStateException("Cannot mute a resolved issue")
+        issue_qs.update(
+            is_muted=True,
+            unmute_on_volume_based_conditions=unmute_on_volume_based_conditions,
+            next_unmute_check=0,
+        )
+        if unmute_after is not None:
+            issue_qs.update(unmute_after=unmute_after)
+    @staticmethod
+    def unmute(issue_qs, triggering_event=None):
+        issue_qs.update(
+            is_muted=False,
+            unmute_on_volume_based_conditions="[]",
+            unmute_after=None,
+        )
+        assert triggering_event is None, "this method can only be called from the UI, i.e. user-not-event-triggered"
+        for issue in issue_qs:
+            IssueStateManager.unmute(issue, triggering_event)
+    @staticmethod
+    def delete(issue_qs):
+        for issue in issue_qs:
+            issue.delete_deferred()
+class TurningPointKind(models.IntegerChoices):
+    FIRST_SEEN = 1, "First seen"
+    RESOLVED = 2, "Resolved"
+    MUTED = 3, "Muted"
+    REGRESSED = 4, "Marked as regressed"
+    UNMUTED = 5, "Unmuted"
+    NEXT_MATERIALIZED = 10, "Release info added"
+    MANUAL_ANNOTATION = 100, "Manual annotation"
+class TurningPoint(models.Model):
+    """A TurningPoint models a point in time in the history of an issue."""
+    project = models.ForeignKey("projects.Project", blank=False, null=False, on_delete=models.DO_NOTHING)
+    issue = models.ForeignKey("Issue", blank=False, null=False, on_delete=models.DO_NOTHING)
+    triggering_event = models.ForeignKey("events.Event", blank=True, null=True, on_delete=models.DO_NOTHING)
+    user = models.ForeignKey(settings.AUTH_USER_MODEL, blank=True, null=True, on_delete=models.SET_NULL)
+    timestamp = models.DateTimeField(blank=False, null=False)  # this info is also in the event, but event is nullable
+    kind = models.IntegerField(blank=False, null=False, choices=TurningPointKind.choices)
+    metadata = models.TextField(blank=False, null=False, default="{}")  # json string
+    comment = models.TextField(blank=True, null=False, default="")
+    class Meta:
+        ordering = ["-timestamp", "-id"]
+        indexes = [
+            models.Index(fields=["timestamp"]),
+        ]
+    def parsed_metadata(self):
+        if not hasattr(self, "_parsed_metadata"):
+            self._parsed_metadata = json.loads(self.metadata)
+            if "mute_for" in self._parsed_metadata and "unmute_after" in self._parsed_metadata["mute_for"]:
+                self._parsed_metadata["mute_for"]["unmute_after"] = \
+                    parse_timestamp(self._parsed_metadata["mute_for"]["unmute_after"])
+        return self._parsed_metadata

--- a//dev/null
+++ b/issues/tasks.py
@@ -0,0 +1,59 @@
+from snappea.decorators import shared_task
+from bugsink.utils import get_model_topography, delete_deps_with_budget
+from bugsink.transaction import immediate_atomic, delay_on_commit
+def get_model_topography_with_issue_override():
+    """
+    Returns the model topography with ordering adjusted to prefer deletions via .issue, when available.
+    This assumes that Issue is not only the root of the dependency graph, but also that if a model has an .issue
+    ForeignKey, deleting it via that path is sufficient, meaning we can safely avoid visiting the same model again
+    through other ForeignKey routes (e.g. Event.grouping or TurningPoint.triggering_event).
+    The preference is encoded via an explicit list of models, which are visited early and only via their .issue path.
+    """
+    from issues.models import TurningPoint, Grouping
+    from events.models import Event
+    from tags.models import IssueTag, EventTag
+    preferred = [
+        TurningPoint,  # above Event, to avoid deletions via .triggering_event
+        EventTag,      # above Event, to avoid deletions via .event
+        Event,         # above Grouping, to avoid deletions via .grouping
+        Grouping,
+        IssueTag,
+    ]
+    def as_preferred(lst):
+        """
+        Sorts the list of (model, fk_name) tuples such that the models are in the preferred order as indicated above,
+        and models which occur with another fk_name are pruned
+        """
+        return sorted(
+            [(model, fk_name) for model, fk_name in lst if fk_name == "issue" or model not in preferred],
+            key=lambda x: preferred.index(x[0]) if x[0] in preferred else len(preferred),
+        )
+    topo = get_model_topography()
+    for k, lst in topo.items():
+        topo[k] = as_preferred(lst)
+    return topo
+@shared_task
+def delete_issue_deps(project_id, issue_id):
+    from .models import Issue   # avoid circular import
+    with immediate_atomic():
+        budget = 500
+        num_deleted = 0
+        dep_graph = get_model_topography_with_issue_override()
+        for model_for_recursion, fk_name_for_recursion in dep_graph["issues.Issue"]:
+            this_num_deleted = delete_deps_with_budget(
+                project_id,
+                model_for_recursion,
+                fk_name_for_recursion,
+                [issue_id],
+                budget - num_deleted,
+                dep_graph,
+                is_for_project=False,
+            )
+            num_deleted += this_num_deleted
+            if num_deleted >= budget:
+                delay_on_commit(delete_issue_deps, project_id, issue_id)
+                return
+        if budget - num_deleted <= 0:
+            delay_on_commit(delete_issue_deps, project_id, issue_id)
+        else:
+            Issue.objects.filter(pk=issue_id).delete()

--- a//dev/null
+++ b/issues/urls.py
@@ -0,0 +1,52 @@
+from django.urls import path, register_converter
+from .views import (
+    issue_list, issue_event_stacktrace, issue_event_details, issue_event_list, issue_history, issue_grouping,
+    issue_event_breadcrumbs, event_by_internal_id, history_comment_new, history_comment_edit, history_comment_delete,
+    issue_tags)
+def regex_converter(passed_regex):
+    class RegexConverter:
+        regex = passed_regex
+        def to_python(self, value):
+            return value
+        def to_url(self, value):
+            return value
+    return RegexConverter
+register_converter(regex_converter("(first|last)"), "first-last")
+register_converter(regex_converter("(prev|next)"), "prev-next")
+urlpatterns = [
+    path('<int:project_pk>/', issue_list, {"state_filter": "open"}, name="issue_list_open"),
+    path('<int:project_pk>/unresolved', issue_list, {"state_filter": "unresolved"}, name="issue_list_unresolved"),
+    path('<int:project_pk>/resolved/', issue_list, {"state_filter": "resolved"}, name="issue_list_resolved"),
+    path('<int:project_pk>/muted/', issue_list, {"state_filter": "muted"}, name="issue_list_muted"),
+    path('<int:project_pk>/all/', issue_list, {"state_filter": "all"}, name="issue_list_all"),
+    path('issue/<uuid:issue_pk>/event/<uuid:event_pk>/', issue_event_stacktrace, name="event_stacktrace"),
+    path('issue/<uuid:issue_pk>/event/<uuid:event_pk>/details/', issue_event_details, name="event_details"),
+    path('issue/<uuid:issue_pk>/event/<uuid:event_pk>/breadcrumbs/', issue_event_breadcrumbs, name="event_breadcrumbs"),
+    path('issue/<uuid:issue_pk>/event/<int:digest_order>/', issue_event_stacktrace, name="event_stacktrace"),
+    path('issue/<uuid:issue_pk>/event/<int:digest_order>/details/', issue_event_details, name="event_details"),
+    path('issue/<uuid:issue_pk>/event/<int:digest_order>/breadcrumbs/', issue_event_breadcrumbs,
+         name="event_breadcrumbs"),
+    path('issue/<uuid:issue_pk>/event/<int:digest_order>/', issue_event_stacktrace, name="event_stacktrace"),
+    path('issue/<uuid:issue_pk>/event/<int:digest_order>/details/', issue_event_details, name="event_details"),
+    path('issue/<uuid:issue_pk>/event/<int:digest_order>/breadcrumbs/', issue_event_breadcrumbs,
+         name="event_breadcrumbs"),
+    path('issue/<uuid:issue_pk>/event/<int:digest_order>/<prev-next:nav>/', issue_event_stacktrace,
+         name="event_stacktrace"),
+    path('issue/<uuid:issue_pk>/event/<int:digest_order>/<prev-next:nav>/details/', issue_event_details,
+         name="event_details"),
+    path('issue/<uuid:issue_pk>/event/<int:digest_order>/<prev-next:nav>/breadcrumbs/', issue_event_breadcrumbs,
+         name="event_breadcrumbs"),
+    path('issue/<uuid:issue_pk>/event/<first-last:nav>/', issue_event_stacktrace, name="event_stacktrace"),
+    path('issue/<uuid:issue_pk>/event/<first-last:nav>/details/', issue_event_details, name="event_details"),
+    path(
+        'issue/<uuid:issue_pk>/event/<first-last:nav>/breadcrumbs/', issue_event_breadcrumbs, name="event_breadcrumbs"),
+    path('issue/<uuid:issue_pk>/tags/', issue_tags),
+    path('issue/<uuid:issue_pk>/history/', issue_history),
+    path('issue/<uuid:issue_pk>/grouping/', issue_grouping),
+    path('issue/<uuid:issue_pk>/events/', issue_event_list),
+    path('event/<uuid:event_pk>/', event_by_internal_id, name="event_by_internal_id"),
+    path('issue/<uuid:issue_pk>/history/comment/', history_comment_new, name="history_comment_new"),
+    path('issue/<uuid:issue_pk>/history/comment/<int:comment_pk>/', history_comment_edit, name="history_comment_edit"),
+    path('issue/<uuid:issue_pk>/history/comment/<int:comment_pk>/delete/', history_comment_delete,
+         name="history_comment_delete"),
+]

--- a//dev/null
+++ b/issues/views.py
@@ -0,0 +1,571 @@
+from collections import namedtuple
+import json
+import sentry_sdk
+import logging
+from django.db.models import Q
+from django.utils import timezone
+from django.shortcuts import render, get_object_or_404, redirect
+from django.http import HttpResponseRedirect, HttpResponseNotAllowed
+from django.utils.safestring import mark_safe
+from django.template.defaultfilters import date
+from django.urls import reverse
+from django.core.exceptions import PermissionDenied
+from django.http import Http404
+from django.core.paginator import Paginator, Page
+from django.db.utils import OperationalError
+from django.conf import settings
+from django.utils.functional import cached_property
+from sentry.utils.safe import get_path
+from sentry_sdk_extensions import capture_or_log_exception
+from bugsink.decorators import project_membership_required, issue_membership_required, atomic_for_request_method
+from bugsink.transaction import durable_atomic
+from bugsink.period_utils import add_periods_to_datetime
+from bugsink.timed_sqlite_backend.base import different_runtime_limit
+from events.models import Event
+from events.ua_stuff import get_contexts_enriched_with_ua
+from compat.timestamp import format_timestamp
+from projects.models import ProjectMembership
+from tags.search import search_issues, search_events, search_events_optimized
+from .models import Issue, IssueQuerysetStateManager, IssueStateManager, TurningPoint, TurningPointKind
+from .forms import CommentForm
+from .utils import get_values, get_main_exception
+from events.utils import annotate_with_meta, apply_sourcemaps
+logger = logging.getLogger("bugsink.issues")
+MuteOption = namedtuple("MuteOption", ["for_or_until", "period_name", "nr_of_periods", "gte_threshold"])
+GLOBAL_MUTE_OPTIONS = [
+    MuteOption("for", "day", 1, None),
+    MuteOption("for", "week", 1, None),
+    MuteOption("for", "month", 1, None),
+    MuteOption("for", "month", 3, None),
+    MuteOption("until", "hour", 1, 5),
+    MuteOption("until", "hour", 24, 5),
+    MuteOption("until", "hour", 24, 100),
+]
+class EagerPaginator(Paginator):
+    def _get_page(self, *args, **kwargs):
+        object_list = args[0]
+        object_list = list(object_list)
+        return Page(object_list, *(args[1:]), **kwargs)
+class KnownCountPaginator(EagerPaginator):
+    """optimization: we know the total count of the queryset, so we can avoid a count() query"""
+    def __init__(self, *args, **kwargs):
+        self._count = kwargs.pop("count")
+        super().__init__(*args, **kwargs)
+    @property
+    def count(self):
+        return self._count
+class UncountablePage(Page):
+    """The Page subclass to be used with UncountablePaginator."""
+    @cached_property
+    def has_next(self):
+        return len(self.object_list) == self.paginator.per_page
+    @cached_property
+    def end_index(self):
+        return (self.paginator.per_page * (self.number - 1)) + len(self.object_list)
+class UncountablePaginator(EagerPaginator):
+    """optimization: counting is too expensive; to be used in a template w/o .count and .last"""
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+    def _get_page(self, *args, **kwargs):
+        object_list = args[0]
+        object_list = list(object_list)
+        return UncountablePage(object_list, *(args[1:]), **kwargs)
+    @property
+    def count(self):
+        return 1_000_000_000  # big enough to be bigger than what you can click through or store in the DB.
+def _request_repr(parsed_data):
+    if "request" not in parsed_data:
+        return ""
+    return parsed_data["request"].get("method", "") + " " + parsed_data["request"].get("url", "")
+def _is_valid_action(action, issue):
+    """We take the 'strict' approach of complaining even when the action is simply a no-op, because you're already in
+    the desired state."""
+    if action == "delete":
+        return True
+    if issue.is_resolved:
+        return False
+    if action.startswith("resolved_release:"):
+        release_version = action.split(":", 1)[1]
+        if release_version + "\n" in issue.events_at:
+            return False
+    elif action.startswith("mute"):
+        if issue.is_muted:
+            return False
+    elif action == "unmute":
+        if not issue.is_muted:
+            return False
+    return True
+def _q_for_invalid_for_action(action):
+    """returns a Q obj of issues for which the action is not valid."""
+    if action == "delete":
+        return Q(pk__in=[])
+    illegal_conditions = Q(is_resolved=True)  # any action is illegal on resolved issues (as per our current UI)
+    if action.startswith("resolved_release:"):
+        release_version = action.split(":", 1)[1]
+        illegal_conditions = illegal_conditions | Q(events_at__contains=release_version + "\n")
+    elif action.startswith("mute"):
+        illegal_conditions = illegal_conditions | Q(is_muted=True)
+    elif action == "unmute":
+        illegal_conditions = illegal_conditions | Q(is_muted=False)
+    return illegal_conditions
+def _make_history(issue_or_qs, action, user):
+    if action == "delete":
+        return  # we're about to delete the issue, so no history is needed (nor possible)
+    elif action == "resolve":
+        kind = TurningPointKind.RESOLVED
+    elif action.startswith("resolved"):
+        kind = TurningPointKind.RESOLVED
+    elif action.startswith("mute"):
+        kind = TurningPointKind.MUTED
+    elif action == "unmute":
+        kind = TurningPointKind.UNMUTED
+    else:
+        raise ValueError(f"unknown action: {action}")
+    if action.startswith("mute_for:"):
+        mute_for_params = action.split(":", 1)[1]
+        period_name, nr_of_periods, _ = mute_for_params.split(",")
+        unmute_after = add_periods_to_datetime(timezone.now(), int(nr_of_periods), period_name)
+        metadata = {"mute_for": {
+            "period_name": period_name, "nr_of_periods": int(nr_of_periods),
+            "unmute_after": format_timestamp(unmute_after)}}
+    elif action.startswith("mute_until:"):
+        mute_for_params = action.split(":", 1)[1]
+        period_name, nr_of_periods, gte_threshold = mute_for_params.split(",")
+        metadata = {"mute_until": {
+            "period_name": period_name, "nr_of_periods": int(nr_of_periods), "gte_threshold": gte_threshold}}
+    elif action == "mute":
+        metadata = {"mute_unconditionally": True}
+    elif action.startswith("resolved_release:"):
+        release_version = action.split(":", 1)[1]
+        metadata = {"resolved_release": release_version}
+    elif action == "resolved_next":
+        metadata = {"resolve_by_next": True}
+    elif action == "resolve":
+        metadata = {"resolved_unconditionally": True}
+    else:
+        metadata = {}
+    now = timezone.now()
+    if isinstance(issue_or_qs, Issue):
+        TurningPoint.objects.create(
+            project=issue_or_qs.project,
+            issue=issue_or_qs, kind=kind, user=user, metadata=json.dumps(metadata), timestamp=now)
+    else:
+        TurningPoint.objects.bulk_create([
+            TurningPoint(
+                project_id=issue.project_id, issue=issue, kind=kind, user=user, metadata=json.dumps(metadata),
+                timestamp=now)
+            for issue in issue_or_qs
+        ])
+def _apply_action(manager, issue_or_qs, action, user):
+    _make_history(issue_or_qs, action, user)
+    if action == "resolve":
+        manager.resolve(issue_or_qs)
+    elif action.startswith("resolved_release:"):
+        release_version = action.split(":", 1)[1]
+        manager.resolve_by_release(issue_or_qs, release_version)
+    elif action == "resolved_next":
+        manager.resolve_by_next(issue_or_qs)
+    elif action == "mute":
+        manager.mute(issue_or_qs)
+    elif action.startswith("mute_for:"):
+        mute_for_params = action.split(":", 1)[1]
+        period_name, nr_of_periods, _ = mute_for_params.split(",")
+        unmute_after = add_periods_to_datetime(timezone.now(), int(nr_of_periods), period_name)
+        manager.mute(issue_or_qs, unmute_after=unmute_after)
+    elif action.startswith("mute_until:"):
+        mute_for_params = action.split(":", 1)[1]
+        period_name, nr_of_periods, gte_threshold = mute_for_params.split(",")
+        manager.mute(issue_or_qs, unmute_on_volume_based_conditions=json.dumps([{
+            "period": period_name,
+            "nr_of_periods": int(nr_of_periods),
+            "volume": int(gte_threshold),
+        }]))
+    elif action == "unmute":
+        manager.unmute(issue_or_qs)
+    elif action == "delete":
+        manager.delete(issue_or_qs)
+def issue_list(request, project_pk, state_filter="open"):
+    project, unapplied_issue_ids = _issue_list_pt_1(request, project_pk=project_pk, state_filter=state_filter)
+    with durable_atomic():
+        return _issue_list_pt_2(request, project, state_filter, unapplied_issue_ids)
+@atomic_for_request_method
+@project_membership_required
+def _issue_list_pt_1(request, project, state_filter="open"):
+    if request.method == "POST":
+        issue_ids = request.POST.getlist('issue_ids[]')
+        issue_qs = Issue.objects.filter(pk__in=issue_ids)
+        illegal_conditions = _q_for_invalid_for_action(request.POST["action"])
+        unapplied_issue_ids = list(issue_qs.filter(illegal_conditions).values_list("id", flat=True))
+        _apply_action(
+            IssueQuerysetStateManager, issue_qs.exclude(illegal_conditions), request.POST["action"], request.user)
+    else:
+        unapplied_issue_ids = None
+    return project, unapplied_issue_ids
+def _issue_list_pt_2(request, project, state_filter, unapplied_issue_ids):
+    d_state_filter = {
+        "open": lambda qs: qs.filter(is_resolved=False, is_muted=False),
+        "unresolved": lambda qs: qs.filter(is_resolved=False),
+        "resolved": lambda qs: qs.filter(is_resolved=True),
+        "muted": lambda qs: qs.filter(is_muted=True),
+        "all": lambda qs: qs,
+    }
+    issue_list = d_state_filter[state_filter](
+        Issue.objects.filter(project=project, is_deleted=False)
+    ).order_by("-last_seen")
+    if request.GET.get("q"):
+        issue_list = search_issues(project, issue_list, request.GET["q"])
+    paginator = UncountablePaginator(issue_list, 250)
+    page_number = request.GET.get("page")
+    page_obj = paginator.get_page(page_number)
+    try:
+        member = ProjectMembership.objects.get(project=project, user=request.user)
+    except ProjectMembership.DoesNotExist:
+        member = None  # this can happen if the user is superuser (as per `project_membership_required` decorator)
+    return render(request, "issues/issue_list.html", {
+        "project": project,
+        "member": member,
+        "state_filter": state_filter,
+        "mute_options": GLOBAL_MUTE_OPTIONS,
+        "unapplied_issue_ids": unapplied_issue_ids,
+        "disable_resolve_buttons": state_filter in ("resolved"),
+        "disable_mute_buttons": state_filter in ("resolved", "muted"),
+        "disable_unmute_buttons": state_filter in ("resolved", "open"),
+        "q": request.GET.get("q", ""),
+        "page_obj": page_obj,
+    })
+def event_by_internal_id(request, event_pk):
+    event = get_object_or_404(Event, id=event_pk)
+    issue = event.issue
+    return redirect(issue_event_stacktrace, issue_pk=issue.pk, event_pk=event.pk)
+def _handle_post(request, issue):
+    if _is_valid_action(request.POST["action"], issue):
+        _apply_action(IssueStateManager, issue, request.POST["action"], request.user)
+        issue.save()
+    return HttpResponseRedirect(request.path_info)
+def _get_event(qs, issue, event_pk, digest_order, nav, bounds):
+    """
+    Returns the event using the "url lookup".
+    The passed qs is "something you can use to deduce digest_order (for next/prev)."
+    When a direct (non-nav) method is used, we do _not_ check against existence in qs; this is more performant, and it's
+    not clear that being pedantic in this case is actually more valuable from a UX perspective.
+    """
+    if nav is not None:
+        if nav not in ["first", "last", "prev", "next"]:
+            raise Http404("Cannot look up with '%s'" % nav)
+        if nav == "first":
+            digest_order = bounds[0]
+        elif nav == "last":
+            digest_order = bounds[1]
+        elif nav in ["prev", "next"]:
+            if digest_order is None:
+                raise Http404("Cannot look up with '%s' without digest_order" % nav)
+            if nav == "prev":
+                digest_order = qs.filter(digest_order__lt=digest_order).values_list("digest_order", flat=True)\
+                    .order_by("-digest_order").first()
+            elif nav == "next":
+                digest_order = qs.filter(digest_order__gt=digest_order).values_list("digest_order", flat=True)\
+                    .order_by("digest_order").first()
+        if digest_order is None:
+            raise Event.DoesNotExist
+        return Event.objects.get(issue=issue, digest_order=digest_order)
+    elif event_pk is not None:
+        try:
+            return Event.objects.get(pk=event_pk)
+        except Event.DoesNotExist:
+            return Event.objects.get(event_id=event_pk)
+    elif digest_order is not None:
+        return Event.objects.get(digest_order=digest_order)
+    else:
+        raise Http404("Either event_pk, nav, or digest_order must be provided")
+def _event_count(request, issue, event_x_qs):
+    with different_runtime_limit(0.1):
+        try:
+            return event_x_qs.count() if request.GET.get("q") else issue.stored_event_count
+        except OperationalError as e:
+            if e.args[0] != "interrupted":
+                raise
+            return "many"
+@atomic_for_request_method
+@issue_membership_required
+def issue_event_stacktrace(request, issue, event_pk=None, digest_order=None, nav=None):
+    if request.method == "POST":
+        return _handle_post(request, issue)
+    event_x_qs = search_events_optimized(issue.project, issue, request.GET.get("q", ""))
+    first_do, last_do = _first_last(event_x_qs)
+    try:
+        event = _get_event(event_x_qs, issue, event_pk, digest_order, nav, (first_do, last_do))
+    except Event.DoesNotExist:
+        return issue_event_404(request, issue, event_x_qs, "stacktrace", "event_stacktrace")
+    parsed_data = event.get_parsed_data()
+    exceptions = get_values(parsed_data["exception"]) if "exception" in parsed_data else None
+    try:
+        meta_values = get_values(parsed_data.get("_meta", {}).get("exception", {"values": {}}))
+        annotate_with_meta(exceptions, meta_values)
+    except Exception as e:
+        sentry_sdk.capture_exception(e)
+    try:
+        apply_sourcemaps(parsed_data)
+    except Exception as e:
+        if settings.DEBUG or settings.I_AM_RUNNING == "TEST":
+            raise
+        capture_or_log_exception(e, logger)
+    stack_of_plates = event.platform != "python"  # Python is the only platform that has chronological stacktraces
+    if exceptions is not None and len(exceptions) > 0:
+        if exceptions[-1].get('stacktrace') and exceptions[-1]['stacktrace'].get('frames'):
+            exceptions[-1]['stacktrace']['frames'][-1]['raise_point'] = True
+        if stack_of_plates:
+            exceptions = [e for e in reversed(exceptions)]
+            for exception in exceptions:
+                if not exception.get('stacktrace'):
+                    continue
+                exception['stacktrace']['frames'] = [f for f in reversed(exception['stacktrace']['frames'])]
+    return render(request, "issues/stacktrace.html", {
+        "tab": "stacktrace",
+        "this_view": "event_stacktrace",
+        "project": issue.project,
+        "issue": issue,
+        "event": event,
+        "is_event_page": True,
+        "parsed_data": parsed_data,
+        "request_repr": _request_repr(parsed_data),
+        "exceptions": exceptions,
+        "stack_of_plates": stack_of_plates,
+        "mute_options": GLOBAL_MUTE_OPTIONS,
+        "q": request.GET.get("q", ""),
+        "event_qs_count": _event_count(request, issue, event_x_qs) if request.GET.get("q") else None,
+        "has_prev": event.digest_order > first_do,
+        "has_next": event.digest_order < last_do,
+    })
+def issue_event_404(request, issue, event_x_qs, tab, this_view):
+    """If the Event is 404, but the issue is not, we can still show the issue page; we show a message for the event"""
+    return render(request, "issues/event_404.html", {
+        "tab": tab,
+        "this_view": this_view,
+        "project": issue.project,
+        "issue": issue,
+        "is_event_page": False,  # this variable is used to denote "we have event-related info", which we don't
+        "mute_options": GLOBAL_MUTE_OPTIONS,
+        "q": request.GET.get("q", ""),
+        "event_qs_count": _event_count(request, issue, event_x_qs),
+    })
+@atomic_for_request_method
+@issue_membership_required
+def issue_event_breadcrumbs(request, issue, event_pk=None, digest_order=None, nav=None):
+    if request.method == "POST":
+        return _handle_post(request, issue)
+    event_x_qs = search_events_optimized(issue.project, issue, request.GET.get("q", ""))
+    first_do, last_do = _first_last(event_x_qs)
+    try:
+        event = _get_event(event_x_qs, issue, event_pk, digest_order, nav, (first_do, last_do))
+    except Event.DoesNotExist:
+        return issue_event_404(request, issue, event_x_qs, "breadcrumbs", "event_breadcrumbs")
+    parsed_data = event.get_parsed_data()
+    return render(request, "issues/breadcrumbs.html", {
+        "tab": "breadcrumbs",
+        "this_view": "event_breadcrumbs",
+        "project": issue.project,
+        "issue": issue,
+        "event": event,
+        "is_event_page": True,
+        "request_repr": _request_repr(parsed_data),
+        "breadcrumbs": get_values(parsed_data.get("breadcrumbs")),
+        "mute_options": GLOBAL_MUTE_OPTIONS,
+        "q": request.GET.get("q", ""),
+        "event_qs_count": _event_count(request, issue, event_x_qs) if request.GET.get("q") else None,
+        "has_prev": event.digest_order > first_do,
+        "has_next": event.digest_order < last_do,
+    })
+def _date_with_milis_html(timestamp):
+    return mark_safe(
+        date(timestamp, "j M G:i:s") + "." +
+        '<span class="text-xs">' + date(timestamp, "u")[:3] + '</span>')
+def _first_last(qs_with_digest_order):
+    first = qs_with_digest_order.order_by("digest_order").values_list("digest_order", flat=True).first()
+    last = qs_with_digest_order.order_by("-digest_order").values_list("digest_order", flat=True).first()
+    return first, last
+@atomic_for_request_method
+@issue_membership_required
+def issue_event_details(request, issue, event_pk=None, digest_order=None, nav=None):
+    if request.method == "POST":
+        return _handle_post(request, issue)
+    event_x_qs = search_events_optimized(issue.project, issue, request.GET.get("q", ""))
+    first_do, last_do = _first_last(event_x_qs)
+    try:
+        event = _get_event(event_x_qs, issue, event_pk, digest_order, nav, (first_do, last_do))
+    except Event.DoesNotExist:
+        return issue_event_404(request, issue, event_x_qs, "event-details", "event_details")
+    parsed_data = event.get_parsed_data()
+    key_info = [
+        ("title", event.title()),
+        ("transaction", issue.transaction),
+        ("event_id", event.event_id),
+        ("bugsink_internal_id", event.id),
+    ]
+    if get_path(get_main_exception(parsed_data), "mechanism", "handled") is not None:
+        key_info += [
+            ("handled", get_path(get_main_exception(parsed_data), "mechanism", "handled")),
+        ]
+    key_info += [
+        ("mechanism", get_path(get_main_exception(parsed_data), "mechanism", "type")),
+        ("issue_id", issue.id),
+        ("timestamp", _date_with_milis_html(event.timestamp)),
+        ("ingested at", _date_with_milis_html(event.ingested_at)),
+        ("digested at", _date_with_milis_html(event.digested_at)),
+        ("digest order", event.digest_order),
+    ]
+    logentry_info = []
+    if parsed_data.get("logger") or parsed_data.get("logentry") or parsed_data.get("message"):
+        if "level" in parsed_data:
+            logentry_info.append(("level", parsed_data["level"]))
+        if parsed_data.get("logger"):
+            logentry_info.append(("logger", parsed_data["logger"]))
+        logentry_key = "logentry" if "logentry" in parsed_data else "message"
+        if isinstance(parsed_data.get(logentry_key), dict):
+            if parsed_data.get(logentry_key, {}).get("formatted"):
+                logentry_info.append(("formatted", parsed_data[logentry_key]["formatted"]))
+            if parsed_data.get(logentry_key, {}).get("message"):
+                logentry_info.append(("message", parsed_data[logentry_key]["message"]))
+            params = parsed_data.get(logentry_key, {}).get("params", {})
+            if isinstance(params, list):
+                for param_i, param_v in enumerate(params):
+                    logentry_info.append(("#%s" % param_i, param_v))
+            elif isinstance(params, dict):
+                for param_k, param_v in params.items():
+                    logentry_info.append((param_k, param_v))
+        elif isinstance(parsed_data.get(logentry_key), str):  # robust for top-level as str (see #55)
+            logentry_info.append(("message", parsed_data[logentry_key]))
+    key_info += [
+        ("grouping key", event.grouping.grouping_key),
+    ]
+    deployment_info = \
+        ([("release", parsed_data["release"])] if "release" in parsed_data else []) + \
+        ([("environment", parsed_data["environment"])] if "environment" in parsed_data else []) + \
+        ([("server_name", parsed_data["server_name"])] if "server_name" in parsed_data else [])
+    contexts = get_contexts_enriched_with_ua(parsed_data)
+    return render(request, "issues/event_details.html", {
+        "tab": "event-details",
+        "this_view": "event_details",
+        "project": issue.project,
+        "issue": issue,
+        "event": event,
+        "is_event_page": True,
+        "parsed_data": parsed_data,
+        "request_repr": _request_repr(parsed_data),
+        "key_info": key_info,
+        "logentry_info": logentry_info,
+        "deployment_info": deployment_info,
+        "contexts": contexts,
+        "mute_options": GLOBAL_MUTE_OPTIONS,
+        "q": request.GET.get("q", ""),
+        "event_qs_count": _event_count(request, issue, event_x_qs) if request.GET.get("q") else None,
+        "has_prev": event.digest_order > first_do,
+        "has_next": event.digest_order < last_do,
+    })
+@atomic_for_request_method
+@issue_membership_required
+def issue_history(request, issue):
+    if request.method == "POST":
+        return _handle_post(request, issue)
+    event_qs = search_events(issue.project, issue, request.GET.get("q", ""))
+    last_event = event_qs.order_by("digest_order").last()
+    return render(request, "issues/history.html", {
+        "tab": "history",
+        "project": issue.project,
+        "issue": issue,
+        "is_event_page": False,
+        "request_repr": _request_repr(last_event.get_parsed_data()) if last_event is not None else "",
+        "mute_options": GLOBAL_MUTE_OPTIONS,
+    })
+@atomic_for_request_method
+@issue_membership_required
+def issue_tags(request, issue):
+    if request.method == "POST":
+        return _handle_post(request, issue)
+    event_qs = search_events(issue.project, issue, request.GET.get("q", ""))
+    last_event = event_qs.order_by("digest_order").last()
+    return render(request, "issues/tags.html", {
+        "tab": "tags",
+        "project": issue.project,
+        "issue": issue,
+        "is_event_page": False,
+        "request_repr": _request_repr(last_event.get_parsed_data()) if last_event is not None else "",
+        "mute_options": GLOBAL_MUTE_OPTIONS,
+    })
+@atomic_for_request_method
+@issue_membership_required
+def issue_grouping(request, issue):
+    if request.method == "POST":
+        return _handle_post(request, issue)
+    event_qs = search_events(issue.project, issue, request.GET.get("q", ""))
+    last_event = event_qs.order_by("digest_order").last()
+    return render(request, "issues/grouping.html", {
+        "tab": "grouping",
+        "project": issue.project,
+        "issue": issue,
+        "is_event_page": False,
+        "request_repr": _request_repr(last_event.get_parsed_data()) if last_event is not None else "",
+        "mute_options": GLOBAL_MUTE_OPTIONS,
+    })
+@atomic_for_request_method
+@issue_membership_required
+def issue_event_list(request, issue):
+    if request.method == "POST":
+        return _handle_post(request, issue)
+    if "q" in request.GET:
+        event_list = search_events(issue.project, issue, request.GET["q"]).order_by("digest_order")
+        event_x_qs = search_events_optimized(issue.project, issue, request.GET.get("q", ""))
+        paginator = KnownCountPaginator(event_list, 250, count=event_x_qs.count())
+    else:
+        event_list = issue.event_set.order_by("digest_order")
+        paginator = KnownCountPaginator(event_list, 250, count=issue.stored_event_count)
+    page_number = request.GET.get("page")
+    page_obj = paginator.get_page(page_number)
+    last_event = event_list.last()
+    return render(request, "issues/event_list.html", {
+        "tab": "event-list",
+        "project": issue.project,
+        "issue": issue,
+        "event_list": event_list,
+        "is_event_page": False,
+        "request_repr": _request_repr(last_event.get_parsed_data()) if last_event is not None else "",
+        "mute_options": GLOBAL_MUTE_OPTIONS,
+        "q": request.GET.get("q", ""),
+        "page_obj": page_obj,
+    })
+@atomic_for_request_method
+@issue_membership_required
+def history_comment_new(request, issue):
+    if request.method == "POST":
+        form = CommentForm(request.POST)
+        assert form.is_valid()  # we have only a textfield with no validation properties; also: no html-side handling
+        if form.cleaned_data["comment"] != "":
+            TurningPoint.objects.create(
+                project=issue.project,
+                issue=issue, kind=TurningPointKind.MANUAL_ANNOTATION, user=request.user,
+                comment=form.cleaned_data["comment"],
+                timestamp=timezone.now())
+        return redirect(issue_history, issue_pk=issue.pk)
+    return HttpResponseNotAllowed(["POST"])
+@atomic_for_request_method
+@issue_membership_required
+def history_comment_edit(request, issue, comment_pk):
+    comment = get_object_or_404(TurningPoint, pk=comment_pk, issue_id=issue.pk)
+    if comment.user_id != request.user.id:
+        raise PermissionDenied("You can only edit your own comments")
+    if request.method == "POST":
+        form = CommentForm(request.POST, instance=comment)
+        assert form.is_valid()
+        form.save()
+        return redirect(reverse(issue_history, kwargs={'issue_pk': issue.pk}) + f"#comment-{ comment_pk }")
+@atomic_for_request_method
+@issue_membership_required
+def history_comment_delete(request, issue, comment_pk):
+    comment = get_object_or_404(TurningPoint, pk=comment_pk, issue_id=issue.pk)
+    if comment.user_id != request.user.id:
+        raise PermissionDenied("You can only delete your own comments")
+    if comment.kind != TurningPointKind.MANUAL_ANNOTATION:
+        raise PermissionDenied("You can only delete manual annotations")
+    if request.method == "POST":
+        comment.delete()
+        return redirect(reverse(issue_history, kwargs={'issue_pk': issue.pk}))
+    return HttpResponseNotAllowed(["POST"])

--- a//dev/null
+++ b/projects/admin.py
@@ -0,0 +1,72 @@
+from django.contrib import admin
+from django.utils.decorators import method_decorator
+from django.views.decorators.csrf import csrf_protect
+from admin_auto_filters.filters import AutocompleteFilter
+from bugsink.transaction import immediate_atomic
+from .models import Project, ProjectMembership
+csrf_protect_m = method_decorator(csrf_protect)
+class ProjectFilter(AutocompleteFilter):
+    title = 'Project'
+    field_name = 'project'
+class UserFilter(AutocompleteFilter):
+    title = 'User'
+    field_name = 'user'
+class ProjectMembershipInline(admin.TabularInline):
+    model = ProjectMembership
+    autocomplete_fields = [
+        'user',
+    ]
+    extra = 0
+@admin.register(Project)
+class ProjectAdmin(admin.ModelAdmin):
+    search_fields = [
+        'name',
+    ]
+    list_display = [
+        'name',
+        'dsn',
+        'digested_event_count',
+        'stored_event_count',
+    ]
+    readonly_fields = [
+        'dsn',
+    ]
+    inlines = [
+        ProjectMembershipInline,
+    ]
+    prepopulated_fields = {
+        'slug': ['name'],
+    }
+    def get_deleted_objects(self, objs, request):
+        to_delete = list(objs) + ["...all its related objects... (delayed)"]
+        model_count = {
+            Project: len(objs),
+        }
+        perms_needed = set()
+        protected = []
+        return to_delete, model_count, perms_needed, protected
+    def delete_queryset(self, request, queryset):
+        with immediate_atomic():
+            for obj in queryset:
+                obj.delete_deferred()
+    def delete_model(self, request, obj):
+        with immediate_atomic():
+            obj.delete_deferred()
+    @csrf_protect_m
+    def delete_view(self, request, object_id, extra_context=None):
+        return self._delete_view(request, object_id, extra_context)
+@admin.register(ProjectMembership)
+class ProjectMembershipAdmin(admin.ModelAdmin):
+    list_filter = [
+        ProjectFilter,
+        UserFilter,
+    ]
+    list_display = [
+        'project',
+        'user',
+        'send_email_alerts',
+    ]
+    autocomplete_fields = [
+        'project',
+        'user',
+    ]

--- a//dev/null
+++ b/projects/migrations/0012_project_is_deleted.py
@@ -0,0 +1,12 @@
+from django.db import migrations, models
+class Migration(migrations.Migration):
+    dependencies = [
+        ("projects", "0011_fill_stored_event_count"),
+    ]
+    operations = [
+        migrations.AddField(
+            model_name="project",
+            name="is_deleted",
+            field=models.BooleanField(default=False),
+        ),
+    ]

--- a//dev/null
+++ b/projects/migrations/0013_delete_objects_pointing_to_null_project.py
@@ -0,0 +1,26 @@
+from django.db import migrations
+def delete_objects_pointing_to_null_project(apps, schema_editor):
+    preferred = [
+        'tags.EventTag',
+        'tags.IssueTag',
+        'tags.TagValue',
+        'tags.TagKey',
+        'events.Event',
+        'issues.Grouping',
+        'releases.Release',
+        'issues.Issue',
+    ]
+    for model_name in preferred:
+        model = apps.get_model(*model_name.split('.'))
+        model.objects.filter(project__isnull=True).delete()
+class Migration(migrations.Migration):
+    dependencies = [
+        ("projects", "0012_project_is_deleted"),
+        ("issues", "0024_turningpoint_project_alter_not_null"),
+        ("tags", "0004_alter_do_nothing"),
+        ("releases", "0002_release_releases_re_sort_ep_5c07c8_idx"),
+        ("events", "0021_alter_do_nothing"),
+    ]
+    operations = [
+        migrations.RunPython(delete_objects_pointing_to_null_project, reverse_code=migrations.RunPython.noop),
+    ]

--- a//dev/null
+++ b/projects/migrations/0014_alter_projectmembership_project.py
@@ -0,0 +1,15 @@
+from django.db import migrations, models
+import django.db.models.deletion
+class Migration(migrations.Migration):
+    dependencies = [
+        ("projects", "0013_delete_objects_pointing_to_null_project"),
+    ]
+    operations = [
+        migrations.AlterField(
+            model_name="projectmembership",
+            name="project",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="projects.project"
+            ),
+        ),
+    ]

--- a//dev/null
+++ b/projects/models.py
@@ -0,0 +1,88 @@
+import uuid
+from django.db import models
+from django.conf import settings
+from django.utils.text import slugify
+from bugsink.app_settings import get_settings
+from bugsink.transaction import delay_on_commit
+from compat.dsn import build_dsn
+from teams.models import TeamMembership
+from .tasks import delete_project_deps
+class ProjectRole(models.IntegerChoices):
+    MEMBER = 0
+    ADMIN = 1
+class ProjectVisibility(models.IntegerChoices):
+    JOINABLE = 1  # anyone can join
+    DISCOVERABLE = 10
+    TEAM_MEMBERS = 99
+class Project(models.Model):
+    team = models.ForeignKey("teams.Team", blank=False, null=True, on_delete=models.SET_NULL)
+    name = models.CharField(max_length=255, blank=False, null=False, unique=True)
+    slug = models.SlugField(max_length=50, blank=False, null=False, unique=True)
+    is_deleted = models.BooleanField(default=False)
+    sentry_key = models.UUIDField(editable=False, default=uuid.uuid4)
+    users = models.ManyToManyField(settings.AUTH_USER_MODEL, blank=True, through="ProjectMembership")
+    has_releases = models.BooleanField(editable=False, default=False)
+    digested_event_count = models.PositiveIntegerField(null=False, blank=False, default=0, editable=False)
+    stored_event_count = models.IntegerField(blank=False, null=False, default=0, editable=False)
+    alert_on_new_issue = models.BooleanField(default=True)
+    alert_on_regression = models.BooleanField(default=True)
+    alert_on_unmute = models.BooleanField(default=True)
+    visibility = models.IntegerField(
+        choices=ProjectVisibility.choices, default=ProjectVisibility.TEAM_MEMBERS,
+        help_text="Which users can see this project and its issues?")
+    quota_exceeded_until = models.DateTimeField(null=True, blank=True)
+    next_quota_check = models.PositiveIntegerField(null=False, default=0)
+    retention_max_event_count = models.PositiveIntegerField(default=10_000)
+    def __str__(self):
+        return self.name
+    def get_absolute_url(self):
+        return f"/issues/{ self.id }/"
+    @property
+    def dsn(self):
+        return build_dsn(str(get_settings().BASE_URL), self.id, self.sentry_key.hex)
+    def get_latest_release(self):
+        from releases.models import ordered_releases
+        if not hasattr(self, "_latest_release"):  # per-instance cache
+            self._latest_release = list(ordered_releases(project=self))[-1]
+        return self._latest_release
+    def save(self, *args, **kwargs):
+        if self.slug in [None, ""]:
+            base_slug = slugify(self.name)
+            similar_slugs = Project.objects.filter(slug__startswith=base_slug).values_list("slug", flat=True)
+            self.slug = base_slug
+            i = 0
+            while self.slug in similar_slugs:
+                self.slug = f"{base_slug}-{i}"
+                i += 1
+        super().save(*args, **kwargs)
+    def delete_deferred(self):
+        """Marks the project as deleted, and schedules deletion of all related objects"""
+        self.is_deleted = True
+        self.save(update_fields=["is_deleted"])
+        delay_on_commit(delete_project_deps, str(self.id))
+    def is_joinable(self, user=None):
+        if user is not None:
+            try:
+                TeamMembership.objects.get(team=self.team, user=user)
+                return True
+            except TeamMembership.DoesNotExist:
+                pass
+        return self.visibility <= ProjectVisibility.JOINABLE
+    def is_discoverable(self):
+        return self.visibility <= ProjectVisibility.DISCOVERABLE
+    class Meta:
+        indexes = [
+            models.Index(fields=["name"]),
+        ]
+class ProjectMembership(models.Model):
+    project = models.ForeignKey(Project, on_delete=models.DO_NOTHING)
+    user = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.CASCADE)
+    send_email_alerts = models.BooleanField(default=None, null=True)
+    role = models.IntegerField(choices=ProjectRole.choices, default=ProjectRole.MEMBER)
+    accepted = models.BooleanField(default=False)
+    def __str__(self):
+        return f"{self.user} project membership of {self.project}"
+    class Meta:
+        unique_together = ("project", "user")
+    def is_admin(self):
+        return self.role == ProjectRole.ADMIN

--- a//dev/null
+++ b/projects/tasks.py
@@ -0,0 +1,106 @@
+from django.urls import reverse
+from snappea.decorators import shared_task
+from bugsink.app_settings import get_settings
+from bugsink.utils import send_rendered_email
+from bugsink.transaction import immediate_atomic, delay_on_commit
+from bugsink.utils import get_model_topography, delete_deps_with_budget
+@shared_task
+def send_project_invite_email_new_user(email, project_pk, token):
+    from .models import Project   # avoid circular import
+    project = Project.objects.get(pk=project_pk)
+    send_rendered_email(
+        subject='You have been invited to join "%s"' % project.name,
+        base_template_name="mails/project_membership_invite_new_user",
+        recipient_list=[email],
+        context={
+            "site_title": get_settings().SITE_TITLE,
+            "base_url": get_settings().BASE_URL + "/",
+            "project_name": project.name,
+            "url": get_settings().BASE_URL + reverse("project_members_accept_new_user", kwargs={
+                "token": token,
+                "project_pk": project_pk,
+            }),
+        },
+    )
+@shared_task
+def send_project_invite_email(email, project_pk):
+    from .models import Project   # avoid circular import
+    project = Project.objects.get(pk=project_pk)
+    send_rendered_email(
+        subject='You have been invited to join "%s"' % project.name,
+        base_template_name="mails/project_membership_invite",
+        recipient_list=[email],
+        context={
+            "site_title": get_settings().SITE_TITLE,
+            "base_url": get_settings().BASE_URL + "/",
+            "project_name": project.name,
+            "url": get_settings().BASE_URL + reverse("project_members_accept", kwargs={
+                "project_pk": project_pk,
+            }),
+        },
+    )
+def get_model_topography_with_project_override():
+    """
+    Returns the model topography with ordering adjusted to prefer deletions via .project, when available.
+    This assumes that Project is not only the root of the dependency graph, but also that if a model has an .project
+    ForeignKey, deleting it via that path is sufficient, meaning we can safely avoid visiting the same model again
+    through other ForeignKey routes (e.g. any of the .issue paths).
+    The preference is encoded via an explicit list of models, which are visited early and only via their .project path.
+    """
+    from issues.models import Issue, TurningPoint, Grouping
+    from events.models import Event
+    from tags.models import IssueTag, EventTag, TagValue, TagKey
+    from alerts.models import MessagingServiceConfig
+    from releases.models import Release
+    from projects.models import ProjectMembership
+    preferred = [
+        EventTag,      # above Event, to avoid deletions via .event
+        IssueTag,
+        TagValue,
+        TagKey,
+        TurningPoint,  # above Event, to avoid deletions via .triggering_event
+        Event,         # above Grouping, to avoid deletions via .grouping
+        Grouping,
+        MessagingServiceConfig,
+        ProjectMembership,
+        Release,
+        Issue,         # at the bottom, most everything points to this, we'd rather delete those things via .project
+    ]
+    def as_preferred(lst):
+        """
+        Sorts the list of (model, fk_name) tuples such that the models are in the preferred order as indicated above,
+        and models which occur with another fk_name are pruned
+        """
+        return sorted(
+            [(model, fk_name) for model, fk_name in lst if fk_name == "project" or model not in preferred],
+            key=lambda x: preferred.index(x[0]) if x[0] in preferred else len(preferred),
+        )
+    topo = get_model_topography()
+    for k, lst in topo.items():
+        topo[k] = as_preferred(lst)
+    return topo
+@shared_task
+def delete_project_deps(project_id):
+    from .models import Project   # avoid circular import
+    with immediate_atomic():
+        budget = 500
+        num_deleted = 0
+        dep_graph = get_model_topography_with_project_override()
+        for model_for_recursion, fk_name_for_recursion in dep_graph["projects.Project"]:
+            this_num_deleted = delete_deps_with_budget(
+                project_id,
+                model_for_recursion,
+                fk_name_for_recursion,
+                [project_id],
+                budget - num_deleted,
+                dep_graph,
+                is_for_project=True,
+            )
+            num_deleted += this_num_deleted
+            if num_deleted >= budget:
+                delay_on_commit(delete_project_deps, project_id)
+                return
+        if budget - num_deleted <= 0:
+            delay_on_commit(delete_project_deps, project_id)
+        else:
+            Project.objects.filter(pk=project_id).delete()

--- a//dev/null
+++ b/projects/views.py
@@ -0,0 +1,346 @@
+import json
+from datetime import timedelta
+from django.shortcuts import render
+from django.db import models
+from django.shortcuts import redirect
+from django.http import Http404, HttpResponseRedirect
+from django.core.exceptions import PermissionDenied
+from django.contrib.auth import get_user_model
+from django.contrib import messages
+from django.contrib.auth import logout
+from django.urls import reverse
+from django.utils import timezone
+from users.models import EmailVerification
+from teams.models import TeamMembership, Team, TeamRole
+from bugsink.app_settings import get_settings, CB_ANYBODY, CB_MEMBERS, CB_ADMINS
+from bugsink.decorators import login_exempt, atomic_for_request_method
+from alerts.models import MessagingServiceConfig
+from alerts.forms import MessagingServiceConfigForm
+from alerts.service_backends.slack import SlackConfigForm
+from .models import Project, ProjectMembership, ProjectRole, ProjectVisibility
+from .forms import ProjectMembershipForm, MyProjectMembershipForm, ProjectMemberInviteForm, ProjectForm
+from .tasks import send_project_invite_email, send_project_invite_email_new_user
+User = get_user_model()
+@atomic_for_request_method
+def project_list(request, ownership_filter=None):
+    my_memberships = ProjectMembership.objects.filter(user=request.user)
+    my_team_memberships = TeamMembership.objects.filter(user=request.user)
+    my_projects = Project.objects.filter(
+        projectmembership__in=my_memberships, is_deleted=False).order_by('name').distinct()
+    my_teams_projects = \
+        Project.objects \
+        .filter(team__teammembership__in=my_team_memberships, is_deleted=False) \
+        .exclude(projectmembership__in=my_memberships) \
+        .order_by('name').distinct()
+    if request.user.is_superuser:
+        other_projects = Project.objects \
+            .filter(is_deleted=False) \
+            .exclude(projectmembership__in=my_memberships) \
+            .exclude(team__teammembership__in=my_team_memberships) \
+            .order_by('name').distinct()
+    else:
+        other_projects = Project.objects \
+            .filter(is_deleted=False) \
+            .exclude(projectmembership__in=my_memberships) \
+            .exclude(team__teammembership__in=my_team_memberships) \
+            .exclude(visibility=ProjectVisibility.TEAM_MEMBERS) \
+            .order_by('name').distinct()
+    if ownership_filter is None:
+        if my_projects.exists():
+            return redirect('project_list_mine')
+        if my_teams_projects.exists():
+            return redirect('project_list_teams')
+        if other_projects.exists():
+            return redirect('project_list_other')
+        return redirect('project_list_mine')  # if nothing to show, might as well show your own
+    if request.method == 'POST':
+        full_action_str = request.POST.get('action')
+        action, project_pk = full_action_str.split(":", 1)
+        if action == "leave":
+            ProjectMembership.objects.filter(project=project_pk, user=request.user.id).delete()
+        elif action == "join":
+            project = Project.objects.get(id=project_pk)
+            if not project.is_joinable(user=request.user) and not request.user.is_superuser:
+                raise PermissionDenied("This project is not joinable")
+            messages.success(request, 'You have joined the project "%s"' % project.name)
+            ProjectMembership.objects.create(
+                project_id=project_pk, user_id=request.user.id, role=ProjectRole.MEMBER, accepted=True)
+            return redirect('project_member_settings', project_pk=project_pk, user_pk=request.user.id)
+    if ownership_filter == "mine":
+        base_qs = my_projects
+    elif ownership_filter == "teams":
+        base_qs = my_teams_projects
+    elif ownership_filter == "other":
+        base_qs = other_projects
+    else:
+        raise ValueError(f"Invalid ownership_filter: {ownership_filter}")
+    project_list = base_qs.annotate(
+        member_count=models.Count(
+            'projectmembership', distinct=True, filter=models.Q(projectmembership__accepted=True)),
+    ).select_related('team')
+    if ownership_filter == "mine":
+        my_memberships_dict = {m.project_id: m for m in my_memberships}
+        project_list_2 = []
+        for project in project_list:
+            project.member = my_memberships_dict.get(project.id)
+            project_list_2.append(project)
+        project_list = project_list_2
+    return render(request, 'projects/project_list.html', {
+        'can_create':
+            request.user.is_superuser or TeamMembership.objects.filter(user=request.user, role=TeamRole.ADMIN).exists(),
+        'ownership_filter': ownership_filter,
+        'project_list': project_list,
+    })
+@atomic_for_request_method
+def project_new(request):
+    if not (request.user.is_superuser or TeamMembership.objects.filter(user=request.user,
+            role=TeamRole.ADMIN).exists()):
+        raise PermissionDenied("You need to be a team admin to create a project")
+    if get_settings().SINGLE_TEAM and Team.objects.count() == 0:
+        Team.objects.create(name="Single Team")
+    if request.user.is_superuser:
+        team_qs = Team.objects.all()
+    else:
+        my_admin_memberships = TeamMembership.objects.filter(user=request.user, role=TeamRole.ADMIN, accepted=True)
+        team_qs = Team.objects.filter(teammembership__in=my_admin_memberships).distinct()
+    if request.method == 'POST':
+        form = ProjectForm(request.POST, team_qs=team_qs)
+        if form.is_valid():
+            project = form.save()
+            ProjectMembership.objects.create(project=project, user=request.user, role=ProjectRole.ADMIN, accepted=True)
+            return redirect('project_sdk_setup', project_pk=project.id)
+    else:
+        form = ProjectForm(team_qs=team_qs)
+    return render(request, 'projects/project_new.html', {
+        'form': form,
+    })
+def _check_project_admin(project, user):
+    if not user.is_superuser and \
+       not ProjectMembership.objects.filter(
+            project=project, user=user, role=ProjectRole.ADMIN, accepted=True).exists() and \
+       not TeamMembership.objects.filter(team=project.team, user=user, role=TeamRole.ADMIN, accepted=True).exists():
+        raise PermissionDenied("You are not an admin of this project")
+@atomic_for_request_method
+def project_edit(request, project_pk):
+    project = Project.objects.get(id=project_pk, is_deleted=False)
+    _check_project_admin(project, request.user)
+    if request.method == 'POST':
+        action = request.POST.get('action')
+        if action == 'delete':
+            if (not request.user.is_superuser
+                and not ProjectMembership.objects.filter(
+                    project=project, user=request.user, role=ProjectRole.ADMIN, accepted=True).exists()
+                and not TeamMembership.objects.filter(
+                    team=project.team, user=request.user, role=TeamRole.ADMIN, accepted=True).exists()):
+                raise PermissionDenied("Only project or team admins can delete projects")
+            project.delete_deferred()
+            messages.success(request, f'Project "{project.name}" has been deleted successfully.')
+            return redirect('project_list')
+        form = ProjectForm(request.POST, instance=project)
+        if form.is_valid():
+            form.save()
+            return redirect('project_members', project_pk=project.id)
+    else:
+        form = ProjectForm(instance=project)
+    return render(request, 'projects/project_edit.html', {
+        'project': project,
+        'form': form,
+    })
+@atomic_for_request_method
+def project_members(request, project_pk):
+    project = Project.objects.get(id=project_pk, is_deleted=False)
+    _check_project_admin(project, request.user)
+    if request.method == 'POST':
+        full_action_str = request.POST.get('action')
+        action, user_id = full_action_str.split(":", 1)
+        if action == "remove":
+            ProjectMembership.objects.filter(project=project_pk, user=user_id).delete()
+        elif action == "reinvite":
+            user = User.objects.get(id=user_id)
+            _send_project_invite_email(user, project_pk)
+            messages.success(request, f"Invitation resent to {user.email}")
+    return render(request, 'projects/project_members.html', {
+        'project': project,
+        'members': project.projectmembership_set.all().select_related('user'),
+    })
+def _send_project_invite_email(user, project_pk):
+    """Send an email to a user inviting them to a project; (for new users this includes the email-verification link)"""
+    if user.is_active:
+        send_project_invite_email.delay(user.email, project_pk)
+    else:
+        verification = EmailVerification.objects.create(user=user, email=user.username)
+        send_project_invite_email_new_user.delay(user.email, project_pk, verification.token)
+@atomic_for_request_method
+def project_members_invite(request, project_pk):
+    project = Project.objects.get(id=project_pk, is_deleted=False)
+    _check_project_admin(project, request.user)
+    if get_settings().USER_REGISTRATION in [CB_ANYBODY, CB_MEMBERS]:
+        user_must_exist = False
+    elif get_settings().USER_REGISTRATION == CB_ADMINS and request.user.is_superuser:
+        user_must_exist = False
+    else:
+        user_must_exist = True
+    if request.method == 'POST':
+        form = ProjectMemberInviteForm(user_must_exist, request.POST)
+        if form.is_valid():
+            email = form.cleaned_data['email']
+            user, user_created = User.objects.get_or_create(
+                email=email, defaults={'username': email, 'is_active': False})
+            _send_project_invite_email(user, project_pk)
+            _, membership_created = ProjectMembership.objects.get_or_create(project=project, user=user, defaults={
+                'role': form.cleaned_data['role'],
+                'accepted': False,
+            })
+            if membership_created:
+                messages.success(request, f"Invitation sent to {email}")
+            else:
+                messages.success(
+                    request, f"Invitation resent to {email} (it was previously sent and we just sent it again)")
+            if request.POST.get('action') == "invite_and_add_another":
+                return redirect('project_members_invite', project_pk=project_pk)
+            return redirect('project_members', project_pk=project_pk)
+    else:
+        form = ProjectMemberInviteForm(user_must_exist)
+    return render(request, 'projects/project_members_invite.html', {
+        'project': project,
+        'form': form,
+    })
+@atomic_for_request_method
+def project_member_settings(request, project_pk, user_pk):
+    try:
+        your_membership = ProjectMembership.objects.get(project=project_pk, user=request.user)
+    except ProjectMembership.DoesNotExist:
+        raise PermissionDenied("You are not a member of this project")
+    if not your_membership.accepted:
+        return redirect("project_members_accept", project_pk=project_pk)
+    this_is_you = str(user_pk) == str(request.user.id)
+    if not this_is_you:
+        _check_project_admin(Project.objects.get(id=project_pk, is_deleted=False), request.user)
+        membership = ProjectMembership.objects.get(project=project_pk, user=user_pk)
+        create_form = lambda data: ProjectMembershipForm(data, instance=membership)  # noqa
+    else:
+        edit_role = your_membership.role == ProjectRole.ADMIN or request.user.is_superuser
+        create_form = lambda data: MyProjectMembershipForm(data=data, instance=your_membership, edit_role=edit_role)  # noqa
+    if request.method == 'POST':
+        form = create_form(request.POST)
+        if form.is_valid():
+            form.save()
+            if this_is_you:
+                return redirect('project_list')
+            return redirect('project_members', project_pk=project_pk)
+    else:
+        form = create_form(None)
+    return render(request, 'projects/project_member_settings.html', {
+        'this_is_you': this_is_you,
+        'user': User.objects.get(id=user_pk),
+        'project': Project.objects.get(id=project_pk, is_deleted=False),
+        'form': form,
+    })
+@atomic_for_request_method
+@login_exempt  # no login is required, the token is what identifies the user
+def project_members_accept_new_user(request, project_pk, token):
+    EmailVerification.objects.filter(
+        created_at__lt=timezone.now() - timedelta(get_settings().USER_REGISTRATION_VERIFY_EMAIL_EXPIRY)).delete()
+    try:
+        verification = EmailVerification.objects.get(token=token)
+    except EmailVerification.DoesNotExist:
+        raise Http404("Invalid or expired token")
+    user = verification.user
+    if not user.has_usable_password() or not user.is_active:
+        return HttpResponseRedirect(reverse("reset_password", kwargs={"token": token}) + "?next=" + reverse(
+            project_members_accept, kwargs={"project_pk": project_pk})
+        )
+    if request.user.is_authenticated and request.user != user:
+        logout(request)
+    verification.delete()
+    return redirect("project_members_accept", project_pk=project_pk)
+@atomic_for_request_method
+def project_members_accept(request, project_pk):
+    project = Project.objects.get(id=project_pk, is_deleted=False)
+    membership = ProjectMembership.objects.get(project=project, user=request.user)
+    if membership.accepted:
+        return redirect("project_member_settings", project_pk=project_pk, user_pk=request.user.id)
+    if request.method == 'POST':
+        if request.POST["action"] == "decline":
+            membership.delete()
+            return redirect("home")
+        if request.POST["action"] == "accept":
+            membership.accepted = True
+            membership.save()
+            return redirect("project_member_settings", project_pk=project_pk, user_pk=request.user.id)
+        raise Http404("Invalid action")
+    return render(request, "projects/project_members_accept.html", {"project": project, "membership": membership})
+@atomic_for_request_method
+def project_sdk_setup(request, project_pk, platform=""):
+    project = Project.objects.get(id=project_pk, is_deleted=False)
+    if not request.user.is_superuser and not ProjectMembership.objects.filter(project=project, user=request.user,
+                                                                              accepted=True).exists():
+        raise PermissionDenied("You are not a member of this project")
+    assert platform in ["", "python", "javascript", "php"]
+    template_name = "projects/project_sdk_setup%s.html" % ("_" + platform if platform else "")
+    return render(request, template_name, {
+        "project": project,
+        "dsn": project.dsn,
+    })
+@atomic_for_request_method
+def project_alerts_setup(request, project_pk):
+    project = Project.objects.get(id=project_pk, is_deleted=False)
+    _check_project_admin(project, request.user)
+    if request.method == 'POST':
+        full_action_str = request.POST.get('action')
+        action, service_id = full_action_str.split(":", 1)
+        if action == "remove":
+            MessagingServiceConfig.objects.filter(project=project_pk, id=service_id).delete()
+        elif action == "test":
+            service = MessagingServiceConfig.objects.get(project=project_pk, id=service_id)
+            service_backend = service.get_backend()
+            service_backend.send_test_message()
+            messages.success(
+                request, "Test message sent; check the configured service to see if it arrived.")
+    return render(request, 'projects/project_alerts_setup.html', {
+        'project': project,
+        'service_configs': project.service_configs.all(),
+    })
+@atomic_for_request_method
+def project_messaging_service_add(request, project_pk):
+    project = Project.objects.get(id=project_pk, is_deleted=False)
+    _check_project_admin(project, request.user)
+    if request.method == 'POST':
+        form = MessagingServiceConfigForm(project, request.POST)
+        config_form = SlackConfigForm(data=request.POST)
+        if form.is_valid() and config_form.is_valid():
+            service = form.save(commit=False)
+            service.config = json.dumps(config_form.get_config())
+            service.save()
+            messages.success(request, "Messaging service added successfully.")
+            return redirect('project_alerts_setup', project_pk=project_pk)
+    else:
+        form = MessagingServiceConfigForm(project)
+        config_form = SlackConfigForm()
+    return render(request, 'projects/project_messaging_service_edit.html', {
+        'project': project,
+        'form': form,
+        'config_form': config_form,
+    })
+@atomic_for_request_method
+def project_messaging_service_edit(request, project_pk, service_pk):
+    project = Project.objects.get(id=project_pk, is_deleted=False)
+    _check_project_admin(project, request.user)
+    instance = project.service_configs.get(id=service_pk)
+    if request.method == 'POST':
+        form = MessagingServiceConfigForm(project, request.POST, instance=instance)
+        config_form = SlackConfigForm(data=request.POST)
+        if form.is_valid() and config_form.is_valid():
+            service = form.save(commit=False)
+            service.config = json.dumps(config_form.get_config())
+            service.save()
+            messages.success(request, "Messaging service updated successfully.")
+            return redirect('project_alerts_setup', project_pk=project_pk)
+    else:
+        form = MessagingServiceConfigForm(project, instance=instance)
+        config_form = SlackConfigForm(config=json.loads(instance.config))
+    return render(request, 'projects/project_messaging_service_edit.html', {
+        'project': project,
+        'form': form,
+        'config_form': config_form,
+    })

--- a//dev/null
+++ b/releases/migrations/0003_alter_release_project.py
@@ -0,0 +1,16 @@
+from django.db import migrations, models
+import django.db.models.deletion
+class Migration(migrations.Migration):
+    dependencies = [
+        ("projects", "0014_alter_projectmembership_project"),
+        ("releases", "0002_release_releases_re_sort_ep_5c07c8_idx"),
+    ]
+    operations = [
+        migrations.AlterField(
+            model_name="release",
+            name="project",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="projects.project"
+            ),
+        ),
+    ]

--- a//dev/null
+++ b/releases/models.py
@@ -0,0 +1,101 @@
+import json
+import re
+import uuid
+from semver.version import Version
+from django.db import models
+from django.utils import timezone
+from django.db.models.functions import Concat
+from django.db.models import Value
+from issues.models import Issue, TurningPoint, TurningPointKind
+RE_PACKAGE_VERSION = re.compile('((?P<package>.*)[@])?(?P<version>.*)')
+def is_valid_semver(full_version):
+    try:
+        version = RE_PACKAGE_VERSION.match(full_version).groupdict()["version"]
+        Version.parse(version)
+        return True
+    except ValueError:
+        return False
+def release_sort_key(release):
+    return (
+        release.sort_epoch,
+        Version.parse(release.semver) if release.is_semver else release.date_released
+    )
+def ordered_releases(*filter_args, **filter_kwargs):
+    """Sorting Release objects in code (as opposed to in-DB) to facilitate semver-based sorting when applicable"""
+    releases = Release.objects.filter(*filter_args, **filter_kwargs)
+    return sorted(releases, key=release_sort_key)
+class Release(models.Model):
+    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
+    project = models.ForeignKey("projects.Project", blank=False, null=False, on_delete=models.DO_NOTHING)
+    version = models.CharField(max_length=250, null=False, blank=False)
+    date_released = models.DateTimeField(default=timezone.now)
+    semver = models.CharField(max_length=255, null=False, editable=False)
+    is_semver = models.BooleanField(editable=False)
+    sort_epoch = models.IntegerField(editable=False)
+    def save(self, *args, **kwargs):
+        if self.is_semver is None:
+            self.is_semver = is_valid_semver(self.version)
+            if self.is_semver:
+                self.semver = RE_PACKAGE_VERSION.match(self.version)["version"]
+            any_release_from_last_epoch = Release.objects.filter(project=self.project).order_by("sort_epoch").last()
+            if any_release_from_last_epoch is None:
+                self.sort_epoch = 0
+            elif self.is_semver == any_release_from_last_epoch.is_semver:
+                self.sort_epoch = any_release_from_last_epoch.sort_epoch
+            else:
+                self.sort_epoch = any_release_from_last_epoch.sort_epoch + 1
+        super(Release, self).save(*args, **kwargs)
+    class Meta:
+        unique_together = ("project", "version")
+        indexes = [
+            models.Index(fields=["sort_epoch"]),
+        ]
+    def get_short_version(self):
+        if self.version == "":
+            return "Â«no versionÂ»"
+        if self.is_semver:
+            return self.version
+        return self.version[:12]
+def create_release_if_needed(project, version, event, issue=None):
+    if version is None:
+        raise ValueError('The None-like version must be the empty string')
+    version = sanitize_version(version)
+    release, release_created = Release.objects.get_or_create(project=project, version=version)
+    if release_created and version != "":
+        if not project.has_releases:
+            project.has_releases = True
+            project.save()
+        if release == project.get_latest_release():
+            resolved_by_next_qs = Issue.objects.filter(project=project, is_resolved_by_next_release=True)
+            TurningPoint.objects.bulk_create([TurningPoint(
+                    project=project,
+                    issue=issue, kind=TurningPointKind.NEXT_MATERIALIZED, triggering_event=event,
+                    metadata=json.dumps({"actual_release": release.version}), timestamp=event.ingested_at)
+                for issue in resolved_by_next_qs
+            ])
+            event.never_evict = True  # .save() will be called by the caller of this function
+            resolved_by_next_qs.update(
+                fixed_at=Concat("fixed_at", Value(release.version + "\n")),
+                is_resolved_by_next_release=False,
+                )
+            if issue is not None and issue.is_resolved_by_next_release:
+                issue.fixed_at = issue.fixed_at + release.version + "\n"
+                issue.is_resolved_by_next_release = False
+    return release
+def sanitize_version(version):
+    """
+    Implements the folllowing restrictions are from the Sentry documentation:
+    > There are a few restrictions -- the release name cannot:
+    >
+    > - contain newlines, tabulator characters, forward slashes(/), or back slashes(\\)
+    > - be (in their entirety) period (.), double period (..), or space ( )
+    > - exceed 200 characters
+    It does so as sanitize-dont-raise, i.e. it will return a sanitized version of the input string, but will not raise
+    an exception if the input string is invalid. Reason: we care about having valid data (and we rely e.g. on the lack
+    of newlines for our parsing), but we never want an invalid input to lead to discarded events. And there's no one
+    to 'read' a rejected event and fix it.
+    """
+    step_1 = version.replace("\n", "").replace("\t", "").replace("/", "").replace("\\", "")
+    step_2 = "sanitized" if step_1 in (".", "..", " ") else step_1
+    step_3 = step_2[:200]
+    return step_3

--- a//dev/null
+++ b/snappea/foreman.py
@@ -0,0 +1,219 @@
+import contextlib
+import os
+import glob
+import uuid
+import sys
+import json
+import logging
+import time
+import signal
+import threading
+from inotify_simple import INotify, flags
+import sentry_sdk
+from django.conf import settings
+from django.db import connections
+from sentry_sdk_extensions import capture_or_log_exception
+from performance.context_managers import time_to_logger
+from bugsink.transaction import durable_atomic, get_stat
+from . import registry
+from .models import Task
+from .datastructures import Workers
+from .settings import get_settings
+from .utils import run_task_context
+from .stats import Stats
+logger = logging.getLogger("snappea.foreman")
+performance_logger = logging.getLogger("bugsink.performance.snappea")
+def short_id(task_id):
+    return f"{task_id:06}"[-6:-3] + "-" + f"{task_id:06}"[-3:]
+class Foreman:
+    """
+    The Foreman starts workers, as (threading.Thread) threads, based on snappea.Task objects it finds in the sqlite
+    database for that purpose (we use the DB as a simple MQ, we call this DBasMQ).
+    The Foreman code itself is single-threaded, so it can be easily reasoned about.
+    We provide an at-most-once guarantee: Tasks that are picked up are removed from the DB before the works starts.
+    This fits with our model of background tasks, namely
+    * things that you want to happen
+    * but you don't want to wait for them in the request-response loop
+    * getting them done for sure is not more important than in the server itself (which is also subject to os.kill)
+    * you don't care about the answer as part of your code (of course, the DB can be affected)
+    The read/writes to the DB-as-MQ are as such:
+    * "some other process" (the HTTP server) writes _new_ Tasks
+    * the Foreman reads Tasks (determines the workload)
+    * the Foreman deletes (a write operation) Tasks (when done)
+    Because the Foreman has a single sequential loop, and because it is the only thing that _updates_ tasks, there is no
+    need for a locking model of some sort. sqlite locks the whole DB on-write, of course, but in this case we don't use
+    that as a feature. The throughput of our MQ is limited by the speed with which we can do writes to sqlite (2 writes
+    and at most 2 reads are needed per task (2 reads, because 1 to determine 'no more work'; when there are many items
+    at the same time, the average amount of reads may be <1 because we read a whole list)). Performance on personal
+    laptop: 1000 trivial tasks are finished enqueue-to-finished in a few seconds.
+    The main idea is this endless loop of checking for new work and doing it. This leaves the question of how we "go to
+    sleep" when there is no more work and how we wake up from that. This is implemented using inotify on a directory
+    created specifically for that purpose (for each Task a file is dropped there) (and a blocking read on the INotify
+    object). Note that this idea is somewhat incidental though (0MQ or polling the DB in a busy loop are some
+    alternatives). Performance: write/inotify/delete of a single wake-up call is in the order of n*e-5 on my laptop.
+    """
+    def __init__(self):
+        threading.current_thread().name = "FOREMAN"
+        self.settings = get_settings()
+        self.workers = Workers()
+        self.stats = Stats()
+        self.stopping = False
+        signal.signal(signal.SIGINT, self.handle_signal)
+        signal.signal(signal.SIGTERM, self.handle_signal)
+        if not os.path.exists(self.settings.WAKEUP_CALLS_DIR):
+            os.makedirs(self.settings.WAKEUP_CALLS_DIR, exist_ok=True)
+        self.wakeup_calls = INotify()
+        self.wakeup_calls.add_watch(self.settings.WAKEUP_CALLS_DIR, flags.CREATE)
+        pid = os.getpid()
+        logger.info(" =========  SNAPPEA  =========")
+        if self.settings.TASK_ALWAYS_EAGER:
+            logger.info("Startup: Can't run Foreman in TASK_ALWAYS_EAGER mode, EXIT")
+            sys.exit(1)
+        try:
+            if os.path.exists(self.settings.PID_FILE):
+                with open(self.settings.PID_FILE, "r") as f:
+                    old_pid = int(f.read())
+                if os.path.exists(f"/proc/{old_pid}"):
+                    logger.error("Startup: snappea is already running with pid %s, EXIT", old_pid)
+                    sys.exit(1)
+                else:
+                    logger.warning("Startup: stale pid file found, removing %s", self.settings.PID_FILE)
+                    os.remove(self.settings.PID_FILE)
+        except Exception as e:
+            logger.error("Startup: Ignored Error while checking PID file", exc_info=e)
+        os.makedirs(os.path.dirname(self.settings.PID_FILE), exist_ok=True)
+        with open(self.settings.PID_FILE, "w") as f:
+            f.write(str(pid))
+        logger.info("Startup: pid is %s", pid)
+        logger.info("Startup: DB-as-MQ location: %s", settings.DATABASES["snappea"]["NAME"])
+        logger.info("Startup: Wake up calls location: %s", self.settings.WAKEUP_CALLS_DIR)
+        self.signal_semaphore = threading.Semaphore(0)
+        self.worker_semaphore = threading.Semaphore(self.settings.NUM_WORKERS)
+    def connection_close(self, using="default"):
+        connections[using].close()
+    def run_in_thread(self, task_id, function, *args, **kwargs):
+        task_name = "%s.%s" % (function.__module__, function.__name__)
+        logger.info('Starting %s for "%s" with %s, %s', short_id(task_id), task_name, args, kwargs)
+        def non_failing_function(*inner_args, **inner_kwargs):
+            t0 = time.time()
+            try:
+                with run_task_context(inner_args, inner_kwargs):
+                    function(*inner_args, **inner_kwargs)
+            except Exception as e:
+                errored = True  # at the top to avoid error-in-handler leaving us with unset variable
+                if sentry_sdk.is_initialized():
+                    logger.warning("Snappea caught Exception: %s", str(e))
+                capture_or_log_exception(e, logger)
+            else:
+                errored = False
+            finally:
+                for connection in connections.all():
+                    connection.close()
+                runtime = time.time() - t0
+                logger.info('Worker done for "%s" in %.3fs', task_name, runtime)
+                self.stats.done(
+                    task_name, runtime, get_stat("get_write_lock"), get_stat("immediate_transaction"), errored)
+                self.workers.stopped(task_id)
+                self.worker_semaphore.release()
+        worker_thread = threading.Thread(
+            target=non_failing_function, args=args, kwargs=kwargs, name=f"{short_id(task_id)}")
+        worker_thread.daemon = True
+        self.workers.start(task_id, worker_thread)
+        return worker_thread
+    def handle_signal(self, sig, frame):
+        logger.debug("Received %s signal", signal.strsignal(sig))
+        if not self.stopping:  # without this if-statement, repeated signals would extend the deadline
+            self.stopping = True
+            self.stop_deadline = time.time() + self.settings.GRACEFUL_TIMEOUT
+        with open(os.path.join(self.settings.WAKEUP_CALLS_DIR, str(uuid.uuid4())), "w"):
+            pass
+        self.worker_semaphore.release()
+    def run_forever(self):
+        try:
+            self._run_forever()
+        except Exception:
+            logger.info("Stopping: CAUGHT exception in Foreman")
+            self.stop_deadline = time.time() + self.settings.GRACEFUL_TIMEOUT
+            self.stop()
+            logger.info("Stopping: EXCEPTION in Foreman")
+            raise
+    def _run_forever(self):
+        pre_existing_wakeup_notifications = glob.glob(os.path.join(self.settings.WAKEUP_CALLS_DIR, "*"))
+        if len(pre_existing_wakeup_notifications) > 0:
+            logger.info("Startup: Clearing %s items from wakeup dir", len(pre_existing_wakeup_notifications))
+            for filename in pre_existing_wakeup_notifications:
+                os.remove(filename)
+        logger.info("Startup: Clearing Task backlog")
+        while self.create_workers() == self.settings.TASK_QS_LIMIT:
+            pass
+        logger.info("Startup: Task backlog empty now, proceeding to main loop")
+        while True:
+            logger.debug("Main loop: Waiting for wakeup call")
+            for event in self.wakeup_calls.read():
+                logger.debug("Main loop: Removing wakeup notification %s", event.name)
+                with contextlib.suppress(FileNotFoundError):
+                    os.unlink(os.path.join(self.settings.WAKEUP_CALLS_DIR, event.name))
+            self.check_for_stopping()  # check after .read() - it may have unblocked via handle_signal()
+            while self.create_workers() == self.settings.TASK_QS_LIMIT:
+                pass  # `== TASK_QS_LIMIT`: as documented above
+            self.check_for_stopping()
+    def create_workers(self):
+        """returns the number of workers created (AKA tasks done)"""
+        logger.debug("Create workers: Querying for tasks")
+        with durable_atomic(using="snappea"):
+            tasks = list(Task.objects.all()[:self.settings.TASK_QS_LIMIT])
+        task_i = -1
+        for task_i, task in enumerate(tasks):
+            logger.debug("Create workers: Creating worker for with task %s", short_id(task.id))
+            logger.debug("Create workers: Checking (maybe waiting) for available worker slots")
+            self.worker_semaphore.acquire()
+            logger.debug("Create workers: Worker slot available")
+            self.check_for_stopping()  # check after .acquire() - it may have unblocked via handle_signal()
+            task_id = task.id
+            try:
+                function = registry[task.task_name]
+                args = json.loads(task.args)
+                kwargs = json.loads(task.kwargs)
+            except Exception as e:
+                logger.error('Create workers: can\'t execute "%s": %s', task.task_name, e)
+                with time_to_logger(performance_logger, "Snappea delete Task"):
+                    task.delete()  # we delete the task because we can't do anything with it, and we don't want to hang
+                capture_or_log_exception(e, logger)
+                self.worker_semaphore.release()
+                continue
+            self.check_for_stopping()  # check_for_stopping() right before taking on the work
+            with time_to_logger(performance_logger, "Snappea Task.delete()"):
+                task.delete()
+            self.run_in_thread(task_id, function, *args, **kwargs)
+        task_count = task_i + 1
+        if task_count == 0:
+            self.connection_close(using="snappea")
+        logger.debug("Create workers: %s tasks popped from queue", task_count)
+        return task_count
+    def check_for_stopping(self):
+        if not self.stopping:
+            return
+        self.stop()
+        logger.info("Stopping: EXIT")
+        sys.exit()
+    def stop(self):
+        if self.settings.WORKAHOLIC:
+            with durable_atomic(using="snappea"):
+                if Task.objects.exists():
+                    logger.info("Not stopping yet: Workaholic mode, waiting for all tasks to finish")
+                    return
+        logger.info("Stopping: waiting for/stopping workers")
+        for task_id, worker_thread in self.workers.list():
+            if worker_thread.is_alive():
+                time_left = self.stop_deadline - time.time()
+                if time_left > 0:
+                    worker_thread.join(time_left)
+                    if worker_thread.is_alive():
+                        logger.info(
+                            "Stopping: %s did not die in %.1fs, proceeding to kill",
+                            short_id(task_id), self.settings.GRACEFUL_TIMEOUT)
+                else:
+                    logger.info(
+                        "Stopping: %s did not die in %.1fs, proceeding to kill",
+                        short_id(task_id), self.settings.GRACEFUL_TIMEOUT)
+        os.remove(self.settings.PID_FILE)

--- a//dev/null
+++ b/tags/management/commands/vacuum_eventless_issuetags.py
@@ -0,0 +1,7 @@
+from django.core.management.base import BaseCommand
+from tags.tasks import vacuum_eventless_issuetags
+class Command(BaseCommand):
+    help = "Kick off tag cleanup by vacuuming IssueTag objects for which there is no EventTag equivalent"
+    def handle(self, *args, **options):
+        vacuum_eventless_issuetags.delay()
+        self.stdout.write("Started tag vacuum via task queue.")

--- a//dev/null
+++ b/tags/management/commands/vacuum_tags.py
@@ -0,0 +1,7 @@
+from django.core.management.base import BaseCommand
+from tags.tasks import vacuum_tagvalues
+class Command(BaseCommand):
+    help = "Kick off tag cleanup by vacuuming orphaned TagValue and TagKey entries."
+    def handle(self, *args, **options):
+        vacuum_tagvalues.delay()
+        self.stdout.write("Started tag vacuum via task queue.")

--- a//dev/null
+++ b/tags/migrations/0002_no_cascade.py
@@ -0,0 +1,36 @@
+from django.db import migrations, models
+import django.db.models.deletion
+class Migration(migrations.Migration):
+    dependencies = [
+        ("tags", "0001_initial"),
+    ]
+    operations = [
+        migrations.AlterField(
+            model_name="eventtag",
+            name="value",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="tags.tagvalue"
+            ),
+        ),
+        migrations.AlterField(
+            model_name="issuetag",
+            name="key",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="tags.tagkey"
+            ),
+        ),
+        migrations.AlterField(
+            model_name="issuetag",
+            name="value",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="tags.tagvalue"
+            ),
+        ),
+        migrations.AlterField(
+            model_name="tagvalue",
+            name="key",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="tags.tagkey"
+            ),
+        ),
+    ]

--- a//dev/null
+++ b/tags/migrations/0003_remove_objects_with_null_issue.py
@@ -0,0 +1,13 @@
+from django.db import migrations
+def remove_objects_with_null_issue(apps, schema_editor):
+    EventTag = apps.get_model("tags", "EventTag")
+    IssueTag = apps.get_model("tags", "IssueTag")
+    EventTag.objects.filter(issue__isnull=True).delete()
+    IssueTag.objects.filter(issue__isnull=True).delete()
+class Migration(migrations.Migration):
+    dependencies = [
+        ("tags", "0002_no_cascade"),
+    ]
+    operations = [
+        migrations.RunPython(remove_objects_with_null_issue, reverse_code=migrations.RunPython.noop),
+    ]

--- a//dev/null
+++ b/tags/migrations/0004_alter_do_nothing.py
@@ -0,0 +1,27 @@
+from django.db import migrations, models
+import django.db.models.deletion
+class Migration(migrations.Migration):
+    dependencies = [
+        ("issues", "0021_alter_do_nothing"),
+        ("tags", "0003_remove_objects_with_null_issue"),
+    ]
+    operations = [
+        migrations.AlterField(
+            model_name="eventtag",
+            name="issue",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING,
+                related_name="event_tags",
+                to="issues.issue",
+            ),
+        ),
+        migrations.AlterField(
+            model_name="issuetag",
+            name="issue",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING,
+                related_name="tags",
+                to="issues.issue",
+            ),
+        ),
+    ]

--- a//dev/null
+++ b/tags/migrations/0005_alter_eventtag_project_alter_issuetag_project_and_more.py
@@ -0,0 +1,37 @@
+from django.db import migrations, models
+import django.db.models.deletion
+class Migration(migrations.Migration):
+    dependencies = [
+        ("projects", "0014_alter_projectmembership_project"),
+        ("tags", "0004_alter_do_nothing"),
+    ]
+    operations = [
+        migrations.AlterField(
+            model_name="eventtag",
+            name="project",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="projects.project"
+            ),
+        ),
+        migrations.AlterField(
+            model_name="issuetag",
+            name="project",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="projects.project"
+            ),
+        ),
+        migrations.AlterField(
+            model_name="tagkey",
+            name="project",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="projects.project"
+            ),
+        ),
+        migrations.AlterField(
+            model_name="tagvalue",
+            name="project",
+            field=models.ForeignKey(
+                on_delete=django.db.models.deletion.DO_NOTHING, to="projects.project"
+            ),
+        ),
+    ]

--- a//dev/null
+++ b/tags/models.py
@@ -0,0 +1,114 @@
+"""
+Tags provide support for arbitrary key/value pairs (both strings) on Events and Issues, allowing for searching &
+counting. Some notes:
+* Arbitrary Tags can be set programatically in the SDKs, which we need to support (Sentry API Compatability).
+* Some "synthetic" Tags are introduced by Bugsink itself: attributes of an Event are deduced and stored explicitly as a
+  Tag. The main reason to do this: stay flexible in terms of DB design and allow for generic code for searching and
+  counting (especially in the light of Issues, where a single tag can have many values). _However_, we don't make a
+  commitment to any particular implementation, and if the deduce-and-store approach turns out to be a performance
+  bottleneck, it may be replaced. Particular notes on what we deduce are in `deduce_tags`.
+https://docs.sentry.io/platforms/python/enriching-events/tags/
+> Tag keys have a maximum length of 32 characters and can contain only letters (a-zA-Z), numbers (0-9), underscores (_),
+> periods (.), colons (:), and dashes (-).
+>
+> Tag values have a maximum length of 200 characters and they cannot contain the newline (\n) character.
+"""
+from django.db import models
+from django.db.models import Q, F
+from projects.models import Project
+from tags.utils import deduce_tags, is_mostly_unique
+from bugsink.moreiterutils import batched
+class TagKey(models.Model):
+    project = models.ForeignKey(Project, blank=False, null=False, on_delete=models.DO_NOTHING)
+    key = models.CharField(max_length=32, blank=False, null=False)
+    mostly_unique = models.BooleanField(default=False)
+    class Meta:
+        unique_together = ('project', 'key')
+    def __str__(self):
+        return f"{self.key}"
+class TagValue(models.Model):
+    project = models.ForeignKey(Project, blank=False, null=False, on_delete=models.DO_NOTHING)
+    key = models.ForeignKey(TagKey, blank=False, null=False, on_delete=models.DO_NOTHING)
+    value = models.CharField(max_length=200, blank=False, null=False, db_index=True)
+    class Meta:
+        unique_together = ('key', 'value')
+    def __str__(self):
+        return f"{self.key.key}:{self.value}"
+class EventTag(models.Model):
+    project = models.ForeignKey(Project, blank=False, null=False, on_delete=models.DO_NOTHING)
+    value = models.ForeignKey(TagValue, blank=False, null=False, on_delete=models.DO_NOTHING)
+    issue = models.ForeignKey(
+        'issues.Issue', blank=False, null=False, on_delete=models.DO_NOTHING, related_name="event_tags")
+    digest_order = models.PositiveIntegerField(blank=False, null=False)
+    event = models.ForeignKey('events.Event', blank=False, null=False, on_delete=models.DO_NOTHING, related_name='tags')
+    class Meta:
+        unique_together = ('value', 'event')
+        indexes = [
+            models.Index(fields=['event']),  # for lookups by event (for event-details page, event-deletions)
+            models.Index(fields=['value', 'issue', 'digest_order']),
+        ]
+class IssueTag(models.Model):
+    project = models.ForeignKey(Project, blank=False, null=False, on_delete=models.DO_NOTHING)
+    key = models.ForeignKey(TagKey, blank=False, null=False, on_delete=models.DO_NOTHING)
+    value = models.ForeignKey(TagValue, blank=False, null=False, on_delete=models.DO_NOTHING)
+    issue = models.ForeignKey('issues.Issue', blank=False, null=False, on_delete=models.DO_NOTHING, related_name='tags')
+    count = models.PositiveIntegerField(default=0)
+    class Meta:
+        unique_together = ('value', 'issue')
+        indexes = [
+            models.Index(fields=['issue', 'key', 'count']),
+        ]
+def _or_join(q_objects):
+    if len(q_objects) == 0:
+        raise ValueError("empty list of Q objects")
+    result = q_objects[0]
+    for q in q_objects[1:]:
+        result |= q
+    return result
+def digest_tags(event_data, event, issue):
+    tags = {
+        k: str(v)[:200] for k, v in deduce_tags(event_data).items()
+    }
+    store_tags(event, issue, tags)
+def store_tags(event, issue, tags):
+    for kv_batch in batched(tags.items(), 64):
+        _store_tags(event, issue, {k: v for k, v in kv_batch})
+def _store_tags(event, issue, tags):
+    if not tags:
+        return  # short-circuit; which is a performance optimization which also avoids some the need for further guards
+    TagKey.objects.bulk_create([
+        TagKey(project_id=event.project_id, key=key, mostly_unique=is_mostly_unique(key)) for key in tags.keys()
+    ], ignore_conflicts=True)
+    tag_key_objects = TagKey.objects.filter(project_id=event.project_id, key__in=tags.keys())
+    TagValue.objects.bulk_create([
+        TagValue(project_id=event.project_id, key=key_obj, value=tags[key_obj.key]) for key_obj in tag_key_objects
+    ], ignore_conflicts=True)
+    tag_value_objects = TagValue.objects.filter(_or_join([
+        Q(key=key_obj, value=tags[key_obj.key]) for key_obj in tag_key_objects]))
+    EventTag.objects.bulk_create([
+        EventTag(
+            project_id=event.project_id,
+            value=tag_value,
+            event=event,
+            issue=issue,
+            digest_order=event.digest_order,
+        ) for tag_value in tag_value_objects
+    ], ignore_conflicts=True)
+    IssueTag.objects.bulk_create([
+        IssueTag(
+            project_id=event.project_id,
+            key_id=tag_value.key_id,
+            value=tag_value,
+            issue=issue,
+        ) for tag_value in tag_value_objects
+    ], ignore_conflicts=True)
+    IssueTag.objects.filter(value__in=tag_value_objects, issue=issue).update(
+        count=F('count') + 1
+    )
+def prune_tagvalues(ids_to_check):
+    used_in_issuetag = set(
+        IssueTag.objects.filter(value_id__in=ids_to_check).values_list('value_id', flat=True)
+    )
+    unused = [pk for pk in ids_to_check if pk not in used_in_issuetag]
+    if unused:
+        TagValue.objects.filter(id__in=unused).delete()

--- a//dev/null
+++ b/tags/tasks.py
@@ -0,0 +1,80 @@
+from django.db.models import Q
+from snappea.decorators import shared_task
+from bugsink.moreiterutils import batched
+from bugsink.transaction import immediate_atomic, delay_on_commit
+from tags.models import TagValue, TagKey, EventTag, IssueTag, _or_join, prune_tagvalues
+BATCH_SIZE = 10_000
+@shared_task
+def vacuum_tagvalues(min_id=0):
+    with immediate_atomic():
+        ids_to_check = list(
+            TagValue.objects
+            .filter(id__gt=min_id)
+            .order_by('id')
+            .values_list('id', flat=True)[:BATCH_SIZE]
+        )
+        if not ids_to_check:
+            delay_on_commit(vacuum_tagkeys, 0)
+            return
+        used_in_event = set(
+            EventTag.objects.filter(value_id__in=ids_to_check).values_list('value_id', flat=True)
+        )
+        used_in_issue = set(
+            IssueTag.objects.filter(value_id__in=ids_to_check).values_list('value_id', flat=True)
+        )
+        unused = [pk for pk in ids_to_check if pk not in used_in_event and pk not in used_in_issue]
+        if unused:
+            TagValue.objects.filter(id__in=unused).delete()
+    vacuum_tagvalues.delay(ids_to_check[-1])
+@shared_task
+def vacuum_tagkeys(min_id=0):
+    with immediate_atomic():
+        ids_to_check = list(
+            TagKey.objects
+            .filter(id__gt=min_id)
+            .order_by('id')
+            .values_list('id', flat=True)[:BATCH_SIZE]
+        )
+        if not ids_to_check:
+            return  # done
+        used = set(
+            TagValue.objects.filter(key_id__in=ids_to_check).values_list('key_id', flat=True)
+        )
+        unused = [pk for pk in ids_to_check if pk not in used]
+        if unused:
+            TagKey.objects.filter(id__in=unused).delete()
+    vacuum_tagkeys.delay(ids_to_check[-1])
+@shared_task
+def vacuum_eventless_issuetags(min_id=0):
+    BATCH_SIZE = 2048
+    INNER_BATCH_SIZE = 64
+    with immediate_atomic():
+        issue_tag_infos = list(
+            IssueTag.objects
+            .filter(id__gt=min_id)
+            .order_by('id')
+            .values('id', 'issue_id', 'value_id')[:BATCH_SIZE]
+        )
+        for issue_tag_infos_batch in batched(issue_tag_infos, INNER_BATCH_SIZE):
+            matching_eventtags = _or_join([
+                Q(issue_id=it['issue_id'], value_id=it['value_id']) for it in issue_tag_infos_batch
+            ])
+            if matching_eventtags:
+                in_use_issue_value_pairs = set(
+                    EventTag.objects
+                    .filter(matching_eventtags)
+                    .values_list('issue_id', 'value_id')
+                )
+            else:
+                in_use_issue_value_pairs = set()
+            stale_issuetags = [
+                it
+                for it in issue_tag_infos_batch
+                if (it['issue_id'], it['value_id']) not in in_use_issue_value_pairs
+            ]
+            if stale_issuetags:
+                IssueTag.objects.filter(id__in=[it['id'] for it in stale_issuetags]).delete()
+                prune_tagvalues([it['value_id'] for it in stale_issuetags])
+    if not issue_tag_infos:
+        return
+    vacuum_eventless_issuetags.delay(issue_tag_infos[-1]['id'])

--- a//dev/null
+++ b/teams/views.py
@@ -0,0 +1,257 @@
+from datetime import timedelta
+from django.db import models
+from django.shortcuts import render, redirect
+from django.contrib.auth import get_user_model
+from django.http import Http404, HttpResponseRedirect
+from django.core.exceptions import PermissionDenied
+from django.utils import timezone
+from django.urls import reverse
+from django.contrib import messages
+from django.contrib.auth import logout
+from users.models import EmailVerification
+from bugsink.app_settings import get_settings, CB_ANYBODY, CB_ADMINS, CB_MEMBERS
+from bugsink.decorators import login_exempt, atomic_for_request_method
+from .models import Team, TeamMembership, TeamRole, TeamVisibility
+from .forms import TeamMemberInviteForm, TeamMembershipForm, MyTeamMembershipForm, TeamForm
+from .tasks import send_team_invite_email, send_team_invite_email_new_user
+User = get_user_model()
+@atomic_for_request_method
+def team_list(request, ownership_filter=None):
+    my_memberships = TeamMembership.objects.filter(user=request.user)
+    my_teams = Team.objects.filter(teammembership__in=my_memberships)
+    if request.user.is_superuser:
+        other_teams = Team.objects\
+            .exclude(teammembership__in=my_memberships)\
+            .order_by('name').distinct()
+    else:
+        other_teams = Team.objects\
+            .exclude(teammembership__in=my_memberships)\
+            .exclude(visibility=TeamVisibility.HIDDEN)\
+            .order_by('name').distinct()
+    if ownership_filter is None:
+        if my_teams.exists() or not other_teams.exists():
+            return redirect('team_list_mine')
+        return redirect('team_list_other')
+    if request.method == 'POST':
+        full_action_str = request.POST.get('action')
+        action, team_pk = full_action_str.split(":", 1)
+        if action == "leave":
+            TeamMembership.objects.filter(team=team_pk, user=request.user.id).delete()
+        elif action == "join":
+            team = Team.objects.get(id=team_pk)
+            if not team.is_joinable() and not request.user.is_superuser:
+                raise PermissionDenied("This team is not joinable")
+            messages.success(request, 'You have joined the team "%s"' % team.name)
+            TeamMembership.objects.create(team_id=team_pk, user_id=request.user.id, role=TeamRole.MEMBER, accepted=True)
+            return redirect('team_member_settings', team_pk=team_pk, user_pk=request.user.id)
+    if ownership_filter == "mine":
+        base_qs = my_teams
+    elif ownership_filter == "other":
+        base_qs = other_teams
+    else:
+        raise ValueError("Invalid ownership_filter")
+    team_list = base_qs.annotate(
+        project_count=models.Count('project', distinct=True),
+        member_count=models.Count('teammembership', distinct=True, filter=models.Q(teammembership__accepted=True)),
+    )
+    if ownership_filter == "mine":
+        my_memberships_dict = {m.team_id: m for m in my_memberships}
+        team_list_2 = []
+        for team in team_list:
+            team.member = my_memberships_dict.get(team.id)
+            team_list_2.append(team)
+        team_list = team_list_2
+    return render(request, 'teams/team_list.html', {
+        'can_create':
+            get_settings().TEAM_CREATION in [CB_ANYBODY, CB_MEMBERS] or
+            (request.user.is_superuser and get_settings().TEAM_CREATION == CB_ADMINS),
+        'ownership_filter': ownership_filter,
+        'team_list': team_list,
+    })
+@atomic_for_request_method
+def team_new(request):
+    if not (get_settings().TEAM_CREATION in [CB_ANYBODY, CB_MEMBERS] or
+            (request.user.is_superuser and get_settings().TEAM_CREATION == CB_ADMINS)):
+        raise PermissionDenied("You are not allowed to create teams")
+    if request.method == 'POST':
+        form = TeamForm(request.POST)
+        if form.is_valid():
+            team = form.save()
+            TeamMembership.objects.create(team=team, user=request.user, role=TeamRole.ADMIN, accepted=True)
+            return redirect('team_members', team_pk=team.id)
+    else:
+        form = TeamForm()
+    return render(request, 'teams/team_new.html', {
+        'form': form,
+    })
+@atomic_for_request_method
+def team_edit(request, team_pk):
+    team = Team.objects.get(id=team_pk)
+    if (not TeamMembership.objects.filter(team=team, user=request.user, role=TeamRole.ADMIN, accepted=True).exists() and
+            not request.user.is_superuser):
+        raise PermissionDenied("You are not an admin of this team")
+    if request.method == 'POST':
+        action = request.POST.get('action')
+        if action == 'delete':
+            if not (TeamMembership.objects.filter(team=team, user=request.user, role=TeamRole.ADMIN, accepted=True).exists() or
+                    request.user.is_superuser):
+                raise PermissionDenied("Only team admins can delete teams")
+            team.project_set.all().delete()
+            team.delete()
+            messages.success(request, f'Team "{team.name}" has been deleted successfully.')
+            return redirect('team_list')
+        form = TeamForm(request.POST, instance=team)
+        if form.is_valid():
+            form.save()
+            return redirect('team_members', team_pk=team.id)
+    else:
+        form = TeamForm(instance=team)
+    return render(request, 'teams/team_edit.html', {
+        'team': team,
+        'form': form,
+    })
+@atomic_for_request_method
+def team_members(request, team_pk):
+    team = Team.objects.get(id=team_pk)
+    if (not TeamMembership.objects.filter(team=team, user=request.user, role=TeamRole.ADMIN, accepted=True).exists() and
+            not request.user.is_superuser):
+        raise PermissionDenied("You are not an admin of this team")
+    if request.method == 'POST':
+        full_action_str = request.POST.get('action')
+        action, user_id = full_action_str.split(":", 1)
+        if action == "remove":
+            TeamMembership.objects.filter(team=team_pk, user=user_id).delete()
+        elif action == "reinvite":
+            user = User.objects.get(id=user_id)
+            _send_team_invite_email(user, team_pk)
+            messages.success(request, f"Invitation resent to {user.email}")
+    return render(request, 'teams/team_members.html', {
+        'team': team,
+        'members': team.teammembership_set.all().select_related('user'),
+    })
+def _send_team_invite_email(user, team_pk):
+    """Send an email to a user inviting them to a team; (for new users this includes the email-verification link)"""
+    if user.is_active:
+        send_team_invite_email.delay(user.email, team_pk)
+    else:
+        verification = EmailVerification.objects.create(user=user, email=user.username)
+        send_team_invite_email_new_user.delay(user.email, team_pk, verification.token)
+@atomic_for_request_method
+def team_members_invite(request, team_pk):
+    team = Team.objects.get(id=team_pk)
+    if (not TeamMembership.objects.filter(team=team, user=request.user, role=TeamRole.ADMIN, accepted=True).exists() and
+            not request.user.is_superuser):
+        raise PermissionDenied("You are not an admin of this team")
+    if get_settings().USER_REGISTRATION in [CB_ANYBODY, CB_MEMBERS]:
+        user_must_exist = False
+    elif get_settings().USER_REGISTRATION == CB_ADMINS and request.user.is_superuser:
+        user_must_exist = False
+    else:
+        user_must_exist = True
+    if request.method == 'POST':
+        form = TeamMemberInviteForm(user_must_exist, request.POST)
+        if form.is_valid():
+            email = form.cleaned_data['email']
+            user, user_created = User.objects.get_or_create(
+                email=email, defaults={'username': email, 'is_active': False})
+            _send_team_invite_email(user, team_pk)
+            _, membership_created = TeamMembership.objects.get_or_create(team=team, user=user, defaults={
+                'role': form.cleaned_data['role'],
+                'accepted': False,
+            })
+            if membership_created:
+                messages.success(request, f"Invitation sent to {email}")
+            else:
+                messages.success(
+                    request, f"Invitation resent to {email} (it was previously sent and we just sent it again)")
+            if request.POST.get('action') == "invite_and_add_another":
+                return redirect('team_members_invite', team_pk=team_pk)
+            return redirect('team_members', team_pk=team_pk)
+    else:
+        form = TeamMemberInviteForm(user_must_exist)
+    return render(request, 'teams/team_members_invite.html', {
+        'team': team,
+        'form': form,
+    })
+@atomic_for_request_method
+def team_member_settings(request, team_pk, user_pk):
+    try:
+        your_membership = TeamMembership.objects.get(team=team_pk, user=request.user)
+    except TeamMembership.DoesNotExist:
+        raise PermissionDenied("You are not a member of this team")
+    if not your_membership.accepted:
+        return redirect("team_members_accept", team_pk=team_pk)
+    this_is_you = str(user_pk) == str(request.user.id)
+    if not this_is_you:
+        if not request.user.is_superuser or not your_membership.role == TeamRole.ADMIN:
+            raise PermissionDenied("You are not an admin of this team")
+        membership = TeamMembership.objects.get(team=team_pk, user=user_pk)
+        create_form = lambda data: TeamMembershipForm(data, instance=membership)  # noqa
+    else:
+        edit_role = your_membership.role == TeamRole.ADMIN or request.user.is_superuser
+        create_form = lambda data: MyTeamMembershipForm(data=data, instance=your_membership, edit_role=edit_role)  # noqa
+    if request.method == 'POST':
+        form = create_form(request.POST)
+        if form.is_valid():
+            form.save()
+            if this_is_you:
+                return redirect('team_list')
+            return redirect('team_members', team_pk=team_pk)
+    else:
+        form = create_form(None)
+    return render(request, 'teams/team_member_settings.html', {
+        'this_is_you': this_is_you,
+        'user': User.objects.get(id=user_pk),
+        'team': Team.objects.get(id=team_pk),
+        'form': form,
+    })
+@atomic_for_request_method
+@login_exempt  # no login is required, the token is what identifies the user
+def team_members_accept_new_user(request, team_pk, token):
+    EmailVerification.objects.filter(
+        created_at__lt=timezone.now() - timedelta(get_settings().USER_REGISTRATION_VERIFY_EMAIL_EXPIRY)).delete()
+    try:
+        verification = EmailVerification.objects.get(token=token)
+    except EmailVerification.DoesNotExist:
+        raise Http404("Invalid or expired token")
+    user = verification.user
+    if not user.has_usable_password() or not user.is_active:
+        return HttpResponseRedirect(reverse("reset_password", kwargs={"token": token}) + "?next=" + reverse(
+            team_members_accept, kwargs={"team_pk": team_pk})
+        )
+    if request.user.is_authenticated and request.user != user:
+        logout(request)
+    verification.delete()
+    return redirect("team_members_accept", team_pk=team_pk)
+@atomic_for_request_method
+def team_members_accept(request, team_pk):
+    team = Team.objects.get(id=team_pk)
+    membership = TeamMembership.objects.get(team=team, user=request.user)
+    if membership.accepted:
+        return redirect("team_member_settings", team_pk=team_pk, user_pk=request.user.id)
+    if request.method == 'POST':
+        if request.POST["action"] == "decline":
+            membership.delete()
+            return redirect("home")
+        if request.POST["action"] == "accept":
+            membership.accepted = True
+            membership.save()
+            return redirect("team_member_settings", team_pk=team_pk, user_pk=request.user.id)
+        raise Http404("Invalid action")
+    return render(request, "teams/team_members_accept.html", {"team": team, "membership": membership})
+DEBUG_CONTEXTS = {
+    "team_membership_invite_new_user": {
+        "site_title": get_settings().SITE_TITLE,
+        "base_url": get_settings().BASE_URL + "/",
+        "team_name": "Some team",
+        "url": "http://example.com/confirm-email/1234567890abcdef",  # nonsense to avoid circular import
+    },
+    "team_membership_invite": {
+        "site_title": get_settings().SITE_TITLE,
+        "base_url": get_settings().BASE_URL + "/",
+        "team_name": "Some team",
+        "url": "http://example.com/confirm-email/1234567890abcdef",  # nonsense to avoid circular import
+    },
+}
+def debug_email(request, template_name):
+    return render(request, "mails/" + template_name + ".html", DEBUG_CONTEXTS[template_name])

--- a//dev/null
+++ b/theme/static_src/tailwind.config.js
@@ -0,0 +1,68 @@
+/**
+ * This is a minimal config.
+ *
+ * If you need the full config, get it from here:
+ * https://unpkg.com/browse/tailwindcss@latest/stubs/defaultConfig.stub.js
+ */
+const defaultTheme = require("tailwindcss/defaultTheme");
+module.exports = {
+    darkMode: ['selector', '[data-theme="dark"]'],
+    content: [
+        /**
+         * HTML. Paths to Django template files that will contain Tailwind CSS classes.
+         */
+        /*  Templates within theme app (<tailwind_app_name>/templates), e.g. base.html. */
+        '../templates/**/*.html',
+        /*
+         * Main templates directory of the project (BASE_DIR/templates).
+         * Adjust the following line to match your project structure.
+         */
+        '../../templates/**/*.html',
+        /*
+         * Templates in other django apps (BASE_DIR/<any_app_name>/templates).
+         * Adjust the following line to match your project structure.
+         */
+        '../../**/templates/**/*.html',
+        /**
+         * JS: If you use Tailwind CSS in JavaScript, uncomment the following lines and make sure
+         * patterns match your project structure.
+         */
+        /* JS 1: Ignore any JavaScript in node_modules folder. */
+        /* JS 2: Process all JavaScript files in the project. */
+        /**
+         * Python: If you use Tailwind CSS classes in Python, uncomment the following line
+         * and make sure the pattern below matches your project structure.
+         */
+        "../../issues/views.py",
+        "../../theme/templatetags/code.py",
+        "../../theme/templatetags/tailwind_forms.py",
+    ],
+    theme: {
+      extend: {
+        spacing: {
+          '128': '32rem',
+        },
+        fontFamily: {
+          sans: [
+            '"IBM Plex Sans"',
+            ...defaultTheme.fontFamily.sans,
+          ],
+          mono: [
+            '"IBM Plex Mono"',
+            ...defaultTheme.fontFamily.mono,
+          ],
+        },
+      },
+    },
+    plugins: [
+        /**
+         * '@tailwindcss/forms' is the forms plugin that provides a minimal styling
+         * for forms. If you don't like it or have own styling for forms,
+         * comment the line below to disable '@tailwindcss/forms'.
+         */
+        require('@tailwindcss/forms'),
+        require('@tailwindcss/typography'),
+        require('@tailwindcss/line-clamp'),
+        require('@tailwindcss/aspect-ratio'),
+    ],
+}

--- a//dev/null
+++ b/theme/templatetags/code.py
@@ -0,0 +1,24 @@
+from pygments import highlight
+from pygments.lexers import get_lexer_by_name
+from pygments.formatters import HtmlFormatter
+from django import template
+register = template.Library()
+@register.tag(name="code")
+def do_code(parser, token):
+    nodelist = parser.parse(("endcode",))
+    parser.delete_first_token()
+    return CodeNode(nodelist)
+class CodeNode(template.Node):
+    def __init__(self, nodelist):
+        self.nodelist = nodelist
+    def render(self, context):
+        content = self.nodelist.render(context)
+        content = "\n".join([line.rstrip() for line in content.split("\n")])
+        lang_identifier, code = content.split("\n", 1)
+        assert lang_identifier.startswith(":::") or lang_identifier.startswith("#!"), \
+            "Expected code block identifier ':::' or '#!' not " + lang_identifier
+        lang = lang_identifier[3:].strip() if lang_identifier.startswith(":::") else lang_identifier[2:].strip()
+        is_shebang = lang_identifier.startswith("#!")
+        formatter = HtmlFormatter(linenos="table" if is_shebang else False)
+        lexer = get_lexer_by_name(lang, stripall=True)
+        return highlight(code, lexer, formatter).replace("highlight", "p-4 mt-4 bg-slate-50 dark:bg-slate-800 syntax-coloring")

--- a//dev/null
+++ b/theme/templatetags/tailwind_forms.py
@@ -0,0 +1,20 @@
+from django import template
+register = template.Library()
+@register.inclusion_tag('tailwind_forms/formfield.html')
+def tailwind_formfield(formfield, implicit=False):
+    if not formfield:
+        return {"formfield": None}
+    if formfield.errors:
+        formfield.field.widget.attrs['class'] = "bg-red-50 dark:bg-red-900"
+    else:
+        formfield.field.widget.attrs['class'] = "bg-slate-50 dark:bg-slate-800"
+    formfield.field.widget.attrs['class'] += " pl-4 py-2 md:py-4 focus:outline-none w-full"
+    if implicit:
+        formfield.field.widget.attrs['placeholder'] = formfield.label
+    return {
+        'formfield': formfield,
+        'implicit': implicit,
+    }
+@register.inclusion_tag('tailwind_forms/formfield.html')
+def tailwind_formfield_implicit(formfield):
+    return tailwind_formfield(formfield, True)

--- a//dev/null
+++ b/users/forms.py
@@ -0,0 +1,96 @@
+import urllib.parse
+from django import forms
+from django.urls import reverse
+from django.contrib.auth.forms import UserCreationForm as BaseUserCreationForm, SetPasswordForm as BaseSetPasswordForm
+from django.core.validators import EmailValidator
+from django.contrib.auth import get_user_model
+from django.core.exceptions import ValidationError
+from django.contrib.auth import password_validation
+from django.forms import ModelForm
+from django.utils.html import escape, mark_safe
+TRUE_FALSE_CHOICES = (
+    (True, 'Yes'),
+    (False, 'No')
+)
+def _(x):
+    return x
+User = get_user_model()
+class UserCreationForm(BaseUserCreationForm):
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.fields['username'].validators = [EmailValidator()]
+        self.fields['username'].label = "Email"
+        self.fields['username'].help_text = None  # "Email" is descriptive enough
+        self.fields['password1'].help_text = "At least 8 characters"
+        self.fields['password2'].help_text = None  # "Confirm password" is descriptive enough
+    class Meta:
+        model = User
+        fields = ("username",)
+    def clean_username(self):
+        if User.objects.filter(username=self.cleaned_data['username'], is_active=False).exists():
+            raise ValidationError(mark_safe(
+                'This email is already registered but not yet confirmed. Please check your email for the confirmation '
+                'link or <b><a href="' + reverse("resend_confirmation") + "?email=" +
+                urllib.parse.quote(escape(self.cleaned_data['username'])) + '">request it again</a></b>.'))
+        return self.cleaned_data['username']
+    def _post_clean(self):
+        ModelForm._post_clean(self)  # commented out because we want to skip the direct superclass
+        password = self.cleaned_data.get("password1")
+        if password:
+            try:
+                password_validation.validate_password(password, self.instance)
+            except ValidationError as error:
+                self.add_error("password1", error)
+    def save(self, **kwargs):
+        commit = kwargs.pop("commit", True)
+        user = super().save(commit=False)
+        user.email = user.username
+        if commit:
+            user.save()
+        return user
+class UserEditForm(ModelForm):
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.fields['username'].validators = [EmailValidator()]
+        self.fields['username'].label = "Email"
+        self.fields['username'].help_text = None  # "Email" is descriptive enough
+    class Meta:
+        model = User
+        fields = ("username",)
+    def clean_username(self):
+        if User.objects.exclude(pk=self.instance.pk).filter(username=self.cleaned_data['username']).exists():
+            raise ValidationError(mark_safe("This email is already registered by another user."))
+        return self.cleaned_data['username']
+    def save(self, **kwargs):
+        commit = kwargs.pop("commit", True)
+        user = super().save(commit=False)
+        user.email = user.username
+        if commit:
+            user.save()
+        return user
+class ResendConfirmationForm(forms.Form):
+    email = forms.EmailField()
+class RequestPasswordResetForm(forms.Form):
+    email = forms.EmailField()
+    def clean_email(self):
+        email = self.cleaned_data['email']
+        if not User.objects.filter(username=email).exists():
+            raise ValidationError("This email is not registered.")
+        return email
+class SetPasswordForm(BaseSetPasswordForm):
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.fields['new_password1'].help_text = "At least 8 characters"
+        self.fields['new_password2'].help_text = None  # "Confirm password" is descriptive enough
+class PreferencesForm(ModelForm):
+    send_email_alerts = forms.ChoiceField(
+        label=_("Send email alerts"), choices=TRUE_FALSE_CHOICES, required=False, widget=forms.Select())
+    theme_preference = forms.ChoiceField(
+        label=_("Theme preference"),
+        choices=User.THEME_CHOICES,
+        required=True,
+        widget=forms.Select(),
+    )
+    class Meta:
+        model = User
+        fields = ("send_email_alerts", "theme_preference",)

--- a//dev/null
+++ b/users/migrations/0002_user_theme_preference.py
@@ -0,0 +1,12 @@
+from django.db import migrations, models
+class Migration(migrations.Migration):
+    dependencies = [
+        ('users', '0001_initial'),
+    ]
+    operations = [
+        migrations.AddField(
+            model_name='user',
+            name='theme_preference',
+            field=models.CharField(choices=[('system', 'System Default'), ('light', 'Light'), ('dark', 'Dark')], default='system', max_length=10),
+        ),
+    ]

--- a//dev/null
+++ b/users/models.py
@@ -0,0 +1,26 @@
+import secrets
+from django.db import models
+from django.contrib.auth.models import AbstractUser
+from django.conf import settings
+class User(AbstractUser):
+    send_email_alerts = models.BooleanField(default=True, blank=True)
+    THEME_CHOICES = [
+        ("system", "System Default"),
+        ("light", "Light"),
+        ("dark", "Dark"),
+    ]
+    theme_preference = models.CharField(
+        max_length=10,
+        choices=THEME_CHOICES,
+        default="system",
+        blank=False,
+    )
+    class Meta:
+        db_table = 'auth_user'
+class EmailVerification(models.Model):
+    user = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.CASCADE)
+    email = models.EmailField()  # redundant, but future-proof for when we allow multiple emails per user
+    token = models.CharField(max_length=64, default=secrets.token_urlsafe, blank=False, null=False)
+    created_at = models.DateTimeField(auto_now_add=True)
+    def __str__(self):
+        return f"{self.user} ({self.email})"
