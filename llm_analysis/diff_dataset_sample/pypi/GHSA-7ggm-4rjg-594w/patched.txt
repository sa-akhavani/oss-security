# ====================================================================
# FILE: litellm/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 501-541 ---
   501| from .llms.cohere import CohereConfig
   502| from .llms.ai21 import AI21Config
   503| from .llms.together_ai import TogetherAIConfig
   504| from .llms.cloudflare import CloudflareConfig
   505| from .llms.palm import PalmConfig
   506| from .llms.gemini import GeminiConfig
   507| from .llms.nlp_cloud import NLPCloudConfig
   508| from .llms.aleph_alpha import AlephAlphaConfig
   509| from .llms.petals import PetalsConfig
   510| from .llms.vertex_ai import VertexAIConfig
   511| from .llms.sagemaker import SagemakerConfig
   512| from .llms.ollama import OllamaConfig
   513| from .llms.maritalk import MaritTalkConfig
   514| from .llms.bedrock import (
   515|     AmazonTitanConfig,
   516|     AmazonAI21Config,
   517|     AmazonAnthropicConfig,
   518|     AmazonCohereConfig,
   519|     AmazonLlamaConfig,
   520|     AmazonStabilityConfig,
   521|     AmazonMistralConfig
   522| )
   523| from .llms.openai import OpenAIConfig, OpenAITextCompletionConfig
   524| from .llms.azure import AzureOpenAIConfig, AzureOpenAIError
   525| from .main import *  # type: ignore
   526| from .integrations import *
   527| from .exceptions import (
   528|     AuthenticationError,
   529|     InvalidRequestError,
   530|     BadRequestError,
   531|     NotFoundError,
   532|     RateLimitError,
   533|     ServiceUnavailableError,
   534|     OpenAIError,
   535|     ContextWindowExceededError,
   536|     ContentPolicyViolationError,
   537|     BudgetExceededError,
   538|     APIError,
   539|     Timeout,
   540|     APIConnectionError,
   541|     APIResponseValidationError,


# ====================================================================
# FILE: litellm/llms/anthropic.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-58 ---
     1| import os, types
     2| import json
     3| from enum import Enum
     4| import requests
     5| import time
     6| from typing import Callable, Optional
     7| from litellm.utils import ModelResponse, Usage
     8| import litellm
     9| from .prompt_templates.factory import prompt_factory, custom_prompt
    10| import httpx
    11| class AnthropicConstants(Enum):
    12|     HUMAN_PROMPT = "\n\nHuman: "
    13|     AI_PROMPT = "\n\nAssistant: "
    14| class AnthropicError(Exception):
    15|     def __init__(self, status_code, message):
    16|         self.status_code = status_code
    17|         self.message = message
    18|         self.request = httpx.Request(
    19|             method="POST", url="https://api.anthropic.com/v1/messages"
    20|         )
    21|         self.response = httpx.Response(status_code=status_code, request=self.request)
    22|         super().__init__(
    23|             self.message
    24|         )  # Call the base class constructor with the parameters it needs
    25| class AnthropicConfig:
    26|     """
    27|     Reference: https://docs.anthropic.com/claude/reference/complete_post
    28|     to pass metadata to anthropic, it's {"user_id": "any-relevant-information"}
    29|     """
    30|     max_tokens: Optional[int] = litellm.max_tokens  # anthropic requires a default
    31|     stop_sequences: Optional[list] = None
    32|     temperature: Optional[int] = None
    33|     top_p: Optional[int] = None
    34|     top_k: Optional[int] = None
    35|     metadata: Optional[dict] = None
    36|     def __init__(
    37|         self,
    38|         max_tokens: Optional[int] = 256,  # anthropic requires a default
    39|         stop_sequences: Optional[list] = None,
    40|         temperature: Optional[int] = None,
    41|         top_p: Optional[int] = None,
    42|         top_k: Optional[int] = None,
    43|         metadata: Optional[dict] = None,
    44|     ) -> None:
    45|         locals_ = locals()
    46|         for key, value in locals_.items():
    47|             if key != "self" and value is not None:
    48|                 setattr(self.__class__, key, value)
    49|     @classmethod
    50|     def get_config(cls):
    51|         return {
    52|             k: v
    53|             for k, v in cls.__dict__.items()
    54|             if not k.startswith("__")
    55|             and not isinstance(
    56|                 v,
    57|                 (
    58|                     types.FunctionType,

# --- HUNK 2: Lines 88-205 ---
    88|     api_key,
    89|     logging_obj,
    90|     optional_params=None,
    91|     litellm_params=None,
    92|     logger_fn=None,
    93|     headers={},
    94| ):
    95|     headers = validate_environment(api_key, headers)
    96|     if model in custom_prompt_dict:
    97|         model_prompt_details = custom_prompt_dict[model]
    98|         prompt = custom_prompt(
    99|             role_dict=model_prompt_details["roles"],
   100|             initial_prompt_value=model_prompt_details["initial_prompt_value"],
   101|             final_prompt_value=model_prompt_details["final_prompt_value"],
   102|             messages=messages,
   103|         )
   104|     else:
   105|         prompt = prompt_factory(
   106|             model=model, messages=messages, custom_llm_provider="anthropic"
   107|         )
   108|     """
   109|     format messages for anthropic
   110|     1. Anthropic supports roles like "user" and "assistant", (here litellm translates system-> assistant)
   111|     2. The first message always needs to be of role "user"
   112|     3. Each message must alternate between "user" and "assistant" (this is not addressed as now by litellm)
   113|     4. final assistant content cannot end with trailing whitespace (anthropic raises an error otherwise)
   114|     """
   115|     for idx, message in enumerate(messages):
   116|         if message["role"] == "system":
   117|             message["role"] = "assistant"
   118|         if message["role"] == "assistant":
   119|             message["content"] = message["content"].strip()
   120|     if len(messages) > 0:
   121|         if messages[0]["role"] != "user":
   122|             for i, message in enumerate(messages):
   123|                 if message["role"] == "user":
   124|                     break
   125|             messages.pop(i)
   126|             messages = [message] + messages
   127|     config = litellm.AnthropicConfig.get_config()
   128|     for k, v in config.items():
   129|         if (
   130|             k not in optional_params
   131|         ):  # completion(top_k=3) > anthropic_config(top_k=3) <- allows for dynamic variables to be passed in
   132|             optional_params[k] = v
   133|     data = {
   134|         "model": model,
   135|         "messages": messages,
   136|         **optional_params,
   137|     }
   138|     logging_obj.pre_call(
   139|         input=prompt,
   140|         api_key=api_key,
   141|         additional_args={
   142|             "complete_input_dict": data,
   143|             "api_base": api_base,
   144|             "headers": headers,
   145|         },
   146|     )
   147|     if "stream" in optional_params and optional_params["stream"] == True:
   148|         response = requests.post(
   149|             api_base,
   150|             headers=headers,
   151|             data=json.dumps(data),
   152|             stream=optional_params["stream"],
   153|         )
   154|         if response.status_code != 200:
   155|             raise AnthropicError(
   156|                 status_code=response.status_code, message=response.text
   157|             )
   158|         return response.iter_lines()
   159|     else:
   160|         response = requests.post(api_base, headers=headers, data=json.dumps(data))
   161|         if response.status_code != 200:
   162|             raise AnthropicError(
   163|                 status_code=response.status_code, message=response.text
   164|             )
   165|         logging_obj.post_call(
   166|             input=messages,
   167|             api_key=api_key,
   168|             original_response=response.text,
   169|             additional_args={"complete_input_dict": data},
   170|         )
   171|         print_verbose(f"raw model_response: {response.text}")
   172|         try:
   173|             completion_response = response.json()
   174|         except:
   175|             raise AnthropicError(
   176|                 message=response.text, status_code=response.status_code
   177|             )
   178|         if "error" in completion_response:
   179|             raise AnthropicError(
   180|                 message=str(completion_response["error"]),
   181|                 status_code=response.status_code,
   182|             )
   183|         elif len(completion_response["content"]) == 0:
   184|             raise AnthropicError(
   185|                 message="No content in response",
   186|                 status_code=response.status_code,
   187|             )
   188|         else:
   189|             text_content = completion_response["content"][0].get("text", None)
   190|             model_response.choices[0].message.content = text_content  # type: ignore
   191|             model_response.choices[0].finish_reason = completion_response["stop_reason"]
   192|         prompt_tokens = completion_response["usage"]["input_tokens"]
   193|         completion_tokens = completion_response["usage"]["output_tokens"]
   194|         total_tokens = prompt_tokens + completion_tokens
   195|         model_response["created"] = int(time.time())
   196|         model_response["model"] = model
   197|         usage = Usage(
   198|             prompt_tokens=prompt_tokens,
   199|             completion_tokens=completion_tokens,
   200|             total_tokens=prompt_tokens + completion_tokens,
   201|         )
   202|         model_response.usage = usage
   203|         return model_response
   204| def embedding():
   205|     pass


# ====================================================================
# FILE: litellm/llms/bedrock.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 196-279 ---
   196|         }
   197| class AnthropicConstants(Enum):
   198|     HUMAN_PROMPT = "\n\nHuman: "
   199|     AI_PROMPT = "\n\nAssistant: "
   200| class AmazonLlamaConfig:
   201|     """
   202|     Reference: https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/providers?model=meta.llama2-13b-chat-v1
   203|     Supported Params for the Amazon / Meta Llama models:
   204|     - `max_gen_len` (integer) max tokens,
   205|     - `temperature` (float) temperature for model,
   206|     - `top_p` (float) top p for model
   207|     """
   208|     max_gen_len: Optional[int] = None
   209|     temperature: Optional[float] = None
   210|     topP: Optional[float] = None
   211|     def __init__(
   212|         self,
   213|         maxTokenCount: Optional[int] = None,
   214|         temperature: Optional[float] = None,
   215|         topP: Optional[int] = None,
   216|     ) -> None:
   217|         locals_ = locals()
   218|         for key, value in locals_.items():
   219|             if key != "self" and value is not None:
   220|                 setattr(self.__class__, key, value)
   221|     @classmethod
   222|     def get_config(cls):
   223|         return {
   224|             k: v
   225|             for k, v in cls.__dict__.items()
   226|             if not k.startswith("__")
   227|             and not isinstance(
   228|                 v,
   229|                 (
   230|                     types.FunctionType,
   231|                     types.BuiltinFunctionType,
   232|                     classmethod,
   233|                     staticmethod,
   234|                 ),
   235|             )
   236|             and v is not None
   237|         }
   238| class AmazonMistralConfig:
   239|     """
   240|     Reference: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-mistral.html
   241|     Supported Params for the Amazon / Mistral models:
   242|     - `max_tokens` (integer) max tokens,
   243|     - `temperature` (float) temperature for model,
   244|     - `top_p` (float) top p for model
   245|     - `stop` [string] A list of stop sequences that if generated by the model, stops the model from generating further output.
   246|     - `top_k` (float) top k for model
   247|     """
   248|     max_tokens: Optional[int] = None
   249|     temperature: Optional[float] = None
   250|     top_p: Optional[float] = None
   251|     top_k: Optional[float] = None
   252|     stop: Optional[list[str]] = None
   253|     def __init__(
   254|         self,
   255|         max_tokens: Optional[int] = None,
   256|         temperature: Optional[float] = None,
   257|         top_p: Optional[int] = None,
   258|         top_k: Optional[float] = None,
   259|         stop: Optional[list[str]] = None,
   260|     ) -> None:
   261|         locals_ = locals()
   262|         for key, value in locals_.items():
   263|             if key != "self" and value is not None:
   264|                 setattr(self.__class__, key, value)
   265|     @classmethod
   266|     def get_config(cls):
   267|         return {
   268|             k: v
   269|             for k, v in cls.__dict__.items()
   270|             if not k.startswith("__")
   271|             and not isinstance(
   272|                 v,
   273|                 (
   274|                     types.FunctionType,
   275|                     types.BuiltinFunctionType,
   276|                     classmethod,
   277|                     staticmethod,
   278|                 ),
   279|             )

# --- HUNK 2: Lines 428-469 ---
   428|             service_name="bedrock-runtime",
   429|             region_name=region_name,
   430|             endpoint_url=endpoint_url,
   431|             config=config,
   432|         )
   433|     return client
   434| def convert_messages_to_prompt(model, messages, provider, custom_prompt_dict):
   435|     if provider == "anthropic" or provider == "amazon":
   436|         if model in custom_prompt_dict:
   437|             model_prompt_details = custom_prompt_dict[model]
   438|             prompt = custom_prompt(
   439|                 role_dict=model_prompt_details["roles"],
   440|                 initial_prompt_value=model_prompt_details["initial_prompt_value"],
   441|                 final_prompt_value=model_prompt_details["final_prompt_value"],
   442|                 messages=messages,
   443|             )
   444|         else:
   445|             prompt = prompt_factory(
   446|                 model=model, messages=messages, custom_llm_provider="bedrock"
   447|             )
   448|     elif provider == "mistral":
   449|         prompt = prompt_factory(model=model, messages=messages, custom_llm_provider="bedrock")
   450|     else:
   451|         prompt = ""
   452|         for message in messages:
   453|             if "role" in message:
   454|                 if message["role"] == "user":
   455|                     prompt += f"{message['content']}"
   456|                 else:
   457|                     prompt += f"{message['content']}"
   458|             else:
   459|                 prompt += f"{message['content']}"
   460|     return prompt
   461| """
   462| BEDROCK AUTH Keys/Vars
   463| os.environ['AWS_ACCESS_KEY_ID'] = ""
   464| os.environ['AWS_SECRET_ACCESS_KEY'] = ""
   465| """
   466| def completion(
   467|     model: str,
   468|     messages: list,
   469|     custom_prompt_dict: dict,

# --- HUNK 3: Lines 541-588 ---
   541|             config = litellm.AmazonLlamaConfig.get_config()
   542|             for k, v in config.items():
   543|                 if (
   544|                     k not in inference_params
   545|                 ):  # completion(top_k=3) > anthropic_config(top_k=3) <- allows for dynamic variables to be passed in
   546|                     inference_params[k] = v
   547|             data = json.dumps({"prompt": prompt, **inference_params})
   548|         elif provider == "amazon":  # amazon titan
   549|             config = litellm.AmazonTitanConfig.get_config()
   550|             for k, v in config.items():
   551|                 if (
   552|                     k not in inference_params
   553|                 ):  # completion(top_k=3) > amazon_config(top_k=3) <- allows for dynamic variables to be passed in
   554|                     inference_params[k] = v
   555|             data = json.dumps(
   556|                 {
   557|                     "inputText": prompt,
   558|                     "textGenerationConfig": inference_params,
   559|                 }
   560|             )
   561|         elif provider == "mistral":  
   562|             config = litellm.AmazonMistralConfig.get_config()
   563|             for k, v in config.items():
   564|                 if (
   565|                     k not in inference_params
   566|                 ):  # completion(top_k=3) > amazon_config(top_k=3) <- allows for dynamic variables to be passed in
   567|                     inference_params[k] = v
   568|             data = json.dumps({"prompt": prompt, **inference_params})
   569|         else:
   570|             data = json.dumps({})
   571|         accept = "application/json"
   572|         contentType = "application/json"
   573|         if stream == True:
   574|             if provider == "ai21":
   575|                 request_str = f"""
   576|                 response = client.invoke_model(
   577|                     body={data},
   578|                     modelId={modelId},
   579|                     accept=accept,
   580|                     contentType=contentType
   581|                 )
   582|                 """
   583|                 logging_obj.pre_call(
   584|                     input=prompt,
   585|                     api_key="",
   586|                     additional_args={
   587|                         "complete_input_dict": data,
   588|                         "request_str": request_str,

# --- HUNK 4: Lines 642-684 ---
   642|         except Exception as e:
   643|             raise BedrockError(status_code=500, message=str(e))
   644|         response_body = json.loads(response.get("body").read())
   645|         logging_obj.post_call(
   646|             input=prompt,
   647|             api_key="",
   648|             original_response=json.dumps(response_body),
   649|             additional_args={"complete_input_dict": data},
   650|         )
   651|         print_verbose(f"raw model_response: {response}")
   652|         outputText = "default"
   653|         if provider == "ai21":
   654|             outputText = response_body.get("completions")[0].get("data").get("text")
   655|         elif provider == "anthropic":
   656|             outputText = response_body["completion"]
   657|             model_response["finish_reason"] = response_body["stop_reason"]
   658|         elif provider == "cohere":
   659|             outputText = response_body["generations"][0]["text"]
   660|         elif provider == "meta":
   661|             outputText = response_body["generation"]
   662|         elif provider == "mistral":
   663|             outputText = response_body["outputs"][0]["text"]
   664|             model_response["finish_reason"] = response_body["outputs"][0]["stop_reason"]
   665|         else:  # amazon titan
   666|             outputText = response_body.get("results")[0].get("outputText")
   667|         response_metadata = response.get("ResponseMetadata", {})
   668|         if response_metadata.get("HTTPStatusCode", 500) >= 400:
   669|             raise BedrockError(
   670|                 message=outputText,
   671|                 status_code=response_metadata.get("HTTPStatusCode", 500),
   672|             )
   673|         else:
   674|             try:
   675|                 if len(outputText) > 0:
   676|                     model_response["choices"][0]["message"]["content"] = outputText
   677|                 else:
   678|                     raise Exception()
   679|             except:
   680|                 raise BedrockError(
   681|                     message=json.dumps(outputText),
   682|                     status_code=response_metadata.get("HTTPStatusCode", 500),
   683|                 )
   684|         prompt_tokens = response_metadata.get(


# ====================================================================
# FILE: litellm/llms/huggingface_restapi.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 548-628 ---
   548|         self,
   549|         logging_obj,
   550|         api_base: str,
   551|         data: dict,
   552|         headers: dict,
   553|         model_response: ModelResponse,
   554|         model: str,
   555|         timeout: float,
   556|     ):
   557|         async with httpx.AsyncClient(timeout=timeout) as client:
   558|             response = client.stream(
   559|                 "POST", url=f"{api_base}", json=data, headers=headers
   560|             )
   561|             async with response as r:
   562|                 if r.status_code != 200:
   563|                     text = await r.aread()
   564|                     raise HuggingfaceError(
   565|                         status_code=r.status_code,
   566|                         message=str(text),
   567|                     )
   568|                 """
   569|                 Check first chunk for error message. 
   570|                 If error message, raise error. 
   571|                 If not - add back to stream
   572|                 """
   573|                 response_iterator = r.aiter_lines()
   574|                 try:
   575|                     first_chunk = await response_iterator.__anext__()
   576|                 except StopAsyncIteration:
   577|                     first_chunk = ""
   578|                 if (
   579|                     "error" in first_chunk.lower()
   580|                 ):  # Adjust this condition based on how error messages are structured
   581|                     raise HuggingfaceError(
   582|                         status_code=400,
   583|                         message=first_chunk,
   584|                     )
   585|                 return self.async_streaming_generator(
   586|                     first_chunk=first_chunk,
   587|                     response_iterator=response_iterator,
   588|                     model=model,
   589|                     logging_obj=logging_obj,
   590|                 )
   591|     async def async_streaming_generator(
   592|         self, first_chunk, response_iterator, model, logging_obj
   593|     ):
   594|         async def custom_stream_with_first_chunk():
   595|             yield first_chunk  # Yield back the first chunk
   596|             async for (
   597|                 chunk
   598|             ) in response_iterator:  # Continue yielding the rest of the chunks
   599|                 yield chunk
   600|         completion_stream = custom_stream_with_first_chunk()
   601|         streamwrapper = CustomStreamWrapper(
   602|             completion_stream=completion_stream,
   603|             model=model,
   604|             custom_llm_provider="huggingface",
   605|             logging_obj=logging_obj,
   606|         )
   607|         async for transformed_chunk in streamwrapper:
   608|             yield transformed_chunk
   609|     def embedding(
   610|         self,
   611|         model: str,
   612|         input: list,
   613|         api_key: Optional[str] = None,
   614|         api_base: Optional[str] = None,
   615|         logging_obj=None,
   616|         model_response=None,
   617|         encoding=None,
   618|     ):
   619|         super().embedding()
   620|         headers = self.validate_environment(api_key, headers=None)
   621|         embed_url = ""
   622|         if "https" in model:
   623|             embed_url = model
   624|         elif api_base:
   625|             embed_url = api_base
   626|         elif "HF_API_BASE" in os.environ:
   627|             embed_url = os.getenv("HF_API_BASE", "")
   628|         elif "HUGGINGFACE_API_BASE" in os.environ:


# ====================================================================
# FILE: litellm/llms/prompt_templates/factory.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 79-121 ---
    79|         return {"prompt": prompt, "images": images}
    80|     else:
    81|         prompt = "".join(
    82|             (
    83|                 m["content"]
    84|                 if isinstance(m["content"], str) is str
    85|                 else "".join(m["content"])
    86|             )
    87|             for m in messages
    88|         )
    89|     return prompt
    90| def mistral_instruct_pt(messages):
    91|     prompt = custom_prompt(
    92|         initial_prompt_value="<s>",
    93|         role_dict={
    94|             "system": {
    95|                 "pre_message": "[INST] \n",
    96|                 "post_message": " [/INST]\n",
    97|             },
    98|             "user": {"pre_message": "[INST] ", "post_message": " [/INST]\n"},
    99|             "assistant": {"pre_message": " ", "post_message": "</s> "},
   100|         },
   101|         final_prompt_value="",
   102|         messages=messages,
   103|     )
   104|     return prompt
   105| def mistral_api_pt(messages):
   106|     """
   107|     - handles scenario where content is list and not string
   108|     - content list is just text, and no images
   109|     - if image passed in, then just return as is (user-intended)
   110|     Motivation: mistral api doesn't support content as a list
   111|     """
   112|     new_messages = []
   113|     for m in messages:
   114|         texts = ""
   115|         if isinstance(m["content"], list):
   116|             for c in m["content"]:
   117|                 if c["type"] == "image_url":
   118|                     return messages
   119|                 elif c["type"] == "text" and isinstance(c["text"], str):
   120|                     texts += c["text"]
   121|         new_m = {"role": m["role"], "content": texts}

# --- HUNK 2: Lines 536-577 ---
   536|     elif custom_llm_provider == "together_ai":
   537|         prompt_format, chat_template = get_model_info(token=api_key, model=model)
   538|         return format_prompt_togetherai(
   539|             messages=messages, prompt_format=prompt_format, chat_template=chat_template
   540|         )
   541|     elif custom_llm_provider == "gemini":
   542|         if model == "gemini-pro-vision":
   543|             return _gemini_vision_convert_messages(messages=messages)
   544|         else:
   545|             return gemini_text_image_pt(messages=messages)
   546|     elif custom_llm_provider == "mistral":
   547|         return mistral_api_pt(messages=messages)
   548|     elif custom_llm_provider == "bedrock":
   549|         if "amazon.titan-text" in model:
   550|             return amazon_titan_pt(messages=messages)
   551|         elif "anthropic." in model:
   552|             if any(_ in model for _ in ["claude-2.1", "claude-v2:1"]):
   553|                 return claude_2_1_pt(messages=messages)
   554|             else:
   555|                 return anthropic_pt(messages=messages)
   556|         elif "mistral." in model:
   557|             return mistral_instruct_pt(messages=messages)
   558|     try:
   559|         if "meta-llama/llama-2" in model and "chat" in model:
   560|             return llama_2_chat_pt(messages=messages)
   561|         elif (
   562|             "tiiuae/falcon" in model
   563|         ):  # Note: for the instruct models, it's best to use a User: .., Assistant:.. approach in your prompt template.
   564|             if model == "tiiuae/falcon-180B-chat":
   565|                 return falcon_chat_pt(messages=messages)
   566|             elif "instruct" in model:
   567|                 return falcon_instruct_pt(messages=messages)
   568|         elif "mosaicml/mpt" in model:
   569|             if "chat" in model:
   570|                 return mpt_chat_pt(messages=messages)
   571|         elif "codellama/codellama" in model or "togethercomputer/codellama" in model:
   572|             if "instruct" in model:
   573|                 return llama_2_chat_pt(
   574|                     messages=messages
   575|                 )  # https://huggingface.co/blog/codellama#conversational-instructions
   576|         elif "wizardlm/wizardcoder" in model:
   577|             return wizardcoder_pt(messages=messages)


# ====================================================================
# FILE: litellm/main.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 884-924 ---
   884|             if "stream" in optional_params and optional_params["stream"] == True:
   885|                 model_response = CustomStreamWrapper(model_response, model, logging_obj=logging, custom_llm_provider="replicate")  # type: ignore
   886|             if optional_params.get("stream", False) or acompletion == True:
   887|                 logging.post_call(
   888|                     input=messages,
   889|                     api_key=replicate_key,
   890|                     original_response=model_response,
   891|                 )
   892|             response = model_response
   893|         elif custom_llm_provider == "anthropic":
   894|             api_key = (
   895|                 api_key
   896|                 or litellm.anthropic_key
   897|                 or litellm.api_key
   898|                 or os.environ.get("ANTHROPIC_API_KEY")
   899|             )
   900|             api_base = (
   901|                 api_base
   902|                 or litellm.api_base
   903|                 or get_secret("ANTHROPIC_API_BASE")
   904|                 or "https://api.anthropic.com/v1/messages"
   905|             )
   906|             custom_prompt_dict = custom_prompt_dict or litellm.custom_prompt_dict
   907|             response = anthropic.completion(
   908|                 model=model,
   909|                 messages=messages,
   910|                 api_base=api_base,
   911|                 custom_prompt_dict=litellm.custom_prompt_dict,
   912|                 model_response=model_response,
   913|                 print_verbose=print_verbose,
   914|                 optional_params=optional_params,
   915|                 litellm_params=litellm_params,
   916|                 logger_fn=logger_fn,
   917|                 encoding=encoding,  # for calculating input/output tokens
   918|                 api_key=api_key,
   919|                 logging_obj=logging,
   920|                 headers=headers,
   921|             )
   922|             if "stream" in optional_params and optional_params["stream"] == True:
   923|                 response = CustomStreamWrapper(
   924|                     response,


# ====================================================================
# FILE: litellm/proxy/proxy_server.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 125-172 ---
   125|     docs_url="/",
   126|     title="LiteLLM API",
   127|     description=f"Proxy Server to call 100+ LLMs in the OpenAI format\n\n{ui_message}",
   128|     version=version,
   129|     root_path=os.environ.get(
   130|         "SERVER_ROOT_PATH", ""
   131|     ),  # check if user passed root path, FastAPI defaults this value to ""
   132| )
   133| class ProxyException(Exception):
   134|     def __init__(
   135|         self,
   136|         message: str,
   137|         type: str,
   138|         param: Optional[str],
   139|         code: Optional[int],
   140|     ):
   141|         self.message = message
   142|         self.type = type
   143|         self.param = param
   144|         self.code = code
   145|     def to_dict(self) -> dict:
   146|         """Converts the ProxyException instance to a dictionary."""
   147|         return {
   148|             "message": self.message,
   149|             "type": self.type,
   150|             "param": self.param,
   151|             "code": self.code,
   152|         }
   153| @app.exception_handler(ProxyException)
   154| async def openai_exception_handler(request: Request, exc: ProxyException):
   155|     return JSONResponse(
   156|         status_code=(
   157|             int(exc.code) if exc.code else status.HTTP_500_INTERNAL_SERVER_ERROR
   158|         ),
   159|         content={
   160|             "error": {
   161|                 "message": exc.message,
   162|                 "type": exc.type,
   163|                 "param": exc.param,
   164|                 "code": exc.code,
   165|             }
   166|         },
   167|     )
   168| router = APIRouter()
   169| origins = ["*"]
   170| try:
   171|     current_dir = os.path.dirname(os.path.abspath(__file__))
   172|     ui_path = os.path.join(current_dir, "_experimental", "out")

# --- HUNK 2: Lines 1850-1897 ---
  1850|     except Exception as e:
  1851|         traceback.print_exc()
  1852|         await proxy_logging_obj.post_call_failure_hook(
  1853|             user_api_key_dict=user_api_key_dict, original_exception=e
  1854|         )
  1855|         verbose_proxy_logger.debug(
  1856|             f"\033[1;31mAn error occurred: {e}\n\n Debug this by setting `--debug`, e.g. `litellm --model gpt-3.5-turbo --debug`"
  1857|         )
  1858|         router_model_names = (
  1859|             [m["model_name"] for m in llm_model_list]
  1860|             if llm_model_list is not None
  1861|             else []
  1862|         )
  1863|         if user_debug:
  1864|             traceback.print_exc()
  1865|         if isinstance(e, HTTPException):
  1866|             raise e
  1867|         else:
  1868|             error_traceback = traceback.format_exc()
  1869|             error_msg = f"{str(e)}\n\n{error_traceback}"
  1870|         proxy_exception = ProxyException(
  1871|             message=getattr(e, "message", error_msg),
  1872|             type=getattr(e, "type", "None"),
  1873|             param=getattr(e, "param", "None"),
  1874|             code=getattr(e, "status_code", 500),
  1875|         )
  1876|         error_returned = json.dumps({"error": proxy_exception.to_dict()})
  1877|         yield f"data: {error_returned}\n\n"
  1878| def select_data_generator(response, user_api_key_dict):
  1879|     return async_data_generator(response=response, user_api_key_dict=user_api_key_dict)
  1880| def get_litellm_model_info(model: dict = {}):
  1881|     model_info = model.get("model_info", {})
  1882|     model_to_lookup = model.get("litellm_params", {}).get("model", None)
  1883|     try:
  1884|         if "azure" in model_to_lookup:
  1885|             model_to_lookup = model_info.get("base_model", None)
  1886|         litellm_model_info = litellm.get_model_info(model_to_lookup)
  1887|         return litellm_model_info
  1888|     except:
  1889|         return {}
  1890| def parse_cache_control(cache_control):
  1891|     cache_dict = {}
  1892|     directives = cache_control.split(", ")
  1893|     for directive in directives:
  1894|         if "=" in directive:
  1895|             key, value = directive.split("=")
  1896|             cache_dict[key] = value
  1897|         else:


# ====================================================================
# FILE: litellm/utils.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 2207-2247 ---
  2207|             result._response_ms = (
  2208|                 end_time - start_time
  2209|             ).total_seconds() * 1000  # return response latency in ms like openai
  2210|             return result
  2211|         except Exception as e:
  2212|             call_type = original_function.__name__
  2213|             if call_type == CallTypes.completion.value:
  2214|                 num_retries = (
  2215|                     kwargs.get("num_retries", None) or litellm.num_retries or None
  2216|                 )
  2217|                 litellm.num_retries = (
  2218|                     None  # set retries to None to prevent infinite loops
  2219|                 )
  2220|                 context_window_fallback_dict = kwargs.get(
  2221|                     "context_window_fallback_dict", {}
  2222|                 )
  2223|                 if num_retries:
  2224|                     if (
  2225|                         isinstance(e, openai.APIError)
  2226|                         or isinstance(e, openai.Timeout)
  2227|                         or isinstance(e, openai.APIConnectionError)
  2228|                     ):
  2229|                         print_verbose(f"RETRY TRIGGERED!")
  2230|                         kwargs["num_retries"] = num_retries
  2231|                         return litellm.completion_with_retries(*args, **kwargs)
  2232|                 elif (
  2233|                     isinstance(e, litellm.exceptions.ContextWindowExceededError)
  2234|                     and context_window_fallback_dict
  2235|                     and model in context_window_fallback_dict
  2236|                 ):
  2237|                     if len(args) > 0:
  2238|                         args[0] = context_window_fallback_dict[model]
  2239|                     else:
  2240|                         kwargs["model"] = context_window_fallback_dict[model]
  2241|                     return original_function(*args, **kwargs)
  2242|             traceback_exception = traceback.format_exc()
  2243|             crash_reporting(*args, **kwargs, exception=traceback_exception)
  2244|             end_time = datetime.datetime.now()
  2245|             if logging_obj:
  2246|                 logging_obj.failure_handler(
  2247|                     e, traceback_exception, start_time, end_time

# --- HUNK 2: Lines 3630-3670 ---
  3630|                     for s in stop:
  3631|                         if re.match(r"^(\|+|User:)$", s):
  3632|                             filtered_stop.append(s)
  3633|         if filtered_stop is not None:
  3634|             supported_params["stop"] = filtered_stop
  3635|         return supported_params
  3636|     if custom_llm_provider == "anthropic":
  3637|         supported_params = ["stream", "stop", "temperature", "top_p", "max_tokens"]
  3638|         _check_valid_arg(supported_params=supported_params)
  3639|         if stream:
  3640|             optional_params["stream"] = stream
  3641|         if stop is not None:
  3642|             if type(stop) == str:
  3643|                 stop = [stop]  # openai can accept str/list for stop
  3644|             optional_params["stop_sequences"] = stop
  3645|         if temperature is not None:
  3646|             optional_params["temperature"] = temperature
  3647|         if top_p is not None:
  3648|             optional_params["top_p"] = top_p
  3649|         if max_tokens is not None:
  3650|             optional_params["max_tokens"] = max_tokens
  3651|     elif custom_llm_provider == "cohere":
  3652|         supported_params = [
  3653|             "stream",
  3654|             "temperature",
  3655|             "max_tokens",
  3656|             "logit_bias",
  3657|             "top_p",
  3658|             "frequency_penalty",
  3659|             "presence_penalty",
  3660|             "stop",
  3661|             "n",
  3662|         ]
  3663|         _check_valid_arg(supported_params=supported_params)
  3664|         if stream:
  3665|             optional_params["stream"] = stream
  3666|         if temperature is not None:
  3667|             optional_params["temperature"] = temperature
  3668|         if max_tokens is not None:
  3669|             optional_params["max_tokens"] = max_tokens
  3670|         if n is not None:

# --- HUNK 3: Lines 3945-3997 ---
  3945|         elif "meta" in model:  # amazon / meta llms
  3946|             supported_params = ["max_tokens", "temperature", "top_p", "stream"]
  3947|             _check_valid_arg(supported_params=supported_params)
  3948|             if max_tokens is not None:
  3949|                 optional_params["max_gen_len"] = max_tokens
  3950|             if temperature is not None:
  3951|                 optional_params["temperature"] = temperature
  3952|             if top_p is not None:
  3953|                 optional_params["top_p"] = top_p
  3954|             if stream:
  3955|                 optional_params["stream"] = stream
  3956|         elif "cohere" in model:  # cohere models on bedrock
  3957|             supported_params = ["stream", "temperature", "max_tokens"]
  3958|             _check_valid_arg(supported_params=supported_params)
  3959|             if stream:
  3960|                 optional_params["stream"] = stream
  3961|             if temperature is not None:
  3962|                 optional_params["temperature"] = temperature
  3963|             if max_tokens is not None:
  3964|                 optional_params["max_tokens"] = max_tokens
  3965|         elif "mistral" in model:
  3966|             supported_params = ["max_tokens", "temperature", "stop", "top_p", "stream"]
  3967|             _check_valid_arg(supported_params=supported_params)
  3968|             if max_tokens is not None:
  3969|                 optional_params["max_tokens"] = max_tokens
  3970|             if temperature is not None:
  3971|                 optional_params["temperature"] = temperature
  3972|             if top_p is not None:
  3973|                 optional_params["top_p"] = top_p
  3974|             if stop is not None:
  3975|                 optional_params["stop"] = stop
  3976|             if stream is not None:
  3977|                 optional_params["stream"] = stream
  3978|     elif custom_llm_provider == "aleph_alpha":
  3979|         supported_params = [
  3980|             "max_tokens",
  3981|             "stream",
  3982|             "top_p",
  3983|             "temperature",
  3984|             "presence_penalty",
  3985|             "frequency_penalty",
  3986|             "n",
  3987|             "stop",
  3988|         ]
  3989|         _check_valid_arg(supported_params=supported_params)
  3990|         if max_tokens is not None:
  3991|             optional_params["maximum_tokens"] = max_tokens
  3992|         if stream:
  3993|             optional_params["stream"] = stream
  3994|         if temperature is not None:
  3995|             optional_params["temperature"] = temperature
  3996|         if top_p is not None:
  3997|             optional_params["top_p"] = top_p

# --- HUNK 4: Lines 7091-7144 ---
  7091|         curr_chunk = curr_chunk.strip()
  7092|         for token in self.special_tokens:
  7093|             if len(curr_chunk) < len(token) and curr_chunk in token:
  7094|                 hold = True
  7095|             elif len(curr_chunk) >= len(token):
  7096|                 if token in curr_chunk:
  7097|                     self.holding_chunk = curr_chunk.replace(token, "")
  7098|                     hold = True
  7099|             else:
  7100|                 pass
  7101|         if hold is False:  # reset
  7102|             self.holding_chunk = ""
  7103|         return hold, curr_chunk
  7104|     def handle_anthropic_chunk(self, chunk):
  7105|         str_line = chunk.decode("utf-8")  # Convert bytes to string
  7106|         text = ""
  7107|         is_finished = False
  7108|         finish_reason = None
  7109|         if str_line.startswith("data:"):
  7110|             data_json = json.loads(str_line[5:])
  7111|             type_chunk = data_json.get("type", None)
  7112|             if type_chunk == "content_block_delta":
  7113|                 """
  7114|                 Anthropic content chunk
  7115|                 chunk = {'type': 'content_block_delta', 'index': 0, 'delta': {'type': 'text_delta', 'text': 'Hello'}}
  7116|                 """
  7117|                 text = data_json.get("delta", {}).get("text", "")
  7118|             elif type_chunk == "message_delta":
  7119|                 """
  7120|                 Anthropic
  7121|                 chunk = {'type': 'message_delta', 'delta': {'stop_reason': 'max_tokens', 'stop_sequence': None}, 'usage': {'output_tokens': 10}}
  7122|                 """
  7123|                 finish_reason = data_json.get("delta", {}).get("stop_reason", None)
  7124|                 is_finished = True
  7125|             return {
  7126|                 "text": text,
  7127|                 "is_finished": is_finished,
  7128|                 "finish_reason": finish_reason,
  7129|             }
  7130|         elif "error" in str_line:
  7131|             raise ValueError(f"Unable to parse response. Original response: {str_line}")
  7132|         else:
  7133|             return {
  7134|                 "text": text,
  7135|                 "is_finished": is_finished,
  7136|                 "finish_reason": finish_reason,
  7137|             }
  7138|     def handle_together_ai_chunk(self, chunk):
  7139|         chunk = chunk.decode("utf-8")
  7140|         text = ""
  7141|         is_finished = False
  7142|         finish_reason = None
  7143|         if "text" in chunk:
  7144|             text_index = chunk.find('"text":"')  # this checks if text: exists

# --- HUNK 5: Lines 7173-7229 ---
  7173|             text = ""
  7174|             is_finished = False
  7175|             finish_reason = ""
  7176|             print_verbose(f"chunk: {chunk}")
  7177|             if chunk.startswith("data:"):
  7178|                 data_json = json.loads(chunk[5:])
  7179|                 print_verbose(f"data json: {data_json}")
  7180|                 if "token" in data_json and "text" in data_json["token"]:
  7181|                     text = data_json["token"]["text"]
  7182|                 if data_json.get("details", False) and data_json["details"].get(
  7183|                     "finish_reason", False
  7184|                 ):
  7185|                     is_finished = True
  7186|                     finish_reason = data_json["details"]["finish_reason"]
  7187|                 elif data_json.get(
  7188|                     "generated_text", False
  7189|                 ):  # if full generated text exists, then stream is complete
  7190|                     text = ""  # don't return the final bos token
  7191|                     is_finished = True
  7192|                     finish_reason = "stop"
  7193|                 elif data_json.get("error", False):
  7194|                     raise Exception(data_json.get("error"))
  7195|                 return {
  7196|                     "text": text,
  7197|                     "is_finished": is_finished,
  7198|                     "finish_reason": finish_reason,
  7199|                 }
  7200|             elif "error" in chunk:
  7201|                 raise ValueError(chunk)
  7202|             return {
  7203|                 "text": text,
  7204|                 "is_finished": is_finished,
  7205|                 "finish_reason": finish_reason,
  7206|             }
  7207|         except Exception as e:
  7208|             traceback.print_exc()
  7209|             raise e
  7210|     def handle_ai21_chunk(self, chunk):  # fake streaming
  7211|         chunk = chunk.decode("utf-8")
  7212|         data_json = json.loads(chunk)
  7213|         try:
  7214|             text = data_json["completions"][0]["data"]["text"]
  7215|             is_finished = True
  7216|             finish_reason = "stop"
  7217|             return {
  7218|                 "text": text,
  7219|                 "is_finished": is_finished,
  7220|                 "finish_reason": finish_reason,
  7221|             }
  7222|         except:
  7223|             raise ValueError(f"Unable to parse response. Original response: {chunk}")
  7224|     def handle_maritalk_chunk(self, chunk):  # fake streaming
  7225|         chunk = chunk.decode("utf-8")
  7226|         data_json = json.loads(chunk)
  7227|         try:
  7228|             text = data_json["answer"]
  7229|             is_finished = True

