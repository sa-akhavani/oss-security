--- a/litellm/__init__.py
+++ b/litellm/__init__.py
@@ -511,21 +511,20 @@
 from .llms.sagemaker import SagemakerConfig
 from .llms.ollama import OllamaConfig
 from .llms.maritalk import MaritTalkConfig
 from .llms.bedrock import (
     AmazonTitanConfig,
     AmazonAI21Config,
     AmazonAnthropicConfig,
     AmazonCohereConfig,
     AmazonLlamaConfig,
     AmazonStabilityConfig,
-    AmazonMistralConfig
 )
 from .llms.openai import OpenAIConfig, OpenAITextCompletionConfig
 from .llms.azure import AzureOpenAIConfig, AzureOpenAIError
 from .main import *  # type: ignore
 from .integrations import *
 from .exceptions import (
     AuthenticationError,
     InvalidRequestError,
     BadRequestError,
     NotFoundError,

--- a/litellm/llms/anthropic.py
+++ b/litellm/llms/anthropic.py
@@ -9,40 +9,42 @@
 from .prompt_templates.factory import prompt_factory, custom_prompt
 import httpx
 class AnthropicConstants(Enum):
     HUMAN_PROMPT = "\n\nHuman: "
     AI_PROMPT = "\n\nAssistant: "
 class AnthropicError(Exception):
     def __init__(self, status_code, message):
         self.status_code = status_code
         self.message = message
         self.request = httpx.Request(
-            method="POST", url="https://api.anthropic.com/v1/messages"
+            method="POST", url="https://api.anthropic.com/v1/complete"
         )
         self.response = httpx.Response(status_code=status_code, request=self.request)
         super().__init__(
             self.message
         )  # Call the base class constructor with the parameters it needs
 class AnthropicConfig:
     """
     Reference: https://docs.anthropic.com/claude/reference/complete_post
     to pass metadata to anthropic, it's {"user_id": "any-relevant-information"}
     """
-    max_tokens: Optional[int] = litellm.max_tokens  # anthropic requires a default
+    max_tokens_to_sample: Optional[
+        int
+    ] = litellm.max_tokens  # anthropic requires a default
     stop_sequences: Optional[list] = None
     temperature: Optional[int] = None
     top_p: Optional[int] = None
     top_k: Optional[int] = None
     metadata: Optional[dict] = None
     def __init__(
         self,
-        max_tokens: Optional[int] = 256,  # anthropic requires a default
+        max_tokens_to_sample: Optional[int] = 256,  # anthropic requires a default
         stop_sequences: Optional[list] = None,
         temperature: Optional[int] = None,
         top_p: Optional[int] = None,
         top_k: Optional[int] = None,
         metadata: Optional[dict] = None,
     ) -> None:
         locals_ = locals()
         for key, value in locals_.items():
             if key != "self" and value is not None:
                 setattr(self.__class__, key, value)
@@ -98,48 +100,29 @@
         prompt = custom_prompt(
             role_dict=model_prompt_details["roles"],
             initial_prompt_value=model_prompt_details["initial_prompt_value"],
             final_prompt_value=model_prompt_details["final_prompt_value"],
             messages=messages,
         )
     else:
         prompt = prompt_factory(
             model=model, messages=messages, custom_llm_provider="anthropic"
         )
-    """
-    format messages for anthropic
-    1. Anthropic supports roles like "user" and "assistant", (here litellm translates system-> assistant)
-    2. The first message always needs to be of role "user"
-    3. Each message must alternate between "user" and "assistant" (this is not addressed as now by litellm)
-    4. final assistant content cannot end with trailing whitespace (anthropic raises an error otherwise)
-    """
-    for idx, message in enumerate(messages):
-        if message["role"] == "system":
-            message["role"] = "assistant"
-        if message["role"] == "assistant":
-            message["content"] = message["content"].strip()
-    if len(messages) > 0:
-        if messages[0]["role"] != "user":
-            for i, message in enumerate(messages):
-                if message["role"] == "user":
-                    break
-            messages.pop(i)
-            messages = [message] + messages
     config = litellm.AnthropicConfig.get_config()
     for k, v in config.items():
         if (
             k not in optional_params
         ):  # completion(top_k=3) > anthropic_config(top_k=3) <- allows for dynamic variables to be passed in
             optional_params[k] = v
     data = {
         "model": model,
-        "messages": messages,
+        "prompt": prompt,
         **optional_params,
     }
     logging_obj.pre_call(
         input=prompt,
         api_key=api_key,
         additional_args={
             "complete_input_dict": data,
             "api_base": api_base,
             "headers": headers,
         },
@@ -156,49 +139,49 @@
                 status_code=response.status_code, message=response.text
             )
         return response.iter_lines()
     else:
         response = requests.post(api_base, headers=headers, data=json.dumps(data))
         if response.status_code != 200:
             raise AnthropicError(
                 status_code=response.status_code, message=response.text
             )
         logging_obj.post_call(
-            input=messages,
+            input=prompt,
             api_key=api_key,
             original_response=response.text,
             additional_args={"complete_input_dict": data},
         )
         print_verbose(f"raw model_response: {response.text}")
         try:
             completion_response = response.json()
         except:
             raise AnthropicError(
                 message=response.text, status_code=response.status_code
             )
         if "error" in completion_response:
             raise AnthropicError(
                 message=str(completion_response["error"]),
                 status_code=response.status_code,
             )
-        elif len(completion_response["content"]) == 0:
-            raise AnthropicError(
-                message="No content in response",
-                status_code=response.status_code,
-            )
         else:
-            text_content = completion_response["content"][0].get("text", None)
-            model_response.choices[0].message.content = text_content  # type: ignore
+            if len(completion_response["completion"]) > 0:
+                model_response["choices"][0]["message"][
+                    "content"
+                ] = completion_response["completion"]
             model_response.choices[0].finish_reason = completion_response["stop_reason"]
-        prompt_tokens = completion_response["usage"]["input_tokens"]
-        completion_tokens = completion_response["usage"]["output_tokens"]
-        total_tokens = prompt_tokens + completion_tokens
+        prompt_tokens = len(
+            encoding.encode(prompt)
+        )  ##[TODO] use the anthropic tokenizer here
+        completion_tokens = len(
+            encoding.encode(model_response["choices"][0]["message"].get("content", ""))
+        )  ##[TODO] use the anthropic tokenizer here
         model_response["created"] = int(time.time())
         model_response["model"] = model
         usage = Usage(
             prompt_tokens=prompt_tokens,
             completion_tokens=completion_tokens,
             total_tokens=prompt_tokens + completion_tokens,
         )
         model_response.usage = usage
         return model_response
 def embedding():

--- a/litellm/llms/bedrock.py
+++ b/litellm/llms/bedrock.py
@@ -206,64 +206,20 @@
     - `top_p` (float) top p for model
     """
     max_gen_len: Optional[int] = None
     temperature: Optional[float] = None
     topP: Optional[float] = None
     def __init__(
         self,
         maxTokenCount: Optional[int] = None,
         temperature: Optional[float] = None,
         topP: Optional[int] = None,
-    ) -> None:
-        locals_ = locals()
-        for key, value in locals_.items():
-            if key != "self" and value is not None:
-                setattr(self.__class__, key, value)
-    @classmethod
-    def get_config(cls):
-        return {
-            k: v
-            for k, v in cls.__dict__.items()
-            if not k.startswith("__")
-            and not isinstance(
-                v,
-                (
-                    types.FunctionType,
-                    types.BuiltinFunctionType,
-                    classmethod,
-                    staticmethod,
-                ),
-            )
-            and v is not None
-        }
-class AmazonMistralConfig:
-    """
-    Reference: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-mistral.html
-    Supported Params for the Amazon / Mistral models:
-    - `max_tokens` (integer) max tokens,
-    - `temperature` (float) temperature for model,
-    - `top_p` (float) top p for model
-    - `stop` [string] A list of stop sequences that if generated by the model, stops the model from generating further output.
-    - `top_k` (float) top k for model
-    """
-    max_tokens: Optional[int] = None
-    temperature: Optional[float] = None
-    top_p: Optional[float] = None
-    top_k: Optional[float] = None
-    stop: Optional[list[str]] = None
-    def __init__(
-        self,
-        max_tokens: Optional[int] = None,
-        temperature: Optional[float] = None,
-        top_p: Optional[int] = None,
-        top_k: Optional[float] = None,
-        stop: Optional[list[str]] = None,
     ) -> None:
         locals_ = locals()
         for key, value in locals_.items():
             if key != "self" and value is not None:
                 setattr(self.__class__, key, value)
     @classmethod
     def get_config(cls):
         return {
             k: v
             for k, v in cls.__dict__.items()
@@ -438,22 +394,20 @@
             prompt = custom_prompt(
                 role_dict=model_prompt_details["roles"],
                 initial_prompt_value=model_prompt_details["initial_prompt_value"],
                 final_prompt_value=model_prompt_details["final_prompt_value"],
                 messages=messages,
             )
         else:
             prompt = prompt_factory(
                 model=model, messages=messages, custom_llm_provider="bedrock"
             )
-    elif provider == "mistral":
-        prompt = prompt_factory(model=model, messages=messages, custom_llm_provider="bedrock")
     else:
         prompt = ""
         for message in messages:
             if "role" in message:
                 if message["role"] == "user":
                     prompt += f"{message['content']}"
                 else:
                     prompt += f"{message['content']}"
             else:
                 prompt += f"{message['content']}"
@@ -551,28 +505,20 @@
                 if (
                     k not in inference_params
                 ):  # completion(top_k=3) > amazon_config(top_k=3) <- allows for dynamic variables to be passed in
                     inference_params[k] = v
             data = json.dumps(
                 {
                     "inputText": prompt,
                     "textGenerationConfig": inference_params,
                 }
             )
-        elif provider == "mistral":  
-            config = litellm.AmazonMistralConfig.get_config()
-            for k, v in config.items():
-                if (
-                    k not in inference_params
-                ):  # completion(top_k=3) > amazon_config(top_k=3) <- allows for dynamic variables to be passed in
-                    inference_params[k] = v
-            data = json.dumps({"prompt": prompt, **inference_params})
         else:
             data = json.dumps({})
         accept = "application/json"
         contentType = "application/json"
         if stream == True:
             if provider == "ai21":
                 request_str = f"""
                 response = client.invoke_model(
                     body={data},
                     modelId={modelId},
@@ -652,23 +598,20 @@
         outputText = "default"
         if provider == "ai21":
             outputText = response_body.get("completions")[0].get("data").get("text")
         elif provider == "anthropic":
             outputText = response_body["completion"]
             model_response["finish_reason"] = response_body["stop_reason"]
         elif provider == "cohere":
             outputText = response_body["generations"][0]["text"]
         elif provider == "meta":
             outputText = response_body["generation"]
-        elif provider == "mistral":
-            outputText = response_body["outputs"][0]["text"]
-            model_response["finish_reason"] = response_body["outputs"][0]["stop_reason"]
         else:  # amazon titan
             outputText = response_body.get("results")[0].get("outputText")
         response_metadata = response.get("ResponseMetadata", {})
         if response_metadata.get("HTTPStatusCode", 500) >= 400:
             raise BedrockError(
                 message=outputText,
                 status_code=response_metadata.get("HTTPStatusCode", 500),
             )
         else:
             try:

--- a/litellm/llms/huggingface_restapi.py
+++ b/litellm/llms/huggingface_restapi.py
@@ -558,61 +558,28 @@
             response = client.stream(
                 "POST", url=f"{api_base}", json=data, headers=headers
             )
             async with response as r:
                 if r.status_code != 200:
                     text = await r.aread()
                     raise HuggingfaceError(
                         status_code=r.status_code,
                         message=str(text),
                     )
-                """
-                Check first chunk for error message. 
-                If error message, raise error. 
-                If not - add back to stream
-                """
-                response_iterator = r.aiter_lines()
-                try:
-                    first_chunk = await response_iterator.__anext__()
-                except StopAsyncIteration:
-                    first_chunk = ""
-                if (
-                    "error" in first_chunk.lower()
-                ):  # Adjust this condition based on how error messages are structured
-                    raise HuggingfaceError(
-                        status_code=400,
-                        message=first_chunk,
-                    )
-                return self.async_streaming_generator(
-                    first_chunk=first_chunk,
-                    response_iterator=response_iterator,
+                streamwrapper = CustomStreamWrapper(
+                    completion_stream=r.aiter_lines(),
                     model=model,
+                    custom_llm_provider="huggingface",
                     logging_obj=logging_obj,
                 )
-    async def async_streaming_generator(
-        self, first_chunk, response_iterator, model, logging_obj
-    ):
-        async def custom_stream_with_first_chunk():
-            yield first_chunk  # Yield back the first chunk
-            async for (
-                chunk
-            ) in response_iterator:  # Continue yielding the rest of the chunks
-                yield chunk
-        completion_stream = custom_stream_with_first_chunk()
-        streamwrapper = CustomStreamWrapper(
-            completion_stream=completion_stream,
-            model=model,
-            custom_llm_provider="huggingface",
-            logging_obj=logging_obj,
-        )
-        async for transformed_chunk in streamwrapper:
-            yield transformed_chunk
+                async for transformed_chunk in streamwrapper:
+                    yield transformed_chunk
     def embedding(
         self,
         model: str,
         input: list,
         api_key: Optional[str] = None,
         api_base: Optional[str] = None,
         logging_obj=None,
         model_response=None,
         encoding=None,
     ):

--- a/litellm/llms/prompt_templates/factory.py
+++ b/litellm/llms/prompt_templates/factory.py
@@ -89,23 +89,23 @@
     return prompt
 def mistral_instruct_pt(messages):
     prompt = custom_prompt(
         initial_prompt_value="<s>",
         role_dict={
             "system": {
                 "pre_message": "[INST] \n",
                 "post_message": " [/INST]\n",
             },
             "user": {"pre_message": "[INST] ", "post_message": " [/INST]\n"},
-            "assistant": {"pre_message": " ", "post_message": "</s> "},
+            "assistant": {"pre_message": " ", "post_message": " "},
         },
-        final_prompt_value="",
+        final_prompt_value="</s>",
         messages=messages,
     )
     return prompt
 def mistral_api_pt(messages):
     """
     - handles scenario where content is list and not string
     - content list is just text, and no images
     - if image passed in, then just return as is (user-intended)
     Motivation: mistral api doesn't support content as a list
     """
@@ -546,22 +546,20 @@
     elif custom_llm_provider == "mistral":
         return mistral_api_pt(messages=messages)
     elif custom_llm_provider == "bedrock":
         if "amazon.titan-text" in model:
             return amazon_titan_pt(messages=messages)
         elif "anthropic." in model:
             if any(_ in model for _ in ["claude-2.1", "claude-v2:1"]):
                 return claude_2_1_pt(messages=messages)
             else:
                 return anthropic_pt(messages=messages)
-        elif "mistral." in model:
-            return mistral_instruct_pt(messages=messages)
     try:
         if "meta-llama/llama-2" in model and "chat" in model:
             return llama_2_chat_pt(messages=messages)
         elif (
             "tiiuae/falcon" in model
         ):  # Note: for the instruct models, it's best to use a User: .., Assistant:.. approach in your prompt template.
             if model == "tiiuae/falcon-180B-chat":
                 return falcon_chat_pt(messages=messages)
             elif "instruct" in model:
                 return falcon_instruct_pt(messages=messages)

--- a/litellm/main.py
+++ b/litellm/main.py
@@ -894,21 +894,21 @@
             api_key = (
                 api_key
                 or litellm.anthropic_key
                 or litellm.api_key
                 or os.environ.get("ANTHROPIC_API_KEY")
             )
             api_base = (
                 api_base
                 or litellm.api_base
                 or get_secret("ANTHROPIC_API_BASE")
-                or "https://api.anthropic.com/v1/messages"
+                or "https://api.anthropic.com/v1/complete"
             )
             custom_prompt_dict = custom_prompt_dict or litellm.custom_prompt_dict
             response = anthropic.completion(
                 model=model,
                 messages=messages,
                 api_base=api_base,
                 custom_prompt_dict=litellm.custom_prompt_dict,
                 model_response=model_response,
                 print_verbose=print_verbose,
                 optional_params=optional_params,

--- a/litellm/proxy/proxy_server.py
+++ b/litellm/proxy/proxy_server.py
@@ -135,28 +135,20 @@
         self,
         message: str,
         type: str,
         param: Optional[str],
         code: Optional[int],
     ):
         self.message = message
         self.type = type
         self.param = param
         self.code = code
-    def to_dict(self) -> dict:
-        """Converts the ProxyException instance to a dictionary."""
-        return {
-            "message": self.message,
-            "type": self.type,
-            "param": self.param,
-            "code": self.code,
-        }
 @app.exception_handler(ProxyException)
 async def openai_exception_handler(request: Request, exc: ProxyException):
     return JSONResponse(
         status_code=(
             int(exc.code) if exc.code else status.HTTP_500_INTERNAL_SERVER_ERROR
         ),
         content={
             "error": {
                 "message": exc.message,
                 "type": exc.type,
@@ -1860,28 +1852,26 @@
             if llm_model_list is not None
             else []
         )
         if user_debug:
             traceback.print_exc()
         if isinstance(e, HTTPException):
             raise e
         else:
             error_traceback = traceback.format_exc()
             error_msg = f"{str(e)}\n\n{error_traceback}"
-        proxy_exception = ProxyException(
+        raise ProxyException(
             message=getattr(e, "message", error_msg),
             type=getattr(e, "type", "None"),
             param=getattr(e, "param", "None"),
             code=getattr(e, "status_code", 500),
         )
-        error_returned = json.dumps({"error": proxy_exception.to_dict()})
-        yield f"data: {error_returned}\n\n"
 def select_data_generator(response, user_api_key_dict):
     return async_data_generator(response=response, user_api_key_dict=user_api_key_dict)
 def get_litellm_model_info(model: dict = {}):
     model_info = model.get("model_info", {})
     model_to_lookup = model.get("litellm_params", {}).get("model", None)
     try:
         if "azure" in model_to_lookup:
             model_to_lookup = model_info.get("base_model", None)
         litellm_model_info = litellm.get_model_info(model_to_lookup)
         return litellm_model_info

--- a/litellm/utils.py
+++ b/litellm/utils.py
@@ -2217,21 +2217,21 @@
                 litellm.num_retries = (
                     None  # set retries to None to prevent infinite loops
                 )
                 context_window_fallback_dict = kwargs.get(
                     "context_window_fallback_dict", {}
                 )
                 if num_retries:
                     if (
                         isinstance(e, openai.APIError)
                         or isinstance(e, openai.Timeout)
-                        or isinstance(e, openai.APIConnectionError)
+                        or isinstance(openai.APIConnectionError)
                     ):
                         print_verbose(f"RETRY TRIGGERED!")
                         kwargs["num_retries"] = num_retries
                         return litellm.completion_with_retries(*args, **kwargs)
                 elif (
                     isinstance(e, litellm.exceptions.ContextWindowExceededError)
                     and context_window_fallback_dict
                     and model in context_window_fallback_dict
                 ):
                     if len(args) > 0:
@@ -3640,21 +3640,21 @@
             optional_params["stream"] = stream
         if stop is not None:
             if type(stop) == str:
                 stop = [stop]  # openai can accept str/list for stop
             optional_params["stop_sequences"] = stop
         if temperature is not None:
             optional_params["temperature"] = temperature
         if top_p is not None:
             optional_params["top_p"] = top_p
         if max_tokens is not None:
-            optional_params["max_tokens"] = max_tokens
+            optional_params["max_tokens_to_sample"] = max_tokens
     elif custom_llm_provider == "cohere":
         supported_params = [
             "stream",
             "temperature",
             "max_tokens",
             "logit_bias",
             "top_p",
             "frequency_penalty",
             "presence_penalty",
             "stop",
@@ -3955,33 +3955,20 @@
                 optional_params["stream"] = stream
         elif "cohere" in model:  # cohere models on bedrock
             supported_params = ["stream", "temperature", "max_tokens"]
             _check_valid_arg(supported_params=supported_params)
             if stream:
                 optional_params["stream"] = stream
             if temperature is not None:
                 optional_params["temperature"] = temperature
             if max_tokens is not None:
                 optional_params["max_tokens"] = max_tokens
-        elif "mistral" in model:
-            supported_params = ["max_tokens", "temperature", "stop", "top_p", "stream"]
-            _check_valid_arg(supported_params=supported_params)
-            if max_tokens is not None:
-                optional_params["max_tokens"] = max_tokens
-            if temperature is not None:
-                optional_params["temperature"] = temperature
-            if top_p is not None:
-                optional_params["top_p"] = top_p
-            if stop is not None:
-                optional_params["stop"] = stop
-            if stream is not None:
-                optional_params["stream"] = stream
     elif custom_llm_provider == "aleph_alpha":
         supported_params = [
             "max_tokens",
             "stream",
             "top_p",
             "temperature",
             "presence_penalty",
             "frequency_penalty",
             "n",
             "stop",
@@ -7101,34 +7088,24 @@
         if hold is False:  # reset
             self.holding_chunk = ""
         return hold, curr_chunk
     def handle_anthropic_chunk(self, chunk):
         str_line = chunk.decode("utf-8")  # Convert bytes to string
         text = ""
         is_finished = False
         finish_reason = None
         if str_line.startswith("data:"):
             data_json = json.loads(str_line[5:])
-            type_chunk = data_json.get("type", None)
-            if type_chunk == "content_block_delta":
-                """
-                Anthropic content chunk
-                chunk = {'type': 'content_block_delta', 'index': 0, 'delta': {'type': 'text_delta', 'text': 'Hello'}}
-                """
-                text = data_json.get("delta", {}).get("text", "")
-            elif type_chunk == "message_delta":
-                """
-                Anthropic
-                chunk = {'type': 'message_delta', 'delta': {'stop_reason': 'max_tokens', 'stop_sequence': None}, 'usage': {'output_tokens': 10}}
-                """
-                finish_reason = data_json.get("delta", {}).get("stop_reason", None)
+            text = data_json.get("completion", "")
+            if data_json.get("stop_reason", None):
                 is_finished = True
+                finish_reason = data_json["stop_reason"]
             return {
                 "text": text,
                 "is_finished": is_finished,
                 "finish_reason": finish_reason,
             }
         elif "error" in str_line:
             raise ValueError(f"Unable to parse response. Original response: {str_line}")
         else:
             return {
                 "text": text,
@@ -7183,37 +7160,34 @@
                     "finish_reason", False
                 ):
                     is_finished = True
                     finish_reason = data_json["details"]["finish_reason"]
                 elif data_json.get(
                     "generated_text", False
                 ):  # if full generated text exists, then stream is complete
                     text = ""  # don't return the final bos token
                     is_finished = True
                     finish_reason = "stop"
-                elif data_json.get("error", False):
-                    raise Exception(data_json.get("error"))
                 return {
                     "text": text,
                     "is_finished": is_finished,
                     "finish_reason": finish_reason,
                 }
             elif "error" in chunk:
                 raise ValueError(chunk)
             return {
                 "text": text,
                 "is_finished": is_finished,
                 "finish_reason": finish_reason,
             }
         except Exception as e:
             traceback.print_exc()
-            raise e
     def handle_ai21_chunk(self, chunk):  # fake streaming
         chunk = chunk.decode("utf-8")
         data_json = json.loads(chunk)
         try:
             text = data_json["completions"][0]["data"]["text"]
             is_finished = True
             finish_reason = "stop"
             return {
                 "text": text,
                 "is_finished": is_finished,
