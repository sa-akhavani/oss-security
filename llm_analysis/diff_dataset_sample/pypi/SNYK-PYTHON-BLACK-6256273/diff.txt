--- a/docs/conf.py
+++ b/docs/conf.py
@@ -1,27 +1,27 @@
 import os
 import string
-from importlib.metadata import version
 from pathlib import Path
+from pkg_resources import get_distribution
 CURRENT_DIR = Path(__file__).parent
 def make_pypi_svg(version: str) -> None:
     template: Path = CURRENT_DIR / "_static" / "pypi_template.svg"
     target: Path = CURRENT_DIR / "_static" / "pypi.svg"
     with open(str(template), "r", encoding="utf8") as f:
         svg: str = string.Template(f.read()).substitute(version=version)
     with open(str(target), "w", encoding="utf8") as f:
         f.write(svg)
 os.putenv("pythonioencoding", "utf-8")
 project = "Black"
 copyright = "2018-Present, Łukasz Langa and contributors to Black"
 author = "Łukasz Langa and contributors to Black"
-release = version("black").split("+")[0]
+release = get_distribution("black").version.split("+")[0]
 version = release
 for sp in "abcfr":
     version = version.split(sp)[0]
 make_pypi_svg(release)
 needs_sphinx = "4.4"
 extensions = [
     "sphinx.ext.autodoc",
     "sphinx.ext.intersphinx",
     "sphinx.ext.napoleon",
     "myst_parser",

--- a/src/black/__init__.py
+++ b/src/black/__init__.py
@@ -68,33 +68,23 @@
 from black.mode import Mode as Mode  # re-exported
 from black.mode import Preview, TargetVersion, supports_feature
 from black.nodes import (
     STARS,
     is_number_token,
     is_simple_decorator_expression,
     is_string_token,
     syms,
 )
 from black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out
-from black.parsing import (  # noqa F401
-    ASTSafetyError,
-    InvalidInput,
-    lib2to3_parse,
-    parse_ast,
-    stringify_ast,
-)
-from black.ranges import (
-    adjusted_lines,
-    convert_unchanged_lines,
-    parse_line_ranges,
-    sanitized_lines,
-)
+from black.parsing import InvalidInput  # noqa F401
+from black.parsing import lib2to3_parse, parse_ast, stringify_ast
+from black.ranges import adjusted_lines, convert_unchanged_lines, parse_line_ranges
 from black.report import Changed, NothingChanged, Report
 from black.trans import iter_fexpr_spans
 from blib2to3.pgen2 import token
 from blib2to3.pytree import Leaf, Node
 COMPILED = Path(__file__).suffix in (".pyd", ".so")
 FileContent = str
 Encoding = str
 NewLine = str
 class WriteBack(Enum):
     NO = 0
@@ -1078,24 +1068,20 @@
     ...       string_normalization=False,
     ...       is_pyi=False,
     ...     ),
     ...   ),
     ... )
     def f(
         arg: str = '',
     ) -> None:
         hey
     """
-    if lines:
-        lines = sanitized_lines(lines, src_contents)
-        if not lines:
-            return src_contents  # Nothing to format
     dst_contents = _format_str_once(src_contents, mode=mode, lines=lines)
     if src_contents != dst_contents:
         if lines:
             lines = adjusted_lines(lines, src_contents, dst_contents)
         return _format_str_once(dst_contents, mode=mode, lines=lines)
     return dst_contents
 def _format_str_once(
     src_contents: str, *, mode: Mode, lines: Collection[Tuple[int, int]] = ()
 ) -> str:
     src_node = lib2to3_parse(src_contents.lstrip(), mode.target_versions)
@@ -1325,40 +1311,40 @@
                 break
             imports |= set(get_imports_from_children(first_child.children[3:]))
         else:
             break
     return imports
 def assert_equivalent(src: str, dst: str) -> None:
     """Raise AssertionError if `src` and `dst` aren't equivalent."""
     try:
         src_ast = parse_ast(src)
     except Exception as exc:
-        raise ASTSafetyError(
+        raise AssertionError(
             "cannot use --safe with this file; failed to parse source file AST: "
             f"{exc}\n"
             "This could be caused by running Black with an older Python version "
             "that does not support new syntax used in your source file."
         ) from exc
     try:
         dst_ast = parse_ast(dst)
     except Exception as exc:
         log = dump_to_file("".join(traceback.format_tb(exc.__traceback__)), dst)
-        raise ASTSafetyError(
+        raise AssertionError(
             f"INTERNAL ERROR: Black produced invalid code: {exc}. "
             "Please report a bug on https://github.com/psf/black/issues.  "
             f"This invalid output might be helpful: {log}"
         ) from None
     src_ast_str = "\n".join(stringify_ast(src_ast))
     dst_ast_str = "\n".join(stringify_ast(dst_ast))
     if src_ast_str != dst_ast_str:
         log = dump_to_file(diff(src_ast_str, dst_ast_str, "src", "dst"))
-        raise ASTSafetyError(
+        raise AssertionError(
             "INTERNAL ERROR: Black produced code that is not equivalent to the"
             " source.  Please report a bug on "
             f"https://github.com/psf/black/issues.  This diff might be helpful: {log}"
         ) from None
 def assert_stable(
     src: str, dst: str, mode: Mode, *, lines: Collection[Tuple[int, int]] = ()
 ) -> None:
     """Raise AssertionError if `dst` reformats differently the second time."""
     if lines:
         return

--- a/src/black/linegen.py
+++ b/src/black/linegen.py
@@ -3,46 +3,45 @@
 """
 import re
 import sys
 from dataclasses import replace
 from enum import Enum, auto
 from functools import partial, wraps
 from typing import Collection, Iterator, List, Optional, Set, Union, cast
 from black.brackets import (
     COMMA_PRIORITY,
     DOT_PRIORITY,
-    STRING_PRIORITY,
     get_leaves_inside_matching_brackets,
     max_delimiter_priority_in_atom,
 )
 from black.comments import FMT_OFF, generate_comments, list_comments
 from black.lines import (
     Line,
     RHSResult,
     append_leaves,
     can_be_split,
     can_omit_invisible_parens,
     is_line_short_enough,
     line_to_string,
 )
 from black.mode import Feature, Mode, Preview
 from black.nodes import (
     ASSIGNMENTS,
     BRACKETS,
     CLOSING_BRACKETS,
     OPENING_BRACKETS,
+    RARROW,
     STANDALONE_COMMENT,
     STATEMENT,
     WHITESPACE,
     Visitor,
     ensure_visible,
-    get_annotation_type,
     is_arith_like,
     is_async_stmt_or_funcdef,
     is_atom_with_invisible_parens,
     is_docstring,
     is_empty_tuple,
     is_lpar_token,
     is_multiline_string,
     is_name_token,
     is_one_sequence_between,
     is_one_tuple,
@@ -821,21 +820,28 @@
                 original.is_def
                 and opening_bracket.value == "("
                 and not any(
                     leaf.type == token.COMMA
                     and (
                         Preview.typed_params_trailing_comma not in original.mode
                         or not is_part_of_annotation(leaf)
                     )
                     for leaf in leaves
                 )
-                and get_annotation_type(leaves[0]) != "return"
+                and not any(
+                    node.prev_sibling.type == RARROW
+                    for node in (
+                        leaves[0].parent,
+                        getattr(leaves[0].parent, "parent", None),
+                    )
+                    if isinstance(node, Node) and isinstance(node.prev_sibling, Leaf)
+                )
                 and not (
                     leaves[0].parent
                     and leaves[0].parent.next_sibling
                     and leaves[0].parent.next_sibling.type == token.VBAR
                 )
             )
             if original.is_import or no_commas:
                 for i in range(len(leaves) - 1, -1, -1):
                     if leaves[i].type == STANDALONE_COMMENT:
                         continue
@@ -869,102 +875,87 @@
     ) -> Iterator[Line]:
         for split_line in split_func(line, features, mode):
             split_line.leaves[0].prefix = ""
             yield split_line
     return split_wrapper
 def _get_last_non_comment_leaf(line: Line) -> Optional[int]:
     for leaf_idx in range(len(line.leaves) - 1, 0, -1):
         if line.leaves[leaf_idx].type != STANDALONE_COMMENT:
             return leaf_idx
     return None
-def _can_add_trailing_comma(leaf: Leaf, features: Collection[Feature]) -> bool:
-    if is_vararg(leaf, within={syms.typedargslist}):
-        return Feature.TRAILING_COMMA_IN_DEF in features
-    if is_vararg(leaf, within={syms.arglist, syms.argument}):
-        return Feature.TRAILING_COMMA_IN_CALL in features
-    return True
 def _safe_add_trailing_comma(safe: bool, delimiter_priority: int, line: Line) -> Line:
     if (
         safe
         and delimiter_priority == COMMA_PRIORITY
         and line.leaves[-1].type != token.COMMA
         and line.leaves[-1].type != STANDALONE_COMMENT
     ):
         new_comma = Leaf(token.COMMA, ",")
         line.append(new_comma)
     return line
-MIGRATE_COMMENT_DELIMITERS = {STRING_PRIORITY, COMMA_PRIORITY}
 @dont_increase_indentation
 def delimiter_split(
     line: Line, features: Collection[Feature], mode: Mode
 ) -> Iterator[Line]:
     """Split according to delimiters of the highest priority.
     If the appropriate Features are given, the split will add trailing commas
     also in function signatures and calls that contain `*` and `**`.
     """
-    if len(line.leaves) == 0:
+    try:
+        last_leaf = line.leaves[-1]
+    except IndexError:
         raise CannotSplit("Line empty") from None
-    last_leaf = line.leaves[-1]
     bt = line.bracket_tracker
     try:
         delimiter_priority = bt.max_delimiter_priority(exclude={id(last_leaf)})
     except ValueError:
         raise CannotSplit("No delimiters found") from None
-    if (
-        delimiter_priority == DOT_PRIORITY
-        and bt.delimiter_count_with_priority(delimiter_priority) == 1
-    ):
-        raise CannotSplit("Splitting a single attribute from its owner looks wrong")
+    if delimiter_priority == DOT_PRIORITY:
+        if bt.delimiter_count_with_priority(delimiter_priority) == 1:
+            raise CannotSplit("Splitting a single attribute from its owner looks wrong")
     current_line = Line(
         mode=line.mode, depth=line.depth, inside_brackets=line.inside_brackets
     )
     lowest_depth = sys.maxsize
     trailing_comma_safe = True
     def append_to_line(leaf: Leaf) -> Iterator[Line]:
         """Append `leaf` to current line or to new line if appending impossible."""
         nonlocal current_line
         try:
             current_line.append_safe(leaf, preformatted=True)
         except ValueError:
             yield current_line
             current_line = Line(
                 mode=line.mode, depth=line.depth, inside_brackets=line.inside_brackets
             )
             current_line.append(leaf)
-    def append_comments(leaf: Leaf) -> Iterator[Line]:
-        for comment_after in line.comments_after(leaf):
-            yield from append_to_line(comment_after)
     last_non_comment_leaf = _get_last_non_comment_leaf(line)
     for leaf_idx, leaf in enumerate(line.leaves):
         yield from append_to_line(leaf)
-        previous_priority = leaf_idx > 0 and bt.delimiters.get(
-            id(line.leaves[leaf_idx - 1])
-        )
-        if (
-            previous_priority != delimiter_priority
-            or delimiter_priority in MIGRATE_COMMENT_DELIMITERS
-        ):
-            yield from append_comments(leaf)
+        for comment_after in line.comments_after(leaf):
+            yield from append_to_line(comment_after)
         lowest_depth = min(lowest_depth, leaf.bracket_depth)
-        if trailing_comma_safe and leaf.bracket_depth == lowest_depth:
-            trailing_comma_safe = _can_add_trailing_comma(leaf, features)
+        if leaf.bracket_depth == lowest_depth:
+            if is_vararg(leaf, within={syms.typedargslist}):
+                trailing_comma_safe = (
+                    trailing_comma_safe and Feature.TRAILING_COMMA_IN_DEF in features
+                )
+            elif is_vararg(leaf, within={syms.arglist, syms.argument}):
+                trailing_comma_safe = (
+                    trailing_comma_safe and Feature.TRAILING_COMMA_IN_CALL in features
+                )
         if last_leaf.type == STANDALONE_COMMENT and leaf_idx == last_non_comment_leaf:
             current_line = _safe_add_trailing_comma(
                 trailing_comma_safe, delimiter_priority, current_line
             )
         leaf_priority = bt.delimiters.get(id(leaf))
         if leaf_priority == delimiter_priority:
-            if (
-                leaf_idx + 1 < len(line.leaves)
-                and delimiter_priority not in MIGRATE_COMMENT_DELIMITERS
-            ):
-                yield from append_comments(line.leaves[leaf_idx + 1])
             yield current_line
             current_line = Line(
                 mode=line.mode, depth=line.depth, inside_brackets=line.inside_brackets
             )
     if current_line:
         current_line = _safe_add_trailing_comma(
             trailing_comma_safe, delimiter_priority, current_line
         )
         yield current_line
 @dont_increase_indentation

--- a/src/black/nodes.py
+++ b/src/black/nodes.py
@@ -1,26 +1,15 @@
 """
 blib2to3 Node/Leaf transformation-related utility functions.
 """
 import sys
-from typing import (
-    Final,
-    Generic,
-    Iterator,
-    List,
-    Literal,
-    Optional,
-    Set,
-    Tuple,
-    TypeVar,
-    Union,
-)
+from typing import Final, Generic, Iterator, List, Optional, Set, Tuple, TypeVar, Union
 if sys.version_info >= (3, 10):
     from typing import TypeGuard
 else:
     from typing_extensions import TypeGuard
 from mypy_extensions import mypyc_attr
 from black.cache import CACHE_DIR
 from black.mode import Mode, Preview
 from black.strings import get_string_prefix, has_triple_quotes
 from blib2to3 import pygram
 from blib2to3.pgen2 import token
@@ -720,33 +709,30 @@
 def is_name_token(nl: NL) -> TypeGuard[Leaf]:
     return nl.type == token.NAME
 def is_lpar_token(nl: NL) -> TypeGuard[Leaf]:
     return nl.type == token.LPAR
 def is_rpar_token(nl: NL) -> TypeGuard[Leaf]:
     return nl.type == token.RPAR
 def is_string_token(nl: NL) -> TypeGuard[Leaf]:
     return nl.type == token.STRING
 def is_number_token(nl: NL) -> TypeGuard[Leaf]:
     return nl.type == token.NUMBER
-def get_annotation_type(leaf: Leaf) -> Literal["return", "param", None]:
-    """Returns the type of annotation this leaf is part of, if any."""
+def is_part_of_annotation(leaf: Leaf) -> bool:
+    """Returns whether this leaf is part of type annotations."""
     ancestor = leaf.parent
     while ancestor is not None:
         if ancestor.prev_sibling and ancestor.prev_sibling.type == token.RARROW:
-            return "return"
+            return True
         if ancestor.parent and ancestor.parent.type == syms.tname:
-            return "param"
+            return True
         ancestor = ancestor.parent
-    return None
-def is_part_of_annotation(leaf: Leaf) -> bool:
-    """Returns whether this leaf is part of a type annotation."""
-    return get_annotation_type(leaf) is not None
+    return False
 def first_leaf(node: LN) -> Optional[Leaf]:
     """Returns the first leaf of the ancestor node."""
     if isinstance(node, Leaf):
         return node
     elif not node.children:
         return None
     else:
         return first_leaf(node.children[0])
 def last_leaf(node: LN) -> Optional[Leaf]:
     """Returns the last leaf of the ancestor node."""

--- a/src/black/parsing.py
+++ b/src/black/parsing.py
@@ -70,22 +70,20 @@
     try:
         drv.parse_string(src_txt, True)
     except (ParseError, TokenError, IndentationError):
         return False
     else:
         return True
 def lib2to3_unparse(node: Node) -> str:
     """Given a lib2to3 node, return its string representation."""
     code = str(node)
     return code
-class ASTSafetyError(Exception):
-    """Raised when Black's generated code is not equivalent to the old AST."""
 def _parse_single_version(
     src: str, version: Tuple[int, int], *, type_comments: bool
 ) -> ast.AST:
     filename = "<unknown>"
     with warnings.catch_warnings():
         warnings.simplefilter("ignore", SyntaxWarning)
         warnings.simplefilter("ignore", DeprecationWarning)
         return ast.parse(
             src, filename, feature_version=version, type_comments=type_comments
         )
@@ -101,73 +99,54 @@
     for version in sorted(versions, reverse=True):
         try:
             return _parse_single_version(src, version, type_comments=False)
         except SyntaxError:
             pass
     raise SyntaxError(first_error)
 def _normalize(lineend: str, value: str) -> str:
     stripped: List[str] = [i.strip() for i in value.splitlines()]
     normalized = lineend.join(stripped)
     return normalized.strip()
-def stringify_ast(node: ast.AST) -> Iterator[str]:
+def stringify_ast(node: ast.AST, depth: int = 0) -> Iterator[str]:
     """Simple visitor generating strings to compare ASTs by content."""
-    return _stringify_ast(node, [])
-def _stringify_ast_with_new_parent(
-    node: ast.AST, parent_stack: List[ast.AST], new_parent: ast.AST
-) -> Iterator[str]:
-    parent_stack.append(new_parent)
-    yield from _stringify_ast(node, parent_stack)
-    parent_stack.pop()
-def _stringify_ast(node: ast.AST, parent_stack: List[ast.AST]) -> Iterator[str]:
     if (
         isinstance(node, ast.Constant)
         and isinstance(node.value, str)
         and node.kind == "u"
     ):
         node.kind = None
-    yield f"{'    ' * len(parent_stack)}{node.__class__.__name__}("
+    yield f"{'  ' * depth}{node.__class__.__name__}("
     for field in sorted(node._fields):  # noqa: F402
         if isinstance(node, ast.TypeIgnore):
             break
         try:
             value: object = getattr(node, field)
         except AttributeError:
             continue
-        yield f"{'    ' * (len(parent_stack) + 1)}{field}="
+        yield f"{'  ' * (depth + 1)}{field}="
         if isinstance(value, list):
             for item in value:
                 if (
                     field == "targets"
                     and isinstance(node, ast.Delete)
                     and isinstance(item, ast.Tuple)
                 ):
                     for elt in item.elts:
-                        yield from _stringify_ast_with_new_parent(
-                            elt, parent_stack, node
-                        )
+                        yield from stringify_ast(elt, depth + 2)
                 elif isinstance(item, ast.AST):
-                    yield from _stringify_ast_with_new_parent(item, parent_stack, node)
+                    yield from stringify_ast(item, depth + 2)
         elif isinstance(value, ast.AST):
-            yield from _stringify_ast_with_new_parent(value, parent_stack, node)
+            yield from stringify_ast(value, depth + 2)
         else:
             normalized: object
             if (
                 isinstance(node, ast.Constant)
                 and field == "value"
                 and isinstance(value, str)
-                and len(parent_stack) >= 2
-                and isinstance(parent_stack[-1], ast.Expr)
-                and isinstance(
-                    parent_stack[-2],
-                    (ast.FunctionDef, ast.AsyncFunctionDef, ast.Module, ast.ClassDef),
-                )
             ):
                 normalized = _normalize("\n", value)
             elif field == "type_comment" and isinstance(value, str):
                 normalized = value.rstrip()
             else:
                 normalized = value
-            yield (
-                f"{'    ' * (len(parent_stack) + 1)}{normalized!r},  #"
-                f" {value.__class__.__name__}"
-            )
-    yield f"{'    ' * len(parent_stack)})  # /{node.__class__.__name__}"
+            yield f"{'  ' * (depth + 2)}{normalized!r},  # {value.__class__.__name__}"
+    yield f"{'  ' * depth})  # /{node.__class__.__name__}"

--- a/src/black/ranges.py
+++ b/src/black/ranges.py
@@ -30,43 +30,20 @@
             raise ValueError(
                 "Incorrect --line-ranges value, expect integer ranges, found"
                 f" {lines_str!r}"
             ) from None
         else:
             lines.append((start, end))
     return lines
 def is_valid_line_range(lines: Tuple[int, int]) -> bool:
     """Returns whether the line range is valid."""
     return not lines or lines[0] <= lines[1]
-def sanitized_lines(
-    lines: Collection[Tuple[int, int]], src_contents: str
-) -> Collection[Tuple[int, int]]:
-    """Returns the valid line ranges for the given source.
-    This removes ranges that are entirely outside the valid lines.
-    Other ranges are normalized so that the start values are at least 1 and the
-    end values are at most the (1-based) index of the last source line.
-    """
-    if not src_contents:
-        return []
-    good_lines = []
-    src_line_count = src_contents.count("\n")
-    if not src_contents.endswith("\n"):
-        src_line_count += 1
-    for start, end in lines:
-        if start > src_line_count:
-            continue
-        start = max(start, 1)
-        if end < start:
-            continue
-        end = min(end, src_line_count)
-        good_lines.append((start, end))
-    return good_lines
 def adjusted_lines(
     lines: Collection[Tuple[int, int]],
     original_source: str,
     modified_source: str,
 ) -> List[Tuple[int, int]]:
     """Returns the adjusted line ranges based on edits from the original code.
     This computes the new line ranges by diffing original_source and
     modified_source, and adjust each range based on how the range overlaps with
     the diffs.
     Note the diff can contain lines outside of the original line ranges. This can

--- a/src/black/strings.py
+++ b/src/black/strings.py
@@ -4,20 +4,21 @@
 import re
 import sys
 from functools import lru_cache
 from typing import Final, List, Match, Pattern
 from black._width_table import WIDTH_TABLE
 from blib2to3.pytree import Leaf
 STRING_PREFIX_CHARS: Final = "furbFURB"  # All possible string prefix characters.
 STRING_PREFIX_RE: Final = re.compile(
     r"^([" + STRING_PREFIX_CHARS + r"]*)(.*)$", re.DOTALL
 )
+FIRST_NON_WHITESPACE_RE: Final = re.compile(r"\s*\t+\s*(\S)")
 UNICODE_ESCAPE_RE: Final = re.compile(
     r"(?P<backslashes>\\+)(?P<body>"
     r"(u(?P<u>[a-fA-F0-9]{4}))"  # Character with 16-bit hex value xxxx
     r"|(U(?P<U>[a-fA-F0-9]{8}))"  # Character with 32-bit hex value xxxxxxxx
     r"|(x(?P<x>[a-fA-F0-9]{2}))"  # Character with hex value hh
     r"|(N\{(?P<N>[a-zA-Z0-9 \-]{2,})\})"  # Character named name in the Unicode database
     r")",
     re.VERBOSE,
 )
 def sub_twice(regex: Pattern[str], replacement: str, original: str) -> str:
@@ -33,27 +34,29 @@
     """
     raw_string = string.lstrip(STRING_PREFIX_CHARS)
     return raw_string[:3] in {'"""', "'''"}
 def lines_with_leading_tabs_expanded(s: str) -> List[str]:
     """
     Splits string into lines and expands only leading tabs (following the normal
     Python rules)
     """
     lines = []
     for line in s.splitlines():
-        stripped_line = line.lstrip()
-        if not stripped_line or stripped_line == line:
+        match = FIRST_NON_WHITESPACE_RE.match(line)
+        if match:
+            first_non_whitespace_idx = match.start(1)
+            lines.append(
+                line[:first_non_whitespace_idx].expandtabs()
+                + line[first_non_whitespace_idx:]
+            )
+        else:
             lines.append(line)
-        else:
-            prefix_length = len(line) - len(stripped_line)
-            prefix = line[:prefix_length].expandtabs()
-            lines.append(prefix + stripped_line)
     if s.endswith("\n"):
         lines.append("")
     return lines
 def fix_docstring(docstring: str, prefix: str) -> str:
     if not docstring:
         return ""
     lines = lines_with_leading_tabs_expanded(docstring)
     indent = sys.maxsize
     for line in lines[1:]:
         stripped = line.lstrip()

--- a/src/blib2to3/pytree.py
+++ b/src/blib2to3/pytree.py
@@ -18,25 +18,23 @@
 )
 from blib2to3.pgen2.grammar import Grammar
 __author__ = "Guido van Rossum <guido@python.org>"
 import sys
 from io import StringIO
 HUGE: int = 0x7FFFFFFF  # maximum repeat count, default max
 _type_reprs: Dict[int, Union[str, int]] = {}
 def type_repr(type_num: int) -> Union[str, int]:
     global _type_reprs
     if not _type_reprs:
-        from . import pygram
-        if not hasattr(pygram, "python_symbols"):
-            pygram.initialize(cache_dir=None)
-        for name in dir(pygram.python_symbols):
-            val = getattr(pygram.python_symbols, name)
+        from .pygram import python_symbols
+        for name in dir(python_symbols):
+            val = getattr(python_symbols, name)
             if type(val) == int:
                 _type_reprs[val] = name
     return _type_reprs.setdefault(type_num, type_num)
 _P = TypeVar("_P", bound="Base")
 NL = Union["Node", "Leaf"]
 Context = Tuple[str, Tuple[int, int]]
 RawNode = Tuple[int, Optional[str], Optional[Context], Optional[List[NL]]]
 class Base:
     """
     Abstract base class for Node and Leaf.
