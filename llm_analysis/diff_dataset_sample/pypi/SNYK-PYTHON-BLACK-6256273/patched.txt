# ====================================================================
# FILE: docs/conf.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| import os
     2| import string
     3| from importlib.metadata import version
     4| from pathlib import Path
     5| CURRENT_DIR = Path(__file__).parent
     6| def make_pypi_svg(version: str) -> None:
     7|     template: Path = CURRENT_DIR / "_static" / "pypi_template.svg"
     8|     target: Path = CURRENT_DIR / "_static" / "pypi.svg"
     9|     with open(str(template), "r", encoding="utf8") as f:
    10|         svg: str = string.Template(f.read()).substitute(version=version)
    11|     with open(str(target), "w", encoding="utf8") as f:
    12|         f.write(svg)
    13| os.putenv("pythonioencoding", "utf-8")
    14| project = "Black"
    15| copyright = "2018-Present, Łukasz Langa and contributors to Black"
    16| author = "Łukasz Langa and contributors to Black"
    17| release = version("black").split("+")[0]
    18| version = release
    19| for sp in "abcfr":
    20|     version = version.split(sp)[0]
    21| make_pypi_svg(release)
    22| needs_sphinx = "4.4"
    23| extensions = [
    24|     "sphinx.ext.autodoc",
    25|     "sphinx.ext.intersphinx",
    26|     "sphinx.ext.napoleon",
    27|     "myst_parser",
    28|     "sphinxcontrib.programoutput",
    29|     "sphinx_copybutton",
    30| ]
    31| needs_extensions = {"myst_parser": "0.13.7"}
    32| templates_path = ["_templates"]
    33| source_suffix = [".rst", ".md"]
    34| master_doc = "index"
    35| language = "en"
    36| exclude_patterns = ["_build", "Thumbs.db", ".DS_Store"]
    37| pygments_style = "sphinx"


# ====================================================================
# FILE: src/black/__init__.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 58-110 ---
    58|     TRANSFORMED_MAGICS,
    59|     jupyter_dependencies_are_installed,
    60|     mask_cell,
    61|     put_trailing_semicolon_back,
    62|     remove_trailing_semicolon,
    63|     unmask_cell,
    64| )
    65| from black.linegen import LN, LineGenerator, transform_line
    66| from black.lines import EmptyLineTracker, LinesBlock
    67| from black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature
    68| from black.mode import Mode as Mode  # re-exported
    69| from black.mode import Preview, TargetVersion, supports_feature
    70| from black.nodes import (
    71|     STARS,
    72|     is_number_token,
    73|     is_simple_decorator_expression,
    74|     is_string_token,
    75|     syms,
    76| )
    77| from black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out
    78| from black.parsing import (  # noqa F401
    79|     ASTSafetyError,
    80|     InvalidInput,
    81|     lib2to3_parse,
    82|     parse_ast,
    83|     stringify_ast,
    84| )
    85| from black.ranges import (
    86|     adjusted_lines,
    87|     convert_unchanged_lines,
    88|     parse_line_ranges,
    89|     sanitized_lines,
    90| )
    91| from black.report import Changed, NothingChanged, Report
    92| from black.trans import iter_fexpr_spans
    93| from blib2to3.pgen2 import token
    94| from blib2to3.pytree import Leaf, Node
    95| COMPILED = Path(__file__).suffix in (".pyd", ".so")
    96| FileContent = str
    97| Encoding = str
    98| NewLine = str
    99| class WriteBack(Enum):
   100|     NO = 0
   101|     YES = 1
   102|     DIFF = 2
   103|     CHECK = 3
   104|     COLOR_DIFF = 4
   105|     @classmethod
   106|     def from_configuration(
   107|         cls, *, check: bool, diff: bool, color: bool = False
   108|     ) -> "WriteBack":
   109|         if check and not diff:
   110|             return cls.CHECK

# --- HUNK 2: Lines 1068-1111 ---
  1068|     >>> print(black.format_str("def f(arg:str='')->None:...", mode=black.Mode()))
  1069|     def f(arg: str = "") -> None:
  1070|         ...
  1071|     A more complex example:
  1072|     >>> print(
  1073|     ...   black.format_str(
  1074|     ...     "def f(arg:str='')->None: hey",
  1075|     ...     mode=black.Mode(
  1076|     ...       target_versions={black.TargetVersion.PY36},
  1077|     ...       line_length=10,
  1078|     ...       string_normalization=False,
  1079|     ...       is_pyi=False,
  1080|     ...     ),
  1081|     ...   ),
  1082|     ... )
  1083|     def f(
  1084|         arg: str = '',
  1085|     ) -> None:
  1086|         hey
  1087|     """
  1088|     if lines:
  1089|         lines = sanitized_lines(lines, src_contents)
  1090|         if not lines:
  1091|             return src_contents  # Nothing to format
  1092|     dst_contents = _format_str_once(src_contents, mode=mode, lines=lines)
  1093|     if src_contents != dst_contents:
  1094|         if lines:
  1095|             lines = adjusted_lines(lines, src_contents, dst_contents)
  1096|         return _format_str_once(dst_contents, mode=mode, lines=lines)
  1097|     return dst_contents
  1098| def _format_str_once(
  1099|     src_contents: str, *, mode: Mode, lines: Collection[Tuple[int, int]] = ()
  1100| ) -> str:
  1101|     src_node = lib2to3_parse(src_contents.lstrip(), mode.target_versions)
  1102|     dst_blocks: List[LinesBlock] = []
  1103|     if mode.target_versions:
  1104|         versions = mode.target_versions
  1105|     else:
  1106|         future_imports = get_future_imports(src_node)
  1107|         versions = detect_target_versions(src_node, future_imports=future_imports)
  1108|     context_manager_features = {
  1109|         feature
  1110|         for feature in {Feature.PARENTHESIZED_CONTEXT_MANAGERS}
  1111|         if supports_feature(versions, feature)

# --- HUNK 3: Lines 1315-1374 ---
  1315|             if (
  1316|                 len(child.children) == 2
  1317|                 and first_child.type == token.STRING
  1318|                 and child.children[1].type == token.NEWLINE
  1319|             ):
  1320|                 continue
  1321|             break
  1322|         elif first_child.type == syms.import_from:
  1323|             module_name = first_child.children[1]
  1324|             if not isinstance(module_name, Leaf) or module_name.value != "__future__":
  1325|                 break
  1326|             imports |= set(get_imports_from_children(first_child.children[3:]))
  1327|         else:
  1328|             break
  1329|     return imports
  1330| def assert_equivalent(src: str, dst: str) -> None:
  1331|     """Raise AssertionError if `src` and `dst` aren't equivalent."""
  1332|     try:
  1333|         src_ast = parse_ast(src)
  1334|     except Exception as exc:
  1335|         raise ASTSafetyError(
  1336|             "cannot use --safe with this file; failed to parse source file AST: "
  1337|             f"{exc}\n"
  1338|             "This could be caused by running Black with an older Python version "
  1339|             "that does not support new syntax used in your source file."
  1340|         ) from exc
  1341|     try:
  1342|         dst_ast = parse_ast(dst)
  1343|     except Exception as exc:
  1344|         log = dump_to_file("".join(traceback.format_tb(exc.__traceback__)), dst)
  1345|         raise ASTSafetyError(
  1346|             f"INTERNAL ERROR: Black produced invalid code: {exc}. "
  1347|             "Please report a bug on https://github.com/psf/black/issues.  "
  1348|             f"This invalid output might be helpful: {log}"
  1349|         ) from None
  1350|     src_ast_str = "\n".join(stringify_ast(src_ast))
  1351|     dst_ast_str = "\n".join(stringify_ast(dst_ast))
  1352|     if src_ast_str != dst_ast_str:
  1353|         log = dump_to_file(diff(src_ast_str, dst_ast_str, "src", "dst"))
  1354|         raise ASTSafetyError(
  1355|             "INTERNAL ERROR: Black produced code that is not equivalent to the"
  1356|             " source.  Please report a bug on "
  1357|             f"https://github.com/psf/black/issues.  This diff might be helpful: {log}"
  1358|         ) from None
  1359| def assert_stable(
  1360|     src: str, dst: str, mode: Mode, *, lines: Collection[Tuple[int, int]] = ()
  1361| ) -> None:
  1362|     """Raise AssertionError if `dst` reformats differently the second time."""
  1363|     if lines:
  1364|         return
  1365|     newdst = _format_str_once(dst, mode=mode, lines=lines)
  1366|     if dst != newdst:
  1367|         log = dump_to_file(
  1368|             str(mode),
  1369|             diff(src, dst, "source", "first pass"),
  1370|             diff(dst, newdst, "first pass", "second pass"),
  1371|         )
  1372|         raise AssertionError(
  1373|             "INTERNAL ERROR: Black produced different code on the second pass of the"
  1374|             " formatter.  Please report a bug on https://github.com/psf/black/issues."


# ====================================================================
# FILE: src/black/linegen.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-58 ---
     1| """
     2| Generating lines of code.
     3| """
     4| import re
     5| import sys
     6| from dataclasses import replace
     7| from enum import Enum, auto
     8| from functools import partial, wraps
     9| from typing import Collection, Iterator, List, Optional, Set, Union, cast
    10| from black.brackets import (
    11|     COMMA_PRIORITY,
    12|     DOT_PRIORITY,
    13|     STRING_PRIORITY,
    14|     get_leaves_inside_matching_brackets,
    15|     max_delimiter_priority_in_atom,
    16| )
    17| from black.comments import FMT_OFF, generate_comments, list_comments
    18| from black.lines import (
    19|     Line,
    20|     RHSResult,
    21|     append_leaves,
    22|     can_be_split,
    23|     can_omit_invisible_parens,
    24|     is_line_short_enough,
    25|     line_to_string,
    26| )
    27| from black.mode import Feature, Mode, Preview
    28| from black.nodes import (
    29|     ASSIGNMENTS,
    30|     BRACKETS,
    31|     CLOSING_BRACKETS,
    32|     OPENING_BRACKETS,
    33|     STANDALONE_COMMENT,
    34|     STATEMENT,
    35|     WHITESPACE,
    36|     Visitor,
    37|     ensure_visible,
    38|     get_annotation_type,
    39|     is_arith_like,
    40|     is_async_stmt_or_funcdef,
    41|     is_atom_with_invisible_parens,
    42|     is_docstring,
    43|     is_empty_tuple,
    44|     is_lpar_token,
    45|     is_multiline_string,
    46|     is_name_token,
    47|     is_one_sequence_between,
    48|     is_one_tuple,
    49|     is_parent_function_or_class,
    50|     is_part_of_annotation,
    51|     is_rpar_token,
    52|     is_stub_body,
    53|     is_stub_suite,
    54|     is_tuple_containing_walrus,
    55|     is_type_ignore_comment_string,
    56|     is_vararg,
    57|     is_walrus_assignment,
    58|     is_yield,

# --- HUNK 2: Lines 811-851 ---
   811|     If it's the body component, the result line is one-indented inside brackets and as
   812|     such has its first leaf's prefix normalized and a trailing comma added when
   813|     expected.
   814|     """
   815|     result = Line(mode=original.mode, depth=original.depth)
   816|     if component is _BracketSplitComponent.body:
   817|         result.inside_brackets = True
   818|         result.depth += 1
   819|         if leaves:
   820|             no_commas = (
   821|                 original.is_def
   822|                 and opening_bracket.value == "("
   823|                 and not any(
   824|                     leaf.type == token.COMMA
   825|                     and (
   826|                         Preview.typed_params_trailing_comma not in original.mode
   827|                         or not is_part_of_annotation(leaf)
   828|                     )
   829|                     for leaf in leaves
   830|                 )
   831|                 and get_annotation_type(leaves[0]) != "return"
   832|                 and not (
   833|                     leaves[0].parent
   834|                     and leaves[0].parent.next_sibling
   835|                     and leaves[0].parent.next_sibling.type == token.VBAR
   836|                 )
   837|             )
   838|             if original.is_import or no_commas:
   839|                 for i in range(len(leaves) - 1, -1, -1):
   840|                     if leaves[i].type == STANDALONE_COMMENT:
   841|                         continue
   842|                     if leaves[i].type != token.COMMA:
   843|                         new_comma = Leaf(token.COMMA, ",")
   844|                         leaves.insert(i + 1, new_comma)
   845|                     break
   846|     leaves_to_track: Set[LeafID] = set()
   847|     if component is _BracketSplitComponent.head:
   848|         leaves_to_track = get_leaves_inside_matching_brackets(leaves)
   849|     for leaf in leaves:
   850|         result.append(
   851|             leaf,

# --- HUNK 3: Lines 859-980 ---
   859|     ):
   860|         result.should_split_rhs = True
   861|     return result
   862| def dont_increase_indentation(split_func: Transformer) -> Transformer:
   863|     """Normalize prefix of the first leaf in every line returned by `split_func`.
   864|     This is a decorator over relevant split functions.
   865|     """
   866|     @wraps(split_func)
   867|     def split_wrapper(
   868|         line: Line, features: Collection[Feature], mode: Mode
   869|     ) -> Iterator[Line]:
   870|         for split_line in split_func(line, features, mode):
   871|             split_line.leaves[0].prefix = ""
   872|             yield split_line
   873|     return split_wrapper
   874| def _get_last_non_comment_leaf(line: Line) -> Optional[int]:
   875|     for leaf_idx in range(len(line.leaves) - 1, 0, -1):
   876|         if line.leaves[leaf_idx].type != STANDALONE_COMMENT:
   877|             return leaf_idx
   878|     return None
   879| def _can_add_trailing_comma(leaf: Leaf, features: Collection[Feature]) -> bool:
   880|     if is_vararg(leaf, within={syms.typedargslist}):
   881|         return Feature.TRAILING_COMMA_IN_DEF in features
   882|     if is_vararg(leaf, within={syms.arglist, syms.argument}):
   883|         return Feature.TRAILING_COMMA_IN_CALL in features
   884|     return True
   885| def _safe_add_trailing_comma(safe: bool, delimiter_priority: int, line: Line) -> Line:
   886|     if (
   887|         safe
   888|         and delimiter_priority == COMMA_PRIORITY
   889|         and line.leaves[-1].type != token.COMMA
   890|         and line.leaves[-1].type != STANDALONE_COMMENT
   891|     ):
   892|         new_comma = Leaf(token.COMMA, ",")
   893|         line.append(new_comma)
   894|     return line
   895| MIGRATE_COMMENT_DELIMITERS = {STRING_PRIORITY, COMMA_PRIORITY}
   896| @dont_increase_indentation
   897| def delimiter_split(
   898|     line: Line, features: Collection[Feature], mode: Mode
   899| ) -> Iterator[Line]:
   900|     """Split according to delimiters of the highest priority.
   901|     If the appropriate Features are given, the split will add trailing commas
   902|     also in function signatures and calls that contain `*` and `**`.
   903|     """
   904|     if len(line.leaves) == 0:
   905|         raise CannotSplit("Line empty") from None
   906|     last_leaf = line.leaves[-1]
   907|     bt = line.bracket_tracker
   908|     try:
   909|         delimiter_priority = bt.max_delimiter_priority(exclude={id(last_leaf)})
   910|     except ValueError:
   911|         raise CannotSplit("No delimiters found") from None
   912|     if (
   913|         delimiter_priority == DOT_PRIORITY
   914|         and bt.delimiter_count_with_priority(delimiter_priority) == 1
   915|     ):
   916|         raise CannotSplit("Splitting a single attribute from its owner looks wrong")
   917|     current_line = Line(
   918|         mode=line.mode, depth=line.depth, inside_brackets=line.inside_brackets
   919|     )
   920|     lowest_depth = sys.maxsize
   921|     trailing_comma_safe = True
   922|     def append_to_line(leaf: Leaf) -> Iterator[Line]:
   923|         """Append `leaf` to current line or to new line if appending impossible."""
   924|         nonlocal current_line
   925|         try:
   926|             current_line.append_safe(leaf, preformatted=True)
   927|         except ValueError:
   928|             yield current_line
   929|             current_line = Line(
   930|                 mode=line.mode, depth=line.depth, inside_brackets=line.inside_brackets
   931|             )
   932|             current_line.append(leaf)
   933|     def append_comments(leaf: Leaf) -> Iterator[Line]:
   934|         for comment_after in line.comments_after(leaf):
   935|             yield from append_to_line(comment_after)
   936|     last_non_comment_leaf = _get_last_non_comment_leaf(line)
   937|     for leaf_idx, leaf in enumerate(line.leaves):
   938|         yield from append_to_line(leaf)
   939|         previous_priority = leaf_idx > 0 and bt.delimiters.get(
   940|             id(line.leaves[leaf_idx - 1])
   941|         )
   942|         if (
   943|             previous_priority != delimiter_priority
   944|             or delimiter_priority in MIGRATE_COMMENT_DELIMITERS
   945|         ):
   946|             yield from append_comments(leaf)
   947|         lowest_depth = min(lowest_depth, leaf.bracket_depth)
   948|         if trailing_comma_safe and leaf.bracket_depth == lowest_depth:
   949|             trailing_comma_safe = _can_add_trailing_comma(leaf, features)
   950|         if last_leaf.type == STANDALONE_COMMENT and leaf_idx == last_non_comment_leaf:
   951|             current_line = _safe_add_trailing_comma(
   952|                 trailing_comma_safe, delimiter_priority, current_line
   953|             )
   954|         leaf_priority = bt.delimiters.get(id(leaf))
   955|         if leaf_priority == delimiter_priority:
   956|             if (
   957|                 leaf_idx + 1 < len(line.leaves)
   958|                 and delimiter_priority not in MIGRATE_COMMENT_DELIMITERS
   959|             ):
   960|                 yield from append_comments(line.leaves[leaf_idx + 1])
   961|             yield current_line
   962|             current_line = Line(
   963|                 mode=line.mode, depth=line.depth, inside_brackets=line.inside_brackets
   964|             )
   965|     if current_line:
   966|         current_line = _safe_add_trailing_comma(
   967|             trailing_comma_safe, delimiter_priority, current_line
   968|         )
   969|         yield current_line
   970| @dont_increase_indentation
   971| def standalone_comment_split(
   972|     line: Line, features: Collection[Feature], mode: Mode
   973| ) -> Iterator[Line]:
   974|     """Split standalone comments from the rest of the line."""
   975|     if not line.contains_standalone_comments():
   976|         raise CannotSplit("Line does not have any standalone comments")
   977|     current_line = Line(
   978|         mode=line.mode, depth=line.depth, inside_brackets=line.inside_brackets
   979|     )
   980|     def append_to_line(leaf: Leaf) -> Iterator[Line]:


# ====================================================================
# FILE: src/black/nodes.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-36 ---
     1| """
     2| blib2to3 Node/Leaf transformation-related utility functions.
     3| """
     4| import sys
     5| from typing import (
     6|     Final,
     7|     Generic,
     8|     Iterator,
     9|     List,
    10|     Literal,
    11|     Optional,
    12|     Set,
    13|     Tuple,
    14|     TypeVar,
    15|     Union,
    16| )
    17| if sys.version_info >= (3, 10):
    18|     from typing import TypeGuard
    19| else:
    20|     from typing_extensions import TypeGuard
    21| from mypy_extensions import mypyc_attr
    22| from black.cache import CACHE_DIR
    23| from black.mode import Mode, Preview
    24| from black.strings import get_string_prefix, has_triple_quotes
    25| from blib2to3 import pygram
    26| from blib2to3.pgen2 import token
    27| from blib2to3.pytree import NL, Leaf, Node, type_repr
    28| pygram.initialize(CACHE_DIR)
    29| syms: Final = pygram.python_symbols
    30| T = TypeVar("T")
    31| LN = Union[Leaf, Node]
    32| LeafID = int
    33| NodeType = int
    34| WHITESPACE: Final = {token.DEDENT, token.INDENT, token.NEWLINE}
    35| STATEMENT: Final = {
    36|     syms.if_stmt,

# --- HUNK 2: Lines 710-762 ---
   710|     return wrapped
   711| def ensure_visible(leaf: Leaf) -> None:
   712|     """Make sure parentheses are visible.
   713|     They could be invisible as part of some statements (see
   714|     :func:`normalize_invisible_parens` and :func:`visit_import_from`).
   715|     """
   716|     if leaf.type == token.LPAR:
   717|         leaf.value = "("
   718|     elif leaf.type == token.RPAR:
   719|         leaf.value = ")"
   720| def is_name_token(nl: NL) -> TypeGuard[Leaf]:
   721|     return nl.type == token.NAME
   722| def is_lpar_token(nl: NL) -> TypeGuard[Leaf]:
   723|     return nl.type == token.LPAR
   724| def is_rpar_token(nl: NL) -> TypeGuard[Leaf]:
   725|     return nl.type == token.RPAR
   726| def is_string_token(nl: NL) -> TypeGuard[Leaf]:
   727|     return nl.type == token.STRING
   728| def is_number_token(nl: NL) -> TypeGuard[Leaf]:
   729|     return nl.type == token.NUMBER
   730| def get_annotation_type(leaf: Leaf) -> Literal["return", "param", None]:
   731|     """Returns the type of annotation this leaf is part of, if any."""
   732|     ancestor = leaf.parent
   733|     while ancestor is not None:
   734|         if ancestor.prev_sibling and ancestor.prev_sibling.type == token.RARROW:
   735|             return "return"
   736|         if ancestor.parent and ancestor.parent.type == syms.tname:
   737|             return "param"
   738|         ancestor = ancestor.parent
   739|     return None
   740| def is_part_of_annotation(leaf: Leaf) -> bool:
   741|     """Returns whether this leaf is part of a type annotation."""
   742|     return get_annotation_type(leaf) is not None
   743| def first_leaf(node: LN) -> Optional[Leaf]:
   744|     """Returns the first leaf of the ancestor node."""
   745|     if isinstance(node, Leaf):
   746|         return node
   747|     elif not node.children:
   748|         return None
   749|     else:
   750|         return first_leaf(node.children[0])
   751| def last_leaf(node: LN) -> Optional[Leaf]:
   752|     """Returns the last leaf of the ancestor node."""
   753|     if isinstance(node, Leaf):
   754|         return node
   755|     elif not node.children:
   756|         return None
   757|     else:
   758|         return last_leaf(node.children[-1])
   759| def furthest_ancestor_with_last_leaf(leaf: Leaf) -> LN:
   760|     """Returns the furthest ancestor that has this leaf node as the last leaf."""
   761|     node: LN = leaf
   762|     while node.parent and node.parent.children and node is node.parent.children[-1]:


# ====================================================================
# FILE: src/black/parsing.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 60-173 ---
    60|             )
    61|     else:
    62|         assert len(errors) >= 1
    63|         exc = errors[max(errors)]
    64|         raise exc from None
    65|     if isinstance(result, Leaf):
    66|         result = Node(syms.file_input, [result])
    67|     return result
    68| def matches_grammar(src_txt: str, grammar: Grammar) -> bool:
    69|     drv = driver.Driver(grammar)
    70|     try:
    71|         drv.parse_string(src_txt, True)
    72|     except (ParseError, TokenError, IndentationError):
    73|         return False
    74|     else:
    75|         return True
    76| def lib2to3_unparse(node: Node) -> str:
    77|     """Given a lib2to3 node, return its string representation."""
    78|     code = str(node)
    79|     return code
    80| class ASTSafetyError(Exception):
    81|     """Raised when Black's generated code is not equivalent to the old AST."""
    82| def _parse_single_version(
    83|     src: str, version: Tuple[int, int], *, type_comments: bool
    84| ) -> ast.AST:
    85|     filename = "<unknown>"
    86|     with warnings.catch_warnings():
    87|         warnings.simplefilter("ignore", SyntaxWarning)
    88|         warnings.simplefilter("ignore", DeprecationWarning)
    89|         return ast.parse(
    90|             src, filename, feature_version=version, type_comments=type_comments
    91|         )
    92| def parse_ast(src: str) -> ast.AST:
    93|     versions = [(3, minor) for minor in range(3, sys.version_info[1] + 1)]
    94|     first_error = ""
    95|     for version in sorted(versions, reverse=True):
    96|         try:
    97|             return _parse_single_version(src, version, type_comments=True)
    98|         except SyntaxError as e:
    99|             if not first_error:
   100|                 first_error = str(e)
   101|     for version in sorted(versions, reverse=True):
   102|         try:
   103|             return _parse_single_version(src, version, type_comments=False)
   104|         except SyntaxError:
   105|             pass
   106|     raise SyntaxError(first_error)
   107| def _normalize(lineend: str, value: str) -> str:
   108|     stripped: List[str] = [i.strip() for i in value.splitlines()]
   109|     normalized = lineend.join(stripped)
   110|     return normalized.strip()
   111| def stringify_ast(node: ast.AST) -> Iterator[str]:
   112|     """Simple visitor generating strings to compare ASTs by content."""
   113|     return _stringify_ast(node, [])
   114| def _stringify_ast_with_new_parent(
   115|     node: ast.AST, parent_stack: List[ast.AST], new_parent: ast.AST
   116| ) -> Iterator[str]:
   117|     parent_stack.append(new_parent)
   118|     yield from _stringify_ast(node, parent_stack)
   119|     parent_stack.pop()
   120| def _stringify_ast(node: ast.AST, parent_stack: List[ast.AST]) -> Iterator[str]:
   121|     if (
   122|         isinstance(node, ast.Constant)
   123|         and isinstance(node.value, str)
   124|         and node.kind == "u"
   125|     ):
   126|         node.kind = None
   127|     yield f"{'    ' * len(parent_stack)}{node.__class__.__name__}("
   128|     for field in sorted(node._fields):  # noqa: F402
   129|         if isinstance(node, ast.TypeIgnore):
   130|             break
   131|         try:
   132|             value: object = getattr(node, field)
   133|         except AttributeError:
   134|             continue
   135|         yield f"{'    ' * (len(parent_stack) + 1)}{field}="
   136|         if isinstance(value, list):
   137|             for item in value:
   138|                 if (
   139|                     field == "targets"
   140|                     and isinstance(node, ast.Delete)
   141|                     and isinstance(item, ast.Tuple)
   142|                 ):
   143|                     for elt in item.elts:
   144|                         yield from _stringify_ast_with_new_parent(
   145|                             elt, parent_stack, node
   146|                         )
   147|                 elif isinstance(item, ast.AST):
   148|                     yield from _stringify_ast_with_new_parent(item, parent_stack, node)
   149|         elif isinstance(value, ast.AST):
   150|             yield from _stringify_ast_with_new_parent(value, parent_stack, node)
   151|         else:
   152|             normalized: object
   153|             if (
   154|                 isinstance(node, ast.Constant)
   155|                 and field == "value"
   156|                 and isinstance(value, str)
   157|                 and len(parent_stack) >= 2
   158|                 and isinstance(parent_stack[-1], ast.Expr)
   159|                 and isinstance(
   160|                     parent_stack[-2],
   161|                     (ast.FunctionDef, ast.AsyncFunctionDef, ast.Module, ast.ClassDef),
   162|                 )
   163|             ):
   164|                 normalized = _normalize("\n", value)
   165|             elif field == "type_comment" and isinstance(value, str):
   166|                 normalized = value.rstrip()
   167|             else:
   168|                 normalized = value
   169|             yield (
   170|                 f"{'    ' * (len(parent_stack) + 1)}{normalized!r},  #"
   171|                 f" {value.__class__.__name__}"
   172|             )
   173|     yield f"{'    ' * len(parent_stack)})  # /{node.__class__.__name__}"


# ====================================================================
# FILE: src/black/ranges.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 20-82 ---
    20|         parts = lines_str.split("-")
    21|         if len(parts) != 2:
    22|             raise ValueError(
    23|                 "Incorrect --line-ranges format, expect 'START-END', found"
    24|                 f" {lines_str!r}"
    25|             )
    26|         try:
    27|             start = int(parts[0])
    28|             end = int(parts[1])
    29|         except ValueError:
    30|             raise ValueError(
    31|                 "Incorrect --line-ranges value, expect integer ranges, found"
    32|                 f" {lines_str!r}"
    33|             ) from None
    34|         else:
    35|             lines.append((start, end))
    36|     return lines
    37| def is_valid_line_range(lines: Tuple[int, int]) -> bool:
    38|     """Returns whether the line range is valid."""
    39|     return not lines or lines[0] <= lines[1]
    40| def sanitized_lines(
    41|     lines: Collection[Tuple[int, int]], src_contents: str
    42| ) -> Collection[Tuple[int, int]]:
    43|     """Returns the valid line ranges for the given source.
    44|     This removes ranges that are entirely outside the valid lines.
    45|     Other ranges are normalized so that the start values are at least 1 and the
    46|     end values are at most the (1-based) index of the last source line.
    47|     """
    48|     if not src_contents:
    49|         return []
    50|     good_lines = []
    51|     src_line_count = src_contents.count("\n")
    52|     if not src_contents.endswith("\n"):
    53|         src_line_count += 1
    54|     for start, end in lines:
    55|         if start > src_line_count:
    56|             continue
    57|         start = max(start, 1)
    58|         if end < start:
    59|             continue
    60|         end = min(end, src_line_count)
    61|         good_lines.append((start, end))
    62|     return good_lines
    63| def adjusted_lines(
    64|     lines: Collection[Tuple[int, int]],
    65|     original_source: str,
    66|     modified_source: str,
    67| ) -> List[Tuple[int, int]]:
    68|     """Returns the adjusted line ranges based on edits from the original code.
    69|     This computes the new line ranges by diffing original_source and
    70|     modified_source, and adjust each range based on how the range overlaps with
    71|     the diffs.
    72|     Note the diff can contain lines outside of the original line ranges. This can
    73|     happen when the formatting has to be done in adjacent to maintain consistent
    74|     local results. For example:
    75|     1. def my_func(arg1, arg2,
    76|     2.             arg3,):
    77|     3.   pass
    78|     If it restricts to line 2-2, it can't simply reformat line 2, it also has
    79|     to reformat line 1:
    80|     1. def my_func(
    81|     2.     arg1,
    82|     3.     arg2,


# ====================================================================
# FILE: src/black/strings.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-69 ---
     1| """
     2| Simple formatting on strings. Further string formatting code is in trans.py.
     3| """
     4| import re
     5| import sys
     6| from functools import lru_cache
     7| from typing import Final, List, Match, Pattern
     8| from black._width_table import WIDTH_TABLE
     9| from blib2to3.pytree import Leaf
    10| STRING_PREFIX_CHARS: Final = "furbFURB"  # All possible string prefix characters.
    11| STRING_PREFIX_RE: Final = re.compile(
    12|     r"^([" + STRING_PREFIX_CHARS + r"]*)(.*)$", re.DOTALL
    13| )
    14| UNICODE_ESCAPE_RE: Final = re.compile(
    15|     r"(?P<backslashes>\\+)(?P<body>"
    16|     r"(u(?P<u>[a-fA-F0-9]{4}))"  # Character with 16-bit hex value xxxx
    17|     r"|(U(?P<U>[a-fA-F0-9]{8}))"  # Character with 32-bit hex value xxxxxxxx
    18|     r"|(x(?P<x>[a-fA-F0-9]{2}))"  # Character with hex value hh
    19|     r"|(N\{(?P<N>[a-zA-Z0-9 \-]{2,})\})"  # Character named name in the Unicode database
    20|     r")",
    21|     re.VERBOSE,
    22| )
    23| def sub_twice(regex: Pattern[str], replacement: str, original: str) -> str:
    24|     """Replace `regex` with `replacement` twice on `original`.
    25|     This is used by string normalization to perform replaces on
    26|     overlapping matches.
    27|     """
    28|     return regex.sub(replacement, regex.sub(replacement, original))
    29| def has_triple_quotes(string: str) -> bool:
    30|     """
    31|     Returns:
    32|         True iff @string starts with three quotation characters.
    33|     """
    34|     raw_string = string.lstrip(STRING_PREFIX_CHARS)
    35|     return raw_string[:3] in {'"""', "'''"}
    36| def lines_with_leading_tabs_expanded(s: str) -> List[str]:
    37|     """
    38|     Splits string into lines and expands only leading tabs (following the normal
    39|     Python rules)
    40|     """
    41|     lines = []
    42|     for line in s.splitlines():
    43|         stripped_line = line.lstrip()
    44|         if not stripped_line or stripped_line == line:
    45|             lines.append(line)
    46|         else:
    47|             prefix_length = len(line) - len(stripped_line)
    48|             prefix = line[:prefix_length].expandtabs()
    49|             lines.append(prefix + stripped_line)
    50|     if s.endswith("\n"):
    51|         lines.append("")
    52|     return lines
    53| def fix_docstring(docstring: str, prefix: str) -> str:
    54|     if not docstring:
    55|         return ""
    56|     lines = lines_with_leading_tabs_expanded(docstring)
    57|     indent = sys.maxsize
    58|     for line in lines[1:]:
    59|         stripped = line.lstrip()
    60|         if stripped:
    61|             indent = min(indent, len(line) - len(stripped))
    62|     trimmed = [lines[0].strip()]
    63|     if indent < sys.maxsize:
    64|         last_line_idx = len(lines) - 2
    65|         for i, line in enumerate(lines[1:]):
    66|             stripped_line = line[indent:].rstrip()
    67|             if stripped_line or i == last_line_idx:
    68|                 trimmed.append(prefix + stripped_line)
    69|             else:


# ====================================================================
# FILE: src/blib2to3/pytree.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 8-52 ---
     8|     Any,
     9|     Dict,
    10|     Iterable,
    11|     Iterator,
    12|     List,
    13|     Optional,
    14|     Set,
    15|     Tuple,
    16|     TypeVar,
    17|     Union,
    18| )
    19| from blib2to3.pgen2.grammar import Grammar
    20| __author__ = "Guido van Rossum <guido@python.org>"
    21| import sys
    22| from io import StringIO
    23| HUGE: int = 0x7FFFFFFF  # maximum repeat count, default max
    24| _type_reprs: Dict[int, Union[str, int]] = {}
    25| def type_repr(type_num: int) -> Union[str, int]:
    26|     global _type_reprs
    27|     if not _type_reprs:
    28|         from . import pygram
    29|         if not hasattr(pygram, "python_symbols"):
    30|             pygram.initialize(cache_dir=None)
    31|         for name in dir(pygram.python_symbols):
    32|             val = getattr(pygram.python_symbols, name)
    33|             if type(val) == int:
    34|                 _type_reprs[val] = name
    35|     return _type_reprs.setdefault(type_num, type_num)
    36| _P = TypeVar("_P", bound="Base")
    37| NL = Union["Node", "Leaf"]
    38| Context = Tuple[str, Tuple[int, int]]
    39| RawNode = Tuple[int, Optional[str], Optional[Context], Optional[List[NL]]]
    40| class Base:
    41|     """
    42|     Abstract base class for Node and Leaf.
    43|     This provides some default functionality and boilerplate using the
    44|     template pattern.
    45|     A node may be a subnode of at most one parent.
    46|     """
    47|     type: int  # int: token number (< 256) or symbol number (>= 256)
    48|     parent: Optional["Node"] = None  # Parent node pointer, or None
    49|     children: List[NL]  # List of subnodes
    50|     was_changed: bool = False
    51|     was_checked: bool = False
    52|     def __new__(cls, *args, **kwds):

