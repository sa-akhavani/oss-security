# ====================================================================
# FILE: docs/conf.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| import os
     2| import string
     3| from pathlib import Path
     4| from pkg_resources import get_distribution
     5| CURRENT_DIR = Path(__file__).parent
     6| def make_pypi_svg(version: str) -> None:
     7|     template: Path = CURRENT_DIR / "_static" / "pypi_template.svg"
     8|     target: Path = CURRENT_DIR / "_static" / "pypi.svg"
     9|     with open(str(template), "r", encoding="utf8") as f:
    10|         svg: str = string.Template(f.read()).substitute(version=version)
    11|     with open(str(target), "w", encoding="utf8") as f:
    12|         f.write(svg)
    13| os.putenv("pythonioencoding", "utf-8")
    14| project = "Black"
    15| copyright = "2018-Present, Łukasz Langa and contributors to Black"
    16| author = "Łukasz Langa and contributors to Black"
    17| release = get_distribution("black").version.split("+")[0]
    18| version = release
    19| for sp in "abcfr":
    20|     version = version.split(sp)[0]
    21| make_pypi_svg(release)
    22| needs_sphinx = "4.4"
    23| extensions = [
    24|     "sphinx.ext.autodoc",
    25|     "sphinx.ext.intersphinx",
    26|     "sphinx.ext.napoleon",
    27|     "myst_parser",
    28|     "sphinxcontrib.programoutput",
    29|     "sphinx_copybutton",
    30| ]
    31| needs_extensions = {"myst_parser": "0.13.7"}
    32| templates_path = ["_templates"]
    33| source_suffix = [".rst", ".md"]
    34| master_doc = "index"
    35| language = "en"
    36| exclude_patterns = ["_build", "Thumbs.db", ".DS_Store"]
    37| pygments_style = "sphinx"


# ====================================================================
# FILE: src/black/__init__.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 58-100 ---
    58|     TRANSFORMED_MAGICS,
    59|     jupyter_dependencies_are_installed,
    60|     mask_cell,
    61|     put_trailing_semicolon_back,
    62|     remove_trailing_semicolon,
    63|     unmask_cell,
    64| )
    65| from black.linegen import LN, LineGenerator, transform_line
    66| from black.lines import EmptyLineTracker, LinesBlock
    67| from black.mode import FUTURE_FLAG_TO_FEATURE, VERSION_TO_FEATURES, Feature
    68| from black.mode import Mode as Mode  # re-exported
    69| from black.mode import Preview, TargetVersion, supports_feature
    70| from black.nodes import (
    71|     STARS,
    72|     is_number_token,
    73|     is_simple_decorator_expression,
    74|     is_string_token,
    75|     syms,
    76| )
    77| from black.output import color_diff, diff, dump_to_file, err, ipynb_diff, out
    78| from black.parsing import InvalidInput  # noqa F401
    79| from black.parsing import lib2to3_parse, parse_ast, stringify_ast
    80| from black.ranges import adjusted_lines, convert_unchanged_lines, parse_line_ranges
    81| from black.report import Changed, NothingChanged, Report
    82| from black.trans import iter_fexpr_spans
    83| from blib2to3.pgen2 import token
    84| from blib2to3.pytree import Leaf, Node
    85| COMPILED = Path(__file__).suffix in (".pyd", ".so")
    86| FileContent = str
    87| Encoding = str
    88| NewLine = str
    89| class WriteBack(Enum):
    90|     NO = 0
    91|     YES = 1
    92|     DIFF = 2
    93|     CHECK = 3
    94|     COLOR_DIFF = 4
    95|     @classmethod
    96|     def from_configuration(
    97|         cls, *, check: bool, diff: bool, color: bool = False
    98|     ) -> "WriteBack":
    99|         if check and not diff:
   100|             return cls.CHECK

# --- HUNK 2: Lines 1058-1097 ---
  1058|     >>> print(black.format_str("def f(arg:str='')->None:...", mode=black.Mode()))
  1059|     def f(arg: str = "") -> None:
  1060|         ...
  1061|     A more complex example:
  1062|     >>> print(
  1063|     ...   black.format_str(
  1064|     ...     "def f(arg:str='')->None: hey",
  1065|     ...     mode=black.Mode(
  1066|     ...       target_versions={black.TargetVersion.PY36},
  1067|     ...       line_length=10,
  1068|     ...       string_normalization=False,
  1069|     ...       is_pyi=False,
  1070|     ...     ),
  1071|     ...   ),
  1072|     ... )
  1073|     def f(
  1074|         arg: str = '',
  1075|     ) -> None:
  1076|         hey
  1077|     """
  1078|     dst_contents = _format_str_once(src_contents, mode=mode, lines=lines)
  1079|     if src_contents != dst_contents:
  1080|         if lines:
  1081|             lines = adjusted_lines(lines, src_contents, dst_contents)
  1082|         return _format_str_once(dst_contents, mode=mode, lines=lines)
  1083|     return dst_contents
  1084| def _format_str_once(
  1085|     src_contents: str, *, mode: Mode, lines: Collection[Tuple[int, int]] = ()
  1086| ) -> str:
  1087|     src_node = lib2to3_parse(src_contents.lstrip(), mode.target_versions)
  1088|     dst_blocks: List[LinesBlock] = []
  1089|     if mode.target_versions:
  1090|         versions = mode.target_versions
  1091|     else:
  1092|         future_imports = get_future_imports(src_node)
  1093|         versions = detect_target_versions(src_node, future_imports=future_imports)
  1094|     context_manager_features = {
  1095|         feature
  1096|         for feature in {Feature.PARENTHESIZED_CONTEXT_MANAGERS}
  1097|         if supports_feature(versions, feature)

# --- HUNK 3: Lines 1301-1360 ---
  1301|             if (
  1302|                 len(child.children) == 2
  1303|                 and first_child.type == token.STRING
  1304|                 and child.children[1].type == token.NEWLINE
  1305|             ):
  1306|                 continue
  1307|             break
  1308|         elif first_child.type == syms.import_from:
  1309|             module_name = first_child.children[1]
  1310|             if not isinstance(module_name, Leaf) or module_name.value != "__future__":
  1311|                 break
  1312|             imports |= set(get_imports_from_children(first_child.children[3:]))
  1313|         else:
  1314|             break
  1315|     return imports
  1316| def assert_equivalent(src: str, dst: str) -> None:
  1317|     """Raise AssertionError if `src` and `dst` aren't equivalent."""
  1318|     try:
  1319|         src_ast = parse_ast(src)
  1320|     except Exception as exc:
  1321|         raise AssertionError(
  1322|             "cannot use --safe with this file; failed to parse source file AST: "
  1323|             f"{exc}\n"
  1324|             "This could be caused by running Black with an older Python version "
  1325|             "that does not support new syntax used in your source file."
  1326|         ) from exc
  1327|     try:
  1328|         dst_ast = parse_ast(dst)
  1329|     except Exception as exc:
  1330|         log = dump_to_file("".join(traceback.format_tb(exc.__traceback__)), dst)
  1331|         raise AssertionError(
  1332|             f"INTERNAL ERROR: Black produced invalid code: {exc}. "
  1333|             "Please report a bug on https://github.com/psf/black/issues.  "
  1334|             f"This invalid output might be helpful: {log}"
  1335|         ) from None
  1336|     src_ast_str = "\n".join(stringify_ast(src_ast))
  1337|     dst_ast_str = "\n".join(stringify_ast(dst_ast))
  1338|     if src_ast_str != dst_ast_str:
  1339|         log = dump_to_file(diff(src_ast_str, dst_ast_str, "src", "dst"))
  1340|         raise AssertionError(
  1341|             "INTERNAL ERROR: Black produced code that is not equivalent to the"
  1342|             " source.  Please report a bug on "
  1343|             f"https://github.com/psf/black/issues.  This diff might be helpful: {log}"
  1344|         ) from None
  1345| def assert_stable(
  1346|     src: str, dst: str, mode: Mode, *, lines: Collection[Tuple[int, int]] = ()
  1347| ) -> None:
  1348|     """Raise AssertionError if `dst` reformats differently the second time."""
  1349|     if lines:
  1350|         return
  1351|     newdst = _format_str_once(dst, mode=mode, lines=lines)
  1352|     if dst != newdst:
  1353|         log = dump_to_file(
  1354|             str(mode),
  1355|             diff(src, dst, "source", "first pass"),
  1356|             diff(dst, newdst, "first pass", "second pass"),
  1357|         )
  1358|         raise AssertionError(
  1359|             "INTERNAL ERROR: Black produced different code on the second pass of the"
  1360|             " formatter.  Please report a bug on https://github.com/psf/black/issues."


# ====================================================================
# FILE: src/black/linegen.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-57 ---
     1| """
     2| Generating lines of code.
     3| """
     4| import re
     5| import sys
     6| from dataclasses import replace
     7| from enum import Enum, auto
     8| from functools import partial, wraps
     9| from typing import Collection, Iterator, List, Optional, Set, Union, cast
    10| from black.brackets import (
    11|     COMMA_PRIORITY,
    12|     DOT_PRIORITY,
    13|     get_leaves_inside_matching_brackets,
    14|     max_delimiter_priority_in_atom,
    15| )
    16| from black.comments import FMT_OFF, generate_comments, list_comments
    17| from black.lines import (
    18|     Line,
    19|     RHSResult,
    20|     append_leaves,
    21|     can_be_split,
    22|     can_omit_invisible_parens,
    23|     is_line_short_enough,
    24|     line_to_string,
    25| )
    26| from black.mode import Feature, Mode, Preview
    27| from black.nodes import (
    28|     ASSIGNMENTS,
    29|     BRACKETS,
    30|     CLOSING_BRACKETS,
    31|     OPENING_BRACKETS,
    32|     RARROW,
    33|     STANDALONE_COMMENT,
    34|     STATEMENT,
    35|     WHITESPACE,
    36|     Visitor,
    37|     ensure_visible,
    38|     is_arith_like,
    39|     is_async_stmt_or_funcdef,
    40|     is_atom_with_invisible_parens,
    41|     is_docstring,
    42|     is_empty_tuple,
    43|     is_lpar_token,
    44|     is_multiline_string,
    45|     is_name_token,
    46|     is_one_sequence_between,
    47|     is_one_tuple,
    48|     is_parent_function_or_class,
    49|     is_part_of_annotation,
    50|     is_rpar_token,
    51|     is_stub_body,
    52|     is_stub_suite,
    53|     is_tuple_containing_walrus,
    54|     is_type_ignore_comment_string,
    55|     is_vararg,
    56|     is_walrus_assignment,
    57|     is_yield,

# --- HUNK 2: Lines 810-857 ---
   810|     If it's the body component, the result line is one-indented inside brackets and as
   811|     such has its first leaf's prefix normalized and a trailing comma added when
   812|     expected.
   813|     """
   814|     result = Line(mode=original.mode, depth=original.depth)
   815|     if component is _BracketSplitComponent.body:
   816|         result.inside_brackets = True
   817|         result.depth += 1
   818|         if leaves:
   819|             no_commas = (
   820|                 original.is_def
   821|                 and opening_bracket.value == "("
   822|                 and not any(
   823|                     leaf.type == token.COMMA
   824|                     and (
   825|                         Preview.typed_params_trailing_comma not in original.mode
   826|                         or not is_part_of_annotation(leaf)
   827|                     )
   828|                     for leaf in leaves
   829|                 )
   830|                 and not any(
   831|                     node.prev_sibling.type == RARROW
   832|                     for node in (
   833|                         leaves[0].parent,
   834|                         getattr(leaves[0].parent, "parent", None),
   835|                     )
   836|                     if isinstance(node, Node) and isinstance(node.prev_sibling, Leaf)
   837|                 )
   838|                 and not (
   839|                     leaves[0].parent
   840|                     and leaves[0].parent.next_sibling
   841|                     and leaves[0].parent.next_sibling.type == token.VBAR
   842|                 )
   843|             )
   844|             if original.is_import or no_commas:
   845|                 for i in range(len(leaves) - 1, -1, -1):
   846|                     if leaves[i].type == STANDALONE_COMMENT:
   847|                         continue
   848|                     if leaves[i].type != token.COMMA:
   849|                         new_comma = Leaf(token.COMMA, ",")
   850|                         leaves.insert(i + 1, new_comma)
   851|                     break
   852|     leaves_to_track: Set[LeafID] = set()
   853|     if component is _BracketSplitComponent.head:
   854|         leaves_to_track = get_leaves_inside_matching_brackets(leaves)
   855|     for leaf in leaves:
   856|         result.append(
   857|             leaf,

# --- HUNK 3: Lines 865-971 ---
   865|     ):
   866|         result.should_split_rhs = True
   867|     return result
   868| def dont_increase_indentation(split_func: Transformer) -> Transformer:
   869|     """Normalize prefix of the first leaf in every line returned by `split_func`.
   870|     This is a decorator over relevant split functions.
   871|     """
   872|     @wraps(split_func)
   873|     def split_wrapper(
   874|         line: Line, features: Collection[Feature], mode: Mode
   875|     ) -> Iterator[Line]:
   876|         for split_line in split_func(line, features, mode):
   877|             split_line.leaves[0].prefix = ""
   878|             yield split_line
   879|     return split_wrapper
   880| def _get_last_non_comment_leaf(line: Line) -> Optional[int]:
   881|     for leaf_idx in range(len(line.leaves) - 1, 0, -1):
   882|         if line.leaves[leaf_idx].type != STANDALONE_COMMENT:
   883|             return leaf_idx
   884|     return None
   885| def _safe_add_trailing_comma(safe: bool, delimiter_priority: int, line: Line) -> Line:
   886|     if (
   887|         safe
   888|         and delimiter_priority == COMMA_PRIORITY
   889|         and line.leaves[-1].type != token.COMMA
   890|         and line.leaves[-1].type != STANDALONE_COMMENT
   891|     ):
   892|         new_comma = Leaf(token.COMMA, ",")
   893|         line.append(new_comma)
   894|     return line
   895| @dont_increase_indentation
   896| def delimiter_split(
   897|     line: Line, features: Collection[Feature], mode: Mode
   898| ) -> Iterator[Line]:
   899|     """Split according to delimiters of the highest priority.
   900|     If the appropriate Features are given, the split will add trailing commas
   901|     also in function signatures and calls that contain `*` and `**`.
   902|     """
   903|     try:
   904|         last_leaf = line.leaves[-1]
   905|     except IndexError:
   906|         raise CannotSplit("Line empty") from None
   907|     bt = line.bracket_tracker
   908|     try:
   909|         delimiter_priority = bt.max_delimiter_priority(exclude={id(last_leaf)})
   910|     except ValueError:
   911|         raise CannotSplit("No delimiters found") from None
   912|     if delimiter_priority == DOT_PRIORITY:
   913|         if bt.delimiter_count_with_priority(delimiter_priority) == 1:
   914|             raise CannotSplit("Splitting a single attribute from its owner looks wrong")
   915|     current_line = Line(
   916|         mode=line.mode, depth=line.depth, inside_brackets=line.inside_brackets
   917|     )
   918|     lowest_depth = sys.maxsize
   919|     trailing_comma_safe = True
   920|     def append_to_line(leaf: Leaf) -> Iterator[Line]:
   921|         """Append `leaf` to current line or to new line if appending impossible."""
   922|         nonlocal current_line
   923|         try:
   924|             current_line.append_safe(leaf, preformatted=True)
   925|         except ValueError:
   926|             yield current_line
   927|             current_line = Line(
   928|                 mode=line.mode, depth=line.depth, inside_brackets=line.inside_brackets
   929|             )
   930|             current_line.append(leaf)
   931|     last_non_comment_leaf = _get_last_non_comment_leaf(line)
   932|     for leaf_idx, leaf in enumerate(line.leaves):
   933|         yield from append_to_line(leaf)
   934|         for comment_after in line.comments_after(leaf):
   935|             yield from append_to_line(comment_after)
   936|         lowest_depth = min(lowest_depth, leaf.bracket_depth)
   937|         if leaf.bracket_depth == lowest_depth:
   938|             if is_vararg(leaf, within={syms.typedargslist}):
   939|                 trailing_comma_safe = (
   940|                     trailing_comma_safe and Feature.TRAILING_COMMA_IN_DEF in features
   941|                 )
   942|             elif is_vararg(leaf, within={syms.arglist, syms.argument}):
   943|                 trailing_comma_safe = (
   944|                     trailing_comma_safe and Feature.TRAILING_COMMA_IN_CALL in features
   945|                 )
   946|         if last_leaf.type == STANDALONE_COMMENT and leaf_idx == last_non_comment_leaf:
   947|             current_line = _safe_add_trailing_comma(
   948|                 trailing_comma_safe, delimiter_priority, current_line
   949|             )
   950|         leaf_priority = bt.delimiters.get(id(leaf))
   951|         if leaf_priority == delimiter_priority:
   952|             yield current_line
   953|             current_line = Line(
   954|                 mode=line.mode, depth=line.depth, inside_brackets=line.inside_brackets
   955|             )
   956|     if current_line:
   957|         current_line = _safe_add_trailing_comma(
   958|             trailing_comma_safe, delimiter_priority, current_line
   959|         )
   960|         yield current_line
   961| @dont_increase_indentation
   962| def standalone_comment_split(
   963|     line: Line, features: Collection[Feature], mode: Mode
   964| ) -> Iterator[Line]:
   965|     """Split standalone comments from the rest of the line."""
   966|     if not line.contains_standalone_comments():
   967|         raise CannotSplit("Line does not have any standalone comments")
   968|     current_line = Line(
   969|         mode=line.mode, depth=line.depth, inside_brackets=line.inside_brackets
   970|     )
   971|     def append_to_line(leaf: Leaf) -> Iterator[Line]:


# ====================================================================
# FILE: src/black/nodes.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| """
     2| blib2to3 Node/Leaf transformation-related utility functions.
     3| """
     4| import sys
     5| from typing import Final, Generic, Iterator, List, Optional, Set, Tuple, TypeVar, Union
     6| if sys.version_info >= (3, 10):
     7|     from typing import TypeGuard
     8| else:
     9|     from typing_extensions import TypeGuard
    10| from mypy_extensions import mypyc_attr
    11| from black.cache import CACHE_DIR
    12| from black.mode import Mode, Preview
    13| from black.strings import get_string_prefix, has_triple_quotes
    14| from blib2to3 import pygram
    15| from blib2to3.pgen2 import token
    16| from blib2to3.pytree import NL, Leaf, Node, type_repr
    17| pygram.initialize(CACHE_DIR)
    18| syms: Final = pygram.python_symbols
    19| T = TypeVar("T")
    20| LN = Union[Leaf, Node]
    21| LeafID = int
    22| NodeType = int
    23| WHITESPACE: Final = {token.DEDENT, token.INDENT, token.NEWLINE}
    24| STATEMENT: Final = {
    25|     syms.if_stmt,

# --- HUNK 2: Lines 699-748 ---
   699|     return wrapped
   700| def ensure_visible(leaf: Leaf) -> None:
   701|     """Make sure parentheses are visible.
   702|     They could be invisible as part of some statements (see
   703|     :func:`normalize_invisible_parens` and :func:`visit_import_from`).
   704|     """
   705|     if leaf.type == token.LPAR:
   706|         leaf.value = "("
   707|     elif leaf.type == token.RPAR:
   708|         leaf.value = ")"
   709| def is_name_token(nl: NL) -> TypeGuard[Leaf]:
   710|     return nl.type == token.NAME
   711| def is_lpar_token(nl: NL) -> TypeGuard[Leaf]:
   712|     return nl.type == token.LPAR
   713| def is_rpar_token(nl: NL) -> TypeGuard[Leaf]:
   714|     return nl.type == token.RPAR
   715| def is_string_token(nl: NL) -> TypeGuard[Leaf]:
   716|     return nl.type == token.STRING
   717| def is_number_token(nl: NL) -> TypeGuard[Leaf]:
   718|     return nl.type == token.NUMBER
   719| def is_part_of_annotation(leaf: Leaf) -> bool:
   720|     """Returns whether this leaf is part of type annotations."""
   721|     ancestor = leaf.parent
   722|     while ancestor is not None:
   723|         if ancestor.prev_sibling and ancestor.prev_sibling.type == token.RARROW:
   724|             return True
   725|         if ancestor.parent and ancestor.parent.type == syms.tname:
   726|             return True
   727|         ancestor = ancestor.parent
   728|     return False
   729| def first_leaf(node: LN) -> Optional[Leaf]:
   730|     """Returns the first leaf of the ancestor node."""
   731|     if isinstance(node, Leaf):
   732|         return node
   733|     elif not node.children:
   734|         return None
   735|     else:
   736|         return first_leaf(node.children[0])
   737| def last_leaf(node: LN) -> Optional[Leaf]:
   738|     """Returns the last leaf of the ancestor node."""
   739|     if isinstance(node, Leaf):
   740|         return node
   741|     elif not node.children:
   742|         return None
   743|     else:
   744|         return last_leaf(node.children[-1])
   745| def furthest_ancestor_with_last_leaf(leaf: Leaf) -> LN:
   746|     """Returns the furthest ancestor that has this leaf node as the last leaf."""
   747|     node: LN = leaf
   748|     while node.parent and node.parent.children and node is node.parent.children[-1]:


# ====================================================================
# FILE: src/black/parsing.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 60-152 ---
    60|             )
    61|     else:
    62|         assert len(errors) >= 1
    63|         exc = errors[max(errors)]
    64|         raise exc from None
    65|     if isinstance(result, Leaf):
    66|         result = Node(syms.file_input, [result])
    67|     return result
    68| def matches_grammar(src_txt: str, grammar: Grammar) -> bool:
    69|     drv = driver.Driver(grammar)
    70|     try:
    71|         drv.parse_string(src_txt, True)
    72|     except (ParseError, TokenError, IndentationError):
    73|         return False
    74|     else:
    75|         return True
    76| def lib2to3_unparse(node: Node) -> str:
    77|     """Given a lib2to3 node, return its string representation."""
    78|     code = str(node)
    79|     return code
    80| def _parse_single_version(
    81|     src: str, version: Tuple[int, int], *, type_comments: bool
    82| ) -> ast.AST:
    83|     filename = "<unknown>"
    84|     with warnings.catch_warnings():
    85|         warnings.simplefilter("ignore", SyntaxWarning)
    86|         warnings.simplefilter("ignore", DeprecationWarning)
    87|         return ast.parse(
    88|             src, filename, feature_version=version, type_comments=type_comments
    89|         )
    90| def parse_ast(src: str) -> ast.AST:
    91|     versions = [(3, minor) for minor in range(3, sys.version_info[1] + 1)]
    92|     first_error = ""
    93|     for version in sorted(versions, reverse=True):
    94|         try:
    95|             return _parse_single_version(src, version, type_comments=True)
    96|         except SyntaxError as e:
    97|             if not first_error:
    98|                 first_error = str(e)
    99|     for version in sorted(versions, reverse=True):
   100|         try:
   101|             return _parse_single_version(src, version, type_comments=False)
   102|         except SyntaxError:
   103|             pass
   104|     raise SyntaxError(first_error)
   105| def _normalize(lineend: str, value: str) -> str:
   106|     stripped: List[str] = [i.strip() for i in value.splitlines()]
   107|     normalized = lineend.join(stripped)
   108|     return normalized.strip()
   109| def stringify_ast(node: ast.AST, depth: int = 0) -> Iterator[str]:
   110|     """Simple visitor generating strings to compare ASTs by content."""
   111|     if (
   112|         isinstance(node, ast.Constant)
   113|         and isinstance(node.value, str)
   114|         and node.kind == "u"
   115|     ):
   116|         node.kind = None
   117|     yield f"{'  ' * depth}{node.__class__.__name__}("
   118|     for field in sorted(node._fields):  # noqa: F402
   119|         if isinstance(node, ast.TypeIgnore):
   120|             break
   121|         try:
   122|             value: object = getattr(node, field)
   123|         except AttributeError:
   124|             continue
   125|         yield f"{'  ' * (depth + 1)}{field}="
   126|         if isinstance(value, list):
   127|             for item in value:
   128|                 if (
   129|                     field == "targets"
   130|                     and isinstance(node, ast.Delete)
   131|                     and isinstance(item, ast.Tuple)
   132|                 ):
   133|                     for elt in item.elts:
   134|                         yield from stringify_ast(elt, depth + 2)
   135|                 elif isinstance(item, ast.AST):
   136|                     yield from stringify_ast(item, depth + 2)
   137|         elif isinstance(value, ast.AST):
   138|             yield from stringify_ast(value, depth + 2)
   139|         else:
   140|             normalized: object
   141|             if (
   142|                 isinstance(node, ast.Constant)
   143|                 and field == "value"
   144|                 and isinstance(value, str)
   145|             ):
   146|                 normalized = _normalize("\n", value)
   147|             elif field == "type_comment" and isinstance(value, str):
   148|                 normalized = value.rstrip()
   149|             else:
   150|                 normalized = value
   151|             yield f"{'  ' * (depth + 2)}{normalized!r},  # {value.__class__.__name__}"
   152|     yield f"{'  ' * depth})  # /{node.__class__.__name__}"


# ====================================================================
# FILE: src/black/ranges.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 20-59 ---
    20|         parts = lines_str.split("-")
    21|         if len(parts) != 2:
    22|             raise ValueError(
    23|                 "Incorrect --line-ranges format, expect 'START-END', found"
    24|                 f" {lines_str!r}"
    25|             )
    26|         try:
    27|             start = int(parts[0])
    28|             end = int(parts[1])
    29|         except ValueError:
    30|             raise ValueError(
    31|                 "Incorrect --line-ranges value, expect integer ranges, found"
    32|                 f" {lines_str!r}"
    33|             ) from None
    34|         else:
    35|             lines.append((start, end))
    36|     return lines
    37| def is_valid_line_range(lines: Tuple[int, int]) -> bool:
    38|     """Returns whether the line range is valid."""
    39|     return not lines or lines[0] <= lines[1]
    40| def adjusted_lines(
    41|     lines: Collection[Tuple[int, int]],
    42|     original_source: str,
    43|     modified_source: str,
    44| ) -> List[Tuple[int, int]]:
    45|     """Returns the adjusted line ranges based on edits from the original code.
    46|     This computes the new line ranges by diffing original_source and
    47|     modified_source, and adjust each range based on how the range overlaps with
    48|     the diffs.
    49|     Note the diff can contain lines outside of the original line ranges. This can
    50|     happen when the formatting has to be done in adjacent to maintain consistent
    51|     local results. For example:
    52|     1. def my_func(arg1, arg2,
    53|     2.             arg3,):
    54|     3.   pass
    55|     If it restricts to line 2-2, it can't simply reformat line 2, it also has
    56|     to reformat line 1:
    57|     1. def my_func(
    58|     2.     arg1,
    59|     3.     arg2,


# ====================================================================
# FILE: src/black/strings.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-72 ---
     1| """
     2| Simple formatting on strings. Further string formatting code is in trans.py.
     3| """
     4| import re
     5| import sys
     6| from functools import lru_cache
     7| from typing import Final, List, Match, Pattern
     8| from black._width_table import WIDTH_TABLE
     9| from blib2to3.pytree import Leaf
    10| STRING_PREFIX_CHARS: Final = "furbFURB"  # All possible string prefix characters.
    11| STRING_PREFIX_RE: Final = re.compile(
    12|     r"^([" + STRING_PREFIX_CHARS + r"]*)(.*)$", re.DOTALL
    13| )
    14| FIRST_NON_WHITESPACE_RE: Final = re.compile(r"\s*\t+\s*(\S)")
    15| UNICODE_ESCAPE_RE: Final = re.compile(
    16|     r"(?P<backslashes>\\+)(?P<body>"
    17|     r"(u(?P<u>[a-fA-F0-9]{4}))"  # Character with 16-bit hex value xxxx
    18|     r"|(U(?P<U>[a-fA-F0-9]{8}))"  # Character with 32-bit hex value xxxxxxxx
    19|     r"|(x(?P<x>[a-fA-F0-9]{2}))"  # Character with hex value hh
    20|     r"|(N\{(?P<N>[a-zA-Z0-9 \-]{2,})\})"  # Character named name in the Unicode database
    21|     r")",
    22|     re.VERBOSE,
    23| )
    24| def sub_twice(regex: Pattern[str], replacement: str, original: str) -> str:
    25|     """Replace `regex` with `replacement` twice on `original`.
    26|     This is used by string normalization to perform replaces on
    27|     overlapping matches.
    28|     """
    29|     return regex.sub(replacement, regex.sub(replacement, original))
    30| def has_triple_quotes(string: str) -> bool:
    31|     """
    32|     Returns:
    33|         True iff @string starts with three quotation characters.
    34|     """
    35|     raw_string = string.lstrip(STRING_PREFIX_CHARS)
    36|     return raw_string[:3] in {'"""', "'''"}
    37| def lines_with_leading_tabs_expanded(s: str) -> List[str]:
    38|     """
    39|     Splits string into lines and expands only leading tabs (following the normal
    40|     Python rules)
    41|     """
    42|     lines = []
    43|     for line in s.splitlines():
    44|         match = FIRST_NON_WHITESPACE_RE.match(line)
    45|         if match:
    46|             first_non_whitespace_idx = match.start(1)
    47|             lines.append(
    48|                 line[:first_non_whitespace_idx].expandtabs()
    49|                 + line[first_non_whitespace_idx:]
    50|             )
    51|         else:
    52|             lines.append(line)
    53|     if s.endswith("\n"):
    54|         lines.append("")
    55|     return lines
    56| def fix_docstring(docstring: str, prefix: str) -> str:
    57|     if not docstring:
    58|         return ""
    59|     lines = lines_with_leading_tabs_expanded(docstring)
    60|     indent = sys.maxsize
    61|     for line in lines[1:]:
    62|         stripped = line.lstrip()
    63|         if stripped:
    64|             indent = min(indent, len(line) - len(stripped))
    65|     trimmed = [lines[0].strip()]
    66|     if indent < sys.maxsize:
    67|         last_line_idx = len(lines) - 2
    68|         for i, line in enumerate(lines[1:]):
    69|             stripped_line = line[indent:].rstrip()
    70|             if stripped_line or i == last_line_idx:
    71|                 trimmed.append(prefix + stripped_line)
    72|             else:


# ====================================================================
# FILE: src/blib2to3/pytree.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 8-50 ---
     8|     Any,
     9|     Dict,
    10|     Iterable,
    11|     Iterator,
    12|     List,
    13|     Optional,
    14|     Set,
    15|     Tuple,
    16|     TypeVar,
    17|     Union,
    18| )
    19| from blib2to3.pgen2.grammar import Grammar
    20| __author__ = "Guido van Rossum <guido@python.org>"
    21| import sys
    22| from io import StringIO
    23| HUGE: int = 0x7FFFFFFF  # maximum repeat count, default max
    24| _type_reprs: Dict[int, Union[str, int]] = {}
    25| def type_repr(type_num: int) -> Union[str, int]:
    26|     global _type_reprs
    27|     if not _type_reprs:
    28|         from .pygram import python_symbols
    29|         for name in dir(python_symbols):
    30|             val = getattr(python_symbols, name)
    31|             if type(val) == int:
    32|                 _type_reprs[val] = name
    33|     return _type_reprs.setdefault(type_num, type_num)
    34| _P = TypeVar("_P", bound="Base")
    35| NL = Union["Node", "Leaf"]
    36| Context = Tuple[str, Tuple[int, int]]
    37| RawNode = Tuple[int, Optional[str], Optional[Context], Optional[List[NL]]]
    38| class Base:
    39|     """
    40|     Abstract base class for Node and Leaf.
    41|     This provides some default functionality and boilerplate using the
    42|     template pattern.
    43|     A node may be a subnode of at most one parent.
    44|     """
    45|     type: int  # int: token number (< 256) or symbol number (>= 256)
    46|     parent: Optional["Node"] = None  # Parent node pointer, or None
    47|     children: List[NL]  # List of subnodes
    48|     was_changed: bool = False
    49|     was_checked: bool = False
    50|     def __new__(cls, *args, **kwds):

