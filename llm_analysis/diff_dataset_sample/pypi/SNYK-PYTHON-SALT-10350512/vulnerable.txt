# ====================================================================
# FILE: doc/conf.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 105-144 ---
   105| spelling_show_suggestions = True
   106| language = "en"
   107| locale_dirs = [
   108|     "_locale",
   109| ]
   110| master_doc = "contents"
   111| templates_path = ["_templates"]
   112| exclude_patterns = ["_build", "_incl/*", "ref/cli/_includes/*.rst"]
   113| extensions = [
   114|     "saltdomain",  # Must come early
   115|     "sphinx.ext.autodoc",
   116|     "sphinx.ext.napoleon",
   117|     "sphinx.ext.autosummary",
   118|     "sphinx.ext.extlinks",
   119|     "sphinx.ext.imgconverter",
   120|     "sphinx.ext.intersphinx",
   121|     "sphinxcontrib.httpdomain",
   122|     "saltrepo",
   123|     "myst_parser",
   124|     "sphinxcontrib.spelling",
   125| ]
   126| modindex_common_prefix = ["salt."]
   127| autosummary_generate = True
   128| autosummary_generate_overwrite = False
   129| autodoc_mock_imports = []
   130| stripped_release = re.sub(r"-\d+-g[0-9a-f]+$", "", release)
   131| rst_prolog = """\
   132| .. |current_release_doc| replace:: :doc:`/topics/releases/{release}`
   133| .. |saltrepo| replace:: https://github.com/saltstack/salt
   134| .. _`salt-users`: https://groups.google.com/forum/#!forum/salt-users
   135| .. _`salt-announce`: https://groups.google.com/forum/#!forum/salt-announce
   136| .. _`salt-packagers`: https://groups.google.com/forum/#!forum/salt-packagers
   137| .. _`salt-slack`: https://via.vmw.com/salt-slack
   138| .. |windownload| raw:: html
   139|      <p>Python3 x86: <a
   140|      href="https://repo.saltproject.io/windows/Salt-Minion-{release}-Py3-x86-Setup.exe"><strong>Salt-Minion-{release}-x86-Setup.exe</strong></a>
   141|       | <a href="https://repo.saltproject.io/windows/Salt-Minion-{release}-Py3-x86-Setup.exe.md5"><strong>md5</strong></a></p>
   142|      <p>Python3 AMD64: <a
   143|      href="https://repo.saltproject.io/windows/Salt-Minion-{release}-Py3-AMD64-Setup.exe"><strong>Salt-Minion-{release}-AMD64-Setup.exe</strong></a>
   144|       | <a href="https://repo.saltproject.io/windows/Salt-Minion-{release}-Py3-AMD64-Setup.exe.md5"><strong>md5</strong></a></p>

# --- HUNK 2: Lines 232-272 ---
   232| """,
   233| }
   234| linkcheck_ignore = [
   235|     r"http://127.0.0.1",
   236|     r"http://salt:\d+",
   237|     r"http://local:\d+",
   238|     r"https://console.aws.amazon.com",
   239|     r"http://192.168.33.10",
   240|     r"http://domain:\d+",
   241|     r"http://123.456.789.012:\d+",
   242|     r"http://localhost",
   243|     r"https://groups.google.com/forum/#!forum/salt-users",
   244|     r"https://www.elastic.co/logstash/docs/latest/inputs/udp",
   245|     r"https://www.elastic.co/logstash/docs/latest/inputs/zeromq",
   246|     r"http://www.youtube.com/saltstack",
   247|     r"https://raven.readthedocs.io",
   248|     r"https://getsentry.com",
   249|     r"https://salt-cloud.readthedocs.io",
   250|     r"https://salt.readthedocs.io",
   251|     r"http://www.pip-installer.org/",
   252|     r"http://www.windowsazure.com/",
   253|     r"https://github.com/watching",
   254|     r"dash-feed://",
   255|     r"https://github.com/saltstack/salt/",
   256|     r"https://bootstrap.saltproject.io",
   257|     r"https://raw.githubusercontent.com/saltstack/salt-bootstrap/stable/bootstrap-salt.sh",
   258|     r"media.readthedocs.org/dash/salt/latest/salt.xml",
   259|     r"https://portal.aws.amazon.com/gp/aws/securityCredentials",
   260|     r"dash-feed://https%3A//media.readthedocs.org/dash/salt/latest/salt.xml",
   261|     r"(?i)dns:.*",
   262|     r"TCP:4506",
   263|     r"https?://",
   264|     r"https://cloud.github.com/downloads/saltstack/.*",
   265|     r"https://INFOBLOX/.*",
   266|     r"https://SOMESERVERIP:.*",
   267|     r"https://community.saltstack.com/.*",
   268|     r"https://github.com/[^/]$",
   269|     r"https://github.com/[^/]/salt$",
   270|     r"tag:key=value",
   271|     r"jdbc:mysql:.*",
   272|     r"http:post",


# ====================================================================
# FILE: noxfile.py
# Total hunks: 8
# ====================================================================
# --- HUNK 1: Lines 52-92 ---
    52|     PRINT_SYSTEM_INFO = PRINT_SYSTEM_INFO == "1"
    53| PRINT_SYSTEM_INFO_ONLY = os.environ.get("PRINT_SYSTEM_INFO_ONLY", "0") == "1"
    54| SKIP_REQUIREMENTS_INSTALL = os.environ.get("SKIP_REQUIREMENTS_INSTALL", "0") == "1"
    55| EXTRA_REQUIREMENTS_INSTALL = os.environ.get("EXTRA_REQUIREMENTS_INSTALL")
    56| COVERAGE_REQUIREMENT = os.environ.get("COVERAGE_REQUIREMENT")
    57| REPO_ROOT = pathlib.Path(os.path.dirname(__file__)).resolve()
    58| ARTIFACTS_DIR = REPO_ROOT / "artifacts"
    59| COVERAGE_OUTPUT_DIR = ARTIFACTS_DIR / "coverage"
    60| COVERAGE_FILE = os.environ.get("COVERAGE_FILE")
    61| if COVERAGE_FILE is None:
    62|     COVERAGE_FILE = str(COVERAGE_OUTPUT_DIR / ".coverage")
    63| IS_DARWIN = sys.platform.lower().startswith("darwin")
    64| IS_WINDOWS = sys.platform.lower().startswith("win")
    65| IS_FREEBSD = sys.platform.lower().startswith("freebsd")
    66| IS_LINUX = sys.platform.lower().startswith("linux")
    67| ONEDIR_ARTIFACT_PATH = ARTIFACTS_DIR / "salt"
    68| if IS_WINDOWS:
    69|     ONEDIR_PYTHON_PATH = ONEDIR_ARTIFACT_PATH / "Scripts" / "python.exe"
    70| else:
    71|     ONEDIR_PYTHON_PATH = ONEDIR_ARTIFACT_PATH / "bin" / "python3"
    72| _PYTHON_VERSIONS = ("3", "3.5", "3.6", "3.7", "3.8", "3.9", "3.10")
    73| nox.options.reuse_existing_virtualenvs = True
    74| os.chdir(str(REPO_ROOT))
    75| RUNTESTS_LOGFILE = ARTIFACTS_DIR.joinpath(
    76|     "logs",
    77|     "runtests-{}.log".format(datetime.datetime.now().strftime("%Y%m%d%H%M%S.%f")),
    78| )
    79| os.environ["PYTHONDONTWRITEBYTECODE"] = "1"
    80| def session_warn(session, message):
    81|     try:
    82|         session.warn(message)
    83|     except AttributeError:
    84|         session.log(f"WARNING: {message}")
    85| def session_run_always(session, *command, **kwargs):
    86|     """
    87|     Patch nox to allow running some commands which would be skipped if --install-only is passed.
    88|     """
    89|     try:
    90|         return session.run_always(*command, **kwargs)
    91|     except AttributeError:
    92|         old_install_only_value = session._runner.global_config.install_only

# --- HUNK 2: Lines 127-170 ---
   127|         version_info = session._runner._real_python_version_info
   128|     except AttributeError:
   129|         session_py_version = session_run_always(
   130|             session,
   131|             "python",
   132|             "-c",
   133|             'import sys; sys.stdout.write("{}.{}.{}".format(*sys.version_info))',
   134|             stderr=None,
   135|             silent=True,
   136|             log=False,
   137|         )
   138|         version_info = tuple(
   139|             int(part)
   140|             for part in session_py_version.strip().split(".")
   141|             if part.isdigit()
   142|         )
   143|         session._runner._real_python_version_info = version_info
   144|     return version_info
   145| def _get_pydir(session):
   146|     version_info = _get_session_python_version_info(session)
   147|     if version_info < (3, 5):
   148|         session.error("Only Python >= 3.5 is supported")
   149|     if IS_WINDOWS and version_info < (3, 6):
   150|         session.error("Only Python >= 3.6 is supported on Windows")
   151|     return "py{}.{}".format(*version_info)
   152| def _get_pip_requirements_file(session, crypto=None, requirements_type="ci"):
   153|     assert requirements_type in ("ci", "pkg")
   154|     pydir = _get_pydir(session)
   155|     if IS_WINDOWS:
   156|         if crypto is None:
   157|             _requirements_file = os.path.join(
   158|                 "requirements", "static", requirements_type, pydir, "windows.txt"
   159|             )
   160|             if os.path.exists(_requirements_file):
   161|                 return _requirements_file
   162|         _requirements_file = os.path.join(
   163|             "requirements", "static", requirements_type, pydir, "windows-crypto.txt"
   164|         )
   165|         if os.path.exists(_requirements_file):
   166|             return _requirements_file
   167|         session.error(f"Could not find a windows requirements file for {pydir}")
   168|     elif IS_DARWIN:
   169|         if crypto is None:
   170|             _requirements_file = os.path.join(

# --- HUNK 3: Lines 761-804 ---
   761|         session_name = session._runner.friendly_name
   762|     session_warn(
   763|         session,
   764|         "This nox session is deprecated, please call {!r} instead".format(
   765|             session_name.replace("pytest-", "test-")
   766|         ),
   767|     )
   768|     session.notify(session_name.replace("pytest-", "test-"))
   769| @nox.session(python=_PYTHON_VERSIONS, name="test-cloud")
   770| @nox.parametrize("coverage", [False, True])
   771| def test_cloud(session, coverage):
   772|     """
   773|     pytest cloud tests session
   774|     """
   775|     pydir = _get_pydir(session)
   776|     if pydir == "py3.5":
   777|         session.error(
   778|             "Due to conflicting and unsupported requirements the cloud tests only run on Py3.6+"
   779|         )
   780|     if _upgrade_pip_setuptools_and_wheel(session):
   781|         requirements_file = os.path.join(
   782|             "requirements", "static", "ci", pydir, "cloud.txt"
   783|         )
   784|         install_command = ["--progress-bar=off", "-r", requirements_file]
   785|         session.install(*install_command, silent=PIP_INSTALL_SILENT)
   786|     cmd_args = [
   787|         "--run-expensive",
   788|         "-k",
   789|         "cloud",
   790|     ] + session.posargs
   791|     _pytest(session, coverage=coverage, cmd_args=cmd_args)
   792| @nox.session(python=_PYTHON_VERSIONS, name="pytest-cloud")
   793| @nox.parametrize("coverage", [False, True])
   794| def pytest_cloud(session, coverage):
   795|     """
   796|     pytest cloud tests session
   797|     """
   798|     try:
   799|         session_name = session.name
   800|     except AttributeError:
   801|         session_name = session._runner.friendly_name
   802|     session_warn(
   803|         session,
   804|         "This nox session is deprecated, please call {!r} instead".format(

# --- HUNK 4: Lines 1088-1140 ---
  1088|                     resolved_link_suffix = resolved_link.split(
  1089|                         f"artifacts{os.sep}salt{os.sep}"
  1090|                     )[-1]
  1091|                     fixed_link = REPO_ROOT.joinpath(
  1092|                         "artifacts", "salt", resolved_link_suffix
  1093|                     )
  1094|                     session.log(
  1095|                         "Fixing broken symlink in nox virtualenv %r, from %r to %r",
  1096|                         dirname.name,
  1097|                         resolved_link,
  1098|                         str(fixed_link.relative_to(REPO_ROOT)),
  1099|                     )
  1100|                     broken_link.unlink()
  1101|                     broken_link.symlink_to(fixed_link)
  1102|                 continue
  1103|             if not path.is_file():
  1104|                 continue
  1105|             if platform != "windows":
  1106|                 try:
  1107|                     fpath = pathlib.Path(path)
  1108|                     contents = fpath.read_text(encoding="utf-8").splitlines()
  1109|                     if (
  1110|                         contents[0].startswith("#!")
  1111|                         and contents[0].endswith("python")
  1112|                         and contents[0] != fixed_shebang
  1113|                     ):
  1114|                         session.log(
  1115|                             "Fixing broken shebang in %r",
  1116|                             str(fpath.relative_to(REPO_ROOT)),
  1117|                         )
  1118|                         fpath.write_text(
  1119|                             "\n".join([fixed_shebang] + contents[1:]), encoding="utf-8"
  1120|                         )
  1121|                 except UnicodeDecodeError:
  1122|                     pass
  1123| @nox.session(python=False, name="compress-dependencies")
  1124| def compress_dependencies(session):
  1125|     if not session.posargs:
  1126|         session.error(
  1127|             "The 'compress-dependencies' session target needs "
  1128|             "two arguments, '<platform> <arch>'."
  1129|         )
  1130|     try:
  1131|         platform = session.posargs.pop(0)
  1132|         arch = session.posargs.pop(0)
  1133|         if session.posargs:
  1134|             session.error(
  1135|                 "The 'compress-dependencies' session target only accepts "
  1136|                 "two arguments, '<platform> <arch>'."
  1137|             )
  1138|     except IndexError:
  1139|         session.error(
  1140|             "The 'compress-dependencies' session target needs "

# --- HUNK 5: Lines 1227-1415 ---
  1227|     python=str(ONEDIR_PYTHON_PATH),
  1228|     name="create-json-coverage-reports-onedir",
  1229|     venv_params=["--system-site-packages"],
  1230| )
  1231| def create_json_coverage_reports_onedir(session):
  1232|     _report_coverage(session, combine=True, cli_report=False, json_report=True)
  1233| class Tee:
  1234|     """
  1235|     Python class to mimic linux tee behaviour
  1236|     """
  1237|     def __init__(self, first, second):
  1238|         self._first = first
  1239|         self._second = second
  1240|     def write(self, b):
  1241|         wrote = self._first.write(b)
  1242|         self._first.flush()
  1243|         self._second.write(b)
  1244|         self._second.flush()
  1245|     def fileno(self):
  1246|         return self._first.fileno()
  1247| def _lint(session, rcfile, flags, paths, upgrade_setuptools_and_pip=True):
  1248|     if _upgrade_pip_setuptools_and_wheel(session, upgrade=upgrade_setuptools_and_pip):
  1249|         base_requirements_file = os.path.join(
  1250|             "requirements", "static", "ci", _get_pydir(session), "linux.txt"
  1251|         )
  1252|         lint_requirements_file = os.path.join(
  1253|             "requirements", "static", "ci", _get_pydir(session), "lint.txt"
  1254|         )
  1255|         install_command = [
  1256|             "--progress-bar=off",
  1257|             "-r",
  1258|             base_requirements_file,
  1259|             "-r",
  1260|             lint_requirements_file,
  1261|         ]
  1262|         session.install(*install_command, silent=PIP_INSTALL_SILENT)
  1263|     cmd_args = ["pylint", f"--rcfile={rcfile}"] + list(flags) + list(paths)
  1264|     cmd_kwargs = {"env": {"PYTHONUNBUFFERED": "1"}}
  1265|     session.run(*cmd_args, **cmd_kwargs)
  1266| def _lint_pre_commit(session, rcfile, flags, paths):
  1267|     if "VIRTUAL_ENV" not in os.environ:
  1268|         session.error(
  1269|             "This should be running from within a virtualenv and "
  1270|             "'VIRTUAL_ENV' was not found as an environment variable."
  1271|         )
  1272|     if "pre-commit" not in os.environ["VIRTUAL_ENV"]:
  1273|         session.error(
  1274|             "This should be running from within a pre-commit virtualenv and "
  1275|             "'VIRTUAL_ENV'({}) does not appear to be a pre-commit virtualenv.".format(
  1276|                 os.environ["VIRTUAL_ENV"]
  1277|             )
  1278|         )
  1279|     from nox.virtualenv import VirtualEnv
  1280|     session._runner.venv = VirtualEnv(
  1281|         os.environ["VIRTUAL_ENV"],
  1282|         interpreter=session._runner.func.python,
  1283|         reuse_existing=True,
  1284|         venv=True,
  1285|     )
  1286|     _lint(
  1287|         session,
  1288|         rcfile,
  1289|         flags,
  1290|         paths,
  1291|         upgrade_setuptools_and_pip=False,
  1292|     )
  1293| @nox.session(python="3")
  1294| def lint(session):
  1295|     """
  1296|     Run PyLint against Salt and it's test suite.
  1297|     """
  1298|     session.notify(f"lint-salt-{session.python}")
  1299|     session.notify(f"lint-tests-{session.python}")
  1300| @nox.session(python="3", name="lint-salt")
  1301| def lint_salt(session):
  1302|     """
  1303|     Run PyLint against Salt.
  1304|     """
  1305|     flags = ["--disable=I"]
  1306|     if session.posargs:
  1307|         paths = session.posargs
  1308|     else:
  1309|         paths = ["setup.py", "noxfile.py", "salt/", "tools/"]
  1310|     _lint(session, ".pylintrc", flags, paths)
  1311| @nox.session(python="3", name="lint-tests")
  1312| def lint_tests(session):
  1313|     """
  1314|     Run PyLint against Salt and it's test suite.
  1315|     """
  1316|     flags = ["--disable=I"]
  1317|     if session.posargs:
  1318|         paths = session.posargs
  1319|     else:
  1320|         paths = ["tests/"]
  1321|     _lint(session, ".pylintrc", flags, paths)
  1322| @nox.session(python=False, name="lint-salt-pre-commit")
  1323| def lint_salt_pre_commit(session):
  1324|     """
  1325|     Run PyLint against Salt.
  1326|     """
  1327|     flags = ["--disable=I"]
  1328|     if session.posargs:
  1329|         paths = session.posargs
  1330|     else:
  1331|         paths = ["setup.py", "noxfile.py", "salt/", "tools/"]
  1332|     _lint_pre_commit(session, ".pylintrc", flags, paths)
  1333| @nox.session(python=False, name="lint-tests-pre-commit")
  1334| def lint_tests_pre_commit(session):
  1335|     """
  1336|     Run PyLint against Salt and it's test suite.
  1337|     """
  1338|     flags = ["--disable=I"]
  1339|     if session.posargs:
  1340|         paths = session.posargs
  1341|     else:
  1342|         paths = ["tests/"]
  1343|     _lint_pre_commit(session, ".pylintrc", flags, paths)
  1344| @nox.session(python="3")
  1345| @nox.parametrize("clean", [False, True])
  1346| @nox.parametrize("update", [False, True])
  1347| @nox.parametrize("compress", [False, True])
  1348| def docs(session, compress, update, clean):
  1349|     """
  1350|     Build Salt's Documentation
  1351|     """
  1352|     session.notify(f"docs-html-{session.python}(compress={compress})")
  1353|     session.notify(
  1354|         find_session_runner(
  1355|             session,
  1356|             "docs-man",
  1357|             session.python,
  1358|             compress=compress,
  1359|             update=update,
  1360|             clean=clean,
  1361|         )
  1362|     )
  1363| @nox.session(name="docs-html", python="3")
  1364| @nox.parametrize("clean", [False, True])
  1365| @nox.parametrize("compress", [False, True])
  1366| def docs_html(session, compress, clean):
  1367|     """
  1368|     Build Salt's HTML Documentation
  1369|     """
  1370|     if _upgrade_pip_setuptools_and_wheel(session):
  1371|         requirements_file = os.path.join(
  1372|             "requirements", "static", "ci", _get_pydir(session), "docs.txt"
  1373|         )
  1374|         install_command = ["--progress-bar=off", "-r", requirements_file]
  1375|         session.install(*install_command, silent=PIP_INSTALL_SILENT)
  1376|     os.chdir("doc/")
  1377|     if clean:
  1378|         session.run("make", "clean", external=True)
  1379|     session.run("make", "html", "SPHINXOPTS=-W", external=True)
  1380|     if compress:
  1381|         session.run("tar", "-cJvf", "html-archive.tar.xz", "_build/html", external=True)
  1382|     os.chdir("..")
  1383| @nox.session(name="docs-man", python="3")
  1384| @nox.parametrize("clean", [False, True])
  1385| @nox.parametrize("update", [False, True])
  1386| @nox.parametrize("compress", [False, True])
  1387| def docs_man(session, compress, update, clean):
  1388|     """
  1389|     Build Salt's Manpages Documentation
  1390|     """
  1391|     if _upgrade_pip_setuptools_and_wheel(session):
  1392|         requirements_file = os.path.join(
  1393|             "requirements", "static", "ci", _get_pydir(session), "docs.txt"
  1394|         )
  1395|         install_command = ["--progress-bar=off", "-r", requirements_file]
  1396|         session.install(*install_command, silent=PIP_INSTALL_SILENT)
  1397|     os.chdir("doc/")
  1398|     if clean:
  1399|         session.run("make", "clean", external=True)
  1400|     session.run("make", "man", "SPHINXOPTS=-W", external=True)
  1401|     if update:
  1402|         session.run("rm", "-rf", "man/", external=True)
  1403|         session.run("cp", "-Rp", "_build/man", "man/", external=True)
  1404|     if compress:
  1405|         session.run("tar", "-cJvf", "man-archive.tar.xz", "_build/man", external=True)
  1406|     os.chdir("..")
  1407| @nox.session(name="changelog", python="3")
  1408| @nox.parametrize("draft", [False, True])
  1409| @nox.parametrize("force", [False, True])
  1410| def changelog(session, draft, force):
  1411|     """
  1412|     Generate salt's changelog
  1413|     """
  1414|     session_warn(
  1415|         session,

# --- HUNK 6: Lines 1440-1480 ---
  1440|         tarinfo.uname = tarinfo.gname = "root"
  1441|         tarinfo.mtime = self.mtime
  1442|         if tarinfo.type == tarfile.DIRTYPE:
  1443|             tarinfo.mode = 0o755
  1444|         else:
  1445|             tarinfo.mode = 0o644
  1446|         if tarinfo.pax_headers:
  1447|             raise ValueError(tarinfo.name, tarinfo.pax_headers)
  1448|         return tarinfo
  1449|     def recompress(self, targz):
  1450|         """
  1451|         Re-compress the passed path.
  1452|         """
  1453|         tempd = pathlib.Path(tempfile.mkdtemp()).resolve()
  1454|         d_src = tempd.joinpath("src")
  1455|         d_src.mkdir()
  1456|         d_tar = tempd.joinpath(targz.stem)
  1457|         d_targz = tempd.joinpath(targz.name)
  1458|         with tarfile.open(d_tar, "w|") as wfile:
  1459|             with tarfile.open(targz, "r:gz") as rfile:
  1460|                 rfile.extractall(d_src)  # nosec
  1461|                 extracted_dir = next(pathlib.Path(d_src).iterdir())
  1462|                 for name in sorted(extracted_dir.rglob("*")):
  1463|                     wfile.add(
  1464|                         str(name),
  1465|                         filter=self.tar_reset,
  1466|                         recursive=False,
  1467|                         arcname=str(name.relative_to(d_src)),
  1468|                     )
  1469|         with open(d_tar, "rb") as rfh:
  1470|             with gzip.GzipFile(
  1471|                 fileobj=open(d_targz, "wb"), mode="wb", filename="", mtime=self.mtime
  1472|             ) as gz:  # pylint: disable=invalid-name
  1473|                 while True:
  1474|                     chunk = rfh.read(1024)
  1475|                     if not chunk:
  1476|                         break
  1477|                     gz.write(chunk)
  1478|         targz.unlink()
  1479|         shutil.move(str(d_targz), str(targz))
  1480| @nox.session(python="3")

# --- HUNK 7: Lines 1539-1682 ---
  1539|         reuse_existing=True,
  1540|         venv=session._runner.venv.venv_or_virtualenv == "venv",
  1541|         venv_params=session._runner.venv.venv_params,
  1542|     )
  1543|     os.environ["VIRTUAL_ENV"] = session._runner.venv.location
  1544|     session._runner.venv.create()
  1545|     if not ONEDIR_ARTIFACT_PATH.exists():
  1546|         session.error(
  1547|             "The salt onedir artifact, expected to be in '{}', was not found".format(
  1548|                 ONEDIR_ARTIFACT_PATH.relative_to(REPO_ROOT)
  1549|             )
  1550|         )
  1551|     common_pytest_args = [
  1552|         "--color=yes",
  1553|         "--sys-stats",
  1554|         "--run-destructive",
  1555|         f"--output-columns={os.environ.get('OUTPUT_COLUMNS') or 120}",
  1556|         "--pkg-system-service",
  1557|     ]
  1558|     chunks = {
  1559|         "install": [],
  1560|         "upgrade": [
  1561|             "--upgrade",
  1562|             "--no-uninstall",
  1563|         ],
  1564|         "downgrade": [
  1565|             "--downgrade",
  1566|             "--no-uninstall",
  1567|         ],
  1568|         "download-pkgs": [
  1569|             "--download-pkgs",
  1570|         ],
  1571|     }
  1572|     if not session.posargs or session.posargs[0] not in chunks:
  1573|         chunk = "install"
  1574|         session.log("Choosing default 'install' test type")
  1575|     else:
  1576|         chunk = session.posargs.pop(0)
  1577|     cmd_args = chunks[chunk]
  1578|     for arg in session.posargs:
  1579|         if arg.startswith("tests/pytests/pkg/"):
  1580|             cmd_args.pop()
  1581|             break
  1582|     if IS_LINUX:
  1583|         session_run_always(session, "python3", "-m", "relenv", "toolchain", "fetch")
  1584|     if _upgrade_pip_setuptools_and_wheel(session):
  1585|         _install_requirements(session, "pyzmq")
  1586|     env = {
  1587|         "ONEDIR_TESTRUN": "1",
  1588|         "PKG_TEST_TYPE": chunk,
  1589|     }
  1590|     pytest_args = (
  1591|         common_pytest_args[:]
  1592|         + cmd_args[:]
  1593|         + [
  1594|             f"--junitxml=artifacts/xml-unittests-output/test-results-{chunk}.xml",
  1595|             f"--log-file=artifacts/logs/runtests-{chunk}.log",
  1596|         ]
  1597|         + session.posargs
  1598|     )
  1599|     append_tests_path = True
  1600|     test_paths = (
  1601|         "tests/pytests/pkg/",
  1602|         str(REPO_ROOT / "tests" / "pytests" / "pkg"),
  1603|     )
  1604|     for arg in session.posargs:
  1605|         if arg.startswith(test_paths):
  1606|             append_tests_path = False
  1607|             break
  1608|     if append_tests_path:
  1609|         pytest_args.append("tests/pytests/pkg/")
  1610|     try:
  1611|         _pytest(session, coverage=False, cmd_args=pytest_args, env=env)
  1612|     except CommandFailed:
  1613|         if os.environ.get("RERUN_FAILURES", "0") == "0":
  1614|             return
  1615|         global PRINT_TEST_SELECTION
  1616|         global PRINT_SYSTEM_INFO
  1617|         PRINT_TEST_SELECTION = False
  1618|         PRINT_SYSTEM_INFO = False
  1619|         pytest_args = (
  1620|             common_pytest_args[:]
  1621|             + cmd_args[:]
  1622|             + [
  1623|                 f"--junitxml=artifacts/xml-unittests-output/test-results-{chunk}-rerun.xml",
  1624|                 f"--log-file=artifacts/logs/runtests-{chunk}-rerun.log",
  1625|                 "--lf",
  1626|             ]
  1627|             + session.posargs
  1628|         )
  1629|         if append_tests_path:
  1630|             pytest_args.append("tests/pytests/pkg/")
  1631|         _pytest(
  1632|             session,
  1633|             coverage=False,
  1634|             cmd_args=pytest_args,
  1635|             env=env,
  1636|             on_rerun=True,
  1637|         )
  1638|     if chunk not in ("install", "download-pkgs"):
  1639|         cmd_args = chunks["install"]
  1640|         pytest_args = (
  1641|             common_pytest_args[:]
  1642|             + cmd_args[:]
  1643|             + [
  1644|                 "--no-install",
  1645|                 "--junitxml=artifacts/xml-unittests-output/test-results-install.xml",
  1646|                 "--log-file=artifacts/logs/runtests-install.log",
  1647|             ]
  1648|             + session.posargs
  1649|         )
  1650|         if "downgrade" in chunk:
  1651|             pytest_args.append("--use-prev-version")
  1652|         if append_tests_path:
  1653|             pytest_args.append("tests/pytests/pkg/")
  1654|         try:
  1655|             _pytest(session, coverage=False, cmd_args=pytest_args, env=env)
  1656|         except CommandFailed:
  1657|             if os.environ.get("RERUN_FAILURES", "0") == "0":
  1658|                 return
  1659|             cmd_args = chunks["install"]
  1660|             pytest_args = (
  1661|                 common_pytest_args[:]
  1662|                 + cmd_args[:]
  1663|                 + [
  1664|                     "--no-install",
  1665|                     "--junitxml=artifacts/xml-unittests-output/test-results-install-rerun.xml",
  1666|                     "--log-file=artifacts/logs/runtests-install-rerun.log",
  1667|                     "--lf",
  1668|                 ]
  1669|                 + session.posargs
  1670|             )
  1671|             if "downgrade" in chunk:
  1672|                 pytest_args.append("--use-prev-version")
  1673|             if append_tests_path:
  1674|                 pytest_args.append("tests/pytests/pkg/")
  1675|             _pytest(
  1676|                 session,
  1677|                 coverage=False,
  1678|                 cmd_args=pytest_args,
  1679|                 env=env,
  1680|                 on_rerun=True,
  1681|             )
  1682|     sys.exit(0)


# ====================================================================
# FILE: run.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-96 ---
     1| import contextlib
     2| import multiprocessing
     3| import os
     4| import pathlib
     5| import sys
     6| import _pyio
     7| import tiamatpip.cli
     8| import tiamatpip.configure
     9| import tiamatpip.utils
    10| import salt.scripts
    11| import salt.utils.platform
    12| AVAIL = (
    13|     "minion",
    14|     "master",
    15|     "call",
    16|     "api",
    17|     "cloud",
    18|     "cp",
    19|     "extend",
    20|     "key",
    21|     "proxy",
    22|     "pip",
    23|     "run",
    24|     "shell",
    25|     "spm",
    26|     "ssh",
    27|     "support",
    28|     "syndic",
    29|     "python",
    30| )
    31| if "TIAMAT_PIP_PYPATH" in os.environ:
    32|     PIP_PATH = pathlib.Path(os.environ["TIAMAT_PIP_PYPATH"]).resolve()
    33| elif not sys.platform.startswith("win"):
    34|     PIP_PATH = pathlib.Path(f"{os.sep}opt", "saltstack", "salt", "pypath")
    35| else:
    36|     PIP_PATH = pathlib.Path(os.getenv("LocalAppData"), "salt", "pypath")
    37| with contextlib.suppress(PermissionError):
    38|     PIP_PATH.mkdir(mode=0o755, parents=True, exist_ok=True)
    39| tiamatpip.configure.set_user_base_path(PIP_PATH)
    40| def py_shell():
    41|     if not sys.platform.startswith("win"):
    42|         import readline
    43|     import code
    44|     variables = globals().copy()
    45|     variables.update(locals())
    46|     shell = code.InteractiveConsole(variables)
    47|     shell.interact()
    48| def python_runtime():
    49|     import traceback
    50|     script = pathlib.Path(sys.argv[2]).expanduser().resolve()
    51|     sys.path.insert(0, str(script.parent))
    52|     sys.argv[:] = sys.argv[2:]
    53|     exec_locals = {"__name__": "__main__", "__file__": str(script), "__doc__": None}
    54|     with open(script, encoding="utf-8") as rfh:
    55|         try:
    56|             exec(rfh.read(), exec_locals)
    57|         except Exception:
    58|             traceback.print_exc()
    59|             sys.exit(1)
    60| def redirect(argv):
    61|     """
    62|     Change the args and redirect to another salt script
    63|     """
    64|     if len(argv) < 2:
    65|         msg = "Must pass in a salt command, available commands are:"
    66|         for cmd in AVAIL:
    67|             msg += f"\n{cmd}"
    68|         print(msg, file=sys.stderr, flush=True)
    69|         sys.exit(1)
    70|     cmd = sys.argv[1]
    71|     if cmd == "shell":
    72|         py_shell()
    73|         return
    74|     if cmd == "python":
    75|         if len(argv) < 3:
    76|             msg = "Must pass script location to this command"
    77|             print(msg, file=sys.stderr, flush=True)
    78|             sys.exit(1)
    79|         python_runtime()
    80|         return
    81|     if tiamatpip.cli.should_redirect_argv(argv):
    82|         tiamatpip.cli.process_pip_argv(argv)
    83|         return
    84|     if cmd not in AVAIL:
    85|         args = ["salt"]
    86|         s_fun = salt.scripts.salt_main
    87|     else:
    88|         args = [f"salt-{cmd}"]
    89|         sys.argv.pop(1)
    90|         s_fun = getattr(salt.scripts, f"salt_{cmd}")
    91|     args.extend(argv[1:])
    92|     with tiamatpip.utils.patched_sys_argv(args):
    93|         s_fun()
    94| if __name__ == "__main__":
    95|     multiprocessing.freeze_support()
    96|     redirect(sys.argv)


# ====================================================================
# FILE: salt/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-80 ---
     1| """
     2| Salt package
     3| """
     4| import importlib
     5| import os
     6| import sys
     7| import warnings
     8| USE_VENDORED_TORNADO = True
     9| class TornadoImporter:
    10|     def find_module(self, module_name, package_path=None):
    11|         if USE_VENDORED_TORNADO:
    12|             if module_name.startswith("tornado"):
    13|                 return self
    14|         else:  # pragma: no cover
    15|             if module_name.startswith("salt.ext.tornado"):
    16|                 return self
    17|         return None
    18|     def create_module(self, spec):
    19|         if USE_VENDORED_TORNADO:
    20|             mod = importlib.import_module(f"salt.ext.{spec.name}")
    21|         else:  # pragma: no cover
    22|             mod = importlib.import_module(spec.name[9:])
    23|         sys.modules[spec.name] = mod
    24|         return mod
    25|     def exec_module(self, module):
    26|         return None
    27| class NaclImporter:
    28|     """
    29|     Import hook to force PyNaCl to perform dlopen on libsodium with the
    30|     RTLD_DEEPBIND flag. This is to work around an issue where pyzmq does a dlopen
    31|     with RTLD_GLOBAL which then causes calls to libsodium to resolve to
    32|     tweetnacl when it's been bundled with pyzmq.
    33|     See:  https://github.com/zeromq/pyzmq/issues/1878
    34|     """
    35|     loading = False
    36|     def find_module(self, module_name, package_path=None):
    37|         if not NaclImporter.loading and module_name.startswith("nacl"):
    38|             NaclImporter.loading = True
    39|             return self
    40|         return None
    41|     def create_module(self, spec):
    42|         dlopen = hasattr(sys, "getdlopenflags")
    43|         if dlopen:
    44|             dlflags = sys.getdlopenflags()
    45|             if hasattr(os, "RTLD_DEEPBIND"):
    46|                 flags = os.RTLD_DEEPBIND | dlflags
    47|             else:
    48|                 flags = dlflags
    49|             sys.setdlopenflags(flags)
    50|         try:
    51|             mod = importlib.import_module(spec.name)
    52|         finally:
    53|             if dlopen:
    54|                 sys.setdlopenflags(dlflags)
    55|         NaclImporter.loading = False
    56|         sys.modules[spec.name] = mod
    57|         return mod
    58|     def exec_module(self, module):
    59|         return None
    60| sys.meta_path = [TornadoImporter(), NaclImporter()] + sys.meta_path
    61| warnings.filterwarnings(
    62|     "once",  # Show once
    63|     "",  # No deprecation message match
    64|     DeprecationWarning,  # This filter is for DeprecationWarnings
    65|     r"^(salt|salt\.(.*))$",  # Match module(s) 'salt' and 'salt.<whatever>'
    66| )
    67| warnings.filterwarnings(
    68|     "ignore",
    69|     "^Module backports was already imported from (.*), but (.*) is being added to sys.path$",
    70|     UserWarning,
    71|     append=True,
    72| )
    73| warnings.filterwarnings(
    74|     "ignore",
    75|     message="Setuptools is replacing distutils.",
    76|     category=UserWarning,
    77|     module="_distutils_hack",
    78| )
    79| warnings.filterwarnings(
    80|     "ignore",


# ====================================================================
# FILE: salt/_logging/handlers.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 53-93 ---
    53|         while self.__messages:
    54|             record = self.__messages.popleft()
    55|             for handler in handlers:
    56|                 if handler is self:
    57|                     continue
    58|                 handler.handle(record)
    59| class FileHandler(ExcInfoOnLogLevelFormatMixin, logging.FileHandler):
    60|     """
    61|     File handler which properly handles exc_info on a per handler basis
    62|     """
    63| class SysLogHandler(ExcInfoOnLogLevelFormatMixin, logging.handlers.SysLogHandler):
    64|     """
    65|     Syslog handler which properly handles exc_info on a per handler basis
    66|     """
    67|     def handleError(self, record):
    68|         """
    69|         Override the default error handling mechanism for py3
    70|         Deal with syslog os errors when the log file does not exist
    71|         """
    72|         handled = False
    73|         if sys.stderr and sys.version_info >= (3, 5, 4):
    74|             exc_type, exc, exc_traceback = sys.exc_info()
    75|             try:
    76|                 if exc_type.__name__ in "FileNotFoundError":
    77|                     sys.stderr.write(
    78|                         "[WARNING ] The log_file does not exist. Logging not "
    79|                         "setup correctly or syslog service not started.\n"
    80|                     )
    81|                     handled = True
    82|             finally:
    83|                 del exc_type, exc, exc_traceback
    84|         if not handled:
    85|             super().handleError(record)
    86| class RotatingFileHandler(
    87|     ExcInfoOnLogLevelFormatMixin, logging.handlers.RotatingFileHandler
    88| ):
    89|     """
    90|     Rotating file handler which properly handles exc_info on a per handler basis
    91|     """
    92|     def handleError(self, record):
    93|         """


# ====================================================================
# FILE: salt/_logging/impl.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 2-49 ---
     2|     salt._logging.impl
     3|     ~~~~~~~~~~~~~~~~~~
     4|     Salt's logging implementation classes/functionality
     5| """
     6| import atexit
     7| import logging
     8| import multiprocessing
     9| import os
    10| import pathlib
    11| import re
    12| import socket
    13| import sys
    14| import traceback
    15| import types
    16| import urllib.parse
    17| PROFILE = logging.PROFILE = 15
    18| TRACE = logging.TRACE = 5
    19| GARBAGE = logging.GARBAGE = 1
    20| QUIET = logging.QUIET = 1000
    21| import salt.defaults.exitcodes  # isort:skip  pylint: disable=unused-import
    22| from salt._logging.handlers import DeferredStreamHandler  # isort:skip
    23| from salt._logging.handlers import RotatingFileHandler  # isort:skip
    24| from salt._logging.handlers import StreamHandler  # isort:skip
    25| from salt._logging.handlers import SysLogHandler  # isort:skip
    26| from salt._logging.handlers import WatchedFileHandler  # isort:skip
    27| from salt._logging.mixins import LoggingMixinMeta  # isort:skip
    28| from salt.exceptions import LoggingRuntimeError  # isort:skip
    29| from salt.utils.ctx import RequestContext  # isort:skip
    30| from salt.utils.immutabletypes import freeze, ImmutableDict  # isort:skip
    31| from salt.utils.textformat import TextFormat  # isort:skip
    32| LOG_LEVELS = {
    33|     "all": logging.NOTSET,
    34|     "debug": logging.DEBUG,
    35|     "error": logging.ERROR,
    36|     "critical": logging.CRITICAL,
    37|     "garbage": GARBAGE,
    38|     "info": logging.INFO,
    39|     "profile": PROFILE,
    40|     "quiet": QUIET,
    41|     "trace": TRACE,
    42|     "warning": logging.WARNING,
    43| }
    44| LOG_VALUES_TO_LEVELS = {v: k for (k, v) in LOG_LEVELS.items()}
    45| LOG_COLORS = {
    46|     "levels": {
    47|         "QUIET": TextFormat("reset"),
    48|         "CRITICAL": TextFormat("bold", "red"),
    49|         "ERROR": TextFormat("bold", "red"),

# --- HUNK 2: Lines 118-158 ---
   118|     except AttributeError:
   119|         return
   120| def set_log_record_factory(factory):
   121|     """
   122|     Set the logging  log record factory
   123|     """
   124|     get_log_record_factory.__factory__ = factory
   125|     logging.setLogRecordFactory(factory)
   126| set_log_record_factory(SaltLogRecord)
   127| LOGGING_LOGGER_CLASS = logging.getLoggerClass()
   128| class SaltLoggingClass(LOGGING_LOGGER_CLASS, metaclass=LoggingMixinMeta):
   129|     def __new__(cls, *args):
   130|         """
   131|         We override `__new__` in our logging logger class in order to provide
   132|         some additional features like expand the module name padding if length
   133|         is being used, and also some Unicode fixes.
   134|         This code overhead will only be executed when the class is
   135|         instantiated, i.e.:
   136|             logging.getLogger(__name__)
   137|         """
   138|         instance = super().__new__(cls)  # pylint: disable=no-value-for-parameter
   139|         try:
   140|             max_logger_length = len(
   141|                 max(list(logging.Logger.manager.loggerDict), key=len)
   142|             )
   143|             if max_logger_length > 80:
   144|                 max_logger_length = 80
   145|             for handler in logging.root.handlers:
   146|                 if handler is get_temp_handler():
   147|                     continue
   148|                 formatter = handler.formatter
   149|                 if not formatter:
   150|                     continue
   151|                 if not handler.lock:
   152|                     handler.createLock()
   153|                 handler.acquire()
   154|                 fmt = formatter._fmt.replace("%", "%%")
   155|                 match = MODNAME_PATTERN.search(fmt)
   156|                 if not match:
   157|                     handler.release()
   158|                     return instance

# --- HUNK 3: Lines 169-254 ---
   169|                         fmt % max_logger_length, datefmt=formatter.datefmt
   170|                     )
   171|                     handler.setFormatter(formatter)
   172|                 handler.release()
   173|         except ValueError:
   174|             pass
   175|         return instance
   176|     def _log(
   177|         self,
   178|         level,
   179|         msg,
   180|         args,
   181|         exc_info=None,
   182|         extra=None,  # pylint: disable=arguments-differ
   183|         stack_info=False,
   184|         stacklevel=1,
   185|         exc_info_on_loglevel=None,
   186|     ):
   187|         if extra is None:
   188|             extra = {}
   189|         current_jid = RequestContext.current.get("data", {}).get("jid", None)
   190|         log_fmt_jid = RequestContext.current.get("opts", {}).get("log_fmt_jid", None)
   191|         if current_jid is not None:
   192|             extra["jid"] = current_jid
   193|         if log_fmt_jid is not None:
   194|             extra["log_fmt_jid"] = log_fmt_jid
   195|         if exc_info and exc_info_on_loglevel:
   196|             raise LoggingRuntimeError(
   197|                 "Only one of 'exc_info' and 'exc_info_on_loglevel' is permitted"
   198|             )
   199|         if exc_info_on_loglevel is not None:
   200|             if isinstance(exc_info_on_loglevel, str):
   201|                 exc_info_on_loglevel = LOG_LEVELS.get(
   202|                     exc_info_on_loglevel, logging.ERROR
   203|                 )
   204|             elif not isinstance(exc_info_on_loglevel, int):
   205|                 raise RuntimeError(
   206|                     "The value of 'exc_info_on_loglevel' needs to be a "
   207|                     "logging level or a logging level name, not '{}'".format(
   208|                         exc_info_on_loglevel
   209|                     )
   210|                 )
   211|         if extra is None:
   212|             extra = {"exc_info_on_loglevel": exc_info_on_loglevel}
   213|         else:
   214|             extra["exc_info_on_loglevel"] = exc_info_on_loglevel
   215|         if sys.version_info < (3, 8):
   216|             LOGGING_LOGGER_CLASS._log(
   217|                 self,
   218|                 level,
   219|                 msg,
   220|                 args,
   221|                 exc_info=exc_info,
   222|                 extra=extra,
   223|                 stack_info=stack_info,
   224|             )
   225|         else:
   226|             LOGGING_LOGGER_CLASS._log(
   227|                 self,
   228|                 level,
   229|                 msg,
   230|                 args,
   231|                 exc_info=exc_info,
   232|                 extra=extra,
   233|                 stack_info=stack_info,
   234|                 stacklevel=stacklevel,
   235|             )
   236|     def makeRecord(
   237|         self,
   238|         name,
   239|         level,
   240|         fn,
   241|         lno,
   242|         msg,
   243|         args,
   244|         exc_info,
   245|         func=None,
   246|         extra=None,
   247|         sinfo=None,
   248|     ):
   249|         exc_info_on_loglevel = extra.pop("exc_info_on_loglevel")
   250|         jid = extra.pop("jid", "")
   251|         if jid:
   252|             log_fmt_jid = extra.pop("log_fmt_jid")
   253|             jid = log_fmt_jid % {"jid": jid}
   254|         if not extra:

# --- HUNK 4: Lines 351-391 ---
   351|     """
   352|     Setup the temporary deferred stream handler
   353|     """
   354|     handler = get_temp_handler()
   355|     if handler is not None:
   356|         log_level = get_logging_level_from_string(log_level)
   357|         if handler.level != log_level:
   358|             handler.setLevel(log_level)
   359|         return
   360|     if log_level is None:
   361|         log_level = logging.WARNING
   362|     log_level = get_logging_level_from_string(log_level)
   363|     handler = None
   364|     for handler in logging.root.handlers:
   365|         if not hasattr(handler, "stream"):
   366|             continue
   367|         if handler.stream is sys.stderr:
   368|             break
   369|     else:
   370|         handler = DeferredStreamHandler(sys.stderr)
   371|         atexit.register(handler.flush)
   372|     handler.setLevel(log_level)
   373|     formatter = logging.Formatter(DFLT_LOG_FMT_CONSOLE, datefmt=DFLT_LOG_DATEFMT)
   374|     handler.setFormatter(formatter)
   375|     logging.root.addHandler(handler)
   376|     setup_temp_handler.__handler__ = handler
   377| def shutdown_temp_handler():
   378|     """
   379|     Shutdown the temporary deferred stream handler
   380|     """
   381|     temp_handler = get_temp_handler()
   382|     if temp_handler is not None:
   383|         for handler in logging.root.handlers[:]:
   384|             if handler is temp_handler:
   385|                 logging.root.handlers.remove(handler)
   386|                 handler.sync_with_handlers(logging.root.handlers)
   387|                 handler.close()
   388|                 break
   389|         setup_temp_handler.__handler__ = None
   390| if logging.getLoggerClass() is not SaltLoggingClass:
   391|     try:


# ====================================================================
# FILE: salt/auth/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 206-246 ---
   206|         tdata = {}
   207|         try:
   208|             tdata = self.tokens["{}.get_token".format(self.opts["eauth_tokens"])](
   209|                 self.opts, tok
   210|             )
   211|         except salt.exceptions.SaltDeserializationError:
   212|             log.warning("Failed to load token %r - removing broken/empty file.", tok)
   213|             rm_tok = True
   214|         else:
   215|             if not tdata:
   216|                 return {}
   217|             rm_tok = False
   218|         if tdata.get("expire", 0) < time.time():
   219|             rm_tok = True
   220|         if rm_tok:
   221|             self.rm_token(tok)
   222|             return {}
   223|         return tdata
   224|     def list_tokens(self):
   225|         """
   226|         List all tokens in eauth_tokn storage.
   227|         """
   228|         return self.tokens["{}.list_tokens".format(self.opts["eauth_tokens"])](
   229|             self.opts
   230|         )
   231|     def rm_token(self, tok):
   232|         """
   233|         Remove the given token from token storage.
   234|         """
   235|         self.tokens["{}.rm_token".format(self.opts["eauth_tokens"])](self.opts, tok)
   236|     def authenticate_token(self, load):
   237|         """
   238|         Authenticate a user by the token specified in load.
   239|         Return the token object or False if auth failed.
   240|         """
   241|         token = self.get_tok(load["token"])
   242|         if not token or token["eauth"] not in self.opts["external_auth"]:
   243|             log.warning('Authentication failure of type "token" occurred.')
   244|             return False
   245|         return token
   246|     def authenticate_eauth(self, load):

# --- HUNK 2: Lines 489-529 ---
   489|         """
   490|         load["cmd"] = "mk_token"
   491|         tdata = self._send_token_request(load)
   492|         return tdata
   493|     def get_token(self, token):
   494|         """
   495|         Request a token from the master
   496|         """
   497|         load = {}
   498|         load["token"] = token
   499|         load["cmd"] = "get_token"
   500|         tdata = self._send_token_request(load)
   501|         return tdata
   502| class AuthUser:
   503|     """
   504|     Represents a user requesting authentication to the salt master
   505|     """
   506|     def __init__(self, user):
   507|         """
   508|         Instantiate an AuthUser object.
   509|         Takes a user to reprsent, as a string.
   510|         """
   511|         self.user = user
   512|     def is_sudo(self):
   513|         """
   514|         Determines if the user is running with sudo
   515|         Returns True if the user is running with sudo and False if the
   516|         user is not running with sudo
   517|         """
   518|         return self.user.startswith("sudo_")
   519|     def is_running_user(self):
   520|         """
   521|         Determines if the user is the same user as the one running
   522|         this process
   523|         Returns True if the user is the same user as the one running
   524|         this process and False if not.
   525|         """
   526|         return self.user == salt.utils.user.get_user()
   527|     def sudo_name(self):
   528|         """
   529|         Returns the username of the sudoer, i.e. self.user without the


# ====================================================================
# FILE: salt/beacons/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 279-319 ---
   279|         return True
   280|     def add_beacon(self, name, beacon_data):
   281|         """
   282|         Add a beacon item
   283|         """
   284|         data = {}
   285|         data[name] = beacon_data
   286|         if name in self._get_beacons(include_opts=False):
   287|             comment = (
   288|                 "Cannot update beacon item {}, "
   289|                 "because it is configured in pillar.".format(name)
   290|             )
   291|             complete = False
   292|         else:
   293|             if name in self.opts["beacons"]:
   294|                 comment = f"Updating settings for beacon item: {name}"
   295|             else:
   296|                 comment = f"Added new beacon item: {name}"
   297|             complete = True
   298|             self.opts["beacons"].update(data)
   299|         with salt.utils.event.get_event("minion", opts=self.opts) as evt:
   300|             evt.fire_event(
   301|                 {
   302|                     "complete": complete,
   303|                     "comment": comment,
   304|                     "beacons": self.opts["beacons"],
   305|                 },
   306|                 tag="/salt/minion/minion_beacon_add_complete",
   307|             )
   308|         return True
   309|     def modify_beacon(self, name, beacon_data):
   310|         """
   311|         Modify a beacon item
   312|         """
   313|         data = {}
   314|         data[name] = beacon_data
   315|         if name in self._get_beacons(include_opts=False):
   316|             comment = f"Cannot modify beacon item {name}, it is configured in pillar."
   317|             complete = False
   318|         else:
   319|             comment = f"Updating settings for beacon item: {name}"


# ====================================================================
# FILE: salt/beacons/twilio_txt_msg.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-32 ---
     1| """
     2| Beacon to emit Twilio text messages
     3| """
     4| import logging
     5| import salt.utils.beacons
     6| try:
     7|     import twilio
     8|     twilio_version = tuple(int(x) for x in twilio.__version_info__)
     9|     if twilio_version > (5,):
    10|         from twilio.rest import Client as TwilioRestClient
    11|     else:
    12|         from twilio.rest import TwilioRestClient
    13|     HAS_TWILIO = True
    14| except ImportError:
    15|     HAS_TWILIO = False
    16| log = logging.getLogger(__name__)
    17| __virtualname__ = "twilio_txt_msg"
    18| def __virtual__():
    19|     if HAS_TWILIO:
    20|         return __virtualname__
    21|     else:
    22|         err_msg = "twilio library is missing."
    23|         log.error("Unable to load %s beacon: %s", __virtualname__, err_msg)
    24|         return False, err_msg
    25| def validate(config):
    26|     """
    27|     Validate the beacon configuration
    28|     """
    29|     if not isinstance(config, list):
    30|         return False, "Configuration for twilio_txt_msg beacon must be a list."
    31|     else:
    32|         config = salt.utils.beacons.list_to_dict(config)


# ====================================================================
# FILE: salt/cache/mysql_cache.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 5-109 ---
     5| infrastructure. All is needed for this plugin is a working MySQL server.
     6| .. warning::
     7|     The mysql.database and mysql.table_name will be directly added into certain
     8|     queries. Salt treats these as trusted input.
     9| The module requires the database (default ``salt_cache``) to exist but creates
    10| its own table if needed. The keys are indexed using the ``bank`` and
    11| ``etcd_key`` columns.
    12| To enable this cache plugin, the master will need the python client for
    13| MySQL installed. This can be easily installed with pip:
    14| .. code-block:: bash
    15|     pip install pymysql
    16| Optionally, depending on the MySQL agent configuration, the following values
    17| could be set in the master config. These are the defaults:
    18| .. code-block:: yaml
    19|     mysql.host: 127.0.0.1
    20|     mysql.port: 2379
    21|     mysql.user: None
    22|     mysql.password: None
    23|     mysql.database: salt_cache
    24|     mysql.table_name: cache
    25| Related docs can be found in the `python-mysql documentation`_.
    26| To use the mysql as a minion data cache backend, set the master ``cache`` config
    27| value to ``mysql``:
    28| .. code-block:: yaml
    29|     cache: mysql
    30| .. _`MySQL documentation`: https://github.com/coreos/mysql
    31| .. _`python-mysql documentation`: http://python-mysql.readthedocs.io/en/latest/
    32| """
    33| import copy
    34| import logging
    35| import time
    36| import salt.payload
    37| import salt.utils.stringutils
    38| from salt.exceptions import SaltCacheError
    39| try:
    40|     import MySQLdb
    41|     import MySQLdb.converters
    42|     import MySQLdb.cursors
    43|     from MySQLdb.connections import OperationalError
    44| except ImportError:
    45|     try:
    46|         import pymysql
    47|         pymysql.install_as_MySQLdb()
    48|         import MySQLdb
    49|         import MySQLdb.converters
    50|         import MySQLdb.cursors
    51|         from MySQLdb.err import OperationalError
    52|     except ImportError:
    53|         MySQLdb = None
    54| _DEFAULT_DATABASE_NAME = "salt_cache"
    55| _DEFAULT_CACHE_TABLE_NAME = "cache"
    56| _RECONNECT_INTERVAL_SEC = 0.050
    57| log = logging.getLogger(__name__)
    58| __virtualname__ = "mysql"
    59| __func_alias__ = {"ls": "list"}
    60| def __virtual__():
    61|     """
    62|     Confirm that a python mysql client is installed.
    63|     """
    64|     return bool(MySQLdb), "No python mysql client installed." if MySQLdb is None else ""
    65| def force_reconnect():
    66|     """
    67|     Force a reconnection to the MySQL database, by removing the client from
    68|     Salt's __context__.
    69|     """
    70|     __context__.pop("mysql_client", None)
    71| def run_query(conn, query, args=None, retries=3):
    72|     """
    73|     Get a cursor and run a query. Reconnect up to ``retries`` times if
    74|     needed.
    75|     Returns: cursor, affected rows counter
    76|     Raises: SaltCacheError, AttributeError, OperationalError
    77|     """
    78|     if conn is None:
    79|         conn = __context__.get("mysql_client")
    80|     try:
    81|         cur = conn.cursor()
    82|         if not args:
    83|             log.debug("Doing query: %s", query)
    84|             out = cur.execute(query)
    85|         else:
    86|             log.debug("Doing query: %s args: %s ", query, repr(args))
    87|             out = cur.execute(query, args)
    88|         return cur, out
    89|     except (AttributeError, OperationalError) as e:
    90|         if retries == 0:
    91|             raise
    92|         time.sleep(_RECONNECT_INTERVAL_SEC)
    93|         if conn is None:
    94|             log.debug("mysql_cache: creating db connection")
    95|         else:
    96|             log.info("mysql_cache: recreating db connection due to: %r", e)
    97|         __context__["mysql_client"] = MySQLdb.connect(**__context__["mysql_kwargs"])
    98|         return run_query(
    99|             conn=__context__.get("mysql_client"),
   100|             query=query,
   101|             args=args,
   102|             retries=(retries - 1),
   103|         )
   104|     except Exception as e:  # pylint: disable=broad-except
   105|         if len(query) > 150:
   106|             query = query[:150] + "<...>"
   107|         raise SaltCacheError(
   108|             "Error running {}{}: {}".format(query, f"- args: {args}" if args else "", e)
   109|         )

# --- HUNK 2: Lines 165-204 ---
   165|     log.info("mysql_cache: creating table %s", __context__["mysql_table_name"])
   166|     cur, _ = run_query(__context__.get("mysql_client"), query)
   167|     cur.close()
   168| def _init_client():
   169|     """Initialize connection and create table if needed"""
   170|     if __context__.get("mysql_client") is not None:
   171|         return
   172|     opts = copy.deepcopy(__opts__)
   173|     mysql_kwargs = {
   174|         "autocommit": True,
   175|         "host": opts.pop("mysql.host", "127.0.0.1"),
   176|         "user": opts.pop("mysql.user", None),
   177|         "passwd": opts.pop("mysql.password", None),
   178|         "db": opts.pop("mysql.database", _DEFAULT_DATABASE_NAME),
   179|         "port": opts.pop("mysql.port", 3306),
   180|         "unix_socket": opts.pop("mysql.unix_socket", None),
   181|         "connect_timeout": opts.pop("mysql.connect_timeout", None),
   182|     }
   183|     mysql_kwargs["autocommit"] = True
   184|     __context__["mysql_table_name"] = opts.pop("mysql.table_name", "salt")
   185|     for k in opts:
   186|         if k.startswith("mysql."):
   187|             _key = k.split(".")[1]
   188|             mysql_kwargs[_key] = opts.get(k)
   189|     for k, v in copy.deepcopy(mysql_kwargs).items():
   190|         if v is None:
   191|             mysql_kwargs.pop(k)
   192|     kwargs_copy = mysql_kwargs.copy()
   193|     kwargs_copy["passwd"] = "<hidden>"
   194|     log.info("mysql_cache: Setting up client with params: %r", kwargs_copy)
   195|     __context__["mysql_kwargs"] = mysql_kwargs
   196|     _create_table()
   197| def store(bank, key, data):
   198|     """
   199|     Store a key value.
   200|     """
   201|     _init_client()
   202|     data = salt.payload.dumps(data)
   203|     query = "REPLACE INTO {} (bank, etcd_key, data) values(%s,%s,%s)".format(
   204|         __context__["mysql_table_name"]

# --- HUNK 3: Lines 216-256 ---
   216|     query = "SELECT data FROM {} WHERE bank=%s AND etcd_key=%s".format(
   217|         __context__["mysql_table_name"]
   218|     )
   219|     cur, _ = run_query(__context__.get("mysql_client"), query, args=(bank, key))
   220|     r = cur.fetchone()
   221|     cur.close()
   222|     if r is None:
   223|         return {}
   224|     return salt.payload.loads(r[0])
   225| def flush(bank, key=None):
   226|     """
   227|     Remove the key from the cache bank with all the key content.
   228|     """
   229|     _init_client()
   230|     query = "DELETE FROM {} WHERE bank=%s".format(__context__["mysql_table_name"])
   231|     if key is None:
   232|         data = (bank,)
   233|     else:
   234|         data = (bank, key)
   235|         query += " AND etcd_key=%s"
   236|     cur, _ = run_query(__context__["mysql_client"], query, args=data)
   237|     cur.close()
   238| def ls(bank):
   239|     """
   240|     Return an iterable object containing all entries stored in the specified
   241|     bank.
   242|     """
   243|     _init_client()
   244|     query = "SELECT etcd_key FROM {} WHERE bank=%s".format(
   245|         __context__["mysql_table_name"]
   246|     )
   247|     cur, _ = run_query(__context__.get("mysql_client"), query, args=(bank,))
   248|     out = [row[0] for row in cur.fetchall()]
   249|     cur.close()
   250|     return out
   251| def contains(bank, key):
   252|     """
   253|     Checks if the specified bank contains the specified key.
   254|     """
   255|     _init_client()
   256|     if key is None:

# --- HUNK 4: Lines 261-284 ---
   261|     else:
   262|         data = (bank, key)
   263|         query = "SELECT COUNT(data) FROM {} WHERE bank=%s AND etcd_key=%s".format(
   264|             __context__["mysql_table_name"]
   265|         )
   266|     cur, _ = run_query(__context__.get("mysql_client"), query, args=data)
   267|     r = cur.fetchone()
   268|     cur.close()
   269|     return r[0] == 1
   270| def updated(bank, key):
   271|     """
   272|     Return the integer Unix epoch update timestamp of the specified bank and
   273|     key.
   274|     """
   275|     _init_client()
   276|     query = (
   277|         "SELECT UNIX_TIMESTAMP(last_update) FROM {} WHERE bank=%s "
   278|         "AND etcd_key=%s".format(__context__["mysql_table_name"])
   279|     )
   280|     data = (bank, key)
   281|     cur, _ = run_query(__context__["mysql_client"], query=query, args=data)
   282|     r = cur.fetchone()
   283|     cur.close()
   284|     return int(r[0]) if r else r


# ====================================================================
# FILE: salt/channel/client.py
# Total hunks: 8
# ====================================================================
# --- HUNK 1: Lines 1-40 ---
     1| """
     2| Encapsulate the different transports available to Salt.
     3| This includes client side transport, for the ReqServer and the Publisher
     4| """
     5| import logging
     6| import os
     7| import time
     8| import uuid
     9| import salt.crypt
    10| import salt.exceptions
    11| import salt.ext.tornado.gen
    12| import salt.ext.tornado.ioloop
    13| import salt.payload
    14| import salt.transport.frame
    15| import salt.utils.event
    16| import salt.utils.files
    17| import salt.utils.minions
    18| import salt.utils.stringutils
    19| import salt.utils.verify
    20| from salt.utils.asynchronous import SyncWrapper
    21| log = logging.getLogger(__name__)
    22| REQUEST_CHANNEL_TIMEOUT = 60
    23| REQUEST_CHANNEL_TRIES = 3
    24| class ReqChannel:
    25|     """
    26|     Factory class to create a sychronous communication channels to the master's
    27|     ReqServer. ReqChannels use transports to connect to the ReqServer.
    28|     """
    29|     @staticmethod
    30|     def factory(opts, **kwargs):
    31|         return SyncWrapper(
    32|             AsyncReqChannel.factory,
    33|             (opts,),
    34|             kwargs,
    35|             loop_kwarg="io_loop",
    36|         )
    37| class PushChannel:
    38|     """
    39|     Factory class to create Sync channel for push side of push/pull IPC
    40|     """

# --- HUNK 2: Lines 67-435 ---
    67|         "crypted_transfer_decode_dictentry",
    68|         "_crypted_transfer",
    69|         "_uncrypted_transfer",
    70|         "send",
    71|         "connect",
    72|     ]
    73|     close_methods = [
    74|         "close",
    75|     ]
    76|     @classmethod
    77|     def factory(cls, opts, **kwargs):
    78|         ttype = "zeromq"
    79|         if "transport" in opts:
    80|             ttype = opts["transport"]
    81|         elif "transport" in opts.get("pillar", {}).get("master", {}):
    82|             ttype = opts["pillar"]["master"]["transport"]
    83|         if "master_uri" not in opts and "master_uri" in kwargs:
    84|             opts["master_uri"] = kwargs["master_uri"]
    85|         io_loop = kwargs.get("io_loop")
    86|         if io_loop is None:
    87|             io_loop = salt.ext.tornado.ioloop.IOLoop.current()
    88|         timeout = opts.get("request_channel_timeout", REQUEST_CHANNEL_TIMEOUT)
    89|         tries = opts.get("request_channel_tries", REQUEST_CHANNEL_TRIES)
    90|         crypt = kwargs.get("crypt", "aes")
    91|         if crypt != "clear":
    92|             auth = salt.crypt.AsyncAuth(opts, io_loop=io_loop)
    93|         else:
    94|             auth = None
    95|         transport = salt.transport.request_client(opts, io_loop=io_loop)
    96|         return cls(opts, transport, auth, tries=tries, timeout=timeout)
    97|     def __init__(
    98|         self,
    99|         opts,
   100|         transport,
   101|         auth,
   102|         timeout=REQUEST_CHANNEL_TIMEOUT,
   103|         tries=REQUEST_CHANNEL_TRIES,
   104|         **kwargs,
   105|     ):
   106|         self.opts = dict(opts)
   107|         self.transport = transport
   108|         self.auth = auth
   109|         self.master_pubkey_path = None
   110|         if self.auth:
   111|             self.master_pubkey_path = os.path.join(self.opts["pki_dir"], self.auth.mpub)
   112|         self._closing = False
   113|         self.timeout = timeout
   114|         self.tries = tries
   115|     @property
   116|     def crypt(self):
   117|         if self.auth:
   118|             return "aes"
   119|         return "clear"
   120|     @property
   121|     def ttype(self):
   122|         return self.transport.ttype
   123|     def _package_load(self, load):
   124|         ret = {
   125|             "enc": self.crypt,
   126|             "load": load,
   127|             "version": 2,
   128|         }
   129|         if self.crypt == "aes":
   130|             ret["enc_algo"] = self.opts["encryption_algorithm"]
   131|             ret["sig_algo"] = self.opts["signing_algorithm"]
   132|         return ret
   133|     @salt.ext.tornado.gen.coroutine
   134|     def _send_with_retry(self, load, tries, timeout):
   135|         _try = 1
   136|         while True:
   137|             try:
   138|                 ret = yield self.transport.send(
   139|                     load,
   140|                     timeout=timeout,
   141|                 )
   142|                 break
   143|             except Exception as exc:  # pylint: disable=broad-except
   144|                 log.trace("Failed to send msg %r", exc)
   145|                 if _try >= tries:
   146|                     raise
   147|                 else:
   148|                     _try += 1
   149|                     continue
   150|         raise salt.ext.tornado.gen.Return(ret)
   151|     @salt.ext.tornado.gen.coroutine
   152|     def crypted_transfer_decode_dictentry(
   153|         self,
   154|         load,
   155|         dictkey=None,
   156|         timeout=None,
   157|         tries=None,
   158|     ):
   159|         if timeout is None:
   160|             timeout = self.timeout
   161|         if tries is None:
   162|             tries = self.tries
   163|         nonce = uuid.uuid4().hex
   164|         load["nonce"] = nonce
   165|         if not self.auth.authenticated:
   166|             yield self.auth.authenticate()
   167|         ret = yield self._send_with_retry(
   168|             self._package_load(self.auth.crypticle.dumps(load)),
   169|             tries,
   170|             timeout,
   171|         )
   172|         key = self.auth.get_keys()
   173|         if "key" not in ret:
   174|             yield self.auth.authenticate()
   175|             ret = yield self._send_with_retry(
   176|                 self._package_load(self.auth.crypticle.dumps(load)),
   177|                 tries,
   178|                 timeout,
   179|             )
   180|         aes = key.decrypt(ret["key"], self.opts["encryption_algorithm"])
   181|         pcrypt = salt.crypt.Crypticle(self.opts, aes)
   182|         signed_msg = pcrypt.loads(ret[dictkey])
   183|         if not self.verify_signature(signed_msg["data"], signed_msg["sig"]):
   184|             raise salt.crypt.AuthenticationError(
   185|                 "Pillar payload signature failed to validate."
   186|             )
   187|         data = salt.payload.loads(signed_msg["data"])
   188|         if data["key"] != ret["key"]:
   189|             raise salt.crypt.AuthenticationError("Key verification failed.")
   190|         if data["nonce"] != nonce:
   191|             raise salt.crypt.AuthenticationError("Pillar nonce verification failed.")
   192|         raise salt.ext.tornado.gen.Return(data["pillar"])
   193|     def verify_signature(self, data, sig):
   194|         return salt.crypt.PublicKey(self.master_pubkey_path).verify(
   195|             data, sig, self.opts["signing_algorithm"]
   196|         )
   197|     @salt.ext.tornado.gen.coroutine
   198|     def _crypted_transfer(self, load, timeout, raw=False):
   199|         """
   200|         Send a load across the wire, with encryption
   201|         In case of authentication errors, try to renegotiate authentication
   202|         and retry the method.
   203|         Indeed, we can fail too early in case of a master restart during a
   204|         minion state execution call
   205|         :param dict load: A load to send across the wire
   206|         :param int timeout: The number of seconds on a response before failing
   207|         """
   208|         nonce = uuid.uuid4().hex
   209|         if load and isinstance(load, dict):
   210|             load["nonce"] = nonce
   211|         @salt.ext.tornado.gen.coroutine
   212|         def _do_transfer():
   213|             data = yield self.transport.send(
   214|                 self._package_load(self.auth.crypticle.dumps(load)),
   215|                 timeout=timeout,
   216|             )
   217|             if data:
   218|                 data = self.auth.crypticle.loads(data, raw, nonce=nonce)
   219|             if not raw or self.ttype == "tcp":  # XXX Why is this needed for tcp
   220|                 data = salt.transport.frame.decode_embedded_strs(data)
   221|             raise salt.ext.tornado.gen.Return(data)
   222|         if not self.auth.authenticated:
   223|             yield self.auth.authenticate()
   224|         try:
   225|             ret = yield _do_transfer()
   226|         except salt.crypt.AuthenticationError:
   227|             yield self.auth.authenticate()
   228|             ret = yield _do_transfer()
   229|         raise salt.ext.tornado.gen.Return(ret)
   230|     @salt.ext.tornado.gen.coroutine
   231|     def _uncrypted_transfer(self, load, timeout):
   232|         """
   233|         Send a load across the wire in cleartext
   234|         :param dict load: A load to send across the wire
   235|         :param int timeout: The number of seconds on a response before failing
   236|         """
   237|         ret = yield self.transport.send(
   238|             self._package_load(load),
   239|             timeout=timeout,
   240|         )
   241|         raise salt.ext.tornado.gen.Return(ret)
   242|     @salt.ext.tornado.gen.coroutine
   243|     def connect(self):
   244|         yield self.transport.connect()
   245|     @salt.ext.tornado.gen.coroutine
   246|     def send(self, load, tries=None, timeout=None, raw=False):
   247|         """
   248|         Send a request, return a future which will complete when we send the message
   249|         :param dict load: A load to send across the wire
   250|         :param int tries: The number of times to make before failure
   251|         :param int timeout: The number of seconds on a response before failing
   252|         """
   253|         if timeout is None:
   254|             timeout = self.timeout
   255|         if tries is None:
   256|             tries = self.tries
   257|         _try = 1
   258|         while True:
   259|             try:
   260|                 if self.crypt == "clear":
   261|                     log.trace("ReqChannel send clear load=%r", load)
   262|                     ret = yield self._uncrypted_transfer(load, timeout=timeout)
   263|                 else:
   264|                     log.trace("ReqChannel send crypt load=%r", load)
   265|                     ret = yield self._crypted_transfer(load, timeout=timeout, raw=raw)
   266|                 break
   267|             except Exception as exc:  # pylint: disable=broad-except
   268|                 log.trace("Failed to send msg %r", exc)
   269|                 if _try >= tries:
   270|                     raise
   271|                 else:
   272|                     _try += 1
   273|                     continue
   274|         raise salt.ext.tornado.gen.Return(ret)
   275|     def close(self):
   276|         """
   277|         Since the message_client creates sockets and assigns them to the IOLoop we have to
   278|         specifically destroy them, since we aren't the only ones with references to the FDs
   279|         """
   280|         if self._closing:
   281|             return
   282|         log.debug("Closing %s instance", self.__class__.__name__)
   283|         self._closing = True
   284|         self.transport.close()
   285|     def __enter__(self):
   286|         return self
   287|     def __exit__(self, *args):
   288|         self.close()
   289| class AsyncPubChannel:
   290|     """
   291|     Factory class to create subscription channels to the master's Publisher
   292|     """
   293|     async_methods = [
   294|         "connect",
   295|         "_decode_messages",
   296|     ]
   297|     close_methods = [
   298|         "close",
   299|     ]
   300|     @classmethod
   301|     def factory(cls, opts, **kwargs):
   302|         ttype = "zeromq"
   303|         if "transport" in opts:
   304|             ttype = opts["transport"]
   305|         elif "transport" in opts.get("pillar", {}).get("master", {}):
   306|             ttype = opts["pillar"]["master"]["transport"]
   307|         if "master_uri" not in opts and "master_uri" in kwargs:
   308|             opts["master_uri"] = kwargs["master_uri"]
   309|         if ttype == "detect":
   310|             opts["detect_mode"] = True
   311|             log.info("Transport is set to detect; using %s", ttype)
   312|         io_loop = kwargs.get("io_loop")
   313|         if io_loop is None:
   314|             io_loop = salt.ext.tornado.ioloop.IOLoop.current()
   315|         auth = salt.crypt.AsyncAuth(opts, io_loop=io_loop)
   316|         transport = salt.transport.publish_client(opts, io_loop)
   317|         return cls(opts, transport, auth, io_loop)
   318|     def __init__(self, opts, transport, auth, io_loop=None):
   319|         self.opts = opts
   320|         self.io_loop = io_loop
   321|         self.auth = auth
   322|         self.token = self.auth.gen_token(b"salt")
   323|         self.transport = transport
   324|         self._closing = False
   325|         self._reconnected = False
   326|         self.event = salt.utils.event.get_event("minion", opts=self.opts, listen=False)
   327|         self.master_pubkey_path = os.path.join(self.opts["pki_dir"], self.auth.mpub)
   328|     @property
   329|     def crypt(self):
   330|         return "aes" if self.auth else "clear"
   331|     @salt.ext.tornado.gen.coroutine
   332|     def connect(self):
   333|         """
   334|         Return a future which completes when connected to the remote publisher
   335|         """
   336|         try:
   337|             if not self.auth.authenticated:
   338|                 yield self.auth.authenticate()
   339|             if int(self.opts.get("publish_port", 4506)) != 4506:
   340|                 publish_port = self.opts.get("publish_port")
   341|             else:
   342|                 publish_port = self.auth.creds["publish_port"]
   343|             yield self.transport.connect(
   344|                 publish_port, self.connect_callback, self.disconnect_callback
   345|             )
   346|         except KeyboardInterrupt:  # pylint: disable=try-except-raise
   347|             raise
   348|         except Exception as exc:  # pylint: disable=broad-except
   349|             if "-|RETRY|-" not in str(exc):
   350|                 raise salt.exceptions.SaltClientError(
   351|                     f"Unable to sign_in to master: {exc}"
   352|                 )  # TODO: better error message
   353|     def close(self):
   354|         """
   355|         Close the channel
   356|         """
   357|         self.transport.close()
   358|         if self.event is not None:
   359|             self.event.destroy()
   360|             self.event = None
   361|     def on_recv(self, callback=None):
   362|         """
   363|         When jobs are received pass them (decoded) to callback
   364|         """
   365|         if callback is None:
   366|             return self.transport.on_recv(None)
   367|         @salt.ext.tornado.gen.coroutine
   368|         def wrap_callback(messages):
   369|             payload = yield self.transport._decode_messages(messages)
   370|             decoded = yield self._decode_payload(payload)
   371|             log.debug("PubChannel received: %r", decoded)
   372|             if decoded is not None:
   373|                 callback(decoded)
   374|         return self.transport.on_recv(wrap_callback)
   375|     def _package_load(self, load):
   376|         return {
   377|             "enc": self.crypt,
   378|             "load": load,
   379|             "version": 2,
   380|         }
   381|     @salt.ext.tornado.gen.coroutine
   382|     def send_id(self, tok, force_auth):
   383|         """
   384|         Send the minion id to the master so that the master may better
   385|         track the connection state of the minion.
   386|         In case of authentication errors, try to renegotiate authentication
   387|         and retry the method.
   388|         """
   389|         load = {"id": self.opts["id"], "tok": tok}
   390|         @salt.ext.tornado.gen.coroutine
   391|         def _do_transfer():
   392|             msg = self._package_load(self.auth.crypticle.dumps(load))
   393|             package = salt.transport.frame.frame_msg(msg, header=None)
   394|             yield self.transport.send(package)
   395|             raise salt.ext.tornado.gen.Return(True)
   396|         if force_auth or not self.auth.authenticated:
   397|             count = 0
   398|             while (
   399|                 count <= self.opts["tcp_authentication_retries"]
   400|                 or self.opts["tcp_authentication_retries"] < 0
   401|             ):
   402|                 try:
   403|                     yield self.auth.authenticate()
   404|                     break
   405|                 except salt.exceptions.SaltClientError as exc:
   406|                     log.debug(exc)
   407|                     count += 1
   408|         try:
   409|             ret = yield _do_transfer()
   410|             raise salt.ext.tornado.gen.Return(ret)
   411|         except salt.crypt.AuthenticationError:
   412|             yield self.auth.authenticate()
   413|             ret = yield _do_transfer()
   414|             raise salt.ext.tornado.gen.Return(ret)
   415|     @salt.ext.tornado.gen.coroutine
   416|     def connect_callback(self, result):
   417|         if self._closing:
   418|             return
   419|         try:
   420|             yield self.send_id(self.token, self._reconnected)
   421|             self.connected = True
   422|             self.event.fire_event({"master": self.opts["master"]}, "__master_connected")
   423|             if self._reconnected:
   424|                 if self.opts.get("__role") == "syndic":
   425|                     data = "Syndic {} started at {}".format(
   426|                         self.opts["id"], time.asctime()
   427|                     )
   428|                     tag = salt.utils.event.tagify([self.opts["id"], "start"], "syndic")
   429|                 else:
   430|                     data = "Minion {} started at {}".format(
   431|                         self.opts["id"], time.asctime()
   432|                     )
   433|                     tag = salt.utils.event.tagify([self.opts["id"], "start"], "minion")
   434|                 load = {
   435|                     "id": self.opts["id"],

# --- HUNK 3: Lines 450-523 ---
   450|                     except Exception:  # pylint: disable=broad-except
   451|                         log.info("fire_master failed", exc_info=True)
   452|             else:
   453|                 self._reconnected = True
   454|         except Exception as exc:  # pylint: disable=broad-except
   455|             log.error(
   456|                 "Caught exception in PubChannel connect callback %r", exc, exc_info=True
   457|             )
   458|     def disconnect_callback(self):
   459|         if self._closing:
   460|             return
   461|         self.connected = False
   462|         self.event.fire_event({"master": self.opts["master"]}, "__master_disconnected")
   463|     def _verify_master_signature(self, payload):
   464|         if self.opts.get("sign_pub_messages"):
   465|             if not payload.get("sig", False):
   466|                 raise salt.crypt.AuthenticationError(
   467|                     "Message signing is enabled but the payload has no signature."
   468|                 )
   469|             if not salt.crypt.verify_signature(
   470|                 self.master_pubkey_path,
   471|                 payload["load"],
   472|                 payload.get("sig"),
   473|                 algorithm=payload["sig_algo"],
   474|             ):
   475|                 raise salt.crypt.AuthenticationError(
   476|                     "Message signature failed to validate."
   477|                 )
   478|     @salt.ext.tornado.gen.coroutine
   479|     def _decode_payload(self, payload):
   480|         log.trace("Decoding payload: %s", payload)
   481|         reauth = False
   482|         if payload["enc"] == "aes":
   483|             self._verify_master_signature(payload)
   484|             try:
   485|                 payload["load"] = self.auth.crypticle.loads(payload["load"])
   486|             except salt.crypt.AuthenticationError:
   487|                 reauth = True
   488|             if reauth:
   489|                 try:
   490|                     yield self.auth.authenticate()
   491|                     payload["load"] = self.auth.crypticle.loads(payload["load"])
   492|                 except salt.crypt.AuthenticationError:
   493|                     log.error(
   494|                         "Payload decryption failed even after re-authenticating with master %s",
   495|                         self.opts["master_ip"],
   496|                     )
   497|         raise salt.ext.tornado.gen.Return(payload)
   498|     def __enter__(self):
   499|         return self
   500|     def __exit__(self, *args):
   501|         self.close()
   502| class AsyncPushChannel:
   503|     """
   504|     Factory class to create IPC Push channels
   505|     """
   506|     @staticmethod
   507|     def factory(opts, **kwargs):
   508|         """
   509|         If we have additional IPC transports other than UxD and TCP, add them here
   510|         """
   511|         import salt.transport.ipc
   512|         return salt.transport.ipc.IPCMessageClient(opts, **kwargs)
   513| class AsyncPullChannel:
   514|     """
   515|     Factory class to create IPC pull channels
   516|     """
   517|     @staticmethod
   518|     def factory(opts, **kwargs):
   519|         """
   520|         If we have additional IPC transports other than UXD and TCP, add them here
   521|         """
   522|         import salt.transport.ipc
   523|         return salt.transport.ipc.IPCMessageServer(opts, **kwargs)


# ====================================================================
# FILE: salt/channel/server.py
# Total hunks: 13
# ====================================================================
# --- HUNK 1: Lines 1-712 ---
     1| """
     2| Encapsulate the different transports available to Salt.
     3| This includes server side transport, for the ReqServer and the Publisher
     4| """
     5| import binascii
     6| import hashlib
     7| import logging
     8| import os
     9| import shutil
    10| import salt.crypt
    11| import salt.ext.tornado.gen
    12| import salt.master
    13| import salt.payload
    14| import salt.transport.frame
    15| import salt.utils.channel
    16| import salt.utils.event
    17| import salt.utils.files
    18| import salt.utils.minions
    19| import salt.utils.platform
    20| import salt.utils.stringutils
    21| import salt.utils.verify
    22| from salt.exceptions import SaltDeserializationError, UnsupportedAlgorithm
    23| from salt.utils.cache import CacheCli
    24| log = logging.getLogger(__name__)
    25| class ReqServerChannel:
    26|     """
    27|     ReqServerChannel handles request/reply messages from ReqChannels.
    28|     """
    29|     @classmethod
    30|     def factory(cls, opts, **kwargs):
    31|         if "master_uri" not in opts and "master_uri" in kwargs:
    32|             opts["master_uri"] = kwargs["master_uri"]
    33|         transport = salt.transport.request_server(opts, **kwargs)
    34|         return cls(opts, transport)
    35|     @classmethod
    36|     def compare_keys(cls, key1, key2):
    37|         """
    38|         Normalize and compare two keys
    39|         Returns:
    40|             bool: ``True`` if the keys match, otherwise ``False``
    41|         """
    42|         return salt.crypt.clean_key(key1) == salt.crypt.clean_key(key2)
    43|     def __init__(self, opts, transport):
    44|         self.opts = opts
    45|         self.transport = transport
    46|         self.event = None
    47|         self.master_key = None
    48|     def pre_fork(self, process_manager):
    49|         """
    50|         Do anything necessary pre-fork. Since this is on the master side this will
    51|         primarily be bind and listen (or the equivalent for your network library)
    52|         """
    53|         if hasattr(self.transport, "pre_fork"):
    54|             self.transport.pre_fork(process_manager)
    55|     def post_fork(self, payload_handler, io_loop):
    56|         """
    57|         Do anything you need post-fork. This should handle all incoming payloads
    58|         and call payload_handler. You will also be passed io_loop, for all of your
    59|         asynchronous needs
    60|         """
    61|         import salt.master
    62|         if self.opts["pub_server_niceness"] and not salt.utils.platform.is_windows():
    63|             log.info(
    64|                 "setting Publish daemon niceness to %i",
    65|                 self.opts["pub_server_niceness"],
    66|             )
    67|             os.nice(self.opts["pub_server_niceness"])
    68|         self.io_loop = io_loop
    69|         self.crypticle = salt.crypt.Crypticle(
    70|             self.opts, salt.master.SMaster.secrets["aes"]["secret"].value
    71|         )
    72|         self.event = salt.utils.event.get_master_event(
    73|             self.opts, self.opts["sock_dir"], listen=False
    74|         )
    75|         self.auto_key = salt.daemons.masterapi.AutoKey(self.opts)
    76|         if self.opts["con_cache"]:
    77|             self.cache_cli = CacheCli(self.opts)
    78|         else:
    79|             self.cache_cli = False
    80|             self.ckminions = salt.utils.minions.CkMinions(self.opts)
    81|         self.master_key = salt.crypt.MasterKeys(self.opts)
    82|         self.payload_handler = payload_handler
    83|         if hasattr(self.transport, "post_fork"):
    84|             self.transport.post_fork(self.handle_message, io_loop)
    85|     @salt.ext.tornado.gen.coroutine
    86|     def handle_message(self, payload):
    87|         try:
    88|             payload = self._decode_payload(payload)
    89|         except Exception as exc:  # pylint: disable=broad-except
    90|             exc_type = type(exc).__name__
    91|             if exc_type == "AuthenticationError":
    92|                 log.debug(
    93|                     "Minion failed to auth to master. Since the payload is "
    94|                     "encrypted, it is not known which minion failed to "
    95|                     "authenticate. It is likely that this is a transient "
    96|                     "failure due to the master rotating its public key."
    97|                 )
    98|             else:
    99|                 log.error("Bad load from minion: %s: %s", exc_type, exc)
   100|             raise salt.ext.tornado.gen.Return("bad load")
   101|         if not isinstance(payload, dict) or not isinstance(payload.get("load"), dict):
   102|             log.error(
   103|                 "payload and load must be a dict. Payload was: %s and load was %s",
   104|                 payload,
   105|                 payload.get("load"),
   106|             )
   107|             raise salt.ext.tornado.gen.Return("payload and load must be a dict")
   108|         try:
   109|             id_ = payload["load"].get("id", "")
   110|             if "\0" in id_:
   111|                 log.error("Payload contains an id with a null byte: %s", payload)
   112|                 raise salt.ext.tornado.gen.Return("bad load: id contains a null byte")
   113|         except TypeError:
   114|             log.error("Payload contains non-string id: %s", payload)
   115|             raise salt.ext.tornado.gen.Return(f"bad load: id {id_} is not a string")
   116|         version = 0
   117|         if "version" in payload:
   118|             version = payload["version"]
   119|         sign_messages = False
   120|         if version > 1:
   121|             sign_messages = True
   122|         if payload["enc"] == "clear" and payload.get("load", {}).get("cmd") == "_auth":
   123|             raise salt.ext.tornado.gen.Return(
   124|                 self._auth(payload["load"], sign_messages)
   125|             )
   126|         nonce = None
   127|         if version > 1:
   128|             nonce = payload["load"].pop("nonce", None)
   129|         try:
   130|             ret, req_opts = yield self.payload_handler(payload)
   131|         except Exception as e:  # pylint: disable=broad-except
   132|             log.error("Some exception handling a payload from minion", exc_info=True)
   133|             raise salt.ext.tornado.gen.Return("Some exception handling minion payload")
   134|         req_fun = req_opts.get("fun", "send")
   135|         if req_fun == "send_clear":
   136|             raise salt.ext.tornado.gen.Return(ret)
   137|         elif req_fun == "send":
   138|             raise salt.ext.tornado.gen.Return(self.crypticle.dumps(ret, nonce))
   139|         elif req_fun == "send_private":
   140|             raise salt.ext.tornado.gen.Return(
   141|                 self._encrypt_private(
   142|                     ret,
   143|                     req_opts["key"],
   144|                     req_opts["tgt"],
   145|                     nonce,
   146|                     sign_messages,
   147|                     payload.get("enc_algo", salt.crypt.OAEP_SHA1),
   148|                     payload.get("sig_algo", salt.crypt.PKCS1v15_SHA1),
   149|                 ),
   150|             )
   151|         log.error("Unknown req_fun %s", req_fun)
   152|         raise salt.ext.tornado.gen.Return("Server-side exception handling payload")
   153|     def _encrypt_private(
   154|         self,
   155|         ret,
   156|         dictkey,
   157|         target,
   158|         nonce=None,
   159|         sign_messages=True,
   160|         encryption_algorithm=salt.crypt.OAEP_SHA1,
   161|         signing_algorithm=salt.crypt.PKCS1v15_SHA1,
   162|     ):
   163|         """
   164|         The server equivalent of ReqChannel.crypted_transfer_decode_dictentry
   165|         """
   166|         pubfn = os.path.join(self.opts["pki_dir"], "minions", target)
   167|         key = salt.crypt.Crypticle.generate_key_string()
   168|         pcrypt = salt.crypt.Crypticle(self.opts, key)
   169|         try:
   170|             pub = salt.crypt.PublicKey(pubfn)
   171|         except (ValueError, IndexError, TypeError):
   172|             return self.crypticle.dumps({})
   173|         except OSError:
   174|             log.error("AES key not found")
   175|             return {"error": "AES key not found"}
   176|         pret = {}
   177|         pret["key"] = pub.encrypt(
   178|             salt.utils.stringutils.to_bytes(key), encryption_algorithm
   179|         )
   180|         if ret is False:
   181|             ret = {}
   182|         if sign_messages:
   183|             if nonce is None:
   184|                 return {"error": "Nonce not included in request"}
   185|             tosign = salt.payload.dumps(
   186|                 {"key": pret["key"], "pillar": ret, "nonce": nonce}
   187|             )
   188|             master_pem_path = os.path.join(self.opts["pki_dir"], "master.pem")
   189|             signed_msg = {
   190|                 "data": tosign,
   191|                 "sig": salt.crypt.PrivateKey(master_pem_path).sign(
   192|                     tosign, algorithm=signing_algorithm
   193|                 ),
   194|             }
   195|             pret[dictkey] = pcrypt.dumps(signed_msg)
   196|         else:
   197|             pret[dictkey] = pcrypt.dumps(ret)
   198|         return pret
   199|     def _clear_signed(self, load, algorithm):
   200|         try:
   201|             master_pem_path = os.path.join(self.opts["pki_dir"], "master.pem")
   202|             tosign = salt.payload.dumps(load)
   203|             return {
   204|                 "enc": "clear",
   205|                 "load": tosign,
   206|                 "sig": salt.crypt.PrivateKey(master_pem_path).sign(
   207|                     tosign, algorithm=algorithm
   208|                 ),
   209|             }
   210|         except UnsupportedAlgorithm:
   211|             log.info(
   212|                 "Minion tried to authenticate with unsupported signing algorithm: %s",
   213|                 algorithm,
   214|             )
   215|             return {"enc": "clear", "load": {"ret": "bad sig algo"}}
   216|     def _update_aes(self):
   217|         """
   218|         Check to see if a fresh AES key is available and update the components
   219|         of the worker
   220|         """
   221|         import salt.master
   222|         if (
   223|             salt.master.SMaster.secrets["aes"]["secret"].value
   224|             != self.crypticle.key_string
   225|         ):
   226|             self.crypticle = salt.crypt.Crypticle(
   227|                 self.opts, salt.master.SMaster.secrets["aes"]["secret"].value
   228|             )
   229|             return True
   230|         return False
   231|     def _decode_payload(self, payload):
   232|         if (
   233|             not isinstance(payload, dict)
   234|             or "enc" not in payload
   235|             or "load" not in payload
   236|         ):
   237|             raise SaltDeserializationError("bad load received on socket!")
   238|         if payload["enc"] == "aes":
   239|             try:
   240|                 payload["load"] = self.crypticle.loads(payload["load"])
   241|             except salt.crypt.AuthenticationError:
   242|                 if not self._update_aes():
   243|                     raise
   244|                 payload["load"] = self.crypticle.loads(payload["load"])
   245|         return payload
   246|     def _auth(self, load, sign_messages=False):
   247|         """
   248|         Authenticate the client, use the sent public key to encrypt the AES key
   249|         which was generated at start up.
   250|         This method fires an event over the master event manager. The event is
   251|         tagged "auth" and returns a dict with information about the auth
   252|         event
   253|             - Verify that the key we are receiving matches the stored key
   254|             - Store the key if it is not there
   255|             - Make an RSA key with the pub key
   256|             - Encrypt the AES key as an encrypted salt.payload
   257|             - Package the return and return it
   258|         """
   259|         import salt.master
   260|         enc_algo = load.get("enc_algo", salt.crypt.OAEP_SHA1)
   261|         sig_algo = load.get("sig_algo", salt.crypt.PKCS1v15_SHA1)
   262|         if not salt.utils.verify.valid_id(self.opts, load["id"]):
   263|             log.info("Authentication request from invalid id %s", load["id"])
   264|             if sign_messages:
   265|                 return self._clear_signed(
   266|                     {"ret": False, "nonce": load["nonce"]}, sig_algo
   267|                 )
   268|             else:
   269|                 return {"enc": "clear", "load": {"ret": False}}
   270|         log.info("Authentication request from %s", load["id"])
   271|         if self.opts["max_minions"] > 0:
   272|             if self.cache_cli:
   273|                 minions = self.cache_cli.get_cached()
   274|             else:
   275|                 minions = self.ckminions.connected_ids()
   276|                 if len(minions) > 1000:
   277|                     log.info(
   278|                         "With large numbers of minions it is advised "
   279|                         "to enable the ConCache with 'con_cache: True' "
   280|                         "in the masters configuration file."
   281|                     )
   282|             if not len(minions) <= self.opts["max_minions"]:
   283|                 if load["id"] not in minions:
   284|                     log.info(
   285|                         "Too many minions connected (max_minions=%s). "
   286|                         "Rejecting connection from id %s",
   287|                         self.opts["max_minions"],
   288|                         load["id"],
   289|                     )
   290|                     eload = {
   291|                         "result": False,
   292|                         "act": "full",
   293|                         "id": load["id"],
   294|                         "pub": load["pub"],
   295|                     }
   296|                     if self.opts.get("auth_events") is True:
   297|                         self.event.fire_event(
   298|                             eload, salt.utils.event.tagify(prefix="auth")
   299|                         )
   300|                     if sign_messages:
   301|                         return self._clear_signed(
   302|                             {"ret": "full", "nonce": load["nonce"]}, sig_algo
   303|                         )
   304|                     else:
   305|                         return {"enc": "clear", "load": {"ret": "full"}}
   306|         auto_reject = self.auto_key.check_autoreject(load["id"])
   307|         auto_sign = self.auto_key.check_autosign(
   308|             load["id"], load.get("autosign_grains", None)
   309|         )
   310|         pubfn = os.path.join(self.opts["pki_dir"], "minions", load["id"])
   311|         pubfn_pend = os.path.join(self.opts["pki_dir"], "minions_pre", load["id"])
   312|         pubfn_rejected = os.path.join(
   313|             self.opts["pki_dir"], "minions_rejected", load["id"]
   314|         )
   315|         pubfn_denied = os.path.join(self.opts["pki_dir"], "minions_denied", load["id"])
   316|         if self.opts["open_mode"]:
   317|             pass
   318|         elif os.path.isfile(pubfn_rejected):
   319|             log.info(
   320|                 "Public key rejected for %s. Key is present in rejection key dir.",
   321|                 load["id"],
   322|             )
   323|             eload = {"result": False, "id": load["id"], "pub": load["pub"]}
   324|             if self.opts.get("auth_events") is True:
   325|                 self.event.fire_event(eload, salt.utils.event.tagify(prefix="auth"))
   326|             if sign_messages:
   327|                 return self._clear_signed(
   328|                     {"ret": False, "nonce": load["nonce"]}, sig_algo
   329|                 )
   330|             else:
   331|                 return {"enc": "clear", "load": {"ret": False}}
   332|         elif os.path.isfile(pubfn):
   333|             with salt.utils.files.fopen(pubfn, "r") as pubfn_handle:
   334|                 if not self.compare_keys(pubfn_handle.read(), load["pub"]):
   335|                     log.error(
   336|                         "Authentication attempt from %s failed, the public "
   337|                         "keys did not match. This may be an attempt to compromise "
   338|                         "the Salt cluster.",
   339|                         load["id"],
   340|                     )
   341|                     with salt.utils.files.fopen(pubfn_denied, "w+") as fp_:
   342|                         fp_.write(load["pub"])
   343|                     eload = {
   344|                         "result": False,
   345|                         "id": load["id"],
   346|                         "act": "denied",
   347|                         "pub": load["pub"],
   348|                     }
   349|                     if self.opts.get("auth_events") is True:
   350|                         self.event.fire_event(
   351|                             eload, salt.utils.event.tagify(prefix="auth")
   352|                         )
   353|                     if sign_messages:
   354|                         return self._clear_signed(
   355|                             {"ret": False, "nonce": load["nonce"]}, sig_algo
   356|                         )
   357|                     else:
   358|                         return {"enc": "clear", "load": {"ret": False}}
   359|         elif not os.path.isfile(pubfn_pend):
   360|             if os.path.isdir(pubfn_pend):
   361|                 log.info("New public key %s is a directory", load["id"])
   362|                 eload = {"result": False, "id": load["id"], "pub": load["pub"]}
   363|                 if self.opts.get("auth_events") is True:
   364|                     self.event.fire_event(eload, salt.utils.event.tagify(prefix="auth"))
   365|                 if sign_messages:
   366|                     return self._clear_signed(
   367|                         {"ret": False, "nonce": load["nonce"]}, sig_algo
   368|                     )
   369|                 else:
   370|                     return {"enc": "clear", "load": {"ret": False}}
   371|             if auto_reject:
   372|                 key_path = pubfn_rejected
   373|                 log.info(
   374|                     "New public key for %s rejected via autoreject_file", load["id"]
   375|                 )
   376|                 key_act = "reject"
   377|                 key_result = False
   378|             elif not auto_sign:
   379|                 key_path = pubfn_pend
   380|                 log.info("New public key for %s placed in pending", load["id"])
   381|                 key_act = "pend"
   382|                 key_result = True
   383|             else:
   384|                 key_path = None
   385|             if key_path is not None:
   386|                 with salt.utils.files.fopen(key_path, "w+") as fp_:
   387|                     fp_.write(load["pub"])
   388|                 eload = {
   389|                     "result": key_result,
   390|                     "act": key_act,
   391|                     "id": load["id"],
   392|                     "pub": load["pub"],
   393|                 }
   394|                 if self.opts.get("auth_events") is True:
   395|                     self.event.fire_event(eload, salt.utils.event.tagify(prefix="auth"))
   396|                 if sign_messages:
   397|                     return self._clear_signed(
   398|                         {"ret": key_result, "nonce": load["nonce"]},
   399|                         sig_algo,
   400|                     )
   401|                 else:
   402|                     return {"enc": "clear", "load": {"ret": key_result}}
   403|         elif os.path.isfile(pubfn_pend):
   404|             if auto_reject:
   405|                 try:
   406|                     shutil.move(pubfn_pend, pubfn_rejected)
   407|                 except OSError:
   408|                     pass
   409|                 log.info(
   410|                     "Pending public key for %s rejected via autoreject_file",
   411|                     load["id"],
   412|                 )
   413|                 eload = {
   414|                     "result": False,
   415|                     "act": "reject",
   416|                     "id": load["id"],
   417|                     "pub": load["pub"],
   418|                 }
   419|                 if self.opts.get("auth_events") is True:
   420|                     self.event.fire_event(eload, salt.utils.event.tagify(prefix="auth"))
   421|                 if sign_messages:
   422|                     return self._clear_signed(
   423|                         {"ret": False, "nonce": load["nonce"]}, sig_algo
   424|                     )
   425|                 else:
   426|                     return {"enc": "clear", "load": {"ret": False}}
   427|             elif not auto_sign:
   428|                 with salt.utils.files.fopen(pubfn_pend, "r") as pubfn_handle:
   429|                     if not self.compare_keys(pubfn_handle.read(), load["pub"]):
   430|                         log.error(
   431|                             "Authentication attempt from %s failed, the public "
   432|                             "key in pending did not match. This may be an "
   433|                             "attempt to compromise the Salt cluster.",
   434|                             load["id"],
   435|                         )
   436|                         with salt.utils.files.fopen(pubfn_denied, "w+") as fp_:
   437|                             fp_.write(load["pub"])
   438|                         eload = {
   439|                             "result": False,
   440|                             "id": load["id"],
   441|                             "act": "denied",
   442|                             "pub": load["pub"],
   443|                         }
   444|                         if self.opts.get("auth_events") is True:
   445|                             self.event.fire_event(
   446|                                 eload, salt.utils.event.tagify(prefix="auth")
   447|                             )
   448|                         if sign_messages:
   449|                             return self._clear_signed(
   450|                                 {"ret": False, "nonce": load["nonce"]}, sig_algo
   451|                             )
   452|                         else:
   453|                             return {"enc": "clear", "load": {"ret": False}}
   454|                     else:
   455|                         log.info(
   456|                             "Authentication failed from host %s, the key is in "
   457|                             "pending and needs to be accepted with salt-key "
   458|                             "-a %s",
   459|                             load["id"],
   460|                             load["id"],
   461|                         )
   462|                         eload = {
   463|                             "result": True,
   464|                             "act": "pend",
   465|                             "id": load["id"],
   466|                             "pub": load["pub"],
   467|                         }
   468|                         if self.opts.get("auth_events") is True:
   469|                             self.event.fire_event(
   470|                                 eload, salt.utils.event.tagify(prefix="auth")
   471|                             )
   472|                         if sign_messages:
   473|                             return self._clear_signed(
   474|                                 {"ret": True, "nonce": load["nonce"]}, sig_algo
   475|                             )
   476|                         else:
   477|                             return {"enc": "clear", "load": {"ret": True}}
   478|             else:
   479|                 with salt.utils.files.fopen(pubfn_pend, "r") as pubfn_handle:
   480|                     if not self.compare_keys(pubfn_handle.read(), load["pub"]):
   481|                         log.error(
   482|                             "Authentication attempt from %s failed, the public "
   483|                             "keys in pending did not match. This may be an "
   484|                             "attempt to compromise the Salt cluster.",
   485|                             load["id"],
   486|                         )
   487|                         with salt.utils.files.fopen(pubfn_denied, "w+") as fp_:
   488|                             fp_.write(load["pub"])
   489|                         eload = {"result": False, "id": load["id"], "pub": load["pub"]}
   490|                         if self.opts.get("auth_events") is True:
   491|                             self.event.fire_event(
   492|                                 eload, salt.utils.event.tagify(prefix="auth")
   493|                             )
   494|                         if sign_messages:
   495|                             return self._clear_signed(
   496|                                 {"ret": False, "nonce": load["nonce"]}, sig_algo
   497|                             )
   498|                         else:
   499|                             return {"enc": "clear", "load": {"ret": False}}
   500|                     else:
   501|                         os.remove(pubfn_pend)
   502|         else:
   503|             log.warning("Unaccounted for authentication failure")
   504|             eload = {"result": False, "id": load["id"], "pub": load["pub"]}
   505|             if self.opts.get("auth_events") is True:
   506|                 self.event.fire_event(eload, salt.utils.event.tagify(prefix="auth"))
   507|             if sign_messages:
   508|                 return self._clear_signed(
   509|                     {"ret": False, "nonce": load["nonce"]}, sig_algo
   510|                 )
   511|             else:
   512|                 return {"enc": "clear", "load": {"ret": False}}
   513|         log.info("Authentication accepted from %s", load["id"])
   514|         if not os.path.isfile(pubfn) and not self.opts["open_mode"]:
   515|             with salt.utils.files.fopen(pubfn, "w+") as fp_:
   516|                 fp_.write(load["pub"])
   517|         elif self.opts["open_mode"]:
   518|             disk_key = ""
   519|             if os.path.isfile(pubfn):
   520|                 with salt.utils.files.fopen(pubfn, "r") as fp_:
   521|                     disk_key = fp_.read()
   522|             if load["pub"] and load["pub"] != disk_key:
   523|                 log.debug("Host key change detected in open mode.")
   524|                 with salt.utils.files.fopen(pubfn, "w+") as fp_:
   525|                     fp_.write(load["pub"])
   526|             elif not load["pub"]:
   527|                 log.error("Public key is empty: %s", load["id"])
   528|                 if sign_messages:
   529|                     return self._clear_signed(
   530|                         {"ret": False, "nonce": load["nonce"]}, sig_algo
   531|                     )
   532|                 else:
   533|                     return {"enc": "clear", "load": {"ret": False}}
   534|         pub = None
   535|         if self.cache_cli:
   536|             self.cache_cli.put_cache([load["id"]])
   537|         try:
   538|             pub = salt.crypt.PublicKey(pubfn)
   539|         except salt.crypt.InvalidKeyError as err:
   540|             log.error('Corrupt public key "%s": %s', pubfn, err)
   541|             if sign_messages:
   542|                 return self._clear_signed(
   543|                     {"ret": False, "nonce": load["nonce"]}, sig_algo
   544|                 )
   545|             else:
   546|                 return {"enc": "clear", "load": {"ret": False}}
   547|         ret = {
   548|             "enc": "pub",
   549|             "pub_key": self.master_key.get_pub_str(),
   550|             "publish_port": self.opts["publish_port"],
   551|         }
   552|         if self.opts["master_sign_pubkey"]:
   553|             if self.master_key.pubkey_signature():
   554|                 log.debug("Adding pubkey signature to auth-reply")
   555|                 log.debug(self.master_key.pubkey_signature())
   556|                 ret.update({"pub_sig": self.master_key.pubkey_signature()})
   557|             else:
   558|                 key_pass = salt.utils.sdb.sdb_get(
   559|                     self.opts["signing_key_pass"], self.opts
   560|                 )
   561|                 log.debug("Signing master public key before sending")
   562|                 pub_sign = salt.crypt.sign_message(
   563|                     self.master_key.get_sign_paths()[1],
   564|                     ret["pub_key"],
   565|                     key_pass,
   566|                     algorithm=sig_algo,
   567|                 )
   568|                 ret.update({"pub_sig": binascii.b2a_base64(pub_sign)})
   569|         if self.opts["auth_mode"] >= 2:
   570|             if "token" in load:
   571|                 try:
   572|                     mtoken = self.master_key.key.decrypt(load["token"], enc_algo)
   573|                     aes = "{}_|-{}".format(
   574|                         salt.master.SMaster.secrets["aes"]["secret"].value, mtoken
   575|                     )
   576|                 except UnsupportedAlgorithm as exc:
   577|                     log.info(
   578|                         "Minion %s tried to authenticate with unsupported encryption algorithm: %s",
   579|                         load["id"],
   580|                         enc_algo,
   581|                     )
   582|                     return {"enc": "clear", "load": {"ret": "bad enc algo"}}
   583|                 except Exception as exc:  # pylint: disable=broad-except
   584|                     log.warning("Token failed to decrypt %s", exc)
   585|             else:
   586|                 aes = salt.master.SMaster.secrets["aes"]["secret"].value
   587|             ret["aes"] = pub.encrypt(aes, enc_algo)
   588|         else:
   589|             if "token" in load:
   590|                 try:
   591|                     mtoken = self.master_key.key.decrypt(load["token"], enc_algo)
   592|                     ret["token"] = pub.encrypt(mtoken, enc_algo)
   593|                 except UnsupportedAlgorithm as exc:
   594|                     log.info(
   595|                         "Minion %s tried to authenticate with unsupported encryption algorithm: %s",
   596|                         load["id"],
   597|                         enc_algo,
   598|                     )
   599|                     return {"enc": "clear", "load": {"ret": "bad enc algo"}}
   600|                 except Exception as exc:  # pylint: disable=broad-except
   601|                     log.warning("Token failed to decrypt: %r", exc)
   602|             aes = salt.master.SMaster.secrets["aes"]["secret"].value
   603|             ret["aes"] = pub.encrypt(aes, enc_algo)
   604|         digest = salt.utils.stringutils.to_bytes(hashlib.sha256(aes).hexdigest())
   605|         ret["sig"] = salt.crypt.private_encrypt(self.master_key.key, digest)
   606|         eload = {"result": True, "act": "accept", "id": load["id"], "pub": load["pub"]}
   607|         if self.opts.get("auth_events") is True:
   608|             self.event.fire_event(eload, salt.utils.event.tagify(prefix="auth"))
   609|         if sign_messages:
   610|             ret["nonce"] = load["nonce"]
   611|             return self._clear_signed(ret, sig_algo)
   612|         return ret
   613|     def close(self):
   614|         self.transport.close()
   615|         if self.event is not None:
   616|             self.event.destroy()
   617| class PubServerChannel:
   618|     """
   619|     Factory class to create subscription channels to the master's Publisher
   620|     """
   621|     @classmethod
   622|     def factory(cls, opts, **kwargs):
   623|         if "master_uri" not in opts and "master_uri" in kwargs:
   624|             opts["master_uri"] = kwargs["master_uri"]
   625|         presence_events = False
   626|         if opts.get("presence_events", False):
   627|             tcp_only = True
   628|             for transport, _ in salt.utils.channel.iter_transport_opts(opts):
   629|                 if transport != "tcp":
   630|                     tcp_only = False
   631|             if tcp_only:
   632|                 presence_events = True
   633|         transport = salt.transport.publish_server(opts, **kwargs)
   634|         return cls(opts, transport, presence_events=presence_events)
   635|     def __init__(self, opts, transport, presence_events=False):
   636|         self.opts = opts
   637|         self.ckminions = salt.utils.minions.CkMinions(self.opts)
   638|         self.transport = transport
   639|         self.aes_funcs = salt.master.AESFuncs(self.opts)
   640|         self.present = {}
   641|         self.presence_events = presence_events
   642|         self.event = salt.utils.event.get_event("master", opts=self.opts, listen=False)
   643|     def __getstate__(self):
   644|         return {
   645|             "opts": self.opts,
   646|             "transport": self.transport,
   647|             "presence_events": self.presence_events,
   648|         }
   649|     def __setstate__(self, state):
   650|         self.opts = state["opts"]
   651|         self.state = state["presence_events"]
   652|         self.transport = state["transport"]
   653|         self.event = salt.utils.event.get_event("master", opts=self.opts, listen=False)
   654|         self.ckminions = salt.utils.minions.CkMinions(self.opts)
   655|         self.present = {}
   656|     def close(self):
   657|         self.transport.close()
   658|         if self.event is not None:
   659|             self.event.destroy()
   660|             self.event = None
   661|         if self.aes_funcs is not None:
   662|             self.aes_funcs.destroy()
   663|             self.aes_funcs = None
   664|     def pre_fork(self, process_manager, kwargs=None):
   665|         """
   666|         Do anything necessary pre-fork. Since this is on the master side this will
   667|         primarily be used to create IPC channels and create our daemon process to
   668|         do the actual publishing
   669|         :param func process_manager: A ProcessManager, from salt.utils.process.ProcessManager
   670|         """
   671|         if hasattr(self.transport, "publish_daemon"):
   672|             process_manager.add_process(self._publish_daemon, kwargs=kwargs)
   673|     def _publish_daemon(self, **kwargs):
   674|         if self.opts["pub_server_niceness"] and not salt.utils.platform.is_windows():
   675|             log.info(
   676|                 "setting Publish daemon niceness to %i",
   677|                 self.opts["pub_server_niceness"],
   678|             )
   679|             os.nice(self.opts["pub_server_niceness"])
   680|         secrets = kwargs.get("secrets", None)
   681|         if secrets is not None:
   682|             salt.master.SMaster.secrets = secrets
   683|         self.master_key = salt.crypt.MasterKeys(self.opts)
   684|         self.transport.publish_daemon(
   685|             self.publish_payload, self.presence_callback, self.remove_presence_callback
   686|         )
   687|     def presence_callback(self, subscriber, msg):
   688|         if msg["enc"] != "aes":
   689|             return
   690|         crypticle = salt.crypt.Crypticle(
   691|             self.opts, salt.master.SMaster.secrets["aes"]["secret"].value
   692|         )
   693|         load = crypticle.loads(msg["load"])
   694|         load = salt.transport.frame.decode_embedded_strs(load)
   695|         if not self.aes_funcs.verify_minion(load["id"], load["tok"]):
   696|             return
   697|         subscriber.id_ = load["id"]
   698|         self._add_client_present(subscriber)
   699|     def remove_presence_callback(self, subscriber):
   700|         self._remove_client_present(subscriber)
   701|     def _add_client_present(self, client):
   702|         id_ = client.id_
   703|         if id_ in self.present:
   704|             clients = self.present[id_]
   705|             clients.add(client)
   706|         else:
   707|             self.present[id_] = {client}
   708|             if self.presence_events:
   709|                 data = {"new": [id_], "lost": []}
   710|                 self.event.fire_event(
   711|                     data, salt.utils.event.tagify("change", "presence")
   712|                 )

# --- HUNK 2: Lines 716-788 ---
   716|                 )
   717|     def _remove_client_present(self, client):
   718|         id_ = client.id_
   719|         if id_ is None or id_ not in self.present:
   720|             return
   721|         clients = self.present[id_]
   722|         if client not in clients:
   723|             return
   724|         clients.remove(client)
   725|         if len(clients) == 0:
   726|             del self.present[id_]
   727|             if self.presence_events:
   728|                 data = {"new": [], "lost": [id_]}
   729|                 self.event.fire_event(
   730|                     data, salt.utils.event.tagify("change", "presence")
   731|                 )
   732|                 data = {"present": list(self.present.keys())}
   733|                 self.event.fire_event(
   734|                     data, salt.utils.event.tagify("present", "presence")
   735|                 )
   736|     @salt.ext.tornado.gen.coroutine
   737|     def publish_payload(self, load, *args):
   738|         unpacked_package = self.wrap_payload(load)
   739|         try:
   740|             payload = salt.payload.loads(unpacked_package["payload"])
   741|         except KeyError:
   742|             log.error("Invalid package %r", unpacked_package)
   743|             raise
   744|         if "topic_lst" in unpacked_package:
   745|             topic_list = unpacked_package["topic_lst"]
   746|             ret = yield self.transport.publish_payload(payload, topic_list)
   747|         else:
   748|             ret = yield self.transport.publish_payload(payload)
   749|         raise salt.ext.tornado.gen.Return(ret)
   750|     def wrap_payload(self, load):
   751|         payload = {"enc": "aes"}
   752|         load["serial"] = salt.master.SMaster.get_serial()
   753|         crypticle = salt.crypt.Crypticle(
   754|             self.opts, salt.master.SMaster.secrets["aes"]["secret"].value
   755|         )
   756|         payload["load"] = crypticle.dumps(load)
   757|         if self.opts["sign_pub_messages"]:
   758|             master_pem_path = os.path.join(self.opts["pki_dir"], "master.pem")
   759|             log.debug("Signing data packet")
   760|             payload["sig_algo"] = self.opts["publish_signing_algorithm"]
   761|             payload["sig"] = salt.crypt.PrivateKey(
   762|                 self.master_key.rsa_path,
   763|             ).sign(payload["load"], self.opts["publish_signing_algorithm"])
   764|         int_payload = {"payload": salt.payload.dumps(payload)}
   765|         match_targets = ["pcre", "glob", "list"]
   766|         if self.transport.topic_support and load["tgt_type"] in match_targets:
   767|             if load["tgt_type"] == "list":
   768|                 int_payload["topic_lst"] = load["tgt"]
   769|             if isinstance(load["tgt"], str):
   770|                 _res = self.ckminions.check_minions(
   771|                     load["tgt"], tgt_type=load["tgt_type"]
   772|                 )
   773|                 match_ids = _res["minions"]
   774|                 log.debug("Publish Side Match: %s", match_ids)
   775|                 int_payload["topic_lst"] = match_ids
   776|             else:
   777|                 int_payload["topic_lst"] = load["tgt"]
   778|         return int_payload
   779|     def publish(self, load):
   780|         """
   781|         Publish "load" to minions
   782|         """
   783|         log.debug(
   784|             "Sending payload to publish daemon. jid=%s load=%s",
   785|             load.get("jid", None),
   786|             repr(load)[:40],
   787|         )
   788|         self.transport.publish(load)


# ====================================================================
# FILE: salt/cli/daemons.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 77-149 ---
    77|         Log environment failure for the daemon and exit with the error code.
    78|         :param error:
    79|         :return:
    80|         """
    81|         log.exception(
    82|             "Failed to create environment for %s: %s",
    83|             self.__class__.__name__,
    84|             get_error_message(error),
    85|         )
    86|         self.shutdown(error)
    87| class Master(
    88|     salt.utils.parsers.MasterOptionParser, DaemonsMixin
    89| ):  # pylint: disable=no-init
    90|     """
    91|     Creates a master server
    92|     """
    93|     def _handle_signals(self, signum, sigframe):
    94|         if hasattr(self.master, "process_manager"):
    95|             self.master.process_manager._handle_signals(signum, sigframe)
    96|         super()._handle_signals(signum, sigframe)
    97|     def prepare(self):
    98|         """
    99|         Run the preparation sequence required to start a salt master server.
   100|         If sub-classed, don't **ever** forget to run:
   101|             super(YourSubClass, self).prepare()
   102|         """
   103|         super().prepare()
   104|         try:
   105|             if self.config["verify_env"]:
   106|                 v_dirs = [
   107|                     self.config["pki_dir"],
   108|                     os.path.join(self.config["pki_dir"], "minions"),
   109|                     os.path.join(self.config["pki_dir"], "minions_pre"),
   110|                     os.path.join(self.config["pki_dir"], "minions_denied"),
   111|                     os.path.join(self.config["pki_dir"], "minions_autosign"),
   112|                     os.path.join(self.config["pki_dir"], "minions_rejected"),
   113|                     self.config["cachedir"],
   114|                     os.path.join(self.config["cachedir"], "jobs"),
   115|                     os.path.join(self.config["cachedir"], "proc"),
   116|                     self.config["sock_dir"],
   117|                     self.config["token_dir"],
   118|                     self.config["syndic_dir"],
   119|                     self.config["sqlite_queue_dir"],
   120|                 ]
   121|                 verify_env(
   122|                     v_dirs,
   123|                     self.config["user"],
   124|                     permissive=self.config["permissive_pki_access"],
   125|                     root_dir=self.config["root_dir"],
   126|                     pki_dir=self.config["pki_dir"],
   127|                 )
   128|                 for syndic_file in os.listdir(self.config["syndic_dir"]):
   129|                     os.remove(os.path.join(self.config["syndic_dir"], syndic_file))
   130|         except OSError as error:
   131|             self.environment_failure(error)
   132|         self.action_log_info("Setting up")
   133|         if not verify_socket(
   134|             self.config["interface"],
   135|             self.config["publish_port"],
   136|             self.config["ret_port"],
   137|         ):
   138|             self.shutdown(4, "The ports are not available to bind")
   139|         self.config["interface"] = ip_bracket(self.config["interface"])
   140|         migrations.migrate_paths(self.config)
   141|         import salt.master
   142|         self.master = salt.master.Master(self.config)
   143|         self.daemonize_if_required()
   144|         self.set_pidfile()
   145|         salt.utils.process.notify_systemd()
   146|     def start(self):
   147|         """
   148|         Start the actual master.
   149|         If sub-classed, don't **ever** forget to run:


# ====================================================================
# FILE: salt/cli/salt.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 153-194 ---
   153|                         ret.update(full_ret)
   154|                     except KeyError:
   155|                         errors.append(full_ret)
   156|             if self.config["cli_summary"] is True:
   157|                 if self.config["fun"] != "sys.doc":
   158|                     if self.options.output is None:
   159|                         self._print_returns_summary(ret)
   160|                         self._print_errors_summary(errors)
   161|             if not all(
   162|                 exit_code == salt.defaults.exitcodes.EX_OK for exit_code in retcodes
   163|             ):
   164|                 sys.stderr.write("ERROR: Minions returned with non-zero exit code\n")
   165|                 sys.exit(salt.defaults.exitcodes.EX_GENERIC)
   166|         except (
   167|             AuthenticationError,
   168|             AuthorizationError,
   169|             SaltInvocationError,
   170|             EauthAuthenticationError,
   171|             SaltClientError,
   172|         ) as exc:
   173|             ret = str(exc)
   174|             self._output_ret(ret, "", retcode=1)
   175|         finally:
   176|             self.local_client.destroy()
   177|     def _preview_target(self):
   178|         """
   179|         Return a list of minions from a given target
   180|         """
   181|         return self.local_client.gather_minions(
   182|             self.config["tgt"], self.selected_target_option or "glob"
   183|         )
   184|     def _run_batch(self):
   185|         import salt.cli.batch
   186|         eauth = {}
   187|         if "token" in self.config:
   188|             eauth["token"] = self.config["token"]
   189|         if "token" not in eauth and self.options.eauth:
   190|             import salt.auth
   191|             resolver = salt.auth.Resolver(self.config)
   192|             res = resolver.cli(self.options.eauth)
   193|             if self.options.mktoken and res:
   194|                 tok = resolver.token_cli(self.options.eauth, res)


# ====================================================================
# FILE: salt/client/__init__.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 1-40 ---
     1| """
     2| The client module is used to create a client connection to the publisher
     3| The data structure needs to be:
     4|     {'enc': 'clear',
     5|      'load': {'fun': '<mod.callable>',
     6|               'arg':, ('arg1', 'arg2', ...),
     7|               'tgt': '<glob or id>',
     8|               'key': '<read in the key file>'}
     9| """
    10| import logging
    11| import os
    12| import random
    13| import sys
    14| import time
    15| from datetime import datetime
    16| import salt.cache
    17| import salt.channel.client
    18| import salt.config
    19| import salt.defaults.exitcodes
    20| import salt.ext.tornado.gen
    21| import salt.loader
    22| import salt.payload
    23| import salt.syspaths as syspaths
    24| import salt.utils.args
    25| import salt.utils.event
    26| import salt.utils.files
    27| import salt.utils.jid
    28| import salt.utils.minions
    29| import salt.utils.network
    30| import salt.utils.platform
    31| import salt.utils.stringutils
    32| import salt.utils.user
    33| import salt.utils.verify
    34| from salt.exceptions import (
    35|     AuthenticationError,
    36|     AuthorizationError,
    37|     EauthAuthenticationError,
    38|     PublishError,
    39|     SaltClientError,
    40|     SaltInvocationError,

# --- HUNK 2: Lines 308-348 ---
   308|                 timeout=self._get_timeout(timeout),
   309|                 listen=listen,
   310|                 **kwargs,
   311|             )
   312|         except SaltClientError:
   313|             raise SaltClientError(
   314|                 "The salt master could not be contacted. Is master running?"
   315|             )
   316|         except AuthenticationError as err:
   317|             raise
   318|         except AuthorizationError as err:
   319|             raise
   320|         except Exception as general_exception:  # pylint: disable=broad-except
   321|             raise SaltClientError(general_exception)
   322|         return self._check_pub_data(pub_data, listen=listen)
   323|     def gather_minions(self, tgt, expr_form):
   324|         _res = salt.utils.minions.CkMinions(self.opts).check_minions(
   325|             tgt, tgt_type=expr_form
   326|         )
   327|         return _res["minions"]
   328|     @salt.ext.tornado.gen.coroutine
   329|     def run_job_async(
   330|         self,
   331|         tgt,
   332|         fun,
   333|         arg=(),
   334|         tgt_type="glob",
   335|         ret="",
   336|         timeout=None,
   337|         jid="",
   338|         kwarg=None,
   339|         listen=True,
   340|         io_loop=None,
   341|         **kwargs,
   342|     ):
   343|         """
   344|         Asynchronously send a command to connected minions
   345|         Prep the job directory and publish a command to any targeted minions.
   346|         :return: A dictionary of (validated) ``pub_data`` or an empty
   347|             dictionary on failure. The ``pub_data`` contains the job ID and a
   348|             list of all minions that are expected to return data.

# --- HUNK 3: Lines 357-397 ---
   357|                 fun,
   358|                 arg,
   359|                 tgt_type,
   360|                 ret,
   361|                 jid=jid,
   362|                 timeout=self._get_timeout(timeout),
   363|                 io_loop=io_loop,
   364|                 listen=listen,
   365|                 **kwargs,
   366|             )
   367|         except SaltClientError:
   368|             raise SaltClientError(
   369|                 "The salt master could not be contacted. Is master running?"
   370|             )
   371|         except AuthenticationError as err:
   372|             raise AuthenticationError(err)
   373|         except AuthorizationError as err:
   374|             raise AuthorizationError(err)
   375|         except Exception as general_exception:  # pylint: disable=broad-except
   376|             raise SaltClientError(general_exception)
   377|         raise salt.ext.tornado.gen.Return(self._check_pub_data(pub_data, listen=listen))
   378|     def cmd_async(
   379|         self, tgt, fun, arg=(), tgt_type="glob", ret="", jid="", kwarg=None, **kwargs
   380|     ):
   381|         """
   382|         Asynchronously send a command to connected minions
   383|         The function signature is the same as :py:meth:`cmd` with the
   384|         following exceptions.
   385|         :returns: A job ID or 0 on failure.
   386|         .. code-block:: python
   387|             >>> local.cmd_async('*', 'test.sleep', [300])
   388|             '20131219215921857715'
   389|         """
   390|         pub_data = self.run_job(
   391|             tgt, fun, arg, tgt_type, ret, jid=jid, kwarg=kwarg, listen=False, **kwargs
   392|         )
   393|         try:
   394|             return pub_data["jid"]
   395|         except KeyError:
   396|             return 0
   397|     def cmd_subset(

# --- HUNK 4: Lines 1032-1072 ---
  1032|                 if "jid" not in jinfo:
  1033|                     jinfo_iter = []
  1034|                 else:
  1035|                     jinfo_iter = self.get_returns_no_block(
  1036|                         "salt/job/{}".format(jinfo["jid"])
  1037|                     )
  1038|                 timeout_at = time.time() + gather_job_timeout
  1039|                 if self.opts["order_masters"]:
  1040|                     timeout_at += self.opts.get("syndic_wait", 1)
  1041|             for raw in jinfo_iter:
  1042|                 if raw is None:
  1043|                     break
  1044|                 try:
  1045|                     if raw["data"]["retcode"] > 0:
  1046|                         log.error(
  1047|                             "saltutil returning errors on minion %s", raw["data"]["id"]
  1048|                         )
  1049|                         minions.remove(raw["data"]["id"])
  1050|                         break
  1051|                 except KeyError as exc:
  1052|                     missing_key = str(exc).strip("'\"")
  1053|                     if missing_key == "retcode":
  1054|                         log.debug("retcode missing from client return")
  1055|                     else:
  1056|                         log.debug(
  1057|                             "Passing on saltutil error. Key '%s' missing "
  1058|                             "from client return. This may be an error in "
  1059|                             "the client.",
  1060|                             missing_key,
  1061|                         )
  1062|                 open_jids.add(jinfo["jid"])
  1063|                 if "minions" in raw.get("data", {}):
  1064|                     minions.update(raw["data"]["minions"])
  1065|                     continue
  1066|                 if "syndic" in raw.get("data", {}):
  1067|                     minions.update(raw["syndic"])
  1068|                     continue
  1069|                 if "return" not in raw.get("data", {}):
  1070|                     continue
  1071|                 if raw["data"]["return"] == {}:
  1072|                     continue

# --- HUNK 5: Lines 1554-1594 ---
  1554|             if not payload:
  1555|                 key = self.__read_master_key()
  1556|                 if key == self.key:
  1557|                     return payload
  1558|                 self.key = key
  1559|                 payload_kwargs["key"] = self.key
  1560|                 payload = channel.send(payload_kwargs)
  1561|             error = payload.pop("error", None)
  1562|             if error is not None:
  1563|                 if isinstance(error, dict):
  1564|                     err_name = error.get("name", "")
  1565|                     err_msg = error.get("message", "")
  1566|                     if err_name == "AuthenticationError":
  1567|                         raise AuthenticationError(err_msg)
  1568|                     elif err_name == "AuthorizationError":
  1569|                         raise AuthorizationError(err_msg)
  1570|                 raise PublishError(error)
  1571|             if not payload:
  1572|                 return payload
  1573|         return {"jid": payload["load"]["jid"], "minions": payload["load"]["minions"]}
  1574|     @salt.ext.tornado.gen.coroutine
  1575|     def pub_async(
  1576|         self,
  1577|         tgt,
  1578|         fun,
  1579|         arg=(),
  1580|         tgt_type="glob",
  1581|         ret="",
  1582|         jid="",
  1583|         timeout=5,
  1584|         io_loop=None,
  1585|         listen=True,
  1586|         **kwargs,
  1587|     ):
  1588|         """
  1589|         Take the required arguments and publish the given command.
  1590|         Arguments:
  1591|             tgt:
  1592|                 The tgt is a regex or a glob used to match up the ids on
  1593|                 the minions. Salt works by always publishing every command
  1594|                 to all of the minions and then the minions determine if

# --- HUNK 6: Lines 1626-1682 ---
  1626|         with salt.channel.client.AsyncReqChannel.factory(
  1627|             self.opts, io_loop=io_loop, crypt="clear", master_uri=master_uri
  1628|         ) as channel:
  1629|             try:
  1630|                 if listen and not self.event.connect_pub(timeout=timeout):
  1631|                     raise SaltReqTimeoutError()
  1632|                 payload = yield channel.send(payload_kwargs, timeout=timeout)
  1633|             except SaltReqTimeoutError:
  1634|                 raise SaltReqTimeoutError(
  1635|                     "Salt request timed out. The master is not responding. You "
  1636|                     "may need to run your command with `--async` in order to "
  1637|                     "bypass the congested event bus. With `--async`, the CLI tool "
  1638|                     "will print the job id (jid) and exit immediately without "
  1639|                     "listening for responses. You can then use "
  1640|                     "`salt-run jobs.lookup_jid` to look up the results of the job "
  1641|                     "in the job cache later."
  1642|                 )
  1643|             if not payload:
  1644|                 key = self.__read_master_key()
  1645|                 if key == self.key:
  1646|                     raise salt.ext.tornado.gen.Return(payload)
  1647|                 self.key = key
  1648|                 payload_kwargs["key"] = self.key
  1649|                 payload = yield channel.send(payload_kwargs)
  1650|             error = payload.pop("error", None)
  1651|             if error is not None:
  1652|                 if isinstance(error, dict):
  1653|                     err_name = error.get("name", "")
  1654|                     err_msg = error.get("message", "")
  1655|                     if err_name == "AuthenticationError":
  1656|                         raise AuthenticationError(err_msg)
  1657|                     elif err_name == "AuthorizationError":
  1658|                         raise AuthorizationError(err_msg)
  1659|                 raise PublishError(error)
  1660|             if not payload:
  1661|                 raise salt.ext.tornado.gen.Return(payload)
  1662|         raise salt.ext.tornado.gen.Return(
  1663|             {"jid": payload["load"]["jid"], "minions": payload["load"]["minions"]}
  1664|         )
  1665|     def __del__(self):
  1666|         self.destroy()
  1667|     def _clean_up_subscriptions(self, job_id):
  1668|         if self.opts.get("order_masters"):
  1669|             self.event.unsubscribe(f"syndic/.*/{job_id}", "regex")
  1670|         self.event.unsubscribe(f"salt/job/{job_id}")
  1671|     def destroy(self):
  1672|         if self.event is not None:
  1673|             self.event.destroy()
  1674|             self.event = None
  1675|     def __enter__(self):
  1676|         return self
  1677|     def __exit__(self, *args):
  1678|         self.destroy()
  1679| class FunctionWrapper(dict):
  1680|     """
  1681|     Create a function wrapper that looks like the functions dict on the minion
  1682|     but invoked commands on the minion via a LocalClient.


# ====================================================================
# FILE: salt/client/mixins.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-35 ---
     1| """
     2| A collection of mixins useful for the various *Client interfaces
     3| """
     4| import copy
     5| import fnmatch
     6| import logging
     7| import os
     8| import signal
     9| import traceback
    10| import weakref
    11| from collections.abc import Mapping, MutableMapping
    12| import salt._logging
    13| import salt.channel.client
    14| import salt.exceptions
    15| import salt.ext.tornado.stack_context
    16| import salt.minion
    17| import salt.output
    18| import salt.utils.args
    19| import salt.utils.doc
    20| import salt.utils.error
    21| import salt.utils.event
    22| import salt.utils.jid
    23| import salt.utils.job
    24| import salt.utils.lazy
    25| import salt.utils.platform
    26| import salt.utils.process
    27| import salt.utils.state
    28| import salt.utils.user
    29| import salt.utils.versions
    30| log = logging.getLogger(__name__)
    31| CLIENT_INTERNAL_KEYWORDS = frozenset(
    32|     [
    33|         "client",
    34|         "cmd",
    35|         "eauth",

# --- HUNK 2: Lines 283-342 ---
   283|                         continue
   284|                     mod, _ = mod_name.split(".", 1)
   285|                     if mod in completed_funcs:
   286|                         continue
   287|                     completed_funcs.append(mod)
   288|                     for global_key, value in func_globals.items():
   289|                         self.functions[mod_name].__globals__[global_key] = value
   290|                 if "arg" in low and "kwarg" in low:
   291|                     args = low["arg"]
   292|                     kwargs = low["kwarg"]
   293|                 else:
   294|                     f_call = salt.utils.args.format_call(
   295|                         self.functions[fun],
   296|                         low,
   297|                         expected_extra_kws=CLIENT_INTERNAL_KEYWORDS,
   298|                     )
   299|                     args = f_call.get("args", ())
   300|                     kwargs = f_call.get("kwargs", {})
   301|                 data["fun_args"] = list(args) + ([kwargs] if kwargs else [])
   302|                 func_globals["__jid_event__"].fire_event(data, "new")
   303|                 with salt.ext.tornado.stack_context.StackContext(
   304|                     self.functions.context_dict.clone
   305|                 ):
   306|                     func = self.functions[fun]
   307|                     try:
   308|                         data["return"] = func(*args, **kwargs)
   309|                     except TypeError as exc:
   310|                         data["return"] = (
   311|                             "\nPassed invalid arguments: {}\n\nUsage:\n{}".format(
   312|                                 exc, func.__doc__
   313|                             )
   314|                         )
   315|                     try:
   316|                         data["success"] = self.context.get("retcode", 0) == 0
   317|                     except AttributeError:
   318|                         data["success"] = True
   319|                     if isinstance(data["return"], dict) and "data" in data["return"]:
   320|                         data["success"] = salt.utils.state.check_result(
   321|                             data["return"]["data"]
   322|                         )
   323|             except (Exception, SystemExit) as ex:  # pylint: disable=broad-except
   324|                 if isinstance(ex, salt.exceptions.NotImplemented):
   325|                     data["return"] = str(ex)
   326|                 else:
   327|                     data["return"] = "Exception occurred in {} {}: {}".format(
   328|                         self.client,
   329|                         fun,
   330|                         traceback.format_exc(),
   331|                     )
   332|                 data["success"] = False
   333|                 data["retcode"] = 1
   334|             if self.store_job:
   335|                 try:
   336|                     salt.utils.job.store_job(
   337|                         self.opts,
   338|                         {
   339|                             "id": self.opts["id"],
   340|                             "tgt": self.opts["id"],
   341|                             "jid": data["jid"],
   342|                             "return": data,


# ====================================================================
# FILE: salt/client/ssh/__init__.py
# Total hunks: 12
# ====================================================================
# --- HUNK 1: Lines 360-521 ---
   360|             "__master_opts__" in self.opts
   361|             and self.opts["__master_opts__"].get("ssh_use_home_key")
   362|             and os.path.isfile(os.path.expanduser("~/.ssh/id_rsa"))
   363|         ):
   364|             priv = os.path.expanduser("~/.ssh/id_rsa")
   365|         else:
   366|             priv = self.opts.get(
   367|                 "ssh_priv", os.path.join(self.opts["pki_dir"], "ssh", "salt-ssh.rsa")
   368|             )
   369|         pub = f"{priv}.pub"
   370|         with salt.utils.files.fopen(pub, "r") as fp_:
   371|             return f"{fp_.read().split()[1]} rsa root@master"
   372|     def key_deploy(self, host, ret):
   373|         """
   374|         Deploy the SSH key if the minions don't auth
   375|         """
   376|         if not isinstance(ret[host], dict) or self.opts.get("ssh_key_deploy"):
   377|             target = self.targets[host]
   378|             if target.get("passwd", False) or self.opts["ssh_passwd"]:
   379|                 self._key_deploy_run(host, target, False)
   380|             return ret
   381|         stderr = ret[host].get("stderr", "")
   382|         ignore_err = ["failed to upload file"]
   383|         check_err = [x for x in ignore_err if stderr.count(x)]
   384|         if "Permission denied" in stderr and not check_err:
   385|             target = self.targets[host]
   386|             print(
   387|                 "Permission denied for host {}, do you want to deploy "
   388|                 "the salt-ssh key? (password required):".format(host)
   389|             )
   390|             deploy = input("[Y/n] ")
   391|             if deploy.startswith(("n", "N")):
   392|                 return ret
   393|             target["passwd"] = getpass.getpass(
   394|                 "Password for {}@{}: ".format(target["user"], host)
   395|             )
   396|             return self._key_deploy_run(host, target, True)
   397|         return ret
   398|     def _key_deploy_run(self, host, target, re_run=True):
   399|         """
   400|         The ssh-copy-id routine
   401|         """
   402|         argv = [
   403|             "ssh.set_auth_key",
   404|             target.get("user", "root"),
   405|             self.get_pubkey(),
   406|         ]
   407|         single = Single(
   408|             self.opts,
   409|             argv,
   410|             host,
   411|             mods=self.mods,
   412|             fsclient=self.fsclient,
   413|             thin=self.thin,
   414|             **target,
   415|         )
   416|         if salt.utils.path.which("ssh-copy-id"):
   417|             stdout, stderr, retcode = single.shell.copy_id()
   418|         else:
   419|             stdout, stderr, retcode = single.run()
   420|         if re_run:
   421|             target.pop("passwd")
   422|             single = Single(
   423|                 self.opts,
   424|                 self.opts["argv"],
   425|                 host,
   426|                 mods=self.mods,
   427|                 fsclient=self.fsclient,
   428|                 thin=self.thin,
   429|                 **target,
   430|             )
   431|             stdout, stderr, retcode = single.cmd_block()
   432|             try:
   433|                 data = salt.utils.json.find_json(stdout)
   434|                 return {host: data.get("local", data)}
   435|             except Exception:  # pylint: disable=broad-except
   436|                 if stderr:
   437|                     return {host: stderr}
   438|                 return {host: "Bad Return"}
   439|         if salt.defaults.exitcodes.EX_OK != retcode:
   440|             return {host: stderr}
   441|         return {host: stdout}
   442|     def handle_routine(self, que, opts, host, target, mine=False):
   443|         """
   444|         Run the routine in a "Thread", put a dict on the queue
   445|         """
   446|         opts = copy.deepcopy(opts)
   447|         single = Single(
   448|             opts,
   449|             opts["argv"],
   450|             host,
   451|             mods=self.mods,
   452|             fsclient=self.fsclient,
   453|             thin=self.thin,
   454|             mine=mine,
   455|             **target,
   456|         )
   457|         ret = {"id": single.id}
   458|         stdout, stderr, retcode = single.run()
   459|         try:
   460|             retcode = int(retcode)
   461|         except (TypeError, ValueError):
   462|             log.warning("Got an invalid retcode for host '%s': '%s'", host, retcode)
   463|             retcode = 1
   464|         try:
   465|             data = salt.utils.json.find_json(stdout)
   466|             if len(data) < 2 and "local" in data:
   467|                 ret["ret"] = data["local"]
   468|                 try:
   469|                     remote_retcode = data["local"]["retcode"]
   470|                     try:
   471|                         retcode = int(remote_retcode)
   472|                     except (TypeError, ValueError):
   473|                         log.warning(
   474|                             "Host '%s' reported an invalid retcode: '%s'",
   475|                             host,
   476|                             remote_retcode,
   477|                         )
   478|                         retcode = max(retcode, 1)
   479|                 except (KeyError, TypeError):
   480|                     pass
   481|             else:
   482|                 ret["ret"] = {
   483|                     "stdout": stdout,
   484|                     "stderr": stderr,
   485|                     "retcode": retcode,
   486|                 }
   487|         except Exception:  # pylint: disable=broad-except
   488|             ret["ret"] = {
   489|                 "stdout": stdout,
   490|                 "stderr": stderr,
   491|                 "retcode": retcode,
   492|             }
   493|         que.put((ret, retcode))
   494|     def handle_ssh(self, mine=False):
   495|         """
   496|         Spin up the needed threads or processes and execute the subsequent
   497|         routines
   498|         """
   499|         que = multiprocessing.Queue()
   500|         running = {}
   501|         target_iter = iter(self.targets)
   502|         returned = set()
   503|         rets = set()
   504|         init = False
   505|         while True:
   506|             if not self.targets:
   507|                 log.error("No matching targets found in roster.")
   508|                 break
   509|             if len(running) < self.opts.get("ssh_max_procs", 25) and not init:
   510|                 try:
   511|                     host = next(target_iter)
   512|                 except StopIteration:
   513|                     init = True
   514|                     continue
   515|                 for default in self.defaults:
   516|                     if default not in self.targets[host]:
   517|                         self.targets[host][default] = self.defaults[default]
   518|                 if "host" not in self.targets[host]:
   519|                     self.targets[host]["host"] = host
   520|                 if self.targets[host].get("winrm") and not HAS_WINSHELL:
   521|                     returned.add(host)

# --- HUNK 2: Lines 530-570 ---
   530|                         "jid": None,
   531|                         "return": log_msg,
   532|                         "retcode": 1,
   533|                         "fun": "",
   534|                         "id": host,
   535|                     }
   536|                     yield {host: no_ret}, 1
   537|                     continue
   538|                 args = (
   539|                     que,
   540|                     self.opts,
   541|                     host,
   542|                     self.targets[host],
   543|                     mine,
   544|                 )
   545|                 routine = Process(target=self.handle_routine, args=args)
   546|                 routine.start()
   547|                 running[host] = {"thread": routine}
   548|                 continue
   549|             ret = {}
   550|             retcode = 0
   551|             try:
   552|                 ret, retcode = que.get(False)
   553|                 if "id" in ret:
   554|                     returned.add(ret["id"])
   555|                     yield {ret["id"]: ret["ret"]}, retcode
   556|             except queue.Empty:
   557|                 pass
   558|             for host in running:
   559|                 if not running[host]["thread"].is_alive():
   560|                     if host not in returned:
   561|                         try:
   562|                             while True:
   563|                                 ret, retcode = que.get(False)
   564|                                 if "id" in ret:
   565|                                     returned.add(ret["id"])
   566|                                     yield {ret["id"]: ret["ret"]}, retcode
   567|                         except queue.Empty:
   568|                             pass
   569|                         if host not in returned:
   570|                             error = (

# --- HUNK 3: Lines 601-654 ---
   601|             args = argv
   602|         else:
   603|             fun = argv[0] if argv else ""
   604|             args = argv[1:]
   605|         job_load = {
   606|             "jid": jid,
   607|             "tgt_type": self.tgt_type,
   608|             "tgt": self.opts["tgt"],
   609|             "user": self.opts["user"],
   610|             "fun": fun,
   611|             "arg": args,
   612|         }
   613|         if self.opts["master_job_cache"] == "local_cache":
   614|             self.returners["{}.save_load".format(self.opts["master_job_cache"])](
   615|                 jid, job_load, minions=self.targets.keys()
   616|             )
   617|         else:
   618|             self.returners["{}.save_load".format(self.opts["master_job_cache"])](
   619|                 jid, job_load
   620|             )
   621|         for ret, _ in self.handle_ssh(mine=mine):
   622|             host = next(iter(ret))
   623|             self.cache_job(jid, host, ret[host], fun)
   624|             if self.event:
   625|                 id_, data = next(iter(ret.items()))
   626|                 if isinstance(data, str):
   627|                     data = {"return": data}
   628|                 if "id" not in data:
   629|                     data["id"] = id_
   630|                 if "fun" not in data:
   631|                     data["fun"] = fun
   632|                 data["jid"] = (
   633|                     jid  # make the jid in the payload the same as the jid in the tag
   634|                 )
   635|                 self.event.fire_event(
   636|                     data, salt.utils.event.tagify([jid, "ret", host], "job")
   637|                 )
   638|             yield ret
   639|     def cache_job(self, jid, id_, ret, fun):
   640|         """
   641|         Cache the job information
   642|         """
   643|         self.returners["{}.returner".format(self.opts["master_job_cache"])](
   644|             {"jid": jid, "id": id_, "return": ret, "fun": fun}
   645|         )
   646|     def run(self, jid=None):
   647|         """
   648|         Execute the overall routine, print results via outputters
   649|         """
   650|         if self.opts.get("list_hosts"):
   651|             self._get_roster()
   652|             ret = {}
   653|             for roster_file in self.__parsed_rosters:
   654|                 if roster_file.startswith("#"):

# --- HUNK 4: Lines 684-758 ---
   684|                     jid, job_load, minions=self.targets.keys()
   685|                 )
   686|             else:
   687|                 self.returners["{}.save_load".format(self.opts["master_job_cache"])](
   688|                     jid, job_load
   689|                 )
   690|         except Exception as exc:  # pylint: disable=broad-except
   691|             log.error(
   692|                 "Could not save load with returner %s: %s",
   693|                 self.opts["master_job_cache"],
   694|                 exc,
   695|                 exc_info=True,
   696|             )
   697|         if self.opts.get("verbose"):
   698|             msg = f"Executing job with jid {jid}"
   699|             print(msg)
   700|             print("-" * len(msg) + "\n")
   701|             print("")
   702|         sret = {}
   703|         outputter = self.opts.get("output", "nested")
   704|         final_exit = 0
   705|         for ret, retcode in self.handle_ssh():
   706|             host = next(iter(ret))
   707|             if not isinstance(retcode, int):
   708|                 log.warning("Host '%s' returned an invalid retcode: %s", host, retcode)
   709|                 retcode = 1
   710|             final_exit = max(final_exit, retcode)
   711|             self.cache_job(jid, host, ret[host], fun)
   712|             ret = self.key_deploy(host, ret)
   713|             if isinstance(ret[host], dict) and (
   714|                 ret[host].get("stderr") or ""
   715|             ).startswith("ssh:"):
   716|                 ret[host] = ret[host]["stderr"]
   717|             if not isinstance(ret[host], dict):
   718|                 p_data = {host: ret[host]}
   719|             elif "return" not in ret[host]:
   720|                 p_data = ret
   721|             else:
   722|                 outputter = ret[host].get("out", self.opts.get("output", "nested"))
   723|                 p_data = {host: ret[host].get("return", {})}
   724|             if self.opts.get("static"):
   725|                 sret.update(p_data)
   726|             else:
   727|                 salt.output.display_output(p_data, outputter, self.opts)
   728|             if self.event:
   729|                 id_, data = next(iter(ret.items()))
   730|                 if isinstance(data, str):
   731|                     data = {"return": data}
   732|                 if "id" not in data:
   733|                     data["id"] = id_
   734|                 if "fun" not in data:
   735|                     data["fun"] = fun
   736|                 data["jid"] = (
   737|                     jid  # make the jid in the payload the same as the jid in the tag
   738|                 )
   739|                 self.event.fire_event(
   740|                     data, salt.utils.event.tagify([jid, "ret", host], "job")
   741|                 )
   742|         if self.event is not None:
   743|             self.event.destroy()
   744|         if self.opts.get("static"):
   745|             salt.output.display_output(sret, outputter, self.opts)
   746|         if final_exit:
   747|             sys.exit(salt.defaults.exitcodes.EX_AGGREGATE)
   748| class Single:
   749|     """
   750|     Hold onto a single ssh execution
   751|     """
   752|     def __init__(
   753|         self,
   754|         opts,
   755|         argv,
   756|         id_,
   757|         host,
   758|         user=None,

# --- HUNK 5: Lines 868-908 ---
   868|         args = parsed[0]
   869|         kws = parsed[1]
   870|         return fun, args, kws
   871|     def _escape_arg(self, arg):
   872|         """
   873|         Properly escape argument to protect special characters from shell
   874|         interpretation.  This avoids having to do tricky argument quoting.
   875|         Effectively just escape all characters in the argument that are not
   876|         alphanumeric!
   877|         """
   878|         if self.winrm:
   879|             return arg
   880|         return "".join(["\\" + char if re.match(r"\W", char) else char for char in arg])
   881|     def run_ssh_pre_flight(self):
   882|         """
   883|         Run our pre_flight script before running any ssh commands
   884|         """
   885|         with tempfile.NamedTemporaryFile() as temp:
   886|             try:
   887|                 shutil.copyfile(self.ssh_pre_flight, temp.name)
   888|             except OSError as err:
   889|                 return (
   890|                     "",
   891|                     "Could not copy pre flight script to temporary path",
   892|                     1,
   893|                 )
   894|             target_script = f".{pathlib.Path(temp.name).name}"
   895|             log.trace("Copying the pre flight script to target")
   896|             stdout, stderr, retcode = self.shell.send(temp.name, target_script)
   897|             if retcode != 0:
   898|                 log.error("Could not copy the pre flight script to target")
   899|                 return stdout, stderr, retcode
   900|             log.trace("Executing the pre flight script on target")
   901|             return self.execute_script(
   902|                 target_script, script_args=self.ssh_pre_flight_args
   903|             )
   904|     def check_thin_dir(self):
   905|         """
   906|         check if the thindir exists on the remote machine
   907|         """
   908|         stdout, stderr, retcode = self.shell.exec_cmd(f"test -d {self.thin_dir}")

# --- HUNK 6: Lines 922-975 ---
   922|     def deploy_ext(self):
   923|         """
   924|         Deploy the ext_mods tarball
   925|         """
   926|         if self.mods.get("file"):
   927|             self.shell.send(
   928|                 self.mods["file"],
   929|                 os.path.join(self.thin_dir, "salt-ext_mods.tgz"),
   930|             )
   931|         return True
   932|     def run(self, deploy_attempted=False):
   933|         """
   934|         Execute the routine, the routine can be either:
   935|         1. Execute a raw shell command
   936|         2. Execute a wrapper func
   937|         3. Execute a remote Salt command
   938|         If a (re)deploy is needed, then retry the operation after a deploy
   939|         attempt
   940|         Returns tuple of (stdout, stderr, retcode)
   941|         """
   942|         stdout = stderr = retcode = None
   943|         if self.ssh_pre_flight:
   944|             if not self.opts.get("ssh_run_pre_flight", False) and self.check_thin_dir():
   945|                 log.info(
   946|                     "%s thin dir already exists. Not running ssh_pre_flight script",
   947|                     self.thin_dir,
   948|                 )
   949|             elif not os.path.exists(self.ssh_pre_flight):
   950|                 log.error(
   951|                     "The ssh_pre_flight script %s does not exist", self.ssh_pre_flight
   952|                 )
   953|             else:
   954|                 stdout, stderr, retcode = self.run_ssh_pre_flight()
   955|                 if retcode != 0:
   956|                     log.error(
   957|                         "Error running ssh_pre_flight script %s", self.ssh_pre_file
   958|                     )
   959|                     return stdout, stderr, retcode
   960|                 log.info(
   961|                     "Successfully ran the ssh_pre_flight script: %s", self.ssh_pre_file
   962|                 )
   963|         if self.opts.get("raw_shell", False):
   964|             cmd_str = " ".join([self._escape_arg(arg) for arg in self.argv])
   965|             stdout, stderr, retcode = self.shell.exec_cmd(cmd_str)
   966|         elif self.fun in self.wfuncs or self.mine:
   967|             stdout, retcode = self.run_wfunc()
   968|         else:
   969|             stdout, stderr, retcode = self.cmd_block()
   970|         return stdout, stderr, retcode
   971|     def run_wfunc(self):
   972|         """
   973|         Execute a wrapper function
   974|         Returns tuple of (json_data, '')
   975|         """

# --- HUNK 7: Lines 985-1046 ---
   985|         else:
   986|             passed_time = (time.time() - os.stat(datap).st_mtime) / 60
   987|             if passed_time > self.opts.get("cache_life", 60):
   988|                 refresh = True
   989|         if self.opts.get("refresh_cache"):
   990|             refresh = True
   991|         conf_grains = {}
   992|         if "ssh_grains" in self.opts:
   993|             conf_grains = self.opts["ssh_grains"]
   994|         if not data_cache:
   995|             refresh = True
   996|         if refresh:
   997|             pre_wrapper = salt.client.ssh.wrapper.FunctionWrapper(
   998|                 self.opts,
   999|                 self.id,
  1000|                 fsclient=self.fsclient,
  1001|                 minion_opts=self.minion_opts,
  1002|                 **self.target,
  1003|             )
  1004|             opts_pkg = pre_wrapper["test.opts_pkg"]()  # pylint: disable=E1102
  1005|             if "_error" in opts_pkg:
  1006|                 retcode = opts_pkg["retcode"]
  1007|                 ret = salt.utils.json.dumps({"local": opts_pkg})
  1008|                 return ret, retcode
  1009|             opts_pkg["file_roots"] = self.opts["file_roots"]
  1010|             opts_pkg["pillar_roots"] = self.opts["pillar_roots"]
  1011|             opts_pkg["ext_pillar"] = self.opts["ext_pillar"]
  1012|             opts_pkg["extension_modules"] = self.opts["extension_modules"]
  1013|             opts_pkg["module_dirs"] = self.opts["module_dirs"]
  1014|             opts_pkg["_ssh_version"] = self.opts["_ssh_version"]
  1015|             opts_pkg["thin_dir"] = self.opts["thin_dir"]
  1016|             opts_pkg["master_tops"] = self.opts["master_tops"]
  1017|             opts_pkg["extra_filerefs"] = self.opts.get("extra_filerefs", "")
  1018|             opts_pkg["__master_opts__"] = self.context["master_opts"]
  1019|             if "known_hosts_file" in self.opts:
  1020|                 opts_pkg["known_hosts_file"] = self.opts["known_hosts_file"]
  1021|             if "_caller_cachedir" in self.opts:
  1022|                 opts_pkg["_caller_cachedir"] = self.opts["_caller_cachedir"]
  1023|             else:
  1024|                 opts_pkg["_caller_cachedir"] = self.opts["cachedir"]
  1025|             opts_pkg["id"] = self.id
  1026|             retcode = 0
  1027|             for grain in conf_grains:
  1028|                 opts_pkg["grains"][grain] = conf_grains[grain]
  1029|             if "grains" in self.target:
  1030|                 for grain in self.target["grains"]:
  1031|                     opts_pkg["grains"][grain] = self.target["grains"][grain]
  1032|             popts = {}
  1033|             popts.update(opts_pkg)
  1034|             popts.update(opts_pkg["__master_opts__"])
  1035|             pillar = salt.pillar.Pillar(
  1036|                 popts,
  1037|                 opts_pkg["grains"],
  1038|                 opts_pkg["id"],
  1039|                 opts_pkg.get("saltenv", "base"),
  1040|             )
  1041|             pillar_data = pillar.compile_pillar()
  1042|             data = {
  1043|                 "opts": opts_pkg,
  1044|                 "grains": opts_pkg["grains"],
  1045|                 "pillar": pillar_data,
  1046|             }

# --- HUNK 8: Lines 1086-1138 ---
  1086|                 mine_args = mine_fun_data
  1087|             elif isinstance(mine_fun_data, list):
  1088|                 for item in mine_fun_data[:]:
  1089|                     if isinstance(item, dict) and "mine_function" in item:
  1090|                         mine_fun = item["mine_function"]
  1091|                         mine_fun_data.pop(mine_fun_data.index(item))
  1092|                 mine_args = mine_fun_data
  1093|             else:
  1094|                 mine_args = mine_fun_data
  1095|             if isinstance(mine_args, dict):
  1096|                 self.args = []
  1097|                 self.kwargs = mine_args
  1098|             elif isinstance(mine_args, list):
  1099|                 self.args = mine_args
  1100|                 self.kwargs = {}
  1101|         try:
  1102|             if self.mine:
  1103|                 result = wrapper[mine_fun](*self.args, **self.kwargs)
  1104|             else:
  1105|                 result = self.wfuncs[self.fun](*self.args, **self.kwargs)
  1106|         except TypeError as exc:
  1107|             result = f"TypeError encountered executing {self.fun}: {exc}"
  1108|             log.error(result, exc_info_on_loglevel=logging.DEBUG)
  1109|             retcode = 1
  1110|         except Exception as exc:  # pylint: disable=broad-except
  1111|             result = "An Exception occurred while executing {}: {}".format(
  1112|                 self.fun, exc
  1113|             )
  1114|             log.error(result, exc_info_on_loglevel=logging.DEBUG)
  1115|             retcode = 1
  1116|         retcode = max(retcode, self.context.get("retcode", 0))
  1117|         if isinstance(result, dict) and "local" in result:
  1118|             ret = salt.utils.json.dumps({"local": result["local"]})
  1119|         else:
  1120|             ret = salt.utils.json.dumps({"local": {"return": result}})
  1121|         return ret, retcode
  1122|     def _cmd_str(self):
  1123|         """
  1124|         Prepare the command string
  1125|         """
  1126|         if self.target.get("sudo"):
  1127|             sudo = (
  1128|                 f"sudo -p '{salt.client.ssh.shell.SUDO_PROMPT}'"
  1129|                 if self.target.get("passwd")
  1130|                 else "sudo"
  1131|             )
  1132|         else:
  1133|             sudo = ""
  1134|         sudo_user = self.target["sudo_user"]
  1135|         if "_caller_cachedir" in self.opts:
  1136|             cachedir = self.opts["_caller_cachedir"]
  1137|         else:
  1138|             cachedir = self.opts["cachedir"]

# --- HUNK 9: Lines 1203-1243 ---
  1203|                 ret = saltwinshell.call_python(self, script)
  1204|         if not self.winrm:
  1205|             self.shell.exec_cmd(f"rm '{pre_dir}{script}'")
  1206|         else:
  1207|             self.shell.exec_cmd(f"del {script}")
  1208|         return ret
  1209|     def shim_cmd(self, cmd_str, extension="py"):
  1210|         """
  1211|         Run a shim command.
  1212|         If tty is enabled, we must scp the shim to the target system and
  1213|         execute it there
  1214|         """
  1215|         if not self.tty and not self.winrm:
  1216|             return self.shell.exec_cmd(cmd_str)
  1217|         with tempfile.NamedTemporaryFile(mode="w+b", delete=False) as shim_tmp_file:
  1218|             shim_tmp_file.write(salt.utils.stringutils.to_bytes(cmd_str))
  1219|         target_shim_file = f".{pathlib.Path(shim_tmp_file.name).name}"
  1220|         if self.winrm:
  1221|             target_shim_file = saltwinshell.get_target_shim_file(self, target_shim_file)
  1222|         stdout, stderr, retcode = self.shell.send(
  1223|             shim_tmp_file.name, target_shim_file, makedirs=True
  1224|         )
  1225|         if retcode != 0:
  1226|             log.error("Could not copy the shim script to target")
  1227|             return stdout, stderr, retcode
  1228|         try:
  1229|             os.remove(shim_tmp_file.name)
  1230|         except OSError:
  1231|             pass
  1232|         ret = self.execute_script(script=target_shim_file, extension=extension)
  1233|         return ret
  1234|     def cmd_block(self, is_retry=False):
  1235|         """
  1236|         Prepare the pre-check command to send to the subsystem
  1237|         1. execute SHIM + command
  1238|         2. check if SHIM returns a master request or if it completed
  1239|         3. handle any master request
  1240|         4. re-execute SHIM + command
  1241|         5. split SHIM results from command results
  1242|         6. return command results
  1243|         """

# --- HUNK 10: Lines 1442-1481 ---
  1442|     ret = []
  1443|     if isinstance(data, str):
  1444|         if data.startswith(proto):
  1445|             return [data]
  1446|     if isinstance(data, list):
  1447|         for comp in data:
  1448|             if isinstance(comp, str):
  1449|                 if comp.startswith(proto):
  1450|                     ret.append(comp)
  1451|     return ret
  1452| def mod_data(fsclient):
  1453|     """
  1454|     Generate the module arguments for the shim data
  1455|     """
  1456|     sync_refs = [
  1457|         "modules",
  1458|         "states",
  1459|         "grains",
  1460|         "renderers",
  1461|         "returners",
  1462|     ]
  1463|     ret = {}
  1464|     with fsclient:
  1465|         envs = fsclient.envs()
  1466|         ver_base = ""
  1467|         for env in envs:
  1468|             files = fsclient.file_list(env)
  1469|             for ref in sync_refs:
  1470|                 mods_data = {}
  1471|                 pref = f"_{ref}"
  1472|                 for fn_ in sorted(files):
  1473|                     if fn_.startswith(pref):
  1474|                         if fn_.endswith((".py", ".so", ".pyx")):
  1475|                             full = salt.utils.url.create(fn_)
  1476|                             mod_path = fsclient.cache_file(full, env)
  1477|                             if not os.path.isfile(mod_path):
  1478|                                 continue
  1479|                             mods_data[os.path.basename(fn_)] = mod_path
  1480|                             chunk = salt.utils.hashutils.get_hash(mod_path)
  1481|                             ver_base += chunk

# --- HUNK 11: Lines 1493-1533 ---
  1493|         if os.path.isfile(ext_tar_path):
  1494|             return mods
  1495|         tfp = tarfile.open(ext_tar_path, "w:gz")
  1496|         verfile = os.path.join(fsclient.opts["cachedir"], "ext_mods.ver")
  1497|         with salt.utils.files.fopen(verfile, "w+") as fp_:
  1498|             fp_.write(ver)
  1499|         tfp.add(verfile, "ext_version")
  1500|         for ref in ret:
  1501|             for fn_ in ret[ref]:
  1502|                 tfp.add(ret[ref][fn_], os.path.join(ref, fn_))
  1503|         tfp.close()
  1504|         return mods
  1505| def ssh_version():
  1506|     """
  1507|     Returns the version of the installed ssh command
  1508|     """
  1509|     ret = subprocess.Popen(
  1510|         ["ssh", "-V"], stdout=subprocess.PIPE, stderr=subprocess.PIPE
  1511|     ).communicate()
  1512|     try:
  1513|         version_parts = ret[1].split(b",", maxsplit=1)[0].split(b"_")[1]
  1514|         parts = []
  1515|         for part in version_parts:
  1516|             try:
  1517|                 parts.append(int(part))
  1518|             except ValueError:
  1519|                 return tuple(parts)
  1520|         return tuple(parts)
  1521|     except IndexError:
  1522|         return (2, 0)
  1523| def _convert_args(args):
  1524|     """
  1525|     Take a list of args, and convert any dicts inside the list to keyword
  1526|     args in the form of `key=value`, ready to be passed to salt-ssh
  1527|     """
  1528|     converted = []
  1529|     for arg in args:
  1530|         if isinstance(arg, dict):
  1531|             for key in list(arg.keys()):
  1532|                 if key == "__kwarg__":
  1533|                     continue


# ====================================================================
# FILE: salt/client/ssh/shell.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-54 ---
     1| """
     2| Manage transport commands via ssh
     3| """
     4| import logging
     5| import os
     6| import re
     7| import shlex
     8| import subprocess
     9| import sys
    10| import time
    11| import salt.defaults.exitcodes
    12| import salt.utils.json
    13| import salt.utils.nb_popen
    14| import salt.utils.path
    15| import salt.utils.vt
    16| log = logging.getLogger(__name__)
    17| SSH_PASSWORD_PROMPT_RE = re.compile(r"(?:.*)[Pp]assword(?: for .*)?:\s*$", re.M)
    18| KEY_VALID_RE = re.compile(r".*\(yes\/no\).*")
    19| SSH_PRIVATE_KEY_PASSWORD_PROMPT_RE = re.compile(r"Enter passphrase for key", re.M)
    20| SUDO_PROMPT = "[salt:sudo:d11bd4221135c33324a6bdc09674146fbfdf519989847491e34a689369bbce23]passwd:"
    21| SUDO_PROMPT_RE = re.compile(
    22|     r"\[salt:sudo:d11bd4221135c33324a6bdc09674146fbfdf519989847491e34a689369bbce23\]passwd:",
    23|     re.M,
    24| )
    25| RSTR = "_edbc7885e4f9aac9b83b35999b68d015148caf467b78fa39c05f669c0ff89878"
    26| RSTR_RE = re.compile(r"(?:^|\r?\n)" + RSTR + r"(?:\r?\n|$)")
    27| SSH_KEYGEN_PATH = salt.utils.path.which("ssh-keygen") or "ssh-keygen"
    28| SSH_PATH = salt.utils.path.which("ssh") or "ssh"
    29| SCP_PATH = salt.utils.path.which("scp") or "scp"
    30| def gen_key(path):
    31|     """
    32|     Generate a key for use with salt-ssh
    33|     """
    34|     cmd = [SSH_KEYGEN_PATH, "-P", "", "-f", path, "-t", "rsa", "-q"]
    35|     dirname = os.path.dirname(path)
    36|     if dirname and not os.path.isdir(dirname):
    37|         os.makedirs(os.path.dirname(path))
    38|     subprocess.call(cmd)
    39| def gen_shell(opts, **kwargs):
    40|     """
    41|     Return the correct shell interface for the target system
    42|     """
    43|     if kwargs["winrm"]:
    44|         try:
    45|             import saltwinshell
    46|             shell = saltwinshell.Shell(opts, **kwargs)
    47|         except ImportError:
    48|             log.error("The saltwinshell library is not available")
    49|             sys.exit(salt.defaults.exitcodes.EX_GENERIC)
    50|     else:
    51|         shell = Shell(opts, **kwargs)
    52|     return shell
    53| class Shell:
    54|     """

# --- HUNK 2: Lines 194-245 ---
   194|         """
   195|         if self.passwd:
   196|             return "{} {} {} -p {} {} {}@{}".format(
   197|                 "ssh-copy-id",
   198|                 f"-i {self.priv}.pub",
   199|                 self._passwd_opts(),
   200|                 self.port,
   201|                 self._ssh_opts(),
   202|                 self.user,
   203|                 self.host,
   204|             )
   205|         return None
   206|     def copy_id(self):
   207|         """
   208|         Execute ssh-copy-id to plant the id file on the target
   209|         """
   210|         stdout, stderr, retcode = self._run_cmd(self._copy_id_str_old())
   211|         if salt.defaults.exitcodes.EX_OK != retcode and "Usage" in stderr:
   212|             stdout, stderr, retcode = self._run_cmd(self._copy_id_str_new())
   213|         return stdout, stderr, retcode
   214|     def _cmd_str(self, cmd, ssh=SSH_PATH):
   215|         """
   216|         Return the cmd string to execute
   217|         """
   218|         command = [ssh]
   219|         if ssh != SCP_PATH:
   220|             command.append(self.host)
   221|         if self.tty and ssh == SSH_PATH:
   222|             command.append("-t -t")
   223|         if self.passwd or self.priv:
   224|             command.append(self.priv and self._key_opts() or self._passwd_opts())
   225|         if ssh != SCP_PATH and self.remote_port_forwards:
   226|             command.append(
   227|                 " ".join(
   228|                     [f"-R {item}" for item in self.remote_port_forwards.split(",")]
   229|                 )
   230|             )
   231|         if self.ssh_options:
   232|             command.append(self._ssh_opts())
   233|         command.append(cmd)
   234|         return " ".join(command)
   235|     def _run_nb_cmd(self, cmd):
   236|         """
   237|         cmd iterator
   238|         """
   239|         try:
   240|             proc = salt.utils.nb_popen.NonBlockingPopen(
   241|                 self._split_cmd(cmd),
   242|                 stderr=subprocess.PIPE,
   243|                 stdout=subprocess.PIPE,
   244|             )
   245|             while True:

# --- HUNK 3: Lines 276-321 ---
   276|     def exec_cmd(self, cmd):
   277|         """
   278|         Execute a remote command
   279|         """
   280|         cmd = self._cmd_str(cmd)
   281|         logmsg = f"Executing command: {cmd}"
   282|         if self.passwd:
   283|             logmsg = logmsg.replace(self.passwd, ("*" * 6))
   284|         if 'decode("base64")' in logmsg or "base64.b64decode(" in logmsg:
   285|             log.debug("Executed SHIM command. Command logged to TRACE")
   286|             log.trace(logmsg)
   287|         else:
   288|             log.debug(logmsg)
   289|         ret = self._run_cmd(cmd)
   290|         return ret
   291|     def send(self, local, remote, makedirs=False):
   292|         """
   293|         scp a file or files to a remote system
   294|         """
   295|         if makedirs:
   296|             self.exec_cmd(f"mkdir -p {os.path.dirname(remote)}")
   297|         host = self.host
   298|         if ":" in host:
   299|             host = f"[{host}]"
   300|         cmd = f"{local} {host}:{remote}"
   301|         cmd = self._cmd_str(cmd, ssh=SCP_PATH)
   302|         logmsg = f"Executing command: {cmd}"
   303|         if self.passwd:
   304|             logmsg = logmsg.replace(self.passwd, ("*" * 6))
   305|         log.debug(logmsg)
   306|         return self._run_cmd(cmd)
   307|     def _split_cmd(self, cmd):
   308|         """
   309|         Split a command string so that it is suitable to pass to Popen without
   310|         shell=True. This prevents shell injection attacks in the options passed
   311|         to ssh or some other command.
   312|         """
   313|         try:
   314|             ssh_part, cmd_part = cmd.split("/bin/sh")
   315|         except ValueError:
   316|             cmd_lst = shlex.split(cmd)
   317|         else:
   318|             cmd_lst = shlex.split(ssh_part)
   319|             cmd_lst.append(f"/bin/sh {cmd_part}")
   320|         return cmd_lst
   321|     def _run_cmd(self, cmd, key_accept=False, passwd_retries=3):


# ====================================================================
# FILE: salt/client/ssh/state.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 82-157 ---
    82|     stack = []
    83|     def __init__(
    84|         self,
    85|         opts,
    86|         pillar_override=None,
    87|         wrapper=None,
    88|         fsclient=None,
    89|         context=None,
    90|         initial_pillar=None,
    91|     ):
    92|         self.client = fsclient
    93|         salt.state.BaseHighState.__init__(self, opts)
    94|         self.state = SSHState(
    95|             opts,
    96|             pillar_override,
    97|             wrapper,
    98|             context=context,
    99|             initial_pillar=initial_pillar,
   100|         )
   101|         self.matchers = salt.loader.matchers(self.opts)
   102|         self.tops = salt.loader.tops(self.opts)
   103|         self._pydsl_all_decls = {}
   104|         self._pydsl_render_stack = []
   105|     def push_active(self):
   106|         salt.state.HighState.stack.append(self)
   107|     def load_dynamic(self, matches):
   108|         """
   109|         Stub out load_dynamic
   110|         """
   111|         return
   112|     def _master_tops(self):
   113|         """
   114|         Evaluate master_tops locally
   115|         """
   116|         if "id" not in self.opts:
   117|             log.error("Received call for external nodes without an id")
   118|             return {}
   119|         if not salt.utils.verify.valid_id(self.opts, self.opts["id"]):
   120|             return {}
   121|         grains = {}
   122|         ret = {}
   123|         if "grains" in self.opts:
   124|             grains = self.opts["grains"]
   125|         for fun in self.tops:
   126|             if fun not in self.opts.get("master_tops", {}):
   127|                 continue
   128|             try:
   129|                 ret.update(self.tops[fun](opts=self.opts, grains=grains))
   130|             except Exception as exc:  # pylint: disable=broad-except
   131|                 log.error(
   132|                     "Top function %s failed with error %s for minion %s",
   133|                     fun,
   134|                     exc,
   135|                     self.opts["id"],
   136|                 )
   137|         return ret
   138|     def destroy(self):
   139|         if self.client:
   140|             self.client.destroy()
   141|     def __enter__(self):
   142|         return self
   143|     def __exit__(self, *_):
   144|         self.destroy()
   145| def lowstate_file_refs(chunks, extras=""):
   146|     """
   147|     Create a list of file ref objects to reconcile
   148|     """
   149|     refs = {}
   150|     for chunk in chunks:
   151|         if not isinstance(chunk, dict):
   152|             continue
   153|         saltenv = "base"
   154|         crefs = []
   155|         for state in chunk:
   156|             if state == "__env__":
   157|                 saltenv = chunk[state]


# ====================================================================
# FILE: salt/client/ssh/wrapper/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| """
     2| The ssh client wrapper system contains the routines that are used to alter
     3| how executions are run in the salt-ssh system, this allows for state routines
     4| to be easily rewritten to execute in a way that makes them do the same tasks
     5| as ZeroMQ salt, but via ssh.
     6| """
     7| import copy
     8| import salt.client.ssh
     9| import salt.loader
    10| import salt.utils.data
    11| import salt.utils.json
    12| class FunctionWrapper:
    13|     """
    14|     Create an object that acts like the salt function dict and makes function
    15|     calls remotely via the SSH shell system
    16|     """
    17|     def __init__(
    18|         self,
    19|         opts,
    20|         id_,
    21|         host,
    22|         wfuncs=None,
    23|         mods=None,
    24|         fsclient=None,
    25|         cmd_prefix=None,
    26|         aliases=None,
    27|         minion_opts=None,
    28|         **kwargs,
    29|     ):
    30|         super().__init__()
    31|         self.cmd_prefix = cmd_prefix

# --- HUNK 2: Lines 83-142 ---
    83|             argv = [cmd]
    84|             argv.extend([salt.utils.json.dumps(arg) for arg in args])
    85|             argv.extend(
    86|                 [
    87|                     "{}={}".format(
    88|                         salt.utils.stringutils.to_str(key), salt.utils.json.dumps(val)
    89|                     )
    90|                     for key, val in kwargs.items()
    91|                 ]
    92|             )
    93|             single = salt.client.ssh.Single(
    94|                 self.opts,
    95|                 argv,
    96|                 mods=self.mods,
    97|                 disable_wipe=True,
    98|                 fsclient=self.fsclient,
    99|                 minion_opts=self.minion_opts,
   100|                 **self.kwargs,
   101|             )
   102|             stdout, stderr, retcode = single.cmd_block()
   103|             if stderr.count("Permission Denied"):
   104|                 return {
   105|                     "_error": "Permission Denied",
   106|                     "stdout": stdout,
   107|                     "stderr": stderr,
   108|                     "retcode": retcode,
   109|                 }
   110|             try:
   111|                 ret = salt.utils.json.loads(stdout)
   112|                 if len(ret) < 2 and "local" in ret:
   113|                     ret = ret["local"]
   114|                 ret = ret.get("return", {})
   115|             except ValueError:
   116|                 ret = {
   117|                     "_error": "Failed to return clean data",
   118|                     "stderr": stderr,
   119|                     "stdout": stdout,
   120|                     "retcode": retcode,
   121|                 }
   122|             return ret
   123|         return caller
   124|     def __setitem__(self, cmd, value):
   125|         """
   126|         Set aliases for functions
   127|         """
   128|         if "." not in cmd and not self.cmd_prefix:
   129|             raise KeyError(f"Cannot assign to module key {cmd} in the FunctionWrapper")
   130|         if self.cmd_prefix:
   131|             cmd = f"{self.cmd_prefix}.{cmd}"
   132|         if cmd in self.wfuncs:
   133|             self.wfuncs[cmd] = value
   134|         self.aliases[cmd] = value
   135|     def get(self, cmd, default):
   136|         """
   137|         Mirrors behavior of dict.get
   138|         """
   139|         if cmd in self:
   140|             return self[cmd]
   141|         else:
   142|             return default


# ====================================================================
# FILE: salt/client/ssh/wrapper/cp.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-108 ---
     1| """
     2| Wrap the cp module allowing for managed ssh file transfers
     3| """
     4| import logging
     5| import os
     6| import salt.client.ssh
     7| import salt.utils.files
     8| import salt.utils.stringutils
     9| import salt.utils.templates
    10| from salt.exceptions import CommandExecutionError
    11| log = logging.getLogger(__name__)
    12| def get_file(path, dest, saltenv="base", makedirs=False, template=None, gzip=None):
    13|     """
    14|     Send a file from the master to the location in specified
    15|     .. note::
    16|         gzip compression is not supported in the salt-ssh version of
    17|         cp.get_file. The argument is only accepted for interface compatibility.
    18|     """
    19|     if gzip is not None:
    20|         log.warning("The gzip argument to cp.get_file in salt-ssh is unsupported")
    21|     if template is not None:
    22|         (path, dest) = _render_filenames(path, dest, saltenv, template)
    23|     src = __context__["fileclient"].cache_file(
    24|         path, saltenv, cachedir=os.path.join("salt-ssh", __salt__.kwargs["id_"])
    25|     )
    26|     single = salt.client.ssh.Single(__opts__, "", **__salt__.kwargs)
    27|     ret = single.shell.send(src, dest, makedirs)
    28|     return not ret[2]
    29| def get_dir(path, dest, saltenv="base"):
    30|     """
    31|     Transfer a directory down
    32|     """
    33|     src = __context__["fileclient"].cache_dir(
    34|         path, saltenv, cachedir=os.path.join("salt-ssh", __salt__.kwargs["id_"])
    35|     )
    36|     src = " ".join(src)
    37|     single = salt.client.ssh.Single(__opts__, "", **__salt__.kwargs)
    38|     ret = single.shell.send(src, dest)
    39|     return not ret[2]
    40| def get_url(path, dest, saltenv="base"):
    41|     """
    42|     retrieve a URL
    43|     """
    44|     src = __context__["fileclient"].cache_file(
    45|         path, saltenv, cachedir=os.path.join("salt-ssh", __salt__.kwargs["id_"])
    46|     )
    47|     single = salt.client.ssh.Single(__opts__, "", **__salt__.kwargs)
    48|     ret = single.shell.send(src, dest)
    49|     return not ret[2]
    50| def list_states(saltenv="base"):
    51|     """
    52|     List all the available state modules in an environment
    53|     """
    54|     return __context__["fileclient"].list_states(saltenv)
    55| def list_master(saltenv="base", prefix=""):
    56|     """
    57|     List all of the files stored on the master
    58|     """
    59|     return __context__["fileclient"].file_list(saltenv, prefix)
    60| def list_master_dirs(saltenv="base", prefix=""):
    61|     """
    62|     List all of the directories stored on the master
    63|     """
    64|     return __context__["fileclient"].dir_list(saltenv, prefix)
    65| def list_master_symlinks(saltenv="base", prefix=""):
    66|     """
    67|     List all of the symlinks stored on the master
    68|     """
    69|     return __context__["fileclient"].symlink_list(saltenv, prefix)
    70| def _render_filenames(path, dest, saltenv, template):
    71|     """
    72|     Process markup in the :param:`path` and :param:`dest` variables (NOT the
    73|     files under the paths they ultimately point to) according to the markup
    74|     format provided by :param:`template`.
    75|     """
    76|     if not template:
    77|         return (path, dest)
    78|     if template not in salt.utils.templates.TEMPLATE_REGISTRY:
    79|         raise CommandExecutionError(
    80|             f"Attempted to render file paths with unavailable engine {template}"
    81|         )
    82|     kwargs = {}
    83|     kwargs["salt"] = __salt__.value()
    84|     kwargs["pillar"] = __pillar__.value()
    85|     kwargs["grains"] = __grains__.value()
    86|     kwargs["opts"] = __opts__
    87|     kwargs["saltenv"] = saltenv
    88|     def _render(contents):
    89|         """
    90|         Render :param:`contents` into a literal pathname by writing it to a
    91|         temp file, rendering that file, and returning the result.
    92|         """
    93|         tmp_path_fn = salt.utils.files.mkstemp()
    94|         with salt.utils.files.fopen(tmp_path_fn, "w+") as fp_:
    95|             fp_.write(salt.utils.stringutils.to_str(contents))
    96|         data = salt.utils.templates.TEMPLATE_REGISTRY[template](
    97|             tmp_path_fn, to_str=True, **kwargs
    98|         )
    99|         salt.utils.files.safe_rm(tmp_path_fn)
   100|         if not data["result"]:
   101|             raise CommandExecutionError(
   102|                 "Failed to render file path with error: {}".format(data["data"])
   103|             )
   104|         else:
   105|             return data["data"]
   106|     path = _render(path)
   107|     dest = _render(dest)
   108|     return (path, dest)


# ====================================================================
# FILE: salt/client/ssh/wrapper/grains.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-50 ---
     1| """
     2| Return/control aspects of the grains data
     3| """
     4| import math
     5| import salt.utils.data
     6| import salt.utils.dictupdate
     7| import salt.utils.json
     8| from salt.defaults import DEFAULT_TARGET_DELIM
     9| __grains__ = {}
    10| def _serial_sanitizer(instr):
    11|     """
    12|     Replaces the last 1/4 of a string with X's
    13|     """
    14|     length = len(instr)
    15|     index = int(math.floor(length * 0.75))
    16|     return "{}{}".format(instr[:index], "X" * (length - index))
    17| def _fqdn_sanitizer(x):
    18|     return "MINION.DOMAINNAME"
    19| def _hostname_sanitizer(x):
    20|     return "MINION"
    21| def _domainname_sanitizer(x):
    22|     return "DOMAINNAME"
    23| _SANITIZERS = {
    24|     "serialnumber": _serial_sanitizer,
    25|     "domain": _domainname_sanitizer,
    26|     "fqdn": _fqdn_sanitizer,
    27|     "id": _fqdn_sanitizer,
    28|     "host": _hostname_sanitizer,
    29|     "localhost": _hostname_sanitizer,
    30|     "nodename": _hostname_sanitizer,
    31| }
    32| def get(key, default="", delimiter=DEFAULT_TARGET_DELIM, ordered=True):
    33|     """
    34|     Attempt to retrieve the named value from grains, if the named value is not
    35|     available return the passed default. The default return is an empty string.
    36|     The value can also represent a value in a nested dict using a ":" delimiter
    37|     for the dict. This means that if a dict in grains looks like this::
    38|         {'pkg': {'apache': 'httpd'}}
    39|     To retrieve the value associated with the apache key in the pkg dict this
    40|     key can be passed::
    41|         pkg:apache
    42|     CLI Example:
    43|     .. code-block:: bash
    44|         salt '*' grains.get pkg:apache
    45|     """
    46|     if ordered is True:
    47|         grains = __grains__.value()
    48|     else:
    49|         grains = salt.utils.json.loads(salt.utils.json.dumps(__grains__.value()))
    50|     return salt.utils.data.traverse_dict_and_list(grains, key, default, delimiter)


# ====================================================================
# FILE: salt/client/ssh/wrapper/mine.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-41 ---
     1| """
     2| Wrapper function for mine operations for salt-ssh
     3| .. versionadded:: 2015.5.0
     4| """
     5| import copy
     6| import salt.client.ssh
     7| def get(tgt, fun, tgt_type="glob", roster="flat"):
     8|     """
     9|     Get data from the mine based on the target, function and tgt_type
    10|     This will actually run the function on all targeted minions (like
    11|     publish.publish), as salt-ssh clients can't update the mine themselves.
    12|     We will look for mine_functions in the roster, pillar, and master config,
    13|     in that order, looking for a match for the defined function
    14|     Targets can be matched based on any standard matching system that can be
    15|     matched on the defined roster (in salt-ssh) via these keywords::
    16|     CLI Example:
    17|     .. code-block:: bash
    18|         salt-ssh '*' mine.get '*' network.interfaces
    19|         salt-ssh '*' mine.get 'myminion' network.interfaces roster=flat
    20|         salt-ssh '*' mine.get '192.168.5.0' network.ipaddrs roster=scan
    21|     """
    22|     opts = copy.deepcopy(__context__["master_opts"])
    23|     minopts = copy.deepcopy(__opts__)
    24|     opts.update(minopts)
    25|     if roster:
    26|         opts["roster"] = roster
    27|     opts["argv"] = [fun]
    28|     opts["selected_target_option"] = tgt_type
    29|     opts["tgt"] = tgt
    30|     opts["arg"] = []
    31|     ssh = salt.client.ssh.SSH(opts)
    32|     rets = {}
    33|     for ret in ssh.run_iter(mine=True):
    34|         rets.update(ret)
    35|     cret = {}
    36|     for host in rets:
    37|         if "return" in rets[host]:
    38|             cret[host] = rets[host]["return"]
    39|         else:
    40|             cret[host] = rets[host]
    41|     return cret


# ====================================================================
# FILE: salt/client/ssh/wrapper/publish.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-163 ---
     1| """
     2| .. versionadded:: 2015.5.0
     3| Salt-ssh wrapper functions for the publish module.
     4| Publish will never actually execute on the minions, so we just create new
     5| salt-ssh calls and return the data from them.
     6| No access control is needed because calls cannot originate from the minions.
     7| """
     8| import copy
     9| import logging
    10| import salt.client.ssh
    11| import salt.runner
    12| import salt.utils.args
    13| log = logging.getLogger(__name__)
    14| def _publish(
    15|     tgt,
    16|     fun,
    17|     arg=None,
    18|     tgt_type="glob",
    19|     returner="",
    20|     timeout=None,
    21|     form="clean",
    22|     roster=None,
    23| ):
    24|     """
    25|     Publish a command "from the minion out to other minions". In reality, the
    26|     minion does not execute this function, it is executed by the master. Thus,
    27|     no access control is enabled, as minions cannot initiate publishes
    28|     themselves.
    29|     Salt-ssh publishes will default to whichever roster was used for the
    30|     initiating salt-ssh call, and can be overridden using the ``roster``
    31|     argument
    32|     Returners are not currently supported
    33|     The arguments sent to the minion publish function are separated with
    34|     commas. This means that for a minion executing a command with multiple
    35|     args it will look like this::
    36|         salt-ssh system.example.com publish.publish '*' user.add 'foo,1020,1020'
    37|     CLI Example:
    38|     .. code-block:: bash
    39|         salt-ssh system.example.com publish.publish '*' cmd.run 'ls -la /tmp'
    40|     """
    41|     if fun.startswith("publish."):
    42|         log.info("Cannot publish publish calls. Returning {}")
    43|         return {}
    44|     if returner:
    45|         log.warning("Returners currently not supported in salt-ssh publish")
    46|     if arg is None:
    47|         arg = []
    48|     elif not isinstance(arg, list):
    49|         arg = [salt.utils.args.yamlify_arg(arg)]
    50|     else:
    51|         arg = [salt.utils.args.yamlify_arg(x) for x in arg]
    52|     if len(arg) == 1 and arg[0] is None:
    53|         arg = []
    54|     opts = copy.deepcopy(__context__["master_opts"])
    55|     minopts = copy.deepcopy(__opts__)
    56|     opts.update(minopts)
    57|     if roster:
    58|         opts["roster"] = roster
    59|     if timeout:
    60|         opts["timeout"] = timeout
    61|     opts["argv"] = [fun] + arg
    62|     opts["selected_target_option"] = tgt_type
    63|     opts["tgt"] = tgt
    64|     opts["arg"] = arg
    65|     ssh = salt.client.ssh.SSH(opts)
    66|     rets = {}
    67|     for ret in ssh.run_iter():
    68|         rets.update(ret)
    69|     if form == "clean":
    70|         cret = {}
    71|         for host in rets:
    72|             if "return" in rets[host]:
    73|                 cret[host] = rets[host]["return"]
    74|             else:
    75|                 cret[host] = rets[host]
    76|         return cret
    77|     else:
    78|         return rets
    79| def publish(tgt, fun, arg=None, tgt_type="glob", returner="", timeout=5, roster=None):
    80|     """
    81|     Publish a command "from the minion out to other minions". In reality, the
    82|     minion does not execute this function, it is executed by the master. Thus,
    83|     no access control is enabled, as minions cannot initiate publishes
    84|     themselves.
    85|     Salt-ssh publishes will default to whichever roster was used for the
    86|     initiating salt-ssh call, and can be overridden using the ``roster``
    87|     argument
    88|     Returners are not currently supported
    89|     The tgt_type argument is used to pass a target other than a glob into
    90|     the execution, the available options are:
    91|     - glob
    92|     - pcre
    93|     .. versionchanged:: 2017.7.0
    94|         The ``expr_form`` argument has been renamed to ``tgt_type``, earlier
    95|         releases must use ``expr_form``.
    96|     The arguments sent to the minion publish function are separated with
    97|     commas. This means that for a minion executing a command with multiple
    98|     args it will look like this:
    99|     .. code-block:: bash
   100|         salt-ssh system.example.com publish.publish '*' user.add 'foo,1020,1020'
   101|         salt-ssh system.example.com publish.publish '127.0.0.1' network.interfaces '' roster=scan
   102|     CLI Example:
   103|     .. code-block:: bash
   104|         salt-ssh system.example.com publish.publish '*' cmd.run 'ls -la /tmp'
   105|     .. admonition:: Attention
   106|         If you need to pass a value to a function argument and that value
   107|         contains an equal sign, you **must** include the argument name.
   108|         For example:
   109|         .. code-block:: bash
   110|             salt-ssh '*' publish.publish test.kwarg arg='cheese=spam'
   111|         Multiple keyword arguments should be passed as a list.
   112|         .. code-block:: bash
   113|             salt-ssh '*' publish.publish test.kwarg arg="['cheese=spam','spam=cheese']"
   114|     """
   115|     return _publish(
   116|         tgt,
   117|         fun,
   118|         arg=arg,
   119|         tgt_type=tgt_type,
   120|         returner=returner,
   121|         timeout=timeout,
   122|         form="clean",
   123|         roster=roster,
   124|     )
   125| def full_data(tgt, fun, arg=None, tgt_type="glob", returner="", timeout=5, roster=None):
   126|     """
   127|     Return the full data about the publication, this is invoked in the same
   128|     way as the publish function
   129|     CLI Example:
   130|     .. code-block:: bash
   131|         salt-ssh system.example.com publish.full_data '*' cmd.run 'ls -la /tmp'
   132|     .. admonition:: Attention
   133|         If you need to pass a value to a function argument and that value
   134|         contains an equal sign, you **must** include the argument name.
   135|         For example:
   136|         .. code-block:: bash
   137|             salt-ssh '*' publish.full_data test.kwarg arg='cheese=spam'
   138|     """
   139|     return _publish(
   140|         tgt,
   141|         fun,
   142|         arg=arg,
   143|         tgt_type=tgt_type,
   144|         returner=returner,
   145|         timeout=timeout,
   146|         form="full",
   147|         roster=roster,
   148|     )
   149| def runner(fun, arg=None, timeout=5):
   150|     """
   151|     Execute a runner on the master and return the data from the runnr function
   152|     CLI Example:
   153|     .. code-block:: bash
   154|         salt-ssh '*' publish.runner jobs.lookup_jid 20140916125524463507
   155|     """
   156|     if not isinstance(arg, list):
   157|         arg = [salt.utils.args.yamlify_arg(arg)]
   158|     else:
   159|         arg = [salt.utils.args.yamlify_arg(x) for x in arg]
   160|     if len(arg) == 1 and arg[0] is None:
   161|         arg = []
   162|     runner = salt.runner.RunnerClient(__opts__["__master_opts__"])
   163|     return runner.cmd(fun, arg)


# ====================================================================
# FILE: salt/client/ssh/wrapper/state.py
# Total hunks: 15
# ====================================================================
# --- HUNK 1: Lines 13-97 ---
    13| import salt.state
    14| import salt.utils.args
    15| import salt.utils.data
    16| import salt.utils.files
    17| import salt.utils.hashutils
    18| import salt.utils.jid
    19| import salt.utils.json
    20| import salt.utils.platform
    21| import salt.utils.state
    22| import salt.utils.thin
    23| from salt.exceptions import SaltInvocationError
    24| __func_alias__ = {"apply_": "apply"}
    25| log = logging.getLogger(__name__)
    26| def _ssh_state(chunks, st_kwargs, kwargs, pillar, test=False):
    27|     """
    28|     Function to run a state with the given chunk via salt-ssh
    29|     """
    30|     file_refs = salt.client.ssh.state.lowstate_file_refs(
    31|         chunks,
    32|         _merge_extra_filerefs(
    33|             kwargs.get("extra_filerefs", ""), __opts__.get("extra_filerefs", "")
    34|         ),
    35|     )
    36|     trans_tar = salt.client.ssh.state.prep_trans_tar(
    37|         __context__["fileclient"],
    38|         chunks,
    39|         file_refs,
    40|         pillar,
    41|         st_kwargs["id_"],
    42|     )
    43|     trans_tar_sum = salt.utils.hashutils.get_hash(trans_tar, __opts__["hash_type"])
    44|     cmd = "state.pkg {}/salt_state.tgz test={} pkg_sum={} hash_type={}".format(
    45|         __opts__["thin_dir"], test, trans_tar_sum, __opts__["hash_type"]
    46|     )
    47|     single = salt.client.ssh.Single(
    48|         __opts__,
    49|         cmd,
    50|         fsclient=__context__["fileclient"],
    51|         minion_opts=__salt__.minion_opts,
    52|         **st_kwargs,
    53|     )
    54|     single.shell.send(trans_tar, "{}/salt_state.tgz".format(__opts__["thin_dir"]))
    55|     stdout, stderr, _ = single.cmd_block()
    56|     try:
    57|         os.remove(trans_tar)
    58|     except OSError:
    59|         pass
    60|     try:
    61|         return salt.utils.data.decode(
    62|             salt.utils.json.loads(stdout, object_hook=salt.utils.data.encode_dict)
    63|         )
    64|     except Exception as e:  # pylint: disable=broad-except
    65|         log.error("JSON Render failed for: %s\n%s", stdout, stderr)
    66|         log.error(str(e))
    67|     return salt.utils.data.decode(stdout)
    68| def _set_retcode(ret, highstate=None):
    69|     """
    70|     Set the return code based on the data back from the state system
    71|     """
    72|     __context__["retcode"] = salt.defaults.exitcodes.EX_OK
    73|     if isinstance(ret, list):
    74|         __context__["retcode"] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR
    75|         return
    76|     if not salt.utils.state.check_result(ret, highstate=highstate):
    77|         __context__["retcode"] = salt.defaults.exitcodes.EX_STATE_FAILURE
    78| def _check_pillar(kwargs, pillar=None):
    79|     """
    80|     Check the pillar for errors, refuse to run the state if there are errors
    81|     in the pillar and return the pillar errors
    82|     """
    83|     if kwargs.get("force"):
    84|         return True
    85|     pillar_dict = pillar if pillar is not None else __pillar__.value()
    86|     if "_errors" in pillar_dict:
    87|         return False
    88|     return True
    89| def _wait(jid):
    90|     """
    91|     Wait for all previously started state jobs to finish running
    92|     """
    93|     if jid is None:
    94|         jid = salt.utils.jid.gen_jid(__opts__)
    95|     states = _prior_running_states(jid)
    96|     while states:
    97|         time.sleep(1)

# --- HUNK 2: Lines 174-250 ---
   174|             if "__exclude__" in high_data:
   175|                 high_data["__exclude__"].extend(exclude)
   176|             else:
   177|                 high_data["__exclude__"] = exclude
   178|         high_data, ext_errors = st_.state.reconcile_extend(high_data)
   179|         errors += ext_errors
   180|         errors += st_.state.verify_high(high_data)
   181|         if errors:
   182|             __context__["retcode"] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR
   183|             return errors
   184|         high_data, req_in_errors = st_.state.requisite_in(high_data)
   185|         errors += req_in_errors
   186|         high_data = st_.state.apply_exclude(high_data)
   187|         if errors:
   188|             __context__["retcode"] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR
   189|             return errors
   190|         chunks = st_.state.compile_high_data(high_data)
   191|         file_refs = salt.client.ssh.state.lowstate_file_refs(
   192|             chunks,
   193|             _merge_extra_filerefs(
   194|                 kwargs.get("extra_filerefs", ""), opts.get("extra_filerefs", "")
   195|             ),
   196|         )
   197|         roster = salt.roster.Roster(opts, opts.get("roster", "flat"))
   198|         roster_grains = roster.opts["grains"]
   199|         _cleanup_slsmod_low_data(chunks)
   200|         trans_tar = salt.client.ssh.state.prep_trans_tar(
   201|             __context__["fileclient"],
   202|             chunks,
   203|             file_refs,
   204|             pillar,
   205|             st_kwargs["id_"],
   206|             roster_grains,
   207|         )
   208|         trans_tar_sum = salt.utils.hashutils.get_hash(trans_tar, opts["hash_type"])
   209|         cmd = "state.pkg {}/salt_state.tgz test={} pkg_sum={} hash_type={}".format(
   210|             opts["thin_dir"], test, trans_tar_sum, opts["hash_type"]
   211|         )
   212|         single = salt.client.ssh.Single(
   213|             opts,
   214|             cmd,
   215|             fsclient=__context__["fileclient"],
   216|             minion_opts=__salt__.minion_opts,
   217|             **st_kwargs,
   218|         )
   219|         single.shell.send(trans_tar, "{}/salt_state.tgz".format(opts["thin_dir"]))
   220|         stdout, stderr, _ = single.cmd_block()
   221|         try:
   222|             os.remove(trans_tar)
   223|         except OSError:
   224|             pass
   225|         try:
   226|             return salt.utils.json.loads(stdout)
   227|         except Exception as e:  # pylint: disable=broad-except
   228|             log.error("JSON Render failed for: %s\n%s", stdout, stderr)
   229|             log.error(str(e))
   230|         return stdout
   231| def running(concurrent=False):
   232|     """
   233|     Return a list of strings that contain state return data if a state function
   234|     is already running. This function is used to prevent multiple state calls
   235|     from being run at the same time.
   236|     CLI Example:
   237|     .. code-block:: bash
   238|         salt '*' state.running
   239|     """
   240|     ret = []
   241|     if concurrent:
   242|         return ret
   243|     active = __salt__["saltutil.is_running"]("state.*")
   244|     for data in active:
   245|         err = (
   246|             'The function "{}" is running as PID {} and was started at {} '
   247|             "with jid {}".format(
   248|                 data["fun"],
   249|                 data["pid"],
   250|                 salt.utils.jid.jid_to_time(data["jid"]),

# --- HUNK 3: Lines 294-369 ---
   294|     __opts__["grains"] = __grains__.value()
   295|     chunks = [data]
   296|     with salt.client.ssh.state.SSHHighState(
   297|         __opts__,
   298|         None,
   299|         __salt__.value(),
   300|         __context__["fileclient"],
   301|         context=__context__.value(),
   302|         initial_pillar=__pillar__.value(),
   303|     ) as st_:
   304|         for chunk in chunks:
   305|             chunk["__id__"] = (
   306|                 chunk["name"] if not chunk.get("__id__") else chunk["__id__"]
   307|             )
   308|         err = st_.state.verify_data(data)
   309|         if err:
   310|             return err
   311|         file_refs = salt.client.ssh.state.lowstate_file_refs(
   312|             chunks,
   313|             _merge_extra_filerefs(
   314|                 kwargs.get("extra_filerefs", ""), __opts__.get("extra_filerefs", "")
   315|             ),
   316|         )
   317|         roster = salt.roster.Roster(__opts__, __opts__.get("roster", "flat"))
   318|         roster_grains = roster.opts["grains"]
   319|         trans_tar = salt.client.ssh.state.prep_trans_tar(
   320|             __context__["fileclient"],
   321|             chunks,
   322|             file_refs,
   323|             __pillar__.value(),
   324|             st_kwargs["id_"],
   325|             roster_grains,
   326|         )
   327|         trans_tar_sum = salt.utils.hashutils.get_hash(trans_tar, __opts__["hash_type"])
   328|         cmd = "state.pkg {}/salt_state.tgz pkg_sum={} hash_type={}".format(
   329|             __opts__["thin_dir"], trans_tar_sum, __opts__["hash_type"]
   330|         )
   331|         single = salt.client.ssh.Single(
   332|             __opts__,
   333|             cmd,
   334|             fsclient=__context__["fileclient"],
   335|             minion_opts=__salt__.minion_opts,
   336|             **st_kwargs,
   337|         )
   338|         single.shell.send(trans_tar, "{}/salt_state.tgz".format(__opts__["thin_dir"]))
   339|         stdout, stderr, _ = single.cmd_block()
   340|         try:
   341|             os.remove(trans_tar)
   342|         except OSError:
   343|             pass
   344|         try:
   345|             return salt.utils.json.loads(stdout)
   346|         except Exception as e:  # pylint: disable=broad-except
   347|             log.error("JSON Render failed for: %s\n%s", stdout, stderr)
   348|             log.error(str(e))
   349|         return stdout
   350| def _get_test_value(test=None, **kwargs):
   351|     """
   352|     Determine the correct value for the test flag.
   353|     """
   354|     ret = True
   355|     if test is None:
   356|         if salt.utils.args.test_mode(test=test, **kwargs):
   357|             ret = True
   358|         else:
   359|             ret = __opts__.get("test", None)
   360|     else:
   361|         ret = test
   362|     return ret
   363| def high(data, **kwargs):
   364|     """
   365|     Execute the compound calls stored in a single set of high data
   366|     This function is mostly intended for testing the state system
   367|     CLI Example:
   368|     .. code-block:: bash
   369|         salt '*' state.high '{"vim": {"pkg": ["installed"]}}'

# --- HUNK 4: Lines 375-451 ---
   375|     initial_pillar = _get_initial_pillar(opts)
   376|     with salt.client.ssh.state.SSHHighState(
   377|         opts,
   378|         pillar_override,
   379|         __salt__.value(),
   380|         __context__["fileclient"],
   381|         context=__context__.value(),
   382|         initial_pillar=initial_pillar,
   383|     ) as st_:
   384|         try:
   385|             pillar = st_.opts["pillar"].value()
   386|         except AttributeError:
   387|             pillar = st_.opts["pillar"]
   388|         if pillar_override is not None or initial_pillar is None:
   389|             __pillar__.update(pillar)
   390|         st_.push_active()
   391|         chunks = st_.state.compile_high_data(data)
   392|         file_refs = salt.client.ssh.state.lowstate_file_refs(
   393|             chunks,
   394|             _merge_extra_filerefs(
   395|                 kwargs.get("extra_filerefs", ""), opts.get("extra_filerefs", "")
   396|             ),
   397|         )
   398|         roster = salt.roster.Roster(opts, opts.get("roster", "flat"))
   399|         roster_grains = roster.opts["grains"]
   400|         _cleanup_slsmod_low_data(chunks)
   401|         trans_tar = salt.client.ssh.state.prep_trans_tar(
   402|             __context__["fileclient"],
   403|             chunks,
   404|             file_refs,
   405|             pillar,
   406|             st_kwargs["id_"],
   407|             roster_grains,
   408|         )
   409|         trans_tar_sum = salt.utils.hashutils.get_hash(trans_tar, opts["hash_type"])
   410|         cmd = "state.pkg {}/salt_state.tgz pkg_sum={} hash_type={}".format(
   411|             opts["thin_dir"], trans_tar_sum, opts["hash_type"]
   412|         )
   413|         single = salt.client.ssh.Single(
   414|             opts,
   415|             cmd,
   416|             fsclient=__context__["fileclient"],
   417|             minion_opts=__salt__.minion_opts,
   418|             **st_kwargs,
   419|         )
   420|         single.shell.send(trans_tar, "{}/salt_state.tgz".format(opts["thin_dir"]))
   421|         stdout, stderr, _ = single.cmd_block()
   422|         try:
   423|             os.remove(trans_tar)
   424|         except OSError:
   425|             pass
   426|         try:
   427|             return salt.utils.json.loads(stdout)
   428|         except Exception as e:  # pylint: disable=broad-except
   429|             log.error("JSON Render failed for: %s\n%s", stdout, stderr)
   430|             log.error(str(e))
   431|         return stdout
   432| def apply_(mods=None, **kwargs):
   433|     """
   434|     .. versionadded:: 2015.5.3
   435|     Apply states! This function will call highstate or state.sls based on the
   436|     arguments passed in, state.apply is intended to be the main gateway for
   437|     all state executions.
   438|     CLI Example:
   439|     .. code-block:: bash
   440|         salt '*' state.apply
   441|         salt '*' state.apply test
   442|         salt '*' state.apply test,pkgs
   443|     """
   444|     if mods:
   445|         return sls(mods, **kwargs)
   446|     return highstate(**kwargs)
   447| def request(mods=None, **kwargs):
   448|     """
   449|     .. versionadded:: 2017.7.3
   450|     Request that the local admin execute a state run via
   451|     `salt-call state.run_request`

# --- HUNK 5: Lines 577-657 ---
   577|         __context__["fileclient"],
   578|         context=__context__.value(),
   579|         initial_pillar=initial_pillar,
   580|     ) as st_:
   581|         if not _check_pillar(kwargs, st_.opts["pillar"]):
   582|             __context__["retcode"] = salt.defaults.exitcodes.EX_PILLAR_FAILURE
   583|             err = ["Pillar failed to render with the following messages:"]
   584|             err += st_.opts["pillar"]["_errors"]
   585|             return err
   586|         try:
   587|             pillar = st_.opts["pillar"].value()
   588|         except AttributeError:
   589|             pillar = st_.opts["pillar"]
   590|         if pillar_override is not None or initial_pillar is None:
   591|             __pillar__.update(pillar)
   592|         st_.push_active()
   593|         chunks = st_.compile_low_chunks(context=__context__.value())
   594|         file_refs = salt.client.ssh.state.lowstate_file_refs(
   595|             chunks,
   596|             _merge_extra_filerefs(
   597|                 kwargs.get("extra_filerefs", ""), opts.get("extra_filerefs", "")
   598|             ),
   599|         )
   600|         for chunk in chunks:
   601|             if not isinstance(chunk, dict):
   602|                 __context__["retcode"] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR
   603|                 return chunks
   604|         roster = salt.roster.Roster(opts, opts.get("roster", "flat"))
   605|         roster_grains = roster.opts["grains"]
   606|         _cleanup_slsmod_low_data(chunks)
   607|         trans_tar = salt.client.ssh.state.prep_trans_tar(
   608|             __context__["fileclient"],
   609|             chunks,
   610|             file_refs,
   611|             pillar,
   612|             st_kwargs["id_"],
   613|             roster_grains,
   614|         )
   615|         trans_tar_sum = salt.utils.hashutils.get_hash(trans_tar, opts["hash_type"])
   616|         cmd = "state.pkg {}/salt_state.tgz test={} pkg_sum={} hash_type={}".format(
   617|             opts["thin_dir"], test, trans_tar_sum, opts["hash_type"]
   618|         )
   619|         single = salt.client.ssh.Single(
   620|             opts,
   621|             cmd,
   622|             fsclient=__context__["fileclient"],
   623|             minion_opts=__salt__.minion_opts,
   624|             **st_kwargs,
   625|         )
   626|         single.shell.send(trans_tar, "{}/salt_state.tgz".format(opts["thin_dir"]))
   627|         stdout, stderr, _ = single.cmd_block()
   628|         try:
   629|             os.remove(trans_tar)
   630|         except OSError:
   631|             pass
   632|         try:
   633|             return salt.utils.json.loads(stdout)
   634|         except Exception as e:  # pylint: disable=broad-except
   635|             log.error("JSON Render failed for: %s\n%s", stdout, stderr)
   636|             log.error(str(e))
   637|         return stdout
   638| def top(topfn, test=None, **kwargs):
   639|     """
   640|     Execute a specific top file instead of the default
   641|     CLI Example:
   642|     .. code-block:: bash
   643|         salt '*' state.top reverse_top.sls
   644|         salt '*' state.top reverse_top.sls exclude=sls_to_exclude
   645|         salt '*' state.top reverse_top.sls exclude="[{'id': 'id_to_exclude'}, {'sls': 'sls_to_exclude'}]"
   646|     """
   647|     st_kwargs = __salt__.kwargs
   648|     __opts__["grains"] = __grains__.value()
   649|     opts = salt.utils.state.get_sls_opts(__opts__, **kwargs)
   650|     opts["test"] = _get_test_value(test, **kwargs)
   651|     pillar_override = kwargs.get("pillar")
   652|     initial_pillar = _get_initial_pillar(opts)
   653|     with salt.client.ssh.state.SSHHighState(
   654|         opts,
   655|         pillar_override,
   656|         __salt__.value(),
   657|         __context__["fileclient"],

# --- HUNK 6: Lines 662-738 ---
   662|             __context__["retcode"] = salt.defaults.exitcodes.EX_PILLAR_FAILURE
   663|             err = ["Pillar failed to render with the following messages:"]
   664|             err += st_.opts["pillar"]["_errors"]
   665|             return err
   666|         try:
   667|             pillar = st_.opts["pillar"].value()
   668|         except AttributeError:
   669|             pillar = st_.opts["pillar"]
   670|         if pillar_override is not None or initial_pillar is None:
   671|             __pillar__.update(pillar)
   672|         st_.opts["state_top"] = os.path.join("salt://", topfn)
   673|         st_.push_active()
   674|         chunks = st_.compile_low_chunks(context=__context__.value())
   675|         for chunk in chunks:
   676|             if not isinstance(chunk, dict):
   677|                 __context__["retcode"] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR
   678|                 return chunks
   679|         file_refs = salt.client.ssh.state.lowstate_file_refs(
   680|             chunks,
   681|             _merge_extra_filerefs(
   682|                 kwargs.get("extra_filerefs", ""), opts.get("extra_filerefs", "")
   683|             ),
   684|         )
   685|         roster = salt.roster.Roster(opts, opts.get("roster", "flat"))
   686|         roster_grains = roster.opts["grains"]
   687|         _cleanup_slsmod_low_data(chunks)
   688|         trans_tar = salt.client.ssh.state.prep_trans_tar(
   689|             __context__["fileclient"],
   690|             chunks,
   691|             file_refs,
   692|             pillar,
   693|             st_kwargs["id_"],
   694|             roster_grains,
   695|         )
   696|         trans_tar_sum = salt.utils.hashutils.get_hash(trans_tar, opts["hash_type"])
   697|         cmd = "state.pkg {}/salt_state.tgz test={} pkg_sum={} hash_type={}".format(
   698|             opts["thin_dir"], test, trans_tar_sum, opts["hash_type"]
   699|         )
   700|         single = salt.client.ssh.Single(
   701|             opts,
   702|             cmd,
   703|             fsclient=__context__["fileclient"],
   704|             minion_opts=__salt__.minion_opts,
   705|             **st_kwargs,
   706|         )
   707|         single.shell.send(trans_tar, "{}/salt_state.tgz".format(opts["thin_dir"]))
   708|         stdout, stderr, _ = single.cmd_block()
   709|         try:
   710|             os.remove(trans_tar)
   711|         except OSError:
   712|             pass
   713|         try:
   714|             return salt.utils.json.loads(stdout)
   715|         except Exception as e:  # pylint: disable=broad-except
   716|             log.error("JSON Render failed for: %s\n%s", stdout, stderr)
   717|             log.error(str(e))
   718|         return stdout
   719| def show_highstate(**kwargs):
   720|     """
   721|     Retrieve the highstate data from the salt master and display it
   722|     CLI Example:
   723|     .. code-block:: bash
   724|         salt '*' state.show_highstate
   725|     """
   726|     __opts__["grains"] = __grains__.value()
   727|     opts = salt.utils.state.get_sls_opts(__opts__, **kwargs)
   728|     pillar_override = kwargs.get("pillar")
   729|     initial_pillar = _get_initial_pillar(opts)
   730|     with salt.client.ssh.state.SSHHighState(
   731|         opts,
   732|         pillar_override,
   733|         __salt__,
   734|         __context__["fileclient"],
   735|         context=__context__.value(),
   736|         initial_pillar=initial_pillar,
   737|     ) as st_:
   738|         if not _check_pillar(kwargs, st_.opts["pillar"]):

# --- HUNK 7: Lines 835-875 ---
   835|         st_.push_active()
   836|         high_, errors = st_.render_highstate(
   837|             {opts["saltenv"]: split_mods}, context=__context__.value()
   838|         )
   839|         errors += st_.state.verify_high(high_)
   840|         high_, req_in_errors = st_.state.requisite_in(high_)
   841|         if req_in_errors:
   842|             errors.extend(req_in_errors)
   843|         if errors:
   844|             __context__["retcode"] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR
   845|             return errors
   846|         chunks = st_.state.compile_high_data(high_)
   847|         chunk = [x for x in chunks if x.get("__id__", "") == id_]
   848|         if not chunk:
   849|             raise SaltInvocationError(
   850|                 "No matches for ID '{}' found in SLS '{}' within saltenv '{}'".format(
   851|                     id_, mods, opts["saltenv"]
   852|                 )
   853|             )
   854|         ret = _ssh_state(chunk, st_kwargs, kwargs, pillar, test=test)
   855|         _set_retcode(ret, highstate=highstate)
   856|         __opts__["test"] = orig_test
   857|         return ret
   858| def show_sls(mods, saltenv="base", test=None, **kwargs):
   859|     """
   860|     Display the state data from a specific sls or list of sls files on the
   861|     master
   862|     CLI Example:
   863|     .. code-block:: bash
   864|         salt '*' state.show_sls core,edit.vim dev
   865|     """
   866|     __opts__["grains"] = __grains__.value()
   867|     opts = salt.utils.state.get_sls_opts(__opts__, **kwargs)
   868|     opts["test"] = _get_test_value(test, **kwargs)
   869|     pillar_override = kwargs.get("pillar")
   870|     initial_pillar = _get_initial_pillar(opts)
   871|     with salt.client.ssh.state.SSHHighState(
   872|         opts,
   873|         pillar_override,
   874|         __salt__,
   875|         __context__["fileclient"],

# --- HUNK 8: Lines 1005-1070 ---
  1005|         return "Invalid function passed"
  1006|     kwargs.update({"state": comps[0], "fun": comps[1], "__id__": name, "name": name})
  1007|     opts = salt.utils.state.get_sls_opts(__opts__, **kwargs)
  1008|     opts["test"] = _get_test_value(test, **kwargs)
  1009|     pillar_override = kwargs.pop("pillar", None)
  1010|     st_ = salt.client.ssh.state.SSHState(
  1011|         opts, pillar_override, initial_pillar=_get_initial_pillar(opts)
  1012|     )
  1013|     try:
  1014|         pillar = st_.opts["pillar"].value()
  1015|     except AttributeError:
  1016|         pillar = st_.opts["pillar"]
  1017|     err = st_.verify_data(kwargs)
  1018|     if err:
  1019|         __context__["retcode"] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR
  1020|         return err
  1021|     chunks = [kwargs]
  1022|     file_refs = salt.client.ssh.state.lowstate_file_refs(
  1023|         chunks,
  1024|         _merge_extra_filerefs(
  1025|             kwargs.get("extra_filerefs", ""), opts.get("extra_filerefs", "")
  1026|         ),
  1027|     )
  1028|     roster = salt.roster.Roster(opts, opts.get("roster", "flat"))
  1029|     roster_grains = roster.opts["grains"]
  1030|     trans_tar = salt.client.ssh.state.prep_trans_tar(
  1031|         __context__["fileclient"],
  1032|         chunks,
  1033|         file_refs,
  1034|         pillar,
  1035|         st_kwargs["id_"],
  1036|         roster_grains,
  1037|     )
  1038|     trans_tar_sum = salt.utils.hashutils.get_hash(trans_tar, opts["hash_type"])
  1039|     cmd = "state.pkg {}/salt_state.tgz test={} pkg_sum={} hash_type={}".format(
  1040|         opts["thin_dir"], test, trans_tar_sum, opts["hash_type"]
  1041|     )
  1042|     single = salt.client.ssh.Single(
  1043|         opts,
  1044|         cmd,
  1045|         fsclient=__context__["fileclient"],
  1046|         minion_opts=__salt__.minion_opts,
  1047|         **st_kwargs,
  1048|     )
  1049|     single.shell.send(trans_tar, "{}/salt_state.tgz".format(opts["thin_dir"]))
  1050|     stdout, stderr, _ = single.cmd_block()
  1051|     try:
  1052|         os.remove(trans_tar)
  1053|     except OSError:
  1054|         pass
  1055|     try:
  1056|         return salt.utils.json.loads(stdout)
  1057|     except Exception as e:  # pylint: disable=broad-except
  1058|         log.error("JSON Render failed for: %s\n%s", stdout, stderr)
  1059|         log.error(str(e))
  1060|     return stdout
  1061| def test(*args, **kwargs):
  1062|     """
  1063|     .. versionadded:: 3001
  1064|     Alias for `state.apply` with the kwarg `test` forced to `True`.
  1065|     This is a nicety to avoid the need to type out `test=True` and the possibility of
  1066|     a typo causing changes you do not intend.
  1067|     """
  1068|     kwargs["test"] = True
  1069|     ret = apply_(*args, **kwargs)
  1070|     return ret


# ====================================================================
# FILE: salt/cloud/__init__.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 15-54 ---
    15| import salt.config
    16| import salt.loader
    17| import salt.syspaths
    18| import salt.utils.args
    19| import salt.utils.cloud
    20| import salt.utils.context
    21| import salt.utils.crypt
    22| import salt.utils.data
    23| import salt.utils.dictupdate
    24| import salt.utils.files
    25| import salt.utils.user
    26| import salt.utils.verify
    27| import salt.utils.yaml
    28| from salt.exceptions import (
    29|     SaltCloudConfigError,
    30|     SaltCloudException,
    31|     SaltCloudNotFound,
    32|     SaltCloudSystemExit,
    33| )
    34| from salt.template import compile_template
    35| log = logging.getLogger(__name__)
    36| def communicator(func):
    37|     """Warning, this is a picklable decorator !"""
    38|     def _call(queue, args, kwargs):
    39|         """called with [queue, args, kwargs] as first optional arg"""
    40|         kwargs["queue"] = queue
    41|         ret = None
    42|         try:
    43|             ret = func(*args, **kwargs)
    44|             queue.put("END")
    45|         except KeyboardInterrupt as ex:
    46|             trace = traceback.format_exc()
    47|             queue.put("KEYBOARDINT")
    48|             queue.put("Keyboard interrupt")
    49|             queue.put(f"{ex}\n{trace}\n")
    50|         except Exception as ex:  # pylint: disable=broad-except
    51|             trace = traceback.format_exc()
    52|             queue.put("ERROR")
    53|             queue.put("Exception")
    54|             queue.put(f"{ex}\n{trace}\n")

# --- HUNK 2: Lines 1047-1092 ---
  1047|         except KeyError:
  1048|             opt_map = False
  1049|         if self.opts["parallel"] and self.opts["start_action"] and not opt_map:
  1050|             log.info("Running %s on %s", self.opts["start_action"], vm_["name"])
  1051|             with salt.client.get_local_client(mopts=self.opts) as client:
  1052|                 action_out = client.cmd(
  1053|                     vm_["name"],
  1054|                     self.opts["start_action"],
  1055|                     timeout=self.opts["timeout"] * 60,
  1056|                 )
  1057|             output["ret"] = action_out
  1058|         return output
  1059|     @staticmethod
  1060|     def vm_config(name, main, provider, profile, overrides):
  1061|         """
  1062|         Create vm config.
  1063|         :param str name: The name of the vm
  1064|         :param dict main: The main cloud config
  1065|         :param dict provider: The provider config
  1066|         :param dict profile: The profile config
  1067|         :param dict overrides: The vm's config overrides
  1068|         """
  1069|         vm = main.copy()
  1070|         vm = salt.utils.dictupdate.update(vm, provider)
  1071|         vm = salt.utils.dictupdate.update(vm, profile)
  1072|         vm.update(overrides)
  1073|         vm["name"] = name
  1074|         return vm
  1075|     def extras(self, extra_):
  1076|         """
  1077|         Extra actions
  1078|         """
  1079|         output = {}
  1080|         alias, driver = extra_["provider"].split(":")
  1081|         fun = "{}.{}".format(driver, extra_["action"])
  1082|         if fun not in self.clouds:
  1083|             log.error(
  1084|                 "Creating '%s' using '%s' as the provider "
  1085|                 "cannot complete since '%s' is not available",
  1086|                 extra_["name"],
  1087|                 extra_["provider"],
  1088|                 driver,
  1089|             )
  1090|             return
  1091|         try:
  1092|             with salt.utils.context.func_globals_inject(

# --- HUNK 3: Lines 1845-1933 ---
  1845|                             )
  1846|                         )
  1847|                 for obj in output_multip:
  1848|                     next(iter(obj.values()))["ret"] = out[next(iter(obj.keys()))]
  1849|                     output.update(obj)
  1850|             else:
  1851|                 for obj in output_multip:
  1852|                     output.update(obj)
  1853|         return output
  1854| def init_pool_worker():
  1855|     """
  1856|     Make every worker ignore KeyboarInterrup's since it will be handled by the
  1857|     parent process.
  1858|     """
  1859|     signal.signal(signal.SIGINT, signal.SIG_IGN)
  1860| def create_multiprocessing(parallel_data, queue=None):
  1861|     """
  1862|     This function will be called from another process when running a map in
  1863|     parallel mode. The result from the create is always a json object.
  1864|     """
  1865|     parallel_data["opts"]["output"] = "json"
  1866|     cloud = Cloud(parallel_data["opts"])
  1867|     try:
  1868|         output = cloud.create(
  1869|             parallel_data["profile"], local_master=parallel_data["local_master"]
  1870|         )
  1871|     except SaltCloudException as exc:
  1872|         log.error(
  1873|             "Failed to deploy '%s'. Error: %s",
  1874|             parallel_data["name"],
  1875|             exc,
  1876|             exc_info_on_loglevel=logging.DEBUG,
  1877|         )
  1878|         return {parallel_data["name"]: {"Error": str(exc)}}
  1879|     if parallel_data["opts"].get("show_deploy_args", False) is False and isinstance(
  1880|         output, dict
  1881|     ):
  1882|         output.pop("deploy_kwargs", None)
  1883|     return {parallel_data["name"]: salt.utils.data.simple_types_filter(output)}
  1884| def destroy_multiprocessing(parallel_data, queue=None):
  1885|     """
  1886|     This function will be called from another process when running a map in
  1887|     parallel mode. The result from the destroy is always a json object.
  1888|     """
  1889|     parallel_data["opts"]["output"] = "json"
  1890|     clouds = salt.loader.clouds(parallel_data["opts"])
  1891|     try:
  1892|         fun = clouds["{}.destroy".format(parallel_data["driver"])]
  1893|         with salt.utils.context.func_globals_inject(
  1894|             fun,
  1895|             __active_provider_name__=":".join(
  1896|                 [parallel_data["alias"], parallel_data["driver"]]
  1897|             ),
  1898|         ):
  1899|             output = fun(parallel_data["name"])
  1900|     except SaltCloudException as exc:
  1901|         log.error(
  1902|             "Failed to destroy %s. Error: %s",
  1903|             parallel_data["name"],
  1904|             exc,
  1905|             exc_info_on_loglevel=logging.DEBUG,
  1906|         )
  1907|         return {parallel_data["name"]: {"Error": str(exc)}}
  1908|     return {parallel_data["name"]: salt.utils.data.simple_types_filter(output)}
  1909| def run_parallel_map_providers_query(data, queue=None):
  1910|     """
  1911|     This function will be called from another process when building the
  1912|     providers map.
  1913|     """
  1914|     cloud = Cloud(data["opts"])
  1915|     try:
  1916|         with salt.utils.context.func_globals_inject(
  1917|             cloud.clouds[data["fun"]],
  1918|             __active_provider_name__=":".join([data["alias"], data["driver"]]),
  1919|         ):
  1920|             return (
  1921|                 data["alias"],
  1922|                 data["driver"],
  1923|                 salt.utils.data.simple_types_filter(cloud.clouds[data["fun"]]()),
  1924|             )
  1925|     except Exception as err:  # pylint: disable=broad-except
  1926|         log.debug(
  1927|             "Failed to execute '%s()' while querying for running nodes: %s",
  1928|             data["fun"],
  1929|             err,
  1930|             exc_info_on_loglevel=logging.DEBUG,
  1931|         )
  1932|         return data["alias"], data["driver"], ()
  1933| def _run_parallel_map_providers_query(*args, **kw):


# ====================================================================
# FILE: salt/cloud/clouds/azurearm.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-1707 ---
     1| """
     2| Azure ARM Cloud Module
     3| ======================
     4| .. versionadded:: 2016.11.0
     5| .. versionchanged:: 2019.2.0
     6| The Azure ARM cloud module is used to control access to Microsoft Azure Resource Manager
     7| .. warning::
     8|     This cloud provider will be removed from Salt in version 3007 in favor of
     9|     the `saltext.azurerm Salt Extension
    10|     <https://github.com/salt-extensions/saltext-azurerm>`_
    11| :maintainer: <devops@eitr.tech>
    12| :depends:
    13|     * `azure <https://pypi.python.org/pypi/azure>`_ >= 2.0.0rc6
    14|     * `azure-common <https://pypi.python.org/pypi/azure-common>`_ >= 1.1.4
    15|     * `azure-mgmt <https://pypi.python.org/pypi/azure-mgmt>`_ >= 0.30.0rc6
    16|     * `azure-mgmt-compute <https://pypi.python.org/pypi/azure-mgmt-compute>`_ >= 0.33.0
    17|     * `azure-mgmt-network <https://pypi.python.org/pypi/azure-mgmt-network>`_ >= 0.30.0rc6
    18|     * `azure-mgmt-resource <https://pypi.python.org/pypi/azure-mgmt-resource>`_ >= 0.30.0
    19|     * `azure-mgmt-storage <https://pypi.python.org/pypi/azure-mgmt-storage>`_ >= 0.30.0rc6
    20|     * `azure-mgmt-web <https://pypi.python.org/pypi/azure-mgmt-web>`_ >= 0.30.0rc6
    21|     * `azure-storage <https://pypi.python.org/pypi/azure-storage>`_ >= 0.32.0
    22|     * `msrestazure <https://pypi.python.org/pypi/msrestazure>`_ >= 0.4.21
    23| :configuration:
    24|     Required provider parameters:
    25|     if using username and password:
    26|       * ``subscription_id``
    27|       * ``username``
    28|       * ``password``
    29|     if using a service principal:
    30|       * ``subscription_id``
    31|       * ``tenant``
    32|       * ``client_id``
    33|       * ``secret``
    34|     if using Managed Service Identity authentication:
    35|       * ``subscription_id``
    36|     Optional provider parameters:
    37|     **cloud_environment**: Used to point the cloud driver to different API endpoints, such as Azure GovCloud. Possible values:
    38|       * ``AZURE_PUBLIC_CLOUD`` (default)
    39|       * ``AZURE_CHINA_CLOUD``
    40|       * ``AZURE_US_GOV_CLOUD``
    41|       * ``AZURE_GERMAN_CLOUD``
    42|       * HTTP base URL for a custom endpoint, such as Azure Stack. The ``/metadata/endpoints`` path will be added to the URL.
    43|     **userdata** and **userdata_file**:
    44|       Azure Resource Manager uses a separate VirtualMachineExtension object to pass userdata scripts to the virtual
    45|       machine. Arbitrary shell commands can be passed via the ``userdata`` parameter, or via a file local to the Salt
    46|       Cloud system using the ``userdata_file`` parameter. Note that the local file is not treated as a script by the
    47|       extension, so "one-liners" probably work best. If greater functionality is desired, a web-hosted script file can
    48|       be specified via ``userdata_file: https://raw.githubusercontent.com/account/repo/master/azure-script.py``, which
    49|       will be executed on the system after VM creation. For Windows systems, script files ending in ``.ps1`` will be
    50|       executed with ``powershell.exe``. The ``userdata`` parameter takes precedence over the ``userdata_file`` parameter
    51|       when creating the custom script extension.
    52|     **win_installer**:
    53|       This parameter, which holds the local path to the Salt Minion installer package, is used to determine if the
    54|       virtual machine type will be "Windows". Only set this parameter on profiles which install Windows operating systems.
    55| Example ``/etc/salt/cloud.providers`` or
    56| ``/etc/salt/cloud.providers.d/azure.conf`` configuration:
    57| .. code-block:: yaml
    58|     my-azure-config with username and password:
    59|       driver: azurearm
    60|       subscription_id: 3287abc8-f98a-c678-3bde-326766fd3617
    61|       username: larry
    62|       password: 123pass
    63|     Or my-azure-config with service principal:
    64|       driver: azurearm
    65|       subscription_id: 3287abc8-f98a-c678-3bde-326766fd3617
    66|       tenant: ABCDEFAB-1234-ABCD-1234-ABCDEFABCDEF
    67|       client_id: ABCDEFAB-1234-ABCD-1234-ABCDEFABCDEF
    68|       secret: XXXXXXXXXXXXXXXXXXXXXXXX
    69|       cloud_environment: AZURE_US_GOV_CLOUD
    70|       The Service Principal can be created with the new Azure CLI (https://github.com/Azure/azure-cli) with:
    71|       az ad sp create-for-rbac -n "http://<yourappname>" --role <role> --scopes <scope>
    72|       For example, this creates a service principal with 'owner' role for the whole subscription:
    73|       az ad sp create-for-rbac -n "http://mysaltapp" --role owner --scopes /subscriptions/3287abc8-f98a-c678-3bde-326766fd3617
    74|       *Note: review the details of Service Principals. Owner role is more than you normally need, and you can restrict
    75|       scope to a resource group or individual resources.
    76| """
    77| import importlib
    78| import logging
    79| import os
    80| import os.path
    81| import pprint
    82| import string
    83| import time
    84| from functools import wraps
    85| from multiprocessing import cpu_count
    86| from multiprocessing.pool import ThreadPool
    87| import salt.cache
    88| import salt.config as config
    89| import salt.loader
    90| import salt.utils.azurearm
    91| import salt.utils.cloud
    92| import salt.utils.files
    93| import salt.utils.stringutils
    94| import salt.utils.yaml
    95| import salt.version
    96| from salt.exceptions import (
    97|     SaltCloudConfigError,
    98|     SaltCloudExecutionFailure,
    99|     SaltCloudExecutionTimeout,
   100|     SaltCloudSystemExit,
   101| )
   102| HAS_LIBS = False
   103| try:
   104|     import azure.mgmt.compute.models as compute_models
   105|     import azure.mgmt.network.models as network_models
   106|     from azure.storage.blob.blockblobservice import BlockBlobService
   107|     from msrestazure.azure_exceptions import CloudError
   108|     HAS_LIBS = True
   109| except ImportError:
   110|     pass
   111| __virtualname__ = "azurearm"
   112| log = logging.getLogger(__name__)
   113| def __virtual__():
   114|     """
   115|     Check for Azure configurations.
   116|     """
   117|     if get_configured_provider() is False:
   118|         return False
   119|     if get_dependencies() is False:
   120|         return (
   121|             False,
   122|             "The following dependencies are required to use the AzureARM driver: "
   123|             "Microsoft Azure SDK for Python >= 2.0rc6, "
   124|             "Microsoft Azure Storage SDK for Python >= 0.32, "
   125|             "MS REST Azure (msrestazure) >= 0.4",
   126|         )
   127|     return __virtualname__
   128| def _get_active_provider_name():
   129|     try:
   130|         return __active_provider_name__.value()
   131|     except AttributeError:
   132|         return __active_provider_name__
   133| def _deprecation_message(function):
   134|     """
   135|     Decorator wrapper to warn about msazure deprecation
   136|     """
   137|     @wraps(function)
   138|     def wrapped(*args, **kwargs):
   139|         salt.utils.versions.warn_until(
   140|             "Chlorine",
   141|             "This cloud provider will be removed from Salt in version 3007 due to "
   142|             "the deprecation of the 'Classic' API for Azure. Please migrate to "
   143|             "Azure Resource Manager by March 1, 2023 "
   144|             "(https://docs.microsoft.com/en-us/azure/virtual-machines/classic-vm-deprecation)",
   145|             category=FutureWarning,
   146|         )
   147|         ret = function(*args, **salt.utils.args.clean_kwargs(**kwargs))
   148|         return ret
   149|     return wrapped
   150| @_deprecation_message
   151| def get_api_versions(call=None, kwargs=None):  # pylint: disable=unused-argument
   152|     """
   153|     Get a resource type api versions
   154|     """
   155|     if kwargs is None:
   156|         kwargs = {}
   157|     if "resource_provider" not in kwargs:
   158|         raise SaltCloudSystemExit("A resource_provider must be specified")
   159|     if "resource_type" not in kwargs:
   160|         raise SaltCloudSystemExit("A resource_type must be specified")
   161|     api_versions = []
   162|     try:
   163|         resconn = get_conn(client_type="resource")
   164|         provider_query = resconn.providers.get(
   165|             resource_provider_namespace=kwargs["resource_provider"]
   166|         )
   167|         for resource in provider_query.resource_types:
   168|             if str(resource.resource_type) == kwargs["resource_type"]:
   169|                 resource_dict = resource.as_dict()
   170|                 api_versions = resource_dict["api_versions"]
   171|     except CloudError as exc:
   172|         salt.utils.azurearm.log_cloud_error("resource", exc.message)
   173|     return api_versions
   174| @_deprecation_message
   175| def get_resource_by_id(resource_id, api_version, extract_value=None):
   176|     """
   177|     Get an AzureARM resource by id
   178|     """
   179|     ret = {}
   180|     try:
   181|         resconn = get_conn(client_type="resource")
   182|         resource_query = resconn.resources.get_by_id(
   183|             resource_id=resource_id, api_version=api_version
   184|         )
   185|         resource_dict = resource_query.as_dict()
   186|         if extract_value is not None:
   187|             ret = resource_dict[extract_value]
   188|         else:
   189|             ret = resource_dict
   190|     except CloudError as exc:
   191|         salt.utils.azurearm.log_cloud_error("resource", exc.message)
   192|         ret = {"Error": exc.message}
   193|     return ret
   194| def get_configured_provider():
   195|     """
   196|     Return the first configured provider instance.
   197|     """
   198|     key_combos = [
   199|         ("subscription_id", "tenant", "client_id", "secret"),
   200|         ("subscription_id", "username", "password"),
   201|         ("subscription_id",),
   202|     ]
   203|     for combo in key_combos:
   204|         provider = config.is_provider_configured(
   205|             __opts__,
   206|             _get_active_provider_name() or __virtualname__,
   207|             combo,
   208|         )
   209|         if provider:
   210|             return provider
   211|     return provider
   212| @_deprecation_message
   213| def get_dependencies():
   214|     """
   215|     Warn if dependencies aren't met.
   216|     """
   217|     return config.check_driver_dependencies(__virtualname__, {"azurearm": HAS_LIBS})
   218| @_deprecation_message
   219| def get_conn(client_type):
   220|     """
   221|     Return a connection object for a client type.
   222|     """
   223|     conn_kwargs = {}
   224|     conn_kwargs["subscription_id"] = salt.utils.stringutils.to_str(
   225|         config.get_cloud_config_value(
   226|             "subscription_id", get_configured_provider(), __opts__, search_global=False
   227|         )
   228|     )
   229|     cloud_env = config.get_cloud_config_value(
   230|         "cloud_environment", get_configured_provider(), __opts__, search_global=False
   231|     )
   232|     if cloud_env is not None:
   233|         conn_kwargs["cloud_environment"] = cloud_env
   234|     tenant = config.get_cloud_config_value(
   235|         "tenant", get_configured_provider(), __opts__, search_global=False
   236|     )
   237|     if tenant is not None:
   238|         client_id = config.get_cloud_config_value(
   239|             "client_id", get_configured_provider(), __opts__, search_global=False
   240|         )
   241|         secret = config.get_cloud_config_value(
   242|             "secret", get_configured_provider(), __opts__, search_global=False
   243|         )
   244|         conn_kwargs.update({"client_id": client_id, "secret": secret, "tenant": tenant})
   245|     username = config.get_cloud_config_value(
   246|         "username", get_configured_provider(), __opts__, search_global=False
   247|     )
   248|     if username:
   249|         password = config.get_cloud_config_value(
   250|             "password", get_configured_provider(), __opts__, search_global=False
   251|         )
   252|         conn_kwargs.update({"username": username, "password": password})
   253|     client = salt.utils.azurearm.get_client(client_type=client_type, **conn_kwargs)
   254|     return client
   255| @_deprecation_message
   256| def get_location(call=None, kwargs=None):  # pylint: disable=unused-argument
   257|     """
   258|     Return the location that is configured for this provider
   259|     """
   260|     if not kwargs:
   261|         kwargs = {}
   262|     vm_dict = get_configured_provider()
   263|     vm_dict.update(kwargs)
   264|     return config.get_cloud_config_value(
   265|         "location", vm_dict, __opts__, search_global=False
   266|     )
   267| @_deprecation_message
   268| def avail_locations(call=None):
   269|     """
   270|     Return a dict of all available regions.
   271|     """
   272|     if call == "action":
   273|         raise SaltCloudSystemExit(
   274|             "The avail_locations function must be called with "
   275|             "-f or --function, or with the --list-locations option"
   276|         )
   277|     ret = {}
   278|     ret["locations"] = []
   279|     try:
   280|         resconn = get_conn(client_type="resource")
   281|         provider_query = resconn.providers.get(
   282|             resource_provider_namespace="Microsoft.Compute"
   283|         )
   284|         locations = []
   285|         for resource in provider_query.resource_types:
   286|             if str(resource.resource_type) == "virtualMachines":
   287|                 resource_dict = resource.as_dict()
   288|                 locations = resource_dict["locations"]
   289|         for location in locations:
   290|             lowercase = location.lower().replace(" ", "")
   291|             ret["locations"].append(lowercase)
   292|     except CloudError as exc:
   293|         salt.utils.azurearm.log_cloud_error("resource", exc.message)
   294|         ret = {"Error": exc.message}
   295|     return ret
   296| @_deprecation_message
   297| def avail_images(call=None):
   298|     """
   299|     Return a dict of all available images on the provider
   300|     """
   301|     if call == "action":
   302|         raise SaltCloudSystemExit(
   303|             "The avail_images function must be called with "
   304|             "-f or --function, or with the --list-images option"
   305|         )
   306|     compconn = get_conn(client_type="compute")
   307|     region = get_location()
   308|     publishers = []
   309|     ret = {}
   310|     def _get_publisher_images(publisher):
   311|         """
   312|         Get all images from a specific publisher
   313|         """
   314|         data = {}
   315|         try:
   316|             offers = compconn.virtual_machine_images.list_offers(
   317|                 location=region,
   318|                 publisher_name=publisher,
   319|             )
   320|             for offer_obj in offers:
   321|                 offer = offer_obj.as_dict()
   322|                 skus = compconn.virtual_machine_images.list_skus(
   323|                     location=region,
   324|                     publisher_name=publisher,
   325|                     offer=offer["name"],
   326|                 )
   327|                 for sku_obj in skus:
   328|                     sku = sku_obj.as_dict()
   329|                     results = compconn.virtual_machine_images.list(
   330|                         location=region,
   331|                         publisher_name=publisher,
   332|                         offer=offer["name"],
   333|                         skus=sku["name"],
   334|                     )
   335|                     for version_obj in results:
   336|                         version = version_obj.as_dict()
   337|                         name = "|".join(
   338|                             (
   339|                                 publisher,
   340|                                 offer["name"],
   341|                                 sku["name"],
   342|                                 version["name"],
   343|                             )
   344|                         )
   345|                         data[name] = {
   346|                             "publisher": publisher,
   347|                             "offer": offer["name"],
   348|                             "sku": sku["name"],
   349|                             "version": version["name"],
   350|                         }
   351|         except CloudError as exc:
   352|             salt.utils.azurearm.log_cloud_error("compute", exc.message)
   353|             data = {publisher: exc.message}
   354|         return data
   355|     try:
   356|         publishers_query = compconn.virtual_machine_images.list_publishers(
   357|             location=region
   358|         )
   359|         for publisher_obj in publishers_query:
   360|             publisher = publisher_obj.as_dict()
   361|             publishers.append(publisher["name"])
   362|     except CloudError as exc:
   363|         salt.utils.azurearm.log_cloud_error("compute", exc.message)
   364|     pool = ThreadPool(cpu_count() * 6)
   365|     results = pool.map_async(_get_publisher_images, publishers)
   366|     results.wait()
   367|     ret = {k: v for result in results.get() for k, v in result.items()}
   368|     return ret
   369| @_deprecation_message
   370| def avail_sizes(call=None):
   371|     """
   372|     Return a list of sizes available from the provider
   373|     """
   374|     if call == "action":
   375|         raise SaltCloudSystemExit(
   376|             "The avail_sizes function must be called with "
   377|             "-f or --function, or with the --list-sizes option"
   378|         )
   379|     compconn = get_conn(client_type="compute")
   380|     ret = {}
   381|     location = get_location()
   382|     try:
   383|         sizes = compconn.virtual_machine_sizes.list(location=location)
   384|         for size_obj in sizes:
   385|             size = size_obj.as_dict()
   386|             ret[size["name"]] = size
   387|     except CloudError as exc:
   388|         salt.utils.azurearm.log_cloud_error("compute", exc.message)
   389|         ret = {"Error": exc.message}
   390|     return ret
   391| @_deprecation_message
   392| def list_nodes(call=None):
   393|     """
   394|     List VMs on this Azure account
   395|     """
   396|     if call == "action":
   397|         raise SaltCloudSystemExit(
   398|             "The list_nodes function must be called with -f or --function."
   399|         )
   400|     ret = {}
   401|     nodes = list_nodes_full()
   402|     for node in nodes:
   403|         ret[node] = {"name": node}
   404|         for prop in ("id", "image", "size", "state", "private_ips", "public_ips"):
   405|             ret[node][prop] = nodes[node].get(prop)
   406|     return ret
   407| @_deprecation_message
   408| def list_nodes_full(call=None):
   409|     """
   410|     List all VMs on the subscription with full information
   411|     """
   412|     if call == "action":
   413|         raise SaltCloudSystemExit(
   414|             "The list_nodes_full function must be called with -f or --function."
   415|         )
   416|     netapi_versions = get_api_versions(
   417|         kwargs={
   418|             "resource_provider": "Microsoft.Network",
   419|             "resource_type": "networkInterfaces",
   420|         }
   421|     )
   422|     netapi_version = netapi_versions[0]
   423|     compconn = get_conn(client_type="compute")
   424|     ret = {}
   425|     def _get_node_info(node):
   426|         """
   427|         Get node info.
   428|         """
   429|         node_ret = {}
   430|         node["id"] = node["vm_id"]
   431|         node["size"] = node["hardware_profile"]["vm_size"]
   432|         node["state"] = node["provisioning_state"]
   433|         node["public_ips"] = []
   434|         node["private_ips"] = []
   435|         node_ret[node["name"]] = node
   436|         try:
   437|             image_ref = node["storage_profile"]["image_reference"]
   438|             node["image"] = "|".join(
   439|                 [
   440|                     image_ref["publisher"],
   441|                     image_ref["offer"],
   442|                     image_ref["sku"],
   443|                     image_ref["version"],
   444|                 ]
   445|             )
   446|         except (TypeError, KeyError):
   447|             try:
   448|                 node["image"] = node["storage_profile"]["os_disk"]["image"]["uri"]
   449|             except (TypeError, KeyError):
   450|                 node["image"] = (
   451|                     node.get("storage_profile", {}).get("image_reference", {}).get("id")
   452|                 )
   453|         try:
   454|             netifaces = node["network_profile"]["network_interfaces"]
   455|             for index, netiface in enumerate(netifaces):
   456|                 netiface_name = get_resource_by_id(
   457|                     netiface["id"], netapi_version, "name"
   458|                 )
   459|                 netiface, pubips, privips = _get_network_interface(
   460|                     netiface_name, node["resource_group"]
   461|                 )
   462|                 node["network_profile"]["network_interfaces"][index].update(netiface)
   463|                 node["public_ips"].extend(pubips)
   464|                 node["private_ips"].extend(privips)
   465|         except Exception:  # pylint: disable=broad-except
   466|             pass
   467|         node_ret[node["name"]] = node
   468|         return node_ret
   469|     for group in list_resource_groups():
   470|         nodes = []
   471|         nodes_query = compconn.virtual_machines.list(resource_group_name=group)
   472|         for node_obj in nodes_query:
   473|             node = node_obj.as_dict()
   474|             node["resource_group"] = group
   475|             nodes.append(node)
   476|         pool = ThreadPool(cpu_count() * 6)
   477|         results = pool.map_async(_get_node_info, nodes)
   478|         results.wait()
   479|         group_ret = {k: v for result in results.get() for k, v in result.items()}
   480|         ret.update(group_ret)
   481|     return ret
   482| @_deprecation_message
   483| def list_resource_groups(call=None):
   484|     """
   485|     List resource groups associated with the subscription
   486|     """
   487|     if call == "action":
   488|         raise SaltCloudSystemExit(
   489|             "The list_hosted_services function must be called with -f or --function"
   490|         )
   491|     resconn = get_conn(client_type="resource")
   492|     ret = {}
   493|     try:
   494|         groups = resconn.resource_groups.list()
   495|         for group_obj in groups:
   496|             group = group_obj.as_dict()
   497|             ret[group["name"]] = group
   498|     except CloudError as exc:
   499|         salt.utils.azurearm.log_cloud_error("resource", exc.message)
   500|         ret = {"Error": exc.message}
   501|     return ret
   502| @_deprecation_message
   503| def show_instance(name, call=None):
   504|     """
   505|     Show the details from AzureARM concerning an instance
   506|     """
   507|     if call != "action":
   508|         raise SaltCloudSystemExit(
   509|             "The show_instance action must be called with -a or --action."
   510|         )
   511|     try:
   512|         node = list_nodes_full("function")[name]
   513|     except KeyError:
   514|         log.debug("Failed to get data for node '%s'", name)
   515|         node = {}
   516|     __utils__["cloud.cache_node"](node, _get_active_provider_name(), __opts__)
   517|     return node
   518| @_deprecation_message
   519| def delete_interface(call=None, kwargs=None):  # pylint: disable=unused-argument
   520|     """
   521|     Delete a network interface.
   522|     """
   523|     if kwargs is None:
   524|         kwargs = {}
   525|     netconn = get_conn(client_type="network")
   526|     if kwargs.get("resource_group") is None:
   527|         kwargs["resource_group"] = config.get_cloud_config_value(
   528|             "resource_group", {}, __opts__, search_global=True
   529|         )
   530|     ips = []
   531|     iface = netconn.network_interfaces.get(
   532|         kwargs["resource_group"],
   533|         kwargs["iface_name"],
   534|     )
   535|     iface_name = iface.name
   536|     for ip_ in iface.ip_configurations:
   537|         ips.append(ip_.name)
   538|     poller = netconn.network_interfaces.delete(
   539|         kwargs["resource_group"],
   540|         kwargs["iface_name"],
   541|     )
   542|     poller.wait()
   543|     for ip_ in ips:
   544|         poller = netconn.public_ip_addresses.delete(kwargs["resource_group"], ip_)
   545|         poller.wait()
   546|     return {iface_name: ips}
   547| def _get_public_ip(name, resource_group):
   548|     """
   549|     Get the public ip address details by name.
   550|     """
   551|     netconn = get_conn(client_type="network")
   552|     try:
   553|         pubip_query = netconn.public_ip_addresses.get(
   554|             resource_group_name=resource_group, public_ip_address_name=name
   555|         )
   556|         pubip = pubip_query.as_dict()
   557|     except CloudError as exc:
   558|         salt.utils.azurearm.log_cloud_error("network", exc.message)
   559|         pubip = {"error": exc.message}
   560|     return pubip
   561| def _get_network_interface(name, resource_group):
   562|     """
   563|     Get a network interface.
   564|     """
   565|     public_ips = []
   566|     private_ips = []
   567|     netapi_versions = get_api_versions(
   568|         kwargs={
   569|             "resource_provider": "Microsoft.Network",
   570|             "resource_type": "publicIPAddresses",
   571|         }
   572|     )
   573|     netapi_version = netapi_versions[0]
   574|     netconn = get_conn(client_type="network")
   575|     netiface_query = netconn.network_interfaces.get(
   576|         resource_group_name=resource_group, network_interface_name=name
   577|     )
   578|     netiface = netiface_query.as_dict()
   579|     for index, ip_config in enumerate(netiface["ip_configurations"]):
   580|         if ip_config.get("private_ip_address") is not None:
   581|             private_ips.append(ip_config["private_ip_address"])
   582|         if "id" in ip_config.get("public_ip_address", {}):
   583|             public_ip_name = get_resource_by_id(
   584|                 ip_config["public_ip_address"]["id"], netapi_version, "name"
   585|             )
   586|             public_ip = _get_public_ip(public_ip_name, resource_group)
   587|             public_ips.append(public_ip["ip_address"])
   588|             netiface["ip_configurations"][index]["public_ip_address"].update(public_ip)
   589|     return netiface, public_ips, private_ips
   590| @_deprecation_message
   591| def create_network_interface(call=None, kwargs=None):
   592|     """
   593|     Create a network interface.
   594|     """
   595|     if call != "action":
   596|         raise SaltCloudSystemExit(
   597|             "The create_network_interface action must be called with -a or --action."
   598|         )
   599|     IPAllocationMethod = getattr(network_models, "IPAllocationMethod")
   600|     NetworkInterface = getattr(network_models, "NetworkInterface")
   601|     NetworkInterfaceIPConfiguration = getattr(
   602|         network_models, "NetworkInterfaceIPConfiguration"
   603|     )
   604|     PublicIPAddress = getattr(network_models, "PublicIPAddress")
   605|     if not isinstance(kwargs, dict):
   606|         kwargs = {}
   607|     vm_ = kwargs
   608|     netconn = get_conn(client_type="network")
   609|     if kwargs.get("location") is None:
   610|         kwargs["location"] = get_location()
   611|     if kwargs.get("network") is None:
   612|         kwargs["network"] = config.get_cloud_config_value(
   613|             "network", vm_, __opts__, search_global=False
   614|         )
   615|     if kwargs.get("subnet") is None:
   616|         kwargs["subnet"] = config.get_cloud_config_value(
   617|             "subnet", vm_, __opts__, search_global=False
   618|         )
   619|     if kwargs.get("network_resource_group") is None:
   620|         kwargs["network_resource_group"] = config.get_cloud_config_value(
   621|             "resource_group", vm_, __opts__, search_global=False
   622|         )
   623|     if kwargs.get("iface_name") is None:
   624|         kwargs["iface_name"] = "{}-iface0".format(vm_["name"])
   625|     try:
   626|         subnet_obj = netconn.subnets.get(
   627|             resource_group_name=kwargs["network_resource_group"],
   628|             virtual_network_name=kwargs["network"],
   629|             subnet_name=kwargs["subnet"],
   630|         )
   631|     except CloudError as exc:
   632|         raise SaltCloudSystemExit(
   633|             '{} (Resource Group: "{}", VNET: "{}", Subnet: "{}")'.format(
   634|                 exc.message,
   635|                 kwargs["network_resource_group"],
   636|                 kwargs["network"],
   637|                 kwargs["subnet"],
   638|             )
   639|         )
   640|     ip_kwargs = {}
   641|     ip_configurations = None
   642|     if "load_balancer_backend_address_pools" in kwargs:
   643|         pool_dicts = kwargs["load_balancer_backend_address_pools"]
   644|         if isinstance(pool_dicts, dict):
   645|             pool_ids = []
   646|             for load_bal, be_pools in pool_dicts.items():
   647|                 for pool in be_pools:
   648|                     try:
   649|                         lbbep_data = netconn.load_balancer_backend_address_pools.get(
   650|                             kwargs["resource_group"],
   651|                             load_bal,
   652|                             pool,
   653|                         )
   654|                         pool_ids.append({"id": lbbep_data.as_dict()["id"]})
   655|                     except CloudError as exc:
   656|                         log.error("There was a cloud error: %s", str(exc))
   657|                     except KeyError as exc:
   658|                         log.error(
   659|                             "There was an error getting the Backend Pool ID: %s",
   660|                             str(exc),
   661|                         )
   662|             ip_kwargs["load_balancer_backend_address_pools"] = pool_ids
   663|     if "private_ip_address" in kwargs.keys():
   664|         ip_kwargs["private_ip_address"] = kwargs["private_ip_address"]
   665|         ip_kwargs["private_ip_allocation_method"] = IPAllocationMethod.static
   666|     else:
   667|         ip_kwargs["private_ip_allocation_method"] = IPAllocationMethod.dynamic
   668|     if kwargs.get("allocate_public_ip") is True:
   669|         pub_ip_name = "{}-ip".format(kwargs["iface_name"])
   670|         poller = netconn.public_ip_addresses.create_or_update(
   671|             resource_group_name=kwargs["resource_group"],
   672|             public_ip_address_name=pub_ip_name,
   673|             parameters=PublicIPAddress(
   674|                 location=kwargs["location"],
   675|                 public_ip_allocation_method=IPAllocationMethod.static,
   676|             ),
   677|         )
   678|         count = 0
   679|         poller.wait()
   680|         while True:
   681|             try:
   682|                 pub_ip_data = netconn.public_ip_addresses.get(
   683|                     kwargs["resource_group"],
   684|                     pub_ip_name,
   685|                 )
   686|                 if pub_ip_data.ip_address:  # pylint: disable=no-member
   687|                     ip_kwargs["public_ip_address"] = PublicIPAddress(
   688|                         id=str(pub_ip_data.id),  # pylint: disable=no-member
   689|                     )
   690|                     ip_configurations = [
   691|                         NetworkInterfaceIPConfiguration(
   692|                             name="{}-ip".format(kwargs["iface_name"]),
   693|                             subnet=subnet_obj,
   694|                             **ip_kwargs,
   695|                         )
   696|                     ]
   697|                     break
   698|             except CloudError as exc:
   699|                 log.error("There was a cloud error: %s", exc)
   700|             count += 1
   701|             if count > 120:
   702|                 raise ValueError("Timed out waiting for public IP Address.")
   703|             time.sleep(5)
   704|     else:
   705|         priv_ip_name = "{}-ip".format(kwargs["iface_name"])
   706|         ip_configurations = [
   707|             NetworkInterfaceIPConfiguration(
   708|                 name=priv_ip_name, subnet=subnet_obj, **ip_kwargs
   709|             )
   710|         ]
   711|     network_security_group = None
   712|     if kwargs.get("security_group") is not None:
   713|         network_security_group = netconn.network_security_groups.get(
   714|             resource_group_name=kwargs["resource_group"],
   715|             network_security_group_name=kwargs["security_group"],
   716|         )
   717|     iface_params = NetworkInterface(
   718|         location=kwargs["location"],
   719|         network_security_group=network_security_group,
   720|         ip_configurations=ip_configurations,
   721|     )
   722|     poller = netconn.network_interfaces.create_or_update(
   723|         kwargs["resource_group"], kwargs["iface_name"], iface_params
   724|     )
   725|     try:
   726|         poller.wait()
   727|     except Exception as exc:  # pylint: disable=broad-except
   728|         log.warning(
   729|             "Network interface creation could not be polled. "
   730|             "It is likely that we are reusing an existing interface. (%s)",
   731|             exc,
   732|         )
   733|     count = 0
   734|     while True:
   735|         try:
   736|             return _get_network_interface(
   737|                 kwargs["iface_name"], kwargs["resource_group"]
   738|             )
   739|         except CloudError:
   740|             count += 1
   741|             if count > 120:
   742|                 raise ValueError("Timed out waiting for operation to complete.")
   743|             time.sleep(5)
   744| def request_instance(vm_, kwargs=None):
   745|     """
   746|     Request a VM from Azure.
   747|     """
   748|     compconn = get_conn(client_type="compute")
   749|     CachingTypes = getattr(compute_models, "CachingTypes")
   750|     DataDisk = getattr(compute_models, "DataDisk")
   751|     DiskCreateOptionTypes = getattr(compute_models, "DiskCreateOptionTypes")
   752|     HardwareProfile = getattr(compute_models, "HardwareProfile")
   753|     ImageReference = getattr(compute_models, "ImageReference")
   754|     LinuxConfiguration = getattr(compute_models, "LinuxConfiguration")
   755|     SshConfiguration = getattr(compute_models, "SshConfiguration")
   756|     SshPublicKey = getattr(compute_models, "SshPublicKey")
   757|     NetworkInterfaceReference = getattr(compute_models, "NetworkInterfaceReference")
   758|     NetworkProfile = getattr(compute_models, "NetworkProfile")
   759|     OSDisk = getattr(compute_models, "OSDisk")
   760|     OSProfile = getattr(compute_models, "OSProfile")
   761|     StorageProfile = getattr(compute_models, "StorageProfile")
   762|     VirtualHardDisk = getattr(compute_models, "VirtualHardDisk")
   763|     VirtualMachine = getattr(compute_models, "VirtualMachine")
   764|     VirtualMachineSizeTypes = getattr(compute_models, "VirtualMachineSizeTypes")
   765|     subscription_id = config.get_cloud_config_value(
   766|         "subscription_id", get_configured_provider(), __opts__, search_global=False
   767|     )
   768|     if vm_.get("driver") is None:
   769|         vm_["driver"] = "azurearm"
   770|     if vm_.get("location") is None:
   771|         vm_["location"] = get_location()
   772|     if vm_.get("resource_group") is None:
   773|         vm_["resource_group"] = config.get_cloud_config_value(
   774|             "resource_group", vm_, __opts__, search_global=True
   775|         )
   776|     if vm_.get("name") is None:
   777|         vm_["name"] = config.get_cloud_config_value(
   778|             "name", vm_, __opts__, search_global=True
   779|         )
   780|     iface_data, public_ips, private_ips = create_network_interface(
   781|         call="action", kwargs=vm_
   782|     )
   783|     vm_["iface_id"] = iface_data["id"]
   784|     disk_name = "{}-vol0".format(vm_["name"])
   785|     vm_username = config.get_cloud_config_value(
   786|         "ssh_username",
   787|         vm_,
   788|         __opts__,
   789|         search_global=True,
   790|         default=config.get_cloud_config_value(
   791|             "win_username", vm_, __opts__, search_global=True
   792|         ),
   793|     )
   794|     ssh_publickeyfile_contents = None
   795|     ssh_publickeyfile = config.get_cloud_config_value(
   796|         "ssh_publickeyfile", vm_, __opts__, search_global=False, default=None
   797|     )
   798|     if ssh_publickeyfile is not None:
   799|         try:
   800|             with salt.utils.files.fopen(ssh_publickeyfile, "r") as spkc_:
   801|                 ssh_publickeyfile_contents = spkc_.read()
   802|         except Exception as exc:  # pylint: disable=broad-except
   803|             raise SaltCloudConfigError(
   804|                 "Failed to read ssh publickey file '{}': {}".format(
   805|                     ssh_publickeyfile, exc.args[-1]
   806|                 )
   807|             )
   808|     disable_password_authentication = config.get_cloud_config_value(
   809|         "disable_password_authentication",
   810|         vm_,
   811|         __opts__,
   812|         search_global=False,
   813|         default=False,
   814|     )
   815|     os_kwargs = {}
   816|     win_installer = config.get_cloud_config_value(
   817|         "win_installer", vm_, __opts__, search_global=True
   818|     )
   819|     if not win_installer and ssh_publickeyfile_contents is not None:
   820|         sshpublickey = SshPublicKey(
   821|             key_data=ssh_publickeyfile_contents,
   822|             path=f"/home/{vm_username}/.ssh/authorized_keys",
   823|         )
   824|         sshconfiguration = SshConfiguration(
   825|             public_keys=[sshpublickey],
   826|         )
   827|         linuxconfiguration = LinuxConfiguration(
   828|             disable_password_authentication=disable_password_authentication,
   829|             ssh=sshconfiguration,
   830|         )
   831|         os_kwargs["linux_configuration"] = linuxconfiguration
   832|         vm_password = None
   833|     else:
   834|         vm_password = salt.utils.stringutils.to_str(
   835|             config.get_cloud_config_value(
   836|                 "ssh_password",
   837|                 vm_,
   838|                 __opts__,
   839|                 search_global=True,
   840|                 default=config.get_cloud_config_value(
   841|                     "win_password", vm_, __opts__, search_global=True
   842|                 ),
   843|             )
   844|         )
   845|     if win_installer or (
   846|         vm_password is not None and not disable_password_authentication
   847|     ):
   848|         if not isinstance(vm_password, str):
   849|             raise SaltCloudSystemExit("The admin password must be a string.")
   850|         if len(vm_password) < 8 or len(vm_password) > 123:
   851|             raise SaltCloudSystemExit(
   852|                 "The admin password must be between 8-123 characters long."
   853|             )
   854|         complexity = 0
   855|         if any(char.isdigit() for char in vm_password):
   856|             complexity += 1
   857|         if any(char.isupper() for char in vm_password):
   858|             complexity += 1
   859|         if any(char.islower() for char in vm_password):
   860|             complexity += 1
   861|         if any(char in string.punctuation for char in vm_password):
   862|             complexity += 1
   863|         if complexity < 3:
   864|             raise SaltCloudSystemExit(
   865|                 "The admin password must contain at least 3 of the following types: "
   866|                 "upper, lower, digits, special characters"
   867|             )
   868|         os_kwargs["admin_password"] = vm_password
   869|     availability_set = config.get_cloud_config_value(
   870|         "availability_set", vm_, __opts__, search_global=False, default=None
   871|     )
   872|     if availability_set is not None and isinstance(availability_set, str):
   873|         availability_set = {
   874|             "id": "/subscriptions/{}/resourceGroups/{}/providers/Microsoft.Compute/availabilitySets/{}".format(
   875|                 subscription_id, vm_["resource_group"], availability_set
   876|             )
   877|         }
   878|     else:
   879|         availability_set = None
   880|     cloud_env = _get_cloud_environment()
   881|     storage_endpoint_suffix = cloud_env.suffixes.storage_endpoint
   882|     if isinstance(vm_.get("volumes"), str):
   883|         volumes = salt.utils.yaml.safe_load(vm_["volumes"])
   884|     else:
   885|         volumes = vm_.get("volumes")
   886|     data_disks = None
   887|     if isinstance(volumes, list):
   888|         data_disks = []
   889|     else:
   890|         volumes = []
   891|     lun = 0
   892|     luns = []
   893|     for volume in volumes:
   894|         if isinstance(volume, str):
   895|             volume = {"name": volume}
   896|         volume.setdefault(
   897|             "name",
   898|             volume.get(
   899|                 "name",
   900|                 volume.get("name", "{}-datadisk{}".format(vm_["name"], str(lun))),
   901|             ),
   902|         )
   903|         volume.setdefault(
   904|             "disk_size_gb",
   905|             volume.get("logical_disk_size_in_gb", volume.get("size", 100)),
   906|         )
   907|         volume.setdefault("caching", volume.get("host_caching", "ReadOnly"))
   908|         while lun in luns:
   909|             lun += 1
   910|             if lun > 15:
   911|                 log.error("Maximum lun count has been reached")
   912|                 break
   913|         volume.setdefault("lun", lun)
   914|         lun += 1
   915|         if "media_link" in volume:
   916|             volume["vhd"] = VirtualHardDisk(uri=volume["media_link"])
   917|             del volume["media_link"]
   918|         elif volume.get("vhd") == "unmanaged":
   919|             volume["vhd"] = VirtualHardDisk(
   920|                 uri="https://{}.blob.{}/vhds/{}-datadisk{}.vhd".format(
   921|                     vm_["storage_account"],
   922|                     storage_endpoint_suffix,
   923|                     vm_["name"],
   924|                     volume["lun"],
   925|                 ),
   926|             )
   927|         elif "vhd" in volume:
   928|             volume["vhd"] = VirtualHardDisk(uri=volume["vhd"])
   929|         if "image" in volume:
   930|             volume["create_option"] = "from_image"
   931|         elif "attach" in volume:
   932|             volume["create_option"] = "attach"
   933|         else:
   934|             volume["create_option"] = "empty"
   935|         data_disks.append(DataDisk(**volume))
   936|     img_ref = None
   937|     if vm_["image"].startswith("http") or vm_.get("vhd") == "unmanaged":
   938|         if vm_["image"].startswith("http"):
   939|             source_image = VirtualHardDisk(uri=vm_["image"])
   940|         else:
   941|             source_image = None
   942|             if "|" in vm_["image"]:
   943|                 img_pub, img_off, img_sku, img_ver = vm_["image"].split("|")
   944|                 img_ref = ImageReference(
   945|                     publisher=img_pub,
   946|                     offer=img_off,
   947|                     sku=img_sku,
   948|                     version=img_ver,
   949|                 )
   950|             elif vm_["image"].startswith("/subscriptions"):
   951|                 img_ref = ImageReference(id=vm_["image"])
   952|         if win_installer:
   953|             os_type = "Windows"
   954|         else:
   955|             os_type = "Linux"
   956|         os_disk = OSDisk(
   957|             caching=CachingTypes.none,
   958|             create_option=DiskCreateOptionTypes.from_image,
   959|             name=disk_name,
   960|             vhd=VirtualHardDisk(
   961|                 uri="https://{}.blob.{}/vhds/{}.vhd".format(
   962|                     vm_["storage_account"],
   963|                     storage_endpoint_suffix,
   964|                     disk_name,
   965|                 ),
   966|             ),
   967|             os_type=os_type,
   968|             image=source_image,
   969|             disk_size_gb=vm_.get("os_disk_size_gb"),
   970|         )
   971|     else:
   972|         source_image = None
   973|         os_type = None
   974|         os_disk = OSDisk(
   975|             create_option=DiskCreateOptionTypes.from_image,
   976|             disk_size_gb=vm_.get("os_disk_size_gb"),
   977|         )
   978|         if "|" in vm_["image"]:
   979|             img_pub, img_off, img_sku, img_ver = vm_["image"].split("|")
   980|             img_ref = ImageReference(
   981|                 publisher=img_pub,
   982|                 offer=img_off,
   983|                 sku=img_sku,
   984|                 version=img_ver,
   985|             )
   986|         elif vm_["image"].startswith("/subscriptions"):
   987|             img_ref = ImageReference(id=vm_["image"])
   988|     userdata_file = config.get_cloud_config_value(
   989|         "userdata_file", vm_, __opts__, search_global=False, default=None
   990|     )
   991|     userdata = config.get_cloud_config_value(
   992|         "userdata", vm_, __opts__, search_global=False, default=None
   993|     )
   994|     userdata_template = config.get_cloud_config_value(
   995|         "userdata_template", vm_, __opts__, search_global=False, default=None
   996|     )
   997|     if userdata_file:
   998|         if os.path.exists(userdata_file):
   999|             with salt.utils.files.fopen(userdata_file, "r") as fh_:
  1000|                 userdata = fh_.read()
  1001|     if userdata and userdata_template:
  1002|         userdata_sendkeys = config.get_cloud_config_value(
  1003|             "userdata_sendkeys", vm_, __opts__, search_global=False, default=None
  1004|         )
  1005|         if userdata_sendkeys:
  1006|             vm_["priv_key"], vm_["pub_key"] = salt.utils.cloud.gen_keys(
  1007|                 config.get_cloud_config_value("keysize", vm_, __opts__)
  1008|             )
  1009|             key_id = vm_.get("name")
  1010|             if "append_domain" in vm_:
  1011|                 key_id = ".".join([key_id, vm_["append_domain"]])
  1012|             salt.utils.cloud.accept_key(__opts__["pki_dir"], vm_["pub_key"], key_id)
  1013|         userdata = salt.utils.cloud.userdata_template(__opts__, vm_, userdata)
  1014|     custom_extension = None
  1015|     if userdata is not None or userdata_file is not None:
  1016|         try:
  1017|             if win_installer:
  1018|                 publisher = "Microsoft.Compute"
  1019|                 virtual_machine_extension_type = "CustomScriptExtension"
  1020|                 type_handler_version = "1.8"
  1021|                 if userdata_file and userdata_file.endswith(".ps1"):
  1022|                     command_prefix = "powershell -ExecutionPolicy Unrestricted -File "
  1023|                 else:
  1024|                     command_prefix = ""
  1025|             else:
  1026|                 publisher = "Microsoft.Azure.Extensions"
  1027|                 virtual_machine_extension_type = "CustomScript"
  1028|                 type_handler_version = "2.0"
  1029|                 command_prefix = ""
  1030|             settings = {}
  1031|             if userdata:
  1032|                 settings["commandToExecute"] = userdata
  1033|             elif userdata_file.startswith("http"):
  1034|                 settings["fileUris"] = [userdata_file]
  1035|                 settings["commandToExecute"] = (
  1036|                     command_prefix
  1037|                     + "./"
  1038|                     + userdata_file[userdata_file.rfind("/") + 1 :]
  1039|                 )
  1040|             custom_extension = {
  1041|                 "resource_group": vm_["resource_group"],
  1042|                 "virtual_machine_name": vm_["name"],
  1043|                 "extension_name": vm_["name"] + "_custom_userdata_script",
  1044|                 "location": vm_["location"],
  1045|                 "publisher": publisher,
  1046|                 "virtual_machine_extension_type": virtual_machine_extension_type,
  1047|                 "type_handler_version": type_handler_version,
  1048|                 "auto_upgrade_minor_version": True,
  1049|                 "settings": settings,
  1050|                 "protected_settings": None,
  1051|             }
  1052|         except Exception as exc:  # pylint: disable=broad-except
  1053|             log.exception("Failed to encode userdata: %s", exc)
  1054|     params = VirtualMachine(
  1055|         location=vm_["location"],
  1056|         plan=None,
  1057|         hardware_profile=HardwareProfile(
  1058|             vm_size=getattr(VirtualMachineSizeTypes, vm_["size"].lower(), kwargs),
  1059|         ),
  1060|         storage_profile=StorageProfile(
  1061|             os_disk=os_disk,
  1062|             data_disks=data_disks,
  1063|             image_reference=img_ref,
  1064|         ),
  1065|         os_profile=OSProfile(
  1066|             admin_username=vm_username, computer_name=vm_["name"], **os_kwargs
  1067|         ),
  1068|         network_profile=NetworkProfile(
  1069|             network_interfaces=[NetworkInterfaceReference(id=vm_["iface_id"])],
  1070|         ),
  1071|         availability_set=availability_set,
  1072|     )
  1073|     __utils__["cloud.fire_event"](
  1074|         "event",
  1075|         "requesting instance",
  1076|         "salt/cloud/{}/requesting".format(vm_["name"]),
  1077|         args=__utils__["cloud.filter_event"](
  1078|             "requesting", vm_, ["name", "profile", "provider", "driver"]
  1079|         ),
  1080|         sock_dir=__opts__["sock_dir"],
  1081|         transport=__opts__["transport"],
  1082|     )
  1083|     try:
  1084|         vm_create = compconn.virtual_machines.create_or_update(
  1085|             resource_group_name=vm_["resource_group"],
  1086|             vm_name=vm_["name"],
  1087|             parameters=params,
  1088|         )
  1089|         vm_create.wait()
  1090|         vm_result = vm_create.result()
  1091|         vm_result = vm_result.as_dict()
  1092|         if custom_extension:
  1093|             create_or_update_vmextension(kwargs=custom_extension)
  1094|     except CloudError as exc:
  1095|         salt.utils.azurearm.log_cloud_error("compute", exc.message)
  1096|         vm_result = {}
  1097|     return vm_result
  1098| @_deprecation_message
  1099| def create(vm_):
  1100|     """
  1101|     Create a single VM from a data dict.
  1102|     """
  1103|     try:
  1104|         if (
  1105|             vm_["profile"]
  1106|             and config.is_profile_configured(
  1107|                 __opts__,
  1108|                 _get_active_provider_name() or "azurearm",
  1109|                 vm_["profile"],
  1110|                 vm_=vm_,
  1111|             )
  1112|             is False
  1113|         ):
  1114|             return False
  1115|     except AttributeError:
  1116|         pass
  1117|     if vm_.get("bootstrap_interface") is None:
  1118|         vm_["bootstrap_interface"] = "public"
  1119|     __utils__["cloud.fire_event"](
  1120|         "event",
  1121|         "starting create",
  1122|         "salt/cloud/{}/creating".format(vm_["name"]),
  1123|         args=__utils__["cloud.filter_event"](
  1124|             "creating", vm_, ["name", "profile", "provider", "driver"]
  1125|         ),
  1126|         sock_dir=__opts__["sock_dir"],
  1127|         transport=__opts__["transport"],
  1128|     )
  1129|     __utils__["cloud.cachedir_index_add"](
  1130|         vm_["name"], vm_["profile"], "azurearm", vm_["driver"]
  1131|     )
  1132|     if not vm_.get("location"):
  1133|         vm_["location"] = get_location(kwargs=vm_)
  1134|     log.info("Creating Cloud VM %s in %s", vm_["name"], vm_["location"])
  1135|     vm_request = request_instance(vm_=vm_)
  1136|     if not vm_request or "error" in vm_request:
  1137|         err_message = "Error creating VM {}! ({})".format(vm_["name"], str(vm_request))
  1138|         log.error(err_message)
  1139|         raise SaltCloudSystemExit(err_message)
  1140|     def _query_node_data(name, bootstrap_interface):
  1141|         """
  1142|         Query node data.
  1143|         """
  1144|         data = show_instance(name, call="action")
  1145|         if not data:
  1146|             return False
  1147|         ip_address = None
  1148|         if bootstrap_interface == "public":
  1149|             ip_address = data["public_ips"][0]
  1150|         if bootstrap_interface == "private":
  1151|             ip_address = data["private_ips"][0]
  1152|         if ip_address is None:
  1153|             return False
  1154|         return ip_address
  1155|     try:
  1156|         data = salt.utils.cloud.wait_for_ip(
  1157|             _query_node_data,
  1158|             update_args=(
  1159|                 vm_["name"],
  1160|                 vm_["bootstrap_interface"],
  1161|             ),
  1162|             timeout=config.get_cloud_config_value(
  1163|                 "wait_for_ip_timeout", vm_, __opts__, default=10 * 60
  1164|             ),
  1165|             interval=config.get_cloud_config_value(
  1166|                 "wait_for_ip_interval", vm_, __opts__, default=10
  1167|             ),
  1168|             interval_multiplier=config.get_cloud_config_value(
  1169|                 "wait_for_ip_interval_multiplier", vm_, __opts__, default=1
  1170|             ),
  1171|         )
  1172|     except (
  1173|         SaltCloudExecutionTimeout,
  1174|         SaltCloudExecutionFailure,
  1175|         SaltCloudSystemExit,
  1176|     ) as exc:
  1177|         try:
  1178|             log.warning(exc)
  1179|         finally:
  1180|             raise SaltCloudSystemExit(str(exc))
  1181|     vm_["ssh_host"] = data
  1182|     if not vm_.get("ssh_username"):
  1183|         vm_["ssh_username"] = config.get_cloud_config_value(
  1184|             "ssh_username", vm_, __opts__
  1185|         )
  1186|     vm_["password"] = config.get_cloud_config_value("ssh_password", vm_, __opts__)
  1187|     ret = __utils__["cloud.bootstrap"](vm_, __opts__)
  1188|     data = show_instance(vm_["name"], call="action")
  1189|     log.info("Created Cloud VM '%s'", vm_["name"])
  1190|     log.debug("'%s' VM creation details:\n%s", vm_["name"], pprint.pformat(data))
  1191|     ret.update(data)
  1192|     __utils__["cloud.fire_event"](
  1193|         "event",
  1194|         "created instance",
  1195|         "salt/cloud/{}/created".format(vm_["name"]),
  1196|         args=__utils__["cloud.filter_event"](
  1197|             "created", vm_, ["name", "profile", "provider", "driver"]
  1198|         ),
  1199|         sock_dir=__opts__["sock_dir"],
  1200|         transport=__opts__["transport"],
  1201|     )
  1202|     return ret
  1203| @_deprecation_message
  1204| def destroy(name, call=None, kwargs=None):  # pylint: disable=unused-argument
  1205|     """
  1206|     Destroy a VM.
  1207|     CLI Examples:
  1208|     .. code-block:: bash
  1209|         salt-cloud -d myminion
  1210|         salt-cloud -a destroy myminion service_name=myservice
  1211|     """
  1212|     if kwargs is None:
  1213|         kwargs = {}
  1214|     if call == "function":
  1215|         raise SaltCloudSystemExit(
  1216|             "The destroy action must be called with -d, --destroy, -a or --action."
  1217|         )
  1218|     compconn = get_conn(client_type="compute")
  1219|     node_data = show_instance(name, call="action")
  1220|     if node_data["storage_profile"]["os_disk"].get("managed_disk"):
  1221|         vhd = node_data["storage_profile"]["os_disk"]["managed_disk"]["id"]
  1222|     else:
  1223|         vhd = node_data["storage_profile"]["os_disk"]["vhd"]["uri"]
  1224|     ret = {name: {}}
  1225|     log.debug("Deleting VM")
  1226|     result = compconn.virtual_machines.delete(node_data["resource_group"], name)
  1227|     result.wait()
  1228|     if __opts__.get("update_cachedir", False) is True:
  1229|         __utils__["cloud.delete_minion_cachedir"](
  1230|             name, _get_active_provider_name().split(":")[0], __opts__
  1231|         )
  1232|     cleanup_disks = config.get_cloud_config_value(
  1233|         "cleanup_disks",
  1234|         get_configured_provider(),
  1235|         __opts__,
  1236|         search_global=False,
  1237|         default=False,
  1238|     )
  1239|     if cleanup_disks:
  1240|         cleanup_vhds = kwargs.get(
  1241|             "delete_vhd",
  1242|             config.get_cloud_config_value(
  1243|                 "cleanup_vhds",
  1244|                 get_configured_provider(),
  1245|                 __opts__,
  1246|                 search_global=False,
  1247|                 default=False,
  1248|             ),
  1249|         )
  1250|         if cleanup_vhds:
  1251|             log.debug("Deleting vhd")
  1252|             comps = vhd.split("/")
  1253|             container = comps[-2]
  1254|             blob = comps[-1]
  1255|             ret[name]["delete_disk"] = {
  1256|                 "delete_disks": cleanup_disks,
  1257|                 "delete_vhd": cleanup_vhds,
  1258|                 "container": container,
  1259|                 "blob": blob,
  1260|             }
  1261|             if vhd.startswith("http"):
  1262|                 ret[name]["data"] = delete_blob(
  1263|                     kwargs={"container": container, "blob": blob}, call="function"
  1264|                 )
  1265|             else:
  1266|                 ret[name]["data"] = delete_managed_disk(
  1267|                     kwargs={
  1268|                         "resource_group": node_data["resource_group"],
  1269|                         "container": container,
  1270|                         "blob": blob,
  1271|                     },
  1272|                     call="function",
  1273|                 )
  1274|         cleanup_data_disks = kwargs.get(
  1275|             "delete_data_disks",
  1276|             config.get_cloud_config_value(
  1277|                 "cleanup_data_disks",
  1278|                 get_configured_provider(),
  1279|                 __opts__,
  1280|                 search_global=False,
  1281|                 default=False,
  1282|             ),
  1283|         )
  1284|         if cleanup_data_disks:
  1285|             log.debug("Deleting data_disks")
  1286|             ret[name]["data_disks"] = {}
  1287|             for disk in node_data["storage_profile"]["data_disks"]:
  1288|                 datavhd = disk.get("managed_disk", {}).get("id") or disk.get(
  1289|                     "vhd", {}
  1290|                 ).get("uri")
  1291|                 comps = datavhd.split("/")
  1292|                 container = comps[-2]
  1293|                 blob = comps[-1]
  1294|                 ret[name]["data_disks"][disk["name"]] = {
  1295|                     "delete_disks": cleanup_disks,
  1296|                     "delete_vhd": cleanup_vhds,
  1297|                     "container": container,
  1298|                     "blob": blob,
  1299|                 }
  1300|                 if datavhd.startswith("http"):
  1301|                     ret[name]["data"] = delete_blob(
  1302|                         kwargs={"container": container, "blob": blob}, call="function"
  1303|                     )
  1304|                 else:
  1305|                     ret[name]["data"] = delete_managed_disk(
  1306|                         kwargs={
  1307|                             "resource_group": node_data["resource_group"],
  1308|                             "container": container,
  1309|                             "blob": blob,
  1310|                         },
  1311|                         call="function",
  1312|                     )
  1313|     cleanup_interfaces = config.get_cloud_config_value(
  1314|         "cleanup_interfaces",
  1315|         get_configured_provider(),
  1316|         __opts__,
  1317|         search_global=False,
  1318|         default=False,
  1319|     )
  1320|     if cleanup_interfaces:
  1321|         ret[name]["cleanup_network"] = {
  1322|             "cleanup_interfaces": cleanup_interfaces,
  1323|             "resource_group": node_data["resource_group"],
  1324|             "data": [],
  1325|         }
  1326|         ifaces = node_data["network_profile"]["network_interfaces"]
  1327|         for iface in ifaces:
  1328|             resource_group = iface["id"].split("/")[4]
  1329|             ret[name]["cleanup_network"]["data"].append(
  1330|                 delete_interface(
  1331|                     kwargs={
  1332|                         "resource_group": resource_group,
  1333|                         "iface_name": iface["name"],
  1334|                     },
  1335|                     call="function",
  1336|                 )
  1337|             )
  1338|     return ret
  1339| @_deprecation_message
  1340| def list_storage_accounts(call=None):
  1341|     """
  1342|     List storage accounts within the subscription.
  1343|     """
  1344|     if call == "action":
  1345|         raise SaltCloudSystemExit(
  1346|             "The list_storage_accounts function must be called with -f or --function"
  1347|         )
  1348|     storconn = get_conn(client_type="storage")
  1349|     ret = {}
  1350|     try:
  1351|         accounts_query = storconn.storage_accounts.list()
  1352|         accounts = salt.utils.azurearm.paged_object_to_list(accounts_query)
  1353|         for account in accounts:
  1354|             ret[account["name"]] = account
  1355|     except CloudError as exc:
  1356|         salt.utils.azurearm.log_cloud_error("storage", exc.message)
  1357|         ret = {"Error": exc.message}
  1358|     return ret
  1359| def _get_cloud_environment():
  1360|     """
  1361|     Get the cloud environment object.
  1362|     """
  1363|     cloud_environment = config.get_cloud_config_value(
  1364|         "cloud_environment", get_configured_provider(), __opts__, search_global=False
  1365|     )
  1366|     try:
  1367|         cloud_env_module = importlib.import_module("msrestazure.azure_cloud")
  1368|         cloud_env = getattr(cloud_env_module, cloud_environment or "AZURE_PUBLIC_CLOUD")
  1369|     except (AttributeError, ImportError):
  1370|         raise SaltCloudSystemExit(
  1371|             f"The azure {cloud_environment} cloud environment is not available."
  1372|         )
  1373|     return cloud_env
  1374| def _get_block_blob_service(kwargs=None):
  1375|     """
  1376|     Get the block blob storage service.
  1377|     """
  1378|     resource_group = kwargs.get("resource_group") or config.get_cloud_config_value(
  1379|         "resource_group", get_configured_provider(), __opts__, search_global=False
  1380|     )
  1381|     sas_token = kwargs.get("sas_token") or config.get_cloud_config_value(
  1382|         "sas_token", get_configured_provider(), __opts__, search_global=False
  1383|     )
  1384|     storage_account = kwargs.get("storage_account") or config.get_cloud_config_value(
  1385|         "storage_account", get_configured_provider(), __opts__, search_global=False
  1386|     )
  1387|     storage_key = kwargs.get("storage_key") or config.get_cloud_config_value(
  1388|         "storage_key", get_configured_provider(), __opts__, search_global=False
  1389|     )
  1390|     if not resource_group:
  1391|         raise SaltCloudSystemExit("A resource group must be specified")
  1392|     if not storage_account:
  1393|         raise SaltCloudSystemExit("A storage account must be specified")
  1394|     if not storage_key:
  1395|         storconn = get_conn(client_type="storage")
  1396|         storage_keys = storconn.storage_accounts.list_keys(
  1397|             resource_group, storage_account
  1398|         )
  1399|         storage_keys = {v.key_name: v.value for v in storage_keys.keys}
  1400|         storage_key = next(iter(storage_keys.values()))
  1401|     cloud_env = _get_cloud_environment()
  1402|     endpoint_suffix = cloud_env.suffixes.storage_endpoint
  1403|     return BlockBlobService(
  1404|         storage_account,
  1405|         storage_key,
  1406|         sas_token=sas_token,
  1407|         endpoint_suffix=endpoint_suffix,
  1408|     )
  1409| @_deprecation_message
  1410| def list_blobs(call=None, kwargs=None):  # pylint: disable=unused-argument
  1411|     """
  1412|     List blobs.
  1413|     """
  1414|     if kwargs is None:
  1415|         kwargs = {}
  1416|     if "container" not in kwargs:
  1417|         raise SaltCloudSystemExit("A container must be specified")
  1418|     storageservice = _get_block_blob_service(kwargs)
  1419|     ret = {}
  1420|     try:
  1421|         for blob in storageservice.list_blobs(kwargs["container"]).items:
  1422|             ret[blob.name] = {
  1423|                 "blob_type": blob.properties.blob_type,
  1424|                 "last_modified": blob.properties.last_modified.isoformat(),
  1425|                 "server_encrypted": blob.properties.server_encrypted,
  1426|             }
  1427|     except Exception as exc:  # pylint: disable=broad-except
  1428|         log.warning(str(exc))
  1429|     return ret
  1430| @_deprecation_message
  1431| def delete_blob(call=None, kwargs=None):  # pylint: disable=unused-argument
  1432|     """
  1433|     Delete a blob from a container.
  1434|     """
  1435|     if kwargs is None:
  1436|         kwargs = {}
  1437|     if "container" not in kwargs:
  1438|         raise SaltCloudSystemExit("A container must be specified")
  1439|     if "blob" not in kwargs:
  1440|         raise SaltCloudSystemExit("A blob must be specified")
  1441|     storageservice = _get_block_blob_service(kwargs)
  1442|     storageservice.delete_blob(kwargs["container"], kwargs["blob"])
  1443|     return True
  1444| @_deprecation_message
  1445| def delete_managed_disk(call=None, kwargs=None):  # pylint: disable=unused-argument
  1446|     """
  1447|     Delete a managed disk from a resource group.
  1448|     """
  1449|     compconn = get_conn(client_type="compute")
  1450|     try:
  1451|         compconn.disks.delete(kwargs["resource_group"], kwargs["blob"])
  1452|     except Exception as exc:  # pylint: disable=broad-except
  1453|         log.error(
  1454|             "Error deleting managed disk %s - %s",
  1455|             kwargs.get("blob"),
  1456|             str(exc),
  1457|         )
  1458|         return False
  1459|     return True
  1460| @_deprecation_message
  1461| def list_virtual_networks(call=None, kwargs=None):
  1462|     """
  1463|     List virtual networks.
  1464|     """
  1465|     if kwargs is None:
  1466|         kwargs = {}
  1467|     if call == "action":
  1468|         raise SaltCloudSystemExit(
  1469|             "The avail_sizes function must be called with -f or --function"
  1470|         )
  1471|     netconn = get_conn(client_type="network")
  1472|     resource_groups = list_resource_groups()
  1473|     ret = {}
  1474|     for group in resource_groups:
  1475|         try:
  1476|             networks = netconn.virtual_networks.list(resource_group_name=group)
  1477|         except CloudError:
  1478|             networks = {}
  1479|         for network_obj in networks:
  1480|             network = network_obj.as_dict()
  1481|             ret[network["name"]] = network
  1482|             ret[network["name"]]["subnets"] = list_subnets(
  1483|                 kwargs={"resource_group": group, "network": network["name"]}
  1484|             )
  1485|     return ret
  1486| @_deprecation_message
  1487| def list_subnets(call=None, kwargs=None):
  1488|     """
  1489|     List subnets in a virtual network.
  1490|     """
  1491|     if kwargs is None:
  1492|         kwargs = {}
  1493|     if call == "action":
  1494|         raise SaltCloudSystemExit(
  1495|             "The avail_sizes function must be called with -f or --function"
  1496|         )
  1497|     netconn = get_conn(client_type="network")
  1498|     resource_group = kwargs.get("resource_group") or config.get_cloud_config_value(
  1499|         "resource_group", get_configured_provider(), __opts__, search_global=False
  1500|     )
  1501|     if not resource_group and "group" in kwargs and "resource_group" not in kwargs:
  1502|         resource_group = kwargs["group"]
  1503|     if not resource_group:
  1504|         raise SaltCloudSystemExit("A resource group must be specified")
  1505|     if kwargs.get("network") is None:
  1506|         kwargs["network"] = config.get_cloud_config_value(
  1507|             "network", get_configured_provider(), __opts__, search_global=False
  1508|         )
  1509|     if "network" not in kwargs or kwargs["network"] is None:
  1510|         raise SaltCloudSystemExit('A "network" must be specified')
  1511|     ret = {}
  1512|     subnets = netconn.subnets.list(resource_group, kwargs["network"])
  1513|     for subnet in subnets:
  1514|         ret[subnet.name] = subnet.as_dict()
  1515|         ret[subnet.name]["ip_configurations"] = {}
  1516|         for ip_ in subnet.ip_configurations:
  1517|             comps = ip_.id.split("/")
  1518|             name = comps[-1]
  1519|             ret[subnet.name]["ip_configurations"][name] = ip_.as_dict()
  1520|             ret[subnet.name]["ip_configurations"][name]["subnet"] = subnet.name
  1521|         ret[subnet.name]["resource_group"] = resource_group
  1522|     return ret
  1523| @_deprecation_message
  1524| def create_or_update_vmextension(
  1525|     call=None, kwargs=None
  1526| ):  # pylint: disable=unused-argument
  1527|     """
  1528|     .. versionadded:: 2019.2.0
  1529|     Create or update a VM extension object "inside" of a VM object.
  1530|     required kwargs:
  1531|       .. code-block:: yaml
  1532|         extension_name: myvmextension
  1533|         virtual_machine_name: myvm
  1534|         settings: {"commandToExecute": "hostname"}
  1535|     optional kwargs:
  1536|       .. code-block:: yaml
  1537|         resource_group: < inferred from cloud configs >
  1538|         location: < inferred from cloud configs >
  1539|         publisher: < default: Microsoft.Azure.Extensions >
  1540|         virtual_machine_extension_type: < default: CustomScript >
  1541|         type_handler_version: < default: 2.0 >
  1542|         auto_upgrade_minor_version: < default: True >
  1543|         protected_settings: < default: None >
  1544|     """
  1545|     if kwargs is None:
  1546|         kwargs = {}
  1547|     if "extension_name" not in kwargs:
  1548|         raise SaltCloudSystemExit("An extension name must be specified")
  1549|     if "virtual_machine_name" not in kwargs:
  1550|         raise SaltCloudSystemExit("A virtual machine name must be specified")
  1551|     compconn = get_conn(client_type="compute")
  1552|     VirtualMachineExtension = getattr(compute_models, "VirtualMachineExtension")
  1553|     resource_group = kwargs.get("resource_group") or config.get_cloud_config_value(
  1554|         "resource_group", get_configured_provider(), __opts__, search_global=False
  1555|     )
  1556|     if not resource_group:
  1557|         raise SaltCloudSystemExit("A resource group must be specified")
  1558|     location = kwargs.get("location") or get_location()
  1559|     if not location:
  1560|         raise SaltCloudSystemExit("A location must be specified")
  1561|     publisher = kwargs.get("publisher", "Microsoft.Azure.Extensions")
  1562|     virtual_machine_extension_type = kwargs.get(
  1563|         "virtual_machine_extension_type", "CustomScript"
  1564|     )
  1565|     type_handler_version = kwargs.get("type_handler_version", "2.0")
  1566|     auto_upgrade_minor_version = kwargs.get("auto_upgrade_minor_version", True)
  1567|     settings = kwargs.get("settings", {})
  1568|     protected_settings = kwargs.get("protected_settings")
  1569|     if not isinstance(settings, dict):
  1570|         raise SaltCloudSystemExit("VM extension settings are not valid")
  1571|     elif "commandToExecute" not in settings and "script" not in settings:
  1572|         raise SaltCloudSystemExit(
  1573|             "VM extension settings are not valid. Either commandToExecute or script"
  1574|             " must be specified."
  1575|         )
  1576|     log.info("Creating VM extension %s", kwargs["extension_name"])
  1577|     ret = {}
  1578|     try:
  1579|         params = VirtualMachineExtension(
  1580|             location=location,
  1581|             publisher=publisher,
  1582|             virtual_machine_extension_type=virtual_machine_extension_type,
  1583|             type_handler_version=type_handler_version,
  1584|             auto_upgrade_minor_version=auto_upgrade_minor_version,
  1585|             settings=settings,
  1586|             protected_settings=protected_settings,
  1587|         )
  1588|         poller = compconn.virtual_machine_extensions.create_or_update(
  1589|             resource_group,
  1590|             kwargs["virtual_machine_name"],
  1591|             kwargs["extension_name"],
  1592|             params,
  1593|         )
  1594|         ret = poller.result()
  1595|         ret = ret.as_dict()
  1596|     except CloudError as exc:
  1597|         salt.utils.azurearm.log_cloud_error(
  1598|             "compute",
  1599|             f"Error attempting to create the VM extension: {exc.message}",
  1600|         )
  1601|         ret = {"error": exc.message}
  1602|     return ret
  1603| @_deprecation_message
  1604| def stop(name, call=None):
  1605|     """
  1606|     .. versionadded:: 2019.2.0
  1607|     Stop (deallocate) a VM
  1608|     CLI Examples:
  1609|     .. code-block:: bash
  1610|          salt-cloud -a stop myminion
  1611|     """
  1612|     if call == "function":
  1613|         raise SaltCloudSystemExit("The stop action must be called with -a or --action.")
  1614|     compconn = get_conn(client_type="compute")
  1615|     resource_group = config.get_cloud_config_value(
  1616|         "resource_group", get_configured_provider(), __opts__, search_global=False
  1617|     )
  1618|     ret = {}
  1619|     if not resource_group:
  1620|         groups = list_resource_groups()
  1621|         for group in groups:
  1622|             try:
  1623|                 instance = compconn.virtual_machines.deallocate(
  1624|                     vm_name=name, resource_group_name=group
  1625|                 )
  1626|                 instance.wait()
  1627|                 vm_result = instance.result()
  1628|                 ret = vm_result.as_dict()
  1629|                 break
  1630|             except CloudError as exc:
  1631|                 if "was not found" in exc.message:
  1632|                     continue
  1633|                 else:
  1634|                     ret = {"error": exc.message}
  1635|         if not ret:
  1636|             salt.utils.azurearm.log_cloud_error(
  1637|                 "compute", f"Unable to find virtual machine with name: {name}"
  1638|             )
  1639|             ret = {"error": f"Unable to find virtual machine with name: {name}"}
  1640|     else:
  1641|         try:
  1642|             instance = compconn.virtual_machines.deallocate(
  1643|                 vm_name=name, resource_group_name=resource_group
  1644|             )
  1645|             instance.wait()
  1646|             vm_result = instance.result()
  1647|             ret = vm_result.as_dict()
  1648|         except CloudError as exc:
  1649|             salt.utils.azurearm.log_cloud_error(
  1650|                 "compute", f"Error attempting to stop {name}: {exc.message}"
  1651|             )
  1652|             ret = {"error": exc.message}
  1653|     return ret
  1654| @_deprecation_message
  1655| def start(name, call=None):
  1656|     """
  1657|     .. versionadded:: 2019.2.0
  1658|     Start a VM
  1659|     CLI Examples:
  1660|     .. code-block:: bash
  1661|          salt-cloud -a start myminion
  1662|     """
  1663|     if call == "function":
  1664|         raise SaltCloudSystemExit(
  1665|             "The start action must be called with -a or --action."
  1666|         )
  1667|     compconn = get_conn(client_type="compute")
  1668|     resource_group = config.get_cloud_config_value(
  1669|         "resource_group", get_configured_provider(), __opts__, search_global=False
  1670|     )
  1671|     ret = {}
  1672|     if not resource_group:
  1673|         groups = list_resource_groups()
  1674|         for group in groups:
  1675|             try:
  1676|                 instance = compconn.virtual_machines.start(
  1677|                     vm_name=name, resource_group_name=group
  1678|                 )
  1679|                 instance.wait()
  1680|                 vm_result = instance.result()
  1681|                 ret = vm_result.as_dict()
  1682|                 break
  1683|             except CloudError as exc:
  1684|                 if "was not found" in exc.message:
  1685|                     continue
  1686|                 else:
  1687|                     ret = {"error": exc.message}
  1688|         if not ret:
  1689|             salt.utils.azurearm.log_cloud_error(
  1690|                 "compute", f"Unable to find virtual machine with name: {name}"
  1691|             )
  1692|             ret = {"error": f"Unable to find virtual machine with name: {name}"}
  1693|     else:
  1694|         try:
  1695|             instance = compconn.virtual_machines.start(
  1696|                 vm_name=name, resource_group_name=resource_group
  1697|             )
  1698|             instance.wait()
  1699|             vm_result = instance.result()
  1700|             ret = vm_result.as_dict()
  1701|         except CloudError as exc:
  1702|             salt.utils.azurearm.log_cloud_error(
  1703|                 "compute",
  1704|                 f"Error attempting to start {name}: {exc.message}",
  1705|             )
  1706|             ret = {"error": exc.message}
  1707|     return ret


# ====================================================================
# FILE: salt/cloud/clouds/clc.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 226-266 ---
   226| def avail_locations(call=None):
   227|     """
   228|     returns a list of locations available to you
   229|     """
   230|     creds = get_creds()
   231|     clc.v1.SetCredentials(creds["token"], creds["token_pass"])
   232|     locations = clc.v1.Account.GetLocations()
   233|     return locations
   234| def avail_sizes(call=None):
   235|     """
   236|     use templates for this
   237|     """
   238|     return {"Sizes": "Sizes are built into templates. Choose appropriate template"}
   239| def get_build_status(req_id, nodename):
   240|     """
   241|     get the build status from CLC to make sure we don't return to early
   242|     """
   243|     counter = 0
   244|     req_id = str(req_id)
   245|     while counter < 10:
   246|         queue = clc.v1.Blueprint.GetStatus(request_id=req_id)
   247|         if queue["PercentComplete"] == 100:
   248|             server_name = queue["Servers"][0]
   249|             creds = get_creds()
   250|             clc.v2.SetCredentials(creds["user"], creds["password"])
   251|             ip_addresses = clc.v2.Server(server_name).ip_addresses
   252|             internal_ip_address = ip_addresses[0]["internal"]
   253|             return internal_ip_address
   254|         else:
   255|             counter = counter + 1
   256|             log.info(
   257|                 "Creating Cloud VM %s Time out in %s minutes",
   258|                 nodename,
   259|                 str(10 - counter),
   260|             )
   261|             time.sleep(60)
   262| def create(vm_):
   263|     """
   264|     get the system build going
   265|     """
   266|     creds = get_creds()


# ====================================================================
# FILE: salt/cloud/clouds/digitalocean.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 260-317 ---
   260|     if key_filename is not None and not os.path.isfile(key_filename):
   261|         raise SaltCloudConfigError(
   262|             f"The defined key_filename '{key_filename}' does not exist"
   263|         )
   264|     if not __opts__.get("ssh_agent", False) and key_filename is None:
   265|         raise SaltCloudConfigError(
   266|             "The DigitalOcean driver requires an ssh_key_file and an ssh_key_name "
   267|             "because it does not supply a root password upon building the server."
   268|         )
   269|     ssh_interface = config.get_cloud_config_value(
   270|         "ssh_interface", vm_, __opts__, search_global=False, default="public"
   271|     )
   272|     if ssh_interface in ["private", "public"]:
   273|         log.info("ssh_interface: Setting interface for ssh to %s", ssh_interface)
   274|         kwargs["ssh_interface"] = ssh_interface
   275|     else:
   276|         raise SaltCloudConfigError(
   277|             "The DigitalOcean driver requires ssh_interface to be defined as 'public'"
   278|             " or 'private'."
   279|         )
   280|     private_networking = config.get_cloud_config_value(
   281|         "private_networking",
   282|         vm_,
   283|         __opts__,
   284|         search_global=False,
   285|         default=None,
   286|     )
   287|     if private_networking is not None:
   288|         if not isinstance(private_networking, bool):
   289|             raise SaltCloudConfigError(
   290|                 "'private_networking' should be a boolean value."
   291|             )
   292|         kwargs["private_networking"] = private_networking
   293|     if not private_networking and ssh_interface == "private":
   294|         raise SaltCloudConfigError(
   295|             "The DigitalOcean driver requires ssh_interface if defined as 'private' "
   296|             "then private_networking should be set as 'True'."
   297|         )
   298|     backups_enabled = config.get_cloud_config_value(
   299|         "backups_enabled",
   300|         vm_,
   301|         __opts__,
   302|         search_global=False,
   303|         default=None,
   304|     )
   305|     if backups_enabled is not None:
   306|         if not isinstance(backups_enabled, bool):
   307|             raise SaltCloudConfigError("'backups_enabled' should be a boolean value.")
   308|         kwargs["backups"] = backups_enabled
   309|     ipv6 = config.get_cloud_config_value(
   310|         "ipv6",
   311|         vm_,
   312|         __opts__,
   313|         search_global=False,
   314|         default=None,
   315|     )
   316|     if ipv6 is not None:
   317|         if not isinstance(ipv6, bool):

# --- HUNK 2: Lines 366-412 ---
   366|         dns_hostname = config.get_cloud_config_value(
   367|             "dns_hostname",
   368|             vm_,
   369|             __opts__,
   370|             search_global=False,
   371|             default=default_dns_hostname,
   372|         )
   373|         dns_domain = config.get_cloud_config_value(
   374|             "dns_domain",
   375|             vm_,
   376|             __opts__,
   377|             search_global=False,
   378|             default=default_dns_domain,
   379|         )
   380|         if dns_hostname and dns_domain:
   381|             log.info(
   382|                 'create_dns_record: using dns_hostname="%s", dns_domain="%s"',
   383|                 dns_hostname,
   384|                 dns_domain,
   385|             )
   386|             def __add_dns_addr__(t, d):
   387|                 return post_dns_record(
   388|                     dns_domain=dns_domain,
   389|                     name=dns_hostname,
   390|                     record_type=t,
   391|                     record_data=d,
   392|                 )
   393|             log.debug("create_dns_record: %s", __add_dns_addr__)
   394|         else:
   395|             log.error(
   396|                 "create_dns_record: could not determine dns_hostname and/or dns_domain"
   397|             )
   398|             raise SaltCloudConfigError(
   399|                 "'create_dns_record' must be a dict specifying \"domain\" "
   400|                 'and "hostname" or the minion name must be an FQDN.'
   401|             )
   402|     __utils__["cloud.fire_event"](
   403|         "event",
   404|         "requesting instance",
   405|         "salt/cloud/{}/requesting".format(vm_["name"]),
   406|         args=__utils__["cloud.filter_event"]("requesting", kwargs, list(kwargs)),
   407|         sock_dir=__opts__["sock_dir"],
   408|         transport=__opts__["transport"],
   409|     )
   410|     try:
   411|         ret = create_node(kwargs)
   412|     except Exception as exc:  # pylint: disable=broad-except

# --- HUNK 3: Lines 487-549 ---
   487|         ),
   488|         sock_dir=__opts__["sock_dir"],
   489|         transport=__opts__["transport"],
   490|     )
   491|     return ret
   492| def query(
   493|     method="droplets", droplet_id=None, command=None, args=None, http_method="get"
   494| ):
   495|     """
   496|     Make a web call to DigitalOcean
   497|     """
   498|     base_path = str(
   499|         config.get_cloud_config_value(
   500|             "api_root",
   501|             get_configured_provider(),
   502|             __opts__,
   503|             search_global=False,
   504|             default="https://api.digitalocean.com/v2",
   505|         )
   506|     )
   507|     path = f"{base_path}/{method}/"
   508|     if droplet_id:
   509|         path += f"{droplet_id}/"
   510|     if command:
   511|         path += command
   512|     if not isinstance(args, dict):
   513|         args = {}
   514|     personal_access_token = config.get_cloud_config_value(
   515|         "personal_access_token",
   516|         get_configured_provider(),
   517|         __opts__,
   518|         search_global=False,
   519|     )
   520|     data = salt.utils.json.dumps(args)
   521|     requester = getattr(requests, http_method)
   522|     request = requester(
   523|         path,
   524|         data=data,
   525|         headers={
   526|             "Authorization": "Bearer " + personal_access_token,
   527|             "Content-Type": "application/json",
   528|         },
   529|         timeout=120,
   530|     )
   531|     if request.status_code > 299:
   532|         raise SaltCloudSystemExit(
   533|             "An error occurred while querying DigitalOcean. HTTP Code: {}  "
   534|             "Error: '{}'".format(
   535|                 request.status_code,
   536|                 request.text,
   537|             )
   538|         )
   539|     log.debug(request.url)
   540|     if request.status_code == 204:
   541|         return True
   542|     content = request.text
   543|     result = salt.utils.json.loads(content)
   544|     if result.get("status", "").lower() == "error":
   545|         raise SaltCloudSystemExit(pprint.pformat(result.get("error_message", {})))
   546|     return result
   547| def script(vm_):
   548|     """
   549|     Return the script deployment object

# --- HUNK 4: Lines 761-801 ---
   761|                 "data": kwargs["record_data"],
   762|             },
   763|             http_method="post",
   764|         )
   765|         return result
   766|     return False
   767| def destroy_dns_records(fqdn):
   768|     """
   769|     Deletes DNS records for the given hostname if the domain is managed with DO.
   770|     """
   771|     domain = ".".join(fqdn.split(".")[-2:])
   772|     hostname = ".".join(fqdn.split(".")[:-2])
   773|     try:
   774|         response = query(method="domains", droplet_id=domain, command="records")
   775|     except SaltCloudSystemExit:
   776|         log.debug("Failed to find domains.")
   777|         return False
   778|     log.debug("found DNS records: %s", pprint.pformat(response))
   779|     records = response["domain_records"]
   780|     if records:
   781|         record_ids = [r["id"] for r in records if r["name"].decode() == hostname]
   782|         log.debug("deleting DNS record IDs: %s", record_ids)
   783|         for id_ in record_ids:
   784|             try:
   785|                 log.info("deleting DNS record %s", id_)
   786|                 ret = query(
   787|                     method="domains",
   788|                     droplet_id=domain,
   789|                     command=f"records/{id_}",
   790|                     http_method="delete",
   791|                 )
   792|             except SaltCloudSystemExit:
   793|                 log.error(
   794|                     "failed to delete DNS domain %s record ID %s.", domain, hostname
   795|                 )
   796|             log.debug("DNS deletion REST call returned: %s", pprint.pformat(ret))
   797|     return False
   798| def show_pricing(kwargs=None, call=None):
   799|     """
   800|     Show pricing for a particular profile. This is only an estimate, based on
   801|     unofficial pricing sources.

# --- HUNK 5: Lines 959-1007 ---
   959|     .. code-block:: bash
   960|         salt-cloud -f unassign_floating_ip my-digitalocean-config floating_ip='45.55.96.47'
   961|     """
   962|     if call != "function":
   963|         log.error(
   964|             "The inassign_floating_ip function must be called with -f or --function."
   965|         )
   966|         return False
   967|     if not kwargs:
   968|         kwargs = {}
   969|     if "floating_ip" not in kwargs:
   970|         log.error("A floating IP is required.")
   971|         return False
   972|     result = query(
   973|         method="floating_ips",
   974|         command=kwargs["floating_ip"] + "/actions",
   975|         args={"type": "unassign"},
   976|         http_method="post",
   977|     )
   978|     return result
   979| def _list_nodes(full=False, for_output=False):
   980|     """
   981|     Helper function to format and parse node data.
   982|     """
   983|     fetch = True
   984|     page = 1
   985|     ret = {}
   986|     while fetch:
   987|         items = query(method="droplets", command="?page=" + str(page) + "&per_page=200")
   988|         for node in items["droplets"]:
   989|             name = node["name"]
   990|             ret[name] = {}
   991|             if full:
   992|                 ret[name] = _get_full_output(node, for_output=for_output)
   993|             else:
   994|                 public_ips, private_ips = _get_ips(node["networks"])
   995|                 ret[name] = {
   996|                     "id": node["id"],
   997|                     "image": node["image"]["name"],
   998|                     "name": name,
   999|                     "private_ips": private_ips,
  1000|                     "public_ips": public_ips,
  1001|                     "size": node["size_slug"],
  1002|                     "state": str(node["status"]),
  1003|                 }
  1004|         page += 1
  1005|         try:
  1006|             fetch = "next" in items["links"]["pages"]
  1007|         except KeyError:


# ====================================================================
# FILE: salt/cloud/clouds/linode.py
# Total hunks: 10
# ====================================================================
# --- HUNK 1: Lines 1-179 ---
     1| r"""
     2| The Linode Cloud Module
     3| =======================
     4| The Linode cloud module is used to interact with the Linode Cloud.
     5| You can target a specific version of the Linode API with the ``api_version`` parameter. The default is ``v3``.
     6| Provider
     7| --------
     8| The following provider parameters are supported:
     9| - **apikey**: (required) The key to use to authenticate with the Linode API.
    10| - **password**: (required) The default password to set on new VMs. Must be 8 characters with at least one lowercase, uppercase, and numeric.
    11| - **api_version**: (optional) The version of the Linode API to interact with. Defaults to ``v3``.
    12| - **poll_interval**: (optional) The rate of time in milliseconds to poll the Linode API for changes. Defaults to ``500``.
    13| - **ratelimit_sleep**: (optional) The time in seconds to wait before retrying after a ratelimit has been enforced. Defaults to ``0``.
    14| .. note::
    15|     APIv3 usage is deprecated and will be removed in a future release in favor of APIv4. To move to APIv4 now,
    16|     set the ``api_version`` parameter in your provider configuration to ``v4``. See the full migration guide
    17|     here https://docs.saltproject.io/en/latest/topics/cloud/linode.html#migrating-to-apiv4.
    18| Set up the provider configuration at ``/etc/salt/cloud.providers`` or ``/etc/salt/cloud.providers.d/linode.conf``:
    19| .. code-block:: yaml
    20|     my-linode-provider:
    21|         driver: linode
    22|         api_version: v4
    23|         apikey: f4ZsmwtB1c7f85Jdu43RgXVDFlNjuJaeIYV8QMftTqKScEB2vSosFSr...
    24|         password: F00barbaz
    25| For use with APIv3 (deprecated):
    26| .. code-block:: yaml
    27|     my-linode-provider-v3:
    28|         driver: linode
    29|         apikey: f4ZsmwtB1c7f85Jdu43RgXVDFlNjuJaeIYV8QMftTqKScEB2vSosFSr...
    30|         password: F00barbaz
    31| Profile
    32| -------
    33| The following profile parameters are supported:
    34| - **size**: (required) The size of the VM. This should be a Linode instance type ID (i.e. ``g6-standard-2``). For APIv3, this would be a plan ID (i.e. ``Linode 2GB``). Run ``salt-cloud -f avail_sizes my-linode-provider`` for options.
    35| - **location**: (required) The location of the VM. This should be a Linode region (e.g. ``us-east``). For APIv3, this would be a datacenter location (i.e. ``Newark, NJ, USA``). Run ``salt-cloud -f avail_locations my-linode-provider`` for options.
    36| - **image**: (required) The image to deploy the boot disk from. This should be an image ID (e.g. ``linode/ubuntu16.04``); official images start with ``linode/``. For APIv3, this would be an image label (i.e. Ubuntu 16.04). Run ``salt-cloud -f avail_images my-linode-provider`` for more options.
    37| - **password**: (\*required) The default password for the VM. Must be provided at the profile or provider level.
    38| - **assign_private_ip**: (optional) Whether or not to assign a private key to the VM. Defaults to ``False``.
    39| - **ssh_interface**: (optional) The interface with which to connect over SSH. Valid options are ``private_ips`` or ``public_ips``. Defaults to ``public_ips``.
    40| - **ssh_pubkey**: (optional) The public key to authorize for SSH with the VM.
    41| - **swap**: (optional) The amount of disk space to allocate for the swap partition. Defaults to ``256``.
    42| - **clonefrom**: (optional) The name of the Linode to clone from.
    43| - **disk_size**: (deprecated, optional) The amount of disk space to allocate for the OS disk. This has no effect with APIv4; the size of the boot disk will be the remainder of disk space after the swap parition is allocated.
    44| Set up a profile configuration in ``/etc/salt/cloud.profiles.d/``:
    45| .. code-block:: yaml
    46|     my-linode-profile:
    47|         provider: my-linode-provider
    48|         size: g6-standard-1
    49|         image: linode/alpine3.12
    50|         location: us-east
    51|     my-linode-profile-advanced:
    52|         provider: my-linode-provider
    53|         size: g6-standard-3
    54|         image: linode/alpine3.10
    55|         location: eu-west
    56|         password: bogus123X
    57|         assign_private_ip: true
    58|         ssh_interface: private_ips
    59|         ssh_pubkey: ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB...
    60|         swap_size: 512
    61|     my-linode-profile-v3:
    62|         provider: my-linode-provider-v3
    63|         size: Nanode 1GB
    64|         image: Alpine 3.12
    65|         location: Fremont, CA, USA
    66| Migrating to APIv4
    67| ------------------
    68| In order to target APIv4, ensure your provider configuration has ``api_version`` set to ``v4``.
    69| You will also need to generate a new token for your account. See https://www.linode.com/docs/platform/api/getting-started-with-the-linode-api/#create-an-api-token
    70| There are a few changes to note:
    71| - There has been a general move from label references to ID references. The profile configuration parameters ``location``, ``size``, and ``image`` have moved from being label based references to IDs. See the profile section for more information. In addition to these inputs being changed, ``avail_sizes``, ``avail_locations``, and ``avail_images`` now output options sorted by ID instead of label.
    72| - The ``disk_size`` profile configuration parameter has been deprecated and will not be taken into account when creating new VMs while targeting APIv4.
    73| :maintainer: Charles Kenney <ckenney@linode.com>
    74| :maintainer: Phillip Campbell <pcampbell@linode.com>
    75| :depends: requests
    76| """
    77| import abc
    78| import datetime
    79| import json
    80| import logging
    81| import pprint
    82| import re
    83| import time
    84| from pathlib import Path
    85| import salt.config as config
    86| from salt._compat import ipaddress
    87| from salt.exceptions import (
    88|     SaltCloudConfigError,
    89|     SaltCloudException,
    90|     SaltCloudNotFound,
    91|     SaltCloudSystemExit,
    92| )
    93| try:
    94|     import requests
    95|     HAS_REQUESTS = True
    96| except ImportError:
    97|     HAS_REQUESTS = False
    98| log = logging.getLogger(__name__)
    99| HAS_WARNED_FOR_API_V3 = False
   100| LASTCALL = int(time.mktime(datetime.datetime.now().timetuple()))
   101| LINODE_STATUS = {
   102|     "boot_failed": {"code": -2, "descr": "Boot Failed (not in use)"},
   103|     "beeing_created": {"code": -1, "descr": "Being Created"},
   104|     "brand_new": {"code": 0, "descr": "Brand New"},
   105|     "running": {"code": 1, "descr": "Running"},
   106|     "poweroff": {"code": 2, "descr": "Powered Off"},
   107|     "shutdown": {"code": 3, "descr": "Shutting Down (not in use)"},
   108|     "save_to_disk": {"code": 4, "descr": "Saved to Disk (not in use)"},
   109| }
   110| __virtualname__ = "linode"
   111| def __virtual__():
   112|     """
   113|     Check for Linode configs.
   114|     """
   115|     if get_configured_provider() is False:
   116|         return False
   117|     if _get_dependencies() is False:
   118|         return False
   119|     return __virtualname__
   120| def _get_active_provider_name():
   121|     try:
   122|         return __active_provider_name__.value()
   123|     except AttributeError:
   124|         return __active_provider_name__
   125| def get_configured_provider():
   126|     """
   127|     Return the first configured instance.
   128|     """
   129|     return config.is_provider_configured(
   130|         __opts__,
   131|         _get_active_provider_name() or __virtualname__,
   132|         ("apikey", "password"),
   133|     )
   134| def _get_dependencies():
   135|     """
   136|     Warn if dependencies aren't met.
   137|     """
   138|     deps = {"requests": HAS_REQUESTS}
   139|     return config.check_driver_dependencies(__virtualname__, deps)
   140| def _get_api_version():
   141|     """
   142|     Return the configured Linode API version.
   143|     """
   144|     return config.get_cloud_config_value(
   145|         "api_version",
   146|         get_configured_provider(),
   147|         __opts__,
   148|         search_global=False,
   149|         default="v3",
   150|     )
   151| def _is_api_v3():
   152|     """
   153|     Return whether the configured Linode API version is ``v3``.
   154|     """
   155|     return _get_api_version() == "v3"
   156| def _get_cloud_interface():
   157|     if _is_api_v3():
   158|         return LinodeAPIv3()
   159|     return LinodeAPIv4()
   160| def _get_api_key():
   161|     """
   162|     Returned the configured Linode API key.
   163|     """
   164|     val = config.get_cloud_config_value(
   165|         "api_key",
   166|         get_configured_provider(),
   167|         __opts__,
   168|         search_global=False,
   169|         default=config.get_cloud_config_value(
   170|             "apikey", get_configured_provider(), __opts__, search_global=False
   171|         ),
   172|     )
   173|     return val
   174| def _get_ratelimit_sleep():
   175|     """
   176|     Return the configured time to wait before retrying after a ratelimit has been enforced.
   177|     """
   178|     return config.get_cloud_config_value(
   179|         "ratelimit_sleep",

# --- HUNK 2: Lines 190-236 ---
   190|         "poll_interval",
   191|         get_configured_provider(),
   192|         __opts__,
   193|         search_global=False,
   194|         default=500,
   195|     )
   196| def _get_password(vm_):
   197|     r"""
   198|     Return the password to use for a VM.
   199|     vm\_
   200|         The configuration to obtain the password from.
   201|     """
   202|     return config.get_cloud_config_value(
   203|         "password",
   204|         vm_,
   205|         __opts__,
   206|         default=config.get_cloud_config_value(
   207|             "passwd", vm_, __opts__, search_global=False
   208|         ),
   209|         search_global=False,
   210|     )
   211| def _get_root_disk_size(vm_):
   212|     """
   213|     Return the specified size of the data partition.
   214|     """
   215|     return config.get_cloud_config_value(
   216|         "disk_size", vm_, __opts__, search_global=False
   217|     )
   218| def _get_private_ip(vm_):
   219|     """
   220|     Return True if a private ip address is requested
   221|     """
   222|     return config.get_cloud_config_value(
   223|         "assign_private_ip", vm_, __opts__, default=False
   224|     )
   225| def _get_ssh_key_files(vm_):
   226|     """
   227|     Return the configured file paths of the SSH keys.
   228|     """
   229|     return config.get_cloud_config_value(
   230|         "ssh_key_files", vm_, __opts__, search_global=False, default=[]
   231|     )
   232| def _get_ssh_key(vm_):
   233|     r"""
   234|     Return the SSH pubkey.
   235|     vm\_
   236|         The configuration to obtain the public key from.

# --- HUNK 3: Lines 273-426 ---
   273|     .. versionadded:: 2015.5.6
   274|     name
   275|         The VM name to validate
   276|     """
   277|     name = str(name)
   278|     name_length = len(name)
   279|     regex = re.compile(r"^[a-zA-Z0-9][A-Za-z0-9_-]*[a-zA-Z0-9]$")
   280|     if name_length < 3 or name_length > 48:
   281|         ret = False
   282|     elif not re.match(regex, name):
   283|         ret = False
   284|     else:
   285|         ret = True
   286|     if ret is False:
   287|         log.warning(
   288|             "A Linode label may only contain ASCII letters or numbers, dashes, and "
   289|             "underscores, must begin and end with letters or numbers, and be at least "
   290|             "three characters in length."
   291|         )
   292|     return ret
   293| def _warn_for_api_v3():
   294|     global HAS_WARNED_FOR_API_V3
   295|     if not HAS_WARNED_FOR_API_V3:
   296|         log.warning(
   297|             "Linode APIv3 has been deprecated and support will be removed "
   298|             "in future releases. Please plan to upgrade to APIv4. For more "
   299|             "information, see"
   300|             " https://docs.saltproject.io/en/latest/topics/cloud/linode.html#migrating-to-apiv4."
   301|         )
   302|         HAS_WARNED_FOR_API_V3 = True
   303| class LinodeAPI:
   304|     @abc.abstractmethod
   305|     def avail_images(self):
   306|         """avail_images implementation"""
   307|     @abc.abstractmethod
   308|     def avail_locations(self):
   309|         """avail_locations implementation"""
   310|     @abc.abstractmethod
   311|     def avail_sizes(self):
   312|         """avail_sizes implementation"""
   313|     @abc.abstractmethod
   314|     def boot(self, name=None, kwargs=None):
   315|         """boot implementation"""
   316|     @abc.abstractmethod
   317|     def clone(self, kwargs=None):
   318|         """clone implementation"""
   319|     @abc.abstractmethod
   320|     def create_config(self, kwargs=None):
   321|         """create_config implementation"""
   322|     @abc.abstractmethod
   323|     def create(self, vm_):
   324|         """create implementation"""
   325|     @abc.abstractmethod
   326|     def destroy(self, name):
   327|         """destroy implementation"""
   328|     @abc.abstractmethod
   329|     def get_config_id(self, kwargs=None):
   330|         """get_config_id implementation"""
   331|     @abc.abstractmethod
   332|     def list_nodes(self):
   333|         """list_nodes implementation"""
   334|     @abc.abstractmethod
   335|     def list_nodes_full(self):
   336|         """list_nodes_full implementation"""
   337|     @abc.abstractmethod
   338|     def list_nodes_min(self):
   339|         """list_nodes_min implementation"""
   340|     @abc.abstractmethod
   341|     def reboot(self, name):
   342|         """reboot implementation"""
   343|     @abc.abstractmethod
   344|     def show_instance(self, name):
   345|         """show_instance implementation"""
   346|     @abc.abstractmethod
   347|     def show_pricing(self, kwargs=None):
   348|         """show_pricing implementation"""
   349|     @abc.abstractmethod
   350|     def start(self, name):
   351|         """start implementation"""
   352|     @abc.abstractmethod
   353|     def stop(self, name):
   354|         """stop implementation"""
   355|     @abc.abstractmethod
   356|     def _get_linode_by_name(self, name):
   357|         """_get_linode_by_name implementation"""
   358|     @abc.abstractmethod
   359|     def _get_linode_by_id(self, linode_id):
   360|         """_get_linode_by_id implementation"""
   361|     def get_plan_id(self, kwargs=None):
   362|         """get_plan_id implementation"""
   363|         raise SaltCloudSystemExit(
   364|             "The get_plan_id is not supported by this api_version."
   365|         )
   366|     def get_linode(self, kwargs=None):
   367|         name = kwargs.get("name", None)
   368|         linode_id = kwargs.get("linode_id", None)
   369|         if linode_id is not None:
   370|             return self._get_linode_by_id(linode_id)
   371|         elif name is not None:
   372|             return self._get_linode_by_name(name)
   373|         raise SaltCloudSystemExit(
   374|             "The get_linode function requires either a 'name' or a 'linode_id'."
   375|         )
   376|     def list_nodes_select(self, call):
   377|         return __utils__["cloud.list_nodes_select"](
   378|             self.list_nodes_full(),
   379|             __opts__["query.selection"],
   380|             call,
   381|         )
   382| class LinodeAPIv4(LinodeAPI):
   383|     def _query(self, path=None, method="GET", data=None, headers=None):
   384|         """
   385|         Make a call to the Linode API.
   386|         """
   387|         api_version = _get_api_version()
   388|         api_key = _get_api_key()
   389|         ratelimit_sleep = _get_ratelimit_sleep()
   390|         if headers is None:
   391|             headers = {}
   392|         headers["Authorization"] = f"Bearer {api_key}"
   393|         headers["Content-Type"] = "application/json"
   394|         headers["User-Agent"] = "salt-cloud-linode"
   395|         url = f"https://api.linode.com/{api_version}{path}"
   396|         decode = method != "DELETE"
   397|         result = None
   398|         log.debug("Linode API request: %s %s", method, url)
   399|         if data is not None:
   400|             log.trace("Linode API request body: %s", data)
   401|         attempt = 0
   402|         while True:
   403|             try:
   404|                 result = requests.request(
   405|                     method, url, json=data, headers=headers, timeout=120
   406|                 )
   407|                 log.debug("Linode API response status code: %d", result.status_code)
   408|                 log.trace("Linode API response body: %s", result.text)
   409|                 result.raise_for_status()
   410|                 break
   411|             except requests.exceptions.HTTPError as exc:
   412|                 err_response = exc.response
   413|                 err_data = self._get_response_json(err_response)
   414|                 status_code = err_response.status_code
   415|                 if status_code == 429:
   416|                     log.debug(
   417|                         "received rate limit; retrying in %d seconds", ratelimit_sleep
   418|                     )
   419|                     time.sleep(ratelimit_sleep)
   420|                     continue
   421|                 if err_data is not None:
   422|                     if "error" in err_data:
   423|                         raise SaltCloudSystemExit(
   424|                             "Linode API reported error: {}".format(err_data["error"])
   425|                         )
   426|                     elif "errors" in err_data:

# --- HUNK 4: Lines 445-519 ---
   445|             return self._get_response_json(result)
   446|         return result
   447|     def avail_images(self):
   448|         response = self._query(path="/images")
   449|         ret = {}
   450|         for image in response["data"]:
   451|             ret[image["id"]] = image
   452|         return ret
   453|     def avail_locations(self):
   454|         response = self._query(path="/regions")
   455|         ret = {}
   456|         for region in response["data"]:
   457|             ret[region["id"]] = region
   458|         return ret
   459|     def avail_sizes(self):
   460|         response = self._query(path="/linode/types")
   461|         ret = {}
   462|         for instance_type in response["data"]:
   463|             ret[instance_type["id"]] = instance_type
   464|         return ret
   465|     def boot(self, name=None, kwargs=None):
   466|         instance = self.get_linode(
   467|             kwargs={"linode_id": kwargs.get("linode_id", None), "name": name}
   468|         )
   469|         config_id = kwargs.get("config_id", None)
   470|         check_running = kwargs.get("check_running", True)
   471|         linode_id = instance.get("id", None)
   472|         name = instance.get("label", None)
   473|         if check_running:
   474|             if instance["status"] == "running":
   475|                 raise SaltCloudSystemExit(
   476|                     "Cannot boot Linode {0} ({1}). "
   477|                     "Linode {0} is already running.".format(name, linode_id)
   478|                 )
   479|         response = self._query(
   480|             f"/linode/instances/{linode_id}/boot",
   481|             method="POST",
   482|             data={"config_id": config_id},
   483|         )
   484|         self._wait_for_linode_status(linode_id, "running")
   485|         return True
   486|     def clone(self, kwargs=None):
   487|         linode_id = kwargs.get("linode_id", None)
   488|         location = kwargs.get("location", None)
   489|         size = kwargs.get("size", None)
   490|         if "datacenter_id" in kwargs:
   491|             log.warning(
   492|                 "The 'datacenter_id' argument has been deprecated and will be "
   493|                 "removed in future releases. Please use 'location' instead."
   494|             )
   495|         if "plan_id" in kwargs:
   496|             log.warning(
   497|                 "The 'plan_id' argument has been deprecated and will be "
   498|                 "removed in future releases. Please use 'size' instead."
   499|             )
   500|         for item in [linode_id, location, size]:
   501|             if item is None:
   502|                 raise SaltCloudSystemExit(
   503|                     "The clone function requires a 'linode_id', 'location',"
   504|                     "and 'size' to be provided."
   505|                 )
   506|         return self._query(
   507|             f"/linode/instances/{linode_id}/clone",
   508|             method="POST",
   509|             data={"region": location, "type": size},
   510|         )
   511|     def create_config(self, kwargs=None):
   512|         name = kwargs.get("name", None)
   513|         linode_id = kwargs.get("linode_id", None)
   514|         root_disk_id = kwargs.get("root_disk_id", None)
   515|         swap_disk_id = kwargs.get("swap_disk_id", None)
   516|         data_disk_id = kwargs.get("data_disk_id", None)
   517|         if not name and not linode_id:
   518|             raise SaltCloudSystemExit(
   519|                 "The create_config function requires either a 'name' or 'linode_id'"

# --- HUNK 5: Lines 540-603 ---
   540|         if not _validate_name(name):
   541|             return False
   542|         __utils__["cloud.fire_event"](
   543|             "event",
   544|             "starting create",
   545|             f"salt/cloud/{name}/creating",
   546|             args=__utils__["cloud.filter_event"](
   547|                 "creating", vm_, ["name", "profile", "provider", "driver"]
   548|             ),
   549|             sock_dir=__opts__["sock_dir"],
   550|             transport=__opts__["transport"],
   551|         )
   552|         log.info("Creating Cloud VM %s", name)
   553|         result = None
   554|         pub_ssh_keys = _get_ssh_keys(vm_)
   555|         ssh_interface = _get_ssh_interface(vm_)
   556|         use_private_ip = ssh_interface == "private_ips"
   557|         assign_private_ip = _get_private_ip(vm_) or use_private_ip
   558|         password = _get_password(vm_)
   559|         swap_size = _get_swap_size(vm_)
   560|         clonefrom_name = vm_.get("clonefrom", None)
   561|         instance_type = vm_.get("size", None)
   562|         image = vm_.get("image", None)
   563|         should_clone = True if clonefrom_name else False
   564|         if should_clone:
   565|             clone_linode = self.get_linode(kwargs={"name": clonefrom_name})
   566|             result = clone(
   567|                 {
   568|                     "linode_id": clone_linode["id"],
   569|                     "location": clone_linode["region"],
   570|                     "size": clone_linode["type"],
   571|                 }
   572|             )
   573|             if assign_private_ip:
   574|                 self._query(
   575|                     "/networking/ips",
   576|                     method="POST",
   577|                     data={"type": "ipv4", "public": False, "linode_id": result["id"]},
   578|                 )
   579|         else:
   580|             result = self._query(
   581|                 "/linode/instances",
   582|                 method="POST",
   583|                 data={
   584|                     "label": name,
   585|                     "type": instance_type,
   586|                     "region": vm_.get("location", None),
   587|                     "private_ip": assign_private_ip,
   588|                     "booted": True,
   589|                     "root_pass": password,
   590|                     "authorized_keys": pub_ssh_keys,
   591|                     "image": image,
   592|                     "swap_size": swap_size,
   593|                 },
   594|             )
   595|         linode_id = result.get("id", None)
   596|         self._wait_for_event("linode_create", "linode", linode_id, "finished")
   597|         log.debug("linode '%s' has been created", name)
   598|         if should_clone:
   599|             self.boot(kwargs={"linode_id": linode_id})
   600|         self._wait_for_linode_status(linode_id, "running")
   601|         public_ips, private_ips = self._get_ips(linode_id)
   602|         data = {}
   603|         data["id"] = linode_id

# --- HUNK 6: Lines 835-2015 ---
   835|         status = event.get("status")
   836|         action = event.get("action")
   837|         entity = event.get("entity")
   838|         if status == "failed":
   839|             raise SaltCloudSystemExit(
   840|                 "event {} for {} (id={}) failed".format(
   841|                     action, entity["type"], entity["id"]
   842|                 )
   843|             )
   844|         return status == desired_status
   845|     def _wait_for_event(self, action, entity, entity_id, status, timeout=None):
   846|         event_filter = {
   847|             "+order_by": "created",
   848|             "+order": "desc",
   849|             "seen": False,
   850|             "action": action,
   851|             "entity.id": entity_id,
   852|             "entity.type": entity,
   853|         }
   854|         last_event = None
   855|         def condition(event):
   856|             return self._check_event_status(event, status)
   857|         while True:
   858|             if last_event is not None:
   859|                 event_filter["+gt"] = last_event
   860|             filter_json = json.dumps(event_filter, separators=(",", ":"))
   861|             result = self._query("/account/events", headers={"X-Filter": filter_json})
   862|             events = result.get("data", [])
   863|             if len(events) == 0:
   864|                 break
   865|             for event in events:
   866|                 event_id = event.get("id")
   867|                 event_entity = event.get("entity", None)
   868|                 last_event = event_id
   869|                 if not event_entity:
   870|                     continue
   871|                 if not (
   872|                     event_entity["type"] == entity
   873|                     and event_entity["id"] == entity_id
   874|                     and event.get("action") == action
   875|                 ):
   876|                     continue
   877|                 if condition(event):
   878|                     return True
   879|                 return self._poll(
   880|                     f"event {event_id} to be '{status}'",
   881|                     lambda: self._query(f"/account/events/{event_id}"),
   882|                     condition,
   883|                     timeout=timeout,
   884|                 )
   885|         return False
   886|     def _get_response_json(self, response):
   887|         json = None
   888|         try:
   889|             json = response.json()
   890|         except ValueError:
   891|             pass
   892|         return json
   893| class LinodeAPIv3(LinodeAPI):
   894|     def __init__(self):
   895|         _warn_for_api_v3()
   896|     def _query(
   897|         self,
   898|         action=None,
   899|         command=None,
   900|         args=None,
   901|         method="GET",
   902|         header_dict=None,
   903|         data=None,
   904|         url="https://api.linode.com/",
   905|     ):
   906|         """
   907|         Make a web call to the Linode API.
   908|         """
   909|         global LASTCALL
   910|         ratelimit_sleep = _get_ratelimit_sleep()
   911|         apikey = _get_api_key()
   912|         if not isinstance(args, dict):
   913|             args = {}
   914|         if "api_key" not in args.keys():
   915|             args["api_key"] = apikey
   916|         if action and "api_action" not in args.keys():
   917|             args["api_action"] = f"{action}.{command}"
   918|         if header_dict is None:
   919|             header_dict = {}
   920|         if method != "POST":
   921|             header_dict["Accept"] = "application/json"
   922|         decode = True
   923|         if method == "DELETE":
   924|             decode = False
   925|         now = int(time.mktime(datetime.datetime.now().timetuple()))
   926|         if LASTCALL >= now:
   927|             time.sleep(ratelimit_sleep)
   928|         result = __utils__["http.query"](
   929|             url,
   930|             method,
   931|             params=args,
   932|             data=data,
   933|             header_dict=header_dict,
   934|             decode=decode,
   935|             decode_type="json",
   936|             text=True,
   937|             status=True,
   938|             hide_fields=["api_key", "rootPass"],
   939|             opts=__opts__,
   940|         )
   941|         if "ERRORARRAY" in result["dict"]:
   942|             if result["dict"]["ERRORARRAY"]:
   943|                 error_list = []
   944|                 for error in result["dict"]["ERRORARRAY"]:
   945|                     msg = error["ERRORMESSAGE"]
   946|                     if msg == "Authentication failed":
   947|                         raise SaltCloudSystemExit(
   948|                             "Linode API Key is expired or invalid"
   949|                         )
   950|                     else:
   951|                         error_list.append(msg)
   952|                 raise SaltCloudException(
   953|                     "Linode API reported error(s): {}".format(", ".join(error_list))
   954|                 )
   955|         LASTCALL = int(time.mktime(datetime.datetime.now().timetuple()))
   956|         log.debug("Linode Response Status Code: %s", result["status"])
   957|         return result["dict"]
   958|     def avail_images(self):
   959|         response = self._query("avail", "distributions")
   960|         ret = {}
   961|         for item in response["DATA"]:
   962|             name = item["LABEL"]
   963|             ret[name] = item
   964|         return ret
   965|     def avail_locations(self):
   966|         response = self._query("avail", "datacenters")
   967|         ret = {}
   968|         for item in response["DATA"]:
   969|             name = item["LOCATION"]
   970|             ret[name] = item
   971|         return ret
   972|     def avail_sizes(self):
   973|         response = self._query("avail", "LinodePlans")
   974|         ret = {}
   975|         for item in response["DATA"]:
   976|             name = item["LABEL"]
   977|             ret[name] = item
   978|         return ret
   979|     def boot(self, name=None, kwargs=None):
   980|         linode_id = kwargs.get("linode_id", None)
   981|         config_id = kwargs.get("config_id", None)
   982|         check_running = kwargs.get("check_running", True)
   983|         if config_id is None:
   984|             raise SaltCloudSystemExit("The boot function requires a 'config_id'.")
   985|         if linode_id is None:
   986|             linode_id = self._get_linode_id_from_name(name)
   987|             linode_item = name
   988|         else:
   989|             linode_item = linode_id
   990|         if check_running:
   991|             status = get_linode(kwargs={"linode_id": linode_id})["STATUS"]
   992|             if status == "1":
   993|                 raise SaltCloudSystemExit(
   994|                     "Cannot boot Linode {0}. "
   995|                     + f"Linode {linode_item} is already running."
   996|                 )
   997|         response = self._query(
   998|             "linode", "boot", args={"LinodeID": linode_id, "ConfigID": config_id}
   999|         )["DATA"]
  1000|         boot_job_id = response["JobID"]
  1001|         if not self._wait_for_job(linode_id, boot_job_id):
  1002|             log.error("Boot failed for Linode %s.", linode_item)
  1003|             return False
  1004|         return True
  1005|     def clone(self, kwargs=None):
  1006|         linode_id = kwargs.get("linode_id", None)
  1007|         datacenter_id = kwargs.get("datacenter_id", kwargs.get("location"))
  1008|         plan_id = kwargs.get("plan_id", kwargs.get("size"))
  1009|         required_params = [linode_id, datacenter_id, plan_id]
  1010|         for item in required_params:
  1011|             if item is None:
  1012|                 raise SaltCloudSystemExit(
  1013|                     "The clone function requires a 'linode_id', 'datacenter_id', "
  1014|                     "and 'plan_id' to be provided."
  1015|                 )
  1016|         clone_args = {
  1017|             "LinodeID": linode_id,
  1018|             "DatacenterID": datacenter_id,
  1019|             "PlanID": plan_id,
  1020|         }
  1021|         return self._query("linode", "clone", args=clone_args)
  1022|     def create(self, vm_):
  1023|         name = vm_["name"]
  1024|         if not _validate_name(name):
  1025|             return False
  1026|         __utils__["cloud.fire_event"](
  1027|             "event",
  1028|             "starting create",
  1029|             f"salt/cloud/{name}/creating",
  1030|             args=__utils__["cloud.filter_event"](
  1031|                 "creating", vm_, ["name", "profile", "provider", "driver"]
  1032|             ),
  1033|             sock_dir=__opts__["sock_dir"],
  1034|             transport=__opts__["transport"],
  1035|         )
  1036|         log.info("Creating Cloud VM %s", name)
  1037|         data = {}
  1038|         kwargs = {"name": name}
  1039|         plan_id = None
  1040|         size = vm_.get("size")
  1041|         if size:
  1042|             kwargs["size"] = size
  1043|             plan_id = self.get_plan_id(kwargs={"label": size})
  1044|         datacenter_id = None
  1045|         location = vm_.get("location")
  1046|         if location:
  1047|             try:
  1048|                 datacenter_id = self._get_datacenter_id(location)
  1049|             except KeyError:
  1050|                 datacenter_id = 2
  1051|         clonefrom_name = vm_.get("clonefrom")
  1052|         cloning = True if clonefrom_name else False
  1053|         if cloning:
  1054|             linode_id = self._get_linode_id_from_name(clonefrom_name)
  1055|             clone_source = get_linode(kwargs={"linode_id": linode_id})
  1056|             kwargs = {
  1057|                 "clonefrom": clonefrom_name,
  1058|                 "image": f"Clone of {clonefrom_name}",
  1059|             }
  1060|             if size is None:
  1061|                 size = clone_source["TOTALRAM"]
  1062|                 kwargs["size"] = size
  1063|                 plan_id = clone_source["PLANID"]
  1064|             if location is None:
  1065|                 datacenter_id = clone_source["DATACENTERID"]
  1066|             try:
  1067|                 result = clone(
  1068|                     kwargs={
  1069|                         "linode_id": linode_id,
  1070|                         "datacenter_id": datacenter_id,
  1071|                         "plan_id": plan_id,
  1072|                     }
  1073|                 )
  1074|             except Exception as err:  # pylint: disable=broad-except
  1075|                 log.error(
  1076|                     "Error cloning '%s' on Linode.\n\n"
  1077|                     "The following exception was thrown by Linode when trying to "
  1078|                     "clone the specified machine:\n%s",
  1079|                     clonefrom_name,
  1080|                     err,
  1081|                     exc_info_on_loglevel=logging.DEBUG,
  1082|                 )
  1083|                 return False
  1084|         else:
  1085|             kwargs["image"] = vm_["image"]
  1086|             try:
  1087|                 result = self._query(
  1088|                     "linode",
  1089|                     "create",
  1090|                     args={"PLANID": plan_id, "DATACENTERID": datacenter_id},
  1091|                 )
  1092|             except Exception as err:  # pylint: disable=broad-except
  1093|                 log.error(
  1094|                     "Error creating %s on Linode\n\n"
  1095|                     "The following exception was thrown by Linode when trying to "
  1096|                     "run the initial deployment:\n%s",
  1097|                     name,
  1098|                     err,
  1099|                     exc_info_on_loglevel=logging.DEBUG,
  1100|                 )
  1101|                 return False
  1102|         if "ERRORARRAY" in result:
  1103|             for error_data in result["ERRORARRAY"]:
  1104|                 log.error(
  1105|                     "Error creating %s on Linode\n\n"
  1106|                     "The Linode API returned the following: %s\n",
  1107|                     name,
  1108|                     error_data["ERRORMESSAGE"],
  1109|                 )
  1110|                 return False
  1111|         __utils__["cloud.fire_event"](
  1112|             "event",
  1113|             "requesting instance",
  1114|             f"salt/cloud/{name}/requesting",
  1115|             args=__utils__["cloud.filter_event"](
  1116|                 "requesting", vm_, ["name", "profile", "provider", "driver"]
  1117|             ),
  1118|             sock_dir=__opts__["sock_dir"],
  1119|             transport=__opts__["transport"],
  1120|         )
  1121|         node_id = self._clean_data(result)["LinodeID"]
  1122|         data["id"] = node_id
  1123|         if not self._wait_for_status(
  1124|             node_id, status=(self._get_status_id_by_name("brand_new"))
  1125|         ):
  1126|             log.error(
  1127|                 "Error creating %s on LINODE\n\nwhile waiting for initial ready status",
  1128|                 name,
  1129|                 exc_info_on_loglevel=logging.DEBUG,
  1130|             )
  1131|         self._update_linode(node_id, update_args={"Label": name})
  1132|         log.debug("Set name for %s - was linode%s.", name, node_id)
  1133|         private_ip_assignment = _get_private_ip(vm_)
  1134|         if private_ip_assignment:
  1135|             self._create_private_ip(node_id)
  1136|         ssh_interface = _get_ssh_interface(vm_)
  1137|         if ssh_interface == "private_ips" and private_ip_assignment is False:
  1138|             self._create_private_ip(node_id)
  1139|             private_ip_assignment = True
  1140|         if cloning:
  1141|             config_id = get_config_id(kwargs={"linode_id": node_id})["config_id"]
  1142|         else:
  1143|             log.debug("Creating disks for %s", name)
  1144|             root_disk_id = self._create_disk_from_distro(vm_, node_id)["DiskID"]
  1145|             swap_disk_id = self._create_swap_disk(vm_, node_id)["DiskID"]
  1146|             config_id = create_config(
  1147|                 kwargs={
  1148|                     "name": name,
  1149|                     "linode_id": node_id,
  1150|                     "root_disk_id": root_disk_id,
  1151|                     "swap_disk_id": swap_disk_id,
  1152|                 }
  1153|             )["ConfigID"]
  1154|         self.boot(
  1155|             kwargs={
  1156|                 "linode_id": node_id,
  1157|                 "config_id": config_id,
  1158|                 "check_running": False,
  1159|             }
  1160|         )
  1161|         node_data = get_linode(kwargs={"linode_id": node_id})
  1162|         ips = self._get_ips(node_id)
  1163|         state = int(node_data["STATUS"])
  1164|         data["image"] = kwargs["image"]
  1165|         data["name"] = name
  1166|         data["size"] = size
  1167|         data["state"] = self._get_status_descr_by_id(state)
  1168|         data["private_ips"] = ips["private_ips"]
  1169|         data["public_ips"] = ips["public_ips"]
  1170|         if ssh_interface == "private_ips":
  1171|             vm_["ssh_host"] = data["private_ips"][0]
  1172|         else:
  1173|             vm_["ssh_host"] = data["public_ips"][0]
  1174|         vm_["password"] = _get_password(vm_)
  1175|         vm_["public_ips"] = ips["public_ips"]
  1176|         vm_["private_ips"] = ips["private_ips"]
  1177|         __utils__["cloud.fire_event"](
  1178|             "event",
  1179|             "waiting for ssh",
  1180|             f"salt/cloud/{name}/waiting_for_ssh",
  1181|             sock_dir=__opts__["sock_dir"],
  1182|             args={"ip_address": vm_["ssh_host"]},
  1183|             transport=__opts__["transport"],
  1184|         )
  1185|         ret = __utils__["cloud.bootstrap"](vm_, __opts__)
  1186|         ret.update(data)
  1187|         log.info("Created Cloud VM '%s'", name)
  1188|         log.debug("'%s' VM creation details:\n%s", name, pprint.pformat(data))
  1189|         __utils__["cloud.fire_event"](
  1190|             "event",
  1191|             "created instance",
  1192|             f"salt/cloud/{name}/created",
  1193|             args=__utils__["cloud.filter_event"](
  1194|                 "created", vm_, ["name", "profile", "provider", "driver"]
  1195|             ),
  1196|             sock_dir=__opts__["sock_dir"],
  1197|             transport=__opts__["transport"],
  1198|         )
  1199|         return ret
  1200|     def create_config(self, kwargs=None):
  1201|         name = kwargs.get("name", None)
  1202|         linode_id = kwargs.get("linode_id", None)
  1203|         root_disk_id = kwargs.get("root_disk_id", None)
  1204|         swap_disk_id = kwargs.get("swap_disk_id", None)
  1205|         data_disk_id = kwargs.get("data_disk_id", None)
  1206|         kernel_id = kwargs.get("kernel_id", None)
  1207|         if kernel_id is None:
  1208|             kernel_id = 138
  1209|         required_params = [name, linode_id, root_disk_id, swap_disk_id]
  1210|         for item in required_params:
  1211|             if item is None:
  1212|                 raise SaltCloudSystemExit(
  1213|                     "The create_config functions requires a 'name', 'linode_id', "
  1214|                     "'root_disk_id', and 'swap_disk_id'."
  1215|                 )
  1216|         if kernel_id is None:
  1217|             kernel_id = 138
  1218|         if not linode_id:
  1219|             instance = self._get_linode_by_name(name)
  1220|             linode_id = instance.get("id", None)
  1221|         disklist = f"{root_disk_id},{swap_disk_id}"
  1222|         if data_disk_id is not None:
  1223|             disklist = f"{root_disk_id},{swap_disk_id},{data_disk_id}"
  1224|         config_args = {
  1225|             "LinodeID": int(linode_id),
  1226|             "KernelID": int(kernel_id),
  1227|             "Label": name,
  1228|             "DiskList": disklist,
  1229|         }
  1230|         result = self._query("linode", "config.create", args=config_args)
  1231|         return result.get("DATA", None)
  1232|     def _create_disk_from_distro(self, vm_, linode_id):
  1233|         kwargs = {}
  1234|         swap_size = _get_swap_size(vm_)
  1235|         pub_key = _get_ssh_key(vm_)
  1236|         root_password = _get_password(vm_)
  1237|         if pub_key:
  1238|             kwargs.update({"rootSSHKey": pub_key})
  1239|         if root_password:
  1240|             kwargs.update({"rootPass": root_password})
  1241|         else:
  1242|             raise SaltCloudConfigError("The Linode driver requires a password.")
  1243|         kwargs.update(
  1244|             {
  1245|                 "LinodeID": linode_id,
  1246|                 "DistributionID": self._get_distribution_id(vm_),
  1247|                 "Label": vm_["name"],
  1248|                 "Size": self._get_disk_size(vm_, swap_size, linode_id),
  1249|             }
  1250|         )
  1251|         result = self._query("linode", "disk.createfromdistribution", args=kwargs)
  1252|         return self._clean_data(result)
  1253|     def _create_swap_disk(self, vm_, linode_id, swap_size=None):
  1254|         r"""
  1255|         Creates the disk for the specified Linode.
  1256|         vm\_
  1257|             The VM profile to create the swap disk for.
  1258|         linode_id
  1259|             The ID of the Linode to create the swap disk for.
  1260|         swap_size
  1261|             The size of the disk, in MB.
  1262|         """
  1263|         kwargs = {}
  1264|         if not swap_size:
  1265|             swap_size = _get_swap_size(vm_)
  1266|         kwargs.update(
  1267|             {
  1268|                 "LinodeID": linode_id,
  1269|                 "Label": vm_["name"],
  1270|                 "Type": "swap",
  1271|                 "Size": swap_size,
  1272|             }
  1273|         )
  1274|         result = self._query("linode", "disk.create", args=kwargs)
  1275|         return self._clean_data(result)
  1276|     def _create_data_disk(self, vm_=None, linode_id=None, data_size=None):
  1277|         kwargs = {}
  1278|         kwargs.update(
  1279|             {
  1280|                 "LinodeID": linode_id,
  1281|                 "Label": vm_["name"] + "_data",
  1282|                 "Type": "ext4",
  1283|                 "Size": data_size,
  1284|             }
  1285|         )
  1286|         result = self._query("linode", "disk.create", args=kwargs)
  1287|         return self._clean_data(result)
  1288|     def _create_private_ip(self, linode_id):
  1289|         r"""
  1290|         Creates a private IP for the specified Linode.
  1291|         linode_id
  1292|             The ID of the Linode to create the IP address for.
  1293|         """
  1294|         kwargs = {"LinodeID": linode_id}
  1295|         result = self._query("linode", "ip.addprivate", args=kwargs)
  1296|         return self._clean_data(result)
  1297|     def destroy(self, name):
  1298|         __utils__["cloud.fire_event"](
  1299|             "event",
  1300|             "destroying instance",
  1301|             f"salt/cloud/{name}/destroying",
  1302|             args={"name": name},
  1303|             sock_dir=__opts__["sock_dir"],
  1304|             transport=__opts__["transport"],
  1305|         )
  1306|         linode_id = self._get_linode_id_from_name(name)
  1307|         response = self._query(
  1308|             "linode", "delete", args={"LinodeID": linode_id, "skipChecks": True}
  1309|         )
  1310|         __utils__["cloud.fire_event"](
  1311|             "event",
  1312|             "destroyed instance",
  1313|             f"salt/cloud/{name}/destroyed",
  1314|             args={"name": name},
  1315|             sock_dir=__opts__["sock_dir"],
  1316|             transport=__opts__["transport"],
  1317|         )
  1318|         if __opts__.get("update_cachedir", False) is True:
  1319|             __utils__["cloud.delete_minion_cachedir"](
  1320|                 name, _get_active_provider_name().split(":")[0], __opts__
  1321|             )
  1322|         return response
  1323|     def _decode_linode_plan_label(self, label):
  1324|         """
  1325|         Attempts to decode a user-supplied Linode plan label
  1326|         into the format in Linode API output
  1327|         label
  1328|             The label, or name, of the plan to decode.
  1329|         Example:
  1330|             `Linode 2048` will decode to `Linode 2GB`
  1331|         """
  1332|         sizes = self.avail_sizes()
  1333|         if label not in sizes:
  1334|             if "GB" in label:
  1335|                 raise SaltCloudException(
  1336|                     "Invalid Linode plan ({}) specified - call avail_sizes() for all"
  1337|                     " available options".format(label)
  1338|                 )
  1339|             else:
  1340|                 plan = label.split()
  1341|                 if len(plan) != 2:
  1342|                     raise SaltCloudException(
  1343|                         "Invalid Linode plan ({}) specified - call avail_sizes() for"
  1344|                         " all available options".format(label)
  1345|                     )
  1346|                 plan_type = plan[0]
  1347|                 try:
  1348|                     plan_size = int(plan[1])
  1349|                 except TypeError:
  1350|                     plan_size = 0
  1351|                     log.debug(
  1352|                         "Failed to decode Linode plan label in Cloud Profile: %s", label
  1353|                     )
  1354|                 if plan_type == "Linode" and plan_size == 1024:
  1355|                     plan_type = "Nanode"
  1356|                 plan_size = plan_size / 1024
  1357|                 new_label = f"{plan_type} {plan_size}GB"
  1358|                 if new_label not in sizes:
  1359|                     raise SaltCloudException(
  1360|                         "Invalid Linode plan ({}) specified - call avail_sizes() for"
  1361|                         " all available options".format(new_label)
  1362|                     )
  1363|                 log.warning(
  1364|                     "An outdated Linode plan label was detected in your Cloud "
  1365|                     "Profile (%s). Please update the profile to use the new "
  1366|                     "label format (%s) for the requested Linode plan size.",
  1367|                     label,
  1368|                     new_label,
  1369|                 )
  1370|                 label = new_label
  1371|         return sizes[label]["PLANID"]
  1372|     def get_config_id(self, kwargs=None):
  1373|         name = kwargs.get("name", None)
  1374|         linode_id = kwargs.get("linode_id", None)
  1375|         if name is None and linode_id is None:
  1376|             raise SaltCloudSystemExit(
  1377|                 "The get_config_id function requires either a 'name' or a 'linode_id' "
  1378|                 "to be provided."
  1379|             )
  1380|         if linode_id is None:
  1381|             linode_id = self._get_linode_id_from_name(name)
  1382|         response = self._query("linode", "config.list", args={"LinodeID": linode_id})[
  1383|             "DATA"
  1384|         ]
  1385|         config_id = {"config_id": response[0]["ConfigID"]}
  1386|         return config_id
  1387|     def _get_datacenter_id(self, location):
  1388|         """
  1389|         Returns the Linode Datacenter ID.
  1390|         location
  1391|             The location, or name, of the datacenter to get the ID from.
  1392|         """
  1393|         return avail_locations()[location]["DATACENTERID"]
  1394|     def _get_disk_size(self, vm_, swap, linode_id):
  1395|         r"""
  1396|         Returns the size of of the root disk in MB.
  1397|         vm\_
  1398|             The VM to get the disk size for.
  1399|         """
  1400|         disk_size = get_linode(kwargs={"linode_id": linode_id})["TOTALHD"]
  1401|         return config.get_cloud_config_value(
  1402|             "disk_size", vm_, __opts__, default=disk_size - swap
  1403|         )
  1404|     def _get_distribution_id(self, vm_):
  1405|         r"""
  1406|         Returns the distribution ID for a VM
  1407|         vm\_
  1408|             The VM to get the distribution ID for
  1409|         """
  1410|         distributions = self._query("avail", "distributions")["DATA"]
  1411|         vm_image_name = config.get_cloud_config_value("image", vm_, __opts__)
  1412|         distro_id = ""
  1413|         for distro in distributions:
  1414|             if vm_image_name == distro["LABEL"]:
  1415|                 distro_id = distro["DISTRIBUTIONID"]
  1416|                 return distro_id
  1417|         if not distro_id:
  1418|             raise SaltCloudNotFound(
  1419|                 "The DistributionID for the '{}' profile could not be found.\nThe '{}'"
  1420|                 " instance could not be provisioned. The following distributions are"
  1421|                 " available:\n{}".format(
  1422|                     vm_image_name,
  1423|                     vm_["name"],
  1424|                     pprint.pprint(
  1425|                         sorted(
  1426|                             distro["LABEL"].encode(__salt_system_encoding__)
  1427|                             for distro in distributions
  1428|                         )
  1429|                     ),
  1430|                 )
  1431|             )
  1432|     def get_plan_id(self, kwargs=None):
  1433|         label = kwargs.get("label", None)
  1434|         if label is None:
  1435|             raise SaltCloudException("The get_plan_id function requires a 'label'.")
  1436|         return self._decode_linode_plan_label(label)
  1437|     def _get_ips(self, linode_id=None):
  1438|         """
  1439|         Returns public and private IP addresses.
  1440|         linode_id
  1441|             Limits the IP addresses returned to the specified Linode ID.
  1442|         """
  1443|         if linode_id:
  1444|             ips = self._query("linode", "ip.list", args={"LinodeID": linode_id})
  1445|         else:
  1446|             ips = self._query("linode", "ip.list")
  1447|         ips = ips["DATA"]
  1448|         ret = {}
  1449|         for item in ips:
  1450|             node_id = str(item["LINODEID"])
  1451|             if item["ISPUBLIC"] == 1:
  1452|                 key = "public_ips"
  1453|             else:
  1454|                 key = "private_ips"
  1455|             if ret.get(node_id) is None:
  1456|                 ret.update({node_id: {"public_ips": [], "private_ips": []}})
  1457|             ret[node_id][key].append(item["IPADDRESS"])
  1458|         if linode_id:
  1459|             _all_ips = {"public_ips": [], "private_ips": []}
  1460|             matching_id = ret.get(str(linode_id))
  1461|             if matching_id:
  1462|                 _all_ips["private_ips"] = matching_id["private_ips"]
  1463|                 _all_ips["public_ips"] = matching_id["public_ips"]
  1464|             ret = _all_ips
  1465|         return ret
  1466|     def _wait_for_job(self, linode_id, job_id, timeout=300, quiet=True):
  1467|         """
  1468|         Wait for a Job to return.
  1469|         linode_id
  1470|             The ID of the Linode to wait on. Required.
  1471|         job_id
  1472|             The ID of the job to wait for.
  1473|         timeout
  1474|             The amount of time to wait for a status to update.
  1475|         quiet
  1476|             Log status updates to debug logs when True. Otherwise, logs to info.
  1477|         """
  1478|         interval = 5
  1479|         iterations = int(timeout / interval)
  1480|         for i in range(0, iterations):
  1481|             jobs_result = self._query(
  1482|                 "linode", "job.list", args={"LinodeID": linode_id}
  1483|             )["DATA"]
  1484|             if (
  1485|                 jobs_result[0]["JOBID"] == job_id
  1486|                 and jobs_result[0]["HOST_SUCCESS"] == 1
  1487|             ):
  1488|                 return True
  1489|             time.sleep(interval)
  1490|             log.log(
  1491|                 logging.INFO if not quiet else logging.DEBUG,
  1492|                 "Still waiting on Job %s for Linode %s.",
  1493|                 job_id,
  1494|                 linode_id,
  1495|             )
  1496|         return False
  1497|     def _wait_for_status(self, linode_id, status=None, timeout=300, quiet=True):
  1498|         """
  1499|         Wait for a certain status from Linode.
  1500|         linode_id
  1501|             The ID of the Linode to wait on. Required.
  1502|         status
  1503|             The status to look for to update.
  1504|         timeout
  1505|             The amount of time to wait for a status to update.
  1506|         quiet
  1507|             Log status updates to debug logs when False. Otherwise, logs to info.
  1508|         """
  1509|         if status is None:
  1510|             status = self._get_status_id_by_name("brand_new")
  1511|         status_desc_waiting = self._get_status_descr_by_id(status)
  1512|         interval = 5
  1513|         iterations = int(timeout / interval)
  1514|         for i in range(0, iterations):
  1515|             result = get_linode(kwargs={"linode_id": linode_id})
  1516|             if result["STATUS"] == status:
  1517|                 return True
  1518|             status_desc_result = self._get_status_descr_by_id(result["STATUS"])
  1519|             time.sleep(interval)
  1520|             log.log(
  1521|                 logging.INFO if not quiet else logging.DEBUG,
  1522|                 "Status for Linode %s is '%s', waiting for '%s'.",
  1523|                 linode_id,
  1524|                 status_desc_result,
  1525|                 status_desc_waiting,
  1526|             )
  1527|         return False
  1528|     def _list_linodes(self, full=False):
  1529|         nodes = self._query("linode", "list")["DATA"]
  1530|         ips = self._get_ips()
  1531|         ret = {}
  1532|         for node in nodes:
  1533|             this_node = {}
  1534|             linode_id = str(node["LINODEID"])
  1535|             this_node["id"] = linode_id
  1536|             this_node["image"] = node["DISTRIBUTIONVENDOR"]
  1537|             this_node["name"] = node["LABEL"]
  1538|             this_node["size"] = node["TOTALRAM"]
  1539|             state = int(node["STATUS"])
  1540|             this_node["state"] = self._get_status_descr_by_id(state)
  1541|             for key, val in ips.items():
  1542|                 if key == linode_id:
  1543|                     this_node["private_ips"] = val[1]
  1544|                     this_node["public_ips"] = val[0]
  1545|             if full:
  1546|                 this_node["extra"] = node
  1547|             ret[node["LABEL"]] = this_node
  1548|         return ret
  1549|     def list_nodes(self):
  1550|         return self._list_linodes()
  1551|     def list_nodes_full(self):
  1552|         return self._list_linodes(full=True)
  1553|     def list_nodes_min(self):
  1554|         ret = {}
  1555|         nodes = self._query("linode", "list")["DATA"]
  1556|         for node in nodes:
  1557|             name = node["LABEL"]
  1558|             ret[name] = {
  1559|                 "id": str(node["LINODEID"]),
  1560|                 "state": self._get_status_descr_by_id(int(node["STATUS"])),
  1561|             }
  1562|         return ret
  1563|     def show_instance(self, name):
  1564|         node_id = self._get_linode_id_from_name(name)
  1565|         node_data = get_linode(kwargs={"linode_id": node_id})
  1566|         ips = self._get_ips(node_id)
  1567|         state = int(node_data["STATUS"])
  1568|         return {
  1569|             "id": node_data["LINODEID"],
  1570|             "image": node_data["DISTRIBUTIONVENDOR"],
  1571|             "name": node_data["LABEL"],
  1572|             "size": node_data["TOTALRAM"],
  1573|             "state": self._get_status_descr_by_id(state),
  1574|             "private_ips": ips["private_ips"],
  1575|             "public_ips": ips["public_ips"],
  1576|         }
  1577|     def show_pricing(self, kwargs=None):
  1578|         profile = __opts__["profiles"].get(kwargs["profile"], {})
  1579|         if not profile:
  1580|             raise SaltCloudNotFound("The requested profile was not found.")
  1581|         provider = profile.get("provider", "0:0")
  1582|         comps = provider.split(":")
  1583|         if len(comps) < 2 or comps[1] != "linode":
  1584|             raise SaltCloudException("The requested profile does not belong to Linode.")
  1585|         plan_id = self.get_plan_id(kwargs={"label": profile["size"]})
  1586|         response = self._query("avail", "linodeplans", args={"PlanID": plan_id})[
  1587|             "DATA"
  1588|         ][0]
  1589|         ret = {}
  1590|         ret["per_hour"] = response["HOURLY"]
  1591|         ret["per_day"] = ret["per_hour"] * 24
  1592|         ret["per_week"] = ret["per_day"] * 7
  1593|         ret["per_month"] = response["PRICE"]
  1594|         ret["per_year"] = ret["per_month"] * 12
  1595|         return {profile["profile"]: ret}
  1596|     def _update_linode(self, linode_id, update_args=None):
  1597|         update_args.update({"LinodeID": linode_id})
  1598|         result = self._query("linode", "update", args=update_args)
  1599|         return self._clean_data(result)
  1600|     def _get_linode_id_from_name(self, name):
  1601|         node = self._get_linode_by_name(name)
  1602|         return node.get("LINODEID", None)
  1603|     def _get_linode_by_name(self, name):
  1604|         nodes = self._query("linode", "list")["DATA"]
  1605|         for node in nodes:
  1606|             if name == node["LABEL"]:
  1607|                 return node
  1608|         raise SaltCloudNotFound(f"The specified name, {name}, could not be found.")
  1609|     def _get_linode_by_id(self, linode_id):
  1610|         result = self._query("linode", "list", args={"LinodeID": linode_id})
  1611|         return result["DATA"][0]
  1612|     def start(self, name):
  1613|         node_id = self._get_linode_id_from_name(name)
  1614|         node = get_linode(kwargs={"linode_id": node_id})
  1615|         if node["STATUS"] == 1:
  1616|             return {
  1617|                 "success": True,
  1618|                 "action": "start",
  1619|                 "state": "Running",
  1620|                 "msg": "Machine already running",
  1621|             }
  1622|         response = self._query("linode", "boot", args={"LinodeID": node_id})["DATA"]
  1623|         if self._wait_for_job(node_id, response["JobID"]):
  1624|             return {"state": "Running", "action": "start", "success": True}
  1625|         else:
  1626|             return {"action": "start", "success": False}
  1627|     def stop(self, name):
  1628|         node_id = self._get_linode_id_from_name(name)
  1629|         node = get_linode(kwargs={"linode_id": node_id})
  1630|         if node["STATUS"] == 2:
  1631|             return {
  1632|                 "success": True,
  1633|                 "state": "Stopped",
  1634|                 "msg": "Machine already stopped",
  1635|             }
  1636|         response = self._query("linode", "shutdown", args={"LinodeID": node_id})["DATA"]
  1637|         if self._wait_for_job(node_id, response["JobID"]):
  1638|             return {"state": "Stopped", "action": "stop", "success": True}
  1639|         return {"action": "stop", "success": False}
  1640|     def reboot(self, name):
  1641|         node_id = self._get_linode_id_from_name(name)
  1642|         response = self._query("linode", "reboot", args={"LinodeID": node_id})
  1643|         data = self._clean_data(response)
  1644|         reboot_jid = data["JobID"]
  1645|         if not self._wait_for_job(node_id, reboot_jid):
  1646|             log.error("Reboot failed for %s.", name)
  1647|             return False
  1648|         return data
  1649|     def _clean_data(self, api_response):
  1650|         """
  1651|         Returns the DATA response from a Linode API query as a single pre-formatted dictionary
  1652|         api_response
  1653|             The query to be cleaned.
  1654|         """
  1655|         data = {}
  1656|         data.update(api_response["DATA"])
  1657|         if not data:
  1658|             response_data = api_response["DATA"]
  1659|             data.update(response_data)
  1660|         return data
  1661|     def _get_status_descr_by_id(self, status_id):
  1662|         """
  1663|         Return linode status by ID
  1664|         status_id
  1665|             linode VM status ID
  1666|         """
  1667|         for status_name, status_data in LINODE_STATUS.items():
  1668|             if status_data["code"] == int(status_id):
  1669|                 return status_data["descr"]
  1670|         return LINODE_STATUS.get(status_id, None)
  1671|     def _get_status_id_by_name(self, status_name):
  1672|         """
  1673|         Return linode status description by internalstatus name
  1674|         status_name
  1675|             internal linode VM status name
  1676|         """
  1677|         return LINODE_STATUS.get(status_name, {}).get("code", None)
  1678| def avail_images(call=None):
  1679|     """
  1680|     Return available Linode images.
  1681|     CLI Example:
  1682|     .. code-block:: bash
  1683|         salt-cloud --list-images my-linode-config
  1684|         salt-cloud -f avail_images my-linode-config
  1685|     """
  1686|     if call == "action":
  1687|         raise SaltCloudException(
  1688|             "The avail_images function must be called with -f or --function."
  1689|         )
  1690|     return _get_cloud_interface().avail_images()
  1691| def avail_locations(call=None):
  1692|     """
  1693|     Return available Linode datacenter locations.
  1694|     CLI Example:
  1695|     .. code-block:: bash
  1696|         salt-cloud --list-locations my-linode-config
  1697|         salt-cloud -f avail_locations my-linode-config
  1698|     """
  1699|     if call == "action":
  1700|         raise SaltCloudException(
  1701|             "The avail_locations function must be called with -f or --function."
  1702|         )
  1703|     return _get_cloud_interface().avail_locations()
  1704| def avail_sizes(call=None):
  1705|     """
  1706|     Return available Linode sizes.
  1707|     CLI Example:
  1708|     .. code-block:: bash
  1709|         salt-cloud --list-sizes my-linode-config
  1710|         salt-cloud -f avail_sizes my-linode-config
  1711|     """
  1712|     if call == "action":
  1713|         raise SaltCloudException(
  1714|             "The avail_locations function must be called with -f or --function."
  1715|         )
  1716|     return _get_cloud_interface().avail_sizes()
  1717| def boot(name=None, kwargs=None, call=None):
  1718|     """
  1719|     Boot a Linode.
  1720|     name
  1721|         The name of the Linode to boot. Can be used instead of ``linode_id``.
  1722|     linode_id
  1723|         The ID of the Linode to boot. If provided, will be used as an
  1724|         alternative to ``name`` and reduces the number of API calls to
  1725|         Linode by one. Will be preferred over ``name``.
  1726|     config_id
  1727|         The ID of the Config to boot. Required.
  1728|     check_running
  1729|         Defaults to True. If set to False, overrides the call to check if
  1730|         the VM is running before calling the linode.boot API call. Change
  1731|         ``check_running`` to True is useful during the boot call in the
  1732|         create function, since the new VM will not be running yet.
  1733|     Can be called as an action (which requires a name):
  1734|     .. code-block:: bash
  1735|         salt-cloud -a boot my-instance config_id=10
  1736|     ...or as a function (which requires either a name or linode_id):
  1737|     .. code-block:: bash
  1738|         salt-cloud -f boot my-linode-config name=my-instance config_id=10
  1739|         salt-cloud -f boot my-linode-config linode_id=1225876 config_id=10
  1740|     """
  1741|     if name is None and call == "action":
  1742|         raise SaltCloudSystemExit("The boot action requires a 'name'.")
  1743|     linode_id = kwargs.get("linode_id", None)
  1744|     config_id = kwargs.get("config_id", None)
  1745|     if call == "function":
  1746|         name = kwargs.get("name", None)
  1747|     if name is None and linode_id is None:
  1748|         raise SaltCloudSystemExit(
  1749|             "The boot function requires either a 'name' or a 'linode_id'."
  1750|         )
  1751|     return _get_cloud_interface().boot(name=name, kwargs=kwargs)
  1752| def clone(kwargs=None, call=None):
  1753|     """
  1754|     Clone a Linode.
  1755|     linode_id
  1756|         The ID of the Linode to clone. Required.
  1757|     location
  1758|         The location of the new Linode. Required.
  1759|     size
  1760|         The size of the new Linode (must be greater than or equal to the clone source). Required.
  1761|     datacenter_id
  1762|         The ID of the Datacenter where the Linode will be placed. Required for APIv3 usage.
  1763|         Deprecated. Use ``location`` instead.
  1764|     plan_id
  1765|         The ID of the plan (size) of the Linode. Required. Required for APIv3 usage.
  1766|         Deprecated. Use ``size`` instead.
  1767|     CLI Example:
  1768|     .. code-block:: bash
  1769|         salt-cloud -f clone my-linode-config linode_id=1234567 datacenter_id=2 plan_id=5
  1770|     """
  1771|     if call == "action":
  1772|         raise SaltCloudSystemExit(
  1773|             "The clone function must be called with -f or --function."
  1774|         )
  1775|     return _get_cloud_interface().clone(kwargs=kwargs)
  1776| def create(vm_):
  1777|     """
  1778|     Create a single Linode VM.
  1779|     """
  1780|     try:
  1781|         if (
  1782|             vm_["profile"]
  1783|             and config.is_profile_configured(
  1784|                 __opts__,
  1785|                 _get_active_provider_name() or "linode",
  1786|                 vm_["profile"],
  1787|                 vm_=vm_,
  1788|             )
  1789|             is False
  1790|         ):
  1791|             return False
  1792|     except AttributeError:
  1793|         pass
  1794|     return _get_cloud_interface().create(vm_)
  1795| def create_config(kwargs=None, call=None):
  1796|     """
  1797|     Creates a Linode Configuration Profile.
  1798|     name
  1799|         The name of the VM to create the config for.
  1800|     linode_id
  1801|         The ID of the Linode to create the configuration for.
  1802|     root_disk_id
  1803|         The Root Disk ID to be used for this config.
  1804|     swap_disk_id
  1805|         The Swap Disk ID to be used for this config.
  1806|     data_disk_id
  1807|         The Data Disk ID to be used for this config.
  1808|     .. versionadded:: 2016.3.0
  1809|     kernel_id
  1810|         The ID of the kernel to use for this configuration profile.
  1811|     """
  1812|     if call == "action":
  1813|         raise SaltCloudSystemExit(
  1814|             "The create_config function must be called with -f or --function."
  1815|         )
  1816|     return _get_cloud_interface().create_config(kwargs=kwargs)
  1817| def destroy(name, call=None):
  1818|     """
  1819|     Destroys a Linode by name.
  1820|     name
  1821|         The name of VM to be be destroyed.
  1822|     CLI Example:
  1823|     .. code-block:: bash
  1824|         salt-cloud -d vm_name
  1825|     """
  1826|     if call == "function":
  1827|         raise SaltCloudException(
  1828|             "The destroy action must be called with -d, --destroy, -a or --action."
  1829|         )
  1830|     return _get_cloud_interface().destroy(name)
  1831| def get_config_id(kwargs=None, call=None):
  1832|     """
  1833|     Returns a config_id for a given linode.
  1834|     .. versionadded:: 2015.8.0
  1835|     name
  1836|         The name of the Linode for which to get the config_id. Can be used instead
  1837|         of ``linode_id``.
  1838|     linode_id
  1839|         The ID of the Linode for which to get the config_id. Can be used instead
  1840|         of ``name``.
  1841|     CLI Example:
  1842|     .. code-block:: bash
  1843|         salt-cloud -f get_config_id my-linode-config name=my-linode
  1844|         salt-cloud -f get_config_id my-linode-config linode_id=1234567
  1845|     """
  1846|     if call == "action":
  1847|         raise SaltCloudException(
  1848|             "The get_config_id function must be called with -f or --function."
  1849|         )
  1850|     return _get_cloud_interface().get_config_id(kwargs=kwargs)
  1851| def get_linode(kwargs=None, call=None):
  1852|     """
  1853|     Returns data for a single named Linode.
  1854|     name
  1855|         The name of the Linode for which to get data. Can be used instead
  1856|         ``linode_id``. Note this will induce an additional API call
  1857|         compared to using ``linode_id``.
  1858|     linode_id
  1859|         The ID of the Linode for which to get data. Can be used instead of
  1860|         ``name``.
  1861|     CLI Example:
  1862|     .. code-block:: bash
  1863|         salt-cloud -f get_linode my-linode-config name=my-instance
  1864|         salt-cloud -f get_linode my-linode-config linode_id=1234567
  1865|     """
  1866|     if call == "action":
  1867|         raise SaltCloudSystemExit(
  1868|             "The get_linode function must be called with -f or --function."
  1869|         )
  1870|     return _get_cloud_interface().get_linode(kwargs=kwargs)
  1871| def get_plan_id(kwargs=None, call=None):
  1872|     """
  1873|     Returns the Linode Plan ID.
  1874|     label
  1875|         The label, or name, of the plan to get the ID from.
  1876|     CLI Example:
  1877|     .. code-block:: bash
  1878|         salt-cloud -f get_plan_id linode label="Nanode 1GB"
  1879|         salt-cloud -f get_plan_id linode label="Linode 2GB"
  1880|     """
  1881|     if call == "action":
  1882|         raise SaltCloudException(
  1883|             "The show_instance action must be called with -f or --function."
  1884|         )
  1885|     return _get_cloud_interface().get_plan_id(kwargs=kwargs)
  1886| def list_nodes(call=None):
  1887|     """
  1888|     Returns a list of linodes, keeping only a brief listing.
  1889|     CLI Example:
  1890|     .. code-block:: bash
  1891|         salt-cloud -Q
  1892|         salt-cloud --query
  1893|         salt-cloud -f list_nodes my-linode-config
  1894|     .. note::
  1895|         The ``image`` label only displays information about the VM's distribution vendor,
  1896|         such as "Debian" or "RHEL" and does not display the actual image name. This is
  1897|         due to a limitation of the Linode API.
  1898|     """
  1899|     if call == "action":
  1900|         raise SaltCloudException(
  1901|             "The list_nodes function must be called with -f or --function."
  1902|         )
  1903|     return _get_cloud_interface().list_nodes()
  1904| def list_nodes_full(call=None):
  1905|     """
  1906|     List linodes, with all available information.
  1907|     CLI Example:
  1908|     .. code-block:: bash
  1909|         salt-cloud -F
  1910|         salt-cloud --full-query
  1911|         salt-cloud -f list_nodes_full my-linode-config
  1912|     .. note::
  1913|         The ``image`` label only displays information about the VM's distribution vendor,
  1914|         such as "Debian" or "RHEL" and does not display the actual image name. This is
  1915|         due to a limitation of the Linode API.
  1916|     """
  1917|     if call == "action":
  1918|         raise SaltCloudException(
  1919|             "The list_nodes_full function must be called with -f or --function."
  1920|         )
  1921|     return _get_cloud_interface().list_nodes_full()
  1922| def list_nodes_min(call=None):
  1923|     """
  1924|     Return a list of the VMs that are on the provider. Only a list of VM names and
  1925|     their state is returned. This is the minimum amount of information needed to
  1926|     check for existing VMs.
  1927|     .. versionadded:: 2015.8.0
  1928|     CLI Example:
  1929|     .. code-block:: bash
  1930|         salt-cloud -f list_nodes_min my-linode-config
  1931|         salt-cloud --function list_nodes_min my-linode-config
  1932|     """
  1933|     if call == "action":
  1934|         raise SaltCloudSystemExit(
  1935|             "The list_nodes_min function must be called with -f or --function."
  1936|         )
  1937|     return _get_cloud_interface().list_nodes_min()
  1938| def list_nodes_select(call=None):
  1939|     """
  1940|     Return a list of the VMs that are on the provider, with select fields.
  1941|     """
  1942|     return _get_cloud_interface().list_nodes_select(call)
  1943| def reboot(name, call=None):
  1944|     """
  1945|     Reboot a linode.
  1946|     .. versionadded:: 2015.8.0
  1947|     name
  1948|         The name of the VM to reboot.
  1949|     CLI Example:
  1950|     .. code-block:: bash
  1951|         salt-cloud -a reboot vm_name
  1952|     """
  1953|     if call != "action":
  1954|         raise SaltCloudException(
  1955|             "The show_instance action must be called with -a or --action."
  1956|         )
  1957|     return _get_cloud_interface().reboot(name)
  1958| def show_instance(name, call=None):
  1959|     """
  1960|     Displays details about a particular Linode VM. Either a name or a linode_id must
  1961|     be provided.
  1962|     .. versionadded:: 2015.8.0
  1963|     name
  1964|         The name of the VM for which to display details.
  1965|     CLI Example:
  1966|     .. code-block:: bash
  1967|         salt-cloud -a show_instance vm_name
  1968|     .. note::
  1969|         The ``image`` label only displays information about the VM's distribution vendor,
  1970|         such as "Debian" or "RHEL" and does not display the actual image name. This is
  1971|         due to a limitation of the Linode API.
  1972|     """
  1973|     if call != "action":
  1974|         raise SaltCloudException(
  1975|             "The show_instance action must be called with -a or --action."
  1976|         )
  1977|     return _get_cloud_interface().show_instance(name)
  1978| def show_pricing(kwargs=None, call=None):
  1979|     """
  1980|     Show pricing for a particular profile. This is only an estimate, based on
  1981|     unofficial pricing sources.
  1982|     .. versionadded:: 2015.8.0
  1983|     CLI Example:
  1984|     .. code-block:: bash
  1985|         salt-cloud -f show_pricing my-linode-config profile=my-linode-profile
  1986|     """
  1987|     if call != "function":
  1988|         raise SaltCloudException(
  1989|             "The show_instance action must be called with -f or --function."
  1990|         )
  1991|     return _get_cloud_interface().show_pricing(kwargs=kwargs)
  1992| def start(name, call=None):
  1993|     """
  1994|     Start a VM in Linode.
  1995|     name
  1996|         The name of the VM to start.
  1997|     CLI Example:
  1998|     .. code-block:: bash
  1999|         salt-cloud -a stop vm_name
  2000|     """
  2001|     if call != "action":
  2002|         raise SaltCloudException("The start action must be called with -a or --action.")
  2003|     return _get_cloud_interface().start(name)
  2004| def stop(name, call=None):
  2005|     """
  2006|     Stop a VM in Linode.
  2007|     name
  2008|         The name of the VM to stop.
  2009|     CLI Example:
  2010|     .. code-block:: bash
  2011|         salt-cloud -a stop vm_name
  2012|     """
  2013|     if call != "action":
  2014|         raise SaltCloudException("The stop action must be called with -a or --action.")
  2015|     return _get_cloud_interface().stop(name)


# ====================================================================
# FILE: salt/cloud/clouds/msazure.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-2826 ---
     1| """
     2| Azure Cloud Module
     3| ==================
     4| The Azure cloud module is used to control access to Microsoft Azure
     5| .. warning::
     6|     This cloud provider will be removed from Salt in version 3007 due to
     7|     the deprecation of the "Classic" API for Azure. Please migrate to
     8|     `Azure Resource Manager by March 1, 2023
     9|     <https://docs.microsoft.com/en-us/azure/virtual-machines/classic-vm-deprecation>`_
    10| :depends:
    11|     * `Microsoft Azure SDK for Python <https://pypi.python.org/pypi/azure/1.0.2>`_ >= 1.0.2
    12|     * python-requests, for Python < 2.7.9
    13| :configuration:
    14|     Required provider parameters:
    15|     * ``apikey``
    16|     * ``certificate_path``
    17|     * ``subscription_id``
    18|     * ``backend``
    19|     A Management Certificate (.pem and .crt files) must be created and the .pem
    20|     file placed on the same machine that salt-cloud is run from. Information on
    21|     creating the pem file to use, and uploading the associated cer file can be
    22|     found at:
    23|     http://www.windowsazure.com/en-us/develop/python/how-to-guides/service-management/
    24|     For users with Python < 2.7.9, ``backend`` must currently be set to ``requests``.
    25| Example ``/etc/salt/cloud.providers`` or
    26| ``/etc/salt/cloud.providers.d/azure.conf`` configuration:
    27| .. code-block:: yaml
    28|     my-azure-config:
    29|       driver: azure
    30|       subscription_id: 3287abc8-f98a-c678-3bde-326766fd3617
    31|       certificate_path: /etc/salt/azure.pem
    32|       management_host: management.core.windows.net
    33| """
    34| import copy
    35| import logging
    36| import pprint
    37| import time
    38| from functools import wraps
    39| import salt.config as config
    40| import salt.utils.args
    41| import salt.utils.cloud
    42| import salt.utils.stringutils
    43| import salt.utils.versions
    44| import salt.utils.yaml
    45| from salt.exceptions import SaltCloudSystemExit
    46| HAS_LIBS = False
    47| try:
    48|     import azure
    49|     import azure.servicemanagement
    50|     import azure.storage
    51|     from azure.common import (
    52|         AzureConflictHttpError,
    53|         AzureException,
    54|         AzureMissingResourceHttpError,
    55|     )
    56|     import salt.utils.msazure
    57|     from salt.utils.msazure import object_to_dict
    58|     HAS_LIBS = True
    59| except ImportError:
    60|     pass
    61| __virtualname__ = "azure"
    62| log = logging.getLogger(__name__)
    63| def __virtual__():
    64|     """
    65|     Check for Azure configurations.
    66|     """
    67|     if get_configured_provider() is False:
    68|         return False
    69|     if get_dependencies() is False:
    70|         return False
    71|     return __virtualname__
    72| def _get_active_provider_name():
    73|     try:
    74|         return __active_provider_name__.value()
    75|     except AttributeError:
    76|         return __active_provider_name__
    77| def _deprecation_message(function):
    78|     """
    79|     Decorator wrapper to warn about msazure deprecation
    80|     """
    81|     @wraps(function)
    82|     def wrapped(*args, **kwargs):
    83|         salt.utils.versions.warn_until(
    84|             "Chlorine",
    85|             "This cloud provider will be removed from Salt in version 3007 due to "
    86|             "the deprecation of the 'Classic' API for Azure. Please migrate to "
    87|             "Azure Resource Manager by March 1, 2023 "
    88|             "(https://docs.microsoft.com/en-us/azure/virtual-machines/classic-vm-deprecation)",
    89|             category=FutureWarning,
    90|         )
    91|         ret = function(*args, **salt.utils.args.clean_kwargs(**kwargs))
    92|         return ret
    93|     return wrapped
    94| def get_configured_provider():
    95|     """
    96|     Return the first configured instance.
    97|     """
    98|     return config.is_provider_configured(
    99|         __opts__,
   100|         _get_active_provider_name() or __virtualname__,
   101|         ("subscription_id", "certificate_path"),
   102|     )
   103| @_deprecation_message
   104| def get_dependencies():
   105|     """
   106|     Warn if dependencies aren't met.
   107|     """
   108|     return config.check_driver_dependencies(__virtualname__, {"azure": HAS_LIBS})
   109| @_deprecation_message
   110| def get_conn():
   111|     """
   112|     Return a conn object for the passed VM data
   113|     """
   114|     certificate_path = config.get_cloud_config_value(
   115|         "certificate_path", get_configured_provider(), __opts__, search_global=False
   116|     )
   117|     subscription_id = salt.utils.stringutils.to_str(
   118|         config.get_cloud_config_value(
   119|             "subscription_id", get_configured_provider(), __opts__, search_global=False
   120|         )
   121|     )
   122|     management_host = config.get_cloud_config_value(
   123|         "management_host",
   124|         get_configured_provider(),
   125|         __opts__,
   126|         search_global=False,
   127|         default="management.core.windows.net",
   128|     )
   129|     return azure.servicemanagement.ServiceManagementService(
   130|         subscription_id, certificate_path, management_host
   131|     )
   132| @_deprecation_message
   133| def script(vm_):
   134|     """
   135|     Return the script deployment object
   136|     """
   137|     return salt.utils.cloud.os_script(
   138|         config.get_cloud_config_value("script", vm_, __opts__),
   139|         vm_,
   140|         __opts__,
   141|         salt.utils.cloud.salt_config_to_yaml(
   142|             salt.utils.cloud.minion_config(__opts__, vm_)
   143|         ),
   144|     )
   145| @_deprecation_message
   146| def avail_locations(conn=None, call=None):
   147|     """
   148|     List available locations for Azure
   149|     """
   150|     if call == "action":
   151|         raise SaltCloudSystemExit(
   152|             "The avail_locations function must be called with "
   153|             "-f or --function, or with the --list-locations option"
   154|         )
   155|     if not conn:
   156|         conn = get_conn()
   157|     ret = {}
   158|     locations = conn.list_locations()
   159|     for location in locations:
   160|         ret[location.name] = {
   161|             "name": location.name,
   162|             "display_name": location.display_name,
   163|             "available_services": location.available_services,
   164|         }
   165|     return ret
   166| @_deprecation_message
   167| def avail_images(conn=None, call=None):
   168|     """
   169|     List available images for Azure
   170|     """
   171|     if call == "action":
   172|         raise SaltCloudSystemExit(
   173|             "The avail_images function must be called with "
   174|             "-f or --function, or with the --list-images option"
   175|         )
   176|     if not conn:
   177|         conn = get_conn()
   178|     ret = {}
   179|     for item in conn.list_os_images():
   180|         ret[item.name] = object_to_dict(item)
   181|     for item in conn.list_vm_images():
   182|         ret[item.name] = object_to_dict(item)
   183|     return ret
   184| @_deprecation_message
   185| def avail_sizes(call=None):
   186|     """
   187|     Return a list of sizes from Azure
   188|     """
   189|     if call == "action":
   190|         raise SaltCloudSystemExit(
   191|             "The avail_sizes function must be called with "
   192|             "-f or --function, or with the --list-sizes option"
   193|         )
   194|     conn = get_conn()
   195|     data = conn.list_role_sizes()
   196|     ret = {}
   197|     for item in data.role_sizes:
   198|         ret[item.name] = object_to_dict(item)
   199|     return ret
   200| @_deprecation_message
   201| def list_nodes(conn=None, call=None):
   202|     """
   203|     List VMs on this Azure account
   204|     """
   205|     if call == "action":
   206|         raise SaltCloudSystemExit(
   207|             "The list_nodes function must be called with -f or --function."
   208|         )
   209|     ret = {}
   210|     nodes = list_nodes_full(conn, call)
   211|     for node in nodes:
   212|         ret[node] = {"name": node}
   213|         for prop in ("id", "image", "size", "state", "private_ips", "public_ips"):
   214|             ret[node][prop] = nodes[node].get(prop)
   215|     return ret
   216| @_deprecation_message
   217| def list_nodes_full(conn=None, call=None):
   218|     """
   219|     List VMs on this Azure account, with full information
   220|     """
   221|     if call == "action":
   222|         raise SaltCloudSystemExit(
   223|             "The list_nodes_full function must be called with -f or --function."
   224|         )
   225|     if not conn:
   226|         conn = get_conn()
   227|     ret = {}
   228|     services = list_hosted_services(conn=conn, call=call)
   229|     for service in services:
   230|         for deployment in services[service]["deployments"]:
   231|             deploy_dict = services[service]["deployments"][deployment]
   232|             deploy_dict_no_role_info = copy.deepcopy(deploy_dict)
   233|             del deploy_dict_no_role_info["role_list"]
   234|             del deploy_dict_no_role_info["role_instance_list"]
   235|             roles = deploy_dict["role_list"]
   236|             for role in roles:
   237|                 role_instances = deploy_dict["role_instance_list"]
   238|                 ret[role] = roles[role]
   239|                 ret[role].update(role_instances[role])
   240|                 ret[role]["id"] = role
   241|                 ret[role]["hosted_service"] = service
   242|                 if role_instances[role]["power_state"] == "Started":
   243|                     ret[role]["state"] = "running"
   244|                 elif role_instances[role]["power_state"] == "Stopped":
   245|                     ret[role]["state"] = "stopped"
   246|                 else:
   247|                     ret[role]["state"] = "pending"
   248|                 ret[role]["private_ips"] = []
   249|                 ret[role]["public_ips"] = []
   250|                 ret[role]["deployment"] = deploy_dict_no_role_info
   251|                 ret[role]["url"] = deploy_dict["url"]
   252|                 ip_address = role_instances[role]["ip_address"]
   253|                 if ip_address:
   254|                     if salt.utils.cloud.is_public_ip(ip_address):
   255|                         ret[role]["public_ips"].append(ip_address)
   256|                     else:
   257|                         ret[role]["private_ips"].append(ip_address)
   258|                 ret[role]["size"] = role_instances[role]["instance_size"]
   259|                 ret[role]["image"] = roles[role]["role_info"]["os_virtual_hard_disk"][
   260|                     "source_image_name"
   261|                 ]
   262|     return ret
   263| @_deprecation_message
   264| def list_hosted_services(conn=None, call=None):
   265|     """
   266|     List VMs on this Azure account, with full information
   267|     """
   268|     if call == "action":
   269|         raise SaltCloudSystemExit(
   270|             "The list_hosted_services function must be called with -f or --function"
   271|         )
   272|     if not conn:
   273|         conn = get_conn()
   274|     ret = {}
   275|     services = conn.list_hosted_services()
   276|     for service in services:
   277|         props = service.hosted_service_properties
   278|         ret[service.service_name] = {
   279|             "name": service.service_name,
   280|             "url": service.url,
   281|             "affinity_group": props.affinity_group,
   282|             "date_created": props.date_created,
   283|             "date_last_modified": props.date_last_modified,
   284|             "description": props.description,
   285|             "extended_properties": props.extended_properties,
   286|             "label": props.label,
   287|             "location": props.location,
   288|             "status": props.status,
   289|             "deployments": {},
   290|         }
   291|         deployments = conn.get_hosted_service_properties(
   292|             service_name=service.service_name, embed_detail=True
   293|         )
   294|         for deployment in deployments.deployments:
   295|             ret[service.service_name]["deployments"][deployment.name] = {
   296|                 "configuration": deployment.configuration,
   297|                 "created_time": deployment.created_time,
   298|                 "deployment_slot": deployment.deployment_slot,
   299|                 "extended_properties": deployment.extended_properties,
   300|                 "input_endpoint_list": deployment.input_endpoint_list,
   301|                 "label": deployment.label,
   302|                 "last_modified_time": deployment.last_modified_time,
   303|                 "locked": deployment.locked,
   304|                 "name": deployment.name,
   305|                 "persistent_vm_downtime_info": deployment.persistent_vm_downtime_info,
   306|                 "private_id": deployment.private_id,
   307|                 "role_instance_list": {},
   308|                 "role_list": {},
   309|                 "rollback_allowed": deployment.rollback_allowed,
   310|                 "sdk_version": deployment.sdk_version,
   311|                 "status": deployment.status,
   312|                 "upgrade_domain_count": deployment.upgrade_domain_count,
   313|                 "upgrade_status": deployment.upgrade_status,
   314|                 "url": deployment.url,
   315|             }
   316|             for role_instance in deployment.role_instance_list:
   317|                 ret[service.service_name]["deployments"][deployment.name][
   318|                     "role_instance_list"
   319|                 ][role_instance.role_name] = {
   320|                     "fqdn": role_instance.fqdn,
   321|                     "instance_error_code": role_instance.instance_error_code,
   322|                     "instance_fault_domain": role_instance.instance_fault_domain,
   323|                     "instance_name": role_instance.instance_name,
   324|                     "instance_size": role_instance.instance_size,
   325|                     "instance_state_details": role_instance.instance_state_details,
   326|                     "instance_status": role_instance.instance_status,
   327|                     "instance_upgrade_domain": role_instance.instance_upgrade_domain,
   328|                     "ip_address": role_instance.ip_address,
   329|                     "power_state": role_instance.power_state,
   330|                     "role_name": role_instance.role_name,
   331|                 }
   332|             for role in deployment.role_list:
   333|                 ret[service.service_name]["deployments"][deployment.name]["role_list"][
   334|                     role.role_name
   335|                 ] = {
   336|                     "role_name": role.role_name,
   337|                     "os_version": role.os_version,
   338|                 }
   339|                 role_info = conn.get_role(
   340|                     service_name=service.service_name,
   341|                     deployment_name=deployment.name,
   342|                     role_name=role.role_name,
   343|                 )
   344|                 ret[service.service_name]["deployments"][deployment.name]["role_list"][
   345|                     role.role_name
   346|                 ]["role_info"] = {
   347|                     "availability_set_name": role_info.availability_set_name,
   348|                     "configuration_sets": role_info.configuration_sets,
   349|                     "data_virtual_hard_disks": role_info.data_virtual_hard_disks,
   350|                     "os_version": role_info.os_version,
   351|                     "role_name": role_info.role_name,
   352|                     "role_size": role_info.role_size,
   353|                     "role_type": role_info.role_type,
   354|                 }
   355|                 ret[service.service_name]["deployments"][deployment.name]["role_list"][
   356|                     role.role_name
   357|                 ]["role_info"]["os_virtual_hard_disk"] = {
   358|                     "disk_label": role_info.os_virtual_hard_disk.disk_label,
   359|                     "disk_name": role_info.os_virtual_hard_disk.disk_name,
   360|                     "host_caching": role_info.os_virtual_hard_disk.host_caching,
   361|                     "media_link": role_info.os_virtual_hard_disk.media_link,
   362|                     "os": role_info.os_virtual_hard_disk.os,
   363|                     "source_image_name": role_info.os_virtual_hard_disk.source_image_name,
   364|                 }
   365|     return ret
   366| @_deprecation_message
   367| def list_nodes_select(conn=None, call=None):
   368|     """
   369|     Return a list of the VMs that are on the provider, with select fields
   370|     """
   371|     if not conn:
   372|         conn = get_conn()
   373|     return salt.utils.cloud.list_nodes_select(
   374|         list_nodes_full(conn, "function"),
   375|         __opts__["query.selection"],
   376|         call,
   377|     )
   378| @_deprecation_message
   379| def show_instance(name, call=None):
   380|     """
   381|     Show the details from the provider concerning an instance
   382|     """
   383|     if call != "action":
   384|         raise SaltCloudSystemExit(
   385|             "The show_instance action must be called with -a or --action."
   386|         )
   387|     nodes = list_nodes_full()
   388|     if name not in nodes:
   389|         return {}
   390|     if "name" not in nodes[name]:
   391|         nodes[name]["name"] = nodes[name]["id"]
   392|     try:
   393|         __utils__["cloud.cache_node"](
   394|             nodes[name], _get_active_provider_name(), __opts__
   395|         )
   396|     except TypeError:
   397|         log.warning(
   398|             "Unable to show cache node data; this may be because the node has been"
   399|             " deleted"
   400|         )
   401|     return nodes[name]
   402| @_deprecation_message
   403| def create(vm_):
   404|     """
   405|     Create a single VM from a data dict
   406|     """
   407|     try:
   408|         if (
   409|             vm_["profile"]
   410|             and config.is_profile_configured(
   411|                 __opts__,
   412|                 _get_active_provider_name() or "azure",
   413|                 vm_["profile"],
   414|                 vm_=vm_,
   415|             )
   416|             is False
   417|         ):
   418|             return False
   419|     except AttributeError:
   420|         pass
   421|     __utils__["cloud.fire_event"](
   422|         "event",
   423|         "starting create",
   424|         "salt/cloud/{}/creating".format(vm_["name"]),
   425|         args=__utils__["cloud.filter_event"](
   426|             "creating", vm_, ["name", "profile", "provider", "driver"]
   427|         ),
   428|         sock_dir=__opts__["sock_dir"],
   429|         transport=__opts__["transport"],
   430|     )
   431|     log.info("Creating Cloud VM %s", vm_["name"])
   432|     conn = get_conn()
   433|     label = vm_.get("label", vm_["name"])
   434|     service_name = vm_.get("service_name", vm_["name"])
   435|     service_kwargs = {
   436|         "service_name": service_name,
   437|         "label": label,
   438|         "description": vm_.get("desc", vm_["name"]),
   439|     }
   440|     loc_error = False
   441|     if "location" in vm_:
   442|         if "affinity_group" in vm_:
   443|             loc_error = True
   444|         else:
   445|             service_kwargs["location"] = vm_["location"]
   446|     elif "affinity_group" in vm_:
   447|         service_kwargs["affinity_group"] = vm_["affinity_group"]
   448|     else:
   449|         loc_error = True
   450|     if loc_error:
   451|         raise SaltCloudSystemExit(
   452|             "Either a location or affinity group must be specified, but not both"
   453|         )
   454|     ssh_port = config.get_cloud_config_value(
   455|         "port", vm_, __opts__, default=22, search_global=True
   456|     )
   457|     ssh_endpoint = azure.servicemanagement.ConfigurationSetInputEndpoint(
   458|         name="SSH",
   459|         protocol="TCP",
   460|         port=ssh_port,
   461|         local_port=22,
   462|     )
   463|     network_config = azure.servicemanagement.ConfigurationSet()
   464|     network_config.input_endpoints.input_endpoints.append(ssh_endpoint)
   465|     network_config.configuration_set_type = "NetworkConfiguration"
   466|     if "win_username" in vm_:
   467|         system_config = azure.servicemanagement.WindowsConfigurationSet(
   468|             computer_name=vm_["name"],
   469|             admin_username=vm_["win_username"],
   470|             admin_password=vm_["win_password"],
   471|         )
   472|         smb_port = "445"
   473|         if "smb_port" in vm_:
   474|             smb_port = vm_["smb_port"]
   475|         smb_endpoint = azure.servicemanagement.ConfigurationSetInputEndpoint(
   476|             name="SMB",
   477|             protocol="TCP",
   478|             port=smb_port,
   479|             local_port=smb_port,
   480|         )
   481|         network_config.input_endpoints.input_endpoints.append(smb_endpoint)
   482|         system_config.domain_join = None
   483|         system_config.win_rm = None
   484|     else:
   485|         system_config = azure.servicemanagement.LinuxConfigurationSet(
   486|             host_name=vm_["name"],
   487|             user_name=vm_["ssh_username"],
   488|             user_password=vm_["ssh_password"],
   489|             disable_ssh_password_authentication=False,
   490|         )
   491|     media_link = vm_["media_link"]
   492|     media_link += "/{}.vhd".format(vm_["name"])
   493|     os_hd = azure.servicemanagement.OSVirtualHardDisk(vm_["image"], media_link)
   494|     vm_kwargs = {
   495|         "service_name": service_name,
   496|         "deployment_name": service_name,
   497|         "deployment_slot": vm_["slot"],
   498|         "label": label,
   499|         "role_name": vm_["name"],
   500|         "system_config": system_config,
   501|         "os_virtual_hard_disk": os_hd,
   502|         "role_size": vm_["size"],
   503|         "network_config": network_config,
   504|     }
   505|     if "virtual_network_name" in vm_:
   506|         vm_kwargs["virtual_network_name"] = vm_["virtual_network_name"]
   507|         if "subnet_name" in vm_:
   508|             network_config.subnet_names.append(vm_["subnet_name"])
   509|     log.debug("vm_kwargs: %s", vm_kwargs)
   510|     event_kwargs = {
   511|         "service_kwargs": service_kwargs.copy(),
   512|         "vm_kwargs": vm_kwargs.copy(),
   513|     }
   514|     del event_kwargs["vm_kwargs"]["system_config"]
   515|     del event_kwargs["vm_kwargs"]["os_virtual_hard_disk"]
   516|     del event_kwargs["vm_kwargs"]["network_config"]
   517|     __utils__["cloud.fire_event"](
   518|         "event",
   519|         "requesting instance",
   520|         "salt/cloud/{}/requesting".format(vm_["name"]),
   521|         args=__utils__["cloud.filter_event"](
   522|             "requesting", event_kwargs, list(event_kwargs)
   523|         ),
   524|         sock_dir=__opts__["sock_dir"],
   525|         transport=__opts__["transport"],
   526|     )
   527|     log.debug("vm_kwargs: %s", vm_kwargs)
   528|     try:
   529|         conn.create_hosted_service(**service_kwargs)
   530|     except AzureConflictHttpError:
   531|         log.debug("Cloud service already exists")
   532|     except Exception as exc:  # pylint: disable=broad-except
   533|         error = "The hosted service name is invalid."
   534|         if error in str(exc):
   535|             log.error(
   536|                 "Error creating %s on Azure.\n\n"
   537|                 "The hosted service name is invalid. The name can contain "
   538|                 "only letters, numbers, and hyphens. The name must start with "
   539|                 "a letter and must end with a letter or a number.",
   540|                 vm_["name"],
   541|                 exc_info_on_loglevel=logging.DEBUG,
   542|             )
   543|         else:
   544|             log.error(
   545|                 "Error creating %s on Azure\n\n"
   546|                 "The following exception was thrown when trying to "
   547|                 "run the initial deployment: \n%s",
   548|                 vm_["name"],
   549|                 exc,
   550|                 exc_info_on_loglevel=logging.DEBUG,
   551|             )
   552|         return False
   553|     try:
   554|         result = conn.create_virtual_machine_deployment(**vm_kwargs)
   555|         log.debug("Request ID for machine: %s", result.request_id)
   556|         _wait_for_async(conn, result.request_id)
   557|     except AzureConflictHttpError:
   558|         log.debug("Conflict error. The deployment may already exist, trying add_role")
   559|         del vm_kwargs["deployment_slot"]
   560|         del vm_kwargs["label"]
   561|         del vm_kwargs["virtual_network_name"]
   562|         result = conn.add_role(**vm_kwargs)  # pylint: disable=unexpected-keyword-arg
   563|         _wait_for_async(conn, result.request_id)
   564|     except Exception as exc:  # pylint: disable=broad-except
   565|         error = "The hosted service name is invalid."
   566|         if error in str(exc):
   567|             log.error(
   568|                 "Error creating %s on Azure.\n\n"
   569|                 "The VM name is invalid. The name can contain "
   570|                 "only letters, numbers, and hyphens. The name must start with "
   571|                 "a letter and must end with a letter or a number.",
   572|                 vm_["name"],
   573|                 exc_info_on_loglevel=logging.DEBUG,
   574|             )
   575|         else:
   576|             log.error(
   577|                 "Error creating %s on Azure.\n\n"
   578|                 "The Virtual Machine could not be created. If you "
   579|                 "are using an already existing Cloud Service, "
   580|                 "make sure you set up the `port` variable corresponding "
   581|                 "to the SSH port exists and that the port number is not "
   582|                 "already in use.\nThe following exception was thrown when trying to "
   583|                 "run the initial deployment: \n%s",
   584|                 vm_["name"],
   585|                 exc,
   586|                 exc_info_on_loglevel=logging.DEBUG,
   587|             )
   588|         return False
   589|     def wait_for_hostname():
   590|         """
   591|         Wait for the IP address to become available
   592|         """
   593|         try:
   594|             conn.get_role(service_name, service_name, vm_["name"])
   595|             data = show_instance(vm_["name"], call="action")
   596|             if "url" in data and data["url"] != "":
   597|                 return data["url"]
   598|         except AzureMissingResourceHttpError:
   599|             pass
   600|         time.sleep(1)
   601|         return False
   602|     hostname = salt.utils.cloud.wait_for_fun(
   603|         wait_for_hostname,
   604|         timeout=config.get_cloud_config_value(
   605|             "wait_for_fun_timeout", vm_, __opts__, default=15 * 60
   606|         ),
   607|     )
   608|     if not hostname:
   609|         log.error("Failed to get a value for the hostname.")
   610|         return False
   611|     vm_["ssh_host"] = hostname.replace("http://", "").replace("/", "")
   612|     vm_["password"] = config.get_cloud_config_value("ssh_password", vm_, __opts__)
   613|     ret = __utils__["cloud.bootstrap"](vm_, __opts__)
   614|     volumes = config.get_cloud_config_value(
   615|         "volumes", vm_, __opts__, search_global=True
   616|     )
   617|     if volumes:
   618|         __utils__["cloud.fire_event"](
   619|             "event",
   620|             "attaching volumes",
   621|             "salt/cloud/{}/attaching_volumes".format(vm_["name"]),
   622|             args=__utils__["cloud.filter_event"]("attaching_volumes", vm_, ["volumes"]),
   623|             sock_dir=__opts__["sock_dir"],
   624|             transport=__opts__["transport"],
   625|         )
   626|         log.info("Create and attach volumes to node %s", vm_["name"])
   627|         created = create_attach_volumes(
   628|             vm_["name"],
   629|             {
   630|                 "volumes": volumes,
   631|                 "service_name": service_name,
   632|                 "deployment_name": vm_["name"],
   633|                 "media_link": media_link,
   634|                 "role_name": vm_["name"],
   635|                 "del_all_vols_on_destroy": vm_.get(
   636|                     "set_del_all_vols_on_destroy", False
   637|                 ),
   638|             },
   639|             call="action",
   640|         )
   641|         ret["Attached Volumes"] = created
   642|     data = show_instance(vm_["name"], call="action")
   643|     log.info("Created Cloud VM '%s'", vm_)
   644|     log.debug("'%s' VM creation details:\n%s", vm_["name"], pprint.pformat(data))
   645|     ret.update(data)
   646|     __utils__["cloud.fire_event"](
   647|         "event",
   648|         "created instance",
   649|         "salt/cloud/{}/created".format(vm_["name"]),
   650|         args=__utils__["cloud.filter_event"](
   651|             "created", vm_, ["name", "profile", "provider", "driver"]
   652|         ),
   653|         sock_dir=__opts__["sock_dir"],
   654|         transport=__opts__["transport"],
   655|     )
   656|     return ret
   657| @_deprecation_message
   658| def create_attach_volumes(name, kwargs, call=None, wait_to_finish=True):
   659|     """
   660|     Create and attach volumes to created node
   661|     """
   662|     if call != "action":
   663|         raise SaltCloudSystemExit(
   664|             "The create_attach_volumes action must be called with -a or --action."
   665|         )
   666|     if kwargs is None:
   667|         kwargs = {}
   668|     if isinstance(kwargs["volumes"], str):
   669|         volumes = salt.utils.yaml.safe_load(kwargs["volumes"])
   670|     else:
   671|         volumes = kwargs["volumes"]
   672|     conn = get_conn()
   673|     ret = []
   674|     for volume in volumes:
   675|         if "disk_name" in volume:
   676|             log.error("You cannot specify a disk_name. Only new volumes are allowed")
   677|             return False
   678|         volume.setdefault("logical_disk_size_in_gb", volume.get("size", 100))
   679|         volume.setdefault("host_caching", "ReadOnly")
   680|         volume.setdefault("lun", 0)
   681|         volume.setdefault(
   682|             "media_link",
   683|             kwargs["media_link"][:-4] + "-disk-{}.vhd".format(volume["lun"]),
   684|         )
   685|         volume.setdefault(
   686|             "disk_label", kwargs["role_name"] + "-disk-{}".format(volume["lun"])
   687|         )
   688|         volume_dict = {"volume_name": volume["lun"], "disk_label": volume["disk_label"]}
   689|         kwargs_add_data_disk = [
   690|             "lun",
   691|             "host_caching",
   692|             "media_link",
   693|             "disk_label",
   694|             "disk_name",
   695|             "logical_disk_size_in_gb",
   696|             "source_media_link",
   697|         ]
   698|         for key in set(volume.keys()) - set(kwargs_add_data_disk):
   699|             del volume[key]
   700|         attach = conn.add_data_disk(
   701|             kwargs["service_name"],
   702|             kwargs["deployment_name"],
   703|             kwargs["role_name"],
   704|             **volume,
   705|         )
   706|         log.debug(attach)
   707|         if attach:
   708|             msg = "{} attached to {} (aka {})".format(
   709|                 volume_dict["volume_name"],
   710|                 kwargs["role_name"],
   711|                 name,
   712|             )
   713|             log.info(msg)
   714|             ret.append(msg)
   715|         else:
   716|             log.error("Error attaching %s on Azure", volume_dict)
   717|     return ret
   718| @_deprecation_message
   719| def create_attach_volumes(name, kwargs, call=None, wait_to_finish=True):
   720|     """
   721|     Create and attach volumes to created node
   722|     """
   723|     if call != "action":
   724|         raise SaltCloudSystemExit(
   725|             "The create_attach_volumes action must be called with -a or --action."
   726|         )
   727|     if kwargs is None:
   728|         kwargs = {}
   729|     if isinstance(kwargs["volumes"], str):
   730|         volumes = salt.utils.yaml.safe_load(kwargs["volumes"])
   731|     else:
   732|         volumes = kwargs["volumes"]
   733|     conn = get_conn()
   734|     ret = []
   735|     for volume in volumes:
   736|         if "disk_name" in volume:
   737|             log.error("You cannot specify a disk_name. Only new volumes are allowed")
   738|             return False
   739|         volume.setdefault("logical_disk_size_in_gb", volume.get("size", 100))
   740|         volume.setdefault("host_caching", "ReadOnly")
   741|         volume.setdefault("lun", 0)
   742|         volume.setdefault(
   743|             "media_link",
   744|             kwargs["media_link"][:-4] + "-disk-{}.vhd".format(volume["lun"]),
   745|         )
   746|         volume.setdefault(
   747|             "disk_label", kwargs["role_name"] + "-disk-{}".format(volume["lun"])
   748|         )
   749|         volume_dict = {"volume_name": volume["lun"], "disk_label": volume["disk_label"]}
   750|         kwargs_add_data_disk = [
   751|             "lun",
   752|             "host_caching",
   753|             "media_link",
   754|             "disk_label",
   755|             "disk_name",
   756|             "logical_disk_size_in_gb",
   757|             "source_media_link",
   758|         ]
   759|         for key in set(volume.keys()) - set(kwargs_add_data_disk):
   760|             del volume[key]
   761|         result = conn.add_data_disk(
   762|             kwargs["service_name"],
   763|             kwargs["deployment_name"],
   764|             kwargs["role_name"],
   765|             **volume,
   766|         )
   767|         _wait_for_async(conn, result.request_id)
   768|         msg = "{} attached to {} (aka {})".format(
   769|             volume_dict["volume_name"], kwargs["role_name"], name
   770|         )
   771|         log.info(msg)
   772|         ret.append(msg)
   773|     return ret
   774| def _wait_for_async(conn, request_id):
   775|     """
   776|     Helper function for azure tests
   777|     """
   778|     count = 0
   779|     log.debug("Waiting for asynchronous operation to complete")
   780|     result = conn.get_operation_status(request_id)
   781|     while result.status == "InProgress":
   782|         count = count + 1
   783|         if count > 120:
   784|             raise ValueError(
   785|                 "Timed out waiting for asynchronous operation to complete."
   786|             )
   787|         time.sleep(5)
   788|         result = conn.get_operation_status(request_id)
   789|     if result.status != "Succeeded":
   790|         raise AzureException(
   791|             "Operation failed. {message} ({code})".format(
   792|                 message=result.error.message, code=result.error.code
   793|             )
   794|         )
   795| @_deprecation_message
   796| def destroy(name, conn=None, call=None, kwargs=None):
   797|     """
   798|     Destroy a VM
   799|     CLI Examples:
   800|     .. code-block:: bash
   801|         salt-cloud -d myminion
   802|         salt-cloud -a destroy myminion service_name=myservice
   803|     """
   804|     if call == "function":
   805|         raise SaltCloudSystemExit(
   806|             "The destroy action must be called with -d, --destroy, -a or --action."
   807|         )
   808|     if not conn:
   809|         conn = get_conn()
   810|     if kwargs is None:
   811|         kwargs = {}
   812|     instance_data = show_instance(name, call="action")
   813|     service_name = instance_data["deployment"]["name"]
   814|     disk_name = instance_data["role_info"]["os_virtual_hard_disk"]["disk_name"]
   815|     ret = {}
   816|     try:
   817|         log.debug("Deleting role")
   818|         result = conn.delete_role(service_name, service_name, name)
   819|         delete_type = "delete_role"
   820|     except AzureException:
   821|         log.debug("Failed to delete role, deleting deployment")
   822|         try:
   823|             result = conn.delete_deployment(service_name, service_name)
   824|         except AzureConflictHttpError as exc:
   825|             log.error(exc.message)
   826|             raise SaltCloudSystemExit(f"{name}: {exc.message}")
   827|         delete_type = "delete_deployment"
   828|     _wait_for_async(conn, result.request_id)
   829|     ret[name] = {
   830|         delete_type: {"request_id": result.request_id},
   831|     }
   832|     if __opts__.get("update_cachedir", False) is True:
   833|         __utils__["cloud.delete_minion_cachedir"](
   834|             name, _get_active_provider_name().split(":")[0], __opts__
   835|         )
   836|     cleanup_disks = config.get_cloud_config_value(
   837|         "cleanup_disks",
   838|         get_configured_provider(),
   839|         __opts__,
   840|         search_global=False,
   841|         default=False,
   842|     )
   843|     if cleanup_disks:
   844|         cleanup_vhds = kwargs.get(
   845|             "delete_vhd",
   846|             config.get_cloud_config_value(
   847|                 "cleanup_vhds",
   848|                 get_configured_provider(),
   849|                 __opts__,
   850|                 search_global=False,
   851|                 default=False,
   852|             ),
   853|         )
   854|         log.debug("Deleting disk %s", disk_name)
   855|         if cleanup_vhds:
   856|             log.debug("Deleting vhd")
   857|         def wait_for_destroy():
   858|             """
   859|             Wait for the VM to be deleted
   860|             """
   861|             try:
   862|                 data = delete_disk(
   863|                     kwargs={"name": disk_name, "delete_vhd": cleanup_vhds},
   864|                     call="function",
   865|                 )
   866|                 return data
   867|             except AzureConflictHttpError:
   868|                 log.debug("Waiting for VM to be destroyed...")
   869|             time.sleep(5)
   870|             return False
   871|         data = salt.utils.cloud.wait_for_fun(
   872|             wait_for_destroy,
   873|             timeout=config.get_cloud_config_value(
   874|                 "wait_for_fun_timeout", {}, __opts__, default=15 * 60
   875|             ),
   876|         )
   877|         ret[name]["delete_disk"] = {
   878|             "name": disk_name,
   879|             "delete_vhd": cleanup_vhds,
   880|             "data": data,
   881|         }
   882|         cleanup_services = config.get_cloud_config_value(
   883|             "cleanup_services",
   884|             get_configured_provider(),
   885|             __opts__,
   886|             search_global=False,
   887|             default=False,
   888|         )
   889|         if cleanup_services:
   890|             log.debug("Deleting service %s", service_name)
   891|             def wait_for_disk_delete():
   892|                 """
   893|                 Wait for the disk to be deleted
   894|                 """
   895|                 try:
   896|                     data = delete_service(
   897|                         kwargs={"name": service_name}, call="function"
   898|                     )
   899|                     return data
   900|                 except AzureConflictHttpError:
   901|                     log.debug("Waiting for disk to be deleted...")
   902|                 time.sleep(5)
   903|                 return False
   904|             data = salt.utils.cloud.wait_for_fun(
   905|                 wait_for_disk_delete,
   906|                 timeout=config.get_cloud_config_value(
   907|                     "wait_for_fun_timeout", {}, __opts__, default=15 * 60
   908|                 ),
   909|             )
   910|             ret[name]["delete_services"] = {"name": service_name, "data": data}
   911|     return ret
   912| @_deprecation_message
   913| def list_storage_services(conn=None, call=None):
   914|     """
   915|     List VMs on this Azure account, with full information
   916|     """
   917|     if call != "function":
   918|         raise SaltCloudSystemExit(
   919|             "The list_storage_services function must be called with -f or --function."
   920|         )
   921|     if not conn:
   922|         conn = get_conn()
   923|     ret = {}
   924|     accounts = conn.list_storage_accounts()
   925|     for service in accounts.storage_services:
   926|         ret[service.service_name] = {
   927|             "capabilities": service.capabilities,
   928|             "service_name": service.service_name,
   929|             "storage_service_properties": service.storage_service_properties,
   930|             "extended_properties": service.extended_properties,
   931|             "storage_service_keys": service.storage_service_keys,
   932|             "url": service.url,
   933|         }
   934|     return ret
   935| @_deprecation_message
   936| def get_operation_status(kwargs=None, conn=None, call=None):
   937|     """
   938|     .. versionadded:: 2015.8.0
   939|     Get Operation Status, based on a request ID
   940|     CLI Example:
   941|     .. code-block:: bash
   942|         salt-cloud -f get_operation_status my-azure id=0123456789abcdef0123456789abcdef
   943|     """
   944|     if call != "function":
   945|         raise SaltCloudSystemExit(
   946|             "The show_instance function must be called with -f or --function."
   947|         )
   948|     if kwargs is None:
   949|         kwargs = {}
   950|     if "id" not in kwargs:
   951|         raise SaltCloudSystemExit('A request ID must be specified as "id"')
   952|     if not conn:
   953|         conn = get_conn()
   954|     data = conn.get_operation_status(kwargs["id"])
   955|     ret = {
   956|         "http_status_code": data.http_status_code,
   957|         "id": kwargs["id"],
   958|         "status": data.status,
   959|     }
   960|     if hasattr(data.error, "code"):
   961|         ret["error"] = {
   962|             "code": data.error.code,
   963|             "message": data.error.message,
   964|         }
   965|     return ret
   966| @_deprecation_message
   967| def list_storage(kwargs=None, conn=None, call=None):
   968|     """
   969|     .. versionadded:: 2015.8.0
   970|     List storage accounts associated with the account
   971|     CLI Example:
   972|     .. code-block:: bash
   973|         salt-cloud -f list_storage my-azure
   974|     """
   975|     if call != "function":
   976|         raise SaltCloudSystemExit(
   977|             "The list_storage function must be called with -f or --function."
   978|         )
   979|     if not conn:
   980|         conn = get_conn()
   981|     data = conn.list_storage_accounts()
   982|     pprint.pprint(dir(data))
   983|     ret = {}
   984|     for item in data.storage_services:
   985|         ret[item.service_name] = object_to_dict(item)
   986|     return ret
   987| @_deprecation_message
   988| def show_storage(kwargs=None, conn=None, call=None):
   989|     """
   990|     .. versionadded:: 2015.8.0
   991|     List storage service properties
   992|     CLI Example:
   993|     .. code-block:: bash
   994|         salt-cloud -f show_storage my-azure name=my_storage
   995|     """
   996|     if call != "function":
   997|         raise SaltCloudSystemExit(
   998|             "The show_storage function must be called with -f or --function."
   999|         )
  1000|     if not conn:
  1001|         conn = get_conn()
  1002|     if kwargs is None:
  1003|         kwargs = {}
  1004|     if "name" not in kwargs:
  1005|         raise SaltCloudSystemExit('A name must be specified as "name"')
  1006|     data = conn.get_storage_account_properties(
  1007|         kwargs["name"],
  1008|     )
  1009|     return object_to_dict(data)
  1010| get_storage = show_storage
  1011| @_deprecation_message
  1012| def show_storage_keys(kwargs=None, conn=None, call=None):
  1013|     """
  1014|     .. versionadded:: 2015.8.0
  1015|     Show storage account keys
  1016|     CLI Example:
  1017|     .. code-block:: bash
  1018|         salt-cloud -f show_storage_keys my-azure name=my_storage
  1019|     """
  1020|     if call != "function":
  1021|         raise SaltCloudSystemExit(
  1022|             "The show_storage_keys function must be called with -f or --function."
  1023|         )
  1024|     if not conn:
  1025|         conn = get_conn()
  1026|     if kwargs is None:
  1027|         kwargs = {}
  1028|     if "name" not in kwargs:
  1029|         raise SaltCloudSystemExit('A name must be specified as "name"')
  1030|     try:
  1031|         data = conn.get_storage_account_keys(
  1032|             kwargs["name"],
  1033|         )
  1034|     except AzureMissingResourceHttpError as exc:
  1035|         storage_data = show_storage(kwargs={"name": kwargs["name"]}, call="function")
  1036|         if storage_data["storage_service_properties"]["status"] == "Creating":
  1037|             raise SaltCloudSystemExit(
  1038|                 "The storage account keys have not yet been created."
  1039|             )
  1040|         else:
  1041|             raise SaltCloudSystemExit("{}: {}".format(kwargs["name"], exc.message))
  1042|     return object_to_dict(data)
  1043| get_storage_keys = show_storage_keys
  1044| @_deprecation_message
  1045| def create_storage(kwargs=None, conn=None, call=None):
  1046|     """
  1047|     .. versionadded:: 2015.8.0
  1048|     Create a new storage account
  1049|     CLI Example:
  1050|     .. code-block:: bash
  1051|         salt-cloud -f create_storage my-azure name=my_storage label=my_storage location='West US'
  1052|     """
  1053|     if call != "function":
  1054|         raise SaltCloudSystemExit(
  1055|             "The show_storage function must be called with -f or --function."
  1056|         )
  1057|     if kwargs is None:
  1058|         kwargs = {}
  1059|     if not conn:
  1060|         conn = get_conn()
  1061|     if "name" not in kwargs:
  1062|         raise SaltCloudSystemExit('A name must be specified as "name"')
  1063|     if "description" not in kwargs:
  1064|         raise SaltCloudSystemExit('A description must be specified as "description"')
  1065|     if "label" not in kwargs:
  1066|         raise SaltCloudSystemExit('A label must be specified as "label"')
  1067|     if "location" not in kwargs and "affinity_group" not in kwargs:
  1068|         raise SaltCloudSystemExit(
  1069|             "Either a location or an affinity_group must be specified (but not both)"
  1070|         )
  1071|     try:
  1072|         data = conn.create_storage_account(
  1073|             service_name=kwargs["name"],
  1074|             label=kwargs["label"],
  1075|             description=kwargs.get("description", None),
  1076|             location=kwargs.get("location", None),
  1077|             affinity_group=kwargs.get("affinity_group", None),
  1078|             extended_properties=kwargs.get("extended_properties", None),
  1079|             geo_replication_enabled=kwargs.get("geo_replication_enabled", None),
  1080|             account_type=kwargs.get("account_type", "Standard_GRS"),
  1081|         )
  1082|         return {"Success": "The storage account was successfully created"}
  1083|     except AzureConflictHttpError:
  1084|         raise SaltCloudSystemExit(
  1085|             "There was a conflict. This usually means that the storage account already"
  1086|             " exists."
  1087|         )
  1088| @_deprecation_message
  1089| def update_storage(kwargs=None, conn=None, call=None):
  1090|     """
  1091|     .. versionadded:: 2015.8.0
  1092|     Update a storage account's properties
  1093|     CLI Example:
  1094|     .. code-block:: bash
  1095|         salt-cloud -f update_storage my-azure name=my_storage label=my_storage
  1096|     """
  1097|     if call != "function":
  1098|         raise SaltCloudSystemExit(
  1099|             "The show_storage function must be called with -f or --function."
  1100|         )
  1101|     if not conn:
  1102|         conn = get_conn()
  1103|     if kwargs is None:
  1104|         kwargs = {}
  1105|     if "name" not in kwargs:
  1106|         raise SaltCloudSystemExit('A name must be specified as "name"')
  1107|     data = conn.update_storage_account(
  1108|         service_name=kwargs["name"],
  1109|         label=kwargs.get("label", None),
  1110|         description=kwargs.get("description", None),
  1111|         extended_properties=kwargs.get("extended_properties", None),
  1112|         geo_replication_enabled=kwargs.get("geo_replication_enabled", None),
  1113|         account_type=kwargs.get("account_type", "Standard_GRS"),
  1114|     )
  1115|     return show_storage(kwargs={"name": kwargs["name"]}, call="function")
  1116| @_deprecation_message
  1117| def regenerate_storage_keys(kwargs=None, conn=None, call=None):
  1118|     """
  1119|     .. versionadded:: 2015.8.0
  1120|     Regenerate storage account keys. Requires a key_type ("primary" or
  1121|     "secondary") to be specified.
  1122|     CLI Example:
  1123|     .. code-block:: bash
  1124|         salt-cloud -f regenerate_storage_keys my-azure name=my_storage key_type=primary
  1125|     """
  1126|     if call != "function":
  1127|         raise SaltCloudSystemExit(
  1128|             "The show_storage function must be called with -f or --function."
  1129|         )
  1130|     if not conn:
  1131|         conn = get_conn()
  1132|     if kwargs is None:
  1133|         kwargs = {}
  1134|     if "name" not in kwargs:
  1135|         raise SaltCloudSystemExit('A name must be specified as "name"')
  1136|     if "key_type" not in kwargs or kwargs["key_type"] not in ("primary", "secondary"):
  1137|         raise SaltCloudSystemExit(
  1138|             'A key_type must be specified ("primary" or "secondary")'
  1139|         )
  1140|     try:
  1141|         data = conn.regenerate_storage_account_keys(
  1142|             service_name=kwargs["name"],
  1143|             key_type=kwargs["key_type"],
  1144|         )
  1145|         return show_storage_keys(kwargs={"name": kwargs["name"]}, call="function")
  1146|     except AzureConflictHttpError:
  1147|         raise SaltCloudSystemExit(
  1148|             "There was a conflict. This usually means that the storage account already"
  1149|             " exists."
  1150|         )
  1151| @_deprecation_message
  1152| def delete_storage(kwargs=None, conn=None, call=None):
  1153|     """
  1154|     .. versionadded:: 2015.8.0
  1155|     Delete a specific storage account
  1156|     CLI Examples:
  1157|     .. code-block:: bash
  1158|         salt-cloud -f delete_storage my-azure name=my_storage
  1159|     """
  1160|     if call != "function":
  1161|         raise SaltCloudSystemExit(
  1162|             "The delete_storage function must be called with -f or --function."
  1163|         )
  1164|     if kwargs is None:
  1165|         kwargs = {}
  1166|     if "name" not in kwargs:
  1167|         raise SaltCloudSystemExit('A name must be specified as "name"')
  1168|     if not conn:
  1169|         conn = get_conn()
  1170|     try:
  1171|         data = conn.delete_storage_account(kwargs["name"])
  1172|         return {"Success": "The storage account was successfully deleted"}
  1173|     except AzureMissingResourceHttpError as exc:
  1174|         raise SaltCloudSystemExit("{}: {}".format(kwargs["name"], exc.message))
  1175| @_deprecation_message
  1176| def list_services(kwargs=None, conn=None, call=None):
  1177|     """
  1178|     .. versionadded:: 2015.8.0
  1179|     List hosted services associated with the account
  1180|     CLI Example:
  1181|     .. code-block:: bash
  1182|         salt-cloud -f list_services my-azure
  1183|     """
  1184|     if call != "function":
  1185|         raise SaltCloudSystemExit(
  1186|             "The list_services function must be called with -f or --function."
  1187|         )
  1188|     if not conn:
  1189|         conn = get_conn()
  1190|     data = conn.list_hosted_services()
  1191|     ret = {}
  1192|     for item in data.hosted_services:
  1193|         ret[item.service_name] = object_to_dict(item)
  1194|         ret[item.service_name]["name"] = item.service_name
  1195|     return ret
  1196| @_deprecation_message
  1197| def show_service(kwargs=None, conn=None, call=None):
  1198|     """
  1199|     .. versionadded:: 2015.8.0
  1200|     List hosted service properties
  1201|     CLI Example:
  1202|     .. code-block:: bash
  1203|         salt-cloud -f show_service my-azure name=my_service
  1204|     """
  1205|     if call != "function":
  1206|         raise SaltCloudSystemExit(
  1207|             "The show_service function must be called with -f or --function."
  1208|         )
  1209|     if not conn:
  1210|         conn = get_conn()
  1211|     if kwargs is None:
  1212|         kwargs = {}
  1213|     if "name" not in kwargs:
  1214|         raise SaltCloudSystemExit('A name must be specified as "name"')
  1215|     data = conn.get_hosted_service_properties(
  1216|         kwargs["name"], kwargs.get("details", False)
  1217|     )
  1218|     ret = object_to_dict(data)
  1219|     return ret
  1220| @_deprecation_message
  1221| def create_service(kwargs=None, conn=None, call=None):
  1222|     """
  1223|     .. versionadded:: 2015.8.0
  1224|     Create a new hosted service
  1225|     CLI Example:
  1226|     .. code-block:: bash
  1227|         salt-cloud -f create_service my-azure name=my_service label=my_service location='West US'
  1228|     """
  1229|     if call != "function":
  1230|         raise SaltCloudSystemExit(
  1231|             "The create_service function must be called with -f or --function."
  1232|         )
  1233|     if not conn:
  1234|         conn = get_conn()
  1235|     if kwargs is None:
  1236|         kwargs = {}
  1237|     if "name" not in kwargs:
  1238|         raise SaltCloudSystemExit('A name must be specified as "name"')
  1239|     if "label" not in kwargs:
  1240|         raise SaltCloudSystemExit('A label must be specified as "label"')
  1241|     if "location" not in kwargs and "affinity_group" not in kwargs:
  1242|         raise SaltCloudSystemExit(
  1243|             "Either a location or an affinity_group must be specified (but not both)"
  1244|         )
  1245|     try:
  1246|         data = conn.create_hosted_service(
  1247|             kwargs["name"],
  1248|             kwargs["label"],
  1249|             kwargs.get("description", None),
  1250|             kwargs.get("location", None),
  1251|             kwargs.get("affinity_group", None),
  1252|             kwargs.get("extended_properties", None),
  1253|         )
  1254|         return {"Success": "The service was successfully created"}
  1255|     except AzureConflictHttpError:
  1256|         raise SaltCloudSystemExit(
  1257|             "There was a conflict. This usually means that the service already exists."
  1258|         )
  1259| @_deprecation_message
  1260| def delete_service(kwargs=None, conn=None, call=None):
  1261|     """
  1262|     .. versionadded:: 2015.8.0
  1263|     Delete a specific service associated with the account
  1264|     CLI Examples:
  1265|     .. code-block:: bash
  1266|         salt-cloud -f delete_service my-azure name=my_service
  1267|     """
  1268|     if call != "function":
  1269|         raise SaltCloudSystemExit(
  1270|             "The delete_service function must be called with -f or --function."
  1271|         )
  1272|     if kwargs is None:
  1273|         kwargs = {}
  1274|     if "name" not in kwargs:
  1275|         raise SaltCloudSystemExit('A name must be specified as "name"')
  1276|     if not conn:
  1277|         conn = get_conn()
  1278|     try:
  1279|         conn.delete_hosted_service(kwargs["name"])
  1280|         return {"Success": "The service was successfully deleted"}
  1281|     except AzureMissingResourceHttpError as exc:
  1282|         raise SaltCloudSystemExit("{}: {}".format(kwargs["name"], exc.message))
  1283| @_deprecation_message
  1284| def list_disks(kwargs=None, conn=None, call=None):
  1285|     """
  1286|     .. versionadded:: 2015.8.0
  1287|     List disks associated with the account
  1288|     CLI Example:
  1289|     .. code-block:: bash
  1290|         salt-cloud -f list_disks my-azure
  1291|     """
  1292|     if call != "function":
  1293|         raise SaltCloudSystemExit(
  1294|             "The list_disks function must be called with -f or --function."
  1295|         )
  1296|     if not conn:
  1297|         conn = get_conn()
  1298|     data = conn.list_disks()
  1299|     ret = {}
  1300|     for item in data.disks:
  1301|         ret[item.name] = object_to_dict(item)
  1302|     return ret
  1303| @_deprecation_message
  1304| def show_disk(kwargs=None, conn=None, call=None):
  1305|     """
  1306|     .. versionadded:: 2015.8.0
  1307|     Return information about a disk
  1308|     CLI Example:
  1309|     .. code-block:: bash
  1310|         salt-cloud -f show_disk my-azure name=my_disk
  1311|     """
  1312|     if call != "function":
  1313|         raise SaltCloudSystemExit(
  1314|             "The get_disk function must be called with -f or --function."
  1315|         )
  1316|     if not conn:
  1317|         conn = get_conn()
  1318|     if kwargs is None:
  1319|         kwargs = {}
  1320|     if "name" not in kwargs:
  1321|         raise SaltCloudSystemExit('A name must be specified as "name"')
  1322|     data = conn.get_disk(kwargs["name"])
  1323|     return object_to_dict(data)
  1324| get_disk = show_disk
  1325| @_deprecation_message
  1326| def cleanup_unattached_disks(kwargs=None, conn=None, call=None):
  1327|     """
  1328|     .. versionadded:: 2015.8.0
  1329|     Cleans up all disks associated with the account, which are not attached.
  1330|     *** CAUTION *** This is a destructive function with no undo button, and no
  1331|     "Are you sure?" confirmation!
  1332|     CLI Examples:
  1333|     .. code-block:: bash
  1334|         salt-cloud -f cleanup_unattached_disks my-azure name=my_disk
  1335|         salt-cloud -f cleanup_unattached_disks my-azure name=my_disk delete_vhd=True
  1336|     """
  1337|     if call != "function":
  1338|         raise SaltCloudSystemExit(
  1339|             "The delete_disk function must be called with -f or --function."
  1340|         )
  1341|     if kwargs is None:
  1342|         kwargs = {}
  1343|     disks = list_disks(kwargs=kwargs, conn=conn, call="function")
  1344|     for disk in disks:
  1345|         if disks[disk]["attached_to"] is None:
  1346|             del_kwargs = {
  1347|                 "name": disks[disk]["name"],
  1348|                 "delete_vhd": kwargs.get("delete_vhd", False),
  1349|             }
  1350|             log.info(
  1351|                 "Deleting disk %s, deleting VHD: %s",
  1352|                 del_kwargs["name"],
  1353|                 del_kwargs["delete_vhd"],
  1354|             )
  1355|             data = delete_disk(kwargs=del_kwargs, call="function")
  1356|     return True
  1357| @_deprecation_message
  1358| def delete_disk(kwargs=None, conn=None, call=None):
  1359|     """
  1360|     .. versionadded:: 2015.8.0
  1361|     Delete a specific disk associated with the account
  1362|     CLI Examples:
  1363|     .. code-block:: bash
  1364|         salt-cloud -f delete_disk my-azure name=my_disk
  1365|         salt-cloud -f delete_disk my-azure name=my_disk delete_vhd=True
  1366|     """
  1367|     if call != "function":
  1368|         raise SaltCloudSystemExit(
  1369|             "The delete_disk function must be called with -f or --function."
  1370|         )
  1371|     if kwargs is None:
  1372|         kwargs = {}
  1373|     if "name" not in kwargs:
  1374|         raise SaltCloudSystemExit('A name must be specified as "name"')
  1375|     if not conn:
  1376|         conn = get_conn()
  1377|     try:
  1378|         data = conn.delete_disk(kwargs["name"], kwargs.get("delete_vhd", False))
  1379|         return {"Success": "The disk was successfully deleted"}
  1380|     except AzureMissingResourceHttpError as exc:
  1381|         raise SaltCloudSystemExit("{}: {}".format(kwargs["name"], exc.message))
  1382| @_deprecation_message
  1383| def update_disk(kwargs=None, conn=None, call=None):
  1384|     """
  1385|     .. versionadded:: 2015.8.0
  1386|     Update a disk's properties
  1387|     CLI Example:
  1388|     .. code-block:: bash
  1389|         salt-cloud -f update_disk my-azure name=my_disk label=my_disk
  1390|         salt-cloud -f update_disk my-azure name=my_disk new_name=another_disk
  1391|     """
  1392|     if call != "function":
  1393|         raise SaltCloudSystemExit(
  1394|             "The show_disk function must be called with -f or --function."
  1395|         )
  1396|     if not conn:
  1397|         conn = get_conn()
  1398|     if kwargs is None:
  1399|         kwargs = {}
  1400|     if "name" not in kwargs:
  1401|         raise SaltCloudSystemExit('A name must be specified as "name"')
  1402|     old_data = show_disk(kwargs={"name": kwargs["name"]}, call="function")
  1403|     data = conn.update_disk(
  1404|         disk_name=kwargs["name"],
  1405|         has_operating_system=kwargs.get(
  1406|             "has_operating_system", old_data["has_operating_system"]
  1407|         ),
  1408|         label=kwargs.get("label", old_data["label"]),
  1409|         media_link=kwargs.get("media_link", old_data["media_link"]),
  1410|         name=kwargs.get("new_name", old_data["name"]),
  1411|         os=kwargs.get("os", old_data["os"]),
  1412|     )
  1413|     return show_disk(kwargs={"name": kwargs["name"]}, call="function")
  1414| @_deprecation_message
  1415| def list_service_certificates(kwargs=None, conn=None, call=None):
  1416|     """
  1417|     .. versionadded:: 2015.8.0
  1418|     List certificates associated with the service
  1419|     CLI Example:
  1420|     .. code-block:: bash
  1421|         salt-cloud -f list_service_certificates my-azure name=my_service
  1422|     """
  1423|     if call != "function":
  1424|         raise SaltCloudSystemExit(
  1425|             "The list_service_certificates function must be called with -f or"
  1426|             " --function."
  1427|         )
  1428|     if kwargs is None:
  1429|         kwargs = {}
  1430|     if "name" not in kwargs:
  1431|         raise SaltCloudSystemExit('A service name must be specified as "name"')
  1432|     if not conn:
  1433|         conn = get_conn()
  1434|     data = conn.list_service_certificates(service_name=kwargs["name"])
  1435|     ret = {}
  1436|     for item in data.certificates:
  1437|         ret[item.thumbprint] = object_to_dict(item)
  1438|     return ret
  1439| @_deprecation_message
  1440| def show_service_certificate(kwargs=None, conn=None, call=None):
  1441|     """
  1442|     .. versionadded:: 2015.8.0
  1443|     Return information about a service certificate
  1444|     CLI Example:
  1445|     .. code-block:: bash
  1446|         salt-cloud -f show_service_certificate my-azure name=my_service_certificate \\
  1447|             thumbalgorithm=sha1 thumbprint=0123456789ABCDEF
  1448|     """
  1449|     if call != "function":
  1450|         raise SaltCloudSystemExit(
  1451|             "The get_service_certificate function must be called with -f or --function."
  1452|         )
  1453|     if not conn:
  1454|         conn = get_conn()
  1455|     if kwargs is None:
  1456|         kwargs = {}
  1457|     if "name" not in kwargs:
  1458|         raise SaltCloudSystemExit('A service name must be specified as "name"')
  1459|     if "thumbalgorithm" not in kwargs:
  1460|         raise SaltCloudSystemExit(
  1461|             'A thumbalgorithm must be specified as "thumbalgorithm"'
  1462|         )
  1463|     if "thumbprint" not in kwargs:
  1464|         raise SaltCloudSystemExit('A thumbprint must be specified as "thumbprint"')
  1465|     data = conn.get_service_certificate(
  1466|         kwargs["name"],
  1467|         kwargs["thumbalgorithm"],
  1468|         kwargs["thumbprint"],
  1469|     )
  1470|     return object_to_dict(data)
  1471| get_service_certificate = show_service_certificate
  1472| @_deprecation_message
  1473| def add_service_certificate(kwargs=None, conn=None, call=None):
  1474|     """
  1475|     .. versionadded:: 2015.8.0
  1476|     Add a new service certificate
  1477|     CLI Example:
  1478|     .. code-block:: bash
  1479|         salt-cloud -f add_service_certificate my-azure name=my_service_certificate \\
  1480|             data='...CERT_DATA...' certificate_format=sha1 password=verybadpass
  1481|     """
  1482|     if call != "function":
  1483|         raise SaltCloudSystemExit(
  1484|             "The add_service_certificate function must be called with -f or --function."
  1485|         )
  1486|     if not conn:
  1487|         conn = get_conn()
  1488|     if kwargs is None:
  1489|         kwargs = {}
  1490|     if "name" not in kwargs:
  1491|         raise SaltCloudSystemExit('A name must be specified as "name"')
  1492|     if "data" not in kwargs:
  1493|         raise SaltCloudSystemExit('Certificate data must be specified as "data"')
  1494|     if "certificate_format" not in kwargs:
  1495|         raise SaltCloudSystemExit(
  1496|             'A certificate_format must be specified as "certificate_format"'
  1497|         )
  1498|     if "password" not in kwargs:
  1499|         raise SaltCloudSystemExit('A password must be specified as "password"')
  1500|     try:
  1501|         data = conn.add_service_certificate(
  1502|             kwargs["name"],
  1503|             kwargs["data"],
  1504|             kwargs["certificate_format"],
  1505|             kwargs["password"],
  1506|         )
  1507|         return {"Success": "The service certificate was successfully added"}
  1508|     except AzureConflictHttpError:
  1509|         raise SaltCloudSystemExit(
  1510|             "There was a conflict. This usually means that the "
  1511|             "service certificate already exists."
  1512|         )
  1513| @_deprecation_message
  1514| def delete_service_certificate(kwargs=None, conn=None, call=None):
  1515|     """
  1516|     .. versionadded:: 2015.8.0
  1517|     Delete a specific certificate associated with the service
  1518|     CLI Examples:
  1519|     .. code-block:: bash
  1520|         salt-cloud -f delete_service_certificate my-azure name=my_service_certificate \\
  1521|             thumbalgorithm=sha1 thumbprint=0123456789ABCDEF
  1522|     """
  1523|     if call != "function":
  1524|         raise SaltCloudSystemExit(
  1525|             "The delete_service_certificate function must be called with -f or"
  1526|             " --function."
  1527|         )
  1528|     if kwargs is None:
  1529|         kwargs = {}
  1530|     if "name" not in kwargs:
  1531|         raise SaltCloudSystemExit('A name must be specified as "name"')
  1532|     if "thumbalgorithm" not in kwargs:
  1533|         raise SaltCloudSystemExit(
  1534|             'A thumbalgorithm must be specified as "thumbalgorithm"'
  1535|         )
  1536|     if "thumbprint" not in kwargs:
  1537|         raise SaltCloudSystemExit('A thumbprint must be specified as "thumbprint"')
  1538|     if not conn:
  1539|         conn = get_conn()
  1540|     try:
  1541|         data = conn.delete_service_certificate(
  1542|             kwargs["name"],
  1543|             kwargs["thumbalgorithm"],
  1544|             kwargs["thumbprint"],
  1545|         )
  1546|         return {"Success": "The service certificate was successfully deleted"}
  1547|     except AzureMissingResourceHttpError as exc:
  1548|         raise SaltCloudSystemExit("{}: {}".format(kwargs["name"], exc.message))
  1549| @_deprecation_message
  1550| def list_management_certificates(kwargs=None, conn=None, call=None):
  1551|     """
  1552|     .. versionadded:: 2015.8.0
  1553|     List management certificates associated with the subscription
  1554|     CLI Example:
  1555|     .. code-block:: bash
  1556|         salt-cloud -f list_management_certificates my-azure name=my_management
  1557|     """
  1558|     if call != "function":
  1559|         raise SaltCloudSystemExit(
  1560|             "The list_management_certificates function must be called with -f or"
  1561|             " --function."
  1562|         )
  1563|     if not conn:
  1564|         conn = get_conn()
  1565|     data = conn.list_management_certificates()
  1566|     ret = {}
  1567|     for item in data.subscription_certificates:
  1568|         ret[item.subscription_certificate_thumbprint] = object_to_dict(item)
  1569|     return ret
  1570| @_deprecation_message
  1571| def show_management_certificate(kwargs=None, conn=None, call=None):
  1572|     """
  1573|     .. versionadded:: 2015.8.0
  1574|     Return information about a management_certificate
  1575|     CLI Example:
  1576|     .. code-block:: bash
  1577|         salt-cloud -f get_management_certificate my-azure name=my_management_certificate \\
  1578|             thumbalgorithm=sha1 thumbprint=0123456789ABCDEF
  1579|     """
  1580|     if call != "function":
  1581|         raise SaltCloudSystemExit(
  1582|             "The get_management_certificate function must be called with -f or"
  1583|             " --function."
  1584|         )
  1585|     if not conn:
  1586|         conn = get_conn()
  1587|     if kwargs is None:
  1588|         kwargs = {}
  1589|     if "thumbprint" not in kwargs:
  1590|         raise SaltCloudSystemExit('A thumbprint must be specified as "thumbprint"')
  1591|     data = conn.get_management_certificate(kwargs["thumbprint"])
  1592|     return object_to_dict(data)
  1593| get_management_certificate = show_management_certificate
  1594| @_deprecation_message
  1595| def add_management_certificate(kwargs=None, conn=None, call=None):
  1596|     """
  1597|     .. versionadded:: 2015.8.0
  1598|     Add a new management certificate
  1599|     CLI Example:
  1600|     .. code-block:: bash
  1601|         salt-cloud -f add_management_certificate my-azure public_key='...PUBKEY...' \\
  1602|             thumbprint=0123456789ABCDEF data='...CERT_DATA...'
  1603|     """
  1604|     if call != "function":
  1605|         raise SaltCloudSystemExit(
  1606|             "The add_management_certificate function must be called with -f or"
  1607|             " --function."
  1608|         )
  1609|     if not conn:
  1610|         conn = get_conn()
  1611|     if kwargs is None:
  1612|         kwargs = {}
  1613|     if "public_key" not in kwargs:
  1614|         raise SaltCloudSystemExit('A public_key must be specified as "public_key"')
  1615|     if "thumbprint" not in kwargs:
  1616|         raise SaltCloudSystemExit('A thumbprint must be specified as "thumbprint"')
  1617|     if "data" not in kwargs:
  1618|         raise SaltCloudSystemExit('Certificate data must be specified as "data"')
  1619|     try:
  1620|         conn.add_management_certificate(
  1621|             kwargs["name"],
  1622|             kwargs["thumbprint"],
  1623|             kwargs["data"],
  1624|         )
  1625|         return {"Success": "The management certificate was successfully added"}
  1626|     except AzureConflictHttpError:
  1627|         raise SaltCloudSystemExit(
  1628|             "There was a conflict. "
  1629|             "This usually means that the management certificate already exists."
  1630|         )
  1631| @_deprecation_message
  1632| def delete_management_certificate(kwargs=None, conn=None, call=None):
  1633|     """
  1634|     .. versionadded:: 2015.8.0
  1635|     Delete a specific certificate associated with the management
  1636|     CLI Examples:
  1637|     .. code-block:: bash
  1638|         salt-cloud -f delete_management_certificate my-azure name=my_management_certificate \\
  1639|             thumbalgorithm=sha1 thumbprint=0123456789ABCDEF
  1640|     """
  1641|     if call != "function":
  1642|         raise SaltCloudSystemExit(
  1643|             "The delete_management_certificate function must be called with -f or"
  1644|             " --function."
  1645|         )
  1646|     if kwargs is None:
  1647|         kwargs = {}
  1648|     if "thumbprint" not in kwargs:
  1649|         raise SaltCloudSystemExit('A thumbprint must be specified as "thumbprint"')
  1650|     if not conn:
  1651|         conn = get_conn()
  1652|     try:
  1653|         conn.delete_management_certificate(kwargs["thumbprint"])
  1654|         return {"Success": "The management certificate was successfully deleted"}
  1655|     except AzureMissingResourceHttpError as exc:
  1656|         raise SaltCloudSystemExit("{}: {}".format(kwargs["thumbprint"], exc.message))
  1657| @_deprecation_message
  1658| def list_virtual_networks(kwargs=None, conn=None, call=None):
  1659|     """
  1660|     .. versionadded:: 2015.8.0
  1661|     List input endpoints associated with the deployment
  1662|     CLI Example:
  1663|     .. code-block:: bash
  1664|         salt-cloud -f list_virtual_networks my-azure service=myservice deployment=mydeployment
  1665|     """
  1666|     if call != "function":
  1667|         raise SaltCloudSystemExit(
  1668|             "The list_virtual_networks function must be called with -f or --function."
  1669|         )
  1670|     path = "services/networking/virtualnetwork"
  1671|     data = query(path)
  1672|     return data
  1673| @_deprecation_message
  1674| def list_input_endpoints(kwargs=None, conn=None, call=None):
  1675|     """
  1676|     .. versionadded:: 2015.8.0
  1677|     List input endpoints associated with the deployment
  1678|     CLI Example:
  1679|     .. code-block:: bash
  1680|         salt-cloud -f list_input_endpoints my-azure service=myservice deployment=mydeployment
  1681|     """
  1682|     if call != "function":
  1683|         raise SaltCloudSystemExit(
  1684|             "The list_input_endpoints function must be called with -f or --function."
  1685|         )
  1686|     if kwargs is None:
  1687|         kwargs = {}
  1688|     if "service" not in kwargs:
  1689|         raise SaltCloudSystemExit('A service name must be specified as "service"')
  1690|     if "deployment" not in kwargs:
  1691|         raise SaltCloudSystemExit('A deployment name must be specified as "deployment"')
  1692|     path = "services/hostedservices/{}/deployments/{}".format(
  1693|         kwargs["service"],
  1694|         kwargs["deployment"],
  1695|     )
  1696|     data = query(path)
  1697|     if data is None:
  1698|         raise SaltCloudSystemExit(
  1699|             "There was an error listing endpoints with the {} service on the {}"
  1700|             " deployment.".format(kwargs["service"], kwargs["deployment"])
  1701|         )
  1702|     ret = {}
  1703|     for item in data:
  1704|         if "Role" in item:
  1705|             role = item["Role"]
  1706|             if not isinstance(role, dict):
  1707|                 return ret
  1708|             input_endpoint = (
  1709|                 role["ConfigurationSets"]["ConfigurationSet"]
  1710|                 .get("InputEndpoints", {})
  1711|                 .get("InputEndpoint")
  1712|             )
  1713|             if not input_endpoint:
  1714|                 continue
  1715|             if not isinstance(input_endpoint, list):
  1716|                 input_endpoint = [input_endpoint]
  1717|             for endpoint in input_endpoint:
  1718|                 ret[endpoint["Name"]] = endpoint
  1719|             return ret
  1720|     return ret
  1721| @_deprecation_message
  1722| def show_input_endpoint(kwargs=None, conn=None, call=None):
  1723|     """
  1724|     .. versionadded:: 2015.8.0
  1725|     Show an input endpoint associated with the deployment
  1726|     CLI Example:
  1727|     .. code-block:: bash
  1728|         salt-cloud -f show_input_endpoint my-azure service=myservice \\
  1729|             deployment=mydeployment name=SSH
  1730|     """
  1731|     if call != "function":
  1732|         raise SaltCloudSystemExit(
  1733|             "The show_input_endpoint function must be called with -f or --function."
  1734|         )
  1735|     if kwargs is None:
  1736|         kwargs = {}
  1737|     if "name" not in kwargs:
  1738|         raise SaltCloudSystemExit('An endpoint name must be specified as "name"')
  1739|     data = list_input_endpoints(kwargs=kwargs, call="function")
  1740|     return data.get(kwargs["name"], None)
  1741| get_input_endpoint = show_input_endpoint
  1742| @_deprecation_message
  1743| def update_input_endpoint(kwargs=None, conn=None, call=None, activity="update"):
  1744|     """
  1745|     .. versionadded:: 2015.8.0
  1746|     Update an input endpoint associated with the deployment. Please note that
  1747|     there may be a delay before the changes show up.
  1748|     CLI Example:
  1749|     .. code-block:: bash
  1750|         salt-cloud -f update_input_endpoint my-azure service=myservice \\
  1751|             deployment=mydeployment role=myrole name=HTTP local_port=80 \\
  1752|             port=80 protocol=tcp enable_direct_server_return=False \\
  1753|             timeout_for_tcp_idle_connection=4
  1754|     """
  1755|     if call != "function":
  1756|         raise SaltCloudSystemExit(
  1757|             "The update_input_endpoint function must be called with -f or --function."
  1758|         )
  1759|     if kwargs is None:
  1760|         kwargs = {}
  1761|     if "service" not in kwargs:
  1762|         raise SaltCloudSystemExit('A service name must be specified as "service"')
  1763|     if "deployment" not in kwargs:
  1764|         raise SaltCloudSystemExit('A deployment name must be specified as "deployment"')
  1765|     if "name" not in kwargs:
  1766|         raise SaltCloudSystemExit('An endpoint name must be specified as "name"')
  1767|     if "role" not in kwargs:
  1768|         raise SaltCloudSystemExit('An role name must be specified as "role"')
  1769|     if activity != "delete":
  1770|         if "port" not in kwargs:
  1771|             raise SaltCloudSystemExit('An endpoint port must be specified as "port"')
  1772|         if "protocol" not in kwargs:
  1773|             raise SaltCloudSystemExit(
  1774|                 'An endpoint protocol (tcp or udp) must be specified as "protocol"'
  1775|             )
  1776|         if "local_port" not in kwargs:
  1777|             kwargs["local_port"] = kwargs["port"]
  1778|         if "enable_direct_server_return" not in kwargs:
  1779|             kwargs["enable_direct_server_return"] = False
  1780|         kwargs["enable_direct_server_return"] = str(
  1781|             kwargs["enable_direct_server_return"]
  1782|         ).lower()
  1783|         if "timeout_for_tcp_idle_connection" not in kwargs:
  1784|             kwargs["timeout_for_tcp_idle_connection"] = 4
  1785|     old_endpoints = list_input_endpoints(kwargs, call="function")
  1786|     endpoints_xml = ""
  1787|     endpoint_xml = """
  1788|         <InputEndpoint>
  1789|           <LocalPort>{local_port}</LocalPort>
  1790|           <Name>{name}</Name>
  1791|           <Port>{port}</Port>
  1792|           <Protocol>{protocol}</Protocol>
  1793|           <EnableDirectServerReturn>{enable_direct_server_return}</EnableDirectServerReturn>
  1794|           <IdleTimeoutInMinutes>{timeout_for_tcp_idle_connection}</IdleTimeoutInMinutes>
  1795|         </InputEndpoint>"""
  1796|     if activity == "add":
  1797|         old_endpoints[kwargs["name"]] = kwargs
  1798|         old_endpoints[kwargs["name"]]["Name"] = kwargs["name"]
  1799|     for endpoint in old_endpoints:
  1800|         if old_endpoints[endpoint]["Name"] == kwargs["name"]:
  1801|             if activity != "delete":
  1802|                 this_endpoint_xml = endpoint_xml.format(**kwargs)
  1803|                 endpoints_xml += this_endpoint_xml
  1804|         else:
  1805|             this_endpoint_xml = endpoint_xml.format(
  1806|                 local_port=old_endpoints[endpoint]["LocalPort"],
  1807|                 name=old_endpoints[endpoint]["Name"],
  1808|                 port=old_endpoints[endpoint]["Port"],
  1809|                 protocol=old_endpoints[endpoint]["Protocol"],
  1810|                 enable_direct_server_return=old_endpoints[endpoint][
  1811|                     "EnableDirectServerReturn"
  1812|                 ],
  1813|                 timeout_for_tcp_idle_connection=old_endpoints[endpoint].get(
  1814|                     "IdleTimeoutInMinutes", 4
  1815|                 ),
  1816|             )
  1817|             endpoints_xml += this_endpoint_xml
  1818|     request_xml = """<PersistentVMRole xmlns="http://schemas.microsoft.com/windowsazure"
  1819| xmlns:i="http://www.w3.org/2001/XMLSchema-instance">
  1820|   <ConfigurationSets>
  1821|     <ConfigurationSet>
  1822|       <ConfigurationSetType>NetworkConfiguration</ConfigurationSetType>
  1823|       <InputEndpoints>{}
  1824|       </InputEndpoints>
  1825|     </ConfigurationSet>
  1826|   </ConfigurationSets>
  1827|   <OSVirtualHardDisk>
  1828|   </OSVirtualHardDisk>
  1829| </PersistentVMRole>""".format(
  1830|         endpoints_xml
  1831|     )
  1832|     path = "services/hostedservices/{}/deployments/{}/roles/{}".format(
  1833|         kwargs["service"],
  1834|         kwargs["deployment"],
  1835|         kwargs["role"],
  1836|     )
  1837|     query(
  1838|         path=path,
  1839|         method="PUT",
  1840|         header_dict={"Content-Type": "application/xml"},
  1841|         data=request_xml,
  1842|         decode=False,
  1843|     )
  1844|     return True
  1845| @_deprecation_message
  1846| def add_input_endpoint(kwargs=None, conn=None, call=None):
  1847|     """
  1848|     .. versionadded:: 2015.8.0
  1849|     Add an input endpoint to the deployment. Please note that
  1850|     there may be a delay before the changes show up.
  1851|     CLI Example:
  1852|     .. code-block:: bash
  1853|         salt-cloud -f add_input_endpoint my-azure service=myservice \\
  1854|             deployment=mydeployment role=myrole name=HTTP local_port=80 \\
  1855|             port=80 protocol=tcp enable_direct_server_return=False \\
  1856|             timeout_for_tcp_idle_connection=4
  1857|     """
  1858|     return update_input_endpoint(
  1859|         kwargs=kwargs,
  1860|         conn=conn,
  1861|         call="function",
  1862|         activity="add",
  1863|     )
  1864| @_deprecation_message
  1865| def delete_input_endpoint(kwargs=None, conn=None, call=None):
  1866|     """
  1867|     .. versionadded:: 2015.8.0
  1868|     Delete an input endpoint from the deployment. Please note that
  1869|     there may be a delay before the changes show up.
  1870|     CLI Example:
  1871|     .. code-block:: bash
  1872|         salt-cloud -f delete_input_endpoint my-azure service=myservice \\
  1873|             deployment=mydeployment role=myrole name=HTTP
  1874|     """
  1875|     return update_input_endpoint(
  1876|         kwargs=kwargs,
  1877|         conn=conn,
  1878|         call="function",
  1879|         activity="delete",
  1880|     )
  1881| @_deprecation_message
  1882| def show_deployment(kwargs=None, conn=None, call=None):
  1883|     """
  1884|     .. versionadded:: 2015.8.0
  1885|     Return information about a deployment
  1886|     CLI Example:
  1887|     .. code-block:: bash
  1888|         salt-cloud -f show_deployment my-azure name=my_deployment
  1889|     """
  1890|     if call != "function":
  1891|         raise SaltCloudSystemExit(
  1892|             "The get_deployment function must be called with -f or --function."
  1893|         )
  1894|     if not conn:
  1895|         conn = get_conn()
  1896|     if kwargs is None:
  1897|         kwargs = {}
  1898|     if "service_name" not in kwargs:
  1899|         raise SaltCloudSystemExit('A service name must be specified as "service_name"')
  1900|     if "deployment_name" not in kwargs:
  1901|         raise SaltCloudSystemExit(
  1902|             'A deployment name must be specified as "deployment_name"'
  1903|         )
  1904|     data = conn.get_deployment_by_name(
  1905|         service_name=kwargs["service_name"],
  1906|         deployment_name=kwargs["deployment_name"],
  1907|     )
  1908|     return object_to_dict(data)
  1909| get_deployment = show_deployment
  1910| @_deprecation_message
  1911| def list_affinity_groups(kwargs=None, conn=None, call=None):
  1912|     """
  1913|     .. versionadded:: 2015.8.0
  1914|     List input endpoints associated with the deployment
  1915|     CLI Example:
  1916|     .. code-block:: bash
  1917|         salt-cloud -f list_affinity_groups my-azure
  1918|     """
  1919|     if call != "function":
  1920|         raise SaltCloudSystemExit(
  1921|             "The list_affinity_groups function must be called with -f or --function."
  1922|         )
  1923|     if not conn:
  1924|         conn = get_conn()
  1925|     data = conn.list_affinity_groups()
  1926|     ret = {}
  1927|     for item in data.affinity_groups:
  1928|         ret[item.name] = object_to_dict(item)
  1929|     return ret
  1930| @_deprecation_message
  1931| def show_affinity_group(kwargs=None, conn=None, call=None):
  1932|     """
  1933|     .. versionadded:: 2015.8.0
  1934|     Show an affinity group associated with the account
  1935|     CLI Example:
  1936|     .. code-block:: bash
  1937|         salt-cloud -f show_affinity_group my-azure service=myservice \\
  1938|             deployment=mydeployment name=SSH
  1939|     """
  1940|     if call != "function":
  1941|         raise SaltCloudSystemExit(
  1942|             "The show_affinity_group function must be called with -f or --function."
  1943|         )
  1944|     if not conn:
  1945|         conn = get_conn()
  1946|     if kwargs is None:
  1947|         kwargs = {}
  1948|     if "name" not in kwargs:
  1949|         raise SaltCloudSystemExit('An affinity group name must be specified as "name"')
  1950|     data = conn.get_affinity_group_properties(affinity_group_name=kwargs["name"])
  1951|     return object_to_dict(data)
  1952| get_affinity_group = show_affinity_group
  1953| @_deprecation_message
  1954| def create_affinity_group(kwargs=None, conn=None, call=None):
  1955|     """
  1956|     .. versionadded:: 2015.8.0
  1957|     Create a new affinity group
  1958|     CLI Example:
  1959|     .. code-block:: bash
  1960|         salt-cloud -f create_affinity_group my-azure name=my_affinity_group
  1961|     """
  1962|     if call != "function":
  1963|         raise SaltCloudSystemExit(
  1964|             "The create_affinity_group function must be called with -f or --function."
  1965|         )
  1966|     if not conn:
  1967|         conn = get_conn()
  1968|     if kwargs is None:
  1969|         kwargs = {}
  1970|     if "name" not in kwargs:
  1971|         raise SaltCloudSystemExit('A name must be specified as "name"')
  1972|     if "label" not in kwargs:
  1973|         raise SaltCloudSystemExit('A label must be specified as "label"')
  1974|     if "location" not in kwargs:
  1975|         raise SaltCloudSystemExit('A location must be specified as "location"')
  1976|     try:
  1977|         conn.create_affinity_group(
  1978|             kwargs["name"],
  1979|             kwargs["label"],
  1980|             kwargs["location"],
  1981|             kwargs.get("description", None),
  1982|         )
  1983|         return {"Success": "The affinity group was successfully created"}
  1984|     except AzureConflictHttpError:
  1985|         raise SaltCloudSystemExit(
  1986|             "There was a conflict. This usually means that the affinity group already"
  1987|             " exists."
  1988|         )
  1989| @_deprecation_message
  1990| def update_affinity_group(kwargs=None, conn=None, call=None):
  1991|     """
  1992|     .. versionadded:: 2015.8.0
  1993|     Update an affinity group's properties
  1994|     CLI Example:
  1995|     .. code-block:: bash
  1996|         salt-cloud -f update_affinity_group my-azure name=my_group label=my_group
  1997|     """
  1998|     if call != "function":
  1999|         raise SaltCloudSystemExit(
  2000|             "The update_affinity_group function must be called with -f or --function."
  2001|         )
  2002|     if not conn:
  2003|         conn = get_conn()
  2004|     if kwargs is None:
  2005|         kwargs = {}
  2006|     if "name" not in kwargs:
  2007|         raise SaltCloudSystemExit('A name must be specified as "name"')
  2008|     if "label" not in kwargs:
  2009|         raise SaltCloudSystemExit('A label must be specified as "label"')
  2010|     conn.update_affinity_group(
  2011|         affinity_group_name=kwargs["name"],
  2012|         label=kwargs["label"],
  2013|         description=kwargs.get("description", None),
  2014|     )
  2015|     return show_affinity_group(kwargs={"name": kwargs["name"]}, call="function")
  2016| @_deprecation_message
  2017| def delete_affinity_group(kwargs=None, conn=None, call=None):
  2018|     """
  2019|     .. versionadded:: 2015.8.0
  2020|     Delete a specific affinity group associated with the account
  2021|     CLI Examples:
  2022|     .. code-block:: bash
  2023|         salt-cloud -f delete_affinity_group my-azure name=my_affinity_group
  2024|     """
  2025|     if call != "function":
  2026|         raise SaltCloudSystemExit(
  2027|             "The delete_affinity_group function must be called with -f or --function."
  2028|         )
  2029|     if kwargs is None:
  2030|         kwargs = {}
  2031|     if "name" not in kwargs:
  2032|         raise SaltCloudSystemExit('A name must be specified as "name"')
  2033|     if not conn:
  2034|         conn = get_conn()
  2035|     try:
  2036|         conn.delete_affinity_group(kwargs["name"])
  2037|         return {"Success": "The affinity group was successfully deleted"}
  2038|     except AzureMissingResourceHttpError as exc:
  2039|         raise SaltCloudSystemExit("{}: {}".format(kwargs["name"], exc.message))
  2040| @_deprecation_message
  2041| def get_storage_conn(storage_account=None, storage_key=None, conn_kwargs=None):
  2042|     """
  2043|     .. versionadded:: 2015.8.0
  2044|     Return a storage_conn object for the storage account
  2045|     """
  2046|     if conn_kwargs is None:
  2047|         conn_kwargs = {}
  2048|     if not storage_account:
  2049|         storage_account = config.get_cloud_config_value(
  2050|             "storage_account",
  2051|             get_configured_provider(),
  2052|             __opts__,
  2053|             search_global=False,
  2054|             default=conn_kwargs.get("storage_account", None),
  2055|         )
  2056|     if not storage_key:
  2057|         storage_key = config.get_cloud_config_value(
  2058|             "storage_key",
  2059|             get_configured_provider(),
  2060|             __opts__,
  2061|             search_global=False,
  2062|             default=conn_kwargs.get("storage_key", None),
  2063|         )
  2064|     return azure.storage.BlobService(storage_account, storage_key)
  2065| @_deprecation_message
  2066| def make_blob_url(kwargs=None, storage_conn=None, call=None):
  2067|     """
  2068|     .. versionadded:: 2015.8.0
  2069|     Creates the URL to access a blob
  2070|     CLI Example:
  2071|     .. code-block:: bash
  2072|         salt-cloud -f make_blob_url my-azure container=mycontainer blob=myblob
  2073|     container:
  2074|         Name of the container.
  2075|     blob:
  2076|         Name of the blob.
  2077|     account:
  2078|         Name of the storage account. If not specified, derives the host base
  2079|         from the provider configuration.
  2080|     protocol:
  2081|         Protocol to use: 'http' or 'https'. If not specified, derives the host
  2082|         base from the provider configuration.
  2083|     host_base:
  2084|         Live host base URL.  If not specified, derives the host base from the
  2085|         provider configuration.
  2086|     """
  2087|     if call != "function":
  2088|         raise SaltCloudSystemExit(
  2089|             "The make_blob_url function must be called with -f or --function."
  2090|         )
  2091|     if kwargs is None:
  2092|         kwargs = {}
  2093|     if "container" not in kwargs:
  2094|         raise SaltCloudSystemExit('A container name must be specified as "container"')
  2095|     if "blob" not in kwargs:
  2096|         raise SaltCloudSystemExit('A blob name must be specified as "blob"')
  2097|     if not storage_conn:
  2098|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2099|     data = storage_conn.make_blob_url(
  2100|         kwargs["container"],
  2101|         kwargs["blob"],
  2102|         kwargs.get("account", None),
  2103|         kwargs.get("protocol", None),
  2104|         kwargs.get("host_base", None),
  2105|     )
  2106|     ret = {}
  2107|     for item in data.containers:
  2108|         ret[item.name] = object_to_dict(item)
  2109|     return ret
  2110| @_deprecation_message
  2111| def list_storage_containers(kwargs=None, storage_conn=None, call=None):
  2112|     """
  2113|     .. versionadded:: 2015.8.0
  2114|     List containers associated with the storage account
  2115|     CLI Example:
  2116|     .. code-block:: bash
  2117|         salt-cloud -f list_storage_containers my-azure
  2118|     """
  2119|     if call != "function":
  2120|         raise SaltCloudSystemExit(
  2121|             "The list_storage_containers function must be called with -f or --function."
  2122|         )
  2123|     if not storage_conn:
  2124|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2125|     data = storage_conn.list_containers()
  2126|     ret = {}
  2127|     for item in data.containers:
  2128|         ret[item.name] = object_to_dict(item)
  2129|     return ret
  2130| @_deprecation_message
  2131| def create_storage_container(kwargs=None, storage_conn=None, call=None):
  2132|     """
  2133|     .. versionadded:: 2015.8.0
  2134|     Create a storage container
  2135|     CLI Example:
  2136|     .. code-block:: bash
  2137|         salt-cloud -f create_storage_container my-azure name=mycontainer
  2138|     name:
  2139|         Name of container to create.
  2140|     meta_name_values:
  2141|         Optional. A dict with name_value pairs to associate with the
  2142|         container as metadata. Example:{'Category':'test'}
  2143|     blob_public_access:
  2144|         Optional. Possible values include: container, blob
  2145|     fail_on_exist:
  2146|         Specify whether to throw an exception when the container exists.
  2147|     """
  2148|     if call != "function":
  2149|         raise SaltCloudSystemExit(
  2150|             "The create_storage_container function must be called with -f or"
  2151|             " --function."
  2152|         )
  2153|     if not storage_conn:
  2154|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2155|     try:
  2156|         storage_conn.create_container(
  2157|             container_name=kwargs["name"],
  2158|             x_ms_meta_name_values=kwargs.get("meta_name_values", None),
  2159|             x_ms_blob_public_access=kwargs.get("blob_public_access", None),
  2160|             fail_on_exist=kwargs.get("fail_on_exist", False),
  2161|         )
  2162|         return {"Success": "The storage container was successfully created"}
  2163|     except AzureConflictHttpError:
  2164|         raise SaltCloudSystemExit(
  2165|             "There was a conflict. This usually means that the storage container"
  2166|             " already exists."
  2167|         )
  2168| @_deprecation_message
  2169| def show_storage_container(kwargs=None, storage_conn=None, call=None):
  2170|     """
  2171|     .. versionadded:: 2015.8.0
  2172|     Show a container associated with the storage account
  2173|     CLI Example:
  2174|     .. code-block:: bash
  2175|         salt-cloud -f show_storage_container my-azure name=myservice
  2176|     name:
  2177|         Name of container to show.
  2178|     """
  2179|     if call != "function":
  2180|         raise SaltCloudSystemExit(
  2181|             "The show_storage_container function must be called with -f or --function."
  2182|         )
  2183|     if kwargs is None:
  2184|         kwargs = {}
  2185|     if "name" not in kwargs:
  2186|         raise SaltCloudSystemExit(
  2187|             'An storage container name must be specified as "name"'
  2188|         )
  2189|     if not storage_conn:
  2190|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2191|     data = storage_conn.get_container_properties(
  2192|         container_name=kwargs["name"],
  2193|         x_ms_lease_id=kwargs.get("lease_id", None),
  2194|     )
  2195|     return data
  2196| get_storage_container = show_storage_container
  2197| @_deprecation_message
  2198| def show_storage_container_metadata(kwargs=None, storage_conn=None, call=None):
  2199|     """
  2200|     .. versionadded:: 2015.8.0
  2201|     Show a storage container's metadata
  2202|     CLI Example:
  2203|     .. code-block:: bash
  2204|         salt-cloud -f show_storage_container_metadata my-azure name=myservice
  2205|     name:
  2206|         Name of container to show.
  2207|     lease_id:
  2208|         If specified, show_storage_container_metadata only succeeds if the
  2209|         container's lease is active and matches this ID.
  2210|     """
  2211|     if call != "function":
  2212|         raise SaltCloudSystemExit(
  2213|             "The show_storage_container function must be called with -f or --function."
  2214|         )
  2215|     if kwargs is None:
  2216|         kwargs = {}
  2217|     if "name" not in kwargs:
  2218|         raise SaltCloudSystemExit(
  2219|             'An storage container name must be specified as "name"'
  2220|         )
  2221|     if not storage_conn:
  2222|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2223|     data = storage_conn.get_container_metadata(
  2224|         container_name=kwargs["name"],
  2225|         x_ms_lease_id=kwargs.get("lease_id", None),
  2226|     )
  2227|     return data
  2228| get_storage_container_metadata = show_storage_container_metadata
  2229| @_deprecation_message
  2230| def set_storage_container_metadata(kwargs=None, storage_conn=None, call=None):
  2231|     """
  2232|     .. versionadded:: 2015.8.0
  2233|     Set a storage container's metadata
  2234|     CLI Example:
  2235|     .. code-block:: bash
  2236|         salt-cloud -f set_storage_container my-azure name=mycontainer \\
  2237|             x_ms_meta_name_values='{"my_name": "my_value"}'
  2238|     name:
  2239|         Name of existing container.
  2240|     meta_name_values:
  2241|         A dict containing name, value for metadata.
  2242|         Example: {'category':'test'}
  2243|     lease_id:
  2244|         If specified, set_storage_container_metadata only succeeds if the
  2245|         container's lease is active and matches this ID.
  2246|     """
  2247|     if call != "function":
  2248|         raise SaltCloudSystemExit(
  2249|             "The create_storage_container function must be called with -f or"
  2250|             " --function."
  2251|         )
  2252|     if kwargs is None:
  2253|         kwargs = {}
  2254|     if "name" not in kwargs:
  2255|         raise SaltCloudSystemExit(
  2256|             'An storage container name must be specified as "name"'
  2257|         )
  2258|     x_ms_meta_name_values = salt.utils.yaml.safe_load(
  2259|         kwargs.get("meta_name_values", "")
  2260|     )
  2261|     if not storage_conn:
  2262|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2263|     try:
  2264|         storage_conn.set_container_metadata(
  2265|             container_name=kwargs["name"],
  2266|             x_ms_meta_name_values=x_ms_meta_name_values,
  2267|             x_ms_lease_id=kwargs.get("lease_id", None),
  2268|         )
  2269|         return {"Success": "The storage container was successfully updated"}
  2270|     except AzureConflictHttpError:
  2271|         raise SaltCloudSystemExit("There was a conflict.")
  2272| @_deprecation_message
  2273| def show_storage_container_acl(kwargs=None, storage_conn=None, call=None):
  2274|     """
  2275|     .. versionadded:: 2015.8.0
  2276|     Show a storage container's acl
  2277|     CLI Example:
  2278|     .. code-block:: bash
  2279|         salt-cloud -f show_storage_container_acl my-azure name=myservice
  2280|     name:
  2281|         Name of existing container.
  2282|     lease_id:
  2283|         If specified, show_storage_container_acl only succeeds if the
  2284|         container's lease is active and matches this ID.
  2285|     """
  2286|     if call != "function":
  2287|         raise SaltCloudSystemExit(
  2288|             "The show_storage_container function must be called with -f or --function."
  2289|         )
  2290|     if kwargs is None:
  2291|         kwargs = {}
  2292|     if "name" not in kwargs:
  2293|         raise SaltCloudSystemExit(
  2294|             'An storage container name must be specified as "name"'
  2295|         )
  2296|     if not storage_conn:
  2297|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2298|     data = storage_conn.get_container_acl(
  2299|         container_name=kwargs["name"],
  2300|         x_ms_lease_id=kwargs.get("lease_id", None),
  2301|     )
  2302|     return data
  2303| get_storage_container_acl = show_storage_container_acl
  2304| @_deprecation_message
  2305| def set_storage_container_acl(kwargs=None, storage_conn=None, call=None):
  2306|     """
  2307|     .. versionadded:: 2015.8.0
  2308|     Set a storage container's acl
  2309|     CLI Example:
  2310|     .. code-block:: bash
  2311|         salt-cloud -f set_storage_container my-azure name=mycontainer
  2312|     name:
  2313|         Name of existing container.
  2314|     signed_identifiers:
  2315|         SignedIdentifers instance
  2316|     blob_public_access:
  2317|         Optional. Possible values include: container, blob
  2318|     lease_id:
  2319|         If specified, set_storage_container_acl only succeeds if the
  2320|         container's lease is active and matches this ID.
  2321|     """
  2322|     if call != "function":
  2323|         raise SaltCloudSystemExit(
  2324|             "The create_storage_container function must be called with -f or"
  2325|             " --function."
  2326|         )
  2327|     if not storage_conn:
  2328|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2329|     try:
  2330|         data = storage_conn.set_container_acl(
  2331|             container_name=kwargs["name"],
  2332|             signed_identifiers=kwargs.get("signed_identifiers", None),
  2333|             x_ms_blob_public_access=kwargs.get("blob_public_access", None),
  2334|             x_ms_lease_id=kwargs.get("lease_id", None),
  2335|         )
  2336|         return {"Success": "The storage container was successfully updated"}
  2337|     except AzureConflictHttpError:
  2338|         raise SaltCloudSystemExit("There was a conflict.")
  2339| @_deprecation_message
  2340| def delete_storage_container(kwargs=None, storage_conn=None, call=None):
  2341|     """
  2342|     .. versionadded:: 2015.8.0
  2343|     Delete a container associated with the storage account
  2344|     CLI Example:
  2345|     .. code-block:: bash
  2346|         salt-cloud -f delete_storage_container my-azure name=mycontainer
  2347|     name:
  2348|         Name of container to create.
  2349|     fail_not_exist:
  2350|         Specify whether to throw an exception when the container exists.
  2351|     lease_id:
  2352|         If specified, delete_storage_container only succeeds if the
  2353|         container's lease is active and matches this ID.
  2354|     """
  2355|     if call != "function":
  2356|         raise SaltCloudSystemExit(
  2357|             "The delete_storage_container function must be called with -f or"
  2358|             " --function."
  2359|         )
  2360|     if kwargs is None:
  2361|         kwargs = {}
  2362|     if "name" not in kwargs:
  2363|         raise SaltCloudSystemExit(
  2364|             'An storage container name must be specified as "name"'
  2365|         )
  2366|     if not storage_conn:
  2367|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2368|     data = storage_conn.delete_container(
  2369|         container_name=kwargs["name"],
  2370|         fail_not_exist=kwargs.get("fail_not_exist", None),
  2371|         x_ms_lease_id=kwargs.get("lease_id", None),
  2372|     )
  2373|     return data
  2374| @_deprecation_message
  2375| def lease_storage_container(kwargs=None, storage_conn=None, call=None):
  2376|     """
  2377|     .. versionadded:: 2015.8.0
  2378|     Lease a container associated with the storage account
  2379|     CLI Example:
  2380|     .. code-block:: bash
  2381|         salt-cloud -f lease_storage_container my-azure name=mycontainer
  2382|     name:
  2383|         Name of container to create.
  2384|     lease_action:
  2385|         Required. Possible values: acquire|renew|release|break|change
  2386|     lease_id:
  2387|         Required if the container has an active lease.
  2388|     lease_duration:
  2389|         Specifies the duration of the lease, in seconds, or negative one
  2390|         (-1) for a lease that never expires. A non-infinite lease can be
  2391|         between 15 and 60 seconds. A lease duration cannot be changed
  2392|         using renew or change. For backwards compatibility, the default is
  2393|         60, and the value is only used on an acquire operation.
  2394|     lease_break_period:
  2395|         Optional. For a break operation, this is the proposed duration of
  2396|         seconds that the lease should continue before it is broken, between
  2397|         0 and 60 seconds. This break period is only used if it is shorter
  2398|         than the time remaining on the lease. If longer, the time remaining
  2399|         on the lease is used. A new lease will not be available before the
  2400|         break period has expired, but the lease may be held for longer than
  2401|         the break period. If this header does not appear with a break
  2402|         operation, a fixed-duration lease breaks after the remaining lease
  2403|         period elapses, and an infinite lease breaks immediately.
  2404|     proposed_lease_id:
  2405|         Optional for acquire, required for change. Proposed lease ID, in a
  2406|         GUID string format.
  2407|     """
  2408|     if call != "function":
  2409|         raise SaltCloudSystemExit(
  2410|             "The lease_storage_container function must be called with -f or --function."
  2411|         )
  2412|     if kwargs is None:
  2413|         kwargs = {}
  2414|     if "name" not in kwargs:
  2415|         raise SaltCloudSystemExit(
  2416|             'An storage container name must be specified as "name"'
  2417|         )
  2418|     lease_actions = ("acquire", "renew", "release", "break", "change")
  2419|     if kwargs.get("lease_action", None) not in lease_actions:
  2420|         raise SaltCloudSystemExit(
  2421|             "A lease_action must be one of: {}".format(", ".join(lease_actions))
  2422|         )
  2423|     if kwargs["lease_action"] != "acquire" and "lease_id" not in kwargs:
  2424|         raise SaltCloudSystemExit(
  2425|             'A lease ID must be specified for the "{}" lease action '
  2426|             'as "lease_id"'.format(kwargs["lease_action"])
  2427|         )
  2428|     if not storage_conn:
  2429|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2430|     data = storage_conn.lease_container(
  2431|         container_name=kwargs["name"],
  2432|         x_ms_lease_action=kwargs["lease_action"],
  2433|         x_ms_lease_id=kwargs.get("lease_id", None),
  2434|         x_ms_lease_duration=kwargs.get("lease_duration", 60),
  2435|         x_ms_lease_break_period=kwargs.get("lease_break_period", None),
  2436|         x_ms_proposed_lease_id=kwargs.get("proposed_lease_id", None),
  2437|     )
  2438|     return data
  2439| @_deprecation_message
  2440| def list_blobs(kwargs=None, storage_conn=None, call=None):
  2441|     """
  2442|     .. versionadded:: 2015.8.0
  2443|     List blobs associated with the container
  2444|     CLI Example:
  2445|     .. code-block:: bash
  2446|         salt-cloud -f list_blobs my-azure container=mycontainer
  2447|     container:
  2448|         The name of the storage container
  2449|     prefix:
  2450|         Optional. Filters the results to return only blobs whose names
  2451|         begin with the specified prefix.
  2452|     marker:
  2453|         Optional. A string value that identifies the portion of the list
  2454|         to be returned with the next list operation. The operation returns
  2455|         a marker value within the response body if the list returned was
  2456|         not complete. The marker value may then be used in a subsequent
  2457|         call to request the next set of list items. The marker value is
  2458|         opaque to the client.
  2459|     maxresults:
  2460|         Optional. Specifies the maximum number of blobs to return,
  2461|         including all BlobPrefix elements. If the request does not specify
  2462|         maxresults or specifies a value greater than 5,000, the server will
  2463|         return up to 5,000 items. Setting maxresults to a value less than
  2464|         or equal to zero results in error response code 400 (Bad Request).
  2465|     include:
  2466|         Optional. Specifies one or more datasets to include in the
  2467|         response. To specify more than one of these options on the URI,
  2468|         you must separate each option with a comma. Valid values are:
  2469|         snapshots:
  2470|             Specifies that snapshots should be included in the
  2471|             enumeration. Snapshots are listed from oldest to newest in
  2472|             the response.
  2473|         metadata:
  2474|             Specifies that blob metadata be returned in the response.
  2475|         uncommittedblobs:
  2476|             Specifies that blobs for which blocks have been uploaded,
  2477|             but which have not been committed using Put Block List
  2478|             (REST API), be included in the response.
  2479|         copy:
  2480|             Version 2012-02-12 and newer. Specifies that metadata
  2481|             related to any current or previous Copy Blob operation
  2482|             should be included in the response.
  2483|     delimiter:
  2484|         Optional. When the request includes this parameter, the operation
  2485|         returns a BlobPrefix element in the response body that acts as a
  2486|         placeholder for all blobs whose names begin with the same
  2487|         substring up to the appearance of the delimiter character. The
  2488|         delimiter may be a single character or a string.
  2489|     """
  2490|     if call != "function":
  2491|         raise SaltCloudSystemExit(
  2492|             "The list_blobs function must be called with -f or --function."
  2493|         )
  2494|     if kwargs is None:
  2495|         kwargs = {}
  2496|     if "container" not in kwargs:
  2497|         raise SaltCloudSystemExit(
  2498|             'An storage container name must be specified as "container"'
  2499|         )
  2500|     if not storage_conn:
  2501|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2502|     return salt.utils.msazure.list_blobs(storage_conn=storage_conn, **kwargs)
  2503| @_deprecation_message
  2504| def show_blob_service_properties(kwargs=None, storage_conn=None, call=None):
  2505|     """
  2506|     .. versionadded:: 2015.8.0
  2507|     Show a blob's service properties
  2508|     CLI Example:
  2509|     .. code-block:: bash
  2510|         salt-cloud -f show_blob_service_properties my-azure
  2511|     """
  2512|     if call != "function":
  2513|         raise SaltCloudSystemExit(
  2514|             "The show_blob_service_properties function must be called with -f or"
  2515|             " --function."
  2516|         )
  2517|     if not storage_conn:
  2518|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2519|     data = storage_conn.get_blob_service_properties(
  2520|         timeout=kwargs.get("timeout", None),
  2521|     )
  2522|     return data
  2523| get_blob_service_properties = show_blob_service_properties
  2524| @_deprecation_message
  2525| def set_blob_service_properties(kwargs=None, storage_conn=None, call=None):
  2526|     """
  2527|     .. versionadded:: 2015.8.0
  2528|     Sets the properties of a storage account's Blob service, including
  2529|     Windows Azure Storage Analytics. You can also use this operation to
  2530|     set the default request version for all incoming requests that do not
  2531|     have a version specified.
  2532|     CLI Example:
  2533|     .. code-block:: bash
  2534|         salt-cloud -f set_blob_service_properties my-azure
  2535|     properties:
  2536|         a StorageServiceProperties object.
  2537|     timeout:
  2538|         Optional. The timeout parameter is expressed in seconds.
  2539|     """
  2540|     if call != "function":
  2541|         raise SaltCloudSystemExit(
  2542|             "The set_blob_service_properties function must be called with -f or"
  2543|             " --function."
  2544|         )
  2545|     if kwargs is None:
  2546|         kwargs = {}
  2547|     if "properties" not in kwargs:
  2548|         raise SaltCloudSystemExit(
  2549|             'The blob service properties name must be specified as "properties"'
  2550|         )
  2551|     if not storage_conn:
  2552|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2553|     data = storage_conn.get_blob_service_properties(
  2554|         storage_service_properties=kwargs["properties"],
  2555|         timeout=kwargs.get("timeout", None),
  2556|     )
  2557|     return data
  2558| @_deprecation_message
  2559| def show_blob_properties(kwargs=None, storage_conn=None, call=None):
  2560|     """
  2561|     .. versionadded:: 2015.8.0
  2562|     Returns all user-defined metadata, standard HTTP properties, and
  2563|     system properties for the blob.
  2564|     CLI Example:
  2565|     .. code-block:: bash
  2566|         salt-cloud -f show_blob_properties my-azure container=mycontainer blob=myblob
  2567|     container:
  2568|         Name of existing container.
  2569|     blob:
  2570|         Name of existing blob.
  2571|     lease_id:
  2572|         Required if the blob has an active lease.
  2573|     """
  2574|     if call != "function":
  2575|         raise SaltCloudSystemExit(
  2576|             "The show_blob_properties function must be called with -f or --function."
  2577|         )
  2578|     if kwargs is None:
  2579|         kwargs = {}
  2580|     if "container" not in kwargs:
  2581|         raise SaltCloudSystemExit('The container name must be specified as "container"')
  2582|     if "blob" not in kwargs:
  2583|         raise SaltCloudSystemExit('The blob name must be specified as "blob"')
  2584|     if not storage_conn:
  2585|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2586|     try:
  2587|         data = storage_conn.get_blob_properties(
  2588|             container_name=kwargs["container"],
  2589|             blob_name=kwargs["blob"],
  2590|             x_ms_lease_id=kwargs.get("lease_id", None),
  2591|         )
  2592|     except AzureMissingResourceHttpError:
  2593|         raise SaltCloudSystemExit("The specified blob does not exist.")
  2594|     return data
  2595| get_blob_properties = show_blob_properties
  2596| @_deprecation_message
  2597| def set_blob_properties(kwargs=None, storage_conn=None, call=None):
  2598|     """
  2599|     .. versionadded:: 2015.8.0
  2600|     Set a blob's properties
  2601|     CLI Example:
  2602|     .. code-block:: bash
  2603|         salt-cloud -f set_blob_properties my-azure
  2604|     container:
  2605|         Name of existing container.
  2606|     blob:
  2607|         Name of existing blob.
  2608|     blob_cache_control:
  2609|         Optional. Modifies the cache control string for the blob.
  2610|     blob_content_type:
  2611|         Optional. Sets the blob's content type.
  2612|     blob_content_md5:
  2613|         Optional. Sets the blob's MD5 hash.
  2614|     blob_content_encoding:
  2615|         Optional. Sets the blob's content encoding.
  2616|     blob_content_language:
  2617|         Optional. Sets the blob's content language.
  2618|     lease_id:
  2619|         Required if the blob has an active lease.
  2620|     blob_content_disposition:
  2621|         Optional. Sets the blob's Content-Disposition header.
  2622|         The Content-Disposition response header field conveys additional
  2623|         information about how to process the response payload, and also can
  2624|         be used to attach additional metadata. For example, if set to
  2625|         attachment, it indicates that the user-agent should not display the
  2626|         response, but instead show a Save As dialog with a filename other
  2627|         than the blob name specified.
  2628|     """
  2629|     if call != "function":
  2630|         raise SaltCloudSystemExit(
  2631|             "The set_blob_properties function must be called with -f or --function."
  2632|         )
  2633|     if kwargs is None:
  2634|         kwargs = {}
  2635|     if "container" not in kwargs:
  2636|         raise SaltCloudSystemExit(
  2637|             'The blob container name must be specified as "container"'
  2638|         )
  2639|     if "blob" not in kwargs:
  2640|         raise SaltCloudSystemExit('The blob name must be specified as "blob"')
  2641|     if not storage_conn:
  2642|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2643|     data = storage_conn.get_blob_properties(
  2644|         container_name=kwargs["container"],
  2645|         blob_name=kwargs["blob"],
  2646|         x_ms_blob_cache_control=kwargs.get("blob_cache_control", None),
  2647|         x_ms_blob_content_type=kwargs.get("blob_content_type", None),
  2648|         x_ms_blob_content_md5=kwargs.get("blob_content_md5", None),
  2649|         x_ms_blob_content_encoding=kwargs.get("blob_content_encoding", None),
  2650|         x_ms_blob_content_language=kwargs.get("blob_content_language", None),
  2651|         x_ms_lease_id=kwargs.get("lease_id", None),
  2652|         x_ms_blob_content_disposition=kwargs.get("blob_content_disposition", None),
  2653|     )
  2654|     return data
  2655| @_deprecation_message
  2656| def put_blob(kwargs=None, storage_conn=None, call=None):
  2657|     """
  2658|     .. versionadded:: 2015.8.0
  2659|     Upload a blob
  2660|     CLI Examples:
  2661|     .. code-block:: bash
  2662|         salt-cloud -f put_blob my-azure container=base name=top.sls blob_path=/srv/salt/top.sls
  2663|         salt-cloud -f put_blob my-azure container=base name=content.txt blob_content='Some content'
  2664|     container:
  2665|         Name of existing container.
  2666|     name:
  2667|         Name of existing blob.
  2668|     blob_path:
  2669|         The path on the local machine of the file to upload as a blob. Either
  2670|         this or blob_content must be specified.
  2671|     blob_content:
  2672|         The actual content to be uploaded as a blob. Either this or blob_path
  2673|         must me specified.
  2674|     cache_control:
  2675|         Optional. The Blob service stores this value but does not use or
  2676|         modify it.
  2677|     content_language:
  2678|         Optional. Specifies the natural languages used by this resource.
  2679|     content_md5:
  2680|         Optional. An MD5 hash of the blob content. This hash is used to
  2681|         verify the integrity of the blob during transport. When this header
  2682|         is specified, the storage service checks the hash that has arrived
  2683|         with the one that was sent. If the two hashes do not match, the
  2684|         operation will fail with error code 400 (Bad Request).
  2685|     blob_content_type:
  2686|         Optional. Set the blob's content type.
  2687|     blob_content_encoding:
  2688|         Optional. Set the blob's content encoding.
  2689|     blob_content_language:
  2690|         Optional. Set the blob's content language.
  2691|     blob_content_md5:
  2692|         Optional. Set the blob's MD5 hash.
  2693|     blob_cache_control:
  2694|         Optional. Sets the blob's cache control.
  2695|     meta_name_values:
  2696|         A dict containing name, value for metadata.
  2697|     lease_id:
  2698|         Required if the blob has an active lease.
  2699|     """
  2700|     if call != "function":
  2701|         raise SaltCloudSystemExit(
  2702|             "The put_blob function must be called with -f or --function."
  2703|         )
  2704|     if kwargs is None:
  2705|         kwargs = {}
  2706|     if "container" not in kwargs:
  2707|         raise SaltCloudSystemExit(
  2708|             'The blob container name must be specified as "container"'
  2709|         )
  2710|     if "name" not in kwargs:
  2711|         raise SaltCloudSystemExit('The blob name must be specified as "name"')
  2712|     if "blob_path" not in kwargs and "blob_content" not in kwargs:
  2713|         raise SaltCloudSystemExit(
  2714|             'Either a path to a file needs to be passed in as "blob_path" or '
  2715|             'the contents of a blob as "blob_content."'
  2716|         )
  2717|     if not storage_conn:
  2718|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2719|     return salt.utils.msazure.put_blob(storage_conn=storage_conn, **kwargs)
  2720| @_deprecation_message
  2721| def get_blob(kwargs=None, storage_conn=None, call=None):
  2722|     """
  2723|     .. versionadded:: 2015.8.0
  2724|     Download a blob
  2725|     CLI Example:
  2726|     .. code-block:: bash
  2727|         salt-cloud -f get_blob my-azure container=base name=top.sls local_path=/srv/salt/top.sls
  2728|         salt-cloud -f get_blob my-azure container=base name=content.txt return_content=True
  2729|     container:
  2730|         Name of existing container.
  2731|     name:
  2732|         Name of existing blob.
  2733|     local_path:
  2734|         The path on the local machine to download the blob to. Either this or
  2735|         return_content must be specified.
  2736|     return_content:
  2737|         Whether or not to return the content directly from the blob. If
  2738|         specified, must be True or False. Either this or the local_path must
  2739|         be specified.
  2740|     snapshot:
  2741|         Optional. The snapshot parameter is an opaque DateTime value that,
  2742|         when present, specifies the blob snapshot to retrieve.
  2743|     lease_id:
  2744|         Required if the blob has an active lease.
  2745|     progress_callback:
  2746|         callback for progress with signature function(current, total) where
  2747|         current is the number of bytes transferred so far, and total is the
  2748|         size of the blob.
  2749|     max_connections:
  2750|         Maximum number of parallel connections to use when the blob size
  2751|         exceeds 64MB.
  2752|         Set to 1 to download the blob chunks sequentially.
  2753|         Set to 2 or more to download the blob chunks in parallel. This uses
  2754|         more system resources but will download faster.
  2755|     max_retries:
  2756|         Number of times to retry download of blob chunk if an error occurs.
  2757|     retry_wait:
  2758|         Sleep time in secs between retries.
  2759|     """
  2760|     if call != "function":
  2761|         raise SaltCloudSystemExit(
  2762|             "The get_blob function must be called with -f or --function."
  2763|         )
  2764|     if kwargs is None:
  2765|         kwargs = {}
  2766|     if "container" not in kwargs:
  2767|         raise SaltCloudSystemExit(
  2768|             'The blob container name must be specified as "container"'
  2769|         )
  2770|     if "name" not in kwargs:
  2771|         raise SaltCloudSystemExit('The blob name must be specified as "name"')
  2772|     if "local_path" not in kwargs and "return_content" not in kwargs:
  2773|         raise SaltCloudSystemExit(
  2774|             'Either a local path needs to be passed in as "local_path" or '
  2775|             '"return_content" to return the blob contents directly'
  2776|         )
  2777|     if not storage_conn:
  2778|         storage_conn = get_storage_conn(conn_kwargs=kwargs)
  2779|     return salt.utils.msazure.get_blob(storage_conn=storage_conn, **kwargs)
  2780| @_deprecation_message
  2781| def query(path, method="GET", data=None, params=None, header_dict=None, decode=True):
  2782|     """
  2783|     Perform a query directly against the Azure REST API
  2784|     """
  2785|     certificate_path = config.get_cloud_config_value(
  2786|         "certificate_path", get_configured_provider(), __opts__, search_global=False
  2787|     )
  2788|     subscription_id = salt.utils.stringutils.to_str(
  2789|         config.get_cloud_config_value(
  2790|             "subscription_id", get_configured_provider(), __opts__, search_global=False
  2791|         )
  2792|     )
  2793|     management_host = config.get_cloud_config_value(
  2794|         "management_host",
  2795|         get_configured_provider(),
  2796|         __opts__,
  2797|         search_global=False,
  2798|         default="management.core.windows.net",
  2799|     )
  2800|     backend = config.get_cloud_config_value(
  2801|         "backend", get_configured_provider(), __opts__, search_global=False
  2802|     )
  2803|     url = "https://{management_host}/{subscription_id}/{path}".format(
  2804|         management_host=management_host,
  2805|         subscription_id=subscription_id,
  2806|         path=path,
  2807|     )
  2808|     if header_dict is None:
  2809|         header_dict = {}
  2810|     header_dict["x-ms-version"] = "2014-06-01"
  2811|     result = salt.utils.http.query(
  2812|         url,
  2813|         method=method,
  2814|         params=params,
  2815|         data=data,
  2816|         header_dict=header_dict,
  2817|         port=443,
  2818|         text=True,
  2819|         cert=certificate_path,
  2820|         backend=backend,
  2821|         decode=decode,
  2822|         decode_type="xml",
  2823|     )
  2824|     if "dict" in result:
  2825|         return result["dict"]
  2826|     return


# ====================================================================
# FILE: salt/cloud/clouds/proxmox.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 1-66 ---
     1| """
     2| Proxmox Cloud Module
     3| ======================
     4| .. versionadded:: 2014.7.0
     5| The Proxmox cloud module is used to control access to cloud providers using
     6| the Proxmox system (KVM / OpenVZ / LXC).
     7| Set up the cloud configuration at ``/etc/salt/cloud.providers`` or
     8|  ``/etc/salt/cloud.providers.d/proxmox.conf``:
     9| .. code-block:: yaml
    10|     my-proxmox-config:
    11|       user: myuser@pam or myuser@pve
    12|       password: mypassword
    13|       url: hypervisor.domain.tld
    14|       port: 8006
    15|       driver: proxmox
    16|       verify_ssl: True
    17| :maintainer: Frank Klaassen <frank@cloudright.nl>
    18| :depends: requests >= 2.2.1
    19| :depends: IPy >= 0.81
    20| """
    21| import logging
    22| import pprint
    23| import re
    24| import socket
    25| import time
    26| import urllib
    27| import salt.config as config
    28| import salt.utils.cloud
    29| import salt.utils.json
    30| from salt.exceptions import (
    31|     SaltCloudExecutionFailure,
    32|     SaltCloudExecutionTimeout,
    33|     SaltCloudSystemExit,
    34| )
    35| try:
    36|     import requests
    37|     HAS_REQUESTS = True
    38| except ImportError:
    39|     HAS_REQUESTS = False
    40| try:
    41|     from IPy import IP
    42|     HAS_IPY = True
    43| except ImportError:
    44|     HAS_IPY = False
    45| log = logging.getLogger(__name__)
    46| __virtualname__ = "proxmox"
    47| def __virtual__():
    48|     """
    49|     Check for PROXMOX configurations
    50|     """
    51|     if get_configured_provider() is False:
    52|         return False
    53|     if get_dependencies() is False:
    54|         return False
    55|     return __virtualname__
    56| def _get_active_provider_name():
    57|     try:
    58|         return __active_provider_name__.value()
    59|     except AttributeError:
    60|         return __active_provider_name__
    61| def get_configured_provider():
    62|     """
    63|     Return the first configured instance.
    64|     """
    65|     return config.is_provider_configured(
    66|         __opts__, _get_active_provider_name() or __virtualname__, ("user",)

# --- HUNK 2: Lines 88-182 ---
    88|     port = config.get_cloud_config_value(
    89|         "port", get_configured_provider(), __opts__, default=8006, search_global=False
    90|     )
    91|     username = (
    92|         config.get_cloud_config_value(
    93|             "user", get_configured_provider(), __opts__, search_global=False
    94|         ),
    95|     )
    96|     passwd = config.get_cloud_config_value(
    97|         "password", get_configured_provider(), __opts__, search_global=False
    98|     )
    99|     verify_ssl = config.get_cloud_config_value(
   100|         "verify_ssl",
   101|         get_configured_provider(),
   102|         __opts__,
   103|         default=True,
   104|         search_global=False,
   105|     )
   106|     connect_data = {"username": username, "password": passwd}
   107|     full_url = f"https://{url}:{port}/api2/json/access/ticket"
   108|     response = requests.post(
   109|         full_url, verify=verify_ssl, data=connect_data, timeout=120
   110|     )
   111|     response.raise_for_status()
   112|     returned_data = response.json()
   113|     ticket = {"PVEAuthCookie": returned_data["data"]["ticket"]}
   114|     csrf = str(returned_data["data"]["CSRFPreventionToken"])
   115| def query(conn_type, option, post_data=None):
   116|     """
   117|     Execute the HTTP request to the API
   118|     """
   119|     if ticket is None or csrf is None or url is None:
   120|         log.debug("Not authenticated yet, doing that now..")
   121|         _authenticate()
   122|     full_url = f"https://{url}:{port}/api2/json/{option}"
   123|     log.debug("%s: %s (%s)", conn_type, full_url, post_data)
   124|     httpheaders = {
   125|         "Accept": "application/json",
   126|         "Content-Type": "application/x-www-form-urlencoded",
   127|         "User-Agent": "salt-cloud-proxmox",
   128|     }
   129|     if conn_type == "post":
   130|         httpheaders["CSRFPreventionToken"] = csrf
   131|         response = requests.post(
   132|             full_url,
   133|             verify=verify_ssl,
   134|             data=post_data,
   135|             cookies=ticket,
   136|             headers=httpheaders,
   137|             timeout=120,
   138|         )
   139|     elif conn_type == "put":
   140|         httpheaders["CSRFPreventionToken"] = csrf
   141|         response = requests.put(
   142|             full_url,
   143|             verify=verify_ssl,
   144|             data=post_data,
   145|             cookies=ticket,
   146|             headers=httpheaders,
   147|             timeout=120,
   148|         )
   149|     elif conn_type == "delete":
   150|         httpheaders["CSRFPreventionToken"] = csrf
   151|         response = requests.delete(
   152|             full_url,
   153|             verify=verify_ssl,
   154|             data=post_data,
   155|             cookies=ticket,
   156|             headers=httpheaders,
   157|             timeout=120,
   158|         )
   159|     elif conn_type == "get":
   160|         response = requests.get(
   161|             full_url, verify=verify_ssl, cookies=ticket, timeout=120
   162|         )
   163|     try:
   164|         response.raise_for_status()
   165|     except requests.exceptions.RequestException:
   166|         log.error("Error in %s query to %s:\n%s", conn_type, full_url, response.text)
   167|         raise
   168|     try:
   169|         returned_data = response.json()
   170|         if "data" not in returned_data:
   171|             raise SaltCloudExecutionFailure
   172|         return returned_data["data"]
   173|     except Exception:  # pylint: disable=broad-except
   174|         log.error("Error in trying to process JSON")
   175|         log.error(response)
   176| def _get_vm_by_name(name, allDetails=False):
   177|     """
   178|     Since Proxmox works based op id's rather than names as identifiers this
   179|     requires some filtering to retrieve the required information.
   180|     """
   181|     vms = get_resources_vms(includeConfig=allDetails)
   182|     if name in vms:

# --- HUNK 3: Lines 632-672 ---
   632|     for interface in interfaces["result"]:
   633|         if str(interface.get("hardware-address")) == "00:00:00:00:00:00":
   634|             continue
   635|         if "ip-addresses" not in interface:
   636|             continue
   637|         for if_addr in interface["ip-addresses"]:
   638|             ip_addr = if_addr.get("ip-address")
   639|             if ip_addr is not None:
   640|                 ips.append(str(ip_addr))
   641|     if len(ips) > 0:
   642|         return preferred_ip(vm_, ips)
   643|     raise SaltCloudExecutionFailure
   644| def _import_api():
   645|     """
   646|     Download https://<url>/pve-docs/api-viewer/apidoc.js
   647|     Extract content of pveapi var (json formatted)
   648|     Load this json content into global variable "api"
   649|     """
   650|     global api
   651|     full_url = f"https://{url}:{port}/pve-docs/api-viewer/apidoc.js"
   652|     returned_data = requests.get(full_url, verify=verify_ssl, timeout=120)
   653|     re_filter = re.compile(" (?:pveapi|apiSchema) = (.*)^;", re.DOTALL | re.MULTILINE)
   654|     api_json = re_filter.findall(returned_data.text)[0]
   655|     api = salt.utils.json.loads(api_json)
   656| def _get_properties(path="", method="GET", forced_params=None):
   657|     """
   658|     Return the parameter list from api for defined path and HTTP method
   659|     """
   660|     if api is None:
   661|         _import_api()
   662|     sub = api
   663|     path_levels = [level for level in path.split("/") if level != ""]
   664|     search_path = ""
   665|     props = []
   666|     parameters = set([] if forced_params is None else forced_params)
   667|     for elem in path_levels[:-1]:
   668|         search_path += "/" + elem
   669|         sub = next(item for item in sub if item["path"] == search_path)["children"]
   670|     search_path += "/" + path_levels[-1]
   671|     sub = next(item for item in sub if item["path"] == search_path)
   672|     try:


# ====================================================================
# FILE: salt/config/__init__.py
# Total hunks: 21
# ====================================================================
# --- HUNK 1: Lines 1-133 ---
     1| """
     2| All salt configuration loading and defaults should be in this module
     3| """
     4| import codecs
     5| import glob
     6| import logging
     7| import os
     8| import re
     9| import sys
    10| import time
    11| import types
    12| import urllib.parse
    13| from copy import deepcopy
    14| import salt.crypt
    15| import salt.defaults.exitcodes
    16| import salt.exceptions
    17| import salt.features
    18| import salt.syspaths
    19| import salt.utils.data
    20| import salt.utils.dictupdate
    21| import salt.utils.files
    22| import salt.utils.immutabletypes as immutabletypes
    23| import salt.utils.network
    24| import salt.utils.path
    25| import salt.utils.platform
    26| import salt.utils.stringutils
    27| import salt.utils.user
    28| import salt.utils.validate.path
    29| import salt.utils.versions
    30| import salt.utils.xdg
    31| import salt.utils.yaml
    32| from salt._logging import (
    33|     DFLT_LOG_DATEFMT,
    34|     DFLT_LOG_DATEFMT_LOGFILE,
    35|     DFLT_LOG_FMT_CONSOLE,
    36|     DFLT_LOG_FMT_JID,
    37|     DFLT_LOG_FMT_LOGFILE,
    38| )
    39| try:
    40|     import psutil
    41|     HAS_PSUTIL = True
    42| except ImportError:
    43|     HAS_PSUTIL = False
    44| log = logging.getLogger(__name__)
    45| _DFLT_REFSPECS = ["+refs/heads/*:refs/remotes/origin/*", "+refs/tags/*:refs/tags/*"]
    46| DEFAULT_INTERVAL = 60
    47| DEFAULT_HASH_TYPE = "sha256"
    48| if salt.utils.platform.is_windows():
    49|     _DFLT_IPC_MODE = "tcp"
    50|     _DFLT_FQDNS_GRAINS = False
    51|     _MASTER_TRIES = -1
    52|     _MASTER_USER = "SYSTEM"
    53| elif salt.utils.platform.is_proxy():
    54|     _DFLT_IPC_MODE = "ipc"
    55|     _DFLT_FQDNS_GRAINS = False
    56|     _MASTER_TRIES = 1
    57|     _MASTER_USER = salt.utils.user.get_user()
    58| elif salt.utils.platform.is_darwin():
    59|     _DFLT_IPC_MODE = "ipc"
    60|     _DFLT_FQDNS_GRAINS = False
    61|     _MASTER_TRIES = 1
    62|     _MASTER_USER = salt.utils.user.get_user()
    63| else:
    64|     _DFLT_IPC_MODE = "ipc"
    65|     _DFLT_FQDNS_GRAINS = False
    66|     _MASTER_TRIES = 1
    67|     _MASTER_USER = salt.utils.user.get_user()
    68| def _gather_buffer_space():
    69|     """
    70|     Gather some system data and then calculate
    71|     buffer space.
    72|     Result is in bytes.
    73|     """
    74|     if HAS_PSUTIL:
    75|         total_mem = psutil.virtual_memory().total
    76|     else:
    77|         import platform
    78|         import salt.grains.core
    79|         os_data = {"kernel": platform.system()}
    80|         grains = salt.grains.core._memdata(os_data)
    81|         total_mem = grains["mem_total"]
    82|     return max([total_mem * 0.05, 10 << 20])
    83| _DFLT_IPC_WBUFFER = int(_gather_buffer_space() * 0.5)
    84| _DFLT_IPC_RBUFFER = int(_gather_buffer_space() * 0.5)
    85| VALID_OPTS = immutabletypes.freeze(
    86|     {
    87|         "master": (str, list),
    88|         "master_port": (str, int),
    89|         "master_type": str,
    90|         "master_uri_format": str,
    91|         "source_interface_name": str,
    92|         "source_address": str,
    93|         "source_ret_port": (str, int),
    94|         "source_publish_port": (str, int),
    95|         "master_finger": str,
    96|         "master_shuffle": bool,
    97|         "master_alive_interval": int,
    98|         "master_failback": bool,
    99|         "master_failback_interval": int,
   100|         "master_sign_key_name": str,
   101|         "master_sign_pubkey": bool,
   102|         "verify_master_pubkey_sign": bool,
   103|         "always_verify_signature": bool,
   104|         "master_pubkey_signature": str,
   105|         "master_use_pubkey_signature": bool,
   106|         "master_stats": bool,
   107|         "master_stats_event_iter": int,
   108|         "syndic_finger": str,
   109|         "key_cache": str,
   110|         "user": str,
   111|         "root_dir": str,
   112|         "pki_dir": str,
   113|         "id": str,
   114|         "id_function": (dict, str),
   115|         "cachedir": str,
   116|         "append_minionid_config_dirs": list,
   117|         "cache_jobs": bool,
   118|         "conf_file": str,
   119|         "sock_dir": str,
   120|         "sock_pool_size": int,
   121|         "backup_mode": str,
   122|         "renderer": str,
   123|         "renderer_whitelist": list,
   124|         "renderer_blacklist": list,
   125|         "failhard": bool,
   126|         "autoload_dynamic_modules": bool,
   127|         "saltenv": (type(None), str),
   128|         "lock_saltenv": bool,
   129|         "pillarenv": (type(None), str),
   130|         "pillarenv_from_saltenv": bool,
   131|         "state_top": str,
   132|         "state_top_saltenv": (type(None), str),
   133|         "startup_states": str,

# --- HUNK 2: Lines 195-234 ---
   195|         "show_timeout": bool,
   196|         "show_jid": bool,
   197|         "unique_jid": bool,
   198|         "state_queue": (bool, int),
   199|         "state_verbose": bool,
   200|         "state_output": str,
   201|         "state_output_diff": bool,
   202|         "state_output_profile": bool,
   203|         "state_output_pct": bool,
   204|         "state_compress_ids": bool,
   205|         "state_auto_order": bool,
   206|         "state_events": bool,
   207|         "acceptance_wait_time": float,
   208|         "acceptance_wait_time_max": float,
   209|         "rejected_retry": bool,
   210|         "loop_interval": float,
   211|         "verify_env": bool,
   212|         "grains": dict,
   213|         "permissive_pki_access": bool,
   214|         "key_pass": (type(None), str),
   215|         "signing_key_pass": (type(None), str),
   216|         "default_include": str,
   217|         "update_url": (bool, str),
   218|         "update_restart_services": list,
   219|         "retry_dns": float,
   220|         "retry_dns_count": (type(None), int),
   221|         "resolve_dns_fallback": bool,
   222|         "recon_max": float,
   223|         "recon_default": float,
   224|         "recon_randomize": bool,
   225|         "return_retry_timer": int,
   226|         "return_retry_timer_max": int,
   227|         "return_retry_tries": int,
   228|         "event_return": (list, str),
   229|         "event_return_queue": int,
   230|         "event_return_queue_max_seconds": int,
   231|         "event_return_whitelist": list,
   232|         "event_return_blacklist": list,
   233|         "event_match_type": str,
   234|         "pidfile": str,

# --- HUNK 3: Lines 250-290 ---
   250|         "mworker_niceness": (type(None), int),
   251|         "mworker_queue_niceness": (type(None), int),
   252|         "event_return_niceness": (type(None), int),
   253|         "event_publisher_niceness": (type(None), int),
   254|         "reactor_niceness": (type(None), int),
   255|         "worker_threads": int,
   256|         "ret_port": int,
   257|         "keep_jobs": int,
   258|         "keep_jobs_seconds": int,
   259|         "archive_jobs": bool,
   260|         "master_roots": dict,
   261|         "add_proxymodule_to_opts": bool,
   262|         "proxy_merge_pillar_in_opts": bool,
   263|         "proxy_deep_merge_pillar_in_opts": bool,
   264|         "proxy_merge_pillar_in_opts_strategy": str,
   265|         "proxy_mines_pillar": bool,
   266|         "proxy_always_alive": bool,
   267|         "proxy_keep_alive": bool,
   268|         "proxy_keep_alive_interval": int,
   269|         "roots_update_interval": int,
   270|         "azurefs_update_interval": int,
   271|         "gitfs_update_interval": int,
   272|         "git_pillar_update_interval": int,
   273|         "hgfs_update_interval": int,
   274|         "minionfs_update_interval": int,
   275|         "s3fs_update_interval": int,
   276|         "svnfs_update_interval": int,
   277|         "git_pillar_ssl_verify": bool,
   278|         "git_pillar_global_lock": bool,
   279|         "git_pillar_user": str,
   280|         "git_pillar_password": str,
   281|         "git_pillar_insecure_auth": bool,
   282|         "git_pillar_privkey": str,
   283|         "git_pillar_pubkey": str,
   284|         "git_pillar_passphrase": str,
   285|         "git_pillar_refspecs": list,
   286|         "git_pillar_includes": bool,
   287|         "git_pillar_verify_config": bool,
   288|         "gitfs_remotes": list,
   289|         "gitfs_insecure_auth": bool,
   290|         "gitfs_privkey": str,

# --- HUNK 4: Lines 496-538 ---
   496|         "minion_data_cache_events": bool,
   497|         "enable_ssh_minions": bool,
   498|         "thoriumenv": (type(None), str),
   499|         "thorium_top": str,
   500|         "netapi_allow_raw_shell": bool,
   501|         "netapi_enable_clients": list,
   502|         "disabled_requisites": (str, list),
   503|         "global_state_conditions": (type(None), dict),
   504|         "features": dict,
   505|         "fips_mode": bool,
   506|         "detect_remote_minions": bool,
   507|         "remote_minions_port": int,
   508|         "pass_variable_prefix": str,
   509|         "pass_strict_fetch": bool,
   510|         "pass_gnupghome": str,
   511|         "pass_dir": str,
   512|         "maintenance_interval": int,
   513|         "fileserver_interval": int,
   514|         "request_channel_timeout": int,
   515|         "request_channel_tries": int,
   516|         "encryption_algorithm": str,
   517|         "signing_algorithm": str,
   518|         "publish_signing_algorithm": str,
   519|     }
   520| )
   521| DEFAULT_MINION_OPTS = immutabletypes.freeze(
   522|     {
   523|         "interface": "0.0.0.0",
   524|         "master": "salt",
   525|         "master_type": "str",
   526|         "master_uri_format": "default",
   527|         "source_interface_name": "",
   528|         "source_address": "",
   529|         "source_ret_port": 0,
   530|         "source_publish_port": 0,
   531|         "master_port": 4506,
   532|         "master_finger": "",
   533|         "master_shuffle": False,
   534|         "master_alive_interval": 0,
   535|         "master_failback": False,
   536|         "master_failback_interval": 0,
   537|         "verify_master_pubkey_sign": False,
   538|         "sign_pub_messages": False,

# --- HUNK 5: Lines 595-637 ---
   595|             "base": [salt.syspaths.BASE_FILE_ROOTS_DIR, salt.syspaths.SPM_FORMULA_PATH]
   596|         },
   597|         "top_file_merging_strategy": "merge",
   598|         "env_order": [],
   599|         "default_top": "base",
   600|         "file_recv": False,
   601|         "file_recv_max_size": 100,
   602|         "file_ignore_regex": [],
   603|         "file_ignore_glob": [],
   604|         "fileserver_backend": ["roots"],
   605|         "fileserver_followsymlinks": True,
   606|         "fileserver_ignoresymlinks": False,
   607|         "pillar_roots": {
   608|             "base": [salt.syspaths.BASE_PILLAR_ROOTS_DIR, salt.syspaths.SPM_PILLAR_PATH]
   609|         },
   610|         "on_demand_ext_pillar": ["libvirt", "virtkey"],
   611|         "decrypt_pillar": [],
   612|         "decrypt_pillar_delimiter": ":",
   613|         "decrypt_pillar_default": "gpg",
   614|         "decrypt_pillar_renderers": ["gpg"],
   615|         "gpg_decrypt_must_succeed": False,
   616|         "roots_update_interval": DEFAULT_INTERVAL,
   617|         "azurefs_update_interval": DEFAULT_INTERVAL,
   618|         "gitfs_update_interval": DEFAULT_INTERVAL,
   619|         "git_pillar_update_interval": DEFAULT_INTERVAL,
   620|         "hgfs_update_interval": DEFAULT_INTERVAL,
   621|         "minionfs_update_interval": DEFAULT_INTERVAL,
   622|         "s3fs_update_interval": DEFAULT_INTERVAL,
   623|         "svnfs_update_interval": DEFAULT_INTERVAL,
   624|         "git_pillar_base": "master",
   625|         "git_pillar_branch": "master",
   626|         "git_pillar_env": "",
   627|         "git_pillar_fallback": "",
   628|         "git_pillar_root": "",
   629|         "git_pillar_ssl_verify": True,
   630|         "git_pillar_global_lock": True,
   631|         "git_pillar_user": "",
   632|         "git_pillar_password": "",
   633|         "git_pillar_insecure_auth": False,
   634|         "git_pillar_privkey": "",
   635|         "git_pillar_pubkey": "",
   636|         "git_pillar_passphrase": "",
   637|         "git_pillar_refspecs": _DFLT_REFSPECS,

# --- HUNK 6: Lines 801-891 ---
   801|         "proxy_password": "",
   802|         "proxy_port": 0,
   803|         "minion_jid_queue_hwm": 100,
   804|         "ssl": None,
   805|         "multifunc_ordered": False,
   806|         "beacons_before_connect": False,
   807|         "scheduler_before_connect": False,
   808|         "cache": "localfs",
   809|         "salt_cp_chunk_size": 65536,
   810|         "extmod_whitelist": {},
   811|         "extmod_blacklist": {},
   812|         "minion_sign_messages": False,
   813|         "discovery": False,
   814|         "schedule": {},
   815|         "ssh_merge_pillar": True,
   816|         "disabled_requisites": [],
   817|         "global_state_conditions": None,
   818|         "reactor_niceness": None,
   819|         "fips_mode": False,
   820|         "features": {},
   821|         "encryption_algorithm": "OAEP-SHA1",
   822|         "signing_algorithm": "PKCS1v15-SHA1",
   823|     }
   824| )
   825| DEFAULT_MASTER_OPTS = immutabletypes.freeze(
   826|     {
   827|         "interface": "0.0.0.0",
   828|         "publish_port": 4505,
   829|         "zmq_backlog": 1000,
   830|         "pub_hwm": 1000,
   831|         "auth_mode": 1,
   832|         "user": _MASTER_USER,
   833|         "worker_threads": 5,
   834|         "sock_dir": os.path.join(salt.syspaths.SOCK_DIR, "master"),
   835|         "sock_pool_size": 1,
   836|         "ret_port": 4506,
   837|         "timeout": 5,
   838|         "keep_jobs": 24,
   839|         "keep_jobs_seconds": 86400,
   840|         "archive_jobs": False,
   841|         "root_dir": salt.syspaths.ROOT_DIR,
   842|         "pki_dir": os.path.join(salt.syspaths.LIB_STATE_DIR, "pki", "master"),
   843|         "key_cache": "",
   844|         "cachedir": os.path.join(salt.syspaths.CACHE_DIR, "master"),
   845|         "file_roots": {
   846|             "base": [salt.syspaths.BASE_FILE_ROOTS_DIR, salt.syspaths.SPM_FORMULA_PATH]
   847|         },
   848|         "master_roots": {"base": [salt.syspaths.BASE_MASTER_ROOTS_DIR]},
   849|         "pillar_roots": {
   850|             "base": [salt.syspaths.BASE_PILLAR_ROOTS_DIR, salt.syspaths.SPM_PILLAR_PATH]
   851|         },
   852|         "on_demand_ext_pillar": ["libvirt", "virtkey"],
   853|         "decrypt_pillar": [],
   854|         "decrypt_pillar_delimiter": ":",
   855|         "decrypt_pillar_default": "gpg",
   856|         "decrypt_pillar_renderers": ["gpg"],
   857|         "gpg_decrypt_must_succeed": False,
   858|         "thoriumenv": None,
   859|         "thorium_top": "top.sls",
   860|         "thorium_interval": 0.5,
   861|         "thorium_roots": {"base": [salt.syspaths.BASE_THORIUM_ROOTS_DIR]},
   862|         "top_file_merging_strategy": "merge",
   863|         "env_order": [],
   864|         "saltenv": None,
   865|         "lock_saltenv": False,
   866|         "pillarenv": None,
   867|         "default_top": "base",
   868|         "file_client": "local",
   869|         "local": True,
   870|         "roots_update_interval": DEFAULT_INTERVAL,
   871|         "azurefs_update_interval": DEFAULT_INTERVAL,
   872|         "gitfs_update_interval": DEFAULT_INTERVAL,
   873|         "git_pillar_update_interval": DEFAULT_INTERVAL,
   874|         "hgfs_update_interval": DEFAULT_INTERVAL,
   875|         "minionfs_update_interval": DEFAULT_INTERVAL,
   876|         "s3fs_update_interval": DEFAULT_INTERVAL,
   877|         "svnfs_update_interval": DEFAULT_INTERVAL,
   878|         "git_pillar_base": "master",
   879|         "git_pillar_branch": "master",
   880|         "git_pillar_env": "",
   881|         "git_pillar_fallback": "",
   882|         "git_pillar_root": "",
   883|         "git_pillar_ssl_verify": True,
   884|         "git_pillar_global_lock": True,
   885|         "git_pillar_user": "",
   886|         "git_pillar_password": "",
   887|         "git_pillar_insecure_auth": False,
   888|         "git_pillar_privkey": "",
   889|         "git_pillar_pubkey": "",
   890|         "git_pillar_passphrase": "",
   891|         "git_pillar_refspecs": _DFLT_REFSPECS,

# --- HUNK 7: Lines 1046-1085 ---
  1046|         "serial": "msgpack",
  1047|         "test": False,
  1048|         "state_verbose": True,
  1049|         "state_output": "full",
  1050|         "state_output_diff": False,
  1051|         "state_output_profile": True,
  1052|         "state_auto_order": True,
  1053|         "state_events": False,
  1054|         "state_aggregate": False,
  1055|         "search": "",
  1056|         "loop_interval": 60,
  1057|         "nodegroups": {},
  1058|         "ssh_list_nodegroups": {},
  1059|         "ssh_use_home_key": False,
  1060|         "cython_enable": False,
  1061|         "enable_gpu_grains": False,
  1062|         "key_logfile": os.path.join(salt.syspaths.LOGS_DIR, "key"),
  1063|         "verify_env": True,
  1064|         "permissive_pki_access": False,
  1065|         "key_pass": None,
  1066|         "signing_key_pass": None,
  1067|         "default_include": "master.d/*.conf",
  1068|         "winrepo_dir": os.path.join(salt.syspaths.BASE_FILE_ROOTS_DIR, "win", "repo"),
  1069|         "winrepo_dir_ng": os.path.join(
  1070|             salt.syspaths.BASE_FILE_ROOTS_DIR, "win", "repo-ng"
  1071|         ),
  1072|         "winrepo_cachefile": "winrepo.p",
  1073|         "winrepo_remotes": ["https://github.com/saltstack/salt-winrepo.git"],
  1074|         "winrepo_remotes_ng": ["https://github.com/saltstack/salt-winrepo-ng.git"],
  1075|         "winrepo_branch": "master",
  1076|         "winrepo_fallback": "",
  1077|         "winrepo_ssl_verify": True,
  1078|         "winrepo_user": "",
  1079|         "winrepo_password": "",
  1080|         "winrepo_insecure_auth": False,
  1081|         "winrepo_privkey": "",
  1082|         "winrepo_pubkey": "",
  1083|         "winrepo_passphrase": "",
  1084|         "winrepo_refspecs": _DFLT_REFSPECS,
  1085|         "syndic_wait": 5,

# --- HUNK 8: Lines 1144-1185 ---
  1144|         "allow_minion_key_revoke": True,
  1145|         "salt_cp_chunk_size": 98304,
  1146|         "require_minion_sign_messages": False,
  1147|         "drop_messages_signature_fail": False,
  1148|         "discovery": False,
  1149|         "schedule": {},
  1150|         "auth_events": True,
  1151|         "minion_data_cache_events": True,
  1152|         "enable_ssh_minions": False,
  1153|         "netapi_allow_raw_shell": False,
  1154|         "fips_mode": False,
  1155|         "detect_remote_minions": False,
  1156|         "remote_minions_port": 22,
  1157|         "pass_variable_prefix": "",
  1158|         "pass_strict_fetch": False,
  1159|         "pass_gnupghome": "",
  1160|         "pass_dir": "",
  1161|         "netapi_enable_clients": [],
  1162|         "maintenance_interval": 3600,
  1163|         "fileserver_interval": 3600,
  1164|         "features": {},
  1165|         "publish_signing_algorithm": "PKCS1v15-SHA1",
  1166|     }
  1167| )
  1168| DEFAULT_PROXY_MINION_OPTS = immutabletypes.freeze(
  1169|     {
  1170|         "conf_file": os.path.join(salt.syspaths.CONFIG_DIR, "proxy"),
  1171|         "log_file": os.path.join(salt.syspaths.LOGS_DIR, "proxy"),
  1172|         "add_proxymodule_to_opts": False,
  1173|         "proxy_merge_grains_in_module": True,
  1174|         "extension_modules": os.path.join(salt.syspaths.CACHE_DIR, "proxy", "extmods"),
  1175|         "append_minionid_config_dirs": [
  1176|             "cachedir",
  1177|             "pidfile",
  1178|             "default_include",
  1179|             "extension_modules",
  1180|         ],
  1181|         "default_include": "proxy.d/*.conf",
  1182|         "proxy_merge_pillar_in_opts": False,
  1183|         "proxy_deep_merge_pillar_in_opts": False,
  1184|         "proxy_merge_pillar_in_opts_strategy": "smart",
  1185|         "proxy_mines_pillar": True,

# --- HUNK 9: Lines 1570-1619 ---
  1570|                     path,
  1571|                 )
  1572|         for fn_ in sorted(glob_matches):
  1573|             log.debug("Including configuration from '%s'", fn_)
  1574|             try:
  1575|                 opts = _read_conf_file(fn_)
  1576|             except salt.exceptions.SaltConfigurationError as error:
  1577|                 log.error(error)
  1578|                 if exit_on_config_errors:
  1579|                     sys.exit(salt.defaults.exitcodes.EX_GENERIC)
  1580|                 else:
  1581|                     opts = {}
  1582|             schedule = opts.get("schedule", {})
  1583|             if schedule and "schedule" in configuration:
  1584|                 configuration["schedule"].update(schedule)
  1585|             include = opts.get("include", [])
  1586|             if include:
  1587|                 opts.update(include_config(include, fn_, verbose))
  1588|             salt.utils.dictupdate.update(configuration, opts, True, True)
  1589|     return configuration
  1590| def should_prepend_root_dir(key, opts):
  1591|     """
  1592|     Prepend root dir only when the key exists, has a value, and that value is
  1593|     not a URI.
  1594|     """
  1595|     return (
  1596|         key in opts
  1597|         and opts[key] is not None
  1598|         and urllib.parse.urlparse(os.path.splitdrive(opts[key])[1]).scheme == ""
  1599|     )
  1600| def prepend_root_dir(opts, path_options):
  1601|     """
  1602|     Prepends the options that represent filesystem paths with value of the
  1603|     'root_dir' option.
  1604|     """
  1605|     root_dir = os.path.abspath(opts["root_dir"])
  1606|     def_root_dir = salt.syspaths.ROOT_DIR.rstrip(os.sep)
  1607|     for path_option in path_options:
  1608|         if path_option in opts:
  1609|             path = opts[path_option]
  1610|             tmp_path_def_root_dir = None
  1611|             tmp_path_root_dir = None
  1612|             if path == def_root_dir or path.startswith(def_root_dir + os.sep):
  1613|                 tmp_path_def_root_dir = path[len(def_root_dir) :]
  1614|             if root_dir and (path == root_dir or path.startswith(root_dir + os.sep)):
  1615|                 tmp_path_root_dir = path[len(root_dir) :]
  1616|             if tmp_path_def_root_dir and not tmp_path_root_dir:
  1617|                 path = tmp_path_def_root_dir
  1618|             elif tmp_path_root_dir and not tmp_path_def_root_dir:
  1619|                 path = tmp_path_root_dir

# --- HUNK 10: Lines 1811-1851 ---
  1811|         "user": opts.get("syndic_user", opts["user"]),
  1812|         "sock_dir": os.path.join(
  1813|             opts["cachedir"], opts.get("syndic_sock_dir", opts["sock_dir"])
  1814|         ),
  1815|         "sock_pool_size": master_opts["sock_pool_size"],
  1816|         "cachedir": master_opts["cachedir"],
  1817|     }
  1818|     opts.update(syndic_opts)
  1819|     prepend_root_dirs = [
  1820|         "pki_dir",
  1821|         "cachedir",
  1822|         "pidfile",
  1823|         "sock_dir",
  1824|         "extension_modules",
  1825|         "autosign_file",
  1826|         "autoreject_file",
  1827|         "token_dir",
  1828|         "autosign_grains_dir",
  1829|     ]
  1830|     for config_key in ("log_file", "key_logfile", "syndic_log_file"):
  1831|         if should_prepend_root_dir(config_key, opts):
  1832|             prepend_root_dirs.append(config_key)
  1833|     prepend_root_dir(opts, prepend_root_dirs)
  1834|     salt.features.setup_features(opts)
  1835|     return opts
  1836| def apply_sdb(opts, sdb_opts=None):
  1837|     """
  1838|     Recurse for sdb:// links for opts
  1839|     """
  1840|     import salt.utils.sdb
  1841|     if sdb_opts is None:
  1842|         sdb_opts = opts
  1843|     if isinstance(sdb_opts, str) and sdb_opts.startswith("sdb://"):
  1844|         return salt.utils.sdb.sdb_get(sdb_opts, opts)
  1845|     elif isinstance(sdb_opts, dict):
  1846|         for key, value in sdb_opts.items():
  1847|             if value is None:
  1848|                 continue
  1849|             sdb_opts[key] = apply_sdb(opts, value)
  1850|     elif isinstance(sdb_opts, list):
  1851|         for key, value in enumerate(sdb_opts):

# --- HUNK 11: Lines 1974-2015 ---
  1974|                 os.path.dirname(providers_config_path), "cloud.providers.d", "*"
  1975|             )
  1976|             if os.path.isfile(providers_config_path) or glob.glob(providers_confd):
  1977|                 raise salt.exceptions.SaltCloudConfigError(
  1978|                     "Do not mix the old cloud providers configuration with "
  1979|                     "the new one. The providers configuration should now go "
  1980|                     "in the file `{0}` or a separate `*.conf` file within "
  1981|                     "`cloud.providers.d/` which is relative to `{0}`.".format(
  1982|                         os.path.join(salt.syspaths.CONFIG_DIR, "cloud.providers")
  1983|                     )
  1984|                 )
  1985|         providers_config = opts["providers"]
  1986|     elif providers_config_path is not None:
  1987|         providers_config = cloud_providers_config(providers_config_path)
  1988|     opts["providers"] = providers_config
  1989|     if profiles_config is None:
  1990|         profiles_config = vm_profiles_config(profiles_config_path, providers_config)
  1991|     opts["profiles"] = profiles_config
  1992|     apply_sdb(opts)
  1993|     prepend_root_dirs = ["cachedir"]
  1994|     if should_prepend_root_dir("log_file", opts):
  1995|         prepend_root_dirs.append("log_file")
  1996|     prepend_root_dir(opts, prepend_root_dirs)
  1997|     salt.features.setup_features(opts)
  1998|     return opts
  1999| def apply_cloud_config(overrides, defaults=None):
  2000|     """
  2001|     Return a cloud config
  2002|     """
  2003|     if defaults is None:
  2004|         defaults = DEFAULT_CLOUD_OPTS.copy()
  2005|     config = defaults.copy()
  2006|     if overrides:
  2007|         config.update(overrides)
  2008|     if "providers" in config:
  2009|         providers = config["providers"].copy()
  2010|         config["providers"] = {}
  2011|         for alias, details in providers.items():
  2012|             if isinstance(details, list):
  2013|                 for detail in details:
  2014|                     if "driver" not in detail:
  2015|                         raise salt.exceptions.SaltCloudConfigError(

# --- HUNK 12: Lines 2743-2783 ---
  2743|             log.warning(
  2744|                 "The 'saltenv' and 'environment' minion config options "
  2745|                 "cannot both be used. Ignoring 'environment' in favor of "
  2746|                 "'saltenv'."
  2747|             )
  2748|             opts["environment"] = opts["saltenv"]
  2749|         else:
  2750|             log.warning(
  2751|                 "The 'environment' minion config option has been renamed "
  2752|                 "to 'saltenv'. Using %s as the 'saltenv' config value.",
  2753|                 opts["environment"],
  2754|             )
  2755|             opts["saltenv"] = opts["environment"]
  2756|     for idx, val in enumerate(opts["fileserver_backend"]):
  2757|         if val in ("git", "hg", "svn", "minion"):
  2758|             new_val = val + "fs"
  2759|             log.debug(
  2760|                 "Changed %s to %s in minion opts' fileserver_backend list", val, new_val
  2761|             )
  2762|             opts["fileserver_backend"][idx] = new_val
  2763|     opts["__cli"] = salt.utils.stringutils.to_unicode(os.path.basename(sys.argv[0]))
  2764|     using_ip_for_id = False
  2765|     if not opts.get("id"):
  2766|         if minion_id:
  2767|             opts["id"] = minion_id
  2768|         else:
  2769|             opts["id"], using_ip_for_id = get_id(opts, cache_minion_id=cache_minion_id)
  2770|     if not using_ip_for_id and "append_domain" in opts:
  2771|         opts["id"] = _append_domain(opts)
  2772|     for directory in opts.get("append_minionid_config_dirs", []):
  2773|         if directory in ("pki_dir", "cachedir", "extension_modules"):
  2774|             newdirectory = os.path.join(opts[directory], opts["id"])
  2775|             opts[directory] = newdirectory
  2776|         elif directory == "default_include" and directory in opts:
  2777|             include_dir = os.path.dirname(opts[directory])
  2778|             new_include_dir = os.path.join(
  2779|                 include_dir, opts["id"], os.path.basename(opts[directory])
  2780|             )
  2781|             opts[directory] = new_include_dir
  2782|     if "pidfile" in opts.get("append_minionid_config_dirs", []):
  2783|         newpath_list = os.path.split(opts["pidfile"])

# --- HUNK 13: Lines 2787-2848 ---
  2787|     if len(opts["sock_dir"]) > len(opts["cachedir"]) + 10:
  2788|         opts["sock_dir"] = os.path.join(opts["cachedir"], ".salt-unix")
  2789|     opts["open_mode"] = opts["open_mode"] is True
  2790|     opts["file_roots"] = _validate_file_roots(opts["file_roots"])
  2791|     opts["pillar_roots"] = _validate_pillar_roots(opts["pillar_roots"])
  2792|     opts["extension_modules"] = opts.get("extension_modules") or os.path.join(
  2793|         opts["cachedir"], "extmods"
  2794|     )
  2795|     opts["utils_dirs"] = opts.get("utils_dirs") or [
  2796|         os.path.join(opts["extension_modules"], "utils")
  2797|     ]
  2798|     insert_system_path(opts, opts["utils_dirs"])
  2799|     prepend_root_dirs = [
  2800|         "pki_dir",
  2801|         "cachedir",
  2802|         "sock_dir",
  2803|         "extension_modules",
  2804|         "pidfile",
  2805|     ]
  2806|     for config_key in ("log_file", "key_logfile"):
  2807|         if should_prepend_root_dir(config_key, opts):
  2808|             prepend_root_dirs.append(config_key)
  2809|     prepend_root_dir(opts, prepend_root_dirs)
  2810|     if "beacons" not in opts:
  2811|         opts["beacons"] = {}
  2812|     if overrides.get("ipc_write_buffer", "") == "dynamic":
  2813|         opts["ipc_write_buffer"] = _DFLT_IPC_WBUFFER
  2814|     if "ipc_write_buffer" not in overrides:
  2815|         opts["ipc_write_buffer"] = 0
  2816|     opts["hash_type"] = opts["hash_type"].lower()
  2817|     _update_ssl_config(opts)
  2818|     _update_discovery_config(opts)
  2819|     if opts["encryption_algorithm"] not in salt.crypt.VALID_ENCRYPTION_ALGORITHMS:
  2820|         raise salt.exceptions.SaltConfigurationError(
  2821|             f"The encryption algorithm '{opts['encryption_algorithm']}' is not valid. "
  2822|             f"Please specify one of {','.join(salt.crypt.VALID_ENCRYPTION_ALGORITHMS)}."
  2823|         )
  2824|     if opts["signing_algorithm"] not in salt.crypt.VALID_SIGNING_ALGORITHMS:
  2825|         raise salt.exceptions.SaltConfigurationError(
  2826|             f"The signging algorithm '{opts['signing_algorithm']}' is not valid. "
  2827|             f"Please specify one of {','.join(salt.crypt.VALID_SIGNING_ALGORITHMS)}."
  2828|         )
  2829|     return opts
  2830| def _update_discovery_config(opts):
  2831|     """
  2832|     Update discovery config for all instances.
  2833|     :param opts:
  2834|     :return:
  2835|     """
  2836|     if opts.get("discovery") not in (None, False):
  2837|         if opts["discovery"] is True:
  2838|             opts["discovery"] = {}
  2839|         discovery_config = {
  2840|             "attempts": 3,
  2841|             "pause": 5,
  2842|             "port": 4520,
  2843|             "match": "any",
  2844|             "mapping": {},
  2845|         }
  2846|         for key in opts["discovery"]:
  2847|             if key not in discovery_config:
  2848|                 raise salt.exceptions.SaltConfigurationError(

# --- HUNK 14: Lines 2901-2941 ---
  2901|     """
  2902|     Returns master configurations dict.
  2903|     """
  2904|     if defaults is None:
  2905|         defaults = DEFAULT_MASTER_OPTS.copy()
  2906|     if overrides is None:
  2907|         overrides = {}
  2908|     opts = defaults.copy()
  2909|     opts["__role"] = "master"
  2910|     opts["__fs_update"] = True
  2911|     _adjust_log_file_override(overrides, defaults["log_file"])
  2912|     if overrides:
  2913|         opts.update(overrides)
  2914|     if "rest" in opts.get("external_auth", {}):
  2915|         if opts["keep_acl_in_token"] is False:
  2916|             log.warning(
  2917|                 "The 'rest' external_auth backend requires 'keep_acl_in_token' to be True. "
  2918|                 "Setting 'keep_acl_in_token' to True."
  2919|             )
  2920|         opts["keep_acl_in_token"] = True
  2921|     opts["__cli"] = salt.utils.stringutils.to_unicode(os.path.basename(sys.argv[0]))
  2922|     if "environment" in opts:
  2923|         if opts["saltenv"] is not None:
  2924|             log.warning(
  2925|                 "The 'saltenv' and 'environment' master config options "
  2926|                 "cannot both be used. Ignoring 'environment' in favor of "
  2927|                 "'saltenv'."
  2928|             )
  2929|             opts["environment"] = opts["saltenv"]
  2930|         else:
  2931|             log.warning(
  2932|                 "The 'environment' master config option has been renamed "
  2933|                 "to 'saltenv'. Using %s as the 'saltenv' config value.",
  2934|                 opts["environment"],
  2935|             )
  2936|             opts["saltenv"] = opts["environment"]
  2937|     for idx, val in enumerate(opts["fileserver_backend"]):
  2938|         if val in ("git", "hg", "svn", "minion"):
  2939|             new_val = val + "fs"
  2940|             log.debug(
  2941|                 "Changed %s to %s in master opts' fileserver_backend list", val, new_val

# --- HUNK 15: Lines 2962-3039 ---
  2962|         opts["id"], using_ip_for_id = get_id(opts, cache_minion_id=None)
  2963|         append_master = True
  2964|     if not using_ip_for_id and "append_domain" in opts:
  2965|         opts["id"] = _append_domain(opts)
  2966|     if append_master:
  2967|         opts["id"] += "_master"
  2968|     prepend_root_dirs = [
  2969|         "pki_dir",
  2970|         "cachedir",
  2971|         "pidfile",
  2972|         "sock_dir",
  2973|         "extension_modules",
  2974|         "autosign_file",
  2975|         "autoreject_file",
  2976|         "token_dir",
  2977|         "syndic_dir",
  2978|         "sqlite_queue_dir",
  2979|         "autosign_grains_dir",
  2980|     ]
  2981|     for config_key in ("log_file", "key_logfile", "ssh_log_file"):
  2982|         if should_prepend_root_dir(config_key, opts):
  2983|             prepend_root_dirs.append(config_key)
  2984|     prepend_root_dir(opts, prepend_root_dirs)
  2985|     opts["open_mode"] = opts["open_mode"] is True
  2986|     opts["auto_accept"] = opts["auto_accept"] is True
  2987|     opts["file_roots"] = _validate_file_roots(opts["file_roots"])
  2988|     opts["pillar_roots"] = _validate_file_roots(opts["pillar_roots"])
  2989|     if opts["file_ignore_regex"]:
  2990|         if isinstance(opts["file_ignore_regex"], str):
  2991|             ignore_regex = [opts["file_ignore_regex"]]
  2992|         elif isinstance(opts["file_ignore_regex"], list):
  2993|             ignore_regex = opts["file_ignore_regex"]
  2994|         opts["file_ignore_regex"] = []
  2995|         for regex in ignore_regex:
  2996|             try:
  2997|                 re.compile(regex)
  2998|                 opts["file_ignore_regex"].append(regex)
  2999|             except Exception:  # pylint: disable=broad-except
  3000|                 log.warning("Unable to parse file_ignore_regex. Skipping: %s", regex)
  3001|     if opts["file_ignore_glob"]:
  3002|         if isinstance(opts["file_ignore_glob"], str):
  3003|             opts["file_ignore_glob"] = [opts["file_ignore_glob"]]
  3004|     if opts["worker_threads"] < 3 and opts.get("peer", None):
  3005|         log.warning(
  3006|             "The 'worker_threads' setting in '%s' cannot be lower than "
  3007|             "3. Resetting it to the default value of 3.",
  3008|             opts["conf_file"],
  3009|         )
  3010|         opts["worker_threads"] = 3
  3011|     opts.setdefault("pillar_source_merging_strategy", "smart")
  3012|     opts["hash_type"] = opts["hash_type"].lower()
  3013|     _update_ssl_config(opts)
  3014|     _update_discovery_config(opts)
  3015|     if opts["publish_signing_algorithm"] not in salt.crypt.VALID_SIGNING_ALGORITHMS:
  3016|         raise salt.exceptions.SaltConfigurationError(
  3017|             f"The  publish signging algorithm '{opts['publish_signing_algorithm']}' is not valid. "
  3018|             f"Please specify one of {','.join(salt.crypt.VALID_SIGNING_ALGORITHMS)}."
  3019|         )
  3020|     return opts
  3021| def client_config(path, env_var="SALT_CLIENT_CONFIG", defaults=None):
  3022|     """
  3023|     Load Master configuration data
  3024|     Usage:
  3025|     .. code-block:: python
  3026|         import salt.config
  3027|         master_opts = salt.config.client_config('/etc/salt/master')
  3028|     Returns a dictionary of the Salt Master configuration file with necessary
  3029|     options needed to communicate with a locally-running Salt Master daemon.
  3030|     This function searches for client specific configurations and adds them to
  3031|     the data from the master configuration.
  3032|     This is useful for master-side operations like
  3033|     :py:class:`~salt.client.LocalClient`.
  3034|     """
  3035|     if defaults is None:
  3036|         defaults = DEFAULT_MASTER_OPTS.copy()
  3037|     xdg_dir = salt.utils.xdg.xdg_config_dir()
  3038|     if os.path.isdir(xdg_dir):
  3039|         client_config_dir = xdg_dir

# --- HUNK 16: Lines 3103-3126 ---
  3103|     defaults = apply_master_config(overrides, defaults)
  3104|     defaults = apply_spm_config(overrides, defaults)
  3105|     return client_config(path, env_var="SPM_CONFIG", defaults=defaults)
  3106| def apply_spm_config(overrides, defaults):
  3107|     """
  3108|     Returns the spm configurations dict.
  3109|     .. versionadded:: 2015.8.1
  3110|     """
  3111|     opts = defaults.copy()
  3112|     _adjust_log_file_override(overrides, defaults["log_file"])
  3113|     if overrides:
  3114|         opts.update(overrides)
  3115|     prepend_root_dirs = [
  3116|         "formula_path",
  3117|         "pillar_path",
  3118|         "reactor_path",
  3119|         "spm_cache_dir",
  3120|         "spm_build_dir",
  3121|     ]
  3122|     for config_key in ("spm_logfile",):
  3123|         if should_prepend_root_dir(config_key, opts):
  3124|             prepend_root_dirs.append(config_key)
  3125|     prepend_root_dir(opts, prepend_root_dirs)
  3126|     return opts


# ====================================================================
# FILE: salt/crypt.py
# Total hunks: 15
# ====================================================================
# --- HUNK 1: Lines 1-391 ---
     1| """
     2| The crypt module manages all of the cryptography functions for minions and
     3| masters, encrypting and decrypting payloads, preparing messages, and
     4| authenticating peers
     5| """
     6| import base64
     7| import binascii
     8| import copy
     9| import getpass
    10| import hashlib
    11| import hmac
    12| import logging
    13| import os
    14| import random
    15| import stat
    16| import sys
    17| import time
    18| import traceback
    19| import uuid
    20| import weakref
    21| import salt.channel.client
    22| import salt.defaults.exitcodes
    23| import salt.ext.tornado.gen
    24| import salt.payload
    25| import salt.utils.crypt
    26| import salt.utils.decorators
    27| import salt.utils.event
    28| import salt.utils.files
    29| import salt.utils.rsax931
    30| import salt.utils.sdb
    31| import salt.utils.stringutils
    32| import salt.utils.user
    33| import salt.utils.verify
    34| import salt.version
    35| from salt.exceptions import (
    36|     AuthenticationError,
    37|     InvalidKeyError,
    38|     MasterExit,
    39|     SaltClientError,
    40|     SaltReqTimeoutError,
    41|     UnsupportedAlgorithm,
    42| )
    43| try:
    44|     import cryptography.exceptions
    45|     from cryptography.hazmat.primitives import hashes, serialization
    46|     from cryptography.hazmat.primitives.asymmetric import padding, rsa
    47|     from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
    48|     HAS_CRYPTOGRAPHY = True
    49| except ImportError:
    50|     HAS_CRYPTOGRAPHY = False
    51| log = logging.getLogger(__name__)
    52| OAEP = "OAEP"
    53| PKCS1v15 = "PKCS1v15"
    54| SHA1 = "SHA1"
    55| SHA224 = "SHA224"
    56| OAEP_SHA1 = f"{OAEP}-{SHA1}"
    57| OAEP_SHA224 = f"{OAEP}-{SHA224}"
    58| PKCS1v15_SHA1 = f"{PKCS1v15}-{SHA1}"
    59| PKCS1v15_SHA224 = f"{PKCS1v15}-{SHA224}"
    60| VALID_HASHES = (
    61|     SHA1,
    62|     SHA224,
    63| )
    64| VALID_PADDING_FOR_SIGNING = (PKCS1v15,)
    65| VALID_PADDING_FOR_ENCRYPTION = (OAEP,)
    66| VALID_ENCRYPTION_ALGORITHMS = (
    67|     OAEP_SHA1,
    68|     OAEP_SHA224,
    69| )
    70| VALID_SIGNING_ALGORITHMS = (
    71|     PKCS1v15_SHA1,
    72|     PKCS1v15_SHA224,
    73| )
    74| def fips_enabled():
    75|     if HAS_CRYPTOGRAPHY:
    76|         import cryptography.hazmat.backends.openssl.backend
    77|         return cryptography.hazmat.backends.openssl.backend._fips_enabled
    78| def clean_key(key):
    79|     """
    80|     Clean the key so that it only has unix style line endings (\\n)
    81|     """
    82|     return "\n".join(key.strip().splitlines())
    83| def dropfile(cachedir, user=None):
    84|     """
    85|     Set an AES dropfile to request the master update the publish session key
    86|     """
    87|     dfn = os.path.join(cachedir, ".dfn")
    88|     with salt.utils.files.set_umask(0o277):
    89|         log.info("Rotating AES key")
    90|         if os.path.isfile(dfn):
    91|             log.info("AES key rotation already requested")
    92|             return
    93|         if os.path.isfile(dfn) and not os.access(dfn, os.W_OK):
    94|             os.chmod(dfn, stat.S_IRUSR | stat.S_IWUSR)
    95|         with salt.utils.files.fopen(dfn, "wb+") as fp_:
    96|             fp_.write(b"")
    97|         os.chmod(dfn, stat.S_IRUSR)
    98|         if user:
    99|             try:
   100|                 import pwd
   101|                 uid = pwd.getpwnam(user).pw_uid
   102|                 os.chown(dfn, uid, -1)
   103|             except (KeyError, ImportError, OSError):
   104|                 pass
   105| def gen_keys(keydir, keyname, keysize, user=None, passphrase=None, e=65537):
   106|     """
   107|     Generate a RSA public keypair for use with salt
   108|     :param str keydir: The directory to write the keypair to
   109|     :param str keyname: The type of salt server for whom this key should be written. (i.e. 'master' or 'minion')
   110|     :param int keysize: The number of bits in the key
   111|     :param str user: The user on the system who should own this keypair
   112|     :param str passphrase: The passphrase which should be used to encrypt the private key
   113|     :rtype: str
   114|     :return: Path on the filesystem to the RSA private key
   115|     """
   116|     base = os.path.join(keydir, keyname)
   117|     priv = f"{base}.pem"
   118|     pub = f"{base}.pub"
   119|     gen = rsa.generate_private_key(e, keysize)
   120|     if os.path.isfile(priv):
   121|         return priv
   122|     if not os.access(keydir, os.W_OK):
   123|         raise OSError(
   124|             'Write access denied to "{}" for user "{}".'.format(
   125|                 os.path.abspath(keydir), getpass.getuser()
   126|             )
   127|         )
   128|     with salt.utils.files.set_umask(0o277):
   129|         with salt.utils.files.fopen(priv, "wb+") as f:
   130|             if passphrase:
   131|                 enc = serialization.BestAvailableEncryption(passphrase.encode())
   132|                 _format = serialization.PrivateFormat.TraditionalOpenSSL
   133|                 if fips_enabled():
   134|                     _format = serialization.PrivateFormat.PKCS8
   135|             else:
   136|                 enc = serialization.NoEncryption()
   137|                 _format = serialization.PrivateFormat.TraditionalOpenSSL
   138|             pem = gen.private_bytes(
   139|                 encoding=serialization.Encoding.PEM,
   140|                 format=_format,
   141|                 encryption_algorithm=enc,
   142|             )
   143|             f.write(pem)
   144|     pubkey = gen.public_key()
   145|     with salt.utils.files.fopen(pub, "wb+") as f:
   146|         pem = pubkey.public_bytes(
   147|             encoding=serialization.Encoding.PEM,
   148|             format=serialization.PublicFormat.SubjectPublicKeyInfo,
   149|         )
   150|         f.write(pem)
   151|     os.chmod(priv, 0o400)
   152|     if user:
   153|         try:
   154|             import pwd
   155|             uid = pwd.getpwnam(user).pw_uid
   156|             os.chown(priv, uid, -1)
   157|             os.chown(pub, uid, -1)
   158|         except (KeyError, ImportError, OSError):
   159|             pass
   160|     return priv
   161| class BaseKey:
   162|     @staticmethod
   163|     def parse_padding_for_signing(algorithm):
   164|         if algorithm not in VALID_SIGNING_ALGORITHMS:
   165|             raise UnsupportedAlgorithm(f"Invalid signing algorithm: {algorithm}")
   166|         _pad, _hash = algorithm.split("-", 1)
   167|         if _pad not in VALID_PADDING_FOR_SIGNING:
   168|             raise UnsupportedAlgorithm(f"Invalid padding algorithm: {_pad}")
   169|         return getattr(padding, _pad)
   170|     @staticmethod
   171|     def parse_padding_for_encryption(algorithm):
   172|         if algorithm not in VALID_ENCRYPTION_ALGORITHMS:
   173|             raise UnsupportedAlgorithm(f"Invalid encryption algorithm: {algorithm}")
   174|         _pad, _hash = algorithm.split("-", 1)
   175|         if _pad not in VALID_PADDING_FOR_ENCRYPTION:
   176|             raise UnsupportedAlgorithm(f"Invalid padding algorithm: {_pad}")
   177|         return getattr(padding, _pad)
   178|     @staticmethod
   179|     def parse_hash(algorithm):
   180|         if "-" not in algorithm:
   181|             raise UnsupportedAlgorithm(f"Invalid encryption algorithm: {algorithm}")
   182|         _pad, _hash = algorithm.split("-", 1)
   183|         if _hash not in VALID_HASHES:
   184|             raise Exception("Invalid hashing algorithm")
   185|         return getattr(hashes, _hash)
   186| class PrivateKey(BaseKey):
   187|     def __init__(self, path, passphrase=None):
   188|         self.key = get_rsa_key(path, passphrase)
   189|     def encrypt(self, data):
   190|         pem = self.key.private_bytes(
   191|             encoding=serialization.Encoding.PEM,
   192|             format=serialization.PrivateFormat.TraditionalOpenSSL,
   193|             encryption_algorithm=serialization.NoEncryption(),
   194|         )
   195|         return salt.utils.rsax931.RSAX931Signer(pem).sign(data)
   196|     def sign(self, data, algorithm=PKCS1v15_SHA1):
   197|         _padding = self.parse_padding_for_signing(algorithm)
   198|         _hash = self.parse_hash(algorithm)
   199|         try:
   200|             return self.key.sign(
   201|                 salt.utils.stringutils.to_bytes(data), _padding(), _hash()
   202|             )
   203|         except cryptography.exceptions.UnsupportedAlgorithm:
   204|             raise UnsupportedAlgorithm(f"Unsupported algorithm: {algorithm}")
   205|     def decrypt(self, data, algorithm=OAEP_SHA1):
   206|         _padding = self.parse_padding_for_encryption(algorithm)
   207|         _hash = self.parse_hash(algorithm)
   208|         try:
   209|             return self.key.decrypt(
   210|                 data,
   211|                 _padding(
   212|                     mgf=padding.MGF1(algorithm=_hash()),
   213|                     algorithm=_hash(),
   214|                     label=None,
   215|                 ),
   216|             )
   217|         except cryptography.exceptions.UnsupportedAlgorithm:
   218|             raise UnsupportedAlgorithm(f"Unsupported algorithm: {algorithm}")
   219| class PublicKey(BaseKey):
   220|     def __init__(self, path):
   221|         with salt.utils.files.fopen(path, "rb") as fp:
   222|             try:
   223|                 self.key = serialization.load_pem_public_key(fp.read())
   224|             except ValueError as exc:
   225|                 raise InvalidKeyError("Invalid key")
   226|     def encrypt(self, data, algorithm=OAEP_SHA1):
   227|         _padding = self.parse_padding_for_encryption(algorithm)
   228|         _hash = self.parse_hash(algorithm)
   229|         bdata = salt.utils.stringutils.to_bytes(data)
   230|         try:
   231|             return self.key.encrypt(
   232|                 bdata,
   233|                 _padding(
   234|                     mgf=padding.MGF1(algorithm=_hash()),
   235|                     algorithm=_hash(),
   236|                     label=None,
   237|                 ),
   238|             )
   239|         except cryptography.exceptions.UnsupportedAlgorithm:
   240|             raise UnsupportedAlgorithm(f"Unsupported algorithm: {algorithm}")
   241|     def verify(self, data, signature, algorithm=PKCS1v15_SHA1):
   242|         _padding = self.parse_padding_for_signing(algorithm)
   243|         _hash = self.parse_hash(algorithm)
   244|         try:
   245|             self.key.verify(
   246|                 salt.utils.stringutils.to_bytes(signature),
   247|                 salt.utils.stringutils.to_bytes(data),
   248|                 _padding(),
   249|                 _hash(),
   250|             )
   251|         except cryptography.exceptions.InvalidSignature:
   252|             return False
   253|         return True
   254|     def decrypt(self, data):
   255|         pem = self.key.public_bytes(
   256|             encoding=serialization.Encoding.PEM,
   257|             format=serialization.PublicFormat.SubjectPublicKeyInfo,
   258|         )
   259|         verifier = salt.utils.rsax931.RSAX931Verifier(pem)
   260|         return verifier.verify(data)
   261| @salt.utils.decorators.memoize
   262| def _get_key_with_evict(path, timestamp, passphrase):
   263|     """
   264|     Load a private key from disk.  `timestamp` above is intended to be the
   265|     timestamp of the file's last modification. This fn is memoized so if it is
   266|     called with the same path and timestamp (the file's last modified time) the
   267|     second time the result is returned from the memoization.  If the file gets
   268|     modified then the params are different and the key is loaded from disk.
   269|     """
   270|     log.debug("salt.crypt._get_key_with_evict: Loading private key")
   271|     if passphrase:
   272|         password = passphrase.encode()
   273|     else:
   274|         password = None
   275|     with salt.utils.files.fopen(path, "rb") as f:
   276|         return serialization.load_pem_private_key(
   277|             f.read(),
   278|             password=password,
   279|         )
   280| def get_rsa_key(path, passphrase):
   281|     """
   282|     Read a private key off the disk.  Poor man's simple cache in effect here,
   283|     we memoize the result of calling _get_rsa_with_evict.  This means the first
   284|     time _get_key_with_evict is called with a path and a timestamp the result
   285|     is cached.  If the file (the private key) does not change then its
   286|     timestamp will not change and the next time the result is returned from the
   287|     cache.  If the key DOES change the next time _get_rsa_with_evict is called
   288|     it is called with different parameters and the fn is run fully to retrieve
   289|     the key from disk.
   290|     """
   291|     log.debug("salt.crypt.get_rsa_key: Loading private key")
   292|     return _get_key_with_evict(path, str(os.path.getmtime(path)), passphrase)
   293| def get_rsa_pub_key(path):
   294|     """
   295|     Read a public key off the disk.
   296|     """
   297|     log.debug("salt.crypt.get_rsa_pub_key: Loading public key")
   298|     try:
   299|         with salt.utils.files.fopen(path, "rb") as fp:
   300|             return serialization.load_pem_public_key(fp.read())
   301|     except ValueError:
   302|         raise InvalidKeyError("Encountered bad RSA public key")
   303|     except cryptography.exceptions.UnsupportedAlgorithm:
   304|         raise InvalidKeyError("Unsupported key algorithm")
   305| def sign_message(privkey_path, message, passphrase=None, algorithm=PKCS1v15_SHA1):
   306|     """
   307|     Use Crypto.Signature.PKCS1_v1_5 to sign a message. Returns the signature.
   308|     """
   309|     return PrivateKey(privkey_path, passphrase).sign(message, algorithm)
   310| def verify_signature(pubkey_path, message, signature, algorithm=PKCS1v15_SHA1):
   311|     """
   312|     Use Crypto.Signature.PKCS1_v1_5 to verify the signature on a message.
   313|     Returns True for valid signature.
   314|     """
   315|     log.debug("salt.crypt.verify_signature: Loading public key")
   316|     return PublicKey(pubkey_path).verify(message, signature, algorithm)
   317| def gen_signature(priv_path, pub_path, sign_path, passphrase=None):
   318|     """
   319|     creates a signature for the given public-key with
   320|     the given private key and writes it to sign_path
   321|     """
   322|     with salt.utils.files.fopen(pub_path) as fp_:
   323|         mpub_64 = fp_.read()
   324|     mpub_sig = sign_message(priv_path, mpub_64, passphrase)
   325|     mpub_sig_64 = binascii.b2a_base64(mpub_sig)
   326|     if os.path.isfile(sign_path):
   327|         return False
   328|     log.trace(
   329|         "Calculating signature for %s with %s",
   330|         os.path.basename(pub_path),
   331|         os.path.basename(priv_path),
   332|     )
   333|     if os.path.isfile(sign_path):
   334|         log.trace(
   335|             "Signature file %s already exists, please remove it first and try again",
   336|             sign_path,
   337|         )
   338|     else:
   339|         with salt.utils.files.fopen(sign_path, "wb+") as sig_f:
   340|             sig_f.write(salt.utils.stringutils.to_bytes(mpub_sig_64))
   341|         log.trace("Wrote signature to %s", sign_path)
   342|     return True
   343| def private_encrypt(key, message):
   344|     """
   345|     Generate an M2Crypto-compatible signature
   346|     :param Crypto.PublicKey.RSA._RSAobj key: The RSA key object
   347|     :param str message: The message to sign
   348|     :rtype: str
   349|     :return: The signature, or an empty string if the signature operation failed
   350|     """
   351|     return key.encrypt(message)
   352| def pwdata_decrypt(rsa_key, pwdata):
   353|     key = serialization.load_pem_private_key(rsa_key.encode(), password=None)
   354|     password = key.decrypt(
   355|         pwdata,
   356|         padding.PKCS1v15(),
   357|     )
   358|     return salt.utils.stringutils.to_unicode(password)
   359| class MasterKeys(dict):
   360|     """
   361|     The Master Keys class is used to manage the RSA public key pair used for
   362|     authentication by the master.
   363|     It also generates a signing key-pair if enabled with master_sign_key_name.
   364|     """
   365|     def __init__(self, opts):
   366|         super().__init__()
   367|         self.opts = opts
   368|         self.pub_path = os.path.join(self.opts["pki_dir"], "master.pub")
   369|         self.rsa_path = os.path.join(self.opts["pki_dir"], "master.pem")
   370|         key_pass = salt.utils.sdb.sdb_get(self.opts["key_pass"], self.opts)
   371|         self.key = self.__get_keys(passphrase=key_pass)
   372|         self.pub_signature = None
   373|         if opts["master_sign_pubkey"]:
   374|             if opts["master_use_pubkey_signature"]:
   375|                 self.sig_path = os.path.join(
   376|                     self.opts["pki_dir"], opts["master_pubkey_signature"]
   377|                 )
   378|                 if os.path.isfile(self.sig_path):
   379|                     with salt.utils.files.fopen(self.sig_path) as fp_:
   380|                         self.pub_signature = clean_key(fp_.read())
   381|                     log.info(
   382|                         "Read %s's signature from %s",
   383|                         os.path.basename(self.pub_path),
   384|                         self.opts["master_pubkey_signature"],
   385|                     )
   386|                 else:
   387|                     log.error(
   388|                         "Signing the master.pub key with a signature is "
   389|                         "enabled but no signature file found at the defined "
   390|                         "location %s",
   391|                         self.sig_path,

# --- HUNK 2: Lines 393-598 ---
   393|                     log.error(
   394|                         "The signature-file may be either named differently "
   395|                         "or has to be created with 'salt-key --gen-signature'"
   396|                     )
   397|                     sys.exit(1)
   398|             else:
   399|                 key_pass = salt.utils.sdb.sdb_get(
   400|                     self.opts["signing_key_pass"], self.opts
   401|                 )
   402|                 self.pub_sign_path = os.path.join(
   403|                     self.opts["pki_dir"], opts["master_sign_key_name"] + ".pub"
   404|                 )
   405|                 self.rsa_sign_path = os.path.join(
   406|                     self.opts["pki_dir"], opts["master_sign_key_name"] + ".pem"
   407|                 )
   408|                 self.sign_key = self.__get_keys(name=opts["master_sign_key_name"])
   409|     def __setstate__(self, state):
   410|         self.__init__(state["opts"])
   411|     def __getstate__(self):
   412|         return {"opts": self.opts}
   413|     def __get_keys(self, name="master", passphrase=None):
   414|         """
   415|         Returns a key object for a key in the pki-dir
   416|         """
   417|         path = os.path.join(self.opts["pki_dir"], name + ".pem")
   418|         if not os.path.exists(path):
   419|             log.info("Generating %s keys: %s", name, self.opts["pki_dir"])
   420|             gen_keys(
   421|                 self.opts["pki_dir"],
   422|                 name,
   423|                 self.opts["keysize"],
   424|                 self.opts.get("user"),
   425|                 passphrase,
   426|             )
   427|         try:
   428|             key = PrivateKey(path, passphrase)
   429|         except ValueError as e:
   430|             message = f"Unable to read key: {path}; file may be corrupt"
   431|         except TypeError as e:
   432|             message = f"Unable to read key: {path}; passphrase may be incorrect"
   433|         except InvalidKeyError as e:
   434|             message = f"Unable to read key: {path}; key contains unsupported algorithm"
   435|         except cryptography.exceptions.UnsupportedAlgorithm as e:
   436|             message = f"Unable to read key: {path}; key contains unsupported algorithm"
   437|         else:
   438|             log.debug("Loaded %s key: %s", name, path)
   439|             return key
   440|         log.error(message)
   441|         raise MasterExit(message)
   442|     def get_pub_str(self, name="master"):
   443|         """
   444|         Return the string representation of a public key
   445|         in the pki-directory
   446|         """
   447|         path = os.path.join(self.opts["pki_dir"], name + ".pub")
   448|         if not os.path.isfile(path):
   449|             pubkey = self.key.public_key()
   450|             with salt.utils.files.fopen(path, "wb+") as f:
   451|                 f.write(
   452|                     pubkey.public_bytes(
   453|                         encoding=serialization.Encoding.PEM,
   454|                         format=serialization.PublicFormat.SubjectPublicKeyInfo,
   455|                     )
   456|                 )
   457|         with salt.utils.files.fopen(path) as rfh:
   458|             return clean_key(rfh.read())
   459|     def get_mkey_paths(self):
   460|         return self.pub_path, self.rsa_path
   461|     def get_sign_paths(self):
   462|         return self.pub_sign_path, self.rsa_sign_path
   463|     def pubkey_signature(self):
   464|         """
   465|         returns the base64 encoded signature from the signature file
   466|         or None if the master has its own signing keys
   467|         """
   468|         return self.pub_signature
   469| class AsyncAuth:
   470|     """
   471|     Set up an Async object to maintain authentication with the salt master
   472|     """
   473|     instance_map = weakref.WeakKeyDictionary()
   474|     creds_map = {}
   475|     def __new__(cls, opts, io_loop=None):
   476|         """
   477|         Only create one instance of AsyncAuth per __key()
   478|         """
   479|         io_loop = io_loop or salt.ext.tornado.ioloop.IOLoop.current()
   480|         if io_loop not in AsyncAuth.instance_map:
   481|             AsyncAuth.instance_map[io_loop] = weakref.WeakValueDictionary()
   482|         loop_instance_map = AsyncAuth.instance_map[io_loop]
   483|         key = cls.__key(opts)
   484|         auth = loop_instance_map.get(key)
   485|         if auth is None:
   486|             log.debug("Initializing new AsyncAuth for %s", key)
   487|             auth = object.__new__(cls)
   488|             auth.__singleton_init__(opts, io_loop=io_loop)
   489|             loop_instance_map[key] = auth
   490|         else:
   491|             log.debug("Re-using AsyncAuth for %s", key)
   492|         return auth
   493|     @classmethod
   494|     def __key(cls, opts, io_loop=None):
   495|         return (
   496|             opts["pki_dir"],  # where the keys are stored
   497|             opts["id"],  # minion ID
   498|             opts["master_uri"],  # master ID
   499|         )
   500|     def __init__(self, opts, io_loop=None):
   501|         pass
   502|     def __singleton_init__(self, opts, io_loop=None):
   503|         """
   504|         Init an Auth instance
   505|         :param dict opts: Options for this server
   506|         :return: Auth instance
   507|         :rtype: Auth
   508|         """
   509|         self.opts = opts
   510|         self.token = salt.utils.stringutils.to_bytes(Crypticle.generate_key_string())
   511|         self.pub_path = os.path.join(self.opts["pki_dir"], "minion.pub")
   512|         self.rsa_path = os.path.join(self.opts["pki_dir"], "minion.pem")
   513|         if self.opts["__role"] == "syndic":
   514|             self.mpub = "syndic_master.pub"
   515|         else:
   516|             self.mpub = "minion_master.pub"
   517|         if not os.path.isfile(self.pub_path):
   518|             self.get_keys()
   519|         self.io_loop = io_loop or salt.ext.tornado.ioloop.IOLoop.current()
   520|         key = self.__key(self.opts)
   521|         if key in AsyncAuth.creds_map:
   522|             creds = AsyncAuth.creds_map[key]
   523|             self._creds = creds
   524|             self._crypticle = Crypticle(self.opts, creds["aes"])
   525|             self._authenticate_future = salt.ext.tornado.concurrent.Future()
   526|             self._authenticate_future.set_result(True)
   527|         else:
   528|             self.authenticate()
   529|     def __deepcopy__(self, memo):
   530|         cls = self.__class__
   531|         result = cls.__new__(cls, copy.deepcopy(self.opts, memo))
   532|         memo[id(self)] = result
   533|         for key in self.__dict__:
   534|             if key in ("io_loop",):
   535|                 continue
   536|             setattr(result, key, copy.deepcopy(self.__dict__[key], memo))
   537|         return result
   538|     @property
   539|     def creds(self):
   540|         return self._creds
   541|     @property
   542|     def crypticle(self):
   543|         return self._crypticle
   544|     @property
   545|     def authenticated(self):
   546|         return (
   547|             hasattr(self, "_authenticate_future")
   548|             and self._authenticate_future.done()
   549|             and self._authenticate_future.exception() is None
   550|         )
   551|     def invalidate(self):
   552|         if self.authenticated:
   553|             del self._authenticate_future
   554|             key = self.__key(self.opts)
   555|             if key in AsyncAuth.creds_map:
   556|                 del AsyncAuth.creds_map[key]
   557|     def authenticate(self, callback=None):
   558|         """
   559|         Ask for this client to reconnect to the origin
   560|         This function will de-dupe all calls here and return a *single* future
   561|         for the sign-in-- whis way callers can all assume there aren't others
   562|         """
   563|         if (
   564|             hasattr(self, "_authenticate_future")
   565|             and not self._authenticate_future.done()
   566|         ):
   567|             future = self._authenticate_future
   568|         else:
   569|             future = salt.ext.tornado.concurrent.Future()
   570|             self._authenticate_future = future
   571|             self.io_loop.add_callback(self._authenticate)
   572|         if callback is not None:
   573|             def handle_future(future):
   574|                 response = future.result()
   575|                 self.io_loop.add_callback(callback, response)
   576|             future.add_done_callback(handle_future)
   577|         return future
   578|     @salt.ext.tornado.gen.coroutine
   579|     def _authenticate(self):
   580|         """
   581|         Authenticate with the master, this method breaks the functional
   582|         paradigm, it will update the master information from a fresh sign
   583|         in, signing in can occur as often as needed to keep up with the
   584|         revolving master AES key.
   585|         :rtype: Crypticle
   586|         :returns: A crypticle used for encryption operations
   587|         """
   588|         acceptance_wait_time = self.opts["acceptance_wait_time"]
   589|         acceptance_wait_time_max = self.opts["acceptance_wait_time_max"]
   590|         if not acceptance_wait_time_max:
   591|             acceptance_wait_time_max = acceptance_wait_time
   592|         creds = None
   593|         with salt.channel.client.AsyncReqChannel.factory(
   594|             self.opts, crypt="clear", io_loop=self.io_loop
   595|         ) as channel:
   596|             error = None
   597|             while True:
   598|                 try:

# --- HUNK 3: Lines 605-787 ---
   605|                         error = SaltClientError("Detect mode is on")
   606|                         break
   607|                     if self.opts.get("caller"):
   608|                         if self.opts.get("local_masters", None):
   609|                             error = SaltClientError(
   610|                                 "Minion failed to authenticate"
   611|                                 " with the master, has the "
   612|                                 "minion key been accepted?"
   613|                             )
   614|                             break
   615|                         else:
   616|                             print(
   617|                                 "Minion failed to authenticate with the master, "
   618|                                 "has the minion key been accepted?"
   619|                             )
   620|                             sys.exit(2)
   621|                     if acceptance_wait_time:
   622|                         log.info(
   623|                             "Waiting %s seconds before retry.", acceptance_wait_time
   624|                         )
   625|                         yield salt.ext.tornado.gen.sleep(acceptance_wait_time)
   626|                     if acceptance_wait_time < acceptance_wait_time_max:
   627|                         acceptance_wait_time += acceptance_wait_time
   628|                         log.debug(
   629|                             "Authentication wait time is %s", acceptance_wait_time
   630|                         )
   631|                     continue
   632|                 elif creds == "bad enc algo":
   633|                     log.error(
   634|                         "This minion is using a encryption algorithm that is "
   635|                         "not supported by it's Master. Please check your minion configutation."
   636|                     )
   637|                     break
   638|                 elif creds == "bad sig algo":
   639|                     log.error(
   640|                         "This minion is using a signing algorithm that is "
   641|                         "not supported by it's Master. Please check your minion configutation."
   642|                     )
   643|                     break
   644|                 break
   645|             if not isinstance(creds, dict) or "aes" not in creds:
   646|                 if self.opts.get("detect_mode") is True:
   647|                     error = SaltClientError("-|RETRY|-")
   648|                 try:
   649|                     del AsyncAuth.creds_map[self.__key(self.opts)]
   650|                 except KeyError:
   651|                     pass
   652|                 if not error:
   653|                     error = SaltClientError(
   654|                         "Attempt to authenticate with the salt master failed"
   655|                     )
   656|                 self._authenticate_future.set_exception(error)
   657|             else:
   658|                 key = self.__key(self.opts)
   659|                 if key not in AsyncAuth.creds_map:
   660|                     log.debug("%s Got new master aes key.", self)
   661|                     AsyncAuth.creds_map[key] = creds
   662|                     self._creds = creds
   663|                     self._crypticle = Crypticle(self.opts, creds["aes"])
   664|                 elif self._creds["aes"] != creds["aes"]:
   665|                     log.debug("%s The master's aes key has changed.", self)
   666|                     AsyncAuth.creds_map[key] = creds
   667|                     self._creds = creds
   668|                     self._crypticle = Crypticle(self.opts, creds["aes"])
   669|                 self._authenticate_future.set_result(
   670|                     True
   671|                 )  # mark the sign-in as complete
   672|                 if self.opts.get("auth_events") is True:
   673|                     with salt.utils.event.get_event(
   674|                         self.opts.get("__role"), opts=self.opts, listen=False
   675|                     ) as event:
   676|                         event.fire_event(
   677|                             {"key": key, "creds": creds},
   678|                             salt.utils.event.tagify(prefix="auth", suffix="creds"),
   679|                         )
   680|     @salt.ext.tornado.gen.coroutine
   681|     def sign_in(self, timeout=60, safe=True, tries=1, channel=None):
   682|         """
   683|         Send a sign in request to the master, sets the key information and
   684|         returns a dict containing the master publish interface to bind to
   685|         and the decrypted aes key for transport decryption.
   686|         :param int timeout: Number of seconds to wait before timing out the sign-in request
   687|         :param bool safe: If True, do not raise an exception on timeout. Retry instead.
   688|         :param int tries: The number of times to try to authenticate before giving up.
   689|         :raises SaltReqTimeoutError: If the sign-in request has timed out and :param safe: is not set
   690|         :return: Return a string on failure indicating the reason for failure. On success, return a dictionary
   691|         with the publication port and the shared AES key.
   692|         """
   693|         auth_timeout = self.opts.get("auth_timeout", None)
   694|         if auth_timeout is not None:
   695|             timeout = auth_timeout
   696|         auth_safemode = self.opts.get("auth_safemode", None)
   697|         if auth_safemode is not None:
   698|             safe = auth_safemode
   699|         auth_tries = self.opts.get("auth_tries", None)
   700|         if auth_tries is not None:
   701|             tries = auth_tries
   702|         close_channel = False
   703|         if not channel:
   704|             close_channel = True
   705|             channel = salt.channel.client.AsyncReqChannel.factory(
   706|                 self.opts, crypt="clear", io_loop=self.io_loop
   707|             )
   708|         sign_in_payload = self.minion_sign_in_payload()
   709|         try:
   710|             payload = yield channel.send(sign_in_payload, tries=tries, timeout=timeout)
   711|         except SaltReqTimeoutError as e:
   712|             if safe:
   713|                 log.warning("SaltReqTimeoutError: %s", e)
   714|                 raise salt.ext.tornado.gen.Return("retry")
   715|             if self.opts.get("detect_mode") is True:
   716|                 raise salt.ext.tornado.gen.Return("retry")
   717|             else:
   718|                 raise SaltClientError(
   719|                     "Attempt to authenticate with the salt master failed with timeout"
   720|                     " error"
   721|                 )
   722|         finally:
   723|             if close_channel:
   724|                 channel.close()
   725|         ret = self.handle_signin_response(sign_in_payload, payload)
   726|         raise salt.ext.tornado.gen.Return(ret)
   727|     def handle_signin_response(self, sign_in_payload, payload):
   728|         auth = {}
   729|         m_pub_fn = os.path.join(self.opts["pki_dir"], self.mpub)
   730|         auth["master_uri"] = self.opts["master_uri"]
   731|         if not isinstance(payload, dict) or "load" not in payload:
   732|             log.error("Sign-in attempt failed: %s", payload)
   733|             return False
   734|         elif isinstance(payload["load"], dict) and "ret" in payload["load"]:
   735|             if payload["load"]["ret"] == "bad enc algo":
   736|                 log.error("Sign-in attempt failed: %s", payload)
   737|                 return "bad enc algo"
   738|             elif payload["load"]["ret"] == "bad sig algo":
   739|                 log.error("Sign-in attempt failed: %s", payload)
   740|                 return "bad sig algo"
   741|         clear_signed_data = payload["load"]
   742|         clear_signature = payload["sig"]
   743|         payload = salt.payload.loads(clear_signed_data)
   744|         if "pub_key" in payload:
   745|             auth["aes"] = self.verify_master(
   746|                 payload, master_pub="token" in sign_in_payload
   747|             )
   748|             if not auth["aes"]:
   749|                 log.critical(
   750|                     "The Salt Master server's public key did not authenticate!\n"
   751|                     "The master may need to be updated if it is a version of Salt "
   752|                     "lower than %s, or\n"
   753|                     "If you are confident that you are connecting to a valid Salt "
   754|                     "Master, then remove the master public key and restart the "
   755|                     "Salt Minion.\nThe master public key can be found "
   756|                     "at:\n%s",
   757|                     salt.version.__version__,
   758|                     m_pub_fn,
   759|                 )
   760|                 raise SaltClientError("Invalid master key")
   761|         master_pubkey_path = os.path.join(self.opts["pki_dir"], self.mpub)
   762|         if os.path.exists(master_pubkey_path) and not verify_signature(
   763|             master_pubkey_path,
   764|             clear_signed_data,
   765|             clear_signature,
   766|             algorithm=self.opts["signing_algorithm"],
   767|         ):
   768|             log.critical("The payload signature did not validate.")
   769|             raise SaltClientError("Invalid signature")
   770|         if payload["nonce"] != sign_in_payload["nonce"]:
   771|             log.critical("The payload nonce did not validate.")
   772|             raise SaltClientError("Invalid nonce")
   773|         if "ret" in payload:
   774|             if not payload["ret"]:
   775|                 if self.opts["rejected_retry"]:
   776|                     log.error(
   777|                         "The Salt Master has rejected this minion's public "
   778|                         "key.\nTo repair this issue, delete the public key "
   779|                         "for this minion on the Salt Master.\nThe Salt "
   780|                         "Minion will attempt to re-authenicate."
   781|                     )
   782|                     return "retry"
   783|                 else:
   784|                     log.critical(
   785|                         "The Salt Master has rejected this minion's public "
   786|                         "key!\nTo repair this issue, delete the public key "
   787|                         "for this minion on the Salt Master and restart this "

# --- HUNK 4: Lines 822-972 ---
   822|                 ):
   823|                     self._finger_fail(self.opts["master_finger"], m_pub_fn)
   824|         auth["publish_port"] = payload["publish_port"]
   825|         return auth
   826|     def get_keys(self):
   827|         """
   828|         Return keypair object for the minion.
   829|         :rtype: Crypto.PublicKey.RSA._RSAobj
   830|         :return: The RSA keypair
   831|         """
   832|         user = self.opts.get("user", "root")
   833|         salt.utils.verify.check_path_traversal(self.opts["pki_dir"], user)
   834|         if not os.path.exists(self.rsa_path):
   835|             log.info("Generating keys: %s", self.opts["pki_dir"])
   836|             gen_keys(
   837|                 self.opts["pki_dir"],
   838|                 "minion",
   839|                 self.opts["keysize"],
   840|                 self.opts.get("user"),
   841|             )
   842|         key = PrivateKey(self.rsa_path, None)
   843|         log.debug("Loaded minion key: %s", self.rsa_path)
   844|         return key
   845|     def gen_token(self, clear_tok):
   846|         """
   847|         Encrypt a string with the minion private key to verify identity
   848|         with the master.
   849|         :param str clear_tok: A plaintext token to encrypt
   850|         :return: Encrypted token
   851|         :rtype: str
   852|         """
   853|         return private_encrypt(self.get_keys(), clear_tok)
   854|     def minion_sign_in_payload(self):
   855|         """
   856|         Generates the payload used to authenticate with the master
   857|         server. This payload consists of the passed in id_ and the ssh
   858|         public key to encrypt the AES key sent back from the master.
   859|         :return: Payload dictionary
   860|         :rtype: dict
   861|         """
   862|         payload = {}
   863|         payload["cmd"] = "_auth"
   864|         payload["id"] = self.opts["id"]
   865|         payload["nonce"] = uuid.uuid4().hex
   866|         payload["enc_algo"] = self.opts["encryption_algorithm"]
   867|         payload["sig_algo"] = self.opts["signing_algorithm"]
   868|         if "autosign_grains" in self.opts:
   869|             autosign_grains = {}
   870|             for grain in self.opts["autosign_grains"]:
   871|                 autosign_grains[grain] = self.opts["grains"].get(grain, None)
   872|             payload["autosign_grains"] = autosign_grains
   873|         try:
   874|             pubkey_path = os.path.join(self.opts["pki_dir"], self.mpub)
   875|             pub = PublicKey(pubkey_path)
   876|             payload["token"] = pub.encrypt(
   877|                 self.token, self.opts["encryption_algorithm"]
   878|             )
   879|         except FileNotFoundError:
   880|             log.debug("Master public key not found")
   881|         except Exception as exc:  # pylint: disable=broad-except
   882|             log.debug("Exception while encrypting token %s", exc)
   883|         with salt.utils.files.fopen(self.pub_path) as f:
   884|             payload["pub"] = clean_key(f.read())
   885|         return payload
   886|     def decrypt_aes(self, payload, master_pub=True):
   887|         """
   888|         This function is used to decrypt the AES seed phrase returned from
   889|         the master server. The seed phrase is decrypted with the SSH RSA
   890|         host key.
   891|         Pass in the encrypted AES key.
   892|         Returns the decrypted AES seed key, a string
   893|         :param dict payload: The incoming payload. This is a dictionary which may have the following keys:
   894|             'aes': The shared AES key
   895|             'enc': The format of the message. ('clear', 'pub', etc)
   896|             'sig': The message signature
   897|             'publish_port': The TCP port which published the message
   898|             'token': The encrypted token used to verify the message.
   899|             'pub_key': The public key of the sender.
   900|         :rtype: str
   901|         :return: The decrypted token that was provided, with padding.
   902|         :rtype: str
   903|         :return: The decrypted AES seed key
   904|         """
   905|         if self.opts.get("auth_trb", False):
   906|             log.warning("Auth Called: %s", "".join(traceback.format_stack()))
   907|         else:
   908|             log.debug("Decrypting the current master AES key")
   909|         key = self.get_keys()
   910|         key_str = key.decrypt(payload["aes"], self.opts["encryption_algorithm"])
   911|         if "sig" in payload:
   912|             m_path = os.path.join(self.opts["pki_dir"], self.mpub)
   913|             if os.path.exists(m_path):
   914|                 try:
   915|                     mkey = PublicKey(m_path)
   916|                 except Exception:  # pylint: disable=broad-except
   917|                     return "", ""
   918|                 digest = hashlib.sha256(key_str).hexdigest()
   919|                 digest = salt.utils.stringutils.to_bytes(digest)
   920|                 m_digest = mkey.decrypt(payload["sig"])
   921|                 if m_digest != digest:
   922|                     return "", ""
   923|         else:
   924|             return "", ""
   925|         key_str = salt.utils.stringutils.to_str(key_str)
   926|         if "_|-" in key_str:
   927|             return key_str.split("_|-")
   928|         else:
   929|             if "token" in payload:
   930|                 token = key.decrypt(payload["token"], self.opts["encryption_algorithm"])
   931|                 return key_str, token
   932|             elif not master_pub:
   933|                 return key_str, ""
   934|         return "", ""
   935|     def verify_pubkey_sig(self, message, sig):
   936|         """
   937|         Wraps the verify_signature method so we have
   938|         additional checks.
   939|         :rtype: bool
   940|         :return: Success or failure of public key verification
   941|         """
   942|         if self.opts["master_sign_key_name"]:
   943|             path = os.path.join(
   944|                 self.opts["pki_dir"], self.opts["master_sign_key_name"] + ".pub"
   945|             )
   946|             if os.path.isfile(path):
   947|                 res = verify_signature(
   948|                     path,
   949|                     message,
   950|                     binascii.a2b_base64(sig),
   951|                     algorithm=self.opts["signing_algorithm"],
   952|                 )
   953|             else:
   954|                 log.error(
   955|                     "Verification public key %s does not exist. You need to "
   956|                     "copy it from the master to the minions pki directory",
   957|                     os.path.basename(path),
   958|                 )
   959|                 return False
   960|             if res:
   961|                 log.debug(
   962|                     "Successfully verified signature of master public key "
   963|                     "with verification public key %s",
   964|                     self.opts["master_sign_key_name"] + ".pub",
   965|                 )
   966|                 return True
   967|             else:
   968|                 log.debug("Failed to verify signature of public key")
   969|                 return False
   970|         else:
   971|             log.error(
   972|                 "Failed to verify the signature of the message because the "

# --- HUNK 5: Lines 1279-1386 ---
  1279|             )
  1280|         finally:
  1281|             if close_channel:
  1282|                 channel.close()
  1283|         return self.handle_signin_response(sign_in_payload, payload)
  1284| class Crypticle:
  1285|     """
  1286|     Authenticated encryption class
  1287|     Encryption algorithm: AES-CBC
  1288|     Signing algorithm: HMAC-SHA256
  1289|     """
  1290|     PICKLE_PAD = b"pickle::"
  1291|     AES_BLOCK_SIZE = 16
  1292|     SIG_SIZE = hashlib.sha256().digest_size
  1293|     def __init__(self, opts, key_string, key_size=192, serial=0):
  1294|         self.key_string = key_string
  1295|         self.keys = self.extract_keys(self.key_string, key_size)
  1296|         self.key_size = key_size
  1297|         self.serial = serial
  1298|     @classmethod
  1299|     def generate_key_string(cls, key_size=192):
  1300|         key = os.urandom(key_size // 8 + cls.SIG_SIZE)
  1301|         b64key = base64.b64encode(key)
  1302|         b64key = b64key.decode("utf-8")
  1303|         return b64key.replace("\n", "")
  1304|     @classmethod
  1305|     def extract_keys(cls, key_string, key_size):
  1306|         key = salt.utils.stringutils.to_bytes(base64.b64decode(key_string))
  1307|         assert len(key) == key_size / 8 + cls.SIG_SIZE, "invalid key"
  1308|         return key[: -cls.SIG_SIZE], key[-cls.SIG_SIZE :]
  1309|     def encrypt(self, data):
  1310|         """
  1311|         encrypt data with AES-CBC and sign it with HMAC-SHA256
  1312|         """
  1313|         aes_key, hmac_key = self.keys
  1314|         pad = self.AES_BLOCK_SIZE - len(data) % self.AES_BLOCK_SIZE
  1315|         data = data + salt.utils.stringutils.to_bytes(pad * chr(pad))
  1316|         iv_bytes = os.urandom(self.AES_BLOCK_SIZE)
  1317|         cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv_bytes))
  1318|         encryptor = cipher.encryptor()
  1319|         encr = encryptor.update(data)
  1320|         encr += encryptor.finalize()
  1321|         data = iv_bytes + encr
  1322|         sig = hmac.new(hmac_key, data, hashlib.sha256).digest()
  1323|         return data + sig
  1324|     def decrypt(self, data):
  1325|         """
  1326|         verify HMAC-SHA256 signature and decrypt data with AES-CBC
  1327|         """
  1328|         aes_key, hmac_key = self.keys
  1329|         sig = data[-self.SIG_SIZE :]
  1330|         data = data[: -self.SIG_SIZE]
  1331|         if not isinstance(data, bytes):
  1332|             data = salt.utils.stringutils.to_bytes(data)
  1333|         mac_bytes = hmac.new(hmac_key, data, hashlib.sha256).digest()
  1334|         if len(mac_bytes) != len(sig):
  1335|             log.debug("Failed to authenticate message")
  1336|             raise AuthenticationError("message authentication failed")
  1337|         result = 0
  1338|         for zipped_x, zipped_y in zip(mac_bytes, sig):
  1339|             result |= zipped_x ^ zipped_y
  1340|         if result != 0:
  1341|             log.debug("Failed to authenticate message")
  1342|             raise AuthenticationError("message authentication failed")
  1343|         iv_bytes = data[: self.AES_BLOCK_SIZE]
  1344|         data = data[self.AES_BLOCK_SIZE :]
  1345|         cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv_bytes))
  1346|         decryptor = cipher.decryptor()
  1347|         data = decryptor.update(data) + decryptor.finalize()
  1348|         return data[: -data[-1]]
  1349|     def dumps(self, obj, nonce=None):
  1350|         """
  1351|         Serialize and encrypt a python object
  1352|         """
  1353|         if nonce:
  1354|             toencrypt = self.PICKLE_PAD + nonce.encode() + salt.payload.dumps(obj)
  1355|         else:
  1356|             toencrypt = self.PICKLE_PAD + salt.payload.dumps(obj)
  1357|         return self.encrypt(toencrypt)
  1358|     def loads(self, data, raw=False, nonce=None):
  1359|         """
  1360|         Decrypt and un-serialize a python object
  1361|         """
  1362|         data = self.decrypt(data)
  1363|         if not data.startswith(self.PICKLE_PAD):
  1364|             return {}
  1365|         data = data[len(self.PICKLE_PAD) :]
  1366|         if nonce:
  1367|             ret_nonce = data[:32].decode()
  1368|             data = data[32:]
  1369|             if ret_nonce != nonce:
  1370|                 raise SaltClientError("Nonce verification error")
  1371|         payload = salt.payload.loads(data, raw=raw)
  1372|         if isinstance(payload, dict):
  1373|             if "serial" in payload:
  1374|                 serial = payload.pop("serial")
  1375|                 if serial <= self.serial:
  1376|                     log.critical(
  1377|                         "A message with an invalid serial was received.\n"
  1378|                         "this serial: %d\n"
  1379|                         "last serial: %d\n"
  1380|                         "The minion will not honor this request.",
  1381|                         serial,
  1382|                         self.serial,
  1383|                     )
  1384|                     return {}
  1385|                 self.serial = serial
  1386|         return payload


# ====================================================================
# FILE: salt/daemons/masterapi.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 96-136 ---
    96|                     os.remove(cache_file)
    97|                 except OSError as exc:
    98|                     log.critical(
    99|                         "Unable to file_lists cache file %s: %s", cache_file, exc
   100|                     )
   101| def clean_expired_tokens(opts):
   102|     """
   103|     Clean expired tokens from the master
   104|     """
   105|     loadauth = salt.auth.LoadAuth(opts)
   106|     for tok in loadauth.list_tokens():
   107|         token_data = loadauth.get_tok(tok)
   108|         if "expire" not in token_data or token_data.get("expire", 0) < time.time():
   109|             loadauth.rm_token(tok)
   110| def clean_pub_auth(opts):
   111|     try:
   112|         auth_cache = os.path.join(opts["cachedir"], "publish_auth")
   113|         if not os.path.exists(auth_cache):
   114|             return
   115|         else:
   116|             for dirpath, dirnames, filenames in salt.utils.path.os_walk(auth_cache):
   117|                 for auth_file in filenames:
   118|                     auth_file_path = os.path.join(dirpath, auth_file)
   119|                     if not os.path.isfile(auth_file_path):
   120|                         continue
   121|                     if time.time() - os.path.getmtime(auth_file_path) > (
   122|                         salt.utils.job.get_keep_jobs_seconds(opts)
   123|                     ):
   124|                         os.remove(auth_file_path)
   125|     except OSError:
   126|         log.error("Unable to delete pub auth file")
   127| def clean_old_jobs(opts):
   128|     """
   129|     Clean out the old jobs from the job cache
   130|     """
   131|     mminion = salt.minion.MasterMinion(
   132|         opts,
   133|         states=False,
   134|         rend=False,
   135|     )
   136|     fstr = "{}.clean_old_jobs".format(opts["master_job_cache"])

# --- HUNK 2: Lines 162-212 ---
   162|     os.chmod(keyfile, 0o600)
   163|     if HAS_PWD and uid is not None:
   164|         try:
   165|             os.chown(keyfile, uid, -1)
   166|         except OSError:
   167|             pass
   168|     return key
   169| def access_keys(opts):
   170|     """
   171|     A key needs to be placed in the filesystem with permissions 0400 so
   172|     clients are required to run as root.
   173|     """
   174|     keys = {}
   175|     publisher_acl = opts["publisher_acl"]
   176|     acl_users = set(publisher_acl.keys())
   177|     if opts.get("user"):
   178|         acl_users.add(opts["user"])
   179|     acl_users.add(salt.utils.user.get_user())
   180|     for user in acl_users:
   181|         log.info("Preparing the %s key for local communication", user)
   182|         key = mk_key(opts, user)
   183|         if key is not None:
   184|             keys[user] = key
   185|     if opts["client_acl_verify"] and HAS_PWD:
   186|         log.profile("Beginning pwd.getpwall() call in masterapi access_keys function")
   187|         for user in pwd.getpwall():
   188|             user = user.pw_name
   189|             if user not in keys and salt.utils.stringutils.check_whitelist_blacklist(
   190|                 user, whitelist=acl_users
   191|             ):
   192|                 keys[user] = mk_key(opts, user)
   193|         log.profile("End pwd.getpwall() call in masterapi access_keys function")
   194|     return keys
   195| def fileserver_update(fileserver):
   196|     """
   197|     Update the fileserver backends, requires that a salt.fileserver.Fileserver
   198|     object be passed in
   199|     """
   200|     try:
   201|         if not fileserver.servers:
   202|             log.error(
   203|                 "No fileservers loaded, the master will not be able to "
   204|                 "serve files to minions"
   205|             )
   206|             raise salt.exceptions.SaltMasterError("No fileserver backends available")
   207|         fileserver.update()
   208|     except Exception as exc:  # pylint: disable=broad-except
   209|         log.error(
   210|             "Exception %s occurred in file server update",
   211|             exc,
   212|             exc_info_on_loglevel=logging.DEBUG,

# --- HUNK 3: Lines 243-320 ---
   243|         if not self.check_permissions(signing_file):
   244|             log.warning("Wrong permissions for %s, ignoring content", signing_file)
   245|             return False
   246|         mtime = os.path.getmtime(signing_file)
   247|         if self.signing_files.get(signing_file, {}).get("mtime") != mtime:
   248|             self.signing_files.setdefault(signing_file, {})["mtime"] = mtime
   249|             with salt.utils.files.fopen(signing_file, "r") as fp_:
   250|                 self.signing_files[signing_file]["data"] = [
   251|                     entry
   252|                     for entry in [line.strip() for line in fp_]
   253|                     if not entry.strip().startswith("#")
   254|                 ]
   255|         return any(
   256|             salt.utils.stringutils.expr_match(keyid, line)
   257|             for line in self.signing_files[signing_file].get("data", [])
   258|         )
   259|     def check_autosign_dir(self, keyid):
   260|         """
   261|         Check a keyid for membership in a autosign directory.
   262|         """
   263|         autosign_dir = os.path.join(self.opts["pki_dir"], "minions_autosign")
   264|         expire_minutes = self.opts.get("autosign_timeout", 120)
   265|         if expire_minutes > 0:
   266|             min_time = time.time() - (60 * int(expire_minutes))
   267|             for root, dirs, filenames in salt.utils.path.os_walk(autosign_dir):
   268|                 for f in filenames:
   269|                     stub_file = os.path.join(autosign_dir, f)
   270|                     mtime = os.path.getmtime(stub_file)
   271|                     if mtime < min_time:
   272|                         log.warning("Autosign keyid expired %s", stub_file)
   273|                         os.remove(stub_file)
   274|         stub_file = os.path.join(autosign_dir, keyid)
   275|         if not os.path.exists(stub_file):
   276|             return False
   277|         os.remove(stub_file)
   278|         return True
   279|     def check_autosign_grains(self, autosign_grains):
   280|         """
   281|         Check for matching grains in the autosign_grains_dir.
   282|         """
   283|         if not autosign_grains or "autosign_grains_dir" not in self.opts:
   284|             return False
   285|         autosign_grains_dir = self.opts["autosign_grains_dir"]
   286|         for root, dirs, filenames in os.walk(autosign_grains_dir):
   287|             for grain in filenames:
   288|                 if grain in autosign_grains:
   289|                     grain_file = os.path.join(autosign_grains_dir, grain)
   290|                     if not self.check_permissions(grain_file):
   291|                         log.warning(
   292|                             "Wrong permissions for %s, ignoring content", grain_file
   293|                         )
   294|                         continue
   295|                     with salt.utils.files.fopen(grain_file, "r") as f:
   296|                         for line in f:
   297|                             line = salt.utils.stringutils.to_unicode(line).strip()
   298|                             if line.startswith("#"):
   299|                                 continue
   300|                             if autosign_grains[grain] == line:
   301|                                 return True
   302|         return False
   303|     def check_autoreject(self, keyid):
   304|         """
   305|         Checks if the specified keyid should automatically be rejected.
   306|         """
   307|         return self.check_signing_file(keyid, self.opts.get("autoreject_file", None))
   308|     def check_autosign(self, keyid, autosign_grains=None):
   309|         """
   310|         Checks if the specified keyid should automatically be signed.
   311|         """
   312|         if self.opts["auto_accept"]:
   313|             return True
   314|         if self.check_signing_file(keyid, self.opts.get("autosign_file", None)):
   315|             return True
   316|         if self.check_autosign_dir(keyid):
   317|             return True
   318|         if self.check_autosign_grains(autosign_grains):
   319|             return True
   320|         return False

# --- HUNK 4: Lines 593-633 ---
   593|         if "loc" in load and load["loc"] < 0:
   594|             log.error("Invalid file pointer: load[loc] < 0")
   595|             return False
   596|         if load.get("size", 0) > file_recv_max_size:
   597|             log.error("Exceeding file_recv_max_size limit: %s", file_recv_max_size)
   598|             return False
   599|         if len(load["data"]) + load.get("loc", 0) > file_recv_max_size:
   600|             log.error("Exceeding file_recv_max_size limit: %s", file_recv_max_size)
   601|             return False
   602|         normpath = load["path"]
   603|         if ":" in normpath:
   604|             normpath = normpath.replace("\\", "/")
   605|             normpath = os.path.normpath(normpath)
   606|         cpath = os.path.join(
   607|             self.opts["cachedir"], "minions", load["id"], "files", normpath
   608|         )
   609|         cdir = os.path.dirname(cpath)
   610|         if not os.path.isdir(cdir):
   611|             try:
   612|                 os.makedirs(cdir)
   613|             except OSError:
   614|                 pass
   615|         if os.path.isfile(cpath) and load["loc"] != 0:
   616|             mode = "ab"
   617|         else:
   618|             mode = "wb"
   619|         with salt.utils.files.fopen(cpath, mode) as fp_:
   620|             if load["loc"]:
   621|                 fp_.seek(load["loc"])
   622|             fp_.write(salt.utils.stringutils.to_str(load["data"]))
   623|         return True
   624|     def _pillar(self, load):
   625|         """
   626|         Return the pillar data for the minion
   627|         """
   628|         if any(key not in load for key in ("id", "grains")):
   629|             return False
   630|         log.debug("Master _pillar using ext: %s", load.get("ext"))
   631|         pillar = salt.pillar.get_pillar(
   632|             self.opts,
   633|             load["grains"],


# ====================================================================
# FILE: salt/engines/docker_events.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| """
     2| Send events from Docker events
     3| :Depends:   Docker API >= 1.22
     4| """
     5| import logging
     6| import traceback
     7| import salt.utils.event
     8| import salt.utils.json
     9| try:
    10|     import docker  # pylint: disable=import-error,no-name-in-module
    11|     import docker.utils  # pylint: disable=import-error,no-name-in-module
    12|     HAS_DOCKER_PY = True
    13| except ImportError:
    14|     HAS_DOCKER_PY = False
    15| log = logging.getLogger(__name__)  # pylint: disable=invalid-name
    16| CLIENT_TIMEOUT = 60
    17| __virtualname__ = "docker_events"
    18| def __virtual__():
    19|     """
    20|     Only load if docker libs are present
    21|     """
    22|     if not HAS_DOCKER_PY:
    23|         return (False, "Docker_events engine could not be imported")
    24|     return True
    25| def start(
    26|     docker_url="unix://var/run/docker.sock",
    27|     timeout=CLIENT_TIMEOUT,
    28|     tag="salt/engines/docker_events",
    29|     filters=None,
    30| ):
    31|     """
    32|     Scan for Docker events and fire events
    33|     Example Config
    34|     .. code-block:: yaml
    35|         engines:
    36|           - docker_events:
    37|               docker_url: unix://var/run/docker.sock


# ====================================================================
# FILE: salt/engines/ircbot.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 28-112 ---
    28| event <tag> [<extra>, <data>]
    29|     fire event on the master or minion event stream with the tag `salt/engines/ircbot/<tag>` and a data object with a
    30|     list of everything else sent in the message
    31| Example of usage
    32| .. code-block:: text
    33|     08:33:57 @gtmanfred > !ping
    34|     08:33:57   gtmanbot > gtmanfred: pong
    35|     08:34:02 @gtmanfred > !echo ping
    36|     08:34:02   gtmanbot > ping
    37|     08:34:17 @gtmanfred > !event test/tag/ircbot irc is useful
    38|     08:34:17   gtmanbot > gtmanfred: TaDa!
    39| .. code-block:: text
    40|     [DEBUG   ] Sending event: tag = salt/engines/ircbot/test/tag/ircbot; data = {'_stamp': '2016-11-28T14:34:16.633623', 'data': ['irc', 'is', 'useful']}
    41| """
    42| import base64
    43| import logging
    44| import re
    45| import socket
    46| import ssl
    47| from collections import namedtuple
    48| import salt.ext.tornado.ioloop
    49| import salt.ext.tornado.iostream
    50| import salt.utils.event
    51| log = logging.getLogger(__name__)
    52| Event = namedtuple("Event", "source code line")
    53| PrivEvent = namedtuple("PrivEvent", "source nick user host code channel command line")
    54| class IRCClient:
    55|     def __init__(
    56|         self,
    57|         nick,
    58|         host,
    59|         port=6667,
    60|         username=None,
    61|         password=None,
    62|         channels=None,
    63|         use_ssl=False,
    64|         use_sasl=False,
    65|         char="!",
    66|         allow_hosts=False,
    67|         allow_nicks=False,
    68|         disable_query=True,
    69|     ):
    70|         self.nick = nick
    71|         self.host = host
    72|         self.port = port
    73|         self.username = username or nick
    74|         self.password = password
    75|         self.channels = channels or []
    76|         self.ssl = use_ssl
    77|         self.sasl = use_sasl
    78|         self.char = char
    79|         self.allow_hosts = allow_hosts
    80|         self.allow_nicks = allow_nicks
    81|         self.disable_query = disable_query
    82|         self.io_loop = salt.ext.tornado.ioloop.IOLoop(make_current=False)
    83|         self.io_loop.make_current()
    84|         self._connect()
    85|     def _connect(self):
    86|         _sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)
    87|         if self.ssl is True:
    88|             self._stream = salt.ext.tornado.iostream.SSLIOStream(
    89|                 _sock, ssl_options={"cert_reqs": ssl.CERT_NONE}
    90|             )
    91|         else:
    92|             self._stream = salt.ext.tornado.iostream.IOStream(_sock)
    93|         self._stream.set_close_callback(self.on_closed)
    94|         self._stream.connect((self.host, self.port), self.on_connect)
    95|     def read_messages(self):
    96|         self._stream.read_until("\r\n", self._message)
    97|     @staticmethod
    98|     def _event(line):
    99|         log.debug("Received: %s", line)
   100|         search = re.match(
   101|             "^(?:(?P<source>:[^ ]+) )?(?P<code>[^ ]+)(?: (?P<line>.*))?$", line
   102|         )
   103|         source, code, line = (
   104|             search.group("source"),
   105|             search.group("code"),
   106|             search.group("line"),
   107|         )
   108|         return Event(source, code, line)
   109|     def _allow_host(self, host):
   110|         if isinstance(self.allow_hosts, bool):
   111|             return self.allow_hosts
   112|         else:

# --- HUNK 2: Lines 128-229 ---
   128|         search = re.match(
   129|             "^(?P<channel>[^ ]+) :(?:{}(?P<command>[^ ]+)(?: (?P<line>.*))?)?$".format(
   130|                 self.char
   131|             ),
   132|             event.line,
   133|         )
   134|         if search:
   135|             channel, command, line = (
   136|                 search.group("channel"),
   137|                 search.group("command"),
   138|                 search.group("line"),
   139|             )
   140|             if self.disable_query is True and not channel.startswith("#"):
   141|                 return
   142|             if channel == self.nick:
   143|                 channel = nick
   144|             privevent = PrivEvent(
   145|                 event.source, nick, user, host, event.code, channel, command, line
   146|             )
   147|             if (self._allow_nick(nick) or self._allow_host(host)) and hasattr(
   148|                 self, f"_command_{command}"
   149|             ):
   150|                 getattr(self, f"_command_{command}")(privevent)
   151|     def _command_echo(self, event):
   152|         message = f"PRIVMSG {event.channel} :{event.line}"
   153|         self.send_message(message)
   154|     def _command_ping(self, event):
   155|         message = f"PRIVMSG {event.channel} :{event.nick}: pong"
   156|         self.send_message(message)
   157|     def _command_event(self, event):
   158|         if __opts__.get("__role") == "master":
   159|             fire_master = salt.utils.event.get_master_event(
   160|                 __opts__, __opts__["sock_dir"]
   161|             ).fire_event
   162|         else:
   163|             fire_master = None
   164|         def fire(tag, msg):
   165|             """
   166|             How to fire the event
   167|             """
   168|             if fire_master:
   169|                 fire_master(msg, tag)
   170|             else:
   171|                 __salt__["event.send"](tag, msg)
   172|         args = event.line.split(" ")
   173|         tag = args[0]
   174|         if len(args) > 1:
   175|             payload = {"data": args[1:]}
   176|         else:
   177|             payload = {"data": []}
   178|         fire("salt/engines/ircbot/" + tag, payload)
   179|         message = f"PRIVMSG {event.channel} :{event.nick}: TaDa!"
   180|         self.send_message(message)
   181|     def _message(self, raw):
   182|         raw = raw.rstrip(b"\r\n").decode("utf-8")
   183|         event = self._event(raw)
   184|         if event.code == "PING":
   185|             salt.ext.tornado.ioloop.IOLoop.current().spawn_callback(
   186|                 self.send_message, f"PONG {event.line}"
   187|             )
   188|         elif event.code == "PRIVMSG":
   189|             salt.ext.tornado.ioloop.IOLoop.current().spawn_callback(
   190|                 self._privmsg, event
   191|             )
   192|         self.read_messages()
   193|     def join_channel(self, channel):
   194|         if not channel.startswith("#"):
   195|             channel = "#" + channel
   196|         self.send_message(f"JOIN {channel}")
   197|     def on_connect(self):
   198|         logging.info("on_connect")
   199|         if self.sasl is True:
   200|             self.send_message("CAP REQ :sasl")
   201|         self.send_message(f"NICK {self.nick}")
   202|         self.send_message("USER saltstack 0 * :saltstack")
   203|         if self.password:
   204|             if self.sasl is True:
   205|                 authstring = base64.b64encode(
   206|                     "{0}\x00{0}\x00{1}".format(self.username, self.password).encode()
   207|                 )
   208|                 self.send_message("AUTHENTICATE PLAIN")
   209|                 self.send_message(f"AUTHENTICATE {authstring}")
   210|                 self.send_message("CAP END")
   211|             else:
   212|                 self.send_message(
   213|                     "PRIVMSG NickServ :IDENTIFY {} {}".format(
   214|                         self.username, self.password
   215|                     )
   216|                 )
   217|         for channel in self.channels:
   218|             self.join_channel(channel)
   219|         self.read_messages()
   220|     def on_closed(self):
   221|         logging.info("on_closed")
   222|     def send_message(self, line):
   223|         if isinstance(line, str):
   224|             line = line.encode("utf-8")
   225|         log.debug("Sending:  %s", line)
   226|         self._stream.write(line + b"\r\n")
   227| def start(
   228|     nick,
   229|     host,


# ====================================================================
# FILE: salt/engines/slack.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 642-684 ---
   642|                 if count > 10:
   643|                     log.warning(
   644|                         "Breaking in getting messages because count is exceeded"
   645|                     )
   646|                     break
   647|                 if not msg:
   648|                     count += 1
   649|                     log.warning("Skipping an empty message.")
   650|                     continue  # This one is a dud, get the next message
   651|                 if msg.get("done"):
   652|                     log.trace("msg is done")
   653|                     break
   654|                 if fire_all:
   655|                     log.debug("Firing message to the bus with tag: %s", tag)
   656|                     log.debug("%s %s", tag, msg)
   657|                     self.fire("{}/{}".format(tag, msg["message_data"].get("type")), msg)
   658|                 if control and (len(msg) > 1) and msg.get("cmdline"):
   659|                     channel = self.sc.server.channels.find(msg["channel"])
   660|                     jid = self.run_command_async(msg)
   661|                     log.debug("Submitted a job and got jid: %s", jid)
   662|                     outstanding[jid] = (
   663|                         msg  # record so we can return messages to the caller
   664|                     )
   665|                     channel.send_message(
   666|                         "@{}'s job is submitted as salt jid {}".format(
   667|                             msg["user_name"], jid
   668|                         )
   669|                     )
   670|                 count += 1
   671|             start_time = time.time()
   672|             job_status = self.get_jobs_from_runner(
   673|                 outstanding.keys()
   674|             )  # dict of job_ids:results are returned
   675|             log.trace(
   676|                 "Getting %s jobs status took %s seconds",
   677|                 len(job_status),
   678|                 time.time() - start_time,
   679|             )
   680|             for jid in job_status:
   681|                 result = job_status[jid]["data"]
   682|                 function = job_status[jid]["function"]
   683|                 if result:
   684|                     log.debug("ret to send back is %s", result)

# --- HUNK 2: Lines 746-783 ---
   746|                     cmd,
   747|                     arg=args,
   748|                     kwarg=kwargs,
   749|                     tgt_type=str(tgt_type),
   750|                 )
   751|             log.info("ret from local.cmd_async is %s", job_id)
   752|         return job_id
   753| def start(
   754|     token,
   755|     control=False,
   756|     trigger="!",
   757|     groups=None,
   758|     groups_pillar_name=None,
   759|     fire_all=False,
   760|     tag="salt/engines/slack",
   761| ):
   762|     """
   763|     Listen to slack events and forward them to salt, new version
   764|     """
   765|     salt.utils.versions.warn_until(
   766|         "Argon",
   767|         "This 'slack' engine will be deprecated and "
   768|         "will be replace by the slack_bolt engine. This new "
   769|         "engine will use the new Bolt library from Slack and requires "
   770|         "a Slack app and a Slack bot account.",
   771|     )
   772|     if (not token) or (not token.startswith("xoxb")):
   773|         time.sleep(2)  # don't respawn too quickly
   774|         log.error("Slack bot token not found, bailing...")
   775|         raise UserWarning("Slack Engine bot token not configured")
   776|     try:
   777|         client = SlackClient(token=token)
   778|         message_generator = client.generate_triggered_messages(
   779|             token, trigger, groups, groups_pillar_name
   780|         )
   781|         client.run_commands_from_slack_async(message_generator, fire_all, tag, control)
   782|     except Exception:  # pylint: disable=broad-except
   783|         raise Exception(f"{traceback.format_exc()}")


# ====================================================================
# FILE: salt/engines/webhook.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-26 ---
     1| """
     2| Send events from webhook api
     3| """
     4| import salt.ext.tornado.httpserver
     5| import salt.ext.tornado.ioloop
     6| import salt.ext.tornado.web
     7| import salt.utils.event
     8| def start(address=None, port=5000, ssl_crt=None, ssl_key=None):
     9|     """
    10|     Api to listen for webhooks to send to the reactor.
    11|     Implement the webhook behavior in an engine.
    12|     :py:class:`rest_cherrypy Webhook docs <salt.netapi.rest_cherrypy.app.Webhook>`
    13|     Unlike the rest_cherrypy Webhook, this is only an unauthenticated webhook
    14|     endpoint.  If an authenticated webhook endpoint is needed, use the salt-api
    15|     webhook which runs on the master and authenticates through eauth.
    16|     .. note: This is really meant to be used on the minion, because salt-api
    17|              needs to be run on the master for use with eauth.
    18|     .. warning:: Unauthenticated endpoint
    19|         This engine sends webhook calls to the event stream.  If the engine is
    20|         running on a minion with `file_client: local` the event is sent to the
    21|         minion event stream.  Otherwise it is sent to the master event stream.
    22|     Example Config
    23|     .. code-block:: yaml
    24|         engines:
    25|           - webhook: {}
    26|     .. code-block:: yaml

# --- HUNK 2: Lines 30-71 ---
    30|               address: 10.128.1.145
    31|               ssl_crt: /etc/pki/tls/certs/localhost.crt
    32|               ssl_key: /etc/pki/tls/certs/localhost.key
    33|     .. note: For making an unsigned key, use the following command
    34|              `salt-call --local tls.create_self_signed_cert`
    35|     """
    36|     if __opts__.get("__role") == "master":
    37|         fire_master = salt.utils.event.get_master_event(
    38|             __opts__, __opts__["sock_dir"]
    39|         ).fire_event
    40|     else:
    41|         fire_master = None
    42|     def fire(tag, msg):
    43|         """
    44|         How to fire the event
    45|         """
    46|         if fire_master:
    47|             fire_master(msg, tag)
    48|         else:
    49|             __salt__["event.send"](tag, msg)
    50|     class WebHook(
    51|         salt.ext.tornado.web.RequestHandler
    52|     ):  # pylint: disable=abstract-method
    53|         def post(self, tag):  # pylint: disable=arguments-differ
    54|             body = self.request.body
    55|             headers = self.request.headers
    56|             payload = {
    57|                 "headers": headers if isinstance(headers, dict) else dict(headers),
    58|                 "body": body,
    59|             }
    60|             fire("salt/engines/hook/" + tag, payload)
    61|     application = salt.ext.tornado.web.Application([(r"/(.*)", WebHook)])
    62|     ssl_options = None
    63|     if all([ssl_crt, ssl_key]):
    64|         ssl_options = {"certfile": ssl_crt, "keyfile": ssl_key}
    65|     io_loop = salt.ext.tornado.ioloop.IOLoop(make_current=False)
    66|     io_loop.make_current()
    67|     http_server = salt.ext.tornado.httpserver.HTTPServer(
    68|         application, ssl_options=ssl_options
    69|     )
    70|     http_server.listen(port, address=address)
    71|     io_loop.start()


# ====================================================================
# FILE: salt/executors/docker.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| """
     2| Docker executor module
     3| .. versionadded:: 2019.2.0
     4| Used with the docker proxy minion.
     5| """
     6| __virtualname__ = "docker"
     7| DOCKER_MOD_MAP = {
     8|     "state.sls": "docker.sls",
     9|     "state.apply": "docker.apply",
    10|     "state.highstate": "docker.highstate",
    11| }
    12| def __virtual__():
    13|     if "proxy" not in __opts__:
    14|         return (
    15|             False,
    16|             "Docker executor is only meant to be used with Docker Proxy Minions",
    17|         )
    18|     if __opts__.get("proxy", {}).get("proxytype") != __virtualname__:
    19|         return False, f"Proxytype does not match: {__virtualname__}"
    20|     return True
    21| def execute(opts, data, func, args, kwargs):
    22|     """
    23|     Directly calls the given function with arguments
    24|     """
    25|     if data["fun"] == "saltutil.find_job":
    26|         return __executors__["direct_call.execute"](opts, data, func, args, kwargs)
    27|     if data["fun"] in DOCKER_MOD_MAP:
    28|         return __executors__["direct_call.execute"](
    29|             opts,
    30|             data,
    31|             __salt__[DOCKER_MOD_MAP[data["fun"]]],


# ====================================================================
# FILE: salt/ext/tornado/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-4 ---
     1| """The Tornado web server and tools."""
     2| from __future__ import absolute_import, division, print_function
     3| version = "4.5.3"
     4| version_info = (4, 5, 3, 0)


# ====================================================================
# FILE: salt/ext/tornado/_locale_data.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-66 ---
     1| """Data used by the tornado.locale module."""
     2| from __future__ import absolute_import, division, print_function
     3| LOCALE_NAMES = {
     4|     "af_ZA": {"name_en": u"Afrikaans", "name": u"Afrikaans"},
     5|     "am_ET": {"name_en": u"Amharic", "name": u""},
     6|     "ar_AR": {"name_en": u"Arabic", "name": u""},
     7|     "bg_BG": {"name_en": u"Bulgarian", "name": u""},
     8|     "bn_IN": {"name_en": u"Bengali", "name": u""},
     9|     "bs_BA": {"name_en": u"Bosnian", "name": u"Bosanski"},
    10|     "ca_ES": {"name_en": u"Catalan", "name": u"Catal"},
    11|     "cs_CZ": {"name_en": u"Czech", "name": u"etina"},
    12|     "cy_GB": {"name_en": u"Welsh", "name": u"Cymraeg"},
    13|     "da_DK": {"name_en": u"Danish", "name": u"Dansk"},
    14|     "de_DE": {"name_en": u"German", "name": u"Deutsch"},
    15|     "el_GR": {"name_en": u"Greek", "name": u""},
    16|     "en_GB": {"name_en": u"English (UK)", "name": u"English (UK)"},
    17|     "en_US": {"name_en": u"English (US)", "name": u"English (US)"},
    18|     "es_ES": {"name_en": u"Spanish (Spain)", "name": u"Espaol (Espaa)"},
    19|     "es_LA": {"name_en": u"Spanish", "name": u"Espaol"},
    20|     "et_EE": {"name_en": u"Estonian", "name": u"Eesti"},
    21|     "eu_ES": {"name_en": u"Basque", "name": u"Euskara"},
    22|     "fa_IR": {"name_en": u"Persian", "name": u""},
    23|     "fi_FI": {"name_en": u"Finnish", "name": u"Suomi"},
    24|     "fr_CA": {"name_en": u"French (Canada)", "name": u"Franais (Canada)"},
    25|     "fr_FR": {"name_en": u"French", "name": u"Franais"},
    26|     "ga_IE": {"name_en": u"Irish", "name": u"Gaeilge"},
    27|     "gl_ES": {"name_en": u"Galician", "name": u"Galego"},
    28|     "he_IL": {"name_en": u"Hebrew", "name": u""},
    29|     "hi_IN": {"name_en": u"Hindi", "name": u""},
    30|     "hr_HR": {"name_en": u"Croatian", "name": u"Hrvatski"},
    31|     "hu_HU": {"name_en": u"Hungarian", "name": u"Magyar"},
    32|     "id_ID": {"name_en": u"Indonesian", "name": u"Bahasa Indonesia"},
    33|     "is_IS": {"name_en": u"Icelandic", "name": u"slenska"},
    34|     "it_IT": {"name_en": u"Italian", "name": u"Italiano"},
    35|     "ja_JP": {"name_en": u"Japanese", "name": u""},
    36|     "ko_KR": {"name_en": u"Korean", "name": u""},
    37|     "lt_LT": {"name_en": u"Lithuanian", "name": u"Lietuvi"},
    38|     "lv_LV": {"name_en": u"Latvian", "name": u"Latvieu"},
    39|     "mk_MK": {"name_en": u"Macedonian", "name": u""},
    40|     "ml_IN": {"name_en": u"Malayalam", "name": u""},
    41|     "ms_MY": {"name_en": u"Malay", "name": u"Bahasa Melayu"},
    42|     "nb_NO": {"name_en": u"Norwegian (bokmal)", "name": u"Norsk (bokml)"},
    43|     "nl_NL": {"name_en": u"Dutch", "name": u"Nederlands"},
    44|     "nn_NO": {"name_en": u"Norwegian (nynorsk)", "name": u"Norsk (nynorsk)"},
    45|     "pa_IN": {"name_en": u"Punjabi", "name": u""},
    46|     "pl_PL": {"name_en": u"Polish", "name": u"Polski"},
    47|     "pt_BR": {"name_en": u"Portuguese (Brazil)", "name": u"Portugus (Brasil)"},
    48|     "pt_PT": {"name_en": u"Portuguese (Portugal)", "name": u"Portugus (Portugal)"},
    49|     "ro_RO": {"name_en": u"Romanian", "name": u"Romn"},
    50|     "ru_RU": {"name_en": u"Russian", "name": u""},
    51|     "sk_SK": {"name_en": u"Slovak", "name": u"Slovenina"},
    52|     "sl_SI": {"name_en": u"Slovenian", "name": u"Slovenina"},
    53|     "sq_AL": {"name_en": u"Albanian", "name": u"Shqip"},
    54|     "sr_RS": {"name_en": u"Serbian", "name": u""},
    55|     "sv_SE": {"name_en": u"Swedish", "name": u"Svenska"},
    56|     "sw_KE": {"name_en": u"Swahili", "name": u"Kiswahili"},
    57|     "ta_IN": {"name_en": u"Tamil", "name": u""},
    58|     "te_IN": {"name_en": u"Telugu", "name": u""},
    59|     "th_TH": {"name_en": u"Thai", "name": u""},
    60|     "tl_PH": {"name_en": u"Filipino", "name": u"Filipino"},
    61|     "tr_TR": {"name_en": u"Turkish", "name": u"Trke"},
    62|     "uk_UA": {"name_en": u"Ukraini ", "name": u""},
    63|     "vi_VN": {"name_en": u"Vietnamese", "name": u"Ting Vit"},
    64|     "zh_CN": {"name_en": u"Chinese (Simplified)", "name": u"()"},
    65|     "zh_TW": {"name_en": u"Chinese (Traditional)", "name": u"()"},
    66| }


# ====================================================================
# FILE: salt/ext/tornado/auth.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-933 ---
     1| """This module contains implementations of various third-party
     2| authentication schemes.
     3| All the classes in this file are class mixins designed to be used with
     4| the `tornado.web.RequestHandler` class.  They are used in two ways:
     5| * On a login handler, use methods such as ``authenticate_redirect()``,
     6|   ``authorize_redirect()``, and ``get_authenticated_user()`` to
     7|   establish the user's identity and store authentication tokens to your
     8|   database and/or cookies.
     9| * In non-login handlers, use methods such as ``facebook_request()``
    10|   or ``twitter_request()`` to use the authentication tokens to make
    11|   requests to the respective services.
    12| They all take slightly different arguments due to the fact all these
    13| services implement authentication and authorization slightly differently.
    14| See the individual service classes below for complete documentation.
    15| Example usage for Google OAuth:
    16| .. testcode::
    17|     class GoogleOAuth2LoginHandler(tornado.web.RequestHandler,
    18|                                    tornado.auth.GoogleOAuth2Mixin):
    19|         @tornado.gen.coroutine
    20|         def get(self):
    21|             if self.get_argument('code', False):
    22|                 user = yield self.get_authenticated_user(
    23|                     redirect_uri='http://your.site.com/auth/google',
    24|                     code=self.get_argument('code'))
    25|             else:
    26|                 yield self.authorize_redirect(
    27|                     redirect_uri='http://your.site.com/auth/google',
    28|                     client_id=self.settings['google_oauth']['key'],
    29|                     scope=['profile', 'email'],
    30|                     response_type='code',
    31|                     extra_params={'approval_prompt': 'auto'})
    32| .. testoutput::
    33|    :hide:
    34| .. versionchanged:: 4.0
    35|    All of the callback interfaces in this module are now guaranteed
    36|    to run their callback with an argument of ``None`` on error.
    37|    Previously some functions would do this while others would simply
    38|    terminate the request on their own.  This change also ensures that
    39|    errors are more consistently reported through the ``Future`` interfaces.
    40| """
    41| from __future__ import absolute_import, division, print_function
    42| import base64
    43| import binascii
    44| import functools
    45| import hashlib
    46| import hmac
    47| import time
    48| import uuid
    49| from salt.ext.tornado.concurrent import TracebackFuture, return_future, chain_future
    50| from salt.ext.tornado import gen
    51| from salt.ext.tornado import httpclient
    52| from salt.ext.tornado import escape
    53| from salt.ext.tornado.httputil import url_concat
    54| from salt.ext.tornado.log import gen_log
    55| from salt.ext.tornado.stack_context import ExceptionStackContext
    56| from salt.ext.tornado.util import unicode_type, ArgReplacer, PY3
    57| if PY3:
    58|     import urllib.parse as urlparse
    59|     import urllib.parse as urllib_parse
    60|     long = int
    61| else:
    62|     import urlparse
    63|     import urllib as urllib_parse
    64| class AuthError(Exception):
    65|     pass
    66| def _auth_future_to_callback(callback, future):
    67|     try:
    68|         result = future.result()
    69|     except AuthError as e:
    70|         gen_log.warning(str(e))
    71|         result = None
    72|     callback(result)
    73| def _auth_return_future(f):
    74|     """Similar to tornado.concurrent.return_future, but uses the auth
    75|     module's legacy callback interface.
    76|     Note that when using this decorator the ``callback`` parameter
    77|     inside the function will actually be a future.
    78|     """
    79|     replacer = ArgReplacer(f, 'callback')
    80|     @functools.wraps(f)
    81|     def wrapper(*args, **kwargs):
    82|         future = TracebackFuture()
    83|         callback, args, kwargs = replacer.replace(future, args, kwargs)
    84|         if callback is not None:
    85|             future.add_done_callback(
    86|                 functools.partial(_auth_future_to_callback, callback))
    87|         def handle_exception(typ, value, tb):
    88|             if future.done():
    89|                 return False
    90|             else:
    91|                 future.set_exc_info((typ, value, tb))
    92|                 return True
    93|         with ExceptionStackContext(handle_exception):
    94|             f(*args, **kwargs)
    95|         return future
    96|     return wrapper
    97| class OpenIdMixin(object):
    98|     """Abstract implementation of OpenID and Attribute Exchange.
    99|     Class attributes:
   100|     * ``_OPENID_ENDPOINT``: the identity provider's URI.
   101|     """
   102|     @return_future
   103|     def authenticate_redirect(self, callback_uri=None,
   104|                               ax_attrs=["name", "email", "language", "username"],
   105|                               callback=None):
   106|         """Redirects to the authentication URL for this service.
   107|         After authentication, the service will redirect back to the given
   108|         callback URI with additional parameters including ``openid.mode``.
   109|         We request the given attributes for the authenticated user by
   110|         default (name, email, language, and username). If you don't need
   111|         all those attributes for your app, you can request fewer with
   112|         the ax_attrs keyword argument.
   113|         .. versionchanged:: 3.1
   114|            Returns a `.Future` and takes an optional callback.  These are
   115|            not strictly necessary as this method is synchronous,
   116|            but they are supplied for consistency with
   117|            `OAuthMixin.authorize_redirect`.
   118|         """
   119|         callback_uri = callback_uri or self.request.uri
   120|         args = self._openid_args(callback_uri, ax_attrs=ax_attrs)
   121|         self.redirect(self._OPENID_ENDPOINT + "?" + urllib_parse.urlencode(args))
   122|         callback()
   123|     @_auth_return_future
   124|     def get_authenticated_user(self, callback, http_client=None):
   125|         """Fetches the authenticated user data upon redirect.
   126|         This method should be called by the handler that receives the
   127|         redirect from the `authenticate_redirect()` method (which is
   128|         often the same as the one that calls it; in that case you would
   129|         call `get_authenticated_user` if the ``openid.mode`` parameter
   130|         is present and `authenticate_redirect` if it is not).
   131|         The result of this method will generally be used to set a cookie.
   132|         """
   133|         args = dict((k, v[-1]) for k, v in self.request.arguments.items())
   134|         args["openid.mode"] = u"check_authentication"
   135|         url = self._OPENID_ENDPOINT
   136|         if http_client is None:
   137|             http_client = self.get_auth_http_client()
   138|         http_client.fetch(url, functools.partial(
   139|             self._on_authentication_verified, callback),
   140|             method="POST", body=urllib_parse.urlencode(args))
   141|     def _openid_args(self, callback_uri, ax_attrs=[], oauth_scope=None):
   142|         url = urlparse.urljoin(self.request.full_url(), callback_uri)
   143|         args = {
   144|             "openid.ns": "http://specs.openid.net/auth/2.0",
   145|             "openid.claimed_id":
   146|             "http://specs.openid.net/auth/2.0/identifier_select",
   147|             "openid.identity":
   148|             "http://specs.openid.net/auth/2.0/identifier_select",
   149|             "openid.return_to": url,
   150|             "openid.realm": urlparse.urljoin(url, '/'),
   151|             "openid.mode": "checkid_setup",
   152|         }
   153|         if ax_attrs:
   154|             args.update({
   155|                 "openid.ns.ax": "http://openid.net/srv/ax/1.0",
   156|                 "openid.ax.mode": "fetch_request",
   157|             })
   158|             ax_attrs = set(ax_attrs)
   159|             required = []
   160|             if "name" in ax_attrs:
   161|                 ax_attrs -= set(["name", "firstname", "fullname", "lastname"])
   162|                 required += ["firstname", "fullname", "lastname"]
   163|                 args.update({
   164|                     "openid.ax.type.firstname":
   165|                     "http://axschema.org/namePerson/first",
   166|                     "openid.ax.type.fullname":
   167|                     "http://axschema.org/namePerson",
   168|                     "openid.ax.type.lastname":
   169|                     "http://axschema.org/namePerson/last",
   170|                 })
   171|             known_attrs = {
   172|                 "email": "http://axschema.org/contact/email",
   173|                 "language": "http://axschema.org/pref/language",
   174|                 "username": "http://axschema.org/namePerson/friendly",
   175|             }
   176|             for name in ax_attrs:
   177|                 args["openid.ax.type." + name] = known_attrs[name]
   178|                 required.append(name)
   179|             args["openid.ax.required"] = ",".join(required)
   180|         if oauth_scope:
   181|             args.update({
   182|                 "openid.ns.oauth":
   183|                 "http://specs.openid.net/extensions/oauth/1.0",
   184|                 "openid.oauth.consumer": self.request.host.split(":")[0],
   185|                 "openid.oauth.scope": oauth_scope,
   186|             })
   187|         return args
   188|     def _on_authentication_verified(self, future, response):
   189|         if response.error or b"is_valid:true" not in response.body:
   190|             future.set_exception(AuthError(
   191|                 "Invalid OpenID response: %s" % (response.error or
   192|                                                  response.body)))
   193|             return
   194|         ax_ns = None
   195|         for name in self.request.arguments:
   196|             if name.startswith("openid.ns.") and \
   197|                     self.get_argument(name) == u"http://openid.net/srv/ax/1.0":
   198|                 ax_ns = name[10:]
   199|                 break
   200|         def get_ax_arg(uri):
   201|             if not ax_ns:
   202|                 return u""
   203|             prefix = "openid." + ax_ns + ".type."
   204|             ax_name = None
   205|             for name in self.request.arguments.keys():
   206|                 if self.get_argument(name) == uri and name.startswith(prefix):
   207|                     part = name[len(prefix):]
   208|                     ax_name = "openid." + ax_ns + ".value." + part
   209|                     break
   210|             if not ax_name:
   211|                 return u""
   212|             return self.get_argument(ax_name, u"")
   213|         email = get_ax_arg("http://axschema.org/contact/email")
   214|         name = get_ax_arg("http://axschema.org/namePerson")
   215|         first_name = get_ax_arg("http://axschema.org/namePerson/first")
   216|         last_name = get_ax_arg("http://axschema.org/namePerson/last")
   217|         username = get_ax_arg("http://axschema.org/namePerson/friendly")
   218|         locale = get_ax_arg("http://axschema.org/pref/language").lower()
   219|         user = dict()
   220|         name_parts = []
   221|         if first_name:
   222|             user["first_name"] = first_name
   223|             name_parts.append(first_name)
   224|         if last_name:
   225|             user["last_name"] = last_name
   226|             name_parts.append(last_name)
   227|         if name:
   228|             user["name"] = name
   229|         elif name_parts:
   230|             user["name"] = u" ".join(name_parts)
   231|         elif email:
   232|             user["name"] = email.split("@")[0]
   233|         if email:
   234|             user["email"] = email
   235|         if locale:
   236|             user["locale"] = locale
   237|         if username:
   238|             user["username"] = username
   239|         claimed_id = self.get_argument("openid.claimed_id", None)
   240|         if claimed_id:
   241|             user["claimed_id"] = claimed_id
   242|         future.set_result(user)
   243|     def get_auth_http_client(self):
   244|         """Returns the `.AsyncHTTPClient` instance to be used for auth requests.
   245|         May be overridden by subclasses to use an HTTP client other than
   246|         the default.
   247|         """
   248|         return httpclient.AsyncHTTPClient()
   249| class OAuthMixin(object):
   250|     """Abstract implementation of OAuth 1.0 and 1.0a.
   251|     See `TwitterMixin` below for an example implementation.
   252|     Class attributes:
   253|     * ``_OAUTH_AUTHORIZE_URL``: The service's OAuth authorization url.
   254|     * ``_OAUTH_ACCESS_TOKEN_URL``: The service's OAuth access token url.
   255|     * ``_OAUTH_VERSION``: May be either "1.0" or "1.0a".
   256|     * ``_OAUTH_NO_CALLBACKS``: Set this to True if the service requires
   257|       advance registration of callbacks.
   258|     Subclasses must also override the `_oauth_get_user_future` and
   259|     `_oauth_consumer_token` methods.
   260|     """
   261|     @return_future
   262|     def authorize_redirect(self, callback_uri=None, extra_params=None,
   263|                            http_client=None, callback=None):
   264|         """Redirects the user to obtain OAuth authorization for this service.
   265|         The ``callback_uri`` may be omitted if you have previously
   266|         registered a callback URI with the third-party service.  For
   267|         some services (including Friendfeed), you must use a
   268|         previously-registered callback URI and cannot specify a
   269|         callback via this method.
   270|         This method sets a cookie called ``_oauth_request_token`` which is
   271|         subsequently used (and cleared) in `get_authenticated_user` for
   272|         security purposes.
   273|         Note that this method is asynchronous, although it calls
   274|         `.RequestHandler.finish` for you so it may not be necessary
   275|         to pass a callback or use the `.Future` it returns.  However,
   276|         if this method is called from a function decorated with
   277|         `.gen.coroutine`, you must call it with ``yield`` to keep the
   278|         response from being closed prematurely.
   279|         .. versionchanged:: 3.1
   280|            Now returns a `.Future` and takes an optional callback, for
   281|            compatibility with `.gen.coroutine`.
   282|         """
   283|         if callback_uri and getattr(self, "_OAUTH_NO_CALLBACKS", False):
   284|             raise Exception("This service does not support oauth_callback")
   285|         if http_client is None:
   286|             http_client = self.get_auth_http_client()
   287|         if getattr(self, "_OAUTH_VERSION", "1.0a") == "1.0a":
   288|             http_client.fetch(
   289|                 self._oauth_request_token_url(callback_uri=callback_uri,
   290|                                               extra_params=extra_params),
   291|                 functools.partial(
   292|                     self._on_request_token,
   293|                     self._OAUTH_AUTHORIZE_URL,
   294|                     callback_uri,
   295|                     callback))
   296|         else:
   297|             http_client.fetch(
   298|                 self._oauth_request_token_url(),
   299|                 functools.partial(
   300|                     self._on_request_token, self._OAUTH_AUTHORIZE_URL,
   301|                     callback_uri,
   302|                     callback))
   303|     @_auth_return_future
   304|     def get_authenticated_user(self, callback, http_client=None):
   305|         """Gets the OAuth authorized user and access token.
   306|         This method should be called from the handler for your
   307|         OAuth callback URL to complete the registration process. We run the
   308|         callback with the authenticated user dictionary.  This dictionary
   309|         will contain an ``access_key`` which can be used to make authorized
   310|         requests to this service on behalf of the user.  The dictionary will
   311|         also contain other fields such as ``name``, depending on the service
   312|         used.
   313|         """
   314|         future = callback
   315|         request_key = escape.utf8(self.get_argument("oauth_token"))
   316|         oauth_verifier = self.get_argument("oauth_verifier", None)
   317|         request_cookie = self.get_cookie("_oauth_request_token")
   318|         if not request_cookie:
   319|             future.set_exception(AuthError(
   320|                 "Missing OAuth request token cookie"))
   321|             return
   322|         self.clear_cookie("_oauth_request_token")
   323|         cookie_key, cookie_secret = [base64.b64decode(escape.utf8(i)) for i in request_cookie.split("|")]
   324|         if cookie_key != request_key:
   325|             future.set_exception(AuthError(
   326|                 "Request token does not match cookie"))
   327|             return
   328|         token = dict(key=cookie_key, secret=cookie_secret)
   329|         if oauth_verifier:
   330|             token["verifier"] = oauth_verifier
   331|         if http_client is None:
   332|             http_client = self.get_auth_http_client()
   333|         http_client.fetch(self._oauth_access_token_url(token),
   334|                           functools.partial(self._on_access_token, callback))
   335|     def _oauth_request_token_url(self, callback_uri=None, extra_params=None):
   336|         consumer_token = self._oauth_consumer_token()
   337|         url = self._OAUTH_REQUEST_TOKEN_URL
   338|         args = dict(
   339|             oauth_consumer_key=escape.to_basestring(consumer_token["key"]),
   340|             oauth_signature_method="HMAC-SHA1",
   341|             oauth_timestamp=str(int(time.time())),
   342|             oauth_nonce=escape.to_basestring(binascii.b2a_hex(uuid.uuid4().bytes)),
   343|             oauth_version="1.0",
   344|         )
   345|         if getattr(self, "_OAUTH_VERSION", "1.0a") == "1.0a":
   346|             if callback_uri == "oob":
   347|                 args["oauth_callback"] = "oob"
   348|             elif callback_uri:
   349|                 args["oauth_callback"] = urlparse.urljoin(
   350|                     self.request.full_url(), callback_uri)
   351|             if extra_params:
   352|                 args.update(extra_params)
   353|             signature = _oauth10a_signature(consumer_token, "GET", url, args)
   354|         else:
   355|             signature = _oauth_signature(consumer_token, "GET", url, args)
   356|         args["oauth_signature"] = signature
   357|         return url + "?" + urllib_parse.urlencode(args)
   358|     def _on_request_token(self, authorize_url, callback_uri, callback,
   359|                           response):
   360|         if response.error:
   361|             raise Exception("Could not get request token: %s" % response.error)
   362|         request_token = _oauth_parse_response(response.body)
   363|         data = (base64.b64encode(escape.utf8(request_token["key"])) + b"|" +
   364|                 base64.b64encode(escape.utf8(request_token["secret"])))
   365|         self.set_cookie("_oauth_request_token", data)
   366|         args = dict(oauth_token=request_token["key"])
   367|         if callback_uri == "oob":
   368|             self.finish(authorize_url + "?" + urllib_parse.urlencode(args))
   369|             callback()
   370|             return
   371|         elif callback_uri:
   372|             args["oauth_callback"] = urlparse.urljoin(
   373|                 self.request.full_url(), callback_uri)
   374|         self.redirect(authorize_url + "?" + urllib_parse.urlencode(args))
   375|         callback()
   376|     def _oauth_access_token_url(self, request_token):
   377|         consumer_token = self._oauth_consumer_token()
   378|         url = self._OAUTH_ACCESS_TOKEN_URL
   379|         args = dict(
   380|             oauth_consumer_key=escape.to_basestring(consumer_token["key"]),
   381|             oauth_token=escape.to_basestring(request_token["key"]),
   382|             oauth_signature_method="HMAC-SHA1",
   383|             oauth_timestamp=str(int(time.time())),
   384|             oauth_nonce=escape.to_basestring(binascii.b2a_hex(uuid.uuid4().bytes)),
   385|             oauth_version="1.0",
   386|         )
   387|         if "verifier" in request_token:
   388|             args["oauth_verifier"] = request_token["verifier"]
   389|         if getattr(self, "_OAUTH_VERSION", "1.0a") == "1.0a":
   390|             signature = _oauth10a_signature(consumer_token, "GET", url, args,
   391|                                             request_token)
   392|         else:
   393|             signature = _oauth_signature(consumer_token, "GET", url, args,
   394|                                          request_token)
   395|         args["oauth_signature"] = signature
   396|         return url + "?" + urllib_parse.urlencode(args)
   397|     def _on_access_token(self, future, response):
   398|         if response.error:
   399|             future.set_exception(AuthError("Could not fetch access token"))
   400|             return
   401|         access_token = _oauth_parse_response(response.body)
   402|         self._oauth_get_user_future(access_token).add_done_callback(
   403|             functools.partial(self._on_oauth_get_user, access_token, future))
   404|     def _oauth_consumer_token(self):
   405|         """Subclasses must override this to return their OAuth consumer keys.
   406|         The return value should be a `dict` with keys ``key`` and ``secret``.
   407|         """
   408|         raise NotImplementedError()
   409|     @return_future
   410|     def _oauth_get_user_future(self, access_token, callback):
   411|         """Subclasses must override this to get basic information about the
   412|         user.
   413|         Should return a `.Future` whose result is a dictionary
   414|         containing information about the user, which may have been
   415|         retrieved by using ``access_token`` to make a request to the
   416|         service.
   417|         The access token will be added to the returned dictionary to make
   418|         the result of `get_authenticated_user`.
   419|         For backwards compatibility, the callback-based ``_oauth_get_user``
   420|         method is also supported.
   421|         """
   422|         self._oauth_get_user(access_token, callback)
   423|     def _oauth_get_user(self, access_token, callback):
   424|         raise NotImplementedError()
   425|     def _on_oauth_get_user(self, access_token, future, user_future):
   426|         if user_future.exception() is not None:
   427|             future.set_exception(user_future.exception())
   428|             return
   429|         user = user_future.result()
   430|         if not user:
   431|             future.set_exception(AuthError("Error getting user"))
   432|             return
   433|         user["access_token"] = access_token
   434|         future.set_result(user)
   435|     def _oauth_request_parameters(self, url, access_token, parameters={},
   436|                                   method="GET"):
   437|         """Returns the OAuth parameters as a dict for the given request.
   438|         parameters should include all POST arguments and query string arguments
   439|         that will be sent with the request.
   440|         """
   441|         consumer_token = self._oauth_consumer_token()
   442|         base_args = dict(
   443|             oauth_consumer_key=escape.to_basestring(consumer_token["key"]),
   444|             oauth_token=escape.to_basestring(access_token["key"]),
   445|             oauth_signature_method="HMAC-SHA1",
   446|             oauth_timestamp=str(int(time.time())),
   447|             oauth_nonce=escape.to_basestring(binascii.b2a_hex(uuid.uuid4().bytes)),
   448|             oauth_version="1.0",
   449|         )
   450|         args = {}
   451|         args.update(base_args)
   452|         args.update(parameters)
   453|         if getattr(self, "_OAUTH_VERSION", "1.0a") == "1.0a":
   454|             signature = _oauth10a_signature(consumer_token, method, url, args,
   455|                                             access_token)
   456|         else:
   457|             signature = _oauth_signature(consumer_token, method, url, args,
   458|                                          access_token)
   459|         base_args["oauth_signature"] = escape.to_basestring(signature)
   460|         return base_args
   461|     def get_auth_http_client(self):
   462|         """Returns the `.AsyncHTTPClient` instance to be used for auth requests.
   463|         May be overridden by subclasses to use an HTTP client other than
   464|         the default.
   465|         """
   466|         return httpclient.AsyncHTTPClient()
   467| class OAuth2Mixin(object):
   468|     """Abstract implementation of OAuth 2.0.
   469|     See `FacebookGraphMixin` or `GoogleOAuth2Mixin` below for example
   470|     implementations.
   471|     Class attributes:
   472|     * ``_OAUTH_AUTHORIZE_URL``: The service's authorization url.
   473|     * ``_OAUTH_ACCESS_TOKEN_URL``:  The service's access token url.
   474|     """
   475|     @return_future
   476|     def authorize_redirect(self, redirect_uri=None, client_id=None,
   477|                            client_secret=None, extra_params=None,
   478|                            callback=None, scope=None, response_type="code"):
   479|         """Redirects the user to obtain OAuth authorization for this service.
   480|         Some providers require that you register a redirect URL with
   481|         your application instead of passing one via this method. You
   482|         should call this method to log the user in, and then call
   483|         ``get_authenticated_user`` in the handler for your
   484|         redirect URL to complete the authorization process.
   485|         .. versionchanged:: 3.1
   486|            Returns a `.Future` and takes an optional callback.  These are
   487|            not strictly necessary as this method is synchronous,
   488|            but they are supplied for consistency with
   489|            `OAuthMixin.authorize_redirect`.
   490|         """
   491|         args = {
   492|             "redirect_uri": redirect_uri,
   493|             "client_id": client_id,
   494|             "response_type": response_type
   495|         }
   496|         if extra_params:
   497|             args.update(extra_params)
   498|         if scope:
   499|             args['scope'] = ' '.join(scope)
   500|         self.redirect(
   501|             url_concat(self._OAUTH_AUTHORIZE_URL, args))
   502|         callback()
   503|     def _oauth_request_token_url(self, redirect_uri=None, client_id=None,
   504|                                  client_secret=None, code=None,
   505|                                  extra_params=None):
   506|         url = self._OAUTH_ACCESS_TOKEN_URL
   507|         args = dict(
   508|             redirect_uri=redirect_uri,
   509|             code=code,
   510|             client_id=client_id,
   511|             client_secret=client_secret,
   512|         )
   513|         if extra_params:
   514|             args.update(extra_params)
   515|         return url_concat(url, args)
   516|     @_auth_return_future
   517|     def oauth2_request(self, url, callback, access_token=None,
   518|                        post_args=None, **args):
   519|         """Fetches the given URL auth an OAuth2 access token.
   520|         If the request is a POST, ``post_args`` should be provided. Query
   521|         string arguments should be given as keyword arguments.
   522|         Example usage:
   523|         ..testcode::
   524|             class MainHandler(tornado.web.RequestHandler,
   525|                               tornado.auth.FacebookGraphMixin):
   526|                 @tornado.web.authenticated
   527|                 @tornado.gen.coroutine
   528|                 def get(self):
   529|                     new_entry = yield self.oauth2_request(
   530|                         "https://graph.facebook.com/me/feed",
   531|                         post_args={"message": "I am posting from my Tornado application!"},
   532|                         access_token=self.current_user["access_token"])
   533|                     if not new_entry:
   534|                         yield self.authorize_redirect()
   535|                         return
   536|                     self.finish("Posted a message!")
   537|         .. testoutput::
   538|            :hide:
   539|         .. versionadded:: 4.3
   540|         """
   541|         all_args = {}
   542|         if access_token:
   543|             all_args["access_token"] = access_token
   544|             all_args.update(args)
   545|         if all_args:
   546|             url += "?" + urllib_parse.urlencode(all_args)
   547|         callback = functools.partial(self._on_oauth2_request, callback)
   548|         http = self.get_auth_http_client()
   549|         if post_args is not None:
   550|             http.fetch(url, method="POST", body=urllib_parse.urlencode(post_args),
   551|                        callback=callback)
   552|         else:
   553|             http.fetch(url, callback=callback)
   554|     def _on_oauth2_request(self, future, response):
   555|         if response.error:
   556|             future.set_exception(AuthError("Error response %s fetching %s" %
   557|                                            (response.error, response.request.url)))
   558|             return
   559|         future.set_result(escape.json_decode(response.body))
   560|     def get_auth_http_client(self):
   561|         """Returns the `.AsyncHTTPClient` instance to be used for auth requests.
   562|         May be overridden by subclasses to use an HTTP client other than
   563|         the default.
   564|         .. versionadded:: 4.3
   565|         """
   566|         return httpclient.AsyncHTTPClient()
   567| class TwitterMixin(OAuthMixin):
   568|     """Twitter OAuth authentication.
   569|     To authenticate with Twitter, register your application with
   570|     Twitter at http://twitter.com/apps. Then copy your Consumer Key
   571|     and Consumer Secret to the application
   572|     `~tornado.web.Application.settings` ``twitter_consumer_key`` and
   573|     ``twitter_consumer_secret``. Use this mixin on the handler for the
   574|     URL you registered as your application's callback URL.
   575|     When your application is set up, you can use this mixin like this
   576|     to authenticate the user with Twitter and get access to their stream:
   577|     .. testcode::
   578|         class TwitterLoginHandler(tornado.web.RequestHandler,
   579|                                   tornado.auth.TwitterMixin):
   580|             @tornado.gen.coroutine
   581|             def get(self):
   582|                 if self.get_argument("oauth_token", None):
   583|                     user = yield self.get_authenticated_user()
   584|                 else:
   585|                     yield self.authorize_redirect()
   586|     .. testoutput::
   587|        :hide:
   588|     The user object returned by `~OAuthMixin.get_authenticated_user`
   589|     includes the attributes ``username``, ``name``, ``access_token``,
   590|     and all of the custom Twitter user attributes described at
   591|     https://dev.twitter.com/docs/api/1.1/get/users/show
   592|     """
   593|     _OAUTH_REQUEST_TOKEN_URL = "https://api.twitter.com/oauth/request_token"
   594|     _OAUTH_ACCESS_TOKEN_URL = "https://api.twitter.com/oauth/access_token"
   595|     _OAUTH_AUTHORIZE_URL = "https://api.twitter.com/oauth/authorize"
   596|     _OAUTH_AUTHENTICATE_URL = "https://api.twitter.com/oauth/authenticate"
   597|     _OAUTH_NO_CALLBACKS = False
   598|     _TWITTER_BASE_URL = "https://api.twitter.com/1.1"
   599|     @return_future
   600|     def authenticate_redirect(self, callback_uri=None, callback=None):
   601|         """Just like `~OAuthMixin.authorize_redirect`, but
   602|         auto-redirects if authorized.
   603|         This is generally the right interface to use if you are using
   604|         Twitter for single-sign on.
   605|         .. versionchanged:: 3.1
   606|            Now returns a `.Future` and takes an optional callback, for
   607|            compatibility with `.gen.coroutine`.
   608|         """
   609|         http = self.get_auth_http_client()
   610|         http.fetch(self._oauth_request_token_url(callback_uri=callback_uri),
   611|                    functools.partial(
   612|                        self._on_request_token, self._OAUTH_AUTHENTICATE_URL,
   613|                        None, callback))
   614|     @_auth_return_future
   615|     def twitter_request(self, path, callback=None, access_token=None,
   616|                         post_args=None, **args):
   617|         """Fetches the given API path, e.g., ``statuses/user_timeline/btaylor``
   618|         The path should not include the format or API version number.
   619|         (we automatically use JSON format and API version 1).
   620|         If the request is a POST, ``post_args`` should be provided. Query
   621|         string arguments should be given as keyword arguments.
   622|         All the Twitter methods are documented at http://dev.twitter.com/
   623|         Many methods require an OAuth access token which you can
   624|         obtain through `~OAuthMixin.authorize_redirect` and
   625|         `~OAuthMixin.get_authenticated_user`. The user returned through that
   626|         process includes an 'access_token' attribute that can be used
   627|         to make authenticated requests via this method. Example
   628|         usage:
   629|         .. testcode::
   630|             class MainHandler(tornado.web.RequestHandler,
   631|                               tornado.auth.TwitterMixin):
   632|                 @tornado.web.authenticated
   633|                 @tornado.gen.coroutine
   634|                 def get(self):
   635|                     new_entry = yield self.twitter_request(
   636|                         "/statuses/update",
   637|                         post_args={"status": "Testing Tornado Web Server"},
   638|                         access_token=self.current_user["access_token"])
   639|                     if not new_entry:
   640|                         yield self.authorize_redirect()
   641|                         return
   642|                     self.finish("Posted a message!")
   643|         .. testoutput::
   644|            :hide:
   645|         """
   646|         if path.startswith('http:') or path.startswith('https:'):
   647|             url = path
   648|         else:
   649|             url = self._TWITTER_BASE_URL + path + ".json"
   650|         if access_token:
   651|             all_args = {}
   652|             all_args.update(args)
   653|             all_args.update(post_args or {})
   654|             method = "POST" if post_args is not None else "GET"
   655|             oauth = self._oauth_request_parameters(
   656|                 url, access_token, all_args, method=method)
   657|             args.update(oauth)
   658|         if args:
   659|             url += "?" + urllib_parse.urlencode(args)
   660|         http = self.get_auth_http_client()
   661|         http_callback = functools.partial(self._on_twitter_request, callback)
   662|         if post_args is not None:
   663|             http.fetch(url, method="POST", body=urllib_parse.urlencode(post_args),
   664|                        callback=http_callback)
   665|         else:
   666|             http.fetch(url, callback=http_callback)
   667|     def _on_twitter_request(self, future, response):
   668|         if response.error:
   669|             future.set_exception(AuthError(
   670|                 "Error response %s fetching %s" % (response.error,
   671|                                                    response.request.url)))
   672|             return
   673|         future.set_result(escape.json_decode(response.body))
   674|     def _oauth_consumer_token(self):
   675|         self.require_setting("twitter_consumer_key", "Twitter OAuth")
   676|         self.require_setting("twitter_consumer_secret", "Twitter OAuth")
   677|         return dict(
   678|             key=self.settings["twitter_consumer_key"],
   679|             secret=self.settings["twitter_consumer_secret"])
   680|     @gen.coroutine
   681|     def _oauth_get_user_future(self, access_token):
   682|         user = yield self.twitter_request(
   683|             "/account/verify_credentials",
   684|             access_token=access_token)
   685|         if user:
   686|             user["username"] = user["screen_name"]
   687|         raise gen.Return(user)
   688| class GoogleOAuth2Mixin(OAuth2Mixin):
   689|     """Google authentication using OAuth2.
   690|     In order to use, register your application with Google and copy the
   691|     relevant parameters to your application settings.
   692|     * Go to the Google Dev Console at http://console.developers.google.com
   693|     * Select a project, or create a new one.
   694|     * In the sidebar on the left, select APIs & Auth.
   695|     * In the list of APIs, find the Google+ API service and set it to ON.
   696|     * In the sidebar on the left, select Credentials.
   697|     * In the OAuth section of the page, select Create New Client ID.
   698|     * Set the Redirect URI to point to your auth handler
   699|     * Copy the "Client secret" and "Client ID" to the application settings as
   700|       {"google_oauth": {"key": CLIENT_ID, "secret": CLIENT_SECRET}}
   701|     .. versionadded:: 3.2
   702|     """
   703|     _OAUTH_AUTHORIZE_URL = "https://accounts.google.com/o/oauth2/auth"
   704|     _OAUTH_ACCESS_TOKEN_URL = "https://accounts.google.com/o/oauth2/token"
   705|     _OAUTH_USERINFO_URL = "https://www.googleapis.com/oauth2/v1/userinfo"
   706|     _OAUTH_NO_CALLBACKS = False
   707|     _OAUTH_SETTINGS_KEY = 'google_oauth'
   708|     @_auth_return_future
   709|     def get_authenticated_user(self, redirect_uri, code, callback):
   710|         """Handles the login for the Google user, returning an access token.
   711|         The result is a dictionary containing an ``access_token`` field
   712|         ([among others](https://developers.google.com/identity/protocols/OAuth2WebServer#handlingtheresponse)).
   713|         Unlike other ``get_authenticated_user`` methods in this package,
   714|         this method does not return any additional information about the user.
   715|         The returned access token can be used with `OAuth2Mixin.oauth2_request`
   716|         to request additional information (perhaps from
   717|         ``https://www.googleapis.com/oauth2/v2/userinfo``)
   718|         Example usage:
   719|         .. testcode::
   720|             class GoogleOAuth2LoginHandler(tornado.web.RequestHandler,
   721|                                            tornado.auth.GoogleOAuth2Mixin):
   722|                 @tornado.gen.coroutine
   723|                 def get(self):
   724|                     if self.get_argument('code', False):
   725|                         access = yield self.get_authenticated_user(
   726|                             redirect_uri='http://your.site.com/auth/google',
   727|                             code=self.get_argument('code'))
   728|                         user = yield self.oauth2_request(
   729|                             "https://www.googleapis.com/oauth2/v1/userinfo",
   730|                             access_token=access["access_token"])
   731|                     else:
   732|                         yield self.authorize_redirect(
   733|                             redirect_uri='http://your.site.com/auth/google',
   734|                             client_id=self.settings['google_oauth']['key'],
   735|                             scope=['profile', 'email'],
   736|                             response_type='code',
   737|                             extra_params={'approval_prompt': 'auto'})
   738|         .. testoutput::
   739|            :hide:
   740|         """
   741|         http = self.get_auth_http_client()
   742|         body = urllib_parse.urlencode({
   743|             "redirect_uri": redirect_uri,
   744|             "code": code,
   745|             "client_id": self.settings[self._OAUTH_SETTINGS_KEY]['key'],
   746|             "client_secret": self.settings[self._OAUTH_SETTINGS_KEY]['secret'],
   747|             "grant_type": "authorization_code",
   748|         })
   749|         http.fetch(self._OAUTH_ACCESS_TOKEN_URL,
   750|                    functools.partial(self._on_access_token, callback),
   751|                    method="POST", headers={'Content-Type': 'application/x-www-form-urlencoded'}, body=body)
   752|     def _on_access_token(self, future, response):
   753|         """Callback function for the exchange to the access token."""
   754|         if response.error:
   755|             future.set_exception(AuthError('Google auth error: %s' % str(response)))
   756|             return
   757|         args = escape.json_decode(response.body)
   758|         future.set_result(args)
   759| class FacebookGraphMixin(OAuth2Mixin):
   760|     """Facebook authentication using the new Graph API and OAuth2."""
   761|     _OAUTH_ACCESS_TOKEN_URL = "https://graph.facebook.com/oauth/access_token?"
   762|     _OAUTH_AUTHORIZE_URL = "https://www.facebook.com/dialog/oauth?"
   763|     _OAUTH_NO_CALLBACKS = False
   764|     _FACEBOOK_BASE_URL = "https://graph.facebook.com"
   765|     @_auth_return_future
   766|     def get_authenticated_user(self, redirect_uri, client_id, client_secret,
   767|                                code, callback, extra_fields=None):
   768|         """Handles the login for the Facebook user, returning a user object.
   769|         Example usage:
   770|         .. testcode::
   771|             class FacebookGraphLoginHandler(tornado.web.RequestHandler,
   772|                                             tornado.auth.FacebookGraphMixin):
   773|               @tornado.gen.coroutine
   774|               def get(self):
   775|                   if self.get_argument("code", False):
   776|                       user = yield self.get_authenticated_user(
   777|                           redirect_uri='/auth/facebookgraph/',
   778|                           client_id=self.settings["facebook_api_key"],
   779|                           client_secret=self.settings["facebook_secret"],
   780|                           code=self.get_argument("code"))
   781|                   else:
   782|                       yield self.authorize_redirect(
   783|                           redirect_uri='/auth/facebookgraph/',
   784|                           client_id=self.settings["facebook_api_key"],
   785|                           extra_params={"scope": "read_stream,offline_access"})
   786|         .. testoutput::
   787|            :hide:
   788|         This method returns a dictionary which may contain the following fields:
   789|         * ``access_token``, a string which may be passed to `facebook_request`
   790|         * ``session_expires``, an integer encoded as a string representing
   791|           the time until the access token expires in seconds. This field should
   792|           be used like ``int(user['session_expires'])``; in a future version of
   793|           Tornado it will change from a string to an integer.
   794|         * ``id``, ``name``, ``first_name``, ``last_name``, ``locale``, ``picture``,
   795|           ``link``, plus any fields named in the ``extra_fields`` argument. These
   796|           fields are copied from the Facebook graph API `user object <https://developers.facebook.com/docs/graph-api/reference/user>`_
   797|         .. versionchanged:: 4.5
   798|            The ``session_expires`` field was updated to support changes made to the
   799|            Facebook API in March 2017.
   800|         """
   801|         http = self.get_auth_http_client()
   802|         args = {
   803|             "redirect_uri": redirect_uri,
   804|             "code": code,
   805|             "client_id": client_id,
   806|             "client_secret": client_secret,
   807|         }
   808|         fields = set(['id', 'name', 'first_name', 'last_name',
   809|                       'locale', 'picture', 'link'])
   810|         if extra_fields:
   811|             fields.update(extra_fields)
   812|         http.fetch(self._oauth_request_token_url(**args),
   813|                    functools.partial(self._on_access_token, redirect_uri, client_id,
   814|                                      client_secret, callback, fields))
   815|     def _on_access_token(self, redirect_uri, client_id, client_secret,
   816|                          future, fields, response):
   817|         if response.error:
   818|             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))
   819|             return
   820|         args = escape.json_decode(response.body)
   821|         session = {
   822|             "access_token": args.get("access_token"),
   823|             "expires_in": args.get("expires_in")
   824|         }
   825|         self.facebook_request(
   826|             path="/me",
   827|             callback=functools.partial(
   828|                 self._on_get_user_info, future, session, fields),
   829|             access_token=session["access_token"],
   830|             appsecret_proof=hmac.new(key=client_secret.encode('utf8'),
   831|                                      msg=session["access_token"].encode('utf8'),
   832|                                      digestmod=hashlib.sha256).hexdigest(),
   833|             fields=",".join(fields)
   834|         )
   835|     def _on_get_user_info(self, future, session, fields, user):
   836|         if user is None:
   837|             future.set_result(None)
   838|             return
   839|         fieldmap = {}
   840|         for field in fields:
   841|             fieldmap[field] = user.get(field)
   842|         fieldmap.update({"access_token": session["access_token"],
   843|                          "session_expires": str(session.get("expires_in"))})
   844|         future.set_result(fieldmap)
   845|     @_auth_return_future
   846|     def facebook_request(self, path, callback, access_token=None,
   847|                          post_args=None, **args):
   848|         """Fetches the given relative API path, e.g., "/btaylor/picture"
   849|         If the request is a POST, ``post_args`` should be provided. Query
   850|         string arguments should be given as keyword arguments.
   851|         An introduction to the Facebook Graph API can be found at
   852|         http://developers.facebook.com/docs/api
   853|         Many methods require an OAuth access token which you can
   854|         obtain through `~OAuth2Mixin.authorize_redirect` and
   855|         `get_authenticated_user`. The user returned through that
   856|         process includes an ``access_token`` attribute that can be
   857|         used to make authenticated requests via this method.
   858|         Example usage:
   859|         ..testcode::
   860|             class MainHandler(tornado.web.RequestHandler,
   861|                               tornado.auth.FacebookGraphMixin):
   862|                 @tornado.web.authenticated
   863|                 @tornado.gen.coroutine
   864|                 def get(self):
   865|                     new_entry = yield self.facebook_request(
   866|                         "/me/feed",
   867|                         post_args={"message": "I am posting from my Tornado application!"},
   868|                         access_token=self.current_user["access_token"])
   869|                     if not new_entry:
   870|                         yield self.authorize_redirect()
   871|                         return
   872|                     self.finish("Posted a message!")
   873|         .. testoutput::
   874|            :hide:
   875|         The given path is relative to ``self._FACEBOOK_BASE_URL``,
   876|         by default "https://graph.facebook.com".
   877|         This method is a wrapper around `OAuth2Mixin.oauth2_request`;
   878|         the only difference is that this method takes a relative path,
   879|         while ``oauth2_request`` takes a complete url.
   880|         .. versionchanged:: 3.1
   881|            Added the ability to override ``self._FACEBOOK_BASE_URL``.
   882|         """
   883|         url = self._FACEBOOK_BASE_URL + path
   884|         oauth_future = self.oauth2_request(url, access_token=access_token,
   885|                                            post_args=post_args, **args)
   886|         chain_future(oauth_future, callback)
   887| def _oauth_signature(consumer_token, method, url, parameters={}, token=None):
   888|     """Calculates the HMAC-SHA1 OAuth signature for the given request.
   889|     See http://oauth.net/core/1.0/#signing_process
   890|     """
   891|     parts = urlparse.urlparse(url)
   892|     scheme, netloc, path = parts[:3]
   893|     normalized_url = scheme.lower() + "://" + netloc.lower() + path
   894|     base_elems = []
   895|     base_elems.append(method.upper())
   896|     base_elems.append(normalized_url)
   897|     base_elems.append("&".join("%s=%s" % (k, _oauth_escape(str(v)))
   898|                                for k, v in sorted(parameters.items())))
   899|     base_string = "&".join(_oauth_escape(e) for e in base_elems)
   900|     key_elems = [escape.utf8(consumer_token["secret"])]
   901|     key_elems.append(escape.utf8(token["secret"] if token else ""))
   902|     key = b"&".join(key_elems)
   903|     hash = hmac.new(key, escape.utf8(base_string), hashlib.sha1)
   904|     return binascii.b2a_base64(hash.digest())[:-1]
   905| def _oauth10a_signature(consumer_token, method, url, parameters={}, token=None):
   906|     """Calculates the HMAC-SHA1 OAuth 1.0a signature for the given request.
   907|     See http://oauth.net/core/1.0a/#signing_process
   908|     """
   909|     parts = urlparse.urlparse(url)
   910|     scheme, netloc, path = parts[:3]
   911|     normalized_url = scheme.lower() + "://" + netloc.lower() + path
   912|     base_elems = []
   913|     base_elems.append(method.upper())
   914|     base_elems.append(normalized_url)
   915|     base_elems.append("&".join("%s=%s" % (k, _oauth_escape(str(v)))
   916|                                for k, v in sorted(parameters.items())))
   917|     base_string = "&".join(_oauth_escape(e) for e in base_elems)
   918|     key_elems = [escape.utf8(urllib_parse.quote(consumer_token["secret"], safe='~'))]
   919|     key_elems.append(escape.utf8(urllib_parse.quote(token["secret"], safe='~') if token else ""))
   920|     key = b"&".join(key_elems)
   921|     hash = hmac.new(key, escape.utf8(base_string), hashlib.sha1)
   922|     return binascii.b2a_base64(hash.digest())[:-1]
   923| def _oauth_escape(val):
   924|     if isinstance(val, unicode_type):
   925|         val = val.encode("utf-8")
   926|     return urllib_parse.quote(val, safe="~")
   927| def _oauth_parse_response(body):
   928|     body = escape.native_str(body)
   929|     p = urlparse.parse_qs(body, keep_blank_values=False)
   930|     token = dict(key=p["oauth_token"][0], secret=p["oauth_token_secret"][0])
   931|     special = ("oauth_token", "oauth_token_secret")
   932|     token.update((k, p[k][0]) for k in p if k not in special)
   933|     return token


# ====================================================================
# FILE: salt/ext/tornado/autoreload.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-198 ---
     1| """Automatically restart the server when a source file is modified.
     2| Most applications should not access this module directly.  Instead,
     3| pass the keyword argument ``autoreload=True`` to the
     4| `tornado.web.Application` constructor (or ``debug=True``, which
     5| enables this setting and several others).  This will enable autoreload
     6| mode as well as checking for changes to templates and static
     7| resources.  Note that restarting is a destructive operation and any
     8| requests in progress will be aborted when the process restarts.  (If
     9| you want to disable autoreload while using other debug-mode features,
    10| pass both ``debug=True`` and ``autoreload=False``).
    11| This module can also be used as a command-line wrapper around scripts
    12| such as unit test runners.  See the `main` method for details.
    13| The command-line wrapper and Application debug modes can be used together.
    14| This combination is encouraged as the wrapper catches syntax errors and
    15| other import-time failures, while debug mode catches changes once
    16| the server has started.
    17| This module depends on `.IOLoop`, so it will not work in WSGI applications
    18| and Google App Engine.  It also will not work correctly when `.HTTPServer`'s
    19| multi-process mode is used.
    20| Reloading loses any Python interpreter command-line arguments (e.g. ``-u``)
    21| because it re-executes Python using ``sys.executable`` and ``sys.argv``.
    22| Additionally, modifying these variables will cause reloading to behave
    23| incorrectly.
    24| """
    25| from __future__ import absolute_import, division, print_function
    26| import os
    27| import sys
    28| if __name__ == "__main__":
    29|     if sys.path[0] == os.path.dirname(__file__):
    30|         del sys.path[0]
    31| import functools
    32| import logging
    33| import os
    34| import pkgutil  # type: ignore
    35| import sys
    36| import traceback
    37| import types
    38| import subprocess
    39| import weakref
    40| from salt.ext.tornado import ioloop
    41| from salt.ext.tornado.log import gen_log
    42| from salt.ext.tornado import process
    43| from salt.ext.tornado.util import exec_in
    44| try:
    45|     import signal
    46| except ImportError:
    47|     signal = None
    48| _has_execv = sys.platform != 'win32'
    49| _watched_files = set()
    50| _reload_hooks = []
    51| _reload_attempted = False
    52| _io_loops = weakref.WeakKeyDictionary()  # type: ignore
    53| def start(io_loop=None, check_time=500):
    54|     """Begins watching source files for changes.
    55|     .. versionchanged:: 4.1
    56|        The ``io_loop`` argument is deprecated.
    57|     """
    58|     io_loop = io_loop or ioloop.IOLoop.current()
    59|     if io_loop in _io_loops:
    60|         return
    61|     _io_loops[io_loop] = True
    62|     if len(_io_loops) > 1:
    63|         gen_log.warning("tornado.autoreload started more than once in the same process")
    64|     modify_times = {}
    65|     callback = functools.partial(_reload_on_update, modify_times)
    66|     scheduler = ioloop.PeriodicCallback(callback, check_time, io_loop=io_loop)
    67|     scheduler.start()
    68| def wait():
    69|     """Wait for a watched file to change, then restart the process.
    70|     Intended to be used at the end of scripts like unit test runners,
    71|     to run the tests again after any source file changes (but see also
    72|     the command-line interface in `main`)
    73|     """
    74|     io_loop = ioloop.IOLoop()
    75|     start(io_loop)
    76|     io_loop.start()
    77| def watch(filename):
    78|     """Add a file to the watch list.
    79|     All imported modules are watched by default.
    80|     """
    81|     _watched_files.add(filename)
    82| def add_reload_hook(fn):
    83|     """Add a function to be called before reloading the process.
    84|     Note that for open file and socket handles it is generally
    85|     preferable to set the ``FD_CLOEXEC`` flag (using `fcntl` or
    86|     ``tornado.platform.auto.set_close_exec``) instead
    87|     of using a reload hook to close them.
    88|     """
    89|     _reload_hooks.append(fn)
    90| def _reload_on_update(modify_times):
    91|     if _reload_attempted:
    92|         return
    93|     if process.task_id() is not None:
    94|         return
    95|     for module in list(sys.modules.values()):
    96|         if not isinstance(module, types.ModuleType):
    97|             continue
    98|         path = getattr(module, "__file__", None)
    99|         if not path:
   100|             continue
   101|         if path.endswith(".pyc") or path.endswith(".pyo"):
   102|             path = path[:-1]
   103|         _check_file(modify_times, path)
   104|     for path in _watched_files:
   105|         _check_file(modify_times, path)
   106| def _check_file(modify_times, path):
   107|     try:
   108|         modified = os.stat(path).st_mtime
   109|     except Exception:
   110|         return
   111|     if path not in modify_times:
   112|         modify_times[path] = modified
   113|         return
   114|     if modify_times[path] != modified:
   115|         gen_log.info("%s modified; restarting server", path)
   116|         _reload()
   117| def _reload():
   118|     global _reload_attempted
   119|     _reload_attempted = True
   120|     for fn in _reload_hooks:
   121|         fn()
   122|     if hasattr(signal, "setitimer"):
   123|         signal.setitimer(signal.ITIMER_REAL, 0, 0)
   124|     path_prefix = '.' + os.pathsep
   125|     if (sys.path[0] == '' and
   126|             not os.environ.get("PYTHONPATH", "").startswith(path_prefix)):
   127|         os.environ["PYTHONPATH"] = (path_prefix +
   128|                                     os.environ.get("PYTHONPATH", ""))
   129|     if not _has_execv:
   130|         subprocess.Popen([sys.executable] + sys.argv)
   131|         sys.exit(0)
   132|     else:
   133|         try:
   134|             os.execv(sys.executable, [sys.executable] + sys.argv)
   135|         except OSError:
   136|             os.spawnv(os.P_NOWAIT, sys.executable,
   137|                       [sys.executable] + sys.argv)
   138|             os._exit(0)
   139| _USAGE = """\
   140| Usage:
   141|   python -m tornado.autoreload -m module.to.run [args...]
   142|   python -m tornado.autoreload path/to/script.py [args...]
   143| """
   144| def main():
   145|     """Command-line wrapper to re-run a script whenever its source changes.
   146|     Scripts may be specified by filename or module name::
   147|         python -m tornado.autoreload -m tornado.test.runtests
   148|         python -m tornado.autoreload tornado/test/runtests.py
   149|     Running a script with this wrapper is similar to calling
   150|     `tornado.autoreload.wait` at the end of the script, but this wrapper
   151|     can catch import-time problems like syntax errors that would otherwise
   152|     prevent the script from reaching its call to `wait`.
   153|     """
   154|     original_argv = sys.argv
   155|     sys.argv = sys.argv[:]
   156|     if len(sys.argv) >= 3 and sys.argv[1] == "-m":
   157|         mode = "module"
   158|         module = sys.argv[2]
   159|         del sys.argv[1:3]
   160|     elif len(sys.argv) >= 2:
   161|         mode = "script"
   162|         script = sys.argv[1]
   163|         sys.argv = sys.argv[1:]
   164|     else:
   165|         print(_USAGE, file=sys.stderr)
   166|         sys.exit(1)
   167|     try:
   168|         if mode == "module":
   169|             import runpy
   170|             runpy.run_module(module, run_name="__main__", alter_sys=True)
   171|         elif mode == "script":
   172|             with open(script) as f:
   173|                 global __file__
   174|                 __file__ = script
   175|                 global __package__
   176|                 del __package__
   177|                 exec_in(f.read(), globals(), globals())
   178|     except SystemExit as e:
   179|         logging.basicConfig()
   180|         gen_log.info("Script exited with status %s", e.code)
   181|     except Exception as e:
   182|         logging.basicConfig()
   183|         gen_log.warning("Script exited with uncaught exception", exc_info=True)
   184|         for (filename, lineno, name, line) in traceback.extract_tb(sys.exc_info()[2]):
   185|             watch(filename)
   186|         if isinstance(e, SyntaxError):
   187|             watch(e.filename)
   188|     else:
   189|         logging.basicConfig()
   190|         gen_log.info("Script exited normally")
   191|     sys.argv = original_argv
   192|     if mode == 'module':
   193|         loader = pkgutil.get_loader(module)
   194|         if loader is not None:
   195|             watch(loader.get_filename())
   196|     wait()
   197| if __name__ == "__main__":
   198|     main()


# ====================================================================
# FILE: salt/ext/tornado/concurrent.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-397 ---
     1| """Utilities for working with threads and ``Futures``.
     2| ``Futures`` are a pattern for concurrent programming introduced in
     3| Python 3.2 in the `concurrent.futures` package. This package defines
     4| a mostly-compatible `Future` class designed for use from coroutines,
     5| as well as some utility functions for interacting with the
     6| `concurrent.futures` package.
     7| """
     8| from __future__ import absolute_import, division, print_function
     9| import functools
    10| import platform
    11| import sys
    12| import textwrap
    13| import traceback
    14| from salt.ext.tornado.log import app_log
    15| from salt.ext.tornado.stack_context import ExceptionStackContext, wrap
    16| from salt.ext.tornado.util import ArgReplacer, is_finalizing, raise_exc_info
    17| try:
    18|     from concurrent import futures
    19| except ImportError:
    20|     futures = None
    21| try:
    22|     import typing
    23| except ImportError:
    24|     typing = None
    25| _GC_CYCLE_FINALIZERS = platform.python_implementation() == "CPython" and sys.version_info >= (
    26|     3,
    27|     4,
    28| )
    29| class ReturnValueIgnoredError(Exception):
    30|     pass
    31| class _TracebackLogger(object):
    32|     """Helper to log a traceback upon destruction if not cleared.
    33|     This solves a nasty problem with Futures and Tasks that have an
    34|     exception set: if nobody asks for the exception, the exception is
    35|     never logged.  This violates the Zen of Python: 'Errors should
    36|     never pass silently.  Unless explicitly silenced.'
    37|     However, we don't want to log the exception as soon as
    38|     set_exception() is called: if the calling code is written
    39|     properly, it will get the exception and handle it properly.  But
    40|     we *do* want to log it if result() or exception() was never called
    41|     -- otherwise developers waste a lot of time wondering why their
    42|     buggy code fails silently.
    43|     An earlier attempt added a __del__() method to the Future class
    44|     itself, but this backfired because the presence of __del__()
    45|     prevents garbage collection from breaking cycles.  A way out of
    46|     this catch-22 is to avoid having a __del__() method on the Future
    47|     class itself, but instead to have a reference to a helper object
    48|     with a __del__() method that logs the traceback, where we ensure
    49|     that the helper object doesn't participate in cycles, and only the
    50|     Future has a reference to it.
    51|     The helper object is added when set_exception() is called.  When
    52|     the Future is collected, and the helper is present, the helper
    53|     object is also collected, and its __del__() method will log the
    54|     traceback.  When the Future's result() or exception() method is
    55|     called (and a helper object is present), it removes the helper
    56|     object, after calling its clear() method to prevent it from
    57|     logging.
    58|     One downside is that we do a fair amount of work to extract the
    59|     traceback from the exception, even when it is never logged.  It
    60|     would seem cheaper to just store the exception object, but that
    61|     references the traceback, which references stack frames, which may
    62|     reference the Future, which references the _TracebackLogger, and
    63|     then the _TracebackLogger would be included in a cycle, which is
    64|     what we're trying to avoid!  As an optimization, we don't
    65|     immediately format the exception; we only do the work when
    66|     activate() is called, which call is delayed until after all the
    67|     Future's callbacks have run.  Since usually a Future has at least
    68|     one callback (typically set by 'yield From') and usually that
    69|     callback extracts the callback, thereby removing the need to
    70|     format the exception.
    71|     PS. I don't claim credit for this solution.  I first heard of it
    72|     in a discussion about closing files when they are collected.
    73|     """
    74|     __slots__ = ("exc_info", "formatted_tb")
    75|     def __init__(self, exc_info):
    76|         self.exc_info = exc_info
    77|         self.formatted_tb = None
    78|     def activate(self):
    79|         exc_info = self.exc_info
    80|         if exc_info is not None:
    81|             self.exc_info = None
    82|             self.formatted_tb = traceback.format_exception(*exc_info)
    83|     def clear(self):
    84|         self.exc_info = None
    85|         self.formatted_tb = None
    86|     def __del__(self, is_finalizing=is_finalizing):
    87|         if not is_finalizing() and self.formatted_tb:
    88|             app_log.error(
    89|                 "Future exception was never retrieved: %s",
    90|                 "".join(self.formatted_tb).rstrip(),
    91|             )
    92| class Future(object):
    93|     """Placeholder for an asynchronous result.
    94|     A ``Future`` encapsulates the result of an asynchronous
    95|     operation.  In synchronous applications ``Futures`` are used
    96|     to wait for the result from a thread or process pool; in
    97|     Tornado they are normally used with `.IOLoop.add_future` or by
    98|     yielding them in a `.gen.coroutine`.
    99|     `tornado.concurrent.Future` is similar to
   100|     `concurrent.futures.Future`, but not thread-safe (and therefore
   101|     faster for use with single-threaded event loops).
   102|     In addition to ``exception`` and ``set_exception``, methods ``exc_info``
   103|     and ``set_exc_info`` are supported to capture tracebacks in Python 2.
   104|     The traceback is automatically available in Python 3, but in the
   105|     Python 2 futures backport this information is discarded.
   106|     This functionality was previously available in a separate class
   107|     ``TracebackFuture``, which is now a deprecated alias for this class.
   108|     .. versionchanged:: 4.0
   109|        `tornado.concurrent.Future` is always a thread-unsafe ``Future``
   110|        with support for the ``exc_info`` methods.  Previously it would
   111|        be an alias for the thread-safe `concurrent.futures.Future`
   112|        if that package was available and fall back to the thread-unsafe
   113|        implementation if it was not.
   114|     .. versionchanged:: 4.1
   115|        If a `.Future` contains an error but that error is never observed
   116|        (by calling ``result()``, ``exception()``, or ``exc_info()``),
   117|        a stack trace will be logged when the `.Future` is garbage collected.
   118|        This normally indicates an error in the application, but in cases
   119|        where it results in undesired logging it may be necessary to
   120|        suppress the logging by ensuring that the exception is observed:
   121|        ``f.add_done_callback(lambda f: f.exception())``.
   122|     """
   123|     def __init__(self):
   124|         self._done = False
   125|         self._result = None
   126|         self._exc_info = None
   127|         self._log_traceback = False  # Used for Python >= 3.4
   128|         self._tb_logger = None  # Used for Python <= 3.3
   129|         self._callbacks = []
   130|     if sys.version_info >= (3, 3):
   131|         exec(
   132|             textwrap.dedent(
   133|                 """
   134|         def __await__(self):
   135|             return (yield self)
   136|         """
   137|             )
   138|         )
   139|     else:
   140|         def __await__(self):
   141|             result = yield self
   142|             e = StopIteration()
   143|             e.args = (result,)
   144|             raise e
   145|     def cancel(self):
   146|         """Cancel the operation, if possible.
   147|         Tornado ``Futures`` do not support cancellation, so this method always
   148|         returns False.
   149|         """
   150|         return False
   151|     def cancelled(self):
   152|         """Returns True if the operation has been cancelled.
   153|         Tornado ``Futures`` do not support cancellation, so this method
   154|         always returns False.
   155|         """
   156|         return False
   157|     def running(self):
   158|         """Returns True if this operation is currently running."""
   159|         return not self._done
   160|     def done(self):
   161|         """Returns True if the future has finished running."""
   162|         return self._done
   163|     def _clear_tb_log(self):
   164|         self._log_traceback = False
   165|         if self._tb_logger is not None:
   166|             self._tb_logger.clear()
   167|             self._tb_logger = None
   168|     def result(self, timeout=None):
   169|         """If the operation succeeded, return its result.  If it failed,
   170|         re-raise its exception.
   171|         This method takes a ``timeout`` argument for compatibility with
   172|         `concurrent.futures.Future` but it is an error to call it
   173|         before the `Future` is done, so the ``timeout`` is never used.
   174|         """
   175|         self._clear_tb_log()
   176|         if self._result is not None:
   177|             return self._result
   178|         if self._exc_info is not None:
   179|             try:
   180|                 raise_exc_info(self._exc_info)
   181|             finally:
   182|                 self = None
   183|         self._check_done()
   184|         return self._result
   185|     def exception(self, timeout=None):
   186|         """If the operation raised an exception, return the `Exception`
   187|         object.  Otherwise returns None.
   188|         This method takes a ``timeout`` argument for compatibility with
   189|         `concurrent.futures.Future` but it is an error to call it
   190|         before the `Future` is done, so the ``timeout`` is never used.
   191|         """
   192|         self._clear_tb_log()
   193|         if self._exc_info is not None:
   194|             return self._exc_info[1]
   195|         else:
   196|             self._check_done()
   197|             return None
   198|     def add_done_callback(self, fn):
   199|         """Attaches the given callback to the `Future`.
   200|         It will be invoked with the `Future` as its argument when the Future
   201|         has finished running and its result is available.  In Tornado
   202|         consider using `.IOLoop.add_future` instead of calling
   203|         `add_done_callback` directly.
   204|         """
   205|         if self._done:
   206|             fn(self)
   207|         else:
   208|             self._callbacks.append(fn)
   209|     def set_result(self, result):
   210|         """Sets the result of a ``Future``.
   211|         It is undefined to call any of the ``set`` methods more than once
   212|         on the same object.
   213|         """
   214|         self._result = result
   215|         self._set_done()
   216|     def set_exception(self, exception):
   217|         """Sets the exception of a ``Future.``"""
   218|         self.set_exc_info(
   219|             (exception.__class__, exception, getattr(exception, "__traceback__", None))
   220|         )
   221|     def exc_info(self):
   222|         """Returns a tuple in the same format as `sys.exc_info` or None.
   223|         .. versionadded:: 4.0
   224|         """
   225|         self._clear_tb_log()
   226|         return self._exc_info
   227|     def set_exc_info(self, exc_info):
   228|         """Sets the exception information of a ``Future.``
   229|         Preserves tracebacks on Python 2.
   230|         .. versionadded:: 4.0
   231|         """
   232|         self._exc_info = exc_info
   233|         self._log_traceback = True
   234|         if not _GC_CYCLE_FINALIZERS:
   235|             self._tb_logger = _TracebackLogger(exc_info)
   236|         try:
   237|             self._set_done()
   238|         finally:
   239|             if self._log_traceback and self._tb_logger is not None:
   240|                 self._tb_logger.activate()
   241|         self._exc_info = exc_info
   242|     def _check_done(self):
   243|         if not self._done:
   244|             raise Exception("DummyFuture does not support blocking for results")
   245|     def _set_done(self):
   246|         self._done = True
   247|         for cb in self._callbacks:
   248|             try:
   249|                 cb(self)
   250|             except Exception:
   251|                 app_log.exception("Exception in callback %r for %r", cb, self)
   252|         self._callbacks = None
   253|     if _GC_CYCLE_FINALIZERS:
   254|         def __del__(self, is_finalizing=is_finalizing):
   255|             if is_finalizing() or not self._log_traceback:
   256|                 return
   257|             tb = traceback.format_exception(*self._exc_info)
   258|             app_log.error(
   259|                 "Future %r exception was never retrieved: %s",
   260|                 self,
   261|                 "".join(tb).rstrip(),
   262|             )
   263| TracebackFuture = Future
   264| if futures is None:
   265|     FUTURES = Future  # type: typing.Union[type, typing.Tuple[type, ...]]
   266| else:
   267|     FUTURES = (futures.Future, Future)
   268| def is_future(x):
   269|     return isinstance(x, FUTURES)
   270| class DummyExecutor(object):
   271|     def submit(self, fn, *args, **kwargs):
   272|         future = TracebackFuture()
   273|         try:
   274|             future.set_result(fn(*args, **kwargs))
   275|         except Exception:
   276|             future.set_exc_info(sys.exc_info())
   277|         return future
   278|     def shutdown(self, wait=True):
   279|         pass
   280| dummy_executor = DummyExecutor()
   281| def run_on_executor(*args, **kwargs):
   282|     """Decorator to run a synchronous method asynchronously on an executor.
   283|     The decorated method may be called with a ``callback`` keyword
   284|     argument and returns a future.
   285|     The `.IOLoop` and executor to be used are determined by the ``io_loop``
   286|     and ``executor`` attributes of ``self``. To use different attributes,
   287|     pass keyword arguments to the decorator::
   288|         @run_on_executor(executor='_thread_pool')
   289|         def foo(self):
   290|             pass
   291|     .. versionchanged:: 4.2
   292|        Added keyword arguments to use alternative attributes.
   293|     """
   294|     def run_on_executor_decorator(fn):
   295|         executor = kwargs.get("executor", "executor")
   296|         io_loop = kwargs.get("io_loop", "io_loop")
   297|         @functools.wraps(fn)
   298|         def wrapper(self, *args, **kwargs):
   299|             callback = kwargs.pop("callback", None)
   300|             future = getattr(self, executor).submit(fn, self, *args, **kwargs)
   301|             if callback:
   302|                 getattr(self, io_loop).add_future(
   303|                     future, lambda future: callback(future.result())
   304|                 )
   305|             return future
   306|         return wrapper
   307|     if args and kwargs:
   308|         raise ValueError("cannot combine positional and keyword args")
   309|     if len(args) == 1:
   310|         return run_on_executor_decorator(args[0])
   311|     elif len(args) != 0:
   312|         raise ValueError("expected 1 argument, got %d", len(args))
   313|     return run_on_executor_decorator
   314| _NO_RESULT = object()
   315| def return_future(f):
   316|     """Decorator to make a function that returns via callback return a
   317|     `Future`.
   318|     The wrapped function should take a ``callback`` keyword argument
   319|     and invoke it with one argument when it has finished.  To signal failure,
   320|     the function can simply raise an exception (which will be
   321|     captured by the `.StackContext` and passed along to the ``Future``).
   322|     From the caller's perspective, the callback argument is optional.
   323|     If one is given, it will be invoked when the function is complete
   324|     with `Future.result()` as an argument.  If the function fails, the
   325|     callback will not be run and an exception will be raised into the
   326|     surrounding `.StackContext`.
   327|     If no callback is given, the caller should use the ``Future`` to
   328|     wait for the function to complete (perhaps by yielding it in a
   329|     `.gen.engine` function, or passing it to `.IOLoop.add_future`).
   330|     Usage:
   331|     .. testcode::
   332|         @return_future
   333|         def future_func(arg1, arg2, callback):
   334|             callback(result)
   335|         @gen.engine
   336|         def caller(callback):
   337|             yield future_func(arg1, arg2)
   338|             callback()
   339|     ..
   340|     Note that ``@return_future`` and ``@gen.engine`` can be applied to the
   341|     same function, provided ``@return_future`` appears first.  However,
   342|     consider using ``@gen.coroutine`` instead of this combination.
   343|     """
   344|     replacer = ArgReplacer(f, "callback")
   345|     @functools.wraps(f)
   346|     def wrapper(*args, **kwargs):
   347|         future = TracebackFuture()
   348|         callback, args, kwargs = replacer.replace(
   349|             lambda value=_NO_RESULT: future.set_result(value), args, kwargs
   350|         )
   351|         def handle_error(typ, value, tb):
   352|             future.set_exc_info((typ, value, tb))
   353|             return True
   354|         exc_info = None
   355|         with ExceptionStackContext(handle_error):
   356|             try:
   357|                 result = f(*args, **kwargs)
   358|                 if result is not None:
   359|                     raise ReturnValueIgnoredError(
   360|                         "@return_future should not be used with functions "
   361|                         "that return values"
   362|                     )
   363|             except:
   364|                 exc_info = sys.exc_info()
   365|                 raise
   366|         if exc_info is not None:
   367|             future.result()
   368|         if callback is not None:
   369|             def run_callback(future):
   370|                 result = future.result()
   371|                 if result is _NO_RESULT:
   372|                     callback()
   373|                 else:
   374|                     callback(future.result())
   375|             future.add_done_callback(wrap(run_callback))
   376|         return future
   377|     return wrapper
   378| def chain_future(a, b):
   379|     """Chain two futures together so that when one completes, so does the other.
   380|     The result (success or failure) of ``a`` will be copied to ``b``, unless
   381|     ``b`` has already been completed or cancelled by the time ``a`` finishes.
   382|     """
   383|     def copy(future):
   384|         assert future is a
   385|         if b.done():
   386|             return
   387|         if (
   388|             isinstance(a, TracebackFuture)
   389|             and isinstance(b, TracebackFuture)
   390|             and a.exc_info() is not None
   391|         ):
   392|             b.set_exc_info(a.exc_info())
   393|         elif a.exception() is not None:
   394|             b.set_exception(a.exception())
   395|         else:
   396|             b.set_result(a.result())
   397|     a.add_done_callback(copy)


# ====================================================================
# FILE: salt/ext/tornado/curl_httpclient.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-367 ---
     1| """Non-blocking HTTP client implementation using pycurl."""
     2| from __future__ import absolute_import, division, print_function
     3| import collections
     4| import functools
     5| import logging
     6| import pycurl  # type: ignore
     7| import threading
     8| import time
     9| from io import BytesIO
    10| from salt.ext.tornado import httputil
    11| from salt.ext.tornado import ioloop
    12| from salt.ext.tornado import stack_context
    13| from salt.ext.tornado.escape import utf8, native_str
    14| from salt.ext.tornado.httpclient import HTTPResponse, HTTPError, AsyncHTTPClient, main
    15| curl_log = logging.getLogger('tornado.curl_httpclient')
    16| class CurlAsyncHTTPClient(AsyncHTTPClient):
    17|     def initialize(self, io_loop, max_clients=10, defaults=None):
    18|         super(CurlAsyncHTTPClient, self).initialize(io_loop, defaults=defaults)
    19|         self._multi = pycurl.CurlMulti()
    20|         self._multi.setopt(pycurl.M_TIMERFUNCTION, self._set_timeout)
    21|         self._multi.setopt(pycurl.M_SOCKETFUNCTION, self._handle_socket)
    22|         self._curls = [self._curl_create() for i in range(max_clients)]
    23|         self._free_list = self._curls[:]
    24|         self._requests = collections.deque()
    25|         self._fds = {}
    26|         self._timeout = None
    27|         self._force_timeout_callback = ioloop.PeriodicCallback(
    28|             self._handle_force_timeout, 1000, io_loop=io_loop)
    29|         self._force_timeout_callback.start()
    30|         dummy_curl_handle = pycurl.Curl()
    31|         self._multi.add_handle(dummy_curl_handle)
    32|         self._multi.remove_handle(dummy_curl_handle)
    33|     def close(self):
    34|         self._force_timeout_callback.stop()
    35|         if self._timeout is not None:
    36|             self.io_loop.remove_timeout(self._timeout)
    37|         for curl in self._curls:
    38|             curl.close()
    39|         self._multi.close()
    40|         super(CurlAsyncHTTPClient, self).close()
    41|     def fetch_impl(self, request, callback):
    42|         self._requests.append((request, callback))
    43|         self._process_queue()
    44|         self._set_timeout(0)
    45|     def _handle_socket(self, event, fd, multi, data):
    46|         """Called by libcurl when it wants to change the file descriptors
    47|         it cares about.
    48|         """
    49|         event_map = {
    50|             pycurl.POLL_NONE: ioloop.IOLoop.NONE,
    51|             pycurl.POLL_IN: ioloop.IOLoop.READ,
    52|             pycurl.POLL_OUT: ioloop.IOLoop.WRITE,
    53|             pycurl.POLL_INOUT: ioloop.IOLoop.READ | ioloop.IOLoop.WRITE
    54|         }
    55|         if event == pycurl.POLL_REMOVE:
    56|             if fd in self._fds:
    57|                 self.io_loop.remove_handler(fd)
    58|                 del self._fds[fd]
    59|         else:
    60|             ioloop_event = event_map[event]
    61|             if fd in self._fds:
    62|                 self.io_loop.remove_handler(fd)
    63|             self.io_loop.add_handler(fd, self._handle_events,
    64|                                      ioloop_event)
    65|             self._fds[fd] = ioloop_event
    66|     def _set_timeout(self, msecs):
    67|         """Called by libcurl to schedule a timeout."""
    68|         if self._timeout is not None:
    69|             self.io_loop.remove_timeout(self._timeout)
    70|         self._timeout = self.io_loop.add_timeout(
    71|             self.io_loop.time() + msecs / 1000.0, self._handle_timeout)
    72|     def _handle_events(self, fd, events):
    73|         """Called by IOLoop when there is activity on one of our
    74|         file descriptors.
    75|         """
    76|         action = 0
    77|         if events & ioloop.IOLoop.READ:
    78|             action |= pycurl.CSELECT_IN
    79|         if events & ioloop.IOLoop.WRITE:
    80|             action |= pycurl.CSELECT_OUT
    81|         while True:
    82|             try:
    83|                 ret, num_handles = self._multi.socket_action(fd, action)
    84|             except pycurl.error as e:
    85|                 ret = e.args[0]
    86|             if ret != pycurl.E_CALL_MULTI_PERFORM:
    87|                 break
    88|         self._finish_pending_requests()
    89|     def _handle_timeout(self):
    90|         """Called by IOLoop when the requested timeout has passed."""
    91|         with stack_context.NullContext():
    92|             self._timeout = None
    93|             while True:
    94|                 try:
    95|                     ret, num_handles = self._multi.socket_action(
    96|                         pycurl.SOCKET_TIMEOUT, 0)
    97|                 except pycurl.error as e:
    98|                     ret = e.args[0]
    99|                 if ret != pycurl.E_CALL_MULTI_PERFORM:
   100|                     break
   101|             self._finish_pending_requests()
   102|         new_timeout = self._multi.timeout()
   103|         if new_timeout >= 0:
   104|             self._set_timeout(new_timeout)
   105|     def _handle_force_timeout(self):
   106|         """Called by IOLoop periodically to ask libcurl to process any
   107|         events it may have forgotten about.
   108|         """
   109|         with stack_context.NullContext():
   110|             while True:
   111|                 try:
   112|                     ret, num_handles = self._multi.socket_all()
   113|                 except pycurl.error as e:
   114|                     ret = e.args[0]
   115|                 if ret != pycurl.E_CALL_MULTI_PERFORM:
   116|                     break
   117|             self._finish_pending_requests()
   118|     def _finish_pending_requests(self):
   119|         """Process any requests that were completed by the last
   120|         call to multi.socket_action.
   121|         """
   122|         while True:
   123|             num_q, ok_list, err_list = self._multi.info_read()
   124|             for curl in ok_list:
   125|                 self._finish(curl)
   126|             for curl, errnum, errmsg in err_list:
   127|                 self._finish(curl, errnum, errmsg)
   128|             if num_q == 0:
   129|                 break
   130|         self._process_queue()
   131|     def _process_queue(self):
   132|         with stack_context.NullContext():
   133|             while True:
   134|                 started = 0
   135|                 while self._free_list and self._requests:
   136|                     started += 1
   137|                     curl = self._free_list.pop()
   138|                     (request, callback) = self._requests.popleft()
   139|                     curl.info = {
   140|                         "headers": httputil.HTTPHeaders(),
   141|                         "buffer": BytesIO(),
   142|                         "request": request,
   143|                         "callback": callback,
   144|                         "curl_start_time": time.time(),
   145|                     }
   146|                     try:
   147|                         self._curl_setup_request(
   148|                             curl, request, curl.info["buffer"],
   149|                             curl.info["headers"])
   150|                     except Exception as e:
   151|                         self._free_list.append(curl)
   152|                         callback(HTTPResponse(
   153|                             request=request,
   154|                             code=599,
   155|                             error=e))
   156|                     else:
   157|                         self._multi.add_handle(curl)
   158|                 if not started:
   159|                     break
   160|     def _finish(self, curl, curl_error=None, curl_message=None):
   161|         info = curl.info
   162|         curl.info = None
   163|         self._multi.remove_handle(curl)
   164|         self._free_list.append(curl)
   165|         buffer = info["buffer"]
   166|         if curl_error:
   167|             error = CurlError(curl_error, curl_message)
   168|             code = error.code
   169|             effective_url = None
   170|             buffer.close()
   171|             buffer = None
   172|         else:
   173|             error = None
   174|             code = curl.getinfo(pycurl.HTTP_CODE)
   175|             effective_url = curl.getinfo(pycurl.EFFECTIVE_URL)
   176|             buffer.seek(0)
   177|         time_info = dict(
   178|             queue=info["curl_start_time"] - info["request"].start_time,
   179|             namelookup=curl.getinfo(pycurl.NAMELOOKUP_TIME),
   180|             connect=curl.getinfo(pycurl.CONNECT_TIME),
   181|             pretransfer=curl.getinfo(pycurl.PRETRANSFER_TIME),
   182|             starttransfer=curl.getinfo(pycurl.STARTTRANSFER_TIME),
   183|             total=curl.getinfo(pycurl.TOTAL_TIME),
   184|             redirect=curl.getinfo(pycurl.REDIRECT_TIME),
   185|         )
   186|         try:
   187|             info["callback"](HTTPResponse(
   188|                 request=info["request"], code=code, headers=info["headers"],
   189|                 buffer=buffer, effective_url=effective_url, error=error,
   190|                 reason=info['headers'].get("X-Http-Reason", None),
   191|                 request_time=time.time() - info["curl_start_time"],
   192|                 time_info=time_info))
   193|         except Exception:
   194|             self.handle_callback_exception(info["callback"])
   195|     def handle_callback_exception(self, callback):
   196|         self.io_loop.handle_callback_exception(callback)
   197|     def _curl_create(self):
   198|         curl = pycurl.Curl()
   199|         if curl_log.isEnabledFor(logging.DEBUG):
   200|             curl.setopt(pycurl.VERBOSE, 1)
   201|             curl.setopt(pycurl.DEBUGFUNCTION, self._curl_debug)
   202|         if hasattr(pycurl, 'PROTOCOLS'):  # PROTOCOLS first appeared in pycurl 7.19.5 (2014-07-12)
   203|             curl.setopt(pycurl.PROTOCOLS, pycurl.PROTO_HTTP | pycurl.PROTO_HTTPS)
   204|             curl.setopt(pycurl.REDIR_PROTOCOLS, pycurl.PROTO_HTTP | pycurl.PROTO_HTTPS)
   205|         return curl
   206|     def _curl_setup_request(self, curl, request, buffer, headers):
   207|         curl.setopt(pycurl.URL, native_str(request.url))
   208|         if "Expect" not in request.headers:
   209|             request.headers["Expect"] = ""
   210|         if "Pragma" not in request.headers:
   211|             request.headers["Pragma"] = ""
   212|         curl.setopt(pycurl.HTTPHEADER,
   213|                     ["%s: %s" % (native_str(k), native_str(v))
   214|                      for k, v in request.headers.get_all()])
   215|         curl.setopt(pycurl.HEADERFUNCTION,
   216|                     functools.partial(self._curl_header_callback,
   217|                                       headers, request.header_callback))
   218|         if request.streaming_callback:
   219|             def write_function(chunk):
   220|                 self.io_loop.add_callback(request.streaming_callback, chunk)
   221|         else:
   222|             write_function = buffer.write
   223|         if bytes is str:  # py2
   224|             curl.setopt(pycurl.WRITEFUNCTION, write_function)
   225|         else:  # py3
   226|             curl.setopt(pycurl.WRITEFUNCTION, lambda s: write_function(utf8(s)))
   227|         curl.setopt(pycurl.FOLLOWLOCATION, request.follow_redirects)
   228|         curl.setopt(pycurl.MAXREDIRS, request.max_redirects)
   229|         curl.setopt(pycurl.CONNECTTIMEOUT_MS, int(1000 * request.connect_timeout))
   230|         curl.setopt(pycurl.TIMEOUT_MS, int(1000 * request.request_timeout))
   231|         if request.user_agent:
   232|             curl.setopt(pycurl.USERAGENT, native_str(request.user_agent))
   233|         else:
   234|             curl.setopt(pycurl.USERAGENT, "Mozilla/5.0 (compatible; pycurl)")
   235|         if request.network_interface:
   236|             curl.setopt(pycurl.INTERFACE, request.network_interface)
   237|         if request.decompress_response:
   238|             curl.setopt(pycurl.ENCODING, "gzip,deflate")
   239|         else:
   240|             curl.setopt(pycurl.ENCODING, "none")
   241|         if request.proxy_host and request.proxy_port:
   242|             curl.setopt(pycurl.PROXY, request.proxy_host)
   243|             curl.setopt(pycurl.PROXYPORT, request.proxy_port)
   244|             if request.proxy_username:
   245|                 credentials = '%s:%s' % (request.proxy_username,
   246|                                          request.proxy_password)
   247|                 curl.setopt(pycurl.PROXYUSERPWD, credentials)
   248|             if (request.proxy_auth_mode is None or
   249|                     request.proxy_auth_mode == "basic"):
   250|                 curl.setopt(pycurl.PROXYAUTH, pycurl.HTTPAUTH_BASIC)
   251|             elif request.proxy_auth_mode == "digest":
   252|                 curl.setopt(pycurl.PROXYAUTH, pycurl.HTTPAUTH_DIGEST)
   253|             else:
   254|                 raise ValueError(
   255|                     "Unsupported proxy_auth_mode %s" % request.proxy_auth_mode)
   256|         else:
   257|             curl.setopt(pycurl.PROXY, '')
   258|             curl.unsetopt(pycurl.PROXYUSERPWD)
   259|         if request.validate_cert:
   260|             curl.setopt(pycurl.SSL_VERIFYPEER, 1)
   261|             curl.setopt(pycurl.SSL_VERIFYHOST, 2)
   262|         else:
   263|             curl.setopt(pycurl.SSL_VERIFYPEER, 0)
   264|             curl.setopt(pycurl.SSL_VERIFYHOST, 0)
   265|         if request.ca_certs is not None:
   266|             curl.setopt(pycurl.CAINFO, request.ca_certs)
   267|         else:
   268|             pass
   269|         if request.allow_ipv6 is False:
   270|             curl.setopt(pycurl.IPRESOLVE, pycurl.IPRESOLVE_V4)
   271|         else:
   272|             curl.setopt(pycurl.IPRESOLVE, pycurl.IPRESOLVE_WHATEVER)
   273|         curl_options = {
   274|             "GET": pycurl.HTTPGET,
   275|             "POST": pycurl.POST,
   276|             "PUT": pycurl.UPLOAD,
   277|             "HEAD": pycurl.NOBODY,
   278|         }
   279|         custom_methods = set(["DELETE", "OPTIONS", "PATCH"])
   280|         for o in curl_options.values():
   281|             curl.setopt(o, False)
   282|         if request.method in curl_options:
   283|             curl.unsetopt(pycurl.CUSTOMREQUEST)
   284|             curl.setopt(curl_options[request.method], True)
   285|         elif request.allow_nonstandard_methods or request.method in custom_methods:
   286|             curl.setopt(pycurl.CUSTOMREQUEST, request.method)
   287|         else:
   288|             raise KeyError('unknown method ' + request.method)
   289|         body_expected = request.method in ("POST", "PATCH", "PUT")
   290|         body_present = request.body is not None
   291|         if not request.allow_nonstandard_methods:
   292|             if ((body_expected and not body_present) or
   293|                     (body_present and not body_expected)):
   294|                 raise ValueError(
   295|                     'Body must %sbe None for method %s (unless '
   296|                     'allow_nonstandard_methods is true)' %
   297|                     ('not ' if body_expected else '', request.method))
   298|         if body_expected or body_present:
   299|             if request.method == "GET":
   300|                 raise ValueError('Body must be None for GET request')
   301|             request_buffer = BytesIO(utf8(request.body or ''))
   302|             def ioctl(cmd):
   303|                 if cmd == curl.IOCMD_RESTARTREAD:
   304|                     request_buffer.seek(0)
   305|             curl.setopt(pycurl.READFUNCTION, request_buffer.read)
   306|             curl.setopt(pycurl.IOCTLFUNCTION, ioctl)
   307|             if request.method == "POST":
   308|                 curl.setopt(pycurl.POSTFIELDSIZE, len(request.body or ''))
   309|             else:
   310|                 curl.setopt(pycurl.UPLOAD, True)
   311|                 curl.setopt(pycurl.INFILESIZE, len(request.body or ''))
   312|         if request.auth_username is not None:
   313|             userpwd = "%s:%s" % (request.auth_username, request.auth_password or '')
   314|             if request.auth_mode is None or request.auth_mode == "basic":
   315|                 curl.setopt(pycurl.HTTPAUTH, pycurl.HTTPAUTH_BASIC)
   316|             elif request.auth_mode == "digest":
   317|                 curl.setopt(pycurl.HTTPAUTH, pycurl.HTTPAUTH_DIGEST)
   318|             else:
   319|                 raise ValueError("Unsupported auth_mode %s" % request.auth_mode)
   320|             curl.setopt(pycurl.USERPWD, native_str(userpwd))
   321|             curl_log.debug("%s %s (username: %r)", request.method, request.url,
   322|                            request.auth_username)
   323|         else:
   324|             curl.unsetopt(pycurl.USERPWD)
   325|             curl_log.debug("%s %s", request.method, request.url)
   326|         if request.client_cert is not None:
   327|             curl.setopt(pycurl.SSLCERT, request.client_cert)
   328|         if request.client_key is not None:
   329|             curl.setopt(pycurl.SSLKEY, request.client_key)
   330|         if request.ssl_options is not None:
   331|             raise ValueError("ssl_options not supported in curl_httpclient")
   332|         if threading.activeCount() > 1:
   333|             curl.setopt(pycurl.NOSIGNAL, 1)
   334|         if request.prepare_curl_callback is not None:
   335|             request.prepare_curl_callback(curl)
   336|     def _curl_header_callback(self, headers, header_callback, header_line):
   337|         header_line = native_str(header_line.decode('latin1'))
   338|         if header_callback is not None:
   339|             self.io_loop.add_callback(header_callback, header_line)
   340|         header_line = header_line.rstrip()
   341|         if header_line.startswith("HTTP/"):
   342|             headers.clear()
   343|             try:
   344|                 (__, __, reason) = httputil.parse_response_start_line(header_line)
   345|                 header_line = "X-Http-Reason: %s" % reason
   346|             except httputil.HTTPInputError:
   347|                 return
   348|         if not header_line:
   349|             return
   350|         headers.parse_line(header_line)
   351|     def _curl_debug(self, debug_type, debug_msg):
   352|         debug_types = ('I', '<', '>', '<', '>')
   353|         debug_msg = native_str(debug_msg)
   354|         if debug_type == 0:
   355|             curl_log.debug('%s', debug_msg.strip())
   356|         elif debug_type in (1, 2):
   357|             for line in debug_msg.splitlines():
   358|                 curl_log.debug('%s %s', debug_types[debug_type], line)
   359|         elif debug_type == 4:
   360|             curl_log.debug('%s %r', debug_types[debug_type], debug_msg)
   361| class CurlError(HTTPError):
   362|     def __init__(self, errno, message):
   363|         HTTPError.__init__(self, 599, message)
   364|         self.errno = errno
   365| if __name__ == "__main__":
   366|     AsyncHTTPClient.configure(CurlAsyncHTTPClient)
   367|     main()


# ====================================================================
# FILE: salt/ext/tornado/escape.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-258 ---
     1| """Escaping/unescaping methods for HTML, JSON, URLs, and others.
     2| Also includes a few other miscellaneous string manipulation functions that
     3| have crept in over time.
     4| """
     5| from __future__ import absolute_import, division, print_function
     6| import json
     7| import re
     8| from salt.ext.tornado.util import PY3, unicode_type, basestring_type
     9| if PY3:
    10|     from urllib.parse import parse_qs as _parse_qs
    11|     import html.entities as htmlentitydefs
    12|     import urllib.parse as urllib_parse
    13|     unichr = chr
    14| else:
    15|     from urlparse import parse_qs as _parse_qs
    16|     import htmlentitydefs
    17|     import urllib as urllib_parse
    18| try:
    19|     import typing  # noqa
    20| except ImportError:
    21|     pass
    22| _XHTML_ESCAPE_RE = re.compile('[&<>"\']')
    23| _XHTML_ESCAPE_DICT = {'&': '&amp;', '<': '&lt;', '>': '&gt;', '"': '&quot;',
    24|                       '\'': '&#39;'}
    25| def xhtml_escape(value):
    26|     """Escapes a string so it is valid within HTML or XML.
    27|     Escapes the characters ``<``, ``>``, ``"``, ``'``, and ``&``.
    28|     When used in attribute values the escaped strings must be enclosed
    29|     in quotes.
    30|     .. versionchanged:: 3.2
    31|        Added the single quote to the list of escaped characters.
    32|     """
    33|     return _XHTML_ESCAPE_RE.sub(lambda match: _XHTML_ESCAPE_DICT[match.group(0)],
    34|                                 to_basestring(value))
    35| def xhtml_unescape(value):
    36|     """Un-escapes an XML-escaped string."""
    37|     return re.sub(r"&(#?)(\w+?);", _convert_entity, _unicode(value))
    38| def json_encode(value):
    39|     """JSON-encodes the given Python object."""
    40|     return json.dumps(value).replace("</", "<\\/")
    41| def json_decode(value):
    42|     """Returns Python objects for the given JSON string."""
    43|     return json.loads(to_basestring(value))
    44| def squeeze(value):
    45|     """Replace all sequences of whitespace chars with a single space."""
    46|     return re.sub(r"[\x00-\x20]+", " ", value).strip()
    47| def url_escape(value, plus=True):
    48|     """Returns a URL-encoded version of the given value.
    49|     If ``plus`` is true (the default), spaces will be represented
    50|     as "+" instead of "%20".  This is appropriate for query strings
    51|     but not for the path component of a URL.  Note that this default
    52|     is the reverse of Python's urllib module.
    53|     .. versionadded:: 3.1
    54|         The ``plus`` argument
    55|     """
    56|     quote = urllib_parse.quote_plus if plus else urllib_parse.quote
    57|     return quote(utf8(value))
    58| if not PY3:
    59|     def url_unescape(value, encoding='utf-8', plus=True):
    60|         """Decodes the given value from a URL.
    61|         The argument may be either a byte or unicode string.
    62|         If encoding is None, the result will be a byte string.  Otherwise,
    63|         the result is a unicode string in the specified encoding.
    64|         If ``plus`` is true (the default), plus signs will be interpreted
    65|         as spaces (literal plus signs must be represented as "%2B").  This
    66|         is appropriate for query strings and form-encoded values but not
    67|         for the path component of a URL.  Note that this default is the
    68|         reverse of Python's urllib module.
    69|         .. versionadded:: 3.1
    70|            The ``plus`` argument
    71|         """
    72|         unquote = (urllib_parse.unquote_plus if plus else urllib_parse.unquote)
    73|         if encoding is None:
    74|             return unquote(utf8(value))
    75|         else:
    76|             return unicode_type(unquote(utf8(value)), encoding)
    77|     parse_qs_bytes = _parse_qs
    78| else:
    79|     def url_unescape(value, encoding='utf-8', plus=True):
    80|         """Decodes the given value from a URL.
    81|         The argument may be either a byte or unicode string.
    82|         If encoding is None, the result will be a byte string.  Otherwise,
    83|         the result is a unicode string in the specified encoding.
    84|         If ``plus`` is true (the default), plus signs will be interpreted
    85|         as spaces (literal plus signs must be represented as "%2B").  This
    86|         is appropriate for query strings and form-encoded values but not
    87|         for the path component of a URL.  Note that this default is the
    88|         reverse of Python's urllib module.
    89|         .. versionadded:: 3.1
    90|            The ``plus`` argument
    91|         """
    92|         if encoding is None:
    93|             if plus:
    94|                 value = to_basestring(value).replace('+', ' ')
    95|             return urllib_parse.unquote_to_bytes(value)
    96|         else:
    97|             unquote = (urllib_parse.unquote_plus if plus
    98|                        else urllib_parse.unquote)
    99|             return unquote(to_basestring(value), encoding=encoding)
   100|     def parse_qs_bytes(qs, keep_blank_values=False, strict_parsing=False):
   101|         """Parses a query string like urlparse.parse_qs, but returns the
   102|         values as byte strings.
   103|         Keys still become type str (interpreted as latin1 in python3!)
   104|         because it's too painful to keep them as byte strings in
   105|         python3 and in practice they're nearly always ascii anyway.
   106|         """
   107|         result = _parse_qs(qs, keep_blank_values, strict_parsing,
   108|                            encoding='latin1', errors='strict')
   109|         encoded = {}
   110|         for k, v in result.items():
   111|             encoded[k] = [i.encode('latin1') for i in v]
   112|         return encoded
   113| _UTF8_TYPES = (bytes, type(None))
   114| def utf8(value):
   115|     """Converts a string argument to a byte string.
   116|     If the argument is already a byte string or None, it is returned unchanged.
   117|     Otherwise it must be a unicode string and is encoded as utf8.
   118|     """
   119|     if isinstance(value, _UTF8_TYPES):
   120|         return value
   121|     if not isinstance(value, unicode_type):
   122|         raise TypeError(
   123|             "Expected bytes, unicode, or None; got %r" % type(value)
   124|         )
   125|     return value.encode("utf-8")
   126| _TO_UNICODE_TYPES = (unicode_type, type(None))
   127| def to_unicode(value):
   128|     """Converts a string argument to a unicode string.
   129|     If the argument is already a unicode string or None, it is returned
   130|     unchanged.  Otherwise it must be a byte string and is decoded as utf8.
   131|     """
   132|     if isinstance(value, _TO_UNICODE_TYPES):
   133|         return value
   134|     if not isinstance(value, bytes):
   135|         raise TypeError(
   136|             "Expected bytes, unicode, or None; got %r" % type(value)
   137|         )
   138|     return value.decode("utf-8")
   139| _unicode = to_unicode
   140| if str is unicode_type:
   141|     native_str = to_unicode
   142| else:
   143|     native_str = utf8
   144| _BASESTRING_TYPES = (basestring_type, type(None))
   145| def to_basestring(value):
   146|     """Converts a string argument to a subclass of basestring.
   147|     In python2, byte and unicode strings are mostly interchangeable,
   148|     so functions that deal with a user-supplied argument in combination
   149|     with ascii string constants can use either and should return the type
   150|     the user supplied.  In python3, the two types are not interchangeable,
   151|     so this method is needed to convert byte strings to unicode.
   152|     """
   153|     if isinstance(value, _BASESTRING_TYPES):
   154|         return value
   155|     if not isinstance(value, bytes):
   156|         raise TypeError(
   157|             "Expected bytes, unicode, or None; got %r" % type(value)
   158|         )
   159|     return value.decode("utf-8")
   160| def recursive_unicode(obj):
   161|     """Walks a simple data structure, converting byte strings to unicode.
   162|     Supports lists, tuples, and dictionaries.
   163|     """
   164|     if isinstance(obj, dict):
   165|         return dict((recursive_unicode(k), recursive_unicode(v)) for (k, v) in obj.items())
   166|     elif isinstance(obj, list):
   167|         return list(recursive_unicode(i) for i in obj)
   168|     elif isinstance(obj, tuple):
   169|         return tuple(recursive_unicode(i) for i in obj)
   170|     elif isinstance(obj, bytes):
   171|         return to_unicode(obj)
   172|     else:
   173|         return obj
   174| _URL_RE = re.compile(to_unicode(r"""\b((?:([\w-]+):(/{1,3})|www[.])(?:(?:(?:[^\s&()]|&amp;|&quot;)*(?:[^!"#$%&'()*+,.:;<=>?@\[\]^`{|}~\s]))|(?:\((?:[^\s&()]|&amp;|&quot;)*\)))+)"""))
   175| def linkify(text, shorten=False, extra_params="",
   176|             require_protocol=False, permitted_protocols=["http", "https"]):
   177|     """Converts plain text into HTML with links.
   178|     For example: ``linkify("Hello http://tornadoweb.org!")`` would return
   179|     ``Hello <a href="http://tornadoweb.org">http://tornadoweb.org</a>!``
   180|     Parameters:
   181|     * ``shorten``: Long urls will be shortened for display.
   182|     * ``extra_params``: Extra text to include in the link tag, or a callable
   183|         taking the link as an argument and returning the extra text
   184|         e.g. ``linkify(text, extra_params='rel="nofollow" class="external"')``,
   185|         or::
   186|             def extra_params_cb(url):
   187|                 if url.startswith("http://example.com"):
   188|                     return 'class="internal"'
   189|                 else:
   190|                     return 'class="external" rel="nofollow"'
   191|             linkify(text, extra_params=extra_params_cb)
   192|     * ``require_protocol``: Only linkify urls which include a protocol. If
   193|         this is False, urls such as www.facebook.com will also be linkified.
   194|     * ``permitted_protocols``: List (or set) of protocols which should be
   195|         linkified, e.g. ``linkify(text, permitted_protocols=["http", "ftp",
   196|         "mailto"])``. It is very unsafe to include protocols such as
   197|         ``javascript``.
   198|     """
   199|     if extra_params and not callable(extra_params):
   200|         extra_params = " " + extra_params.strip()
   201|     def make_link(m):
   202|         url = m.group(1)
   203|         proto = m.group(2)
   204|         if require_protocol and not proto:
   205|             return url  # not protocol, no linkify
   206|         if proto and proto not in permitted_protocols:
   207|             return url  # bad protocol, no linkify
   208|         href = m.group(1)
   209|         if not proto:
   210|             href = "http://" + href   # no proto specified, use http
   211|         if callable(extra_params):
   212|             params = " " + extra_params(href).strip()
   213|         else:
   214|             params = extra_params
   215|         max_len = 30
   216|         if shorten and len(url) > max_len:
   217|             before_clip = url
   218|             if proto:
   219|                 proto_len = len(proto) + 1 + len(m.group(3) or "")  # +1 for :
   220|             else:
   221|                 proto_len = 0
   222|             parts = url[proto_len:].split("/")
   223|             if len(parts) > 1:
   224|                 url = url[:proto_len] + parts[0] + "/" + \
   225|                     parts[1][:8].split('?')[0].split('.')[0]
   226|             if len(url) > max_len * 1.5:  # still too long
   227|                 url = url[:max_len]
   228|             if url != before_clip:
   229|                 amp = url.rfind('&')
   230|                 if amp > max_len - 5:
   231|                     url = url[:amp]
   232|                 url += "..."
   233|                 if len(url) >= len(before_clip):
   234|                     url = before_clip
   235|                 else:
   236|                     params += ' title="%s"' % href
   237|         return u'<a href="%s"%s>%s</a>' % (href, params, url)
   238|     text = _unicode(xhtml_escape(text))
   239|     return _URL_RE.sub(make_link, text)
   240| def _convert_entity(m):
   241|     if m.group(1) == "#":
   242|         try:
   243|             if m.group(2)[:1].lower() == 'x':
   244|                 return unichr(int(m.group(2)[1:], 16))
   245|             else:
   246|                 return unichr(int(m.group(2)))
   247|         except ValueError:
   248|             return "&#%s;" % m.group(2)
   249|     try:
   250|         return _HTML_UNICODE_MAP[m.group(2)]
   251|     except KeyError:
   252|         return "&%s;" % m.group(2)
   253| def _build_unicode_map():
   254|     unicode_map = {}
   255|     for name, value in htmlentitydefs.name2codepoint.items():
   256|         unicode_map[name] = unichr(value)
   257|     return unicode_map
   258| _HTML_UNICODE_MAP = _build_unicode_map()


# ====================================================================
# FILE: salt/ext/tornado/gen.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-962 ---
     1| """``tornado.gen`` is a generator-based interface to make it easier to
     2| work in an asynchronous environment.  Code using the ``gen`` module
     3| is technically asynchronous, but it is written as a single generator
     4| instead of a collection of separate functions.
     5| For example, the following asynchronous handler:
     6| .. testcode::
     7|     class AsyncHandler(RequestHandler):
     8|         @asynchronous
     9|         def get(self):
    10|             http_client = AsyncHTTPClient()
    11|             http_client.fetch("http://example.com",
    12|                               callback=self.on_fetch)
    13|         def on_fetch(self, response):
    14|             do_something_with_response(response)
    15|             self.render("template.html")
    16| .. testoutput::
    17|    :hide:
    18| could be written with ``gen`` as:
    19| .. testcode::
    20|     class GenAsyncHandler(RequestHandler):
    21|         @gen.coroutine
    22|         def get(self):
    23|             http_client = AsyncHTTPClient()
    24|             response = yield http_client.fetch("http://example.com")
    25|             do_something_with_response(response)
    26|             self.render("template.html")
    27| .. testoutput::
    28|    :hide:
    29| Most asynchronous functions in Tornado return a `.Future`;
    30| yielding this object returns its `~.Future.result`.
    31| You can also yield a list or dict of ``Futures``, which will be
    32| started at the same time and run in parallel; a list or dict of results will
    33| be returned when they are all finished:
    34| .. testcode::
    35|     @gen.coroutine
    36|     def get(self):
    37|         http_client = AsyncHTTPClient()
    38|         response1, response2 = yield [http_client.fetch(url1),
    39|                                       http_client.fetch(url2)]
    40|         response_dict = yield dict(response3=http_client.fetch(url3),
    41|                                    response4=http_client.fetch(url4))
    42|         response3 = response_dict['response3']
    43|         response4 = response_dict['response4']
    44| .. testoutput::
    45|    :hide:
    46| If the `~functools.singledispatch` library is available (standard in
    47| Python 3.4, available via the `singledispatch
    48| <https://pypi.python.org/pypi/singledispatch>`_ package on older
    49| versions), additional types of objects may be yielded. Tornado includes
    50| support for ``asyncio.Future`` and Twisted's ``Deferred`` class when
    51| ``tornado.platform.asyncio`` and ``tornado.platform.twisted`` are imported.
    52| See the `convert_yielded` function to extend this mechanism.
    53| .. versionchanged:: 3.2
    54|    Dict support added.
    55| .. versionchanged:: 4.1
    56|    Support added for yielding ``asyncio`` Futures and Twisted Deferreds
    57|    via ``singledispatch``.
    58| """
    59| from __future__ import absolute_import, division, print_function
    60| import collections
    61| import functools
    62| import itertools
    63| import os
    64| import sys
    65| import textwrap
    66| import types
    67| import weakref
    68| import salt.ext.tornado as tornado
    69| from salt.ext.tornado.concurrent import Future, TracebackFuture, is_future, chain_future
    70| from salt.ext.tornado.ioloop import IOLoop
    71| from salt.ext.tornado.log import app_log
    72| from salt.ext.tornado import stack_context
    73| from salt.ext.tornado.util import PY3, raise_exc_info
    74| try:
    75|     try:
    76|         from functools import singledispatch  # type: ignore
    77|     except ImportError:
    78|         from singledispatch import singledispatch  # backport
    79| except ImportError:
    80|     if 'APPENGINE_RUNTIME' not in os.environ:
    81|         raise
    82|     singledispatch = None
    83| try:
    84|     try:
    85|         from collections.abc import Generator as GeneratorType  # type: ignore
    86|     except ImportError:
    87|         from salt.ext.backports_abc import Generator as GeneratorType  # type: ignore
    88|     try:
    89|         from inspect import isawaitable  # type: ignore
    90|     except ImportError:
    91|         from salt.ext.backports_abc import isawaitable
    92| except ImportError:
    93|     if 'APPENGINE_RUNTIME' not in os.environ:
    94|         raise
    95|     from types import GeneratorType
    96|     def isawaitable(x):  # type: ignore
    97|         return False
    98| if PY3:
    99|     import builtins
   100| else:
   101|     import __builtin__ as builtins
   102| class KeyReuseError(Exception):
   103|     pass
   104| class UnknownKeyError(Exception):
   105|     pass
   106| class LeakedCallbackError(Exception):
   107|     pass
   108| class BadYieldError(Exception):
   109|     pass
   110| class ReturnValueIgnoredError(Exception):
   111|     pass
   112| class TimeoutError(Exception):
   113|     """Exception raised by ``with_timeout``."""
   114| def _value_from_stopiteration(e):
   115|     try:
   116|         return e.value
   117|     except AttributeError:
   118|         pass
   119|     try:
   120|         return e.args[0]
   121|     except (AttributeError, IndexError):
   122|         return None
   123| def engine(func):
   124|     """Callback-oriented decorator for asynchronous generators.
   125|     This is an older interface; for new code that does not need to be
   126|     compatible with versions of Tornado older than 3.0 the
   127|     `coroutine` decorator is recommended instead.
   128|     This decorator is similar to `coroutine`, except it does not
   129|     return a `.Future` and the ``callback`` argument is not treated
   130|     specially.
   131|     In most cases, functions decorated with `engine` should take
   132|     a ``callback`` argument and invoke it with their result when
   133|     they are finished.  One notable exception is the
   134|     `~tornado.web.RequestHandler` :ref:`HTTP verb methods <verbs>`,
   135|     which use ``self.finish()`` in place of a callback argument.
   136|     """
   137|     func = _make_coroutine_wrapper(func, replace_callback=False)
   138|     @functools.wraps(func)
   139|     def wrapper(*args, **kwargs):
   140|         future = func(*args, **kwargs)
   141|         def final_callback(future):
   142|             if future.result() is not None:
   143|                 raise ReturnValueIgnoredError(
   144|                     "@gen.engine functions cannot return values: %r" %
   145|                     (future.result(),))
   146|         future.add_done_callback(stack_context.wrap(final_callback))
   147|     return wrapper
   148| def coroutine(func, replace_callback=True):
   149|     """Decorator for asynchronous generators.
   150|     Any generator that yields objects from this module must be wrapped
   151|     in either this decorator or `engine`.
   152|     Coroutines may "return" by raising the special exception
   153|     `Return(value) <Return>`.  In Python 3.3+, it is also possible for
   154|     the function to simply use the ``return value`` statement (prior to
   155|     Python 3.3 generators were not allowed to also return values).
   156|     In all versions of Python a coroutine that simply wishes to exit
   157|     early may use the ``return`` statement without a value.
   158|     Functions with this decorator return a `.Future`.  Additionally,
   159|     they may be called with a ``callback`` keyword argument, which
   160|     will be invoked with the future's result when it resolves.  If the
   161|     coroutine fails, the callback will not be run and an exception
   162|     will be raised into the surrounding `.StackContext`.  The
   163|     ``callback`` argument is not visible inside the decorated
   164|     function; it is handled by the decorator itself.
   165|     From the caller's perspective, ``@gen.coroutine`` is similar to
   166|     the combination of ``@return_future`` and ``@gen.engine``.
   167|     .. warning::
   168|        When exceptions occur inside a coroutine, the exception
   169|        information will be stored in the `.Future` object. You must
   170|        examine the result of the `.Future` object, or the exception
   171|        may go unnoticed by your code. This means yielding the function
   172|        if called from another coroutine, using something like
   173|        `.IOLoop.run_sync` for top-level calls, or passing the `.Future`
   174|        to `.IOLoop.add_future`.
   175|     """
   176|     return _make_coroutine_wrapper(func, replace_callback=True)
   177| _futures_to_runners = weakref.WeakKeyDictionary()
   178| def _make_coroutine_wrapper(func, replace_callback):
   179|     """The inner workings of ``@gen.coroutine`` and ``@gen.engine``.
   180|     The two decorators differ in their treatment of the ``callback``
   181|     argument, so we cannot simply implement ``@engine`` in terms of
   182|     ``@coroutine``.
   183|     """
   184|     wrapped = func
   185|     if hasattr(types, 'coroutine'):
   186|         func = types.coroutine(func)
   187|     @functools.wraps(wrapped)
   188|     def wrapper(*args, **kwargs):
   189|         future = TracebackFuture()
   190|         if replace_callback and 'callback' in kwargs:
   191|             callback = kwargs.pop('callback')
   192|             IOLoop.current().add_future(
   193|                 future, lambda future: callback(future.result()))
   194|         try:
   195|             result = func(*args, **kwargs)
   196|         except (Return, StopIteration) as e:
   197|             result = _value_from_stopiteration(e)
   198|         except Exception:
   199|             future.set_exc_info(sys.exc_info())
   200|             return future
   201|         else:
   202|             if isinstance(result, GeneratorType):
   203|                 try:
   204|                     orig_stack_contexts = stack_context._state.contexts
   205|                     yielded = next(result)
   206|                     if stack_context._state.contexts is not orig_stack_contexts:
   207|                         yielded = TracebackFuture()
   208|                         yielded.set_exception(
   209|                             stack_context.StackContextInconsistentError(
   210|                                 'stack_context inconsistency (probably caused '
   211|                                 'by yield within a "with StackContext" block)'))
   212|                 except (StopIteration, Return) as e:
   213|                     future.set_result(_value_from_stopiteration(e))
   214|                 except Exception:
   215|                     future.set_exc_info(sys.exc_info())
   216|                 else:
   217|                     _futures_to_runners[future] = Runner(result, future, yielded)
   218|                 yielded = None
   219|                 try:
   220|                     return future
   221|                 finally:
   222|                     future = None
   223|         future.set_result(result)
   224|         return future
   225|     wrapper.__wrapped__ = wrapped
   226|     wrapper.__tornado_coroutine__ = True
   227|     return wrapper
   228| def is_coroutine_function(func):
   229|     """Return whether *func* is a coroutine function, i.e. a function
   230|     wrapped with `~.gen.coroutine`.
   231|     .. versionadded:: 4.5
   232|     """
   233|     return getattr(func, '__tornado_coroutine__', False)
   234| class Return(Exception):
   235|     """Special exception to return a value from a `coroutine`.
   236|     If this exception is raised, its value argument is used as the
   237|     result of the coroutine::
   238|         @gen.coroutine
   239|         def fetch_json(url):
   240|             response = yield AsyncHTTPClient().fetch(url)
   241|             raise gen.Return(json_decode(response.body))
   242|     In Python 3.3, this exception is no longer necessary: the ``return``
   243|     statement can be used directly to return a value (previously
   244|     ``yield`` and ``return`` with a value could not be combined in the
   245|     same function).
   246|     By analogy with the return statement, the value argument is optional,
   247|     but it is never necessary to ``raise gen.Return()``.  The ``return``
   248|     statement can be used with no arguments instead.
   249|     """
   250|     def __init__(self, value=None):
   251|         super(Return, self).__init__()
   252|         self.value = value
   253|         self.args = (value,)
   254| class WaitIterator(object):
   255|     """Provides an iterator to yield the results of futures as they finish.
   256|     Yielding a set of futures like this:
   257|     ``results = yield [future1, future2]``
   258|     pauses the coroutine until both ``future1`` and ``future2``
   259|     return, and then restarts the coroutine with the results of both
   260|     futures. If either future is an exception, the expression will
   261|     raise that exception and all the results will be lost.
   262|     If you need to get the result of each future as soon as possible,
   263|     or if you need the result of some futures even if others produce
   264|     errors, you can use ``WaitIterator``::
   265|       wait_iterator = gen.WaitIterator(future1, future2)
   266|       while not wait_iterator.done():
   267|           try:
   268|               result = yield wait_iterator.next()
   269|           except Exception as e:
   270|               print("Error {} from {}".format(e, wait_iterator.current_future))
   271|           else:
   272|               print("Result {} received from {} at {}".format(
   273|                   result, wait_iterator.current_future,
   274|                   wait_iterator.current_index))
   275|     Because results are returned as soon as they are available the
   276|     output from the iterator *will not be in the same order as the
   277|     input arguments*. If you need to know which future produced the
   278|     current result, you can use the attributes
   279|     ``WaitIterator.current_future``, or ``WaitIterator.current_index``
   280|     to get the index of the future from the input list. (if keyword
   281|     arguments were used in the construction of the `WaitIterator`,
   282|     ``current_index`` will use the corresponding keyword).
   283|     On Python 3.5, `WaitIterator` implements the async iterator
   284|     protocol, so it can be used with the ``async for`` statement (note
   285|     that in this version the entire iteration is aborted if any value
   286|     raises an exception, while the previous example can continue past
   287|     individual errors)::
   288|       async for result in gen.WaitIterator(future1, future2):
   289|           print("Result {} received from {} at {}".format(
   290|               result, wait_iterator.current_future,
   291|               wait_iterator.current_index))
   292|     .. versionadded:: 4.1
   293|     .. versionchanged:: 4.3
   294|        Added ``async for`` support in Python 3.5.
   295|     """
   296|     def __init__(self, *args, **kwargs):
   297|         if args and kwargs:
   298|             raise ValueError(
   299|                 "You must provide args or kwargs, not both")
   300|         if kwargs:
   301|             self._unfinished = dict((f, k) for (k, f) in kwargs.items())
   302|             futures = list(kwargs.values())
   303|         else:
   304|             self._unfinished = dict((f, i) for (i, f) in enumerate(args))
   305|             futures = args
   306|         self._finished = collections.deque()
   307|         self.current_index = self.current_future = None
   308|         self._running_future = None
   309|         for future in futures:
   310|             future.add_done_callback(self._done_callback)
   311|     def done(self):
   312|         """Returns True if this iterator has no more results."""
   313|         if self._finished or self._unfinished:
   314|             return False
   315|         self.current_index = self.current_future = None
   316|         return True
   317|     def next(self):
   318|         """Returns a `.Future` that will yield the next available result.
   319|         Note that this `.Future` will not be the same object as any of
   320|         the inputs.
   321|         """
   322|         self._running_future = TracebackFuture()
   323|         if self._finished:
   324|             self._return_result(self._finished.popleft())
   325|         return self._running_future
   326|     def _done_callback(self, done):
   327|         if self._running_future and not self._running_future.done():
   328|             self._return_result(done)
   329|         else:
   330|             self._finished.append(done)
   331|     def _return_result(self, done):
   332|         """Called set the returned future's state that of the future
   333|         we yielded, and set the current future for the iterator.
   334|         """
   335|         chain_future(done, self._running_future)
   336|         self.current_future = done
   337|         self.current_index = self._unfinished.pop(done)
   338|     def __aiter__(self):
   339|         raise self
   340|     def __anext__(self):
   341|         if self.done():
   342|             raise getattr(builtins, 'StopAsyncIteration')()
   343|         return self.next()
   344| class YieldPoint(object):
   345|     """Base class for objects that may be yielded from the generator.
   346|     .. deprecated:: 4.0
   347|        Use `Futures <.Future>` instead.
   348|     """
   349|     def start(self, runner):
   350|         """Called by the runner after the generator has yielded.
   351|         No other methods will be called on this object before ``start``.
   352|         """
   353|         raise NotImplementedError()
   354|     def is_ready(self):
   355|         """Called by the runner to determine whether to resume the generator.
   356|         Returns a boolean; may be called more than once.
   357|         """
   358|         raise NotImplementedError()
   359|     def get_result(self):
   360|         """Returns the value to use as the result of the yield expression.
   361|         This method will only be called once, and only after `is_ready`
   362|         has returned true.
   363|         """
   364|         raise NotImplementedError()
   365| class Callback(YieldPoint):
   366|     """Returns a callable object that will allow a matching `Wait` to proceed.
   367|     The key may be any value suitable for use as a dictionary key, and is
   368|     used to match ``Callbacks`` to their corresponding ``Waits``.  The key
   369|     must be unique among outstanding callbacks within a single run of the
   370|     generator function, but may be reused across different runs of the same
   371|     function (so constants generally work fine).
   372|     The callback may be called with zero or one arguments; if an argument
   373|     is given it will be returned by `Wait`.
   374|     .. deprecated:: 4.0
   375|        Use `Futures <.Future>` instead.
   376|     """
   377|     def __init__(self, key):
   378|         self.key = key
   379|     def start(self, runner):
   380|         self.runner = runner
   381|         runner.register_callback(self.key)
   382|     def is_ready(self):
   383|         return True
   384|     def get_result(self):
   385|         return self.runner.result_callback(self.key)
   386| class Wait(YieldPoint):
   387|     """Returns the argument passed to the result of a previous `Callback`.
   388|     .. deprecated:: 4.0
   389|        Use `Futures <.Future>` instead.
   390|     """
   391|     def __init__(self, key):
   392|         self.key = key
   393|     def start(self, runner):
   394|         self.runner = runner
   395|     def is_ready(self):
   396|         return self.runner.is_ready(self.key)
   397|     def get_result(self):
   398|         return self.runner.pop_result(self.key)
   399| class WaitAll(YieldPoint):
   400|     """Returns the results of multiple previous `Callbacks <Callback>`.
   401|     The argument is a sequence of `Callback` keys, and the result is
   402|     a list of results in the same order.
   403|     `WaitAll` is equivalent to yielding a list of `Wait` objects.
   404|     .. deprecated:: 4.0
   405|        Use `Futures <.Future>` instead.
   406|     """
   407|     def __init__(self, keys):
   408|         self.keys = keys
   409|     def start(self, runner):
   410|         self.runner = runner
   411|     def is_ready(self):
   412|         return all(self.runner.is_ready(key) for key in self.keys)
   413|     def get_result(self):
   414|         return [self.runner.pop_result(key) for key in self.keys]
   415| def Task(func, *args, **kwargs):
   416|     """Adapts a callback-based asynchronous function for use in coroutines.
   417|     Takes a function (and optional additional arguments) and runs it with
   418|     those arguments plus a ``callback`` keyword argument.  The argument passed
   419|     to the callback is returned as the result of the yield expression.
   420|     .. versionchanged:: 4.0
   421|        ``gen.Task`` is now a function that returns a `.Future`, instead of
   422|        a subclass of `YieldPoint`.  It still behaves the same way when
   423|        yielded.
   424|     """
   425|     future = Future()
   426|     def handle_exception(typ, value, tb):
   427|         if future.done():
   428|             return False
   429|         future.set_exc_info((typ, value, tb))
   430|         return True
   431|     def set_result(result):
   432|         if future.done():
   433|             return
   434|         future.set_result(result)
   435|     with stack_context.ExceptionStackContext(handle_exception):
   436|         func(*args, callback=_argument_adapter(set_result), **kwargs)
   437|     return future
   438| class YieldFuture(YieldPoint):
   439|     def __init__(self, future, io_loop=None):
   440|         """Adapts a `.Future` to the `YieldPoint` interface.
   441|         .. versionchanged:: 4.1
   442|            The ``io_loop`` argument is deprecated.
   443|         """
   444|         self.future = future
   445|         self.io_loop = io_loop or IOLoop.current()
   446|     def start(self, runner):
   447|         if not self.future.done():
   448|             self.runner = runner
   449|             self.key = object()
   450|             runner.register_callback(self.key)
   451|             self.io_loop.add_future(self.future, runner.result_callback(self.key))
   452|         else:
   453|             self.runner = None
   454|             self.result_fn = self.future.result
   455|     def is_ready(self):
   456|         if self.runner is not None:
   457|             return self.runner.is_ready(self.key)
   458|         else:
   459|             return True
   460|     def get_result(self):
   461|         if self.runner is not None:
   462|             return self.runner.pop_result(self.key).result()
   463|         else:
   464|             return self.result_fn()
   465| def _contains_yieldpoint(children):
   466|     """Returns True if ``children`` contains any YieldPoints.
   467|     ``children`` may be a dict or a list, as used by `MultiYieldPoint`
   468|     and `multi_future`.
   469|     """
   470|     if isinstance(children, dict):
   471|         return any(isinstance(i, YieldPoint) for i in children.values())
   472|     if isinstance(children, list):
   473|         return any(isinstance(i, YieldPoint) for i in children)
   474|     return False
   475| def multi(children, quiet_exceptions=()):
   476|     """Runs multiple asynchronous operations in parallel.
   477|     ``children`` may either be a list or a dict whose values are
   478|     yieldable objects. ``multi()`` returns a new yieldable
   479|     object that resolves to a parallel structure containing their
   480|     results. If ``children`` is a list, the result is a list of
   481|     results in the same order; if it is a dict, the result is a dict
   482|     with the same keys.
   483|     That is, ``results = yield multi(list_of_futures)`` is equivalent
   484|     to::
   485|         results = []
   486|         for future in list_of_futures:
   487|             results.append(yield future)
   488|     If any children raise exceptions, ``multi()`` will raise the first
   489|     one. All others will be logged, unless they are of types
   490|     contained in the ``quiet_exceptions`` argument.
   491|     If any of the inputs are `YieldPoints <YieldPoint>`, the returned
   492|     yieldable object is a `YieldPoint`. Otherwise, returns a `.Future`.
   493|     This means that the result of `multi` can be used in a native
   494|     coroutine if and only if all of its children can be.
   495|     In a ``yield``-based coroutine, it is not normally necessary to
   496|     call this function directly, since the coroutine runner will
   497|     do it automatically when a list or dict is yielded. However,
   498|     it is necessary in ``await``-based coroutines, or to pass
   499|     the ``quiet_exceptions`` argument.
   500|     This function is available under the names ``multi()`` and ``Multi()``
   501|     for historical reasons.
   502|     .. versionchanged:: 4.2
   503|        If multiple yieldables fail, any exceptions after the first
   504|        (which is raised) will be logged. Added the ``quiet_exceptions``
   505|        argument to suppress this logging for selected exception types.
   506|     .. versionchanged:: 4.3
   507|        Replaced the class ``Multi`` and the function ``multi_future``
   508|        with a unified function ``multi``. Added support for yieldables
   509|        other than `YieldPoint` and `.Future`.
   510|     """
   511|     if _contains_yieldpoint(children):
   512|         return MultiYieldPoint(children, quiet_exceptions=quiet_exceptions)
   513|     else:
   514|         return multi_future(children, quiet_exceptions=quiet_exceptions)
   515| Multi = multi
   516| class MultiYieldPoint(YieldPoint):
   517|     """Runs multiple asynchronous operations in parallel.
   518|     This class is similar to `multi`, but it always creates a stack
   519|     context even when no children require it. It is not compatible with
   520|     native coroutines.
   521|     .. versionchanged:: 4.2
   522|        If multiple ``YieldPoints`` fail, any exceptions after the first
   523|        (which is raised) will be logged. Added the ``quiet_exceptions``
   524|        argument to suppress this logging for selected exception types.
   525|     .. versionchanged:: 4.3
   526|        Renamed from ``Multi`` to ``MultiYieldPoint``. The name ``Multi``
   527|        remains as an alias for the equivalent `multi` function.
   528|     .. deprecated:: 4.3
   529|        Use `multi` instead.
   530|     """
   531|     def __init__(self, children, quiet_exceptions=()):
   532|         self.keys = None
   533|         if isinstance(children, dict):
   534|             self.keys = list(children.keys())
   535|             children = children.values()
   536|         self.children = []
   537|         for i in children:
   538|             if not isinstance(i, YieldPoint):
   539|                 i = convert_yielded(i)
   540|             if is_future(i):
   541|                 i = YieldFuture(i)
   542|             self.children.append(i)
   543|         assert all(isinstance(i, YieldPoint) for i in self.children)
   544|         self.unfinished_children = set(self.children)
   545|         self.quiet_exceptions = quiet_exceptions
   546|     def start(self, runner):
   547|         for i in self.children:
   548|             i.start(runner)
   549|     def is_ready(self):
   550|         finished = list(itertools.takewhile(
   551|             lambda i: i.is_ready(), self.unfinished_children))
   552|         self.unfinished_children.difference_update(finished)
   553|         return not self.unfinished_children
   554|     def get_result(self):
   555|         result_list = []
   556|         exc_info = None
   557|         for f in self.children:
   558|             try:
   559|                 result_list.append(f.get_result())
   560|             except Exception as e:
   561|                 if exc_info is None:
   562|                     exc_info = sys.exc_info()
   563|                 else:
   564|                     if not isinstance(e, self.quiet_exceptions):
   565|                         app_log.error("Multiple exceptions in yield list",
   566|                                       exc_info=True)
   567|         if exc_info is not None:
   568|             raise_exc_info(exc_info)
   569|         if self.keys is not None:
   570|             return dict(zip(self.keys, result_list))
   571|         else:
   572|             return list(result_list)
   573| def multi_future(children, quiet_exceptions=()):
   574|     """Wait for multiple asynchronous futures in parallel.
   575|     This function is similar to `multi`, but does not support
   576|     `YieldPoints <YieldPoint>`.
   577|     .. versionadded:: 4.0
   578|     .. versionchanged:: 4.2
   579|        If multiple ``Futures`` fail, any exceptions after the first (which is
   580|        raised) will be logged. Added the ``quiet_exceptions``
   581|        argument to suppress this logging for selected exception types.
   582|     .. deprecated:: 4.3
   583|        Use `multi` instead.
   584|     """
   585|     if isinstance(children, dict):
   586|         keys = list(children.keys())
   587|         children = children.values()
   588|     else:
   589|         keys = None
   590|     children = list(map(convert_yielded, children))
   591|     assert all(is_future(i) for i in children)
   592|     unfinished_children = set(children)
   593|     future = Future()
   594|     if not children:
   595|         future.set_result({} if keys is not None else [])
   596|     def callback(f):
   597|         unfinished_children.remove(f)
   598|         if not unfinished_children:
   599|             result_list = []
   600|             for f in children:
   601|                 try:
   602|                     result_list.append(f.result())
   603|                 except Exception as e:
   604|                     if future.done():
   605|                         if not isinstance(e, quiet_exceptions):
   606|                             app_log.error("Multiple exceptions in yield list",
   607|                                           exc_info=True)
   608|                     else:
   609|                         future.set_exc_info(sys.exc_info())
   610|             if not future.done():
   611|                 if keys is not None:
   612|                     future.set_result(dict(zip(keys, result_list)))
   613|                 else:
   614|                     future.set_result(result_list)
   615|     listening = set()
   616|     for f in children:
   617|         if f not in listening:
   618|             listening.add(f)
   619|             f.add_done_callback(callback)
   620|     return future
   621| def maybe_future(x):
   622|     """Converts ``x`` into a `.Future`.
   623|     If ``x`` is already a `.Future`, it is simply returned; otherwise
   624|     it is wrapped in a new `.Future`.  This is suitable for use as
   625|     ``result = yield gen.maybe_future(f())`` when you don't know whether
   626|     ``f()`` returns a `.Future` or not.
   627|     .. deprecated:: 4.3
   628|        This function only handles ``Futures``, not other yieldable objects.
   629|        Instead of `maybe_future`, check for the non-future result types
   630|        you expect (often just ``None``), and ``yield`` anything unknown.
   631|     """
   632|     if is_future(x):
   633|         return x
   634|     else:
   635|         fut = Future()
   636|         fut.set_result(x)
   637|         return fut
   638| def with_timeout(timeout, future, io_loop=None, quiet_exceptions=()):
   639|     """Wraps a `.Future` (or other yieldable object) in a timeout.
   640|     Raises `TimeoutError` if the input future does not complete before
   641|     ``timeout``, which may be specified in any form allowed by
   642|     `.IOLoop.add_timeout` (i.e. a `datetime.timedelta` or an absolute time
   643|     relative to `.IOLoop.time`)
   644|     If the wrapped `.Future` fails after it has timed out, the exception
   645|     will be logged unless it is of a type contained in ``quiet_exceptions``
   646|     (which may be an exception type or a sequence of types).
   647|     Does not support `YieldPoint` subclasses.
   648|     .. versionadded:: 4.0
   649|     .. versionchanged:: 4.1
   650|        Added the ``quiet_exceptions`` argument and the logging of unhandled
   651|        exceptions.
   652|     .. versionchanged:: 4.4
   653|        Added support for yieldable objects other than `.Future`.
   654|     """
   655|     future = convert_yielded(future)
   656|     result = Future()
   657|     chain_future(future, result)
   658|     if io_loop is None:
   659|         io_loop = IOLoop.current()
   660|     def error_callback(future):
   661|         try:
   662|             future.result()
   663|         except Exception as e:
   664|             if not isinstance(e, quiet_exceptions):
   665|                 app_log.error("Exception in Future %r after timeout",
   666|                               future, exc_info=True)
   667|     def timeout_callback():
   668|         result.set_exception(TimeoutError("Timeout"))
   669|         future.add_done_callback(error_callback)
   670|     timeout_handle = io_loop.add_timeout(
   671|         timeout, timeout_callback)
   672|     if isinstance(future, Future):
   673|         future.add_done_callback(
   674|             lambda future: io_loop.remove_timeout(timeout_handle))
   675|     else:
   676|         io_loop.add_future(
   677|             future, lambda future: io_loop.remove_timeout(timeout_handle))
   678|     return result
   679| def sleep(duration):
   680|     """Return a `.Future` that resolves after the given number of seconds.
   681|     When used with ``yield`` in a coroutine, this is a non-blocking
   682|     analogue to `time.sleep` (which should not be used in coroutines
   683|     because it is blocking)::
   684|         yield gen.sleep(0.5)
   685|     Note that calling this function on its own does nothing; you must
   686|     wait on the `.Future` it returns (usually by yielding it).
   687|     .. versionadded:: 4.1
   688|     """
   689|     f = Future()
   690|     IOLoop.current().call_later(duration, lambda: f.set_result(None))
   691|     return f
   692| _null_future = Future()
   693| _null_future.set_result(None)
   694| moment = Future()
   695| moment.__doc__ = \
   696|     """A special object which may be yielded to allow the IOLoop to run for
   697| one iteration.
   698| This is not needed in normal use but it can be helpful in long-running
   699| coroutines that are likely to yield Futures that are ready instantly.
   700| Usage: ``yield gen.moment``
   701| .. versionadded:: 4.0
   702| .. deprecated:: 4.5
   703|    ``yield None`` is now equivalent to ``yield gen.moment``.
   704| """
   705| moment.set_result(None)
   706| class Runner(object):
   707|     """Internal implementation of `tornado.gen.engine`.
   708|     Maintains information about pending callbacks and their results.
   709|     The results of the generator are stored in ``result_future`` (a
   710|     `.TracebackFuture`)
   711|     """
   712|     def __init__(self, gen, result_future, first_yielded):
   713|         self.gen = gen
   714|         self.result_future = result_future
   715|         self.future = _null_future
   716|         self.yield_point = None
   717|         self.pending_callbacks = None
   718|         self.results = None
   719|         self.running = False
   720|         self.finished = False
   721|         self.had_exception = False
   722|         self.io_loop = IOLoop.current()
   723|         self.stack_context_deactivate = None
   724|         if self.handle_yield(first_yielded):
   725|             gen = result_future = first_yielded = None
   726|             self.run()
   727|     def register_callback(self, key):
   728|         """Adds ``key`` to the list of callbacks."""
   729|         if self.pending_callbacks is None:
   730|             self.pending_callbacks = set()
   731|             self.results = {}
   732|         if key in self.pending_callbacks:
   733|             raise KeyReuseError("key %r is already pending" % (key,))
   734|         self.pending_callbacks.add(key)
   735|     def is_ready(self, key):
   736|         """Returns true if a result is available for ``key``."""
   737|         if self.pending_callbacks is None or key not in self.pending_callbacks:
   738|             raise UnknownKeyError("key %r is not pending" % (key,))
   739|         return key in self.results
   740|     def set_result(self, key, result):
   741|         """Sets the result for ``key`` and attempts to resume the generator."""
   742|         self.results[key] = result
   743|         if self.yield_point is not None and self.yield_point.is_ready():
   744|             try:
   745|                 self.future.set_result(self.yield_point.get_result())
   746|             except:
   747|                 self.future.set_exc_info(sys.exc_info())
   748|             self.yield_point = None
   749|             self.run()
   750|     def pop_result(self, key):
   751|         """Returns the result for ``key`` and unregisters it."""
   752|         self.pending_callbacks.remove(key)
   753|         return self.results.pop(key)
   754|     def run(self):
   755|         """Starts or resumes the generator, running until it reaches a
   756|         yield point that is not ready.
   757|         """
   758|         if self.running or self.finished:
   759|             return
   760|         try:
   761|             self.running = True
   762|             while True:
   763|                 future = self.future
   764|                 if not future.done():
   765|                     return
   766|                 self.future = None
   767|                 try:
   768|                     orig_stack_contexts = stack_context._state.contexts
   769|                     exc_info = None
   770|                     try:
   771|                         value = future.result()
   772|                     except Exception:
   773|                         self.had_exception = True
   774|                         exc_info = sys.exc_info()
   775|                     future = None
   776|                     if exc_info is not None:
   777|                         try:
   778|                             yielded = self.gen.throw(*exc_info)
   779|                         finally:
   780|                             exc_info = None
   781|                     else:
   782|                         yielded = self.gen.send(value)
   783|                     if stack_context._state.contexts is not orig_stack_contexts:
   784|                         self.gen.throw(
   785|                             stack_context.StackContextInconsistentError(
   786|                                 'stack_context inconsistency (probably caused '
   787|                                 'by yield within a "with StackContext" block)'))
   788|                 except (StopIteration, Return) as e:
   789|                     self.finished = True
   790|                     self.future = _null_future
   791|                     if self.pending_callbacks and not self.had_exception:
   792|                         raise LeakedCallbackError(
   793|                             "finished without waiting for callbacks %r" %
   794|                             self.pending_callbacks)
   795|                     self.result_future.set_result(_value_from_stopiteration(e))
   796|                     self.result_future = None
   797|                     self._deactivate_stack_context()
   798|                     return
   799|                 except Exception:
   800|                     self.finished = True
   801|                     self.future = _null_future
   802|                     self.result_future.set_exc_info(sys.exc_info())
   803|                     self.result_future = None
   804|                     self._deactivate_stack_context()
   805|                     return
   806|                 if not self.handle_yield(yielded):
   807|                     return
   808|                 yielded = None
   809|         finally:
   810|             self.running = False
   811|     def handle_yield(self, yielded):
   812|         if _contains_yieldpoint(yielded):
   813|             yielded = multi(yielded)
   814|         if isinstance(yielded, YieldPoint):
   815|             self.future = TracebackFuture()
   816|             def start_yield_point():
   817|                 try:
   818|                     yielded.start(self)
   819|                     if yielded.is_ready():
   820|                         self.future.set_result(
   821|                             yielded.get_result())
   822|                     else:
   823|                         self.yield_point = yielded
   824|                 except Exception:
   825|                     self.future = TracebackFuture()
   826|                     self.future.set_exc_info(sys.exc_info())
   827|             if self.stack_context_deactivate is None:
   828|                 with stack_context.ExceptionStackContext(
   829|                         self.handle_exception) as deactivate:
   830|                     self.stack_context_deactivate = deactivate
   831|                     def cb():
   832|                         start_yield_point()
   833|                         self.run()
   834|                     self.io_loop.add_callback(cb)
   835|                     return False
   836|             else:
   837|                 start_yield_point()
   838|         else:
   839|             try:
   840|                 self.future = convert_yielded(yielded)
   841|             except BadYieldError:
   842|                 self.future = TracebackFuture()
   843|                 self.future.set_exc_info(sys.exc_info())
   844|         if not self.future.done() or self.future is moment:
   845|             def inner(f):
   846|                 f = None # noqa
   847|                 self.run()
   848|             self.io_loop.add_future(
   849|                 self.future, inner)
   850|             return False
   851|         return True
   852|     def result_callback(self, key):
   853|         return stack_context.wrap(_argument_adapter(
   854|             functools.partial(self.set_result, key)))
   855|     def handle_exception(self, typ, value, tb):
   856|         if not self.running and not self.finished:
   857|             self.future = TracebackFuture()
   858|             self.future.set_exc_info((typ, value, tb))
   859|             self.run()
   860|             return True
   861|         else:
   862|             return False
   863|     def _deactivate_stack_context(self):
   864|         if self.stack_context_deactivate is not None:
   865|             self.stack_context_deactivate()
   866|             self.stack_context_deactivate = None
   867| Arguments = collections.namedtuple('Arguments', ['args', 'kwargs'])
   868| def _argument_adapter(callback):
   869|     """Returns a function that when invoked runs ``callback`` with one arg.
   870|     If the function returned by this function is called with exactly
   871|     one argument, that argument is passed to ``callback``.  Otherwise
   872|     the args tuple and kwargs dict are wrapped in an `Arguments` object.
   873|     """
   874|     def wrapper(*args, **kwargs):
   875|         if kwargs or len(args) > 1:
   876|             callback(Arguments(args, kwargs))
   877|         elif args:
   878|             callback(args[0])
   879|         else:
   880|             callback(None)
   881|     return wrapper
   882| if sys.version_info >= (3, 3):
   883|     exec(textwrap.dedent("""
   884|     @coroutine
   885|     def _wrap_awaitable(x):
   886|         if hasattr(x, '__await__'):
   887|             x = x.__await__()
   888|         return (yield from x)
   889|     """))
   890| else:
   891|     @coroutine
   892|     def _wrap_awaitable(x):
   893|         if hasattr(x, '__await__'):
   894|             _i = x.__await__()
   895|         else:
   896|             _i = iter(x)
   897|         try:
   898|             _y = next(_i)
   899|         except StopIteration as _e:
   900|             _r = _value_from_stopiteration(_e)
   901|         else:
   902|             while 1:
   903|                 try:
   904|                     _s = yield _y
   905|                 except GeneratorExit as _e:
   906|                     try:
   907|                         _m = _i.close
   908|                     except AttributeError:
   909|                         pass
   910|                     else:
   911|                         _m()
   912|                     raise _e
   913|                 except BaseException as _e:
   914|                     _x = sys.exc_info()
   915|                     try:
   916|                         _m = _i.throw
   917|                     except AttributeError:
   918|                         raise _e
   919|                     else:
   920|                         try:
   921|                             _y = _m(*_x)
   922|                         except StopIteration as _e:
   923|                             _r = _value_from_stopiteration(_e)
   924|                             break
   925|                 else:
   926|                     try:
   927|                         if _s is None:
   928|                             _y = next(_i)
   929|                         else:
   930|                             _y = _i.send(_s)
   931|                     except StopIteration as _e:
   932|                         _r = _value_from_stopiteration(_e)
   933|                         break
   934|         raise Return(_r)
   935| def convert_yielded(yielded):
   936|     """Convert a yielded object into a `.Future`.
   937|     The default implementation accepts lists, dictionaries, and Futures.
   938|     If the `~functools.singledispatch` library is available, this function
   939|     may be extended to support additional types. For example::
   940|         @convert_yielded.register(asyncio.Future)
   941|         def _(asyncio_future):
   942|             return tornado.platform.asyncio.to_tornado_future(asyncio_future)
   943|     .. versionadded:: 4.1
   944|     """
   945|     if yielded is None:
   946|         return moment
   947|     elif isinstance(yielded, (list, dict)):
   948|         return multi(yielded)
   949|     elif is_future(yielded):
   950|         return yielded
   951|     elif isawaitable(yielded):
   952|         return _wrap_awaitable(yielded)
   953|     else:
   954|         raise BadYieldError("yielded unknown object %r" % (yielded,))
   955| if singledispatch is not None:
   956|     convert_yielded = singledispatch(convert_yielded)
   957|     try:
   958|         import salt.ext.tornado.platform.asyncio
   959|     except ImportError:
   960|         pass
   961|     else:
   962|         tornado


# ====================================================================
# FILE: salt/ext/tornado/http1connection.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-572 ---
     1| """Client and server implementations of HTTP/1.x.
     2| .. versionadded:: 4.0
     3| """
     4| from __future__ import absolute_import, division, print_function
     5| import re
     6| from salt.ext.tornado.concurrent import Future
     7| from salt.ext.tornado.escape import native_str, utf8
     8| from salt.ext.tornado import gen
     9| from salt.ext.tornado import httputil
    10| from salt.ext.tornado import iostream
    11| from salt.ext.tornado.log import gen_log, app_log
    12| from salt.ext.tornado import stack_context
    13| from salt.ext.tornado.util import GzipDecompressor, PY3
    14| class _QuietException(Exception):
    15|     def __init__(self):
    16|         pass
    17| class _ExceptionLoggingContext(object):
    18|     """Used with the ``with`` statement when calling delegate methods to
    19|     log any exceptions with the given logger.  Any exceptions caught are
    20|     converted to _QuietException
    21|     """
    22|     def __init__(self, logger):
    23|         self.logger = logger
    24|     def __enter__(self):
    25|         pass
    26|     def __exit__(self, typ, value, tb):
    27|         if value is not None:
    28|             self.logger.error("Uncaught exception", exc_info=(typ, value, tb))
    29|             raise _QuietException
    30| class HTTP1ConnectionParameters(object):
    31|     """Parameters for `.HTTP1Connection` and `.HTTP1ServerConnection`.
    32|     """
    33|     def __init__(self, no_keep_alive=False, chunk_size=None,
    34|                  max_header_size=None, header_timeout=None, max_body_size=None,
    35|                  body_timeout=None, decompress=False):
    36|         """
    37|         :arg bool no_keep_alive: If true, always close the connection after
    38|             one request.
    39|         :arg int chunk_size: how much data to read into memory at once
    40|         :arg int max_header_size:  maximum amount of data for HTTP headers
    41|         :arg float header_timeout: how long to wait for all headers (seconds)
    42|         :arg int max_body_size: maximum amount of data for body
    43|         :arg float body_timeout: how long to wait while reading body (seconds)
    44|         :arg bool decompress: if true, decode incoming
    45|             ``Content-Encoding: gzip``
    46|         """
    47|         self.no_keep_alive = no_keep_alive
    48|         self.chunk_size = chunk_size or 65536
    49|         self.max_header_size = max_header_size or 65536
    50|         self.header_timeout = header_timeout
    51|         self.max_body_size = max_body_size
    52|         self.body_timeout = body_timeout
    53|         self.decompress = decompress
    54| class HTTP1Connection(httputil.HTTPConnection):
    55|     """Implements the HTTP/1.x protocol.
    56|     This class can be on its own for clients, or via `HTTP1ServerConnection`
    57|     for servers.
    58|     """
    59|     def __init__(self, stream, is_client, params=None, context=None):
    60|         """
    61|         :arg stream: an `.IOStream`
    62|         :arg bool is_client: client or server
    63|         :arg params: a `.HTTP1ConnectionParameters` instance or ``None``
    64|         :arg context: an opaque application-defined object that can be accessed
    65|             as ``connection.context``.
    66|         """
    67|         self.is_client = is_client
    68|         self.stream = stream
    69|         if params is None:
    70|             params = HTTP1ConnectionParameters()
    71|         self.params = params
    72|         self.context = context
    73|         self.no_keep_alive = params.no_keep_alive
    74|         self._max_body_size = (self.params.max_body_size or
    75|                                self.stream.max_buffer_size)
    76|         self._body_timeout = self.params.body_timeout
    77|         self._write_finished = False
    78|         self._read_finished = False
    79|         self._finish_future = Future()
    80|         self._disconnect_on_finish = False
    81|         self._clear_callbacks()
    82|         self._request_start_line = None
    83|         self._response_start_line = None
    84|         self._request_headers = None
    85|         self._chunking_output = None
    86|         self._expected_content_remaining = None
    87|         self._pending_write = None
    88|     def read_response(self, delegate):
    89|         """Read a single HTTP response.
    90|         Typical client-mode usage is to write a request using `write_headers`,
    91|         `write`, and `finish`, and then call ``read_response``.
    92|         :arg delegate: a `.HTTPMessageDelegate`
    93|         Returns a `.Future` that resolves to None after the full response has
    94|         been read.
    95|         """
    96|         if self.params.decompress:
    97|             delegate = _GzipMessageDelegate(delegate, self.params.chunk_size)
    98|         return self._read_message(delegate)
    99|     @gen.coroutine
   100|     def _read_message(self, delegate):
   101|         need_delegate_close = False
   102|         try:
   103|             header_future = self.stream.read_until_regex(
   104|                 b"\r?\n\r?\n",
   105|                 max_bytes=self.params.max_header_size)
   106|             if self.params.header_timeout is None:
   107|                 header_data = yield header_future
   108|             else:
   109|                 try:
   110|                     header_data = yield gen.with_timeout(
   111|                         self.stream.io_loop.time() + self.params.header_timeout,
   112|                         header_future,
   113|                         io_loop=self.stream.io_loop,
   114|                         quiet_exceptions=iostream.StreamClosedError)
   115|                 except gen.TimeoutError:
   116|                     self.close()
   117|                     raise gen.Return(False)
   118|             start_line, headers = self._parse_headers(header_data)
   119|             if self.is_client:
   120|                 start_line = httputil.parse_response_start_line(start_line)
   121|                 self._response_start_line = start_line
   122|             else:
   123|                 start_line = httputil.parse_request_start_line(start_line)
   124|                 self._request_start_line = start_line
   125|                 self._request_headers = headers
   126|             self._disconnect_on_finish = not self._can_keep_alive(
   127|                 start_line, headers)
   128|             need_delegate_close = True
   129|             with _ExceptionLoggingContext(app_log):
   130|                 header_future = delegate.headers_received(start_line, headers)
   131|                 if header_future is not None:
   132|                     yield header_future
   133|             if self.stream is None:
   134|                 need_delegate_close = False
   135|                 raise gen.Return(False)
   136|             skip_body = False
   137|             if self.is_client:
   138|                 if (self._request_start_line is not None and
   139|                         self._request_start_line.method == 'HEAD'):
   140|                     skip_body = True
   141|                 code = start_line.code
   142|                 if code == 304:
   143|                     skip_body = True
   144|                 if code >= 100 and code < 200:
   145|                     if ('Content-Length' in headers or
   146|                             'Transfer-Encoding' in headers):
   147|                         raise httputil.HTTPInputError(
   148|                             "Response code %d cannot have body" % code)
   149|                     yield self._read_message(delegate)
   150|             else:
   151|                 if (headers.get("Expect") == "100-continue" and
   152|                         not self._write_finished):
   153|                     self.stream.write(b"HTTP/1.1 100 (Continue)\r\n\r\n")
   154|             if not skip_body:
   155|                 body_future = self._read_body(
   156|                     start_line.code if self.is_client else 0, headers, delegate)
   157|                 if body_future is not None:
   158|                     if self._body_timeout is None:
   159|                         yield body_future
   160|                     else:
   161|                         try:
   162|                             yield gen.with_timeout(
   163|                                 self.stream.io_loop.time() + self._body_timeout,
   164|                                 body_future, self.stream.io_loop,
   165|                                 quiet_exceptions=iostream.StreamClosedError)
   166|                         except gen.TimeoutError:
   167|                             gen_log.info("Timeout reading body from %s",
   168|                                          self.context)
   169|                             self.stream.close()
   170|                             raise gen.Return(False)
   171|             self._read_finished = True
   172|             if not self._write_finished or self.is_client:
   173|                 need_delegate_close = False
   174|                 with _ExceptionLoggingContext(app_log):
   175|                     delegate.finish()
   176|             if (not self._finish_future.done() and
   177|                     self.stream is not None and
   178|                     not self.stream.closed()):
   179|                 self.stream.set_close_callback(self._on_connection_close)
   180|                 yield self._finish_future
   181|             if self.is_client and self._disconnect_on_finish:
   182|                 self.close()
   183|             if self.stream is None:
   184|                 raise gen.Return(False)
   185|         except httputil.HTTPInputError as e:
   186|             gen_log.info("Malformed HTTP message from %s: %s",
   187|                          self.context, e)
   188|             self.close()
   189|             raise gen.Return(False)
   190|         finally:
   191|             if need_delegate_close:
   192|                 with _ExceptionLoggingContext(app_log):
   193|                     delegate.on_connection_close()
   194|             header_future = None
   195|             self._clear_callbacks()
   196|         raise gen.Return(True)
   197|     def _clear_callbacks(self):
   198|         """Clears the callback attributes.
   199|         This allows the request handler to be garbage collected more
   200|         quickly in CPython by breaking up reference cycles.
   201|         """
   202|         self._write_callback = None
   203|         self._write_future = None
   204|         self._close_callback = None
   205|         if self.stream is not None:
   206|             self.stream.set_close_callback(None)
   207|     def set_close_callback(self, callback):
   208|         """Sets a callback that will be run when the connection is closed.
   209|         .. deprecated:: 4.0
   210|             Use `.HTTPMessageDelegate.on_connection_close` instead.
   211|         """
   212|         self._close_callback = stack_context.wrap(callback)
   213|     def _on_connection_close(self):
   214|         if self._close_callback is not None:
   215|             callback = self._close_callback
   216|             self._close_callback = None
   217|             callback()
   218|         if not self._finish_future.done():
   219|             self._finish_future.set_result(None)
   220|         self._clear_callbacks()
   221|     def close(self):
   222|         if self.stream is not None:
   223|             self.stream.close()
   224|         self._clear_callbacks()
   225|         if not self._finish_future.done():
   226|             self._finish_future.set_result(None)
   227|     def detach(self):
   228|         """Take control of the underlying stream.
   229|         Returns the underlying `.IOStream` object and stops all further
   230|         HTTP processing.  May only be called during
   231|         `.HTTPMessageDelegate.headers_received`.  Intended for implementing
   232|         protocols like websockets that tunnel over an HTTP handshake.
   233|         """
   234|         self._clear_callbacks()
   235|         stream = self.stream
   236|         self.stream = None
   237|         if not self._finish_future.done():
   238|             self._finish_future.set_result(None)
   239|         return stream
   240|     def set_body_timeout(self, timeout):
   241|         """Sets the body timeout for a single request.
   242|         Overrides the value from `.HTTP1ConnectionParameters`.
   243|         """
   244|         self._body_timeout = timeout
   245|     def set_max_body_size(self, max_body_size):
   246|         """Sets the body size limit for a single request.
   247|         Overrides the value from `.HTTP1ConnectionParameters`.
   248|         """
   249|         self._max_body_size = max_body_size
   250|     def write_headers(self, start_line, headers, chunk=None, callback=None):
   251|         """Implements `.HTTPConnection.write_headers`."""
   252|         lines = []
   253|         if self.is_client:
   254|             self._request_start_line = start_line
   255|             lines.append(utf8('%s %s HTTP/1.1' % (start_line[0], start_line[1])))
   256|             self._chunking_output = (
   257|                 start_line.method in ('POST', 'PUT', 'PATCH') and
   258|                 'Content-Length' not in headers and
   259|                 'Transfer-Encoding' not in headers)
   260|         else:
   261|             self._response_start_line = start_line
   262|             lines.append(utf8('HTTP/1.1 %d %s' % (start_line[1], start_line[2])))
   263|             self._chunking_output = (
   264|                 self._request_start_line.version == 'HTTP/1.1' and
   265|                 start_line.code not in (204, 304) and
   266|                 (start_line.code < 100 or start_line.code >= 200) and
   267|                 'Content-Length' not in headers and
   268|                 'Transfer-Encoding' not in headers)
   269|             if (self._request_start_line.version == 'HTTP/1.0' and
   270|                 (self._request_headers.get('Connection', '').lower() ==
   271|                  'keep-alive')):
   272|                 headers['Connection'] = 'Keep-Alive'
   273|         if self._chunking_output:
   274|             headers['Transfer-Encoding'] = 'chunked'
   275|         if (not self.is_client and
   276|             (self._request_start_line.method == 'HEAD' or
   277|              start_line.code == 304)):
   278|             self._expected_content_remaining = 0
   279|         elif 'Content-Length' in headers:
   280|             self._expected_content_remaining = int(headers['Content-Length'])
   281|         else:
   282|             self._expected_content_remaining = None
   283|         header_lines = (native_str(n) + ": " + native_str(v) for n, v in headers.get_all())
   284|         if PY3:
   285|             lines.extend(l.encode('latin1') for l in header_lines)
   286|         else:
   287|             lines.extend(header_lines)
   288|         for line in lines:
   289|             if b'\n' in line:
   290|                 raise ValueError('Newline in header: ' + repr(line))
   291|         future = None
   292|         if self.stream.closed():
   293|             future = self._write_future = Future()
   294|             future.set_exception(iostream.StreamClosedError())
   295|             future.exception()
   296|         else:
   297|             if callback is not None:
   298|                 self._write_callback = stack_context.wrap(callback)
   299|             else:
   300|                 future = self._write_future = Future()
   301|             data = b"\r\n".join(lines) + b"\r\n\r\n"
   302|             if chunk:
   303|                 data += self._format_chunk(chunk)
   304|             self._pending_write = self.stream.write(data)
   305|             self._pending_write.add_done_callback(self._on_write_complete)
   306|         return future
   307|     def _format_chunk(self, chunk):
   308|         if self._expected_content_remaining is not None:
   309|             self._expected_content_remaining -= len(chunk)
   310|             if self._expected_content_remaining < 0:
   311|                 self.stream.close()
   312|                 raise httputil.HTTPOutputError(
   313|                     "Tried to write more data than Content-Length")
   314|         if self._chunking_output and chunk:
   315|             return utf8("%x" % len(chunk)) + b"\r\n" + chunk + b"\r\n"
   316|         else:
   317|             return chunk
   318|     def write(self, chunk, callback=None):
   319|         """Implements `.HTTPConnection.write`.
   320|         For backwards compatibility is is allowed but deprecated to
   321|         skip `write_headers` and instead call `write()` with a
   322|         pre-encoded header block.
   323|         """
   324|         future = None
   325|         if self.stream.closed():
   326|             future = self._write_future = Future()
   327|             self._write_future.set_exception(iostream.StreamClosedError())
   328|             self._write_future.exception()
   329|         else:
   330|             if callback is not None:
   331|                 self._write_callback = stack_context.wrap(callback)
   332|             else:
   333|                 future = self._write_future = Future()
   334|             self._pending_write = self.stream.write(self._format_chunk(chunk))
   335|             self._pending_write.add_done_callback(self._on_write_complete)
   336|         return future
   337|     def finish(self):
   338|         """Implements `.HTTPConnection.finish`."""
   339|         if (self._expected_content_remaining is not None and
   340|                 self._expected_content_remaining != 0 and
   341|                 not self.stream.closed()):
   342|             self.stream.close()
   343|             raise httputil.HTTPOutputError(
   344|                 "Tried to write %d bytes less than Content-Length" %
   345|                 self._expected_content_remaining)
   346|         if self._chunking_output:
   347|             if not self.stream.closed():
   348|                 self._pending_write = self.stream.write(b"0\r\n\r\n")
   349|                 self._pending_write.add_done_callback(self._on_write_complete)
   350|         self._write_finished = True
   351|         if not self._read_finished:
   352|             self._disconnect_on_finish = True
   353|         self.stream.set_nodelay(True)
   354|         if self._pending_write is None:
   355|             self._finish_request(None)
   356|         else:
   357|             self._pending_write.add_done_callback(self._finish_request)
   358|     def _on_write_complete(self, future):
   359|         exc = future.exception()
   360|         if exc is not None and not isinstance(exc, iostream.StreamClosedError):
   361|             future.result()
   362|         if self._write_callback is not None:
   363|             callback = self._write_callback
   364|             self._write_callback = None
   365|             self.stream.io_loop.add_callback(callback)
   366|         if self._write_future is not None:
   367|             future = self._write_future
   368|             self._write_future = None
   369|             future.set_result(None)
   370|     def _can_keep_alive(self, start_line, headers):
   371|         if self.params.no_keep_alive:
   372|             return False
   373|         connection_header = headers.get("Connection")
   374|         if connection_header is not None:
   375|             connection_header = connection_header.lower()
   376|         if start_line.version == "HTTP/1.1":
   377|             return connection_header != "close"
   378|         elif ("Content-Length" in headers or
   379|               headers.get("Transfer-Encoding", "").lower() == "chunked" or
   380|               getattr(start_line, 'method', None) in ("HEAD", "GET")):
   381|             return connection_header == "keep-alive"
   382|         return False
   383|     def _finish_request(self, future):
   384|         self._clear_callbacks()
   385|         if not self.is_client and self._disconnect_on_finish:
   386|             self.close()
   387|             return
   388|         self.stream.set_nodelay(False)
   389|         if not self._finish_future.done():
   390|             self._finish_future.set_result(None)
   391|     def _parse_headers(self, data):
   392|         data = native_str(data.decode('latin1')).lstrip("\r\n")
   393|         eol = data.find("\n")
   394|         start_line = data[:eol].rstrip("\r")
   395|         try:
   396|             headers = httputil.HTTPHeaders.parse(data[eol:])
   397|         except ValueError:
   398|             raise httputil.HTTPInputError("Malformed HTTP headers: %r" %
   399|                                           data[eol:100])
   400|         return start_line, headers
   401|     def _read_body(self, code, headers, delegate):
   402|         if "Content-Length" in headers:
   403|             if "Transfer-Encoding" in headers:
   404|                 raise httputil.HTTPInputError(
   405|                     "Response with both Transfer-Encoding and Content-Length")
   406|             if "," in headers["Content-Length"]:
   407|                 pieces = re.split(r',\s*', headers["Content-Length"])
   408|                 if any(i != pieces[0] for i in pieces):
   409|                     raise httputil.HTTPInputError(
   410|                         "Multiple unequal Content-Lengths: %r" %
   411|                         headers["Content-Length"])
   412|                 headers["Content-Length"] = pieces[0]
   413|             try:
   414|                 content_length = int(headers["Content-Length"])
   415|             except ValueError:
   416|                 raise httputil.HTTPInputError(
   417|                     "Only integer Content-Length is allowed: %s" % headers["Content-Length"])
   418|             if content_length > self._max_body_size:
   419|                 raise httputil.HTTPInputError("Content-Length too long")
   420|         else:
   421|             content_length = None
   422|         if code == 204:
   423|             if ("Transfer-Encoding" in headers or
   424|                     content_length not in (None, 0)):
   425|                 raise httputil.HTTPInputError(
   426|                     "Response with code %d should not have body" % code)
   427|             content_length = 0
   428|         if content_length is not None:
   429|             return self._read_fixed_body(content_length, delegate)
   430|         if headers.get("Transfer-Encoding", "").lower() == "chunked":
   431|             return self._read_chunked_body(delegate)
   432|         if self.is_client:
   433|             return self._read_body_until_close(delegate)
   434|         return None
   435|     @gen.coroutine
   436|     def _read_fixed_body(self, content_length, delegate):
   437|         while content_length > 0:
   438|             body = yield self.stream.read_bytes(
   439|                 min(self.params.chunk_size, content_length), partial=True)
   440|             content_length -= len(body)
   441|             if not self._write_finished or self.is_client:
   442|                 with _ExceptionLoggingContext(app_log):
   443|                     ret = delegate.data_received(body)
   444|                     if ret is not None:
   445|                         yield ret
   446|     @gen.coroutine
   447|     def _read_chunked_body(self, delegate):
   448|         total_size = 0
   449|         while True:
   450|             chunk_len = yield self.stream.read_until(b"\r\n", max_bytes=64)
   451|             chunk_len = int(chunk_len.strip(), 16)
   452|             if chunk_len == 0:
   453|                 crlf = yield self.stream.read_bytes(2)
   454|                 if crlf != b'\r\n':
   455|                     raise httputil.HTTPInputError("improperly terminated chunked request")
   456|                 return
   457|             total_size += chunk_len
   458|             if total_size > self._max_body_size:
   459|                 raise httputil.HTTPInputError("chunked body too large")
   460|             bytes_to_read = chunk_len
   461|             while bytes_to_read:
   462|                 chunk = yield self.stream.read_bytes(
   463|                     min(bytes_to_read, self.params.chunk_size), partial=True)
   464|                 bytes_to_read -= len(chunk)
   465|                 if not self._write_finished or self.is_client:
   466|                     with _ExceptionLoggingContext(app_log):
   467|                         ret = delegate.data_received(chunk)
   468|                         if ret is not None:
   469|                             yield ret
   470|             crlf = yield self.stream.read_bytes(2)
   471|             assert crlf == b"\r\n"
   472|     @gen.coroutine
   473|     def _read_body_until_close(self, delegate):
   474|         body = yield self.stream.read_until_close()
   475|         if not self._write_finished or self.is_client:
   476|             with _ExceptionLoggingContext(app_log):
   477|                 delegate.data_received(body)
   478| class _GzipMessageDelegate(httputil.HTTPMessageDelegate):
   479|     """Wraps an `HTTPMessageDelegate` to decode ``Content-Encoding: gzip``.
   480|     """
   481|     def __init__(self, delegate, chunk_size):
   482|         self._delegate = delegate
   483|         self._chunk_size = chunk_size
   484|         self._decompressor = None
   485|     def headers_received(self, start_line, headers):
   486|         if headers.get("Content-Encoding") == "gzip":
   487|             self._decompressor = GzipDecompressor()
   488|             headers.add("X-Consumed-Content-Encoding",
   489|                         headers["Content-Encoding"])
   490|             del headers["Content-Encoding"]
   491|         return self._delegate.headers_received(start_line, headers)
   492|     @gen.coroutine
   493|     def data_received(self, chunk):
   494|         if self._decompressor:
   495|             compressed_data = chunk
   496|             while compressed_data:
   497|                 decompressed = self._decompressor.decompress(
   498|                     compressed_data, self._chunk_size)
   499|                 if decompressed:
   500|                     ret = self._delegate.data_received(decompressed)
   501|                     if ret is not None:
   502|                         yield ret
   503|                 compressed_data = self._decompressor.unconsumed_tail
   504|         else:
   505|             ret = self._delegate.data_received(chunk)
   506|             if ret is not None:
   507|                 yield ret
   508|     def finish(self):
   509|         if self._decompressor is not None:
   510|             tail = self._decompressor.flush()
   511|             if tail:
   512|                 self._delegate.data_received(tail)
   513|         return self._delegate.finish()
   514|     def on_connection_close(self):
   515|         return self._delegate.on_connection_close()
   516| class HTTP1ServerConnection(object):
   517|     """An HTTP/1.x server."""
   518|     def __init__(self, stream, params=None, context=None):
   519|         """
   520|         :arg stream: an `.IOStream`
   521|         :arg params: a `.HTTP1ConnectionParameters` or None
   522|         :arg context: an opaque application-defined object that is accessible
   523|             as ``connection.context``
   524|         """
   525|         self.stream = stream
   526|         if params is None:
   527|             params = HTTP1ConnectionParameters()
   528|         self.params = params
   529|         self.context = context
   530|         self._serving_future = None
   531|     @gen.coroutine
   532|     def close(self):
   533|         """Closes the connection.
   534|         Returns a `.Future` that resolves after the serving loop has exited.
   535|         """
   536|         self.stream.close()
   537|         try:
   538|             yield self._serving_future
   539|         except Exception:
   540|             pass
   541|     def start_serving(self, delegate):
   542|         """Starts serving requests on this connection.
   543|         :arg delegate: a `.HTTPServerConnectionDelegate`
   544|         """
   545|         assert isinstance(delegate, httputil.HTTPServerConnectionDelegate)
   546|         self._serving_future = self._server_request_loop(delegate)
   547|         self.stream.io_loop.add_future(self._serving_future,
   548|                                        lambda f: f.result())
   549|     @gen.coroutine
   550|     def _server_request_loop(self, delegate):
   551|         try:
   552|             while True:
   553|                 conn = HTTP1Connection(self.stream, False,
   554|                                        self.params, self.context)
   555|                 request_delegate = delegate.start_request(self, conn)
   556|                 try:
   557|                     ret = yield conn.read_response(request_delegate)
   558|                 except (iostream.StreamClosedError,
   559|                         iostream.UnsatisfiableReadError):
   560|                     return
   561|                 except _QuietException:
   562|                     conn.close()
   563|                     return
   564|                 except Exception:
   565|                     gen_log.error("Uncaught exception", exc_info=True)
   566|                     conn.close()
   567|                     return
   568|                 if not ret:
   569|                     return
   570|                 yield gen.moment
   571|         finally:
   572|             delegate.on_close(self)


# ====================================================================
# FILE: salt/ext/tornado/httpclient.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-552 ---
     1| """Blocking and non-blocking HTTP client interfaces.
     2| This module defines a common interface shared by two implementations,
     3| ``simple_httpclient`` and ``curl_httpclient``.  Applications may either
     4| instantiate their chosen implementation class directly or use the
     5| `AsyncHTTPClient` class from this module, which selects an implementation
     6| that can be overridden with the `AsyncHTTPClient.configure` method.
     7| The default implementation is ``simple_httpclient``, and this is expected
     8| to be suitable for most users' needs.  However, some applications may wish
     9| to switch to ``curl_httpclient`` for reasons such as the following:
    10| * ``curl_httpclient`` has some features not found in ``simple_httpclient``,
    11|   including support for HTTP proxies and the ability to use a specified
    12|   network interface.
    13| * ``curl_httpclient`` is more likely to be compatible with sites that are
    14|   not-quite-compliant with the HTTP spec, or sites that use little-exercised
    15|   features of HTTP.
    16| * ``curl_httpclient`` is faster.
    17| * ``curl_httpclient`` was the default prior to Tornado 2.0.
    18| Note that if you are using ``curl_httpclient``, it is highly
    19| recommended that you use a recent version of ``libcurl`` and
    20| ``pycurl``.  Currently the minimum supported version of libcurl is
    21| 7.22.0, and the minimum version of pycurl is 7.18.2.  It is highly
    22| recommended that your ``libcurl`` installation is built with
    23| asynchronous DNS resolver (threaded or c-ares), otherwise you may
    24| encounter various problems with request timeouts (for more
    25| information, see
    26| http://curl.haxx.se/libcurl/c/curl_easy_setopt.html#CURLOPTCONNECTTIMEOUTMS
    27| and comments in curl_httpclient.py).
    28| To select ``curl_httpclient``, call `AsyncHTTPClient.configure` at startup::
    29|     AsyncHTTPClient.configure("tornado.curl_httpclient.CurlAsyncHTTPClient")
    30| """
    31| from __future__ import absolute_import, division, print_function
    32| import functools
    33| import time
    34| import weakref
    35| from salt.ext.tornado.concurrent import TracebackFuture
    36| from salt.ext.tornado.escape import utf8, native_str
    37| from salt.ext.tornado import httputil, stack_context
    38| from salt.ext.tornado.ioloop import IOLoop
    39| from salt.ext.tornado.util import Configurable
    40| class HTTPClient(object):
    41|     """A blocking HTTP client.
    42|     This interface is provided for convenience and testing; most applications
    43|     that are running an IOLoop will want to use `AsyncHTTPClient` instead.
    44|     Typical usage looks like this::
    45|         http_client = httpclient.HTTPClient()
    46|         try:
    47|             response = http_client.fetch("http://www.google.com/")
    48|             print(response.body)
    49|         except httpclient.HTTPError as e:
    50|             print("Error: " + str(e))
    51|         except Exception as e:
    52|             print("Error: " + str(e))
    53|         http_client.close()
    54|     """
    55|     def __init__(self, async_client_class=None, **kwargs):
    56|         self._io_loop = IOLoop(make_current=False)
    57|         if async_client_class is None:
    58|             async_client_class = AsyncHTTPClient
    59|         self._async_client = async_client_class(self._io_loop, **kwargs)
    60|         self._closed = False
    61|     def __del__(self):
    62|         self.close()
    63|     def close(self):
    64|         """Closes the HTTPClient, freeing any resources used."""
    65|         if not self._closed:
    66|             self._async_client.close()
    67|             self._io_loop.close()
    68|             self._closed = True
    69|     def fetch(self, request, **kwargs):
    70|         """Executes a request, returning an `HTTPResponse`.
    71|         The request may be either a string URL or an `HTTPRequest` object.
    72|         If it is a string, we construct an `HTTPRequest` using any additional
    73|         kwargs: ``HTTPRequest(request, **kwargs)``
    74|         If an error occurs during the fetch, we raise an `HTTPError` unless
    75|         the ``raise_error`` keyword argument is set to False.
    76|         """
    77|         response = self._io_loop.run_sync(functools.partial(
    78|             self._async_client.fetch, request, **kwargs))
    79|         return response
    80| class AsyncHTTPClient(Configurable):
    81|     """An non-blocking HTTP client.
    82|     Example usage::
    83|         def handle_response(response):
    84|             if response.error:
    85|                 print("Error: %s" % response.error)
    86|             else:
    87|                 print(response.body)
    88|         http_client = AsyncHTTPClient()
    89|         http_client.fetch("http://www.google.com/", handle_response)
    90|     The constructor for this class is magic in several respects: It
    91|     actually creates an instance of an implementation-specific
    92|     subclass, and instances are reused as a kind of pseudo-singleton
    93|     (one per `.IOLoop`).  The keyword argument ``force_instance=True``
    94|     can be used to suppress this singleton behavior.  Unless
    95|     ``force_instance=True`` is used, no arguments other than
    96|     ``io_loop`` should be passed to the `AsyncHTTPClient` constructor.
    97|     The implementation subclass as well as arguments to its
    98|     constructor can be set with the static method `configure()`
    99|     All `AsyncHTTPClient` implementations support a ``defaults``
   100|     keyword argument, which can be used to set default values for
   101|     `HTTPRequest` attributes.  For example::
   102|         AsyncHTTPClient.configure(
   103|             None, defaults=dict(user_agent="MyUserAgent"))
   104|         client = AsyncHTTPClient(force_instance=True,
   105|             defaults=dict(user_agent="MyUserAgent"))
   106|     .. versionchanged:: 4.1
   107|        The ``io_loop`` argument is deprecated.
   108|     """
   109|     @classmethod
   110|     def configurable_base(cls):
   111|         return AsyncHTTPClient
   112|     @classmethod
   113|     def configurable_default(cls):
   114|         from salt.ext.tornado.simple_httpclient import SimpleAsyncHTTPClient
   115|         return SimpleAsyncHTTPClient
   116|     @classmethod
   117|     def _async_clients(cls):
   118|         attr_name = '_async_client_dict_' + cls.__name__
   119|         if not hasattr(cls, attr_name):
   120|             setattr(cls, attr_name, weakref.WeakKeyDictionary())
   121|         return getattr(cls, attr_name)
   122|     def __new__(cls, io_loop=None, force_instance=False, **kwargs):
   123|         io_loop = io_loop or IOLoop.current()
   124|         if force_instance:
   125|             instance_cache = None
   126|         else:
   127|             instance_cache = cls._async_clients()
   128|         if instance_cache is not None and io_loop in instance_cache:
   129|             return instance_cache[io_loop]
   130|         instance = super(AsyncHTTPClient, cls).__new__(cls, io_loop=io_loop,
   131|                                                        **kwargs)
   132|         instance._instance_cache = instance_cache
   133|         if instance_cache is not None:
   134|             instance_cache[instance.io_loop] = instance
   135|         return instance
   136|     def initialize(self, io_loop, defaults=None):
   137|         self.io_loop = io_loop
   138|         self.defaults = dict(HTTPRequest._DEFAULTS)
   139|         if defaults is not None:
   140|             self.defaults.update(defaults)
   141|         self._closed = False
   142|     def close(self):
   143|         """Destroys this HTTP client, freeing any file descriptors used.
   144|         This method is **not needed in normal use** due to the way
   145|         that `AsyncHTTPClient` objects are transparently reused.
   146|         ``close()`` is generally only necessary when either the
   147|         `.IOLoop` is also being closed, or the ``force_instance=True``
   148|         argument was used when creating the `AsyncHTTPClient`.
   149|         No other methods may be called on the `AsyncHTTPClient` after
   150|         ``close()``.
   151|         """
   152|         if self._closed:
   153|             return
   154|         self._closed = True
   155|         if self._instance_cache is not None:
   156|             if self._instance_cache.get(self.io_loop) is not self:
   157|                 raise RuntimeError("inconsistent AsyncHTTPClient cache")
   158|             del self._instance_cache[self.io_loop]
   159|     def fetch(self, request, callback=None, raise_error=True, **kwargs):
   160|         """Executes a request, asynchronously returning an `HTTPResponse`.
   161|         The request may be either a string URL or an `HTTPRequest` object.
   162|         If it is a string, we construct an `HTTPRequest` using any additional
   163|         kwargs: ``HTTPRequest(request, **kwargs)``
   164|         This method returns a `.Future` whose result is an
   165|         `HTTPResponse`. By default, the ``Future`` will raise an
   166|         `HTTPError` if the request returned a non-200 response code
   167|         (other errors may also be raised if the server could not be
   168|         contacted). Instead, if ``raise_error`` is set to False, the
   169|         response will always be returned regardless of the response
   170|         code.
   171|         If a ``callback`` is given, it will be invoked with the `HTTPResponse`.
   172|         In the callback interface, `HTTPError` is not automatically raised.
   173|         Instead, you must check the response's ``error`` attribute or
   174|         call its `~HTTPResponse.rethrow` method.
   175|         """
   176|         if self._closed:
   177|             raise RuntimeError("fetch() called on closed AsyncHTTPClient")
   178|         if not isinstance(request, HTTPRequest):
   179|             request = HTTPRequest(url=request, **kwargs)
   180|         else:
   181|             if kwargs:
   182|                 raise ValueError("kwargs can't be used if request is an HTTPRequest object")
   183|         request.headers = httputil.HTTPHeaders(request.headers)
   184|         request = _RequestProxy(request, self.defaults)
   185|         future = TracebackFuture()
   186|         if callback is not None:
   187|             callback = stack_context.wrap(callback)
   188|             def handle_future(future):
   189|                 exc = future.exception()
   190|                 if isinstance(exc, HTTPError) and exc.response is not None:
   191|                     response = exc.response
   192|                 elif exc is not None:
   193|                     response = HTTPResponse(
   194|                         request, 599, error=exc,
   195|                         request_time=time.time() - request.start_time)
   196|                 else:
   197|                     response = future.result()
   198|                 self.io_loop.add_callback(callback, response)
   199|             future.add_done_callback(handle_future)
   200|         def handle_response(response):
   201|             if raise_error and response.error:
   202|                 future.set_exception(response.error)
   203|             else:
   204|                 future.set_result(response)
   205|         self.fetch_impl(request, handle_response)
   206|         return future
   207|     def fetch_impl(self, request, callback):
   208|         raise NotImplementedError()
   209|     @classmethod
   210|     def configure(cls, impl, **kwargs):
   211|         """Configures the `AsyncHTTPClient` subclass to use.
   212|         ``AsyncHTTPClient()`` actually creates an instance of a subclass.
   213|         This method may be called with either a class object or the
   214|         fully-qualified name of such a class (or ``None`` to use the default,
   215|         ``SimpleAsyncHTTPClient``)
   216|         If additional keyword arguments are given, they will be passed
   217|         to the constructor of each subclass instance created.  The
   218|         keyword argument ``max_clients`` determines the maximum number
   219|         of simultaneous `~AsyncHTTPClient.fetch()` operations that can
   220|         execute in parallel on each `.IOLoop`.  Additional arguments
   221|         may be supported depending on the implementation class in use.
   222|         Example::
   223|            AsyncHTTPClient.configure("tornado.curl_httpclient.CurlAsyncHTTPClient")
   224|         """
   225|         super(AsyncHTTPClient, cls).configure(impl, **kwargs)
   226| class HTTPRequest(object):
   227|     """HTTP client request object."""
   228|     _DEFAULTS = dict(
   229|         connect_timeout=20.0,
   230|         request_timeout=20.0,
   231|         follow_redirects=True,
   232|         max_redirects=5,
   233|         decompress_response=True,
   234|         proxy_password='',
   235|         allow_nonstandard_methods=False,
   236|         validate_cert=True)
   237|     def __init__(self, url, method="GET", headers=None, body=None,
   238|                  auth_username=None, auth_password=None, auth_mode=None,
   239|                  connect_timeout=None, request_timeout=None,
   240|                  if_modified_since=None, follow_redirects=None,
   241|                  max_redirects=None, user_agent=None, use_gzip=None,
   242|                  network_interface=None, streaming_callback=None,
   243|                  header_callback=None, prepare_curl_callback=None,
   244|                  proxy_host=None, proxy_port=None, proxy_username=None,
   245|                  proxy_password=None, proxy_auth_mode=None,
   246|                  allow_nonstandard_methods=None, validate_cert=None,
   247|                  ca_certs=None, allow_ipv6=None, client_key=None,
   248|                  client_cert=None, body_producer=None,
   249|                  expect_100_continue=False, decompress_response=None,
   250|                  ssl_options=None):
   251|         r"""All parameters except ``url`` are optional.
   252|         :arg string url: URL to fetch
   253|         :arg string method: HTTP method, e.g. "GET" or "POST"
   254|         :arg headers: Additional HTTP headers to pass on the request
   255|         :type headers: `~tornado.httputil.HTTPHeaders` or `dict`
   256|         :arg body: HTTP request body as a string (byte or unicode; if unicode
   257|            the utf-8 encoding will be used)
   258|         :arg body_producer: Callable used for lazy/asynchronous request bodies.
   259|            It is called with one argument, a ``write`` function, and should
   260|            return a `.Future`.  It should call the write function with new
   261|            data as it becomes available.  The write function returns a
   262|            `.Future` which can be used for flow control.
   263|            Only one of ``body`` and ``body_producer`` may
   264|            be specified.  ``body_producer`` is not supported on
   265|            ``curl_httpclient``.  When using ``body_producer`` it is recommended
   266|            to pass a ``Content-Length`` in the headers as otherwise chunked
   267|            encoding will be used, and many servers do not support chunked
   268|            encoding on requests.  New in Tornado 4.0
   269|         :arg string auth_username: Username for HTTP authentication
   270|         :arg string auth_password: Password for HTTP authentication
   271|         :arg string auth_mode: Authentication mode; default is "basic".
   272|            Allowed values are implementation-defined; ``curl_httpclient``
   273|            supports "basic" and "digest"; ``simple_httpclient`` only supports
   274|            "basic"
   275|         :arg float connect_timeout: Timeout for initial connection in seconds,
   276|            default 20 seconds
   277|         :arg float request_timeout: Timeout for entire request in seconds,
   278|            default 20 seconds
   279|         :arg if_modified_since: Timestamp for ``If-Modified-Since`` header
   280|         :type if_modified_since: `datetime` or `float`
   281|         :arg bool follow_redirects: Should redirects be followed automatically
   282|            or return the 3xx response? Default True.
   283|         :arg int max_redirects: Limit for ``follow_redirects``, default 5.
   284|         :arg string user_agent: String to send as ``User-Agent`` header
   285|         :arg bool decompress_response: Request a compressed response from
   286|            the server and decompress it after downloading.  Default is True.
   287|            New in Tornado 4.0.
   288|         :arg bool use_gzip: Deprecated alias for ``decompress_response``
   289|            since Tornado 4.0.
   290|         :arg string network_interface: Network interface to use for request.
   291|            ``curl_httpclient`` only; see note below.
   292|         :arg callable streaming_callback: If set, ``streaming_callback`` will
   293|            be run with each chunk of data as it is received, and
   294|            ``HTTPResponse.body`` and ``HTTPResponse.buffer`` will be empty in
   295|            the final response.
   296|         :arg callable header_callback: If set, ``header_callback`` will
   297|            be run with each header line as it is received (including the
   298|            first line, e.g. ``HTTP/1.0 200 OK\r\n``, and a final line
   299|            containing only ``\r\n``.  All lines include the trailing newline
   300|            characters).  ``HTTPResponse.headers`` will be empty in the final
   301|            response.  This is most useful in conjunction with
   302|            ``streaming_callback``, because it's the only way to get access to
   303|            header data while the request is in progress.
   304|         :arg callable prepare_curl_callback: If set, will be called with
   305|            a ``pycurl.Curl`` object to allow the application to make additional
   306|            ``setopt`` calls.
   307|         :arg string proxy_host: HTTP proxy hostname.  To use proxies,
   308|            ``proxy_host`` and ``proxy_port`` must be set; ``proxy_username``,
   309|            ``proxy_pass`` and ``proxy_auth_mode`` are optional.  Proxies are
   310|            currently only supported with ``curl_httpclient``.
   311|         :arg int proxy_port: HTTP proxy port
   312|         :arg string proxy_username: HTTP proxy username
   313|         :arg string proxy_password: HTTP proxy password
   314|         :arg string proxy_auth_mode: HTTP proxy Authentication mode;
   315|            default is "basic". supports "basic" and "digest"
   316|         :arg bool allow_nonstandard_methods: Allow unknown values for ``method``
   317|            argument? Default is False.
   318|         :arg bool validate_cert: For HTTPS requests, validate the server's
   319|            certificate? Default is True.
   320|         :arg string ca_certs: filename of CA certificates in PEM format,
   321|            or None to use defaults.  See note below when used with
   322|            ``curl_httpclient``.
   323|         :arg string client_key: Filename for client SSL key, if any.  See
   324|            note below when used with ``curl_httpclient``.
   325|         :arg string client_cert: Filename for client SSL certificate, if any.
   326|            See note below when used with ``curl_httpclient``.
   327|         :arg ssl.SSLContext ssl_options: `ssl.SSLContext` object for use in
   328|            ``simple_httpclient`` (unsupported by ``curl_httpclient``).
   329|            Overrides ``validate_cert``, ``ca_certs``, ``client_key``,
   330|            and ``client_cert``.
   331|         :arg bool allow_ipv6: Use IPv6 when available?  Default is true.
   332|         :arg bool expect_100_continue: If true, send the
   333|            ``Expect: 100-continue`` header and wait for a continue response
   334|            before sending the request body.  Only supported with
   335|            simple_httpclient.
   336|         .. note::
   337|             When using ``curl_httpclient`` certain options may be
   338|             inherited by subsequent fetches because ``pycurl`` does
   339|             not allow them to be cleanly reset.  This applies to the
   340|             ``ca_certs``, ``client_key``, ``client_cert``, and
   341|             ``network_interface`` arguments.  If you use these
   342|             options, you should pass them on every request (you don't
   343|             have to always use the same values, but it's not possible
   344|             to mix requests that specify these options with ones that
   345|             use the defaults).
   346|         .. versionadded:: 3.1
   347|            The ``auth_mode`` argument.
   348|         .. versionadded:: 4.0
   349|            The ``body_producer`` and ``expect_100_continue`` arguments.
   350|         .. versionadded:: 4.2
   351|            The ``ssl_options`` argument.
   352|         .. versionadded:: 4.5
   353|            The ``proxy_auth_mode`` argument.
   354|         """
   355|         self.headers = headers
   356|         if if_modified_since:
   357|             self.headers["If-Modified-Since"] = httputil.format_timestamp(
   358|                 if_modified_since)
   359|         self.proxy_host = proxy_host
   360|         self.proxy_port = proxy_port
   361|         self.proxy_username = proxy_username
   362|         self.proxy_password = proxy_password
   363|         self.proxy_auth_mode = proxy_auth_mode
   364|         self.url = url
   365|         self.method = method
   366|         self.body = body
   367|         self.body_producer = body_producer
   368|         self.auth_username = auth_username
   369|         self.auth_password = auth_password
   370|         self.auth_mode = auth_mode
   371|         self.connect_timeout = connect_timeout
   372|         self.request_timeout = request_timeout
   373|         self.follow_redirects = follow_redirects
   374|         self.max_redirects = max_redirects
   375|         self.user_agent = user_agent
   376|         if decompress_response is not None:
   377|             self.decompress_response = decompress_response
   378|         else:
   379|             self.decompress_response = use_gzip
   380|         self.network_interface = network_interface
   381|         self.streaming_callback = streaming_callback
   382|         self.header_callback = header_callback
   383|         self.prepare_curl_callback = prepare_curl_callback
   384|         self.allow_nonstandard_methods = allow_nonstandard_methods
   385|         self.validate_cert = validate_cert
   386|         self.ca_certs = ca_certs
   387|         self.allow_ipv6 = allow_ipv6
   388|         self.client_key = client_key
   389|         self.client_cert = client_cert
   390|         self.ssl_options = ssl_options
   391|         self.expect_100_continue = expect_100_continue
   392|         self.start_time = time.time()
   393|     @property
   394|     def headers(self):
   395|         return self._headers
   396|     @headers.setter
   397|     def headers(self, value):
   398|         if value is None:
   399|             self._headers = httputil.HTTPHeaders()
   400|         else:
   401|             self._headers = value
   402|     @property
   403|     def body(self):
   404|         return self._body
   405|     @body.setter
   406|     def body(self, value):
   407|         self._body = utf8(value)
   408|     @property
   409|     def body_producer(self):
   410|         return self._body_producer
   411|     @body_producer.setter
   412|     def body_producer(self, value):
   413|         self._body_producer = stack_context.wrap(value)
   414|     @property
   415|     def streaming_callback(self):
   416|         return self._streaming_callback
   417|     @streaming_callback.setter
   418|     def streaming_callback(self, value):
   419|         self._streaming_callback = stack_context.wrap(value)
   420|     @property
   421|     def header_callback(self):
   422|         return self._header_callback
   423|     @header_callback.setter
   424|     def header_callback(self, value):
   425|         self._header_callback = stack_context.wrap(value)
   426|     @property
   427|     def prepare_curl_callback(self):
   428|         return self._prepare_curl_callback
   429|     @prepare_curl_callback.setter
   430|     def prepare_curl_callback(self, value):
   431|         self._prepare_curl_callback = stack_context.wrap(value)
   432| class HTTPResponse(object):
   433|     """HTTP Response object.
   434|     Attributes:
   435|     * request: HTTPRequest object
   436|     * code: numeric HTTP status code, e.g. 200 or 404
   437|     * reason: human-readable reason phrase describing the status code
   438|     * headers: `tornado.httputil.HTTPHeaders` object
   439|     * effective_url: final location of the resource after following any
   440|       redirects
   441|     * buffer: ``cStringIO`` object for response body
   442|     * body: response body as bytes (created on demand from ``self.buffer``)
   443|     * error: Exception object, if any
   444|     * request_time: seconds from request start to finish
   445|     * time_info: dictionary of diagnostic timing information from the request.
   446|       Available data are subject to change, but currently uses timings
   447|       available from http://curl.haxx.se/libcurl/c/curl_easy_getinfo.html,
   448|       plus ``queue``, which is the delay (if any) introduced by waiting for
   449|       a slot under `AsyncHTTPClient`'s ``max_clients`` setting.
   450|     """
   451|     def __init__(self, request, code, headers=None, buffer=None,
   452|                  effective_url=None, error=None, request_time=None,
   453|                  time_info=None, reason=None):
   454|         if isinstance(request, _RequestProxy):
   455|             self.request = request.request
   456|         else:
   457|             self.request = request
   458|         self.code = code
   459|         self.reason = reason or httputil.responses.get(code, "Unknown")
   460|         if headers is not None:
   461|             self.headers = headers
   462|         else:
   463|             self.headers = httputil.HTTPHeaders()
   464|         self.buffer = buffer
   465|         self._body = None
   466|         if effective_url is None:
   467|             self.effective_url = request.url
   468|         else:
   469|             self.effective_url = effective_url
   470|         if error is None:
   471|             if self.code < 200 or self.code >= 300:
   472|                 self.error = HTTPError(self.code, message=self.reason,
   473|                                        response=self)
   474|             else:
   475|                 self.error = None
   476|         else:
   477|             self.error = error
   478|         self.request_time = request_time
   479|         self.time_info = time_info or {}
   480|     @property
   481|     def body(self):
   482|         if self.buffer is None:
   483|             return None
   484|         elif self._body is None:
   485|             self._body = self.buffer.getvalue()
   486|         return self._body
   487|     def rethrow(self):
   488|         """If there was an error on the request, raise an `HTTPError`."""
   489|         if self.error:
   490|             raise self.error
   491|     def __repr__(self):
   492|         args = ",".join("%s=%r" % i for i in sorted(self.__dict__.items()))
   493|         return "%s(%s)" % (self.__class__.__name__, args)
   494| class HTTPError(Exception):
   495|     """Exception thrown for an unsuccessful HTTP request.
   496|     Attributes:
   497|     * ``code`` - HTTP error integer error code, e.g. 404.  Error code 599 is
   498|       used when no HTTP response was received, e.g. for a timeout.
   499|     * ``response`` - `HTTPResponse` object, if any.
   500|     Note that if ``follow_redirects`` is False, redirects become HTTPErrors,
   501|     and you can look at ``error.response.headers['Location']`` to see the
   502|     destination of the redirect.
   503|     """
   504|     def __init__(self, code, message=None, response=None):
   505|         self.code = code
   506|         self.message = message or httputil.responses.get(code, "Unknown")
   507|         self.response = response
   508|         super(HTTPError, self).__init__(code, message, response)
   509|     def __str__(self):
   510|         return "HTTP %d: %s" % (self.code, self.message)
   511|     __repr__ = __str__
   512| class _RequestProxy(object):
   513|     """Combines an object with a dictionary of defaults.
   514|     Used internally by AsyncHTTPClient implementations.
   515|     """
   516|     def __init__(self, request, defaults):
   517|         self.request = request
   518|         self.defaults = defaults
   519|     def __getattr__(self, name):
   520|         request_attr = getattr(self.request, name)
   521|         if request_attr is not None:
   522|             return request_attr
   523|         elif self.defaults is not None:
   524|             return self.defaults.get(name, None)
   525|         else:
   526|             return None
   527| def main():
   528|     from salt.ext.tornado.options import define, options, parse_command_line
   529|     define("print_headers", type=bool, default=False)
   530|     define("print_body", type=bool, default=True)
   531|     define("follow_redirects", type=bool, default=True)
   532|     define("validate_cert", type=bool, default=True)
   533|     args = parse_command_line()
   534|     client = HTTPClient()
   535|     for arg in args:
   536|         try:
   537|             response = client.fetch(arg,
   538|                                     follow_redirects=options.follow_redirects,
   539|                                     validate_cert=options.validate_cert,
   540|                                     )
   541|         except HTTPError as e:
   542|             if e.response is not None:
   543|                 response = e.response
   544|             else:
   545|                 raise
   546|         if options.print_headers:
   547|             print(response.headers)
   548|         if options.print_body:
   549|             print(native_str(response.body))
   550|     client.close()
   551| if __name__ == "__main__":
   552|     main()


# ====================================================================
# FILE: salt/ext/tornado/httpserver.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-234 ---
     1| """A non-blocking, single-threaded HTTP server.
     2| Typical applications have little direct interaction with the `HTTPServer`
     3| class except to start a server at the beginning of the process
     4| (and even that is often done indirectly via `tornado.web.Application.listen`).
     5| .. versionchanged:: 4.0
     6|    The ``HTTPRequest`` class that used to live in this module has been moved
     7|    to `tornado.httputil.HTTPServerRequest`.  The old name remains as an alias.
     8| """
     9| from __future__ import absolute_import, division, print_function
    10| import socket
    11| from salt.ext.tornado.escape import native_str
    12| from salt.ext.tornado.http1connection import HTTP1ServerConnection, HTTP1ConnectionParameters
    13| from salt.ext.tornado import gen
    14| from salt.ext.tornado import httputil
    15| from salt.ext.tornado import iostream
    16| from salt.ext.tornado import netutil
    17| from salt.ext.tornado.tcpserver import TCPServer
    18| from salt.ext.tornado.util import Configurable
    19| class HTTPServer(TCPServer, Configurable,
    20|                  httputil.HTTPServerConnectionDelegate):
    21|     r"""A non-blocking, single-threaded HTTP server.
    22|     A server is defined by a subclass of `.HTTPServerConnectionDelegate`,
    23|     or, for backwards compatibility, a callback that takes an
    24|     `.HTTPServerRequest` as an argument. The delegate is usually a
    25|     `tornado.web.Application`.
    26|     `HTTPServer` supports keep-alive connections by default
    27|     (automatically for HTTP/1.1, or for HTTP/1.0 when the client
    28|     requests ``Connection: keep-alive``).
    29|     If ``xheaders`` is ``True``, we support the
    30|     ``X-Real-Ip``/``X-Forwarded-For`` and
    31|     ``X-Scheme``/``X-Forwarded-Proto`` headers, which override the
    32|     remote IP and URI scheme/protocol for all requests.  These headers
    33|     are useful when running Tornado behind a reverse proxy or load
    34|     balancer.  The ``protocol`` argument can also be set to ``https``
    35|     if Tornado is run behind an SSL-decoding proxy that does not set one of
    36|     the supported ``xheaders``.
    37|     By default, when parsing the ``X-Forwarded-For`` header, Tornado will
    38|     select the last (i.e., the closest) address on the list of hosts as the
    39|     remote host IP address.  To select the next server in the chain, a list of
    40|     trusted downstream hosts may be passed as the ``trusted_downstream``
    41|     argument.  These hosts will be skipped when parsing the ``X-Forwarded-For``
    42|     header.
    43|     To make this server serve SSL traffic, send the ``ssl_options`` keyword
    44|     argument with an `ssl.SSLContext` object. For compatibility with older
    45|     versions of Python ``ssl_options`` may also be a dictionary of keyword
    46|     arguments for the `ssl.wrap_socket` method.::
    47|        ssl_ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)
    48|        ssl_ctx.load_cert_chain(os.path.join(data_dir, "mydomain.crt"),
    49|                                os.path.join(data_dir, "mydomain.key"))
    50|        HTTPServer(applicaton, ssl_options=ssl_ctx)
    51|     `HTTPServer` initialization follows one of three patterns (the
    52|     initialization methods are defined on `tornado.tcpserver.TCPServer`):
    53|     1. `~tornado.tcpserver.TCPServer.listen`: simple single-process::
    54|             server = HTTPServer(app)
    55|             server.listen(8888)
    56|             IOLoop.current().start()
    57|        In many cases, `tornado.web.Application.listen` can be used to avoid
    58|        the need to explicitly create the `HTTPServer`.
    59|     2. `~tornado.tcpserver.TCPServer.bind`/`~tornado.tcpserver.TCPServer.start`:
    60|        simple multi-process::
    61|             server = HTTPServer(app)
    62|             server.bind(8888)
    63|             server.start(0)  # Forks multiple sub-processes
    64|             IOLoop.current().start()
    65|        When using this interface, an `.IOLoop` must *not* be passed
    66|        to the `HTTPServer` constructor.  `~.TCPServer.start` will always start
    67|        the server on the default singleton `.IOLoop`.
    68|     3. `~tornado.tcpserver.TCPServer.add_sockets`: advanced multi-process::
    69|             sockets = tornado.netutil.bind_sockets(8888)
    70|             tornado.process.fork_processes(0)
    71|             server = HTTPServer(app)
    72|             server.add_sockets(sockets)
    73|             IOLoop.current().start()
    74|        The `~.TCPServer.add_sockets` interface is more complicated,
    75|        but it can be used with `tornado.process.fork_processes` to
    76|        give you more flexibility in when the fork happens.
    77|        `~.TCPServer.add_sockets` can also be used in single-process
    78|        servers if you want to create your listening sockets in some
    79|        way other than `tornado.netutil.bind_sockets`.
    80|     .. versionchanged:: 4.0
    81|        Added ``decompress_request``, ``chunk_size``, ``max_header_size``,
    82|        ``idle_connection_timeout``, ``body_timeout``, ``max_body_size``
    83|        arguments.  Added support for `.HTTPServerConnectionDelegate`
    84|        instances as ``request_callback``.
    85|     .. versionchanged:: 4.1
    86|        `.HTTPServerConnectionDelegate.start_request` is now called with
    87|        two arguments ``(server_conn, request_conn)`` (in accordance with the
    88|        documentation) instead of one ``(request_conn)``.
    89|     .. versionchanged:: 4.2
    90|        `HTTPServer` is now a subclass of `tornado.util.Configurable`.
    91|     .. versionchanged:: 4.5
    92|        Added the ``trusted_downstream`` argument.
    93|     """
    94|     def __init__(self, *args, **kwargs):
    95|         pass
    96|     def initialize(self, request_callback, no_keep_alive=False, io_loop=None,
    97|                    xheaders=False, ssl_options=None, protocol=None,
    98|                    decompress_request=False,
    99|                    chunk_size=None, max_header_size=None,
   100|                    idle_connection_timeout=None, body_timeout=None,
   101|                    max_body_size=None, max_buffer_size=None,
   102|                    trusted_downstream=None):
   103|         self.request_callback = request_callback
   104|         self.no_keep_alive = no_keep_alive
   105|         self.xheaders = xheaders
   106|         self.protocol = protocol
   107|         self.conn_params = HTTP1ConnectionParameters(
   108|             decompress=decompress_request,
   109|             chunk_size=chunk_size,
   110|             max_header_size=max_header_size,
   111|             header_timeout=idle_connection_timeout or 3600,
   112|             max_body_size=max_body_size,
   113|             body_timeout=body_timeout,
   114|             no_keep_alive=no_keep_alive)
   115|         TCPServer.__init__(self, io_loop=io_loop, ssl_options=ssl_options,
   116|                            max_buffer_size=max_buffer_size,
   117|                            read_chunk_size=chunk_size)
   118|         self._connections = set()
   119|         self.trusted_downstream = trusted_downstream
   120|     @classmethod
   121|     def configurable_base(cls):
   122|         return HTTPServer
   123|     @classmethod
   124|     def configurable_default(cls):
   125|         return HTTPServer
   126|     @gen.coroutine
   127|     def close_all_connections(self):
   128|         while self._connections:
   129|             conn = next(iter(self._connections))
   130|             yield conn.close()
   131|     def handle_stream(self, stream, address):
   132|         context = _HTTPRequestContext(stream, address,
   133|                                       self.protocol,
   134|                                       self.trusted_downstream)
   135|         conn = HTTP1ServerConnection(
   136|             stream, self.conn_params, context)
   137|         self._connections.add(conn)
   138|         conn.start_serving(self)
   139|     def start_request(self, server_conn, request_conn):
   140|         if isinstance(self.request_callback, httputil.HTTPServerConnectionDelegate):
   141|             delegate = self.request_callback.start_request(server_conn, request_conn)
   142|         else:
   143|             delegate = _CallableAdapter(self.request_callback, request_conn)
   144|         if self.xheaders:
   145|             delegate = _ProxyAdapter(delegate, request_conn)
   146|         return delegate
   147|     def on_close(self, server_conn):
   148|         self._connections.remove(server_conn)
   149| class _CallableAdapter(httputil.HTTPMessageDelegate):
   150|     def __init__(self, request_callback, request_conn):
   151|         self.connection = request_conn
   152|         self.request_callback = request_callback
   153|         self.request = None
   154|         self.delegate = None
   155|         self._chunks = []
   156|     def headers_received(self, start_line, headers):
   157|         self.request = httputil.HTTPServerRequest(
   158|             connection=self.connection, start_line=start_line,
   159|             headers=headers)
   160|     def data_received(self, chunk):
   161|         self._chunks.append(chunk)
   162|     def finish(self):
   163|         self.request.body = b''.join(self._chunks)
   164|         self.request._parse_body()
   165|         self.request_callback(self.request)
   166|     def on_connection_close(self):
   167|         self._chunks = None
   168| class _HTTPRequestContext(object):
   169|     def __init__(self, stream, address, protocol, trusted_downstream=None):
   170|         self.address = address
   171|         if stream.socket is not None:
   172|             self.address_family = stream.socket.family
   173|         else:
   174|             self.address_family = None
   175|         if (self.address_family in (socket.AF_INET, socket.AF_INET6) and
   176|                 address is not None):
   177|             self.remote_ip = address[0]
   178|         else:
   179|             self.remote_ip = '0.0.0.0'
   180|         if protocol:
   181|             self.protocol = protocol
   182|         elif isinstance(stream, iostream.SSLIOStream):
   183|             self.protocol = "https"
   184|         else:
   185|             self.protocol = "http"
   186|         self._orig_remote_ip = self.remote_ip
   187|         self._orig_protocol = self.protocol
   188|         self.trusted_downstream = set(trusted_downstream or [])
   189|     def __str__(self):
   190|         if self.address_family in (socket.AF_INET, socket.AF_INET6):
   191|             return self.remote_ip
   192|         elif isinstance(self.address, bytes):
   193|             return native_str(self.address)
   194|         else:
   195|             return str(self.address)
   196|     def _apply_xheaders(self, headers):
   197|         """Rewrite the ``remote_ip`` and ``protocol`` fields."""
   198|         ip = headers.get("X-Forwarded-For", self.remote_ip)
   199|         for ip in (cand.strip() for cand in reversed(ip.split(','))):
   200|             if ip not in self.trusted_downstream:
   201|                 break
   202|         ip = headers.get("X-Real-Ip", ip)
   203|         if netutil.is_valid_ip(ip):
   204|             self.remote_ip = ip
   205|         proto_header = headers.get(
   206|             "X-Scheme", headers.get("X-Forwarded-Proto",
   207|                                     self.protocol))
   208|         if proto_header in ("http", "https"):
   209|             self.protocol = proto_header
   210|     def _unapply_xheaders(self):
   211|         """Undo changes from `_apply_xheaders`.
   212|         Xheaders are per-request so they should not leak to the next
   213|         request on the same connection.
   214|         """
   215|         self.remote_ip = self._orig_remote_ip
   216|         self.protocol = self._orig_protocol
   217| class _ProxyAdapter(httputil.HTTPMessageDelegate):
   218|     def __init__(self, delegate, request_conn):
   219|         self.connection = request_conn
   220|         self.delegate = delegate
   221|     def headers_received(self, start_line, headers):
   222|         self.connection.context._apply_xheaders(headers)
   223|         return self.delegate.headers_received(start_line, headers)
   224|     def data_received(self, chunk):
   225|         return self.delegate.data_received(chunk)
   226|     def finish(self):
   227|         self.delegate.finish()
   228|         self._cleanup()
   229|     def on_connection_close(self):
   230|         self.delegate.on_connection_close()
   231|         self._cleanup()
   232|     def _cleanup(self):
   233|         self.connection.context._unapply_xheaders()
   234| HTTPRequest = httputil.HTTPServerRequest


# ====================================================================
# FILE: salt/ext/tornado/httputil.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-775 ---
     1| """HTTP utility code shared by clients and servers.
     2| This module also defines the `HTTPServerRequest` class which is exposed
     3| via `tornado.web.RequestHandler.request`.
     4| """
     5| from __future__ import absolute_import, division, print_function
     6| import calendar
     7| import collections
     8| import copy
     9| import datetime
    10| import email.utils
    11| import numbers
    12| import re
    13| import time
    14| from collections.abc import MutableMapping
    15| from salt.ext.tornado.escape import native_str, parse_qs_bytes, utf8
    16| from salt.ext.tornado.log import gen_log
    17| from salt.ext.tornado.util import PY3, ObjectDict
    18| if PY3:
    19|     import http.cookies as Cookie
    20|     from http.client import responses
    21|     from urllib.parse import urlencode, urlparse, urlunparse, parse_qsl
    22| else:
    23|     import Cookie
    24|     from httplib import responses
    25|     from urllib import urlencode
    26|     from urlparse import urlparse, urlunparse, parse_qsl
    27| responses
    28| try:
    29|     from ssl import SSLError
    30| except ImportError:
    31|     class _SSLError(Exception):
    32|         pass
    33|     SSLError = _SSLError  # type: ignore
    34| try:
    35|     import typing
    36| except ImportError:
    37|     pass
    38| _CRLF_RE = re.compile(r"\r?\n")
    39| class _NormalizedHeaderCache(dict):
    40|     """Dynamic cached mapping of header names to Http-Header-Case.
    41|     Implemented as a dict subclass so that cache hits are as fast as a
    42|     normal dict lookup, without the overhead of a python function
    43|     call.
    44|     >>> normalized_headers = _NormalizedHeaderCache(10)
    45|     >>> normalized_headers["coNtent-TYPE"]
    46|     'Content-Type'
    47|     """
    48|     def __init__(self, size):
    49|         super(_NormalizedHeaderCache, self).__init__()
    50|         self.size = size
    51|         self.queue = collections.deque()
    52|     def __missing__(self, key):
    53|         normalized = "-".join([w.capitalize() for w in key.split("-")])
    54|         self[key] = normalized
    55|         self.queue.append(key)
    56|         if len(self.queue) > self.size:
    57|             old_key = self.queue.popleft()
    58|             del self[old_key]
    59|         return normalized
    60| _normalized_headers = _NormalizedHeaderCache(1000)
    61| class HTTPHeaders(MutableMapping):
    62|     """A dictionary that maintains ``Http-Header-Case`` for all keys.
    63|     Supports multiple values per key via a pair of new methods,
    64|     `add()` and `get_list()`.  The regular dictionary interface
    65|     returns a single value per key, with multiple values joined by a
    66|     comma.
    67|     >>> h = HTTPHeaders({"content-type": "text/html"})
    68|     >>> list(h.keys())
    69|     ['Content-Type']
    70|     >>> h["Content-Type"]
    71|     'text/html'
    72|     >>> h.add("Set-Cookie", "A=B")
    73|     >>> h.add("Set-Cookie", "C=D")
    74|     >>> h["set-cookie"]
    75|     'A=B,C=D'
    76|     >>> h.get_list("set-cookie")
    77|     ['A=B', 'C=D']
    78|     >>> for (k,v) in sorted(h.get_all()):
    79|     ...    print('%s: %s' % (k,v))
    80|     ...
    81|     Content-Type: text/html
    82|     Set-Cookie: A=B
    83|     Set-Cookie: C=D
    84|     """
    85|     def __init__(self, *args, **kwargs):
    86|         self._dict = {}  # type: typing.Dict[str, str]
    87|         self._as_list = {}  # type: typing.Dict[str, typing.List[str]]
    88|         self._last_key = None
    89|         if len(args) == 1 and len(kwargs) == 0 and isinstance(args[0], HTTPHeaders):
    90|             for k, v in args[0].get_all():
    91|                 self.add(k, v)
    92|         else:
    93|             self.update(*args, **kwargs)
    94|     def add(self, name, value):
    95|         """Adds a new value for the given key."""
    96|         norm_name = _normalized_headers[name]
    97|         self._last_key = norm_name
    98|         if norm_name in self:
    99|             self._dict[norm_name] = (
   100|                 native_str(self[norm_name]) + "," + native_str(value)
   101|             )
   102|             self._as_list[norm_name].append(value)
   103|         else:
   104|             self[norm_name] = value
   105|     def get_list(self, name):
   106|         """Returns all values for the given header as a list."""
   107|         norm_name = _normalized_headers[name]
   108|         return self._as_list.get(norm_name, [])
   109|     def get_all(self):
   110|         """Returns an iterable of all (name, value) pairs.
   111|         If a header has multiple values, multiple pairs will be
   112|         returned with the same name.
   113|         """
   114|         for name, values in self._as_list.items():
   115|             for value in values:
   116|                 yield (name, value)
   117|     def parse_line(self, line):
   118|         """Updates the dictionary with a single header line.
   119|         >>> h = HTTPHeaders()
   120|         >>> h.parse_line("Content-Type: text/html")
   121|         >>> h.get('content-type')
   122|         'text/html'
   123|         """
   124|         if line[0].isspace():
   125|             new_part = " " + line.lstrip()
   126|             self._as_list[self._last_key][-1] += new_part
   127|             self._dict[self._last_key] += new_part
   128|         else:
   129|             name, value = line.split(":", 1)
   130|             self.add(name, value.strip())
   131|     @classmethod
   132|     def parse(cls, headers):
   133|         """Returns a dictionary from HTTP header text.
   134|         >>> h = HTTPHeaders.parse("Content-Type: text/html\\r\\nContent-Length: 42\\r\\n")
   135|         >>> sorted(h.items())
   136|         [('Content-Length', '42'), ('Content-Type', 'text/html')]
   137|         """
   138|         h = cls()
   139|         for line in _CRLF_RE.split(headers):
   140|             if line:
   141|                 h.parse_line(line)
   142|         return h
   143|     def __setitem__(self, name, value):
   144|         norm_name = _normalized_headers[name]
   145|         self._dict[norm_name] = value
   146|         self._as_list[norm_name] = [value]
   147|     def __getitem__(self, name):
   148|         return self._dict[_normalized_headers[name]]
   149|     def __delitem__(self, name):
   150|         norm_name = _normalized_headers[name]
   151|         del self._dict[norm_name]
   152|         del self._as_list[norm_name]
   153|     def __len__(self):
   154|         return len(self._dict)
   155|     def __iter__(self):
   156|         return iter(self._dict)
   157|     def copy(self):
   158|         return HTTPHeaders(self)
   159|     __copy__ = copy
   160|     def __str__(self):
   161|         lines = []
   162|         for name, value in self.get_all():
   163|             lines.append("%s: %s\n" % (name, value))
   164|         return "".join(lines)
   165|     __unicode__ = __str__
   166| class HTTPServerRequest(object):
   167|     """A single HTTP request.
   168|     All attributes are type `str` unless otherwise noted.
   169|     .. attribute:: method
   170|        HTTP request method, e.g. "GET" or "POST"
   171|     .. attribute:: uri
   172|        The requested uri.
   173|     .. attribute:: path
   174|        The path portion of `uri`
   175|     .. attribute:: query
   176|        The query portion of `uri`
   177|     .. attribute:: version
   178|        HTTP version specified in request, e.g. "HTTP/1.1"
   179|     .. attribute:: headers
   180|        `.HTTPHeaders` dictionary-like object for request headers.  Acts like
   181|        a case-insensitive dictionary with additional methods for repeated
   182|        headers.
   183|     .. attribute:: body
   184|        Request body, if present, as a byte string.
   185|     .. attribute:: remote_ip
   186|        Client's IP address as a string.  If ``HTTPServer.xheaders`` is set,
   187|        will pass along the real IP address provided by a load balancer
   188|        in the ``X-Real-Ip`` or ``X-Forwarded-For`` header.
   189|     .. versionchanged:: 3.1
   190|        The list format of ``X-Forwarded-For`` is now supported.
   191|     .. attribute:: protocol
   192|        The protocol used, either "http" or "https".  If ``HTTPServer.xheaders``
   193|        is set, will pass along the protocol used by a load balancer if
   194|        reported via an ``X-Scheme`` header.
   195|     .. attribute:: host
   196|        The requested hostname, usually taken from the ``Host`` header.
   197|     .. attribute:: arguments
   198|        GET/POST arguments are available in the arguments property, which
   199|        maps arguments names to lists of values (to support multiple values
   200|        for individual names). Names are of type `str`, while arguments
   201|        are byte strings.  Note that this is different from
   202|        `.RequestHandler.get_argument`, which returns argument values as
   203|        unicode strings.
   204|     .. attribute:: query_arguments
   205|        Same format as ``arguments``, but contains only arguments extracted
   206|        from the query string.
   207|        .. versionadded:: 3.2
   208|     .. attribute:: body_arguments
   209|        Same format as ``arguments``, but contains only arguments extracted
   210|        from the request body.
   211|        .. versionadded:: 3.2
   212|     .. attribute:: files
   213|        File uploads are available in the files property, which maps file
   214|        names to lists of `.HTTPFile`.
   215|     .. attribute:: connection
   216|        An HTTP request is attached to a single HTTP connection, which can
   217|        be accessed through the "connection" attribute. Since connections
   218|        are typically kept open in HTTP/1.1, multiple requests can be handled
   219|        sequentially on a single connection.
   220|     .. versionchanged:: 4.0
   221|        Moved from ``tornado.httpserver.HTTPRequest``.
   222|     """
   223|     def __init__(
   224|         self,
   225|         method=None,
   226|         uri=None,
   227|         version="HTTP/1.0",
   228|         headers=None,
   229|         body=None,
   230|         host=None,
   231|         files=None,
   232|         connection=None,
   233|         start_line=None,
   234|         server_connection=None,
   235|     ):
   236|         if start_line is not None:
   237|             method, uri, version = start_line
   238|         self.method = method
   239|         self.uri = uri
   240|         self.version = version
   241|         self.headers = headers or HTTPHeaders()
   242|         self.body = body or b""
   243|         context = getattr(connection, "context", None)
   244|         self.remote_ip = getattr(context, "remote_ip", None)
   245|         self.protocol = getattr(context, "protocol", "http")
   246|         self.host = host or self.headers.get("Host") or "127.0.0.1"
   247|         self.host_name = split_host_and_port(self.host.lower())[0]
   248|         self.files = files or {}
   249|         self.connection = connection
   250|         self.server_connection = server_connection
   251|         self._start_time = time.time()
   252|         self._finish_time = None
   253|         self.path, sep, self.query = uri.partition("?")
   254|         self.arguments = parse_qs_bytes(self.query, keep_blank_values=True)
   255|         self.query_arguments = copy.deepcopy(self.arguments)
   256|         self.body_arguments = {}
   257|     def supports_http_1_1(self):
   258|         """Returns True if this request supports HTTP/1.1 semantics.
   259|         .. deprecated:: 4.0
   260|            Applications are less likely to need this information with the
   261|            introduction of `.HTTPConnection`.  If you still need it, access
   262|            the ``version`` attribute directly.
   263|         """
   264|         return self.version == "HTTP/1.1"
   265|     @property
   266|     def cookies(self):
   267|         """A dictionary of Cookie.Morsel objects."""
   268|         if not hasattr(self, "_cookies"):
   269|             self._cookies = Cookie.SimpleCookie()
   270|             if "Cookie" in self.headers:
   271|                 try:
   272|                     parsed = parse_cookie(self.headers["Cookie"])
   273|                 except Exception:
   274|                     pass
   275|                 else:
   276|                     for k, v in parsed.items():
   277|                         try:
   278|                             self._cookies[k] = v
   279|                         except Exception:
   280|                             pass
   281|         return self._cookies
   282|     def write(self, chunk, callback=None):
   283|         """Writes the given chunk to the response stream.
   284|         .. deprecated:: 4.0
   285|            Use ``request.connection`` and the `.HTTPConnection` methods
   286|            to write the response.
   287|         """
   288|         assert isinstance(chunk, bytes)
   289|         assert self.version.startswith(
   290|             "HTTP/1."
   291|         ), "deprecated interface only supported in HTTP/1.x"
   292|         self.connection.write(chunk, callback=callback)
   293|     def finish(self):
   294|         """Finishes this HTTP request on the open connection.
   295|         .. deprecated:: 4.0
   296|            Use ``request.connection`` and the `.HTTPConnection` methods
   297|            to write the response.
   298|         """
   299|         self.connection.finish()
   300|         self._finish_time = time.time()
   301|     def full_url(self):
   302|         """Reconstructs the full URL for this request."""
   303|         return self.protocol + "://" + self.host + self.uri
   304|     def request_time(self):
   305|         """Returns the amount of time it took for this request to execute."""
   306|         if self._finish_time is None:
   307|             return time.time() - self._start_time
   308|         else:
   309|             return self._finish_time - self._start_time
   310|     def get_ssl_certificate(self, binary_form=False):
   311|         """Returns the client's SSL certificate, if any.
   312|         To use client certificates, the HTTPServer's
   313|         `ssl.SSLContext.verify_mode` field must be set, e.g.::
   314|             ssl_ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)
   315|             ssl_ctx.load_cert_chain("foo.crt", "foo.key")
   316|             ssl_ctx.load_verify_locations("cacerts.pem")
   317|             ssl_ctx.verify_mode = ssl.CERT_REQUIRED
   318|             server = HTTPServer(app, ssl_options=ssl_ctx)
   319|         By default, the return value is a dictionary (or None, if no
   320|         client certificate is present).  If ``binary_form`` is true, a
   321|         DER-encoded form of the certificate is returned instead.  See
   322|         SSLSocket.getpeercert() in the standard library for more
   323|         details.
   324|         http://docs.python.org/library/ssl.html#sslsocket-objects
   325|         """
   326|         try:
   327|             return self.connection.stream.socket.getpeercert(binary_form=binary_form)
   328|         except SSLError:
   329|             return None
   330|     def _parse_body(self):
   331|         parse_body_arguments(
   332|             self.headers.get("Content-Type", ""),
   333|             self.body,
   334|             self.body_arguments,
   335|             self.files,
   336|             self.headers,
   337|         )
   338|         for k, v in self.body_arguments.items():
   339|             self.arguments.setdefault(k, []).extend(v)
   340|     def __repr__(self):
   341|         attrs = ("protocol", "host", "method", "uri", "version", "remote_ip")
   342|         args = ", ".join(["%s=%r" % (n, getattr(self, n)) for n in attrs])
   343|         return "%s(%s, headers=%s)" % (
   344|             self.__class__.__name__,
   345|             args,
   346|             dict(self.headers),
   347|         )
   348| class HTTPInputError(Exception):
   349|     """Exception class for malformed HTTP requests or responses
   350|     from remote sources.
   351|     .. versionadded:: 4.0
   352|     """
   353|     pass
   354| class HTTPOutputError(Exception):
   355|     """Exception class for errors in HTTP output.
   356|     .. versionadded:: 4.0
   357|     """
   358|     pass
   359| class HTTPServerConnectionDelegate(object):
   360|     """Implement this interface to handle requests from `.HTTPServer`.
   361|     .. versionadded:: 4.0
   362|     """
   363|     def start_request(self, server_conn, request_conn):
   364|         """This method is called by the server when a new request has started.
   365|         :arg server_conn: is an opaque object representing the long-lived
   366|             (e.g. tcp-level) connection.
   367|         :arg request_conn: is a `.HTTPConnection` object for a single
   368|             request/response exchange.
   369|         This method should return a `.HTTPMessageDelegate`.
   370|         """
   371|         raise NotImplementedError()
   372|     def on_close(self, server_conn):
   373|         """This method is called when a connection has been closed.
   374|         :arg server_conn: is a server connection that has previously been
   375|             passed to ``start_request``.
   376|         """
   377|         pass
   378| class HTTPMessageDelegate(object):
   379|     """Implement this interface to handle an HTTP request or response.
   380|     .. versionadded:: 4.0
   381|     """
   382|     def headers_received(self, start_line, headers):
   383|         """Called when the HTTP headers have been received and parsed.
   384|         :arg start_line: a `.RequestStartLine` or `.ResponseStartLine`
   385|             depending on whether this is a client or server message.
   386|         :arg headers: a `.HTTPHeaders` instance.
   387|         Some `.HTTPConnection` methods can only be called during
   388|         ``headers_received``.
   389|         May return a `.Future`; if it does the body will not be read
   390|         until it is done.
   391|         """
   392|         pass
   393|     def data_received(self, chunk):
   394|         """Called when a chunk of data has been received.
   395|         May return a `.Future` for flow control.
   396|         """
   397|         pass
   398|     def finish(self):
   399|         """Called after the last chunk of data has been received."""
   400|         pass
   401|     def on_connection_close(self):
   402|         """Called if the connection is closed without finishing the request.
   403|         If ``headers_received`` is called, either ``finish`` or
   404|         ``on_connection_close`` will be called, but not both.
   405|         """
   406|         pass
   407| class HTTPConnection(object):
   408|     """Applications use this interface to write their responses.
   409|     .. versionadded:: 4.0
   410|     """
   411|     def write_headers(self, start_line, headers, chunk=None, callback=None):
   412|         """Write an HTTP header block.
   413|         :arg start_line: a `.RequestStartLine` or `.ResponseStartLine`.
   414|         :arg headers: a `.HTTPHeaders` instance.
   415|         :arg chunk: the first (optional) chunk of data.  This is an optimization
   416|             so that small responses can be written in the same call as their
   417|             headers.
   418|         :arg callback: a callback to be run when the write is complete.
   419|         The ``version`` field of ``start_line`` is ignored.
   420|         Returns a `.Future` if no callback is given.
   421|         """
   422|         raise NotImplementedError()
   423|     def write(self, chunk, callback=None):
   424|         """Writes a chunk of body data.
   425|         The callback will be run when the write is complete.  If no callback
   426|         is given, returns a Future.
   427|         """
   428|         raise NotImplementedError()
   429|     def finish(self):
   430|         """Indicates that the last body data has been written.
   431|         """
   432|         raise NotImplementedError()
   433| def url_concat(url, args):
   434|     """Concatenate url and arguments regardless of whether
   435|     url has existing query parameters.
   436|     ``args`` may be either a dictionary or a list of key-value pairs
   437|     (the latter allows for multiple values with the same key.
   438|     >>> url_concat("http://example.com/foo", dict(c="d"))
   439|     'http://example.com/foo?c=d'
   440|     >>> url_concat("http://example.com/foo?a=b", dict(c="d"))
   441|     'http://example.com/foo?a=b&c=d'
   442|     >>> url_concat("http://example.com/foo?a=b", [("c", "d"), ("c", "d2")])
   443|     'http://example.com/foo?a=b&c=d&c=d2'
   444|     """
   445|     if args is None:
   446|         return url
   447|     parsed_url = urlparse(url)
   448|     if isinstance(args, dict):
   449|         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)
   450|         parsed_query.extend(args.items())
   451|     elif isinstance(args, list) or isinstance(args, tuple):
   452|         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)
   453|         parsed_query.extend(args)
   454|     else:
   455|         err = "'args' parameter should be dict, list or tuple. Not {0}".format(
   456|             type(args)
   457|         )
   458|         raise TypeError(err)
   459|     final_query = urlencode(parsed_query)
   460|     url = urlunparse(
   461|         (
   462|             parsed_url[0],
   463|             parsed_url[1],
   464|             parsed_url[2],
   465|             parsed_url[3],
   466|             final_query,
   467|             parsed_url[5],
   468|         )
   469|     )
   470|     return url
   471| class HTTPFile(ObjectDict):
   472|     """Represents a file uploaded via a form.
   473|     For backwards compatibility, its instance attributes are also
   474|     accessible as dictionary keys.
   475|     * ``filename``
   476|     * ``body``
   477|     * ``content_type``
   478|     """
   479|     pass
   480| def _parse_request_range(range_header):
   481|     """Parses a Range header.
   482|     Returns either ``None`` or tuple ``(start, end)``.
   483|     Note that while the HTTP headers use inclusive byte positions,
   484|     this method returns indexes suitable for use in slices.
   485|     >>> start, end = _parse_request_range("bytes=1-2")
   486|     >>> start, end
   487|     (1, 3)
   488|     >>> [0, 1, 2, 3, 4][start:end]
   489|     [1, 2]
   490|     >>> _parse_request_range("bytes=6-")
   491|     (6, None)
   492|     >>> _parse_request_range("bytes=-6")
   493|     (-6, None)
   494|     >>> _parse_request_range("bytes=-0")
   495|     (None, 0)
   496|     >>> _parse_request_range("bytes=")
   497|     (None, None)
   498|     >>> _parse_request_range("foo=42")
   499|     >>> _parse_request_range("bytes=1-2,6-10")
   500|     Note: only supports one range (ex, ``bytes=1-2,6-10`` is not allowed).
   501|     See [0] for the details of the range header.
   502|     [0]: http://greenbytes.de/tech/webdav/draft-ietf-httpbis-p5-range-latest.html#byte.ranges
   503|     """
   504|     unit, _, value = range_header.partition("=")
   505|     unit, value = unit.strip(), value.strip()
   506|     if unit != "bytes":
   507|         return None
   508|     start_b, _, end_b = value.partition("-")
   509|     try:
   510|         start = _int_or_none(start_b)
   511|         end = _int_or_none(end_b)
   512|     except ValueError:
   513|         return None
   514|     if end is not None:
   515|         if start is None:
   516|             if end != 0:
   517|                 start = -end
   518|                 end = None
   519|         else:
   520|             end += 1
   521|     return (start, end)
   522| def _get_content_range(start, end, total):
   523|     """Returns a suitable Content-Range header:
   524|     >>> print(_get_content_range(None, 1, 4))
   525|     bytes 0-0/4
   526|     >>> print(_get_content_range(1, 3, 4))
   527|     bytes 1-2/4
   528|     >>> print(_get_content_range(None, None, 4))
   529|     bytes 0-3/4
   530|     """
   531|     start = start or 0
   532|     end = (end or total) - 1
   533|     return "bytes %s-%s/%s" % (start, end, total)
   534| def _int_or_none(val):
   535|     val = val.strip()
   536|     if val == "":
   537|         return None
   538|     return int(val)
   539| def parse_body_arguments(content_type, body, arguments, files, headers=None):
   540|     """Parses a form request body.
   541|     Supports ``application/x-www-form-urlencoded`` and
   542|     ``multipart/form-data``.  The ``content_type`` parameter should be
   543|     a string and ``body`` should be a byte string.  The ``arguments``
   544|     and ``files`` parameters are dictionaries that will be updated
   545|     with the parsed contents.
   546|     """
   547|     if headers and "Content-Encoding" in headers:
   548|         gen_log.warning("Unsupported Content-Encoding: %s", headers["Content-Encoding"])
   549|         return
   550|     if content_type.startswith("application/x-www-form-urlencoded"):
   551|         try:
   552|             uri_arguments = parse_qs_bytes(native_str(body), keep_blank_values=True)
   553|         except Exception as e:
   554|             gen_log.warning("Invalid x-www-form-urlencoded body: %s", e)
   555|             uri_arguments = {}
   556|         for name, values in uri_arguments.items():
   557|             if values:
   558|                 arguments.setdefault(name, []).extend(values)
   559|     elif content_type.startswith("multipart/form-data"):
   560|         try:
   561|             fields = content_type.split(";")
   562|             for field in fields:
   563|                 k, sep, v = field.strip().partition("=")
   564|                 if k == "boundary" and v:
   565|                     parse_multipart_form_data(utf8(v), body, arguments, files)
   566|                     break
   567|             else:
   568|                 raise ValueError("multipart boundary not found")
   569|         except Exception as e:
   570|             gen_log.warning("Invalid multipart/form-data: %s", e)
   571| def parse_multipart_form_data(boundary, data, arguments, files):
   572|     """Parses a ``multipart/form-data`` body.
   573|     The ``boundary`` and ``data`` parameters are both byte strings.
   574|     The dictionaries given in the arguments and files parameters
   575|     will be updated with the contents of the body.
   576|     """
   577|     if boundary.startswith(b'"') and boundary.endswith(b'"'):
   578|         boundary = boundary[1:-1]
   579|     final_boundary_index = data.rfind(b"--" + boundary + b"--")
   580|     if final_boundary_index == -1:
   581|         gen_log.warning("Invalid multipart/form-data: no final boundary")
   582|         return
   583|     parts = data[:final_boundary_index].split(b"--" + boundary + b"\r\n")
   584|     for part in parts:
   585|         if not part:
   586|             continue
   587|         eoh = part.find(b"\r\n\r\n")
   588|         if eoh == -1:
   589|             gen_log.warning("multipart/form-data missing headers")
   590|             continue
   591|         headers = HTTPHeaders.parse(part[:eoh].decode("utf-8"))
   592|         disp_header = headers.get("Content-Disposition", "")
   593|         disposition, disp_params = _parse_header(disp_header)
   594|         if disposition != "form-data" or not part.endswith(b"\r\n"):
   595|             gen_log.warning("Invalid multipart/form-data")
   596|             continue
   597|         value = part[eoh + 4 : -2]
   598|         if not disp_params.get("name"):
   599|             gen_log.warning("multipart/form-data value missing name")
   600|             continue
   601|         name = disp_params["name"]
   602|         if disp_params.get("filename"):
   603|             ctype = headers.get("Content-Type", "application/unknown")
   604|             files.setdefault(name, []).append(
   605|                 HTTPFile(  # type: ignore
   606|                     filename=disp_params["filename"], body=value, content_type=ctype
   607|                 )
   608|             )
   609|         else:
   610|             arguments.setdefault(name, []).append(value)
   611| def format_timestamp(ts):
   612|     """Formats a timestamp in the format used by HTTP.
   613|     The argument may be a numeric timestamp as returned by `time.time`,
   614|     a time tuple as returned by `time.gmtime`, or a `datetime.datetime`
   615|     object.
   616|     >>> format_timestamp(1359312200)
   617|     'Sun, 27 Jan 2013 18:43:20 GMT'
   618|     """
   619|     if isinstance(ts, numbers.Real):
   620|         pass
   621|     elif isinstance(ts, (tuple, time.struct_time)):
   622|         ts = calendar.timegm(ts)
   623|     elif isinstance(ts, datetime.datetime):
   624|         ts = calendar.timegm(ts.utctimetuple())
   625|     else:
   626|         raise TypeError("unknown timestamp type: %r" % ts)
   627|     return email.utils.formatdate(ts, usegmt=True)
   628| RequestStartLine = collections.namedtuple(
   629|     "RequestStartLine", ["method", "path", "version"]
   630| )
   631| def parse_request_start_line(line):
   632|     """Returns a (method, path, version) tuple for an HTTP 1.x request line.
   633|     The response is a `collections.namedtuple`.
   634|     >>> parse_request_start_line("GET /foo HTTP/1.1")
   635|     RequestStartLine(method='GET', path='/foo', version='HTTP/1.1')
   636|     """
   637|     try:
   638|         method, path, version = line.split(" ")
   639|     except ValueError:
   640|         raise HTTPInputError("Malformed HTTP request line")
   641|     if not re.match(r"^HTTP/1\.[0-9]$", version):
   642|         raise HTTPInputError(
   643|             "Malformed HTTP version in HTTP Request-Line: %r" % version
   644|         )
   645|     return RequestStartLine(method, path, version)
   646| ResponseStartLine = collections.namedtuple(
   647|     "ResponseStartLine", ["version", "code", "reason"]
   648| )
   649| def parse_response_start_line(line):
   650|     """Returns a (version, code, reason) tuple for an HTTP 1.x response line.
   651|     The response is a `collections.namedtuple`.
   652|     >>> parse_response_start_line("HTTP/1.1 200 OK")
   653|     ResponseStartLine(version='HTTP/1.1', code=200, reason='OK')
   654|     """
   655|     line = native_str(line)
   656|     match = re.match("(HTTP/1.[0-9]) ([0-9]+) ([^\r]*)", line)
   657|     if not match:
   658|         raise HTTPInputError("Error parsing response start line")
   659|     return ResponseStartLine(match.group(1), int(match.group(2)), match.group(3))
   660| def _parseparam(s):
   661|     while s[:1] == ";":
   662|         s = s[1:]
   663|         end = s.find(";")
   664|         while end > 0 and (s.count('"', 0, end) - s.count('\\"', 0, end)) % 2:
   665|             end = s.find(";", end + 1)
   666|         if end < 0:
   667|             end = len(s)
   668|         f = s[:end]
   669|         yield f.strip()
   670|         s = s[end:]
   671| def _parse_header(line):
   672|     """Parse a Content-type like header.
   673|     Return the main content-type and a dictionary of options.
   674|     """
   675|     parts = _parseparam(";" + line)
   676|     key = next(parts)
   677|     pdict = {}
   678|     for p in parts:
   679|         i = p.find("=")
   680|         if i >= 0:
   681|             name = p[:i].strip().lower()
   682|             value = p[i + 1 :].strip()
   683|             if len(value) >= 2 and value[0] == value[-1] == '"':
   684|                 value = value[1:-1]
   685|                 value = value.replace("\\\\", "\\").replace('\\"', '"')
   686|             pdict[name] = value
   687|         else:
   688|             pdict[p] = None
   689|     return key, pdict
   690| def _encode_header(key, pdict):
   691|     """Inverse of _parse_header.
   692|     >>> _encode_header('permessage-deflate',
   693|     ...     {'client_max_window_bits': 15, 'client_no_context_takeover': None})
   694|     'permessage-deflate; client_max_window_bits=15; client_no_context_takeover'
   695|     """
   696|     if not pdict:
   697|         return key
   698|     out = [key]
   699|     for k, v in sorted(pdict.items()):
   700|         if v is None:
   701|             out.append(k)
   702|         else:
   703|             out.append("%s=%s" % (k, v))
   704|     return "; ".join(out)
   705| def doctests():
   706|     import doctest
   707|     return doctest.DocTestSuite()
   708| def split_host_and_port(netloc):
   709|     """Returns ``(host, port)`` tuple from ``netloc``.
   710|     Returned ``port`` will be ``None`` if not present.
   711|     .. versionadded:: 4.1
   712|     """
   713|     match = re.match(r"^(.+):(\d+)$", netloc)
   714|     if match:
   715|         host = match.group(1)
   716|         port = int(match.group(2))
   717|     else:
   718|         host = netloc
   719|         port = None
   720|     return (host, port)
   721| _OctalPatt = re.compile(r"\\[0-3][0-7][0-7]")
   722| _QuotePatt = re.compile(r"[\\].")
   723| _nulljoin = "".join
   724| def _unquote_cookie(str):
   725|     """Handle double quotes and escaping in cookie values.
   726|     This method is copied verbatim from the Python 3.5 standard
   727|     library (http.cookies._unquote) so we don't have to depend on
   728|     non-public interfaces.
   729|     """
   730|     if str is None or len(str) < 2:
   731|         return str
   732|     if str[0] != '"' or str[-1] != '"':
   733|         return str
   734|     str = str[1:-1]
   735|     i = 0
   736|     n = len(str)
   737|     res = []
   738|     while 0 <= i < n:
   739|         o_match = _OctalPatt.search(str, i)
   740|         q_match = _QuotePatt.search(str, i)
   741|         if not o_match and not q_match:  # Neither matched
   742|             res.append(str[i:])
   743|             break
   744|         j = k = -1
   745|         if o_match:
   746|             j = o_match.start(0)
   747|         if q_match:
   748|             k = q_match.start(0)
   749|         if q_match and (not o_match or k < j):  # QuotePatt matched
   750|             res.append(str[i:k])
   751|             res.append(str[k + 1])
   752|             i = k + 2
   753|         else:  # OctalPatt matched
   754|             res.append(str[i:j])
   755|             res.append(chr(int(str[j + 1 : j + 4], 8)))
   756|             i = j + 4
   757|     return _nulljoin(res)
   758| def parse_cookie(cookie):
   759|     """Parse a ``Cookie`` HTTP header into a dict of name/value pairs.
   760|     This function attempts to mimic browser cookie parsing behavior;
   761|     it specifically does not follow any of the cookie-related RFCs
   762|     (because browsers don't either).
   763|     The algorithm used is identical to that used by Django version 1.9.10.
   764|     .. versionadded:: 4.4.2
   765|     """
   766|     cookiedict = {}
   767|     for chunk in cookie.split(str(";")):
   768|         if str("=") in chunk:
   769|             key, val = chunk.split(str("="), 1)
   770|         else:
   771|             key, val = str(""), chunk
   772|         key, val = key.strip(), val.strip()
   773|         if key or val:
   774|             cookiedict[key] = _unquote_cookie(val)
   775|     return cookiedict


# ====================================================================
# FILE: salt/ext/tornado/ioloop.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-762 ---
     1| """An I/O event loop for non-blocking sockets.
     2| Typical applications will use a single `IOLoop` object, in the
     3| `IOLoop.instance` singleton.  The `IOLoop.start` method should usually
     4| be called at the end of the ``main()`` function.  Atypical applications may
     5| use more than one `IOLoop`, such as one `IOLoop` per thread, or per `unittest`
     6| case.
     7| In addition to I/O events, the `IOLoop` can also schedule time-based events.
     8| `IOLoop.add_timeout` is a non-blocking alternative to `time.sleep`.
     9| """
    10| from __future__ import absolute_import, division, print_function
    11| import collections
    12| import datetime
    13| import errno
    14| import functools
    15| import heapq
    16| import itertools
    17| import logging
    18| import numbers
    19| import os
    20| import select
    21| import sys
    22| import threading
    23| import time
    24| import traceback
    25| import math
    26| from salt.ext.tornado.concurrent import TracebackFuture, is_future
    27| from salt.ext.tornado.log import app_log, gen_log
    28| from salt.ext.tornado.platform.auto import set_close_exec, Waker
    29| from salt.ext.tornado import stack_context
    30| from salt.ext.tornado.util import PY3, Configurable, errno_from_exception, timedelta_to_seconds
    31| try:
    32|     import signal
    33| except ImportError:
    34|     signal = None
    35| if PY3:
    36|     import _thread as thread
    37| else:
    38|     import thread
    39| _POLL_TIMEOUT = 3600.0
    40| class TimeoutError(Exception):
    41|     pass
    42| class IOLoop(Configurable):
    43|     """A level-triggered I/O loop.
    44|     We use ``epoll`` (Linux) or ``kqueue`` (BSD and Mac OS X) if they
    45|     are available, or else we fall back on select(). If you are
    46|     implementing a system that needs to handle thousands of
    47|     simultaneous connections, you should use a system that supports
    48|     either ``epoll`` or ``kqueue``.
    49|     Example usage for a simple TCP server:
    50|     .. testcode::
    51|         import errno
    52|         import functools
    53|         import tornado.ioloop
    54|         import socket
    55|         def connection_ready(sock, fd, events):
    56|             while True:
    57|                 try:
    58|                     connection, address = sock.accept()
    59|                 except socket.error as e:
    60|                     if e.args[0] not in (errno.EWOULDBLOCK, errno.EAGAIN):
    61|                         raise
    62|                     return
    63|                 connection.setblocking(0)
    64|                 handle_connection(connection, address)
    65|         if __name__ == '__main__':
    66|             sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)
    67|             sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    68|             sock.setblocking(0)
    69|             sock.bind(("", port))
    70|             sock.listen(128)
    71|             io_loop = tornado.ioloop.IOLoop.current()
    72|             callback = functools.partial(connection_ready, sock)
    73|             io_loop.add_handler(sock.fileno(), callback, io_loop.READ)
    74|             io_loop.start()
    75|     .. testoutput::
    76|        :hide:
    77|     By default, a newly-constructed `IOLoop` becomes the thread's current
    78|     `IOLoop`, unless there already is a current `IOLoop`. This behavior
    79|     can be controlled with the ``make_current`` argument to the `IOLoop`
    80|     constructor: if ``make_current=True``, the new `IOLoop` will always
    81|     try to become current and it raises an error if there is already a
    82|     current instance. If ``make_current=False``, the new `IOLoop` will
    83|     not try to become current.
    84|     .. versionchanged:: 4.2
    85|        Added the ``make_current`` keyword argument to the `IOLoop`
    86|        constructor.
    87|     """
    88|     _EPOLLIN = 0x001
    89|     _EPOLLPRI = 0x002
    90|     _EPOLLOUT = 0x004
    91|     _EPOLLERR = 0x008
    92|     _EPOLLHUP = 0x010
    93|     _EPOLLRDHUP = 0x2000
    94|     _EPOLLONESHOT = (1 << 30)
    95|     _EPOLLET = (1 << 31)
    96|     NONE = 0
    97|     READ = _EPOLLIN
    98|     WRITE = _EPOLLOUT
    99|     ERROR = _EPOLLERR | _EPOLLHUP
   100|     _instance_lock = threading.Lock()
   101|     _current = threading.local()
   102|     @staticmethod
   103|     def instance():
   104|         """Returns a global `IOLoop` instance.
   105|         Most applications have a single, global `IOLoop` running on the
   106|         main thread.  Use this method to get this instance from
   107|         another thread.  In most other cases, it is better to use `current()`
   108|         to get the current thread's `IOLoop`.
   109|         """
   110|         if not hasattr(IOLoop, "_instance"):
   111|             with IOLoop._instance_lock:
   112|                 if not hasattr(IOLoop, "_instance"):
   113|                     IOLoop._instance = IOLoop()
   114|         return IOLoop._instance
   115|     @staticmethod
   116|     def initialized():
   117|         """Returns true if the singleton instance has been created."""
   118|         return hasattr(IOLoop, "_instance")
   119|     def install(self):
   120|         """Installs this `IOLoop` object as the singleton instance.
   121|         This is normally not necessary as `instance()` will create
   122|         an `IOLoop` on demand, but you may want to call `install` to use
   123|         a custom subclass of `IOLoop`.
   124|         When using an `IOLoop` subclass, `install` must be called prior
   125|         to creating any objects that implicitly create their own
   126|         `IOLoop` (e.g., :class:`tornado.httpclient.AsyncHTTPClient`).
   127|         """
   128|         assert not IOLoop.initialized()
   129|         IOLoop._instance = self
   130|     @staticmethod
   131|     def clear_instance():
   132|         """Clear the global `IOLoop` instance.
   133|         .. versionadded:: 4.0
   134|         """
   135|         if hasattr(IOLoop, "_instance"):
   136|             del IOLoop._instance
   137|     @staticmethod
   138|     def current(instance=True):
   139|         """Returns the current thread's `IOLoop`.
   140|         If an `IOLoop` is currently running or has been marked as
   141|         current by `make_current`, returns that instance.  If there is
   142|         no current `IOLoop`, returns `IOLoop.instance()` (i.e. the
   143|         main thread's `IOLoop`, creating one if necessary) if ``instance``
   144|         is true.
   145|         In general you should use `IOLoop.current` as the default when
   146|         constructing an asynchronous object, and use `IOLoop.instance`
   147|         when you mean to communicate to the main thread from a different
   148|         one.
   149|         .. versionchanged:: 4.1
   150|            Added ``instance`` argument to control the fallback to
   151|            `IOLoop.instance()`.
   152|         """
   153|         current = getattr(IOLoop._current, "instance", None)
   154|         if current is None and instance:
   155|             return IOLoop.instance()
   156|         return current
   157|     def make_current(self):
   158|         """Makes this the `IOLoop` for the current thread.
   159|         An `IOLoop` automatically becomes current for its thread
   160|         when it is started, but it is sometimes useful to call
   161|         `make_current` explicitly before starting the `IOLoop`,
   162|         so that code run at startup time can find the right
   163|         instance.
   164|         .. versionchanged:: 4.1
   165|            An `IOLoop` created while there is no current `IOLoop`
   166|            will automatically become current.
   167|         """
   168|         IOLoop._current.instance = self
   169|     @staticmethod
   170|     def clear_current():
   171|         IOLoop._current.instance = None
   172|     @classmethod
   173|     def configurable_base(cls):
   174|         return IOLoop
   175|     @classmethod
   176|     def configurable_default(cls):
   177|         if hasattr(select, "epoll"):
   178|             from salt.ext.tornado.platform.epoll import EPollIOLoop
   179|             return EPollIOLoop
   180|         if hasattr(select, "kqueue"):
   181|             from salt.ext.tornado.platform.kqueue import KQueueIOLoop
   182|             return KQueueIOLoop
   183|         from salt.ext.tornado.platform.select import SelectIOLoop
   184|         return SelectIOLoop
   185|     def initialize(self, make_current=None):
   186|         if make_current is None:
   187|             if IOLoop.current(instance=False) is None:
   188|                 self.make_current()
   189|         elif make_current:
   190|             if IOLoop.current(instance=False) is not None:
   191|                 raise RuntimeError("current IOLoop already exists")
   192|             self.make_current()
   193|     def close(self, all_fds=False):
   194|         """Closes the `IOLoop`, freeing any resources used.
   195|         If ``all_fds`` is true, all file descriptors registered on the
   196|         IOLoop will be closed (not just the ones created by the
   197|         `IOLoop` itself).
   198|         Many applications will only use a single `IOLoop` that runs for the
   199|         entire lifetime of the process.  In that case closing the `IOLoop`
   200|         is not necessary since everything will be cleaned up when the
   201|         process exits.  `IOLoop.close` is provided mainly for scenarios
   202|         such as unit tests, which create and destroy a large number of
   203|         ``IOLoops``.
   204|         An `IOLoop` must be completely stopped before it can be closed.  This
   205|         means that `IOLoop.stop()` must be called *and* `IOLoop.start()` must
   206|         be allowed to return before attempting to call `IOLoop.close()`.
   207|         Therefore the call to `close` will usually appear just after
   208|         the call to `start` rather than near the call to `stop`.
   209|         .. versionchanged:: 3.1
   210|            If the `IOLoop` implementation supports non-integer objects
   211|            for "file descriptors", those objects will have their
   212|            ``close`` method when ``all_fds`` is true.
   213|         """
   214|         raise NotImplementedError()
   215|     def add_handler(self, fd, handler, events):
   216|         """Registers the given handler to receive the given events for ``fd``.
   217|         The ``fd`` argument may either be an integer file descriptor or
   218|         a file-like object with a ``fileno()`` method (and optionally a
   219|         ``close()`` method, which may be called when the `IOLoop` is shut
   220|         down).
   221|         The ``events`` argument is a bitwise or of the constants
   222|         ``IOLoop.READ``, ``IOLoop.WRITE``, and ``IOLoop.ERROR``.
   223|         When an event occurs, ``handler(fd, events)`` will be run.
   224|         .. versionchanged:: 4.0
   225|            Added the ability to pass file-like objects in addition to
   226|            raw file descriptors.
   227|         """
   228|         raise NotImplementedError()
   229|     def update_handler(self, fd, events):
   230|         """Changes the events we listen for ``fd``.
   231|         .. versionchanged:: 4.0
   232|            Added the ability to pass file-like objects in addition to
   233|            raw file descriptors.
   234|         """
   235|         raise NotImplementedError()
   236|     def remove_handler(self, fd):
   237|         """Stop listening for events on ``fd``.
   238|         .. versionchanged:: 4.0
   239|            Added the ability to pass file-like objects in addition to
   240|            raw file descriptors.
   241|         """
   242|         raise NotImplementedError()
   243|     def set_blocking_signal_threshold(self, seconds, action):
   244|         """Sends a signal if the `IOLoop` is blocked for more than
   245|         ``s`` seconds.
   246|         Pass ``seconds=None`` to disable.  Requires Python 2.6 on a unixy
   247|         platform.
   248|         The action parameter is a Python signal handler.  Read the
   249|         documentation for the `signal` module for more information.
   250|         If ``action`` is None, the process will be killed if it is
   251|         blocked for too long.
   252|         """
   253|         raise NotImplementedError()
   254|     def set_blocking_log_threshold(self, seconds):
   255|         """Logs a stack trace if the `IOLoop` is blocked for more than
   256|         ``s`` seconds.
   257|         Equivalent to ``set_blocking_signal_threshold(seconds,
   258|         self.log_stack)``
   259|         """
   260|         self.set_blocking_signal_threshold(seconds, self.log_stack)
   261|     def log_stack(self, signal, frame):
   262|         """Signal handler to log the stack trace of the current thread.
   263|         For use with `set_blocking_signal_threshold`.
   264|         """
   265|         gen_log.warning('IOLoop blocked for %f seconds in\n%s',
   266|                         self._blocking_signal_threshold,
   267|                         ''.join(traceback.format_stack(frame)))
   268|     def start(self):
   269|         """Starts the I/O loop.
   270|         The loop will run until one of the callbacks calls `stop()`, which
   271|         will make the loop stop after the current event iteration completes.
   272|         """
   273|         raise NotImplementedError()
   274|     def _setup_logging(self):
   275|         """The IOLoop catches and logs exceptions, so it's
   276|         important that log output be visible.  However, python's
   277|         default behavior for non-root loggers (prior to python
   278|         3.2) is to print an unhelpful "no handlers could be
   279|         found" message rather than the actual log entry, so we
   280|         must explicitly configure logging if we've made it this
   281|         far without anything.
   282|         This method should be called from start() in subclasses.
   283|         """
   284|         if not any([logging.getLogger().handlers,
   285|                     logging.getLogger('tornado').handlers,
   286|                     logging.getLogger('tornado.application').handlers]):
   287|             logging.basicConfig()
   288|     def stop(self):
   289|         """Stop the I/O loop.
   290|         If the event loop is not currently running, the next call to `start()`
   291|         will return immediately.
   292|         To use asynchronous methods from otherwise-synchronous code (such as
   293|         unit tests), you can start and stop the event loop like this::
   294|           ioloop = IOLoop()
   295|           async_method(ioloop=ioloop, callback=ioloop.stop)
   296|           ioloop.start()
   297|         ``ioloop.start()`` will return after ``async_method`` has run
   298|         its callback, whether that callback was invoked before or
   299|         after ``ioloop.start``.
   300|         Note that even after `stop` has been called, the `IOLoop` is not
   301|         completely stopped until `IOLoop.start` has also returned.
   302|         Some work that was scheduled before the call to `stop` may still
   303|         be run before the `IOLoop` shuts down.
   304|         """
   305|         raise NotImplementedError()
   306|     def run_sync(self, func, timeout=None):
   307|         """Starts the `IOLoop`, runs the given function, and stops the loop.
   308|         The function must return either a yieldable object or
   309|         ``None``. If the function returns a yieldable object, the
   310|         `IOLoop` will run until the yieldable is resolved (and
   311|         `run_sync()` will return the yieldable's result). If it raises
   312|         an exception, the `IOLoop` will stop and the exception will be
   313|         re-raised to the caller.
   314|         The keyword-only argument ``timeout`` may be used to set
   315|         a maximum duration for the function.  If the timeout expires,
   316|         a `TimeoutError` is raised.
   317|         This method is useful in conjunction with `tornado.gen.coroutine`
   318|         to allow asynchronous calls in a ``main()`` function::
   319|             @gen.coroutine
   320|             def main():
   321|             if __name__ == '__main__':
   322|                 IOLoop.current().run_sync(main)
   323|         .. versionchanged:: 4.3
   324|            Returning a non-``None``, non-yieldable value is now an error.
   325|         """
   326|         future_cell = [None]
   327|         def run():
   328|             try:
   329|                 result = func()
   330|                 if result is not None:
   331|                     from salt.ext.tornado.gen import convert_yielded
   332|                     result = convert_yielded(result)
   333|             except Exception:
   334|                 future_cell[0] = TracebackFuture()
   335|                 future_cell[0].set_exc_info(sys.exc_info())
   336|             else:
   337|                 if is_future(result):
   338|                     future_cell[0] = result
   339|                 else:
   340|                     future_cell[0] = TracebackFuture()
   341|                     future_cell[0].set_result(result)
   342|             self.add_future(future_cell[0], lambda future: self.stop())
   343|         self.add_callback(run)
   344|         if timeout is not None:
   345|             timeout_handle = self.add_timeout(self.time() + timeout, self.stop)
   346|         self.start()
   347|         if timeout is not None:
   348|             self.remove_timeout(timeout_handle)
   349|         if not future_cell[0].done():
   350|             raise TimeoutError('Operation timed out after %s seconds' % timeout)
   351|         return future_cell[0].result()
   352|     def time(self):
   353|         """Returns the current time according to the `IOLoop`'s clock.
   354|         The return value is a floating-point number relative to an
   355|         unspecified time in the past.
   356|         By default, the `IOLoop`'s time function is `time.time`.  However,
   357|         it may be configured to use e.g. `time.monotonic` instead.
   358|         Calls to `add_timeout` that pass a number instead of a
   359|         `datetime.timedelta` should use this function to compute the
   360|         appropriate time, so they can work no matter what time function
   361|         is chosen.
   362|         """
   363|         return time.time()
   364|     def add_timeout(self, deadline, callback, *args, **kwargs):
   365|         """Runs the ``callback`` at the time ``deadline`` from the I/O loop.
   366|         Returns an opaque handle that may be passed to
   367|         `remove_timeout` to cancel.
   368|         ``deadline`` may be a number denoting a time (on the same
   369|         scale as `IOLoop.time`, normally `time.time`), or a
   370|         `datetime.timedelta` object for a deadline relative to the
   371|         current time.  Since Tornado 4.0, `call_later` is a more
   372|         convenient alternative for the relative case since it does not
   373|         require a timedelta object.
   374|         Note that it is not safe to call `add_timeout` from other threads.
   375|         Instead, you must use `add_callback` to transfer control to the
   376|         `IOLoop`'s thread, and then call `add_timeout` from there.
   377|         Subclasses of IOLoop must implement either `add_timeout` or
   378|         `call_at`; the default implementations of each will call
   379|         the other.  `call_at` is usually easier to implement, but
   380|         subclasses that wish to maintain compatibility with Tornado
   381|         versions prior to 4.0 must use `add_timeout` instead.
   382|         .. versionchanged:: 4.0
   383|            Now passes through ``*args`` and ``**kwargs`` to the callback.
   384|         """
   385|         if isinstance(deadline, numbers.Real):
   386|             return self.call_at(deadline, callback, *args, **kwargs)
   387|         elif isinstance(deadline, datetime.timedelta):
   388|             return self.call_at(self.time() + timedelta_to_seconds(deadline),
   389|                                 callback, *args, **kwargs)
   390|         else:
   391|             raise TypeError("Unsupported deadline %r" % deadline)
   392|     def call_later(self, delay, callback, *args, **kwargs):
   393|         """Runs the ``callback`` after ``delay`` seconds have passed.
   394|         Returns an opaque handle that may be passed to `remove_timeout`
   395|         to cancel.  Note that unlike the `asyncio` method of the same
   396|         name, the returned object does not have a ``cancel()`` method.
   397|         See `add_timeout` for comments on thread-safety and subclassing.
   398|         .. versionadded:: 4.0
   399|         """
   400|         return self.call_at(self.time() + delay, callback, *args, **kwargs)
   401|     def call_at(self, when, callback, *args, **kwargs):
   402|         """Runs the ``callback`` at the absolute time designated by ``when``.
   403|         ``when`` must be a number using the same reference point as
   404|         `IOLoop.time`.
   405|         Returns an opaque handle that may be passed to `remove_timeout`
   406|         to cancel.  Note that unlike the `asyncio` method of the same
   407|         name, the returned object does not have a ``cancel()`` method.
   408|         See `add_timeout` for comments on thread-safety and subclassing.
   409|         .. versionadded:: 4.0
   410|         """
   411|         return self.add_timeout(when, callback, *args, **kwargs)
   412|     def remove_timeout(self, timeout):
   413|         """Cancels a pending timeout.
   414|         The argument is a handle as returned by `add_timeout`.  It is
   415|         safe to call `remove_timeout` even if the callback has already
   416|         been run.
   417|         """
   418|         raise NotImplementedError()
   419|     def add_callback(self, callback, *args, **kwargs):
   420|         """Calls the given callback on the next I/O loop iteration.
   421|         It is safe to call this method from any thread at any time,
   422|         except from a signal handler.  Note that this is the **only**
   423|         method in `IOLoop` that makes this thread-safety guarantee; all
   424|         other interaction with the `IOLoop` must be done from that
   425|         `IOLoop`'s thread.  `add_callback()` may be used to transfer
   426|         control from other threads to the `IOLoop`'s thread.
   427|         To add a callback from a signal handler, see
   428|         `add_callback_from_signal`.
   429|         """
   430|         raise NotImplementedError()
   431|     def add_callback_from_signal(self, callback, *args, **kwargs):
   432|         """Calls the given callback on the next I/O loop iteration.
   433|         Safe for use from a Python signal handler; should not be used
   434|         otherwise.
   435|         Callbacks added with this method will be run without any
   436|         `.stack_context`, to avoid picking up the context of the function
   437|         that was interrupted by the signal.
   438|         """
   439|         raise NotImplementedError()
   440|     def spawn_callback(self, callback, *args, **kwargs):
   441|         """Calls the given callback on the next IOLoop iteration.
   442|         Unlike all other callback-related methods on IOLoop,
   443|         ``spawn_callback`` does not associate the callback with its caller's
   444|         ``stack_context``, so it is suitable for fire-and-forget callbacks
   445|         that should not interfere with the caller.
   446|         .. versionadded:: 4.0
   447|         """
   448|         with stack_context.NullContext():
   449|             self.add_callback(callback, *args, **kwargs)
   450|     def add_future(self, future, callback):
   451|         """Schedules a callback on the ``IOLoop`` when the given
   452|         `.Future` is finished.
   453|         The callback is invoked with one argument, the
   454|         `.Future`.
   455|         """
   456|         assert is_future(future)
   457|         callback = stack_context.wrap(callback)
   458|         future.add_done_callback(
   459|             lambda future: self.add_callback(callback, future))
   460|     def _run_callback(self, callback):
   461|         """Runs a callback with error handling.
   462|         For use in subclasses.
   463|         """
   464|         try:
   465|             ret = callback()
   466|             if ret is not None:
   467|                 import salt.ext.tornado.gen
   468|                 try:
   469|                     ret = salt.ext.tornado.gen.convert_yielded(ret)
   470|                 except salt.ext.tornado.gen.BadYieldError:
   471|                     pass
   472|                 else:
   473|                     self.add_future(ret, self._discard_future_result)
   474|         except Exception:
   475|             self.handle_callback_exception(callback)
   476|     def _discard_future_result(self, future):
   477|         """Avoid unhandled-exception warnings from spawned coroutines."""
   478|         future.result()
   479|     def handle_callback_exception(self, callback):
   480|         """This method is called whenever a callback run by the `IOLoop`
   481|         throws an exception.
   482|         By default simply logs the exception as an error.  Subclasses
   483|         may override this method to customize reporting of exceptions.
   484|         The exception itself is not passed explicitly, but is available
   485|         in `sys.exc_info`.
   486|         """
   487|         app_log.error("Exception in callback %r", callback, exc_info=True)
   488|     def split_fd(self, fd):
   489|         """Returns an (fd, obj) pair from an ``fd`` parameter.
   490|         We accept both raw file descriptors and file-like objects as
   491|         input to `add_handler` and related methods.  When a file-like
   492|         object is passed, we must retain the object itself so we can
   493|         close it correctly when the `IOLoop` shuts down, but the
   494|         poller interfaces favor file descriptors (they will accept
   495|         file-like objects and call ``fileno()`` for you, but they
   496|         always return the descriptor itself).
   497|         This method is provided for use by `IOLoop` subclasses and should
   498|         not generally be used by application code.
   499|         .. versionadded:: 4.0
   500|         """
   501|         try:
   502|             return fd.fileno(), fd
   503|         except AttributeError:
   504|             return fd, fd
   505|     def close_fd(self, fd):
   506|         """Utility method to close an ``fd``.
   507|         If ``fd`` is a file-like object, we close it directly; otherwise
   508|         we use `os.close`.
   509|         This method is provided for use by `IOLoop` subclasses (in
   510|         implementations of ``IOLoop.close(all_fds=True)`` and should
   511|         not generally be used by application code.
   512|         .. versionadded:: 4.0
   513|         """
   514|         try:
   515|             try:
   516|                 fd.close()
   517|             except AttributeError:
   518|                 os.close(fd)
   519|         except OSError:
   520|             pass
   521| class PollIOLoop(IOLoop):
   522|     """Base class for IOLoops built around a select-like function.
   523|     For concrete implementations, see `tornado.platform.epoll.EPollIOLoop`
   524|     (Linux), `tornado.platform.kqueue.KQueueIOLoop` (BSD and Mac), or
   525|     `tornado.platform.select.SelectIOLoop` (all platforms).
   526|     """
   527|     def initialize(self, impl, time_func=None, **kwargs):
   528|         super(PollIOLoop, self).initialize(**kwargs)
   529|         self._impl = impl
   530|         if hasattr(self._impl, 'fileno'):
   531|             set_close_exec(self._impl.fileno())
   532|         self.time_func = time_func or time.time
   533|         self._handlers = {}
   534|         self._events = {}
   535|         self._callbacks = collections.deque()
   536|         self._timeouts = []
   537|         self._cancellations = 0
   538|         self._running = False
   539|         self._stopped = False
   540|         self._closing = False
   541|         self._thread_ident = None
   542|         self._blocking_signal_threshold = None
   543|         self._timeout_counter = itertools.count()
   544|         self._waker = Waker()
   545|         self.add_handler(self._waker.fileno(),
   546|                          lambda fd, events: self._waker.consume(),
   547|                          self.READ)
   548|     def close(self, all_fds=False):
   549|         self._closing = True
   550|         self.remove_handler(self._waker.fileno())
   551|         if all_fds:
   552|             for fd, handler in list(self._handlers.values()):
   553|                 self.close_fd(fd)
   554|         self._waker.close()
   555|         self._impl.close()
   556|         self._callbacks = None
   557|         self._timeouts = None
   558|     def add_handler(self, fd, handler, events):
   559|         fd, obj = self.split_fd(fd)
   560|         self._handlers[fd] = (obj, stack_context.wrap(handler))
   561|         self._impl.register(fd, events | self.ERROR)
   562|     def update_handler(self, fd, events):
   563|         fd, obj = self.split_fd(fd)
   564|         self._impl.modify(fd, events | self.ERROR)
   565|     def remove_handler(self, fd):
   566|         fd, obj = self.split_fd(fd)
   567|         self._handlers.pop(fd, None)
   568|         self._events.pop(fd, None)
   569|         try:
   570|             self._impl.unregister(fd)
   571|         except Exception:
   572|             gen_log.debug("Error deleting fd from IOLoop", exc_info=True)
   573|     def set_blocking_signal_threshold(self, seconds, action):
   574|         if not hasattr(signal, "setitimer"):
   575|             gen_log.error("set_blocking_signal_threshold requires a signal module "
   576|                           "with the setitimer method")
   577|             return
   578|         self._blocking_signal_threshold = seconds
   579|         if seconds is not None:
   580|             signal.signal(signal.SIGALRM,
   581|                           action if action is not None else signal.SIG_DFL)
   582|     def start(self):
   583|         if self._running:
   584|             raise RuntimeError("IOLoop is already running")
   585|         self._setup_logging()
   586|         if self._stopped:
   587|             self._stopped = False
   588|             return
   589|         old_current = getattr(IOLoop._current, "instance", None)
   590|         IOLoop._current.instance = self
   591|         self._thread_ident = thread.get_ident()
   592|         self._running = True
   593|         old_wakeup_fd = None
   594|         if hasattr(signal, 'set_wakeup_fd') and os.name == 'posix':
   595|             try:
   596|                 old_wakeup_fd = signal.set_wakeup_fd(self._waker.write_fileno())
   597|                 if old_wakeup_fd != -1:
   598|                     signal.set_wakeup_fd(old_wakeup_fd)
   599|                     old_wakeup_fd = None
   600|             except ValueError:
   601|                 old_wakeup_fd = None
   602|         try:
   603|             while True:
   604|                 ncallbacks = len(self._callbacks)
   605|                 due_timeouts = []
   606|                 if self._timeouts:
   607|                     now = self.time()
   608|                     while self._timeouts:
   609|                         if self._timeouts[0].callback is None:
   610|                             heapq.heappop(self._timeouts)
   611|                             self._cancellations -= 1
   612|                         elif self._timeouts[0].deadline <= now:
   613|                             due_timeouts.append(heapq.heappop(self._timeouts))
   614|                         else:
   615|                             break
   616|                     if (self._cancellations > 512 and
   617|                             self._cancellations > (len(self._timeouts) >> 1)):
   618|                         self._cancellations = 0
   619|                         self._timeouts = [x for x in self._timeouts
   620|                                           if x.callback is not None]
   621|                         heapq.heapify(self._timeouts)
   622|                 for i in range(ncallbacks):
   623|                     self._run_callback(self._callbacks.popleft())
   624|                 for timeout in due_timeouts:
   625|                     if timeout.callback is not None:
   626|                         self._run_callback(timeout.callback)
   627|                 due_timeouts = timeout = None
   628|                 if self._callbacks:
   629|                     poll_timeout = 0.0
   630|                 elif self._timeouts:
   631|                     poll_timeout = self._timeouts[0].deadline - self.time()
   632|                     poll_timeout = max(0, min(poll_timeout, _POLL_TIMEOUT))
   633|                 else:
   634|                     poll_timeout = _POLL_TIMEOUT
   635|                 if not self._running:
   636|                     break
   637|                 if self._blocking_signal_threshold is not None:
   638|                     signal.setitimer(signal.ITIMER_REAL, 0, 0)
   639|                 try:
   640|                     event_pairs = self._impl.poll(poll_timeout)
   641|                 except Exception as e:
   642|                     if errno_from_exception(e) == errno.EINTR:
   643|                         continue
   644|                     else:
   645|                         raise
   646|                 if self._blocking_signal_threshold is not None:
   647|                     signal.setitimer(signal.ITIMER_REAL,
   648|                                      self._blocking_signal_threshold, 0)
   649|                 self._events.update(event_pairs)
   650|                 while self._events:
   651|                     fd, events = self._events.popitem()
   652|                     try:
   653|                         fd_obj, handler_func = self._handlers[fd]
   654|                         handler_func(fd_obj, events)
   655|                     except (OSError, IOError) as e:
   656|                         if errno_from_exception(e) == errno.EPIPE:
   657|                             pass
   658|                         else:
   659|                             self.handle_callback_exception(self._handlers.get(fd))
   660|                     except Exception:
   661|                         self.handle_callback_exception(self._handlers.get(fd))
   662|                 fd_obj = handler_func = None
   663|         finally:
   664|             self._stopped = False
   665|             if self._blocking_signal_threshold is not None:
   666|                 signal.setitimer(signal.ITIMER_REAL, 0, 0)
   667|             IOLoop._current.instance = old_current
   668|             if old_wakeup_fd is not None:
   669|                 signal.set_wakeup_fd(old_wakeup_fd)
   670|     def stop(self):
   671|         self._running = False
   672|         self._stopped = True
   673|         self._waker.wake()
   674|     def time(self):
   675|         return self.time_func()
   676|     def call_at(self, deadline, callback, *args, **kwargs):
   677|         timeout = _Timeout(
   678|             deadline,
   679|             functools.partial(stack_context.wrap(callback), *args, **kwargs),
   680|             self)
   681|         heapq.heappush(self._timeouts, timeout)
   682|         return timeout
   683|     def remove_timeout(self, timeout):
   684|         timeout.callback = None
   685|         self._cancellations += 1
   686|     def add_callback(self, callback, *args, **kwargs):
   687|         if self._closing:
   688|             return
   689|         self._callbacks.append(functools.partial(
   690|             stack_context.wrap(callback), *args, **kwargs))
   691|         if thread.get_ident() != self._thread_ident:
   692|             self._waker.wake()
   693|         else:
   694|             pass
   695|     def add_callback_from_signal(self, callback, *args, **kwargs):
   696|         with stack_context.NullContext():
   697|             self.add_callback(callback, *args, **kwargs)
   698| class _Timeout(object):
   699|     """An IOLoop timeout, a UNIX timestamp and a callback"""
   700|     __slots__ = ['deadline', 'callback', 'tdeadline']
   701|     def __init__(self, deadline, callback, io_loop):
   702|         if not isinstance(deadline, numbers.Real):
   703|             raise TypeError("Unsupported deadline %r" % deadline)
   704|         self.deadline = deadline
   705|         self.callback = callback
   706|         self.tdeadline = (deadline, next(io_loop._timeout_counter))
   707|     def __lt__(self, other):
   708|         return self.tdeadline < other.tdeadline
   709|     def __le__(self, other):
   710|         return self.tdeadline <= other.tdeadline
   711| class PeriodicCallback(object):
   712|     """Schedules the given callback to be called periodically.
   713|     The callback is called every ``callback_time`` milliseconds.
   714|     Note that the timeout is given in milliseconds, while most other
   715|     time-related functions in Tornado use seconds.
   716|     If the callback runs for longer than ``callback_time`` milliseconds,
   717|     subsequent invocations will be skipped to get back on schedule.
   718|     `start` must be called after the `PeriodicCallback` is created.
   719|     .. versionchanged:: 4.1
   720|        The ``io_loop`` argument is deprecated.
   721|     """
   722|     def __init__(self, callback, callback_time, io_loop=None):
   723|         self.callback = callback
   724|         if callback_time <= 0:
   725|             raise ValueError("Periodic callback must have a positive callback_time")
   726|         self.callback_time = callback_time
   727|         self.io_loop = io_loop or IOLoop.current()
   728|         self._running = False
   729|         self._timeout = None
   730|     def start(self):
   731|         """Starts the timer."""
   732|         self._running = True
   733|         self._next_timeout = self.io_loop.time()
   734|         self._schedule_next()
   735|     def stop(self):
   736|         """Stops the timer."""
   737|         self._running = False
   738|         if self._timeout is not None:
   739|             self.io_loop.remove_timeout(self._timeout)
   740|             self._timeout = None
   741|     def is_running(self):
   742|         """Return True if this `.PeriodicCallback` has been started.
   743|         .. versionadded:: 4.1
   744|         """
   745|         return self._running
   746|     def _run(self):
   747|         if not self._running:
   748|             return
   749|         try:
   750|             return self.callback()
   751|         except Exception:
   752|             self.io_loop.handle_callback_exception(self.callback)
   753|         finally:
   754|             self._schedule_next()
   755|     def _schedule_next(self):
   756|         if self._running:
   757|             current_time = self.io_loop.time()
   758|             if self._next_timeout <= current_time:
   759|                 callback_time_sec = self.callback_time / 1000.0
   760|                 self._next_timeout += (math.floor((current_time - self._next_timeout) /
   761|                                                   callback_time_sec) + 1) * callback_time_sec
   762|             self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)


# ====================================================================
# FILE: salt/ext/tornado/iostream.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-1156 ---
     1| """Utility classes to write to and read from non-blocking files and sockets.
     2| Contents:
     3| * `BaseIOStream`: Generic interface for reading and writing.
     4| * `IOStream`: Implementation of BaseIOStream using non-blocking sockets.
     5| * `SSLIOStream`: SSL-aware version of IOStream.
     6| * `PipeIOStream`: Pipe-based IOStream implementation.
     7| """
     8| from __future__ import absolute_import, division, print_function
     9| import collections
    10| import errno
    11| import numbers
    12| import os
    13| import socket
    14| import sys
    15| import re
    16| from salt.ext.tornado.concurrent import TracebackFuture
    17| from salt.ext.tornado import ioloop
    18| from salt.ext.tornado.log import gen_log, app_log
    19| from salt.ext.tornado.netutil import ssl_wrap_socket, ssl_match_hostname, SSLCertificateError, _client_ssl_defaults, _server_ssl_defaults
    20| from salt.ext.tornado import stack_context
    21| from salt.ext.tornado.util import errno_from_exception
    22| try:
    23|     from salt.ext.tornado.platform.posix import _set_nonblocking
    24| except ImportError:
    25|     _set_nonblocking = None
    26| try:
    27|     import ssl
    28| except ImportError:
    29|     ssl = None
    30| _ERRNO_WOULDBLOCK = (errno.EWOULDBLOCK, errno.EAGAIN)
    31| if hasattr(errno, "WSAEWOULDBLOCK"):
    32|     _ERRNO_WOULDBLOCK += (errno.WSAEWOULDBLOCK,)  # type: ignore
    33| _ERRNO_CONNRESET = (errno.ECONNRESET, errno.ECONNABORTED, errno.EPIPE,
    34|                     errno.ETIMEDOUT)
    35| if hasattr(errno, "WSAECONNRESET"):
    36|     _ERRNO_CONNRESET += (errno.WSAECONNRESET, errno.WSAECONNABORTED, errno.WSAETIMEDOUT)  # type: ignore
    37| if sys.platform == 'darwin':
    38|     _ERRNO_CONNRESET += (errno.EPROTOTYPE,)  # type: ignore
    39| _ERRNO_INPROGRESS = (errno.EINPROGRESS,)
    40| if hasattr(errno, "WSAEINPROGRESS"):
    41|     _ERRNO_INPROGRESS += (errno.WSAEINPROGRESS,)  # type: ignore
    42| _WINDOWS = sys.platform.startswith('win')
    43| class StreamClosedError(IOError):
    44|     """Exception raised by `IOStream` methods when the stream is closed.
    45|     Note that the close callback is scheduled to run *after* other
    46|     callbacks on the stream (to allow for buffered data to be processed),
    47|     so you may see this error before you see the close callback.
    48|     The ``real_error`` attribute contains the underlying error that caused
    49|     the stream to close (if any).
    50|     .. versionchanged:: 4.3
    51|        Added the ``real_error`` attribute.
    52|     """
    53|     def __init__(self, real_error=None):
    54|         super(StreamClosedError, self).__init__('Stream is closed')
    55|         self.real_error = real_error
    56| class UnsatisfiableReadError(Exception):
    57|     """Exception raised when a read cannot be satisfied.
    58|     Raised by ``read_until`` and ``read_until_regex`` with a ``max_bytes``
    59|     argument.
    60|     """
    61|     pass
    62| class StreamBufferFullError(Exception):
    63|     """Exception raised by `IOStream` methods when the buffer is full.
    64|     """
    65| class BaseIOStream(object):
    66|     """A utility class to write to and read from a non-blocking file or socket.
    67|     We support a non-blocking ``write()`` and a family of ``read_*()`` methods.
    68|     All of the methods take an optional ``callback`` argument and return a
    69|     `.Future` only if no callback is given.  When the operation completes,
    70|     the callback will be run or the `.Future` will resolve with the data
    71|     read (or ``None`` for ``write()``).  All outstanding ``Futures`` will
    72|     resolve with a `StreamClosedError` when the stream is closed; users
    73|     of the callback interface will be notified via
    74|     `.BaseIOStream.set_close_callback` instead.
    75|     When a stream is closed due to an error, the IOStream's ``error``
    76|     attribute contains the exception object.
    77|     Subclasses must implement `fileno`, `close_fd`, `write_to_fd`,
    78|     `read_from_fd`, and optionally `get_fd_error`.
    79|     """
    80|     def __init__(self, io_loop=None, max_buffer_size=None,
    81|                  read_chunk_size=None, max_write_buffer_size=None):
    82|         """`BaseIOStream` constructor.
    83|         :arg io_loop: The `.IOLoop` to use; defaults to `.IOLoop.current`.
    84|                       Deprecated since Tornado 4.1.
    85|         :arg max_buffer_size: Maximum amount of incoming data to buffer;
    86|             defaults to 100MB.
    87|         :arg read_chunk_size: Amount of data to read at one time from the
    88|             underlying transport; defaults to 64KB.
    89|         :arg max_write_buffer_size: Amount of outgoing data to buffer;
    90|             defaults to unlimited.
    91|         .. versionchanged:: 4.0
    92|            Add the ``max_write_buffer_size`` parameter.  Changed default
    93|            ``read_chunk_size`` to 64KB.
    94|         """
    95|         self.io_loop = io_loop or ioloop.IOLoop.current()
    96|         self.max_buffer_size = max_buffer_size or 104857600
    97|         self.read_chunk_size = min(read_chunk_size or 65536,
    98|                                    self.max_buffer_size // 2)
    99|         self.max_write_buffer_size = max_write_buffer_size
   100|         self.error = None
   101|         self._read_buffer = bytearray()
   102|         self._read_buffer_pos = 0
   103|         self._read_buffer_size = 0
   104|         self._write_buffer = bytearray()
   105|         self._write_buffer_pos = 0
   106|         self._write_buffer_size = 0
   107|         self._write_buffer_frozen = False
   108|         self._total_write_index = 0
   109|         self._total_write_done_index = 0
   110|         self._pending_writes_while_frozen = []
   111|         self._read_delimiter = None
   112|         self._read_regex = None
   113|         self._read_max_bytes = None
   114|         self._read_bytes = None
   115|         self._read_partial = False
   116|         self._read_until_close = False
   117|         self._read_callback = None
   118|         self._read_future = None
   119|         self._streaming_callback = None
   120|         self._write_callback = None
   121|         self._write_futures = collections.deque()
   122|         self._close_callback = None
   123|         self._connect_callback = None
   124|         self._connect_future = None
   125|         self._ssl_connect_future = None
   126|         self._connecting = False
   127|         self._state = None
   128|         self._pending_callbacks = 0
   129|         self._closed = False
   130|     def fileno(self):
   131|         """Returns the file descriptor for this stream."""
   132|         raise NotImplementedError()
   133|     def close_fd(self):
   134|         """Closes the file underlying this stream.
   135|         ``close_fd`` is called by `BaseIOStream` and should not be called
   136|         elsewhere; other users should call `close` instead.
   137|         """
   138|         raise NotImplementedError()
   139|     def write_to_fd(self, data):
   140|         """Attempts to write ``data`` to the underlying file.
   141|         Returns the number of bytes written.
   142|         """
   143|         raise NotImplementedError()
   144|     def read_from_fd(self):
   145|         """Attempts to read from the underlying file.
   146|         Returns ``None`` if there was nothing to read (the socket
   147|         returned `~errno.EWOULDBLOCK` or equivalent), otherwise
   148|         returns the data.  When possible, should return no more than
   149|         ``self.read_chunk_size`` bytes at a time.
   150|         """
   151|         raise NotImplementedError()
   152|     def get_fd_error(self):
   153|         """Returns information about any error on the underlying file.
   154|         This method is called after the `.IOLoop` has signaled an error on the
   155|         file descriptor, and should return an Exception (such as `socket.error`
   156|         with additional information, or None if no such information is
   157|         available.
   158|         """
   159|         return None
   160|     def read_until_regex(self, regex, callback=None, max_bytes=None):
   161|         """Asynchronously read until we have matched the given regex.
   162|         The result includes the data that matches the regex and anything
   163|         that came before it.  If a callback is given, it will be run
   164|         with the data as an argument; if not, this method returns a
   165|         `.Future`.
   166|         If ``max_bytes`` is not None, the connection will be closed
   167|         if more than ``max_bytes`` bytes have been read and the regex is
   168|         not satisfied.
   169|         .. versionchanged:: 4.0
   170|             Added the ``max_bytes`` argument.  The ``callback`` argument is
   171|             now optional and a `.Future` will be returned if it is omitted.
   172|         """
   173|         future = self._set_read_callback(callback)
   174|         self._read_regex = re.compile(regex)
   175|         self._read_max_bytes = max_bytes
   176|         try:
   177|             self._try_inline_read()
   178|         except UnsatisfiableReadError as e:
   179|             gen_log.info("Unsatisfiable read, closing connection: %s" % e)
   180|             self.close(exc_info=True)
   181|             return future
   182|         except:
   183|             if future is not None:
   184|                 future.add_done_callback(lambda f: f.exception())
   185|             raise
   186|         return future
   187|     def read_until(self, delimiter, callback=None, max_bytes=None):
   188|         """Asynchronously read until we have found the given delimiter.
   189|         The result includes all the data read including the delimiter.
   190|         If a callback is given, it will be run with the data as an argument;
   191|         if not, this method returns a `.Future`.
   192|         If ``max_bytes`` is not None, the connection will be closed
   193|         if more than ``max_bytes`` bytes have been read and the delimiter
   194|         is not found.
   195|         .. versionchanged:: 4.0
   196|             Added the ``max_bytes`` argument.  The ``callback`` argument is
   197|             now optional and a `.Future` will be returned if it is omitted.
   198|         """
   199|         future = self._set_read_callback(callback)
   200|         self._read_delimiter = delimiter
   201|         self._read_max_bytes = max_bytes
   202|         try:
   203|             self._try_inline_read()
   204|         except UnsatisfiableReadError as e:
   205|             gen_log.info("Unsatisfiable read, closing connection: %s" % e)
   206|             self.close(exc_info=True)
   207|             return future
   208|         except:
   209|             if future is not None:
   210|                 future.add_done_callback(lambda f: f.exception())
   211|             raise
   212|         return future
   213|     def read_bytes(self, num_bytes, callback=None, streaming_callback=None,
   214|                    partial=False):
   215|         """Asynchronously read a number of bytes.
   216|         If a ``streaming_callback`` is given, it will be called with chunks
   217|         of data as they become available, and the final result will be empty.
   218|         Otherwise, the result is all the data that was read.
   219|         If a callback is given, it will be run with the data as an argument;
   220|         if not, this method returns a `.Future`.
   221|         If ``partial`` is true, the callback is run as soon as we have
   222|         any bytes to return (but never more than ``num_bytes``)
   223|         .. versionchanged:: 4.0
   224|             Added the ``partial`` argument.  The callback argument is now
   225|             optional and a `.Future` will be returned if it is omitted.
   226|         """
   227|         future = self._set_read_callback(callback)
   228|         assert isinstance(num_bytes, numbers.Integral)
   229|         self._read_bytes = num_bytes
   230|         self._read_partial = partial
   231|         self._streaming_callback = stack_context.wrap(streaming_callback)
   232|         try:
   233|             self._try_inline_read()
   234|         except:
   235|             if future is not None:
   236|                 future.add_done_callback(lambda f: f.exception())
   237|             raise
   238|         return future
   239|     def read_until_close(self, callback=None, streaming_callback=None):
   240|         """Asynchronously reads all data from the socket until it is closed.
   241|         If a ``streaming_callback`` is given, it will be called with chunks
   242|         of data as they become available, and the final result will be empty.
   243|         Otherwise, the result is all the data that was read.
   244|         If a callback is given, it will be run with the data as an argument;
   245|         if not, this method returns a `.Future`.
   246|         Note that if a ``streaming_callback`` is used, data will be
   247|         read from the socket as quickly as it becomes available; there
   248|         is no way to apply backpressure or cancel the reads. If flow
   249|         control or cancellation are desired, use a loop with
   250|         `read_bytes(partial=True) <.read_bytes>` instead.
   251|         .. versionchanged:: 4.0
   252|             The callback argument is now optional and a `.Future` will
   253|             be returned if it is omitted.
   254|         """
   255|         future = self._set_read_callback(callback)
   256|         self._streaming_callback = stack_context.wrap(streaming_callback)
   257|         if self.closed():
   258|             if self._streaming_callback is not None:
   259|                 self._run_read_callback(self._read_buffer_size, True)
   260|             self._run_read_callback(self._read_buffer_size, False)
   261|             return future
   262|         self._read_until_close = True
   263|         try:
   264|             self._try_inline_read()
   265|         except:
   266|             if future is not None:
   267|                 future.add_done_callback(lambda f: f.exception())
   268|             raise
   269|         return future
   270|     def write(self, data, callback=None):
   271|         """Asynchronously write the given data to this stream.
   272|         If ``callback`` is given, we call it when all of the buffered write
   273|         data has been successfully written to the stream. If there was
   274|         previously buffered write data and an old write callback, that
   275|         callback is simply overwritten with this new callback.
   276|         If no ``callback`` is given, this method returns a `.Future` that
   277|         resolves (with a result of ``None``) when the write has been
   278|         completed.
   279|         The ``data`` argument may be of type `bytes` or `memoryview`.
   280|         .. versionchanged:: 4.0
   281|             Now returns a `.Future` if no callback is given.
   282|         .. versionchanged:: 4.5
   283|             Added support for `memoryview` arguments.
   284|         """
   285|         self._check_closed()
   286|         if data:
   287|             if (self.max_write_buffer_size is not None and
   288|                     self._write_buffer_size + len(data) > self.max_write_buffer_size):
   289|                 raise StreamBufferFullError("Reached maximum write buffer size")
   290|             if self._write_buffer_frozen:
   291|                 self._pending_writes_while_frozen.append(data)
   292|             else:
   293|                 self._write_buffer += data
   294|                 self._write_buffer_size += len(data)
   295|             self._total_write_index += len(data)
   296|         if callback is not None:
   297|             self._write_callback = stack_context.wrap(callback)
   298|             future = None
   299|         else:
   300|             future = TracebackFuture()
   301|             future.add_done_callback(lambda f: f.exception())
   302|             self._write_futures.append((self._total_write_index, future))
   303|         if not self._connecting:
   304|             self._handle_write()
   305|             if self._write_buffer_size:
   306|                 self._add_io_state(self.io_loop.WRITE)
   307|             self._maybe_add_error_listener()
   308|         return future
   309|     def set_close_callback(self, callback):
   310|         """Call the given callback when the stream is closed.
   311|         This is not necessary for applications that use the `.Future`
   312|         interface; all outstanding ``Futures`` will resolve with a
   313|         `StreamClosedError` when the stream is closed.
   314|         """
   315|         self._close_callback = stack_context.wrap(callback)
   316|         self._maybe_add_error_listener()
   317|     def close(self, exc_info=False):
   318|         """Close this stream.
   319|         If ``exc_info`` is true, set the ``error`` attribute to the current
   320|         exception from `sys.exc_info` (or if ``exc_info`` is a tuple,
   321|         use that instead of `sys.exc_info`).
   322|         """
   323|         if not self.closed():
   324|             if exc_info:
   325|                 if not isinstance(exc_info, tuple):
   326|                     exc_info = sys.exc_info()
   327|                 if any(exc_info):
   328|                     self.error = exc_info[1]
   329|             if self._read_until_close:
   330|                 if (self._streaming_callback is not None and
   331|                         self._read_buffer_size):
   332|                     self._run_read_callback(self._read_buffer_size, True)
   333|                 self._read_until_close = False
   334|                 self._run_read_callback(self._read_buffer_size, False)
   335|             if self._state is not None:
   336|                 self.io_loop.remove_handler(self.fileno())
   337|                 self._state = None
   338|             self.close_fd()
   339|             self._closed = True
   340|         self._maybe_run_close_callback()
   341|     def _maybe_run_close_callback(self):
   342|         if self.closed() and self._pending_callbacks == 0:
   343|             futures = []
   344|             if self._read_future is not None:
   345|                 futures.append(self._read_future)
   346|                 self._read_future = None
   347|             futures += [future for _, future in self._write_futures]
   348|             self._write_futures.clear()
   349|             if self._connect_future is not None:
   350|                 futures.append(self._connect_future)
   351|                 self._connect_future = None
   352|             if self._ssl_connect_future is not None:
   353|                 futures.append(self._ssl_connect_future)
   354|                 self._ssl_connect_future = None
   355|             for future in futures:
   356|                 future.set_exception(StreamClosedError(real_error=self.error))
   357|             if self._close_callback is not None:
   358|                 cb = self._close_callback
   359|                 self._close_callback = None
   360|                 self._run_callback(cb)
   361|             self._read_callback = self._write_callback = None
   362|             self._write_buffer = None
   363|             self._write_buffer_size = 0
   364|     def reading(self):
   365|         """Returns true if we are currently reading from the stream."""
   366|         return self._read_callback is not None or self._read_future is not None
   367|     def writing(self):
   368|         """Returns true if we are currently writing to the stream."""
   369|         return self._write_buffer_size > 0
   370|     def closed(self):
   371|         """Returns true if the stream has been closed."""
   372|         return self._closed
   373|     def set_nodelay(self, value):
   374|         """Sets the no-delay flag for this stream.
   375|         By default, data written to TCP streams may be held for a time
   376|         to make the most efficient use of bandwidth (according to
   377|         Nagle's algorithm).  The no-delay flag requests that data be
   378|         written as soon as possible, even if doing so would consume
   379|         additional bandwidth.
   380|         This flag is currently defined only for TCP-based ``IOStreams``.
   381|         .. versionadded:: 3.1
   382|         """
   383|         pass
   384|     def _handle_events(self, fd, events):
   385|         if self.closed():
   386|             gen_log.warning("Got events for closed stream %s", fd)
   387|             return
   388|         try:
   389|             if self._connecting:
   390|                 self._handle_connect()
   391|             if self.closed():
   392|                 return
   393|             if events & self.io_loop.READ:
   394|                 self._handle_read()
   395|             if self.closed():
   396|                 return
   397|             if events & self.io_loop.WRITE:
   398|                 self._handle_write()
   399|             if self.closed():
   400|                 return
   401|             if events & self.io_loop.ERROR:
   402|                 self.error = self.get_fd_error()
   403|                 self.io_loop.add_callback(self.close)
   404|                 return
   405|             state = self.io_loop.ERROR
   406|             if self.reading():
   407|                 state |= self.io_loop.READ
   408|             if self.writing():
   409|                 state |= self.io_loop.WRITE
   410|             if state == self.io_loop.ERROR and self._read_buffer_size == 0:
   411|                 state |= self.io_loop.READ
   412|             if state != self._state:
   413|                 assert self._state is not None, \
   414|                     "shouldn't happen: _handle_events without self._state"
   415|                 self._state = state
   416|                 self.io_loop.update_handler(self.fileno(), self._state)
   417|         except UnsatisfiableReadError as e:
   418|             gen_log.info("Unsatisfiable read, closing connection: %s" % e)
   419|             self.close(exc_info=True)
   420|         except Exception:
   421|             gen_log.error("Uncaught exception, closing connection.",
   422|                           exc_info=True)
   423|             self.close(exc_info=True)
   424|             raise
   425|     def _run_callback(self, callback, *args):
   426|         def wrapper():
   427|             self._pending_callbacks -= 1
   428|             try:
   429|                 return callback(*args)
   430|             except Exception:
   431|                 app_log.error("Uncaught exception, closing connection.",
   432|                               exc_info=True)
   433|                 self.close(exc_info=True)
   434|                 raise
   435|             finally:
   436|                 self._maybe_add_error_listener()
   437|         with stack_context.NullContext():
   438|             self._pending_callbacks += 1
   439|             self.io_loop.add_callback(wrapper)
   440|     def _read_to_buffer_loop(self):
   441|         try:
   442|             if self._read_bytes is not None:
   443|                 target_bytes = self._read_bytes
   444|             elif self._read_max_bytes is not None:
   445|                 target_bytes = self._read_max_bytes
   446|             elif self.reading():
   447|                 target_bytes = None
   448|             else:
   449|                 target_bytes = 0
   450|             next_find_pos = 0
   451|             self._pending_callbacks += 1
   452|             while not self.closed():
   453|                 if self._read_to_buffer() == 0:
   454|                     break
   455|                 self._run_streaming_callback()
   456|                 if (target_bytes is not None and
   457|                         self._read_buffer_size >= target_bytes):
   458|                     break
   459|                 if self._read_buffer_size >= next_find_pos:
   460|                     pos = self._find_read_pos()
   461|                     if pos is not None:
   462|                         return pos
   463|                     next_find_pos = self._read_buffer_size * 2
   464|             return self._find_read_pos()
   465|         finally:
   466|             self._pending_callbacks -= 1
   467|     def _handle_read(self):
   468|         try:
   469|             pos = self._read_to_buffer_loop()
   470|         except UnsatisfiableReadError:
   471|             raise
   472|         except Exception as e:
   473|             gen_log.warning("error on read: %s" % e)
   474|             self.close(exc_info=True)
   475|             return
   476|         if pos is not None:
   477|             self._read_from_buffer(pos)
   478|             return
   479|         else:
   480|             self._maybe_run_close_callback()
   481|     def _set_read_callback(self, callback):
   482|         assert self._read_callback is None, "Already reading"
   483|         assert self._read_future is None, "Already reading"
   484|         if callback is not None:
   485|             self._read_callback = stack_context.wrap(callback)
   486|         else:
   487|             self._read_future = TracebackFuture()
   488|         return self._read_future
   489|     def _run_read_callback(self, size, streaming):
   490|         if streaming:
   491|             callback = self._streaming_callback
   492|         else:
   493|             callback = self._read_callback
   494|             self._read_callback = self._streaming_callback = None
   495|             if self._read_future is not None:
   496|                 assert callback is None
   497|                 future = self._read_future
   498|                 self._read_future = None
   499|                 future.set_result(self._consume(size))
   500|         if callback is not None:
   501|             assert (self._read_future is None) or streaming
   502|             self._run_callback(callback, self._consume(size))
   503|         else:
   504|             self._maybe_add_error_listener()
   505|     def _try_inline_read(self):
   506|         """Attempt to complete the current read operation from buffered data.
   507|         If the read can be completed without blocking, schedules the
   508|         read callback on the next IOLoop iteration; otherwise starts
   509|         listening for reads on the socket.
   510|         """
   511|         self._run_streaming_callback()
   512|         pos = self._find_read_pos()
   513|         if pos is not None:
   514|             self._read_from_buffer(pos)
   515|             return
   516|         self._check_closed()
   517|         try:
   518|             pos = self._read_to_buffer_loop()
   519|         except Exception:
   520|             self._maybe_run_close_callback()
   521|             raise
   522|         if pos is not None:
   523|             self._read_from_buffer(pos)
   524|             return
   525|         if self.closed():
   526|             self._maybe_run_close_callback()
   527|         else:
   528|             self._add_io_state(ioloop.IOLoop.READ)
   529|     def _read_to_buffer(self):
   530|         """Reads from the socket and appends the result to the read buffer.
   531|         Returns the number of bytes read.  Returns 0 if there is nothing
   532|         to read (i.e. the read returns EWOULDBLOCK or equivalent).  On
   533|         error closes the socket and raises an exception.
   534|         """
   535|         while True:
   536|             try:
   537|                 chunk = self.read_from_fd()
   538|             except (socket.error, IOError, OSError) as e:
   539|                 if errno_from_exception(e) == errno.EINTR:
   540|                     continue
   541|                 if self._is_connreset(e):
   542|                     self.close(exc_info=True)
   543|                     return
   544|                 self.close(exc_info=True)
   545|                 raise
   546|             break
   547|         if chunk is None:
   548|             return 0
   549|         self._read_buffer += chunk
   550|         self._read_buffer_size += len(chunk)
   551|         if self._read_buffer_size > self.max_buffer_size:
   552|             gen_log.error("Reached maximum read buffer size")
   553|             self.close()
   554|             raise StreamBufferFullError("Reached maximum read buffer size")
   555|         return len(chunk)
   556|     def _run_streaming_callback(self):
   557|         if self._streaming_callback is not None and self._read_buffer_size:
   558|             bytes_to_consume = self._read_buffer_size
   559|             if self._read_bytes is not None:
   560|                 bytes_to_consume = min(self._read_bytes, bytes_to_consume)
   561|                 self._read_bytes -= bytes_to_consume
   562|             self._run_read_callback(bytes_to_consume, True)
   563|     def _read_from_buffer(self, pos):
   564|         """Attempts to complete the currently-pending read from the buffer.
   565|         The argument is either a position in the read buffer or None,
   566|         as returned by _find_read_pos.
   567|         """
   568|         self._read_bytes = self._read_delimiter = self._read_regex = None
   569|         self._read_partial = False
   570|         self._run_read_callback(pos, False)
   571|     def _find_read_pos(self):
   572|         """Attempts to find a position in the read buffer that satisfies
   573|         the currently-pending read.
   574|         Returns a position in the buffer if the current read can be satisfied,
   575|         or None if it cannot.
   576|         """
   577|         if (self._read_bytes is not None and
   578|             (self._read_buffer_size >= self._read_bytes or
   579|              (self._read_partial and self._read_buffer_size > 0))):
   580|             num_bytes = min(self._read_bytes, self._read_buffer_size)
   581|             return num_bytes
   582|         elif self._read_delimiter is not None:
   583|             if self._read_buffer:
   584|                 loc = self._read_buffer.find(self._read_delimiter,
   585|                                              self._read_buffer_pos)
   586|                 if loc != -1:
   587|                     loc -= self._read_buffer_pos
   588|                     delimiter_len = len(self._read_delimiter)
   589|                     self._check_max_bytes(self._read_delimiter,
   590|                                           loc + delimiter_len)
   591|                     return loc + delimiter_len
   592|                 self._check_max_bytes(self._read_delimiter,
   593|                                       self._read_buffer_size)
   594|         elif self._read_regex is not None:
   595|             if self._read_buffer:
   596|                 m = self._read_regex.search(self._read_buffer,
   597|                                             self._read_buffer_pos)
   598|                 if m is not None:
   599|                     loc = m.end() - self._read_buffer_pos
   600|                     self._check_max_bytes(self._read_regex, loc)
   601|                     return loc
   602|                 self._check_max_bytes(self._read_regex, self._read_buffer_size)
   603|         return None
   604|     def _check_max_bytes(self, delimiter, size):
   605|         if (self._read_max_bytes is not None and
   606|                 size > self._read_max_bytes):
   607|             raise UnsatisfiableReadError(
   608|                 "delimiter %r not found within %d bytes" % (
   609|                     delimiter, self._read_max_bytes))
   610|     def _freeze_write_buffer(self, size):
   611|         self._write_buffer_frozen = size
   612|     def _unfreeze_write_buffer(self):
   613|         self._write_buffer_frozen = False
   614|         self._write_buffer += b''.join(self._pending_writes_while_frozen)
   615|         self._write_buffer_size += sum(map(len, self._pending_writes_while_frozen))
   616|         self._pending_writes_while_frozen[:] = []
   617|     def _got_empty_write(self, size):
   618|         """
   619|         Called when a non-blocking write() failed writing anything.
   620|         Can be overridden in subclasses.
   621|         """
   622|     def _handle_write(self):
   623|         while self._write_buffer_size:
   624|             assert self._write_buffer_size >= 0
   625|             try:
   626|                 start = self._write_buffer_pos
   627|                 if self._write_buffer_frozen:
   628|                     size = self._write_buffer_frozen
   629|                 elif _WINDOWS:
   630|                     size = 128 * 1024
   631|                 else:
   632|                     size = self._write_buffer_size
   633|                 num_bytes = self.write_to_fd(
   634|                     memoryview(self._write_buffer)[start:start + size])
   635|                 if num_bytes == 0:
   636|                     self._got_empty_write(size)
   637|                     break
   638|                 self._write_buffer_pos += num_bytes
   639|                 self._write_buffer_size -= num_bytes
   640|                 if self._write_buffer_pos > self._write_buffer_size:
   641|                     del self._write_buffer[:self._write_buffer_pos]
   642|                     self._write_buffer_pos = 0
   643|                 if self._write_buffer_frozen:
   644|                     self._unfreeze_write_buffer()
   645|                 self._total_write_done_index += num_bytes
   646|             except (socket.error, IOError, OSError) as e:
   647|                 if e.args[0] in _ERRNO_WOULDBLOCK:
   648|                     self._got_empty_write(size)
   649|                     break
   650|                 else:
   651|                     if not self._is_connreset(e):
   652|                         gen_log.warning("Write error on %s: %s",
   653|                                         self.fileno(), e)
   654|                     self.close(exc_info=True)
   655|                     return
   656|         while self._write_futures:
   657|             index, future = self._write_futures[0]
   658|             if index > self._total_write_done_index:
   659|                 break
   660|             self._write_futures.popleft()
   661|             future.set_result(None)
   662|         if not self._write_buffer_size:
   663|             if self._write_callback:
   664|                 callback = self._write_callback
   665|                 self._write_callback = None
   666|                 self._run_callback(callback)
   667|     def _consume(self, loc):
   668|         if loc == 0:
   669|             return b""
   670|         assert loc <= self._read_buffer_size
   671|         b = (memoryview(self._read_buffer)
   672|              [self._read_buffer_pos:self._read_buffer_pos + loc]
   673|              ).tobytes()
   674|         self._read_buffer_pos += loc
   675|         self._read_buffer_size -= loc
   676|         if self._read_buffer_pos > self._read_buffer_size:
   677|             del self._read_buffer[:self._read_buffer_pos]
   678|             self._read_buffer_pos = 0
   679|         return b
   680|     def _check_closed(self):
   681|         if self.closed():
   682|             raise StreamClosedError(real_error=self.error)
   683|     def _maybe_add_error_listener(self):
   684|         if self._pending_callbacks != 0:
   685|             return
   686|         if self._state is None or self._state == ioloop.IOLoop.ERROR:
   687|             if self.closed():
   688|                 self._maybe_run_close_callback()
   689|             elif (self._read_buffer_size == 0 and
   690|                   self._close_callback is not None):
   691|                 self._add_io_state(ioloop.IOLoop.READ)
   692|     def _add_io_state(self, state):
   693|         """Adds `state` (IOLoop.{READ,WRITE} flags) to our event handler.
   694|         Implementation notes: Reads and writes have a fast path and a
   695|         slow path.  The fast path reads synchronously from socket
   696|         buffers, while the slow path uses `_add_io_state` to schedule
   697|         an IOLoop callback.  Note that in both cases, the callback is
   698|         run asynchronously with `_run_callback`.
   699|         To detect closed connections, we must have called
   700|         `_add_io_state` at some point, but we want to delay this as
   701|         much as possible so we don't have to set an `IOLoop.ERROR`
   702|         listener that will be overwritten by the next slow-path
   703|         operation.  As long as there are callbacks scheduled for
   704|         fast-path ops, those callbacks may do more reads.
   705|         If a sequence of fast-path ops do not end in a slow-path op,
   706|         (e.g. for an @asynchronous long-poll request), we must add
   707|         the error handler.  This is done in `_run_callback` and `write`
   708|         (since the write callback is optional so we can have a
   709|         fast-path write with no `_run_callback`)
   710|         """
   711|         if self.closed():
   712|             return
   713|         if self._state is None:
   714|             self._state = ioloop.IOLoop.ERROR | state
   715|             with stack_context.NullContext():
   716|                 self.io_loop.add_handler(
   717|                     self.fileno(), self._handle_events, self._state)
   718|         elif not self._state & state:
   719|             self._state = self._state | state
   720|             self.io_loop.update_handler(self.fileno(), self._state)
   721|     def _is_connreset(self, exc):
   722|         """Return true if exc is ECONNRESET or equivalent.
   723|         May be overridden in subclasses.
   724|         """
   725|         return (isinstance(exc, (socket.error, IOError)) and
   726|                 errno_from_exception(exc) in _ERRNO_CONNRESET)
   727| class IOStream(BaseIOStream):
   728|     r"""Socket-based `IOStream` implementation.
   729|     This class supports the read and write methods from `BaseIOStream`
   730|     plus a `connect` method.
   731|     The ``socket`` parameter may either be connected or unconnected.
   732|     For server operations the socket is the result of calling
   733|     `socket.accept <socket.socket.accept>`.  For client operations the
   734|     socket is created with `socket.socket`, and may either be
   735|     connected before passing it to the `IOStream` or connected with
   736|     `IOStream.connect`.
   737|     A very simple (and broken) HTTP client using this class:
   738|     .. testcode::
   739|         import tornado.ioloop
   740|         import tornado.iostream
   741|         import socket
   742|         def send_request():
   743|             stream.write(b"GET / HTTP/1.0\r\nHost: friendfeed.com\r\n\r\n")
   744|             stream.read_until(b"\r\n\r\n", on_headers)
   745|         def on_headers(data):
   746|             headers = {}
   747|             for line in data.split(b"\r\n"):
   748|                parts = line.split(b":")
   749|                if len(parts) == 2:
   750|                    headers[parts[0].strip()] = parts[1].strip()
   751|             stream.read_bytes(int(headers[b"Content-Length"]), on_body)
   752|         def on_body(data):
   753|             print(data)
   754|             stream.close()
   755|             tornado.ioloop.IOLoop.current().stop()
   756|         if __name__ == '__main__':
   757|             s = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)
   758|             stream = tornado.iostream.IOStream(s)
   759|             stream.connect(("friendfeed.com", 80), send_request)
   760|             tornado.ioloop.IOLoop.current().start()
   761|     .. testoutput::
   762|        :hide:
   763|     """
   764|     def __init__(self, socket, *args, **kwargs):
   765|         self.socket = socket
   766|         self.socket.setblocking(False)
   767|         super(IOStream, self).__init__(*args, **kwargs)
   768|     def fileno(self):
   769|         return self.socket
   770|     def close_fd(self):
   771|         self.socket.close()
   772|         self.socket = None
   773|     def get_fd_error(self):
   774|         errno = self.socket.getsockopt(socket.SOL_SOCKET,
   775|                                        socket.SO_ERROR)
   776|         return socket.error(errno, os.strerror(errno))
   777|     def read_from_fd(self):
   778|         try:
   779|             chunk = self.socket.recv(self.read_chunk_size)
   780|         except socket.error as e:
   781|             if e.args[0] in _ERRNO_WOULDBLOCK:
   782|                 return None
   783|             else:
   784|                 raise
   785|         if not chunk:
   786|             self.close()
   787|             return None
   788|         return chunk
   789|     def write_to_fd(self, data):
   790|         try:
   791|             return self.socket.send(data)
   792|         finally:
   793|             del data
   794|     def connect(self, address, callback=None, server_hostname=None):
   795|         """Connects the socket to a remote address without blocking.
   796|         May only be called if the socket passed to the constructor was
   797|         not previously connected.  The address parameter is in the
   798|         same format as for `socket.connect <socket.socket.connect>` for
   799|         the type of socket passed to the IOStream constructor,
   800|         e.g. an ``(ip, port)`` tuple.  Hostnames are accepted here,
   801|         but will be resolved synchronously and block the IOLoop.
   802|         If you have a hostname instead of an IP address, the `.TCPClient`
   803|         class is recommended instead of calling this method directly.
   804|         `.TCPClient` will do asynchronous DNS resolution and handle
   805|         both IPv4 and IPv6.
   806|         If ``callback`` is specified, it will be called with no
   807|         arguments when the connection is completed; if not this method
   808|         returns a `.Future` (whose result after a successful
   809|         connection will be the stream itself).
   810|         In SSL mode, the ``server_hostname`` parameter will be used
   811|         for certificate validation (unless disabled in the
   812|         ``ssl_options``) and SNI (if supported; requires Python
   813|         2.7.9+).
   814|         Note that it is safe to call `IOStream.write
   815|         <BaseIOStream.write>` while the connection is pending, in
   816|         which case the data will be written as soon as the connection
   817|         is ready.  Calling `IOStream` read methods before the socket is
   818|         connected works on some platforms but is non-portable.
   819|         .. versionchanged:: 4.0
   820|             If no callback is given, returns a `.Future`.
   821|         .. versionchanged:: 4.2
   822|            SSL certificates are validated by default; pass
   823|            ``ssl_options=dict(cert_reqs=ssl.CERT_NONE)`` or a
   824|            suitably-configured `ssl.SSLContext` to the
   825|            `SSLIOStream` constructor to disable.
   826|         """
   827|         self._connecting = True
   828|         if callback is not None:
   829|             self._connect_callback = stack_context.wrap(callback)
   830|             future = None
   831|         else:
   832|             future = self._connect_future = TracebackFuture()
   833|         try:
   834|             self.socket.connect(address)
   835|         except socket.error as e:
   836|             if (errno_from_exception(e) not in _ERRNO_INPROGRESS and
   837|                     errno_from_exception(e) not in _ERRNO_WOULDBLOCK):
   838|                 if future is None:
   839|                     gen_log.warning("Connect error on fd %s: %s",
   840|                                     self.socket.fileno(), e)
   841|                 self.close(exc_info=True)
   842|                 return future
   843|         self._add_io_state(self.io_loop.WRITE)
   844|         return future
   845|     def start_tls(self, server_side, ssl_options=None, server_hostname=None):
   846|         """Convert this `IOStream` to an `SSLIOStream`.
   847|         This enables protocols that begin in clear-text mode and
   848|         switch to SSL after some initial negotiation (such as the
   849|         ``STARTTLS`` extension to SMTP and IMAP).
   850|         This method cannot be used if there are outstanding reads
   851|         or writes on the stream, or if there is any data in the
   852|         IOStream's buffer (data in the operating system's socket
   853|         buffer is allowed).  This means it must generally be used
   854|         immediately after reading or writing the last clear-text
   855|         data.  It can also be used immediately after connecting,
   856|         before any reads or writes.
   857|         The ``ssl_options`` argument may be either an `ssl.SSLContext`
   858|         object or a dictionary of keyword arguments for the
   859|         `ssl.wrap_socket` function.  The ``server_hostname`` argument
   860|         will be used for certificate validation unless disabled
   861|         in the ``ssl_options``.
   862|         This method returns a `.Future` whose result is the new
   863|         `SSLIOStream`.  After this method has been called,
   864|         any other operation on the original stream is undefined.
   865|         If a close callback is defined on this stream, it will be
   866|         transferred to the new stream.
   867|         .. versionadded:: 4.0
   868|         .. versionchanged:: 4.2
   869|            SSL certificates are validated by default; pass
   870|            ``ssl_options=dict(cert_reqs=ssl.CERT_NONE)`` or a
   871|            suitably-configured `ssl.SSLContext` to disable.
   872|         """
   873|         if (self._read_callback or self._read_future or
   874|                 self._write_callback or self._write_futures or
   875|                 self._connect_callback or self._connect_future or
   876|                 self._pending_callbacks or self._closed or
   877|                 self._read_buffer or self._write_buffer):
   878|             raise ValueError("IOStream is not idle; cannot convert to SSL")
   879|         if ssl_options is None:
   880|             if server_side:
   881|                 ssl_options = _server_ssl_defaults
   882|             else:
   883|                 ssl_options = _client_ssl_defaults
   884|         socket = self.socket
   885|         self.io_loop.remove_handler(socket)
   886|         self.socket = None
   887|         socket = ssl_wrap_socket(socket, ssl_options,
   888|                                  server_hostname=server_hostname,
   889|                                  server_side=server_side,
   890|                                  do_handshake_on_connect=False)
   891|         orig_close_callback = self._close_callback
   892|         self._close_callback = None
   893|         future = TracebackFuture()
   894|         ssl_stream = SSLIOStream(socket, ssl_options=ssl_options,
   895|                                  io_loop=self.io_loop)
   896|         def close_callback():
   897|             if not future.done():
   898|                 future.set_exception(ssl_stream.error or StreamClosedError())
   899|             if orig_close_callback is not None:
   900|                 orig_close_callback()
   901|         ssl_stream.set_close_callback(close_callback)
   902|         ssl_stream._ssl_connect_callback = lambda: future.set_result(ssl_stream)
   903|         ssl_stream.max_buffer_size = self.max_buffer_size
   904|         ssl_stream.read_chunk_size = self.read_chunk_size
   905|         return future
   906|     def _handle_connect(self):
   907|         err = self.socket.getsockopt(socket.SOL_SOCKET, socket.SO_ERROR)
   908|         if err != 0:
   909|             self.error = socket.error(err, os.strerror(err))
   910|             if self._connect_future is None:
   911|                 gen_log.warning("Connect error on fd %s: %s",
   912|                                 self.socket.fileno(), errno.errorcode[err])
   913|             self.close()
   914|             return
   915|         if self._connect_callback is not None:
   916|             callback = self._connect_callback
   917|             self._connect_callback = None
   918|             self._run_callback(callback)
   919|         if self._connect_future is not None:
   920|             future = self._connect_future
   921|             self._connect_future = None
   922|             future.set_result(self)
   923|         self._connecting = False
   924|     def set_nodelay(self, value):
   925|         if (self.socket is not None and
   926|                 self.socket.family in (socket.AF_INET, socket.AF_INET6)):
   927|             try:
   928|                 self.socket.setsockopt(socket.IPPROTO_TCP,
   929|                                        socket.TCP_NODELAY, 1 if value else 0)
   930|             except socket.error as e:
   931|                 if e.errno != errno.EINVAL and not self._is_connreset(e):
   932|                     raise
   933| class SSLIOStream(IOStream):
   934|     """A utility class to write to and read from a non-blocking SSL socket.
   935|     If the socket passed to the constructor is already connected,
   936|     it should be wrapped with::
   937|         ssl.wrap_socket(sock, do_handshake_on_connect=False, **kwargs)
   938|     before constructing the `SSLIOStream`.  Unconnected sockets will be
   939|     wrapped when `IOStream.connect` is finished.
   940|     """
   941|     def __init__(self, *args, **kwargs):
   942|         """The ``ssl_options`` keyword argument may either be an
   943|         `ssl.SSLContext` object or a dictionary of keywords arguments
   944|         for `ssl.wrap_socket`
   945|         """
   946|         self._ssl_options = kwargs.pop('ssl_options', _client_ssl_defaults)
   947|         super(SSLIOStream, self).__init__(*args, **kwargs)
   948|         self._ssl_accepting = True
   949|         self._handshake_reading = False
   950|         self._handshake_writing = False
   951|         self._ssl_connect_callback = None
   952|         self._server_hostname = None
   953|         try:
   954|             self.socket.getpeername()
   955|         except socket.error:
   956|             pass
   957|         else:
   958|             self._add_io_state(self.io_loop.WRITE)
   959|     def reading(self):
   960|         return self._handshake_reading or super(SSLIOStream, self).reading()
   961|     def writing(self):
   962|         return self._handshake_writing or super(SSLIOStream, self).writing()
   963|     def _got_empty_write(self, size):
   964|         self._freeze_write_buffer(size)
   965|     def _do_ssl_handshake(self):
   966|         try:
   967|             self._handshake_reading = False
   968|             self._handshake_writing = False
   969|             self.socket.do_handshake()
   970|         except ssl.SSLError as err:
   971|             if err.args[0] == ssl.SSL_ERROR_WANT_READ:
   972|                 self._handshake_reading = True
   973|                 return
   974|             elif err.args[0] == ssl.SSL_ERROR_WANT_WRITE:
   975|                 self._handshake_writing = True
   976|                 return
   977|             elif err.args[0] in (ssl.SSL_ERROR_EOF,
   978|                                  ssl.SSL_ERROR_ZERO_RETURN):
   979|                 return self.close(exc_info=True)
   980|             elif err.args[0] == ssl.SSL_ERROR_SSL:
   981|                 try:
   982|                     peer = self.socket.getpeername()
   983|                 except Exception:
   984|                     peer = '(not connected)'
   985|                 gen_log.warning("SSL Error on %s %s: %s",
   986|                                 self.socket.fileno(), peer, err)
   987|                 return self.close(exc_info=True)
   988|             raise
   989|         except socket.error as err:
   990|             if (self._is_connreset(err) or
   991|                     err.args[0] in (errno.EBADF, errno.ENOTCONN)):
   992|                 return self.close(exc_info=True)
   993|             raise
   994|         except AttributeError:
   995|             return self.close(exc_info=True)
   996|         else:
   997|             self._ssl_accepting = False
   998|             if not self._verify_cert(self.socket.getpeercert()):
   999|                 self.close()
  1000|                 return
  1001|             self._run_ssl_connect_callback()
  1002|     def _run_ssl_connect_callback(self):
  1003|         if self._ssl_connect_callback is not None:
  1004|             callback = self._ssl_connect_callback
  1005|             self._ssl_connect_callback = None
  1006|             self._run_callback(callback)
  1007|         if self._ssl_connect_future is not None:
  1008|             future = self._ssl_connect_future
  1009|             self._ssl_connect_future = None
  1010|             future.set_result(self)
  1011|     def _verify_cert(self, peercert):
  1012|         """Returns True if peercert is valid according to the configured
  1013|         validation mode and hostname.
  1014|         The ssl handshake already tested the certificate for a valid
  1015|         CA signature; the only thing that remains is to check
  1016|         the hostname.
  1017|         """
  1018|         if isinstance(self._ssl_options, dict):
  1019|             verify_mode = self._ssl_options.get('cert_reqs', ssl.CERT_NONE)
  1020|         elif isinstance(self._ssl_options, ssl.SSLContext):
  1021|             verify_mode = self._ssl_options.verify_mode
  1022|         assert verify_mode in (ssl.CERT_NONE, ssl.CERT_REQUIRED, ssl.CERT_OPTIONAL)
  1023|         if verify_mode == ssl.CERT_NONE or self._server_hostname is None:
  1024|             return True
  1025|         cert = self.socket.getpeercert()
  1026|         if cert is None and verify_mode == ssl.CERT_REQUIRED:
  1027|             gen_log.warning("No SSL certificate given")
  1028|             return False
  1029|         try:
  1030|             ssl_match_hostname(peercert, self._server_hostname)
  1031|         except SSLCertificateError as e:
  1032|             gen_log.warning("Invalid SSL certificate: %s" % e)
  1033|             return False
  1034|         else:
  1035|             return True
  1036|     def _handle_read(self):
  1037|         if self._ssl_accepting:
  1038|             self._do_ssl_handshake()
  1039|             return
  1040|         super(SSLIOStream, self)._handle_read()
  1041|     def _handle_write(self):
  1042|         if self._ssl_accepting:
  1043|             self._do_ssl_handshake()
  1044|             return
  1045|         super(SSLIOStream, self)._handle_write()
  1046|     def connect(self, address, callback=None, server_hostname=None):
  1047|         self._server_hostname = server_hostname
  1048|         super(SSLIOStream, self).connect(address, callback=lambda: None)
  1049|         return self.wait_for_handshake(callback)
  1050|     def _handle_connect(self):
  1051|         super(SSLIOStream, self)._handle_connect()
  1052|         if self.closed():
  1053|             return
  1054|         self.io_loop.remove_handler(self.socket)
  1055|         old_state = self._state
  1056|         self._state = None
  1057|         self.socket = ssl_wrap_socket(self.socket, self._ssl_options,
  1058|                                       server_hostname=self._server_hostname,
  1059|                                       do_handshake_on_connect=False)
  1060|         self._add_io_state(old_state)
  1061|     def wait_for_handshake(self, callback=None):
  1062|         """Wait for the initial SSL handshake to complete.
  1063|         If a ``callback`` is given, it will be called with no
  1064|         arguments once the handshake is complete; otherwise this
  1065|         method returns a `.Future` which will resolve to the
  1066|         stream itself after the handshake is complete.
  1067|         Once the handshake is complete, information such as
  1068|         the peer's certificate and NPN/ALPN selections may be
  1069|         accessed on ``self.socket``.
  1070|         This method is intended for use on server-side streams
  1071|         or after using `IOStream.start_tls`; it should not be used
  1072|         with `IOStream.connect` (which already waits for the
  1073|         handshake to complete). It may only be called once per stream.
  1074|         .. versionadded:: 4.2
  1075|         """
  1076|         if (self._ssl_connect_callback is not None or
  1077|                 self._ssl_connect_future is not None):
  1078|             raise RuntimeError("Already waiting")
  1079|         if callback is not None:
  1080|             self._ssl_connect_callback = stack_context.wrap(callback)
  1081|             future = None
  1082|         else:
  1083|             future = self._ssl_connect_future = TracebackFuture()
  1084|         if not self._ssl_accepting:
  1085|             self._run_ssl_connect_callback()
  1086|         return future
  1087|     def write_to_fd(self, data):
  1088|         try:
  1089|             return self.socket.send(data)
  1090|         except ssl.SSLError as e:
  1091|             if e.args[0] == ssl.SSL_ERROR_WANT_WRITE:
  1092|                 return 0
  1093|             raise
  1094|         finally:
  1095|             del data
  1096|     def read_from_fd(self):
  1097|         if self._ssl_accepting:
  1098|             return None
  1099|         try:
  1100|             chunk = self.socket.read(self.read_chunk_size)
  1101|         except ssl.SSLError as e:
  1102|             if e.args[0] == ssl.SSL_ERROR_WANT_READ:
  1103|                 return None
  1104|             else:
  1105|                 raise
  1106|         except socket.error as e:
  1107|             if e.args[0] in _ERRNO_WOULDBLOCK:
  1108|                 return None
  1109|             else:
  1110|                 raise
  1111|         if not chunk:
  1112|             self.close()
  1113|             return None
  1114|         return chunk
  1115|     def _is_connreset(self, e):
  1116|         if isinstance(e, ssl.SSLError) and e.args[0] == ssl.SSL_ERROR_EOF:
  1117|             return True
  1118|         return super(SSLIOStream, self)._is_connreset(e)
  1119| class PipeIOStream(BaseIOStream):
  1120|     """Pipe-based `IOStream` implementation.
  1121|     The constructor takes an integer file descriptor (such as one returned
  1122|     by `os.pipe`) rather than an open file object.  Pipes are generally
  1123|     one-way, so a `PipeIOStream` can be used for reading or writing but not
  1124|     both.
  1125|     """
  1126|     def __init__(self, fd, *args, **kwargs):
  1127|         self.fd = fd
  1128|         _set_nonblocking(fd)
  1129|         super(PipeIOStream, self).__init__(*args, **kwargs)
  1130|     def fileno(self):
  1131|         return self.fd
  1132|     def close_fd(self):
  1133|         os.close(self.fd)
  1134|     def write_to_fd(self, data):
  1135|         try:
  1136|             return os.write(self.fd, data)
  1137|         finally:
  1138|             del data
  1139|     def read_from_fd(self):
  1140|         try:
  1141|             chunk = os.read(self.fd, self.read_chunk_size)
  1142|         except (IOError, OSError) as e:
  1143|             if errno_from_exception(e) in _ERRNO_WOULDBLOCK:
  1144|                 return None
  1145|             elif errno_from_exception(e) == errno.EBADF:
  1146|                 self.close(exc_info=True)
  1147|                 return None
  1148|             else:
  1149|                 raise
  1150|         if not chunk:
  1151|             self.close()
  1152|             return None
  1153|         return chunk
  1154| def doctests():
  1155|     import doctest
  1156|     return doctest.DocTestSuite()


# ====================================================================
# FILE: salt/ext/tornado/locale.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-405 ---
     1| """Translation methods for generating localized strings.
     2| To load a locale and generate a translated string::
     3|     user_locale = tornado.locale.get("es_LA")
     4|     print(user_locale.translate("Sign out"))
     5| `tornado.locale.get()` returns the closest matching locale, not necessarily the
     6| specific locale you requested. You can support pluralization with
     7| additional arguments to `~Locale.translate()`, e.g.::
     8|     people = [...]
     9|     message = user_locale.translate(
    10|         "%(list)s is online", "%(list)s are online", len(people))
    11|     print(message % {"list": user_locale.list(people)})
    12| The first string is chosen if ``len(people) == 1``, otherwise the second
    13| string is chosen.
    14| Applications should call one of `load_translations` (which uses a simple
    15| CSV format) or `load_gettext_translations` (which uses the ``.mo`` format
    16| supported by `gettext` and related tools).  If neither method is called,
    17| the `Locale.translate` method will simply return the original string.
    18| """
    19| from __future__ import absolute_import, division, print_function
    20| import codecs
    21| import csv
    22| import datetime
    23| from io import BytesIO
    24| import numbers
    25| import os
    26| import re
    27| from salt.ext.tornado import escape
    28| from salt.ext.tornado.log import gen_log
    29| from salt.ext.tornado.util import PY3
    30| from salt.ext.tornado._locale_data import LOCALE_NAMES
    31| _default_locale = "en_US"
    32| _translations = {}  # type: dict
    33| _supported_locales = frozenset([_default_locale])
    34| _use_gettext = False
    35| CONTEXT_SEPARATOR = "\x04"
    36| def get(*locale_codes):
    37|     """Returns the closest match for the given locale codes.
    38|     We iterate over all given locale codes in order. If we have a tight
    39|     or a loose match for the code (e.g., "en" for "en_US"), we return
    40|     the locale. Otherwise we move to the next code in the list.
    41|     By default we return ``en_US`` if no translations are found for any of
    42|     the specified locales. You can change the default locale with
    43|     `set_default_locale()`.
    44|     """
    45|     return Locale.get_closest(*locale_codes)
    46| def set_default_locale(code):
    47|     """Sets the default locale.
    48|     The default locale is assumed to be the language used for all strings
    49|     in the system. The translations loaded from disk are mappings from
    50|     the default locale to the destination locale. Consequently, you don't
    51|     need to create a translation file for the default locale.
    52|     """
    53|     global _default_locale
    54|     global _supported_locales
    55|     _default_locale = code
    56|     _supported_locales = frozenset(list(_translations.keys()) + [_default_locale])
    57| def load_translations(directory, encoding=None):
    58|     """Loads translations from CSV files in a directory.
    59|     Translations are strings with optional Python-style named placeholders
    60|     (e.g., ``My name is %(name)s``) and their associated translations.
    61|     The directory should have translation files of the form ``LOCALE.csv``,
    62|     e.g. ``es_GT.csv``. The CSV files should have two or three columns: string,
    63|     translation, and an optional plural indicator. Plural indicators should
    64|     be one of "plural" or "singular". A given string can have both singular
    65|     and plural forms. For example ``%(name)s liked this`` may have a
    66|     different verb conjugation depending on whether %(name)s is one
    67|     name or a list of names. There should be two rows in the CSV file for
    68|     that string, one with plural indicator "singular", and one "plural".
    69|     For strings with no verbs that would change on translation, simply
    70|     use "unknown" or the empty string (or don't include the column at all).
    71|     The file is read using the `csv` module in the default "excel" dialect.
    72|     In this format there should not be spaces after the commas.
    73|     If no ``encoding`` parameter is given, the encoding will be
    74|     detected automatically (among UTF-8 and UTF-16) if the file
    75|     contains a byte-order marker (BOM), defaulting to UTF-8 if no BOM
    76|     is present.
    77|     Example translation ``es_LA.csv``::
    78|         "I love you","Te amo"
    79|         "%(name)s liked this","A %(name)s les gust esto","plural"
    80|         "%(name)s liked this","A %(name)s le gust esto","singular"
    81|     .. versionchanged:: 4.3
    82|        Added ``encoding`` parameter. Added support for BOM-based encoding
    83|        detection, UTF-16, and UTF-8-with-BOM.
    84|     """
    85|     global _translations
    86|     global _supported_locales
    87|     _translations = {}
    88|     for path in os.listdir(directory):
    89|         if not path.endswith(".csv"):
    90|             continue
    91|         locale, extension = path.split(".")
    92|         if not re.match("[a-z]+(_[A-Z]+)?$", locale):
    93|             gen_log.error("Unrecognized locale %r (path: %s)", locale,
    94|                           os.path.join(directory, path))
    95|             continue
    96|         full_path = os.path.join(directory, path)
    97|         if encoding is None:
    98|             with open(full_path, 'rb') as f:
    99|                 data = f.read(len(codecs.BOM_UTF16_LE))
   100|             if data in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):
   101|                 encoding = 'utf-16'
   102|             else:
   103|                 encoding = 'utf-8-sig'
   104|         if PY3:
   105|             f = open(full_path, "r", encoding=encoding)
   106|         else:
   107|             f = BytesIO()
   108|             with codecs.open(full_path, "r", encoding=encoding) as infile:
   109|                 f.write(escape.utf8(infile.read()))
   110|             f.seek(0)
   111|         _translations[locale] = {}
   112|         for i, row in enumerate(csv.reader(f)):
   113|             if not row or len(row) < 2:
   114|                 continue
   115|             row = [escape.to_unicode(c).strip() for c in row]
   116|             english, translation = row[:2]
   117|             if len(row) > 2:
   118|                 plural = row[2] or "unknown"
   119|             else:
   120|                 plural = "unknown"
   121|             if plural not in ("plural", "singular", "unknown"):
   122|                 gen_log.error("Unrecognized plural indicator %r in %s line %d",
   123|                               plural, path, i + 1)
   124|                 continue
   125|             _translations[locale].setdefault(plural, {})[english] = translation
   126|         f.close()
   127|     _supported_locales = frozenset(list(_translations.keys()) + [_default_locale])
   128|     gen_log.debug("Supported locales: %s", sorted(_supported_locales))
   129| def load_gettext_translations(directory, domain):
   130|     """Loads translations from `gettext`'s locale tree
   131|     Locale tree is similar to system's ``/usr/share/locale``, like::
   132|         {directory}/{lang}/LC_MESSAGES/{domain}.mo
   133|     Three steps are required to have your app translated:
   134|     1. Generate POT translation file::
   135|         xgettext --language=Python --keyword=_:1,2 -d mydomain file1.py file2.html etc
   136|     2. Merge against existing POT file::
   137|         msgmerge old.po mydomain.po > new.po
   138|     3. Compile::
   139|         msgfmt mydomain.po -o {directory}/pt_BR/LC_MESSAGES/mydomain.mo
   140|     """
   141|     import gettext
   142|     global _translations
   143|     global _supported_locales
   144|     global _use_gettext
   145|     _translations = {}
   146|     for lang in os.listdir(directory):
   147|         if lang.startswith('.'):
   148|             continue  # skip .svn, etc
   149|         if os.path.isfile(os.path.join(directory, lang)):
   150|             continue
   151|         try:
   152|             os.stat(os.path.join(directory, lang, "LC_MESSAGES", domain + ".mo"))
   153|             _translations[lang] = gettext.translation(domain, directory,
   154|                                                       languages=[lang])
   155|         except Exception as e:
   156|             gen_log.error("Cannot load translation for '%s': %s", lang, str(e))
   157|             continue
   158|     _supported_locales = frozenset(list(_translations.keys()) + [_default_locale])
   159|     _use_gettext = True
   160|     gen_log.debug("Supported locales: %s", sorted(_supported_locales))
   161| def get_supported_locales():
   162|     """Returns a list of all the supported locale codes."""
   163|     return _supported_locales
   164| class Locale(object):
   165|     """Object representing a locale.
   166|     After calling one of `load_translations` or `load_gettext_translations`,
   167|     call `get` or `get_closest` to get a Locale object.
   168|     """
   169|     @classmethod
   170|     def get_closest(cls, *locale_codes):
   171|         """Returns the closest match for the given locale code."""
   172|         for code in locale_codes:
   173|             if not code:
   174|                 continue
   175|             code = code.replace("-", "_")
   176|             parts = code.split("_")
   177|             if len(parts) > 2:
   178|                 continue
   179|             elif len(parts) == 2:
   180|                 code = parts[0].lower() + "_" + parts[1].upper()
   181|             if code in _supported_locales:
   182|                 return cls.get(code)
   183|             if parts[0].lower() in _supported_locales:
   184|                 return cls.get(parts[0].lower())
   185|         return cls.get(_default_locale)
   186|     @classmethod
   187|     def get(cls, code):
   188|         """Returns the Locale for the given locale code.
   189|         If it is not supported, we raise an exception.
   190|         """
   191|         if not hasattr(cls, "_cache"):
   192|             cls._cache = {}
   193|         if code not in cls._cache:
   194|             assert code in _supported_locales
   195|             translations = _translations.get(code, None)
   196|             if translations is None:
   197|                 locale = CSVLocale(code, {})
   198|             elif _use_gettext:
   199|                 locale = GettextLocale(code, translations)
   200|             else:
   201|                 locale = CSVLocale(code, translations)
   202|             cls._cache[code] = locale
   203|         return cls._cache[code]
   204|     def __init__(self, code, translations):
   205|         self.code = code
   206|         self.name = LOCALE_NAMES.get(code, {}).get("name", u"Unknown")
   207|         self.rtl = False
   208|         for prefix in ["fa", "ar", "he"]:
   209|             if self.code.startswith(prefix):
   210|                 self.rtl = True
   211|                 break
   212|         self.translations = translations
   213|         _ = self.translate
   214|         self._months = [
   215|             _("January"), _("February"), _("March"), _("April"),
   216|             _("May"), _("June"), _("July"), _("August"),
   217|             _("September"), _("October"), _("November"), _("December")]
   218|         self._weekdays = [
   219|             _("Monday"), _("Tuesday"), _("Wednesday"), _("Thursday"),
   220|             _("Friday"), _("Saturday"), _("Sunday")]
   221|     def translate(self, message, plural_message=None, count=None):
   222|         """Returns the translation for the given message for this locale.
   223|         If ``plural_message`` is given, you must also provide
   224|         ``count``. We return ``plural_message`` when ``count != 1``,
   225|         and we return the singular form for the given message when
   226|         ``count == 1``.
   227|         """
   228|         raise NotImplementedError()
   229|     def pgettext(self, context, message, plural_message=None, count=None):
   230|         raise NotImplementedError()
   231|     def format_date(self, date, gmt_offset=0, relative=True, shorter=False,
   232|                     full_format=False):
   233|         """Formats the given date (which should be GMT).
   234|         By default, we return a relative time (e.g., "2 minutes ago"). You
   235|         can return an absolute date string with ``relative=False``.
   236|         You can force a full format date ("July 10, 1980") with
   237|         ``full_format=True``.
   238|         This method is primarily intended for dates in the past.
   239|         For dates in the future, we fall back to full format.
   240|         """
   241|         if isinstance(date, numbers.Real):
   242|             date = datetime.datetime.utcfromtimestamp(date)
   243|         now = datetime.datetime.utcnow()
   244|         if date > now:
   245|             if relative and (date - now).seconds < 60:
   246|                 date = now
   247|             else:
   248|                 full_format = True
   249|         local_date = date - datetime.timedelta(minutes=gmt_offset)
   250|         local_now = now - datetime.timedelta(minutes=gmt_offset)
   251|         local_yesterday = local_now - datetime.timedelta(hours=24)
   252|         difference = now - date
   253|         seconds = difference.seconds
   254|         days = difference.days
   255|         _ = self.translate
   256|         format = None
   257|         if not full_format:
   258|             if relative and days == 0:
   259|                 if seconds < 50:
   260|                     return _("1 second ago", "%(seconds)d seconds ago",
   261|                              seconds) % {"seconds": seconds}
   262|                 if seconds < 50 * 60:
   263|                     minutes = round(seconds / 60.0)
   264|                     return _("1 minute ago", "%(minutes)d minutes ago",
   265|                              minutes) % {"minutes": minutes}
   266|                 hours = round(seconds / (60.0 * 60))
   267|                 return _("1 hour ago", "%(hours)d hours ago",
   268|                          hours) % {"hours": hours}
   269|             if days == 0:
   270|                 format = _("%(time)s")
   271|             elif days == 1 and local_date.day == local_yesterday.day and \
   272|                     relative:
   273|                 format = _("yesterday") if shorter else \
   274|                     _("yesterday at %(time)s")
   275|             elif days < 5:
   276|                 format = _("%(weekday)s") if shorter else \
   277|                     _("%(weekday)s at %(time)s")
   278|             elif days < 334:  # 11mo, since confusing for same month last year
   279|                 format = _("%(month_name)s %(day)s") if shorter else \
   280|                     _("%(month_name)s %(day)s at %(time)s")
   281|         if format is None:
   282|             format = _("%(month_name)s %(day)s, %(year)s") if shorter else \
   283|                 _("%(month_name)s %(day)s, %(year)s at %(time)s")
   284|         tfhour_clock = self.code not in ("en", "en_US", "zh_CN")
   285|         if tfhour_clock:
   286|             str_time = "%d:%02d" % (local_date.hour, local_date.minute)
   287|         elif self.code == "zh_CN":
   288|             str_time = "%s%d:%02d" % (
   289|                 (u'\u4e0a\u5348', u'\u4e0b\u5348')[local_date.hour >= 12],
   290|                 local_date.hour % 12 or 12, local_date.minute)
   291|         else:
   292|             str_time = "%d:%02d %s" % (
   293|                 local_date.hour % 12 or 12, local_date.minute,
   294|                 ("am", "pm")[local_date.hour >= 12])
   295|         return format % {
   296|             "month_name": self._months[local_date.month - 1],
   297|             "weekday": self._weekdays[local_date.weekday()],
   298|             "day": str(local_date.day),
   299|             "year": str(local_date.year),
   300|             "time": str_time
   301|         }
   302|     def format_day(self, date, gmt_offset=0, dow=True):
   303|         """Formats the given date as a day of week.
   304|         Example: "Monday, January 22". You can remove the day of week with
   305|         ``dow=False``.
   306|         """
   307|         local_date = date - datetime.timedelta(minutes=gmt_offset)
   308|         _ = self.translate
   309|         if dow:
   310|             return _("%(weekday)s, %(month_name)s %(day)s") % {
   311|                 "month_name": self._months[local_date.month - 1],
   312|                 "weekday": self._weekdays[local_date.weekday()],
   313|                 "day": str(local_date.day),
   314|             }
   315|         else:
   316|             return _("%(month_name)s %(day)s") % {
   317|                 "month_name": self._months[local_date.month - 1],
   318|                 "day": str(local_date.day),
   319|             }
   320|     def list(self, parts):
   321|         """Returns a comma-separated list for the given list of parts.
   322|         The format is, e.g., "A, B and C", "A and B" or just "A" for lists
   323|         of size 1.
   324|         """
   325|         _ = self.translate
   326|         if len(parts) == 0:
   327|             return ""
   328|         if len(parts) == 1:
   329|             return parts[0]
   330|         comma = u' \u0648 ' if self.code.startswith("fa") else u", "
   331|         return _("%(commas)s and %(last)s") % {
   332|             "commas": comma.join(parts[:-1]),
   333|             "last": parts[len(parts) - 1],
   334|         }
   335|     def friendly_number(self, value):
   336|         """Returns a comma-separated number for the given integer."""
   337|         if self.code not in ("en", "en_US"):
   338|             return str(value)
   339|         value = str(value)
   340|         parts = []
   341|         while value:
   342|             parts.append(value[-3:])
   343|             value = value[:-3]
   344|         return ",".join(reversed(parts))
   345| class CSVLocale(Locale):
   346|     """Locale implementation using tornado's CSV translation format."""
   347|     def translate(self, message, plural_message=None, count=None):
   348|         if plural_message is not None:
   349|             assert count is not None
   350|             if count != 1:
   351|                 message = plural_message
   352|                 message_dict = self.translations.get("plural", {})
   353|             else:
   354|                 message_dict = self.translations.get("singular", {})
   355|         else:
   356|             message_dict = self.translations.get("unknown", {})
   357|         return message_dict.get(message, message)
   358|     def pgettext(self, context, message, plural_message=None, count=None):
   359|         if self.translations:
   360|             gen_log.warning('pgettext is not supported by CSVLocale')
   361|         return self.translate(message, plural_message, count)
   362| class GettextLocale(Locale):
   363|     """Locale implementation using the `gettext` module."""
   364|     def __init__(self, code, translations):
   365|         try:
   366|             self.ngettext = translations.ungettext
   367|             self.gettext = translations.ugettext
   368|         except AttributeError:
   369|             self.ngettext = translations.ngettext
   370|             self.gettext = translations.gettext
   371|         super(GettextLocale, self).__init__(code, translations)
   372|     def translate(self, message, plural_message=None, count=None):
   373|         if plural_message is not None:
   374|             assert count is not None
   375|             return self.ngettext(message, plural_message, count)
   376|         else:
   377|             return self.gettext(message)
   378|     def pgettext(self, context, message, plural_message=None, count=None):
   379|         """Allows to set context for translation, accepts plural forms.
   380|         Usage example::
   381|             pgettext("law", "right")
   382|             pgettext("good", "right")
   383|         Plural message example::
   384|             pgettext("organization", "club", "clubs", len(clubs))
   385|             pgettext("stick", "club", "clubs", len(clubs))
   386|         To generate POT file with context, add following options to step 1
   387|         of `load_gettext_translations` sequence::
   388|             xgettext [basic options] --keyword=pgettext:1c,2 --keyword=pgettext:1c,2,3
   389|         .. versionadded:: 4.2
   390|         """
   391|         if plural_message is not None:
   392|             assert count is not None
   393|             msgs_with_ctxt = ("%s%s%s" % (context, CONTEXT_SEPARATOR, message),
   394|                               "%s%s%s" % (context, CONTEXT_SEPARATOR, plural_message),
   395|                               count)
   396|             result = self.ngettext(*msgs_with_ctxt)
   397|             if CONTEXT_SEPARATOR in result:
   398|                 result = self.ngettext(message, plural_message, count)
   399|             return result
   400|         else:
   401|             msg_with_ctxt = "%s%s%s" % (context, CONTEXT_SEPARATOR, message)
   402|             result = self.gettext(msg_with_ctxt)
   403|             if CONTEXT_SEPARATOR in result:
   404|                 result = message
   405|             return result


# ====================================================================
# FILE: salt/ext/tornado/locks.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-361 ---
     1| from __future__ import absolute_import, division, print_function
     2| import collections
     3| from salt.ext.tornado import gen, ioloop
     4| from salt.ext.tornado.concurrent import Future
     5| __all__ = ['Condition', 'Event', 'Semaphore', 'BoundedSemaphore', 'Lock']
     6| class _TimeoutGarbageCollector(object):
     7|     """Base class for objects that periodically clean up timed-out waiters.
     8|     Avoids memory leak in a common pattern like:
     9|         while True:
    10|             yield condition.wait(short_timeout)
    11|             print('looping....')
    12|     """
    13|     def __init__(self):
    14|         self._waiters = collections.deque()  # Futures.
    15|         self._timeouts = 0
    16|     def _garbage_collect(self):
    17|         self._timeouts += 1
    18|         if self._timeouts > 100:
    19|             self._timeouts = 0
    20|             self._waiters = collections.deque(
    21|                 w for w in self._waiters if not w.done())
    22| class Condition(_TimeoutGarbageCollector):
    23|     """A condition allows one or more coroutines to wait until notified.
    24|     Like a standard `threading.Condition`, but does not need an underlying lock
    25|     that is acquired and released.
    26|     With a `Condition`, coroutines can wait to be notified by other coroutines:
    27|     .. testcode::
    28|         from salt.ext.tornado import gen
    29|         from salt.ext.tornado.ioloop import IOLoop
    30|         from salt.ext.tornado.locks import Condition
    31|         condition = Condition()
    32|         @gen.coroutine
    33|         def waiter():
    34|             print("I'll wait right here")
    35|             yield condition.wait()  # Yield a Future.
    36|             print("I'm done waiting")
    37|         @gen.coroutine
    38|         def notifier():
    39|             print("About to notify")
    40|             condition.notify()
    41|             print("Done notifying")
    42|         @gen.coroutine
    43|         def runner():
    44|             yield [waiter(), notifier()]
    45|         IOLoop.current().run_sync(runner)
    46|     .. testoutput::
    47|         I'll wait right here
    48|         About to notify
    49|         Done notifying
    50|         I'm done waiting
    51|     `wait` takes an optional ``timeout`` argument, which is either an absolute
    52|     timestamp::
    53|         io_loop = IOLoop.current()
    54|         yield condition.wait(timeout=io_loop.time() + 1)
    55|     ...or a `datetime.timedelta` for a timeout relative to the current time::
    56|         yield condition.wait(timeout=datetime.timedelta(seconds=1))
    57|     The method raises `tornado.gen.TimeoutError` if there's no notification
    58|     before the deadline.
    59|     """
    60|     def __init__(self):
    61|         super(Condition, self).__init__()
    62|         self.io_loop = ioloop.IOLoop.current()
    63|     def __repr__(self):
    64|         result = '<%s' % (self.__class__.__name__, )
    65|         if self._waiters:
    66|             result += ' waiters[%s]' % len(self._waiters)
    67|         return result + '>'
    68|     def wait(self, timeout=None):
    69|         """Wait for `.notify`.
    70|         Returns a `.Future` that resolves ``True`` if the condition is notified,
    71|         or ``False`` after a timeout.
    72|         """
    73|         waiter = Future()
    74|         self._waiters.append(waiter)
    75|         if timeout:
    76|             def on_timeout():
    77|                 waiter.set_result(False)
    78|                 self._garbage_collect()
    79|             io_loop = ioloop.IOLoop.current()
    80|             timeout_handle = io_loop.add_timeout(timeout, on_timeout)
    81|             waiter.add_done_callback(
    82|                 lambda _: io_loop.remove_timeout(timeout_handle))
    83|         return waiter
    84|     def notify(self, n=1):
    85|         """Wake ``n`` waiters."""
    86|         waiters = []  # Waiters we plan to run right now.
    87|         while n and self._waiters:
    88|             waiter = self._waiters.popleft()
    89|             if not waiter.done():  # Might have timed out.
    90|                 n -= 1
    91|                 waiters.append(waiter)
    92|         for waiter in waiters:
    93|             waiter.set_result(True)
    94|     def notify_all(self):
    95|         """Wake all waiters."""
    96|         self.notify(len(self._waiters))
    97| class Event(object):
    98|     """An event blocks coroutines until its internal flag is set to True.
    99|     Similar to `threading.Event`.
   100|     A coroutine can wait for an event to be set. Once it is set, calls to
   101|     ``yield event.wait()`` will not block unless the event has been cleared:
   102|     .. testcode::
   103|         from salt.ext.tornado import gen
   104|         from salt.ext.tornado.ioloop import IOLoop
   105|         from salt.ext.tornado.locks import Event
   106|         event = Event()
   107|         @gen.coroutine
   108|         def waiter():
   109|             print("Waiting for event")
   110|             yield event.wait()
   111|             print("Not waiting this time")
   112|             yield event.wait()
   113|             print("Done")
   114|         @gen.coroutine
   115|         def setter():
   116|             print("About to set the event")
   117|             event.set()
   118|         @gen.coroutine
   119|         def runner():
   120|             yield [waiter(), setter()]
   121|         IOLoop.current().run_sync(runner)
   122|     .. testoutput::
   123|         Waiting for event
   124|         About to set the event
   125|         Not waiting this time
   126|         Done
   127|     """
   128|     def __init__(self):
   129|         self._future = Future()
   130|     def __repr__(self):
   131|         return '<%s %s>' % (
   132|             self.__class__.__name__, 'set' if self.is_set() else 'clear')
   133|     def is_set(self):
   134|         """Return ``True`` if the internal flag is true."""
   135|         return self._future.done()
   136|     def set(self):
   137|         """Set the internal flag to ``True``. All waiters are awakened.
   138|         Calling `.wait` once the flag is set will not block.
   139|         """
   140|         if not self._future.done():
   141|             self._future.set_result(None)
   142|     def clear(self):
   143|         """Reset the internal flag to ``False``.
   144|         Calls to `.wait` will block until `.set` is called.
   145|         """
   146|         if self._future.done():
   147|             self._future = Future()
   148|     def wait(self, timeout=None):
   149|         """Block until the internal flag is true.
   150|         Returns a Future, which raises `tornado.gen.TimeoutError` after a
   151|         timeout.
   152|         """
   153|         if timeout is None:
   154|             return self._future
   155|         else:
   156|             return gen.with_timeout(timeout, self._future)
   157| class _ReleasingContextManager(object):
   158|     """Releases a Lock or Semaphore at the end of a "with" statement.
   159|         with (yield semaphore.acquire()):
   160|             pass
   161|     """
   162|     def __init__(self, obj):
   163|         self._obj = obj
   164|     def __enter__(self):
   165|         pass
   166|     def __exit__(self, exc_type, exc_val, exc_tb):
   167|         self._obj.release()
   168| class Semaphore(_TimeoutGarbageCollector):
   169|     """A lock that can be acquired a fixed number of times before blocking.
   170|     A Semaphore manages a counter representing the number of `.release` calls
   171|     minus the number of `.acquire` calls, plus an initial value. The `.acquire`
   172|     method blocks if necessary until it can return without making the counter
   173|     negative.
   174|     Semaphores limit access to a shared resource. To allow access for two
   175|     workers at a time:
   176|     .. testsetup:: semaphore
   177|        from collections import deque
   178|        from salt.ext.tornado import gen
   179|        from salt.ext.tornado.ioloop import IOLoop
   180|        from salt.ext.tornado.concurrent import Future
   181|        futures_q = deque([Future() for _ in range(3)])
   182|        @gen.coroutine
   183|        def simulator(futures):
   184|            for f in futures:
   185|                yield gen.moment
   186|                f.set_result(None)
   187|        IOLoop.current().add_callback(simulator, list(futures_q))
   188|        def use_some_resource():
   189|            return futures_q.popleft()
   190|     .. testcode:: semaphore
   191|         from salt.ext.tornado import gen
   192|         from salt.ext.tornado.ioloop import IOLoop
   193|         from salt.ext.tornado.locks import Semaphore
   194|         sem = Semaphore(2)
   195|         @gen.coroutine
   196|         def worker(worker_id):
   197|             yield sem.acquire()
   198|             try:
   199|                 print("Worker %d is working" % worker_id)
   200|                 yield use_some_resource()
   201|             finally:
   202|                 print("Worker %d is done" % worker_id)
   203|                 sem.release()
   204|         @gen.coroutine
   205|         def runner():
   206|             yield [worker(i) for i in range(3)]
   207|         IOLoop.current().run_sync(runner)
   208|     .. testoutput:: semaphore
   209|         Worker 0 is working
   210|         Worker 1 is working
   211|         Worker 0 is done
   212|         Worker 2 is working
   213|         Worker 1 is done
   214|         Worker 2 is done
   215|     Workers 0 and 1 are allowed to run concurrently, but worker 2 waits until
   216|     the semaphore has been released once, by worker 0.
   217|     `.acquire` is a context manager, so ``worker`` could be written as::
   218|         @gen.coroutine
   219|         def worker(worker_id):
   220|             with (yield sem.acquire()):
   221|                 print("Worker %d is working" % worker_id)
   222|                 yield use_some_resource()
   223|             print("Worker %d is done" % worker_id)
   224|     In Python 3.5, the semaphore itself can be used as an async context
   225|     manager::
   226|         async def worker(worker_id):
   227|             async with sem:
   228|                 print("Worker %d is working" % worker_id)
   229|                 await use_some_resource()
   230|             print("Worker %d is done" % worker_id)
   231|     .. versionchanged:: 4.3
   232|        Added ``async with`` support in Python 3.5.
   233|     """
   234|     def __init__(self, value=1):
   235|         super(Semaphore, self).__init__()
   236|         if value < 0:
   237|             raise ValueError('semaphore initial value must be >= 0')
   238|         self._value = value
   239|     def __repr__(self):
   240|         res = super(Semaphore, self).__repr__()
   241|         extra = 'locked' if self._value == 0 else 'unlocked,value:{0}'.format(
   242|             self._value)
   243|         if self._waiters:
   244|             extra = '{0},waiters:{1}'.format(extra, len(self._waiters))
   245|         return '<{0} [{1}]>'.format(res[1:-1], extra)
   246|     def release(self):
   247|         """Increment the counter and wake one waiter."""
   248|         self._value += 1
   249|         while self._waiters:
   250|             waiter = self._waiters.popleft()
   251|             if not waiter.done():
   252|                 self._value -= 1
   253|                 waiter.set_result(_ReleasingContextManager(self))
   254|                 break
   255|     def acquire(self, timeout=None):
   256|         """Decrement the counter. Returns a Future.
   257|         Block if the counter is zero and wait for a `.release`. The Future
   258|         raises `.TimeoutError` after the deadline.
   259|         """
   260|         waiter = Future()
   261|         if self._value > 0:
   262|             self._value -= 1
   263|             waiter.set_result(_ReleasingContextManager(self))
   264|         else:
   265|             self._waiters.append(waiter)
   266|             if timeout:
   267|                 def on_timeout():
   268|                     waiter.set_exception(gen.TimeoutError())
   269|                     self._garbage_collect()
   270|                 io_loop = ioloop.IOLoop.current()
   271|                 timeout_handle = io_loop.add_timeout(timeout, on_timeout)
   272|                 waiter.add_done_callback(
   273|                     lambda _: io_loop.remove_timeout(timeout_handle))
   274|         return waiter
   275|     def __enter__(self):
   276|         raise RuntimeError(
   277|             "Use Semaphore like 'with (yield semaphore.acquire())', not like"
   278|             " 'with semaphore'")
   279|     __exit__ = __enter__
   280|     @gen.coroutine
   281|     def __aenter__(self):
   282|         yield self.acquire()
   283|     @gen.coroutine
   284|     def __aexit__(self, typ, value, tb):
   285|         self.release()
   286| class BoundedSemaphore(Semaphore):
   287|     """A semaphore that prevents release() being called too many times.
   288|     If `.release` would increment the semaphore's value past the initial
   289|     value, it raises `ValueError`. Semaphores are mostly used to guard
   290|     resources with limited capacity, so a semaphore released too many times
   291|     is a sign of a bug.
   292|     """
   293|     def __init__(self, value=1):
   294|         super(BoundedSemaphore, self).__init__(value=value)
   295|         self._initial_value = value
   296|     def release(self):
   297|         """Increment the counter and wake one waiter."""
   298|         if self._value >= self._initial_value:
   299|             raise ValueError("Semaphore released too many times")
   300|         super(BoundedSemaphore, self).release()
   301| class Lock(object):
   302|     """A lock for coroutines.
   303|     A Lock begins unlocked, and `acquire` locks it immediately. While it is
   304|     locked, a coroutine that yields `acquire` waits until another coroutine
   305|     calls `release`.
   306|     Releasing an unlocked lock raises `RuntimeError`.
   307|     `acquire` supports the context manager protocol in all Python versions:
   308|     >>> from salt.ext.tornado import gen, locks
   309|     >>> lock = locks.Lock()
   310|     >>>
   311|     >>> @gen.coroutine
   312|     ... def f():
   313|     ...    with (yield lock.acquire()):
   314|     ...        # Do something holding the lock.
   315|     ...        pass
   316|     ...
   317|     ...    # Now the lock is released.
   318|     In Python 3.5, `Lock` also supports the async context manager
   319|     protocol. Note that in this case there is no `acquire`, because
   320|     ``async with`` includes both the ``yield`` and the ``acquire``
   321|     (just as it does with `threading.Lock`):
   322|     >>> async def f():  # doctest: +SKIP
   323|     ...    async with lock:
   324|     ...        # Do something holding the lock.
   325|     ...        pass
   326|     ...
   327|     ...    # Now the lock is released.
   328|     .. versionchanged:: 4.3
   329|        Added ``async with`` support in Python 3.5.
   330|     """
   331|     def __init__(self):
   332|         self._block = BoundedSemaphore(value=1)
   333|     def __repr__(self):
   334|         return "<%s _block=%s>" % (
   335|             self.__class__.__name__,
   336|             self._block)
   337|     def acquire(self, timeout=None):
   338|         """Attempt to lock. Returns a Future.
   339|         Returns a Future, which raises `tornado.gen.TimeoutError` after a
   340|         timeout.
   341|         """
   342|         return self._block.acquire(timeout)
   343|     def release(self):
   344|         """Unlock.
   345|         The first coroutine in line waiting for `acquire` gets the lock.
   346|         If not locked, raise a `RuntimeError`.
   347|         """
   348|         try:
   349|             self._block.release()
   350|         except ValueError:
   351|             raise RuntimeError('release unlocked lock')
   352|     def __enter__(self):
   353|         raise RuntimeError(
   354|             "Use Lock like 'with (yield lock)', not like 'with lock'")
   355|     __exit__ = __enter__
   356|     @gen.coroutine
   357|     def __aenter__(self):
   358|         yield self.acquire()
   359|     @gen.coroutine
   360|     def __aexit__(self, typ, value, tb):
   361|         self.release()


# ====================================================================
# FILE: salt/ext/tornado/log.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-201 ---
     1| """Logging support for Tornado.
     2| Tornado uses three logger streams:
     3| * ``tornado.access``: Per-request logging for Tornado's HTTP servers (and
     4|   potentially other servers in the future)
     5| * ``tornado.application``: Logging of errors from application code (i.e.
     6|   uncaught exceptions from callbacks)
     7| * ``tornado.general``: General-purpose logging, including any errors
     8|   or warnings from Tornado itself.
     9| These streams may be configured independently using the standard library's
    10| `logging` module.  For example, you may wish to send ``tornado.access`` logs
    11| to a separate file for analysis.
    12| """
    13| from __future__ import absolute_import, division, print_function
    14| import logging
    15| import logging.handlers
    16| import sys
    17| from salt.ext.tornado.escape import _unicode
    18| from salt.ext.tornado.util import unicode_type, basestring_type
    19| try:
    20|     import colorama
    21| except ImportError:
    22|     colorama = None
    23| try:
    24|     import curses  # type: ignore
    25| except ImportError:
    26|     curses = None
    27| access_log = logging.getLogger("tornado.access")
    28| app_log = logging.getLogger("tornado.application")
    29| gen_log = logging.getLogger("tornado.general")
    30| def _stderr_supports_color():
    31|     try:
    32|         if hasattr(sys.stderr, 'isatty') and sys.stderr.isatty():
    33|             if curses:
    34|                 curses.setupterm()
    35|                 if curses.tigetnum("colors") > 0:
    36|                     return True
    37|             elif colorama:
    38|                 if sys.stderr is getattr(colorama.initialise, 'wrapped_stderr',
    39|                                          object()):
    40|                     return True
    41|     except Exception:
    42|         pass
    43|     return False
    44| def _safe_unicode(s):
    45|     try:
    46|         return _unicode(s)
    47|     except UnicodeDecodeError:
    48|         return repr(s)
    49| class LogFormatter(logging.Formatter):
    50|     """Log formatter used in Tornado.
    51|     Key features of this formatter are:
    52|     * Color support when logging to a terminal that supports it.
    53|     * Timestamps on every log line.
    54|     * Robust against str/bytes encoding problems.
    55|     This formatter is enabled automatically by
    56|     `tornado.options.parse_command_line` or `tornado.options.parse_config_file`
    57|     (unless ``--logging=none`` is used).
    58|     Color support on Windows versions that do not support ANSI color codes is
    59|     enabled by use of the colorama__ library. Applications that wish to use
    60|     this must first initialize colorama with a call to ``colorama.init``.
    61|     See the colorama documentation for details.
    62|     __ https://pypi.python.org/pypi/colorama
    63|     .. versionchanged:: 4.5
    64|        Added support for ``colorama``. Changed the constructor
    65|        signature to be compatible with `logging.config.dictConfig`.
    66|     """
    67|     DEFAULT_FORMAT = '%(color)s[%(levelname)1.1s %(asctime)s %(module)s:%(lineno)d]%(end_color)s %(message)s'
    68|     DEFAULT_DATE_FORMAT = '%y%m%d %H:%M:%S'
    69|     DEFAULT_COLORS = {
    70|         logging.DEBUG: 4,  # Blue
    71|         logging.INFO: 2,  # Green
    72|         logging.WARNING: 3,  # Yellow
    73|         logging.ERROR: 1,  # Red
    74|     }
    75|     def __init__(self, fmt=DEFAULT_FORMAT, datefmt=DEFAULT_DATE_FORMAT,
    76|                  style='%', color=True, colors=DEFAULT_COLORS):
    77|         r"""
    78|         :arg bool color: Enables color support.
    79|         :arg string fmt: Log message format.
    80|           It will be applied to the attributes dict of log records. The
    81|           text between ``%(color)s`` and ``%(end_color)s`` will be colored
    82|           depending on the level if color support is on.
    83|         :arg dict colors: color mappings from logging level to terminal color
    84|           code
    85|         :arg string datefmt: Datetime format.
    86|           Used for formatting ``(asctime)`` placeholder in ``prefix_fmt``.
    87|         .. versionchanged:: 3.2
    88|            Added ``fmt`` and ``datefmt`` arguments.
    89|         """
    90|         logging.Formatter.__init__(self, datefmt=datefmt)
    91|         self._fmt = fmt
    92|         self._colors = {}
    93|         if color and _stderr_supports_color():
    94|             if curses is not None:
    95|                 fg_color = (curses.tigetstr("setaf") or
    96|                             curses.tigetstr("setf") or "")
    97|                 if (3, 0) < sys.version_info < (3, 2, 3):
    98|                     fg_color = unicode_type(fg_color, "ascii")
    99|                 for levelno, code in colors.items():
   100|                     self._colors[levelno] = unicode_type(curses.tparm(fg_color, code), "ascii")
   101|                 self._normal = unicode_type(curses.tigetstr("sgr0"), "ascii")
   102|             else:
   103|                 for levelno, code in colors.items():
   104|                     self._colors[levelno] = '\033[2;3%dm' % code
   105|                 self._normal = '\033[0m'
   106|         else:
   107|             self._normal = ''
   108|     def format(self, record):
   109|         try:
   110|             message = record.getMessage()
   111|             assert isinstance(message, basestring_type)  # guaranteed by logging
   112|             record.message = _safe_unicode(message)
   113|         except Exception as e:
   114|             record.message = "Bad message (%r): %r" % (e, record.__dict__)
   115|         record.asctime = self.formatTime(record, self.datefmt)
   116|         if record.levelno in self._colors:
   117|             record.color = self._colors[record.levelno]
   118|             record.end_color = self._normal
   119|         else:
   120|             record.color = record.end_color = ''
   121|         formatted = self._fmt % record.__dict__
   122|         if record.exc_info:
   123|             if not record.exc_text:
   124|                 record.exc_text = self.formatException(record.exc_info)
   125|         if record.exc_text:
   126|             lines = [formatted.rstrip()]
   127|             lines.extend(_safe_unicode(ln) for ln in record.exc_text.split('\n'))
   128|             formatted = '\n'.join(lines)
   129|         return formatted.replace("\n", "\n    ")
   130| def enable_pretty_logging(options=None, logger=None):
   131|     """Turns on formatted logging output as configured.
   132|     This is called automatically by `tornado.options.parse_command_line`
   133|     and `tornado.options.parse_config_file`.
   134|     """
   135|     if options is None:
   136|         import salt.ext.tornado.options
   137|         options = salt.ext.tornado.options.options
   138|     if options.logging is None or options.logging.lower() == 'none':
   139|         return
   140|     if logger is None:
   141|         logger = logging.getLogger()
   142|     logger.setLevel(getattr(logging, options.logging.upper()))
   143|     if options.log_file_prefix:
   144|         rotate_mode = options.log_rotate_mode
   145|         if rotate_mode == 'size':
   146|             channel = logging.handlers.RotatingFileHandler(
   147|                 filename=options.log_file_prefix,
   148|                 maxBytes=options.log_file_max_size,
   149|                 backupCount=options.log_file_num_backups)
   150|         elif rotate_mode == 'time':
   151|             channel = logging.handlers.TimedRotatingFileHandler(
   152|                 filename=options.log_file_prefix,
   153|                 when=options.log_rotate_when,
   154|                 interval=options.log_rotate_interval,
   155|                 backupCount=options.log_file_num_backups)
   156|         else:
   157|             error_message = 'The value of log_rotate_mode option should be ' +\
   158|                             '"size" or "time", not "%s".' % rotate_mode
   159|             raise ValueError(error_message)
   160|         channel.setFormatter(LogFormatter(color=False))
   161|         logger.addHandler(channel)
   162|     if (options.log_to_stderr or
   163|             (options.log_to_stderr is None and not logger.handlers)):
   164|         channel = logging.StreamHandler()
   165|         channel.setFormatter(LogFormatter())
   166|         logger.addHandler(channel)
   167| def define_logging_options(options=None):
   168|     """Add logging-related flags to ``options``.
   169|     These options are present automatically on the default options instance;
   170|     this method is only necessary if you have created your own `.OptionParser`.
   171|     .. versionadded:: 4.2
   172|         This function existed in prior versions but was broken and undocumented until 4.2.
   173|     """
   174|     if options is None:
   175|         import salt.ext.tornado.options
   176|         options = salt.ext.tornado.options.options
   177|     options.define("logging", default="info",
   178|                    help=("Set the Python log level. If 'none', tornado won't touch the "
   179|                          "logging configuration."),
   180|                    metavar="debug|info|warning|error|none")
   181|     options.define("log_to_stderr", type=bool, default=None,
   182|                    help=("Send log output to stderr (colorized if possible). "
   183|                          "By default use stderr if --log_file_prefix is not set and "
   184|                          "no other logging is configured."))
   185|     options.define("log_file_prefix", type=str, default=None, metavar="PATH",
   186|                    help=("Path prefix for log files. "
   187|                          "Note that if you are running multiple tornado processes, "
   188|                          "log_file_prefix must be different for each of them (e.g. "
   189|                          "include the port number)"))
   190|     options.define("log_file_max_size", type=int, default=100 * 1000 * 1000,
   191|                    help="max size of log files before rollover")
   192|     options.define("log_file_num_backups", type=int, default=10,
   193|                    help="number of log files to keep")
   194|     options.define("log_rotate_when", type=str, default='midnight',
   195|                    help=("specify the type of TimedRotatingFileHandler interval "
   196|                          "other options:('S', 'M', 'H', 'D', 'W0'-'W6')"))
   197|     options.define("log_rotate_interval", type=int, default=1,
   198|                    help="The interval value of timed rotating")
   199|     options.define("log_rotate_mode", type=str, default='size',
   200|                    help="The mode of rotating files(time or size)")
   201|     options.add_parse_callback(lambda: enable_pretty_logging(options))


# ====================================================================
# FILE: salt/ext/tornado/netutil.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-356 ---
     1| """Miscellaneous network utility code."""
     2| from __future__ import absolute_import, division, print_function
     3| import errno
     4| import os
     5| import sys
     6| import socket
     7| import stat
     8| from salt.ext.tornado.concurrent import dummy_executor, run_on_executor
     9| from salt.ext.tornado.ioloop import IOLoop
    10| from salt.ext.tornado.platform.auto import set_close_exec
    11| from salt.ext.tornado.util import PY3, Configurable, errno_from_exception
    12| try:
    13|     import ssl
    14| except ImportError:
    15|     ssl = None
    16| try:
    17|     import certifi
    18| except ImportError:
    19|     if ssl is None or hasattr(ssl, 'create_default_context'):
    20|         certifi = None
    21|     else:
    22|         raise
    23| if PY3:
    24|     xrange = range
    25| if hasattr(ssl, 'match_hostname') and hasattr(ssl, 'CertificateError'):  # python 3.2+
    26|     ssl_match_hostname = ssl.match_hostname
    27|     SSLCertificateError = ssl.CertificateError
    28| elif ssl is None:
    29|     ssl_match_hostname = SSLCertificateError = None  # type: ignore
    30| else:
    31|     import backports.ssl_match_hostname
    32|     ssl_match_hostname = backports.ssl_match_hostname.match_hostname
    33|     SSLCertificateError = backports.ssl_match_hostname.CertificateError  # type: ignore
    34| if hasattr(ssl, 'SSLContext'):
    35|     if hasattr(ssl, 'create_default_context'):
    36|         _client_ssl_defaults = ssl.create_default_context(
    37|             ssl.Purpose.SERVER_AUTH)
    38|         _server_ssl_defaults = ssl.create_default_context(
    39|             ssl.Purpose.CLIENT_AUTH)
    40|     else:
    41|         _client_ssl_defaults = ssl.SSLContext(ssl.PROTOCOL_SSLv23)
    42|         _client_ssl_defaults.verify_mode = ssl.CERT_REQUIRED
    43|         _client_ssl_defaults.load_verify_locations(certifi.where())
    44|         _server_ssl_defaults = ssl.SSLContext(ssl.PROTOCOL_SSLv23)
    45|         if hasattr(ssl, 'OP_NO_COMPRESSION'):
    46|             _client_ssl_defaults.options |= ssl.OP_NO_COMPRESSION
    47|             _server_ssl_defaults.options |= ssl.OP_NO_COMPRESSION
    48| elif ssl:
    49|     _client_ssl_defaults = dict(cert_reqs=ssl.CERT_REQUIRED,
    50|                                 ca_certs=certifi.where())
    51|     _server_ssl_defaults = {}
    52| else:
    53|     _client_ssl_defaults = dict(cert_reqs=None,
    54|                                 ca_certs=None)
    55|     _server_ssl_defaults = {}
    56| u'foo'.encode('idna')
    57| u'foo'.encode('latin1')
    58| _ERRNO_WOULDBLOCK = (errno.EWOULDBLOCK, errno.EAGAIN)
    59| if hasattr(errno, "WSAEWOULDBLOCK"):
    60|     _ERRNO_WOULDBLOCK += (errno.WSAEWOULDBLOCK,)  # type: ignore
    61| _DEFAULT_BACKLOG = 128
    62| def bind_sockets(port, address=None, family=socket.AF_UNSPEC,
    63|                  backlog=_DEFAULT_BACKLOG, flags=None, reuse_port=False):
    64|     """Creates listening sockets bound to the given port and address.
    65|     Returns a list of socket objects (multiple sockets are returned if
    66|     the given address maps to multiple IP addresses, which is most common
    67|     for mixed IPv4 and IPv6 use).
    68|     Address may be either an IP address or hostname.  If it's a hostname,
    69|     the server will listen on all IP addresses associated with the
    70|     name.  Address may be an empty string or None to listen on all
    71|     available interfaces.  Family may be set to either `socket.AF_INET`
    72|     or `socket.AF_INET6` to restrict to IPv4 or IPv6 addresses, otherwise
    73|     both will be used if available.
    74|     The ``backlog`` argument has the same meaning as for
    75|     `socket.listen() <socket.socket.listen>`.
    76|     ``flags`` is a bitmask of AI_* flags to `~socket.getaddrinfo`, like
    77|     ``socket.AI_PASSIVE | socket.AI_NUMERICHOST``.
    78|     ``reuse_port`` option sets ``SO_REUSEPORT`` option for every socket
    79|     in the list. If your platform doesn't support this option ValueError will
    80|     be raised.
    81|     """
    82|     if reuse_port and not hasattr(socket, "SO_REUSEPORT"):
    83|         raise ValueError("the platform doesn't support SO_REUSEPORT")
    84|     sockets = []
    85|     if address == "":
    86|         address = None
    87|     if not socket.has_ipv6 and family == socket.AF_UNSPEC:
    88|         family = socket.AF_INET
    89|     if flags is None:
    90|         flags = socket.AI_PASSIVE
    91|     bound_port = None
    92|     for res in set(socket.getaddrinfo(address, port, family, socket.SOCK_STREAM,
    93|                                       0, flags)):
    94|         af, socktype, proto, canonname, sockaddr = res
    95|         if (sys.platform == 'darwin' and address == 'localhost' and
    96|                 af == socket.AF_INET6 and sockaddr[3] != 0):
    97|             continue
    98|         try:
    99|             sock = socket.socket(af, socktype, proto)
   100|         except socket.error as e:
   101|             if errno_from_exception(e) == errno.EAFNOSUPPORT:
   102|                 continue
   103|             raise
   104|         set_close_exec(sock.fileno())
   105|         if os.name != 'nt':
   106|             sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
   107|         if reuse_port:
   108|             sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)
   109|         if af == socket.AF_INET6:
   110|             if hasattr(socket, "IPPROTO_IPV6"):
   111|                 sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 1)
   112|         host, requested_port = sockaddr[:2]
   113|         if requested_port == 0 and bound_port is not None:
   114|             sockaddr = tuple([host, bound_port] + list(sockaddr[2:]))
   115|         sock.setblocking(0)
   116|         sock.bind(sockaddr)
   117|         bound_port = sock.getsockname()[1]
   118|         sock.listen(backlog)
   119|         sockets.append(sock)
   120|     return sockets
   121| if hasattr(socket, 'AF_UNIX'):
   122|     def bind_unix_socket(file, mode=0o600, backlog=_DEFAULT_BACKLOG):
   123|         """Creates a listening unix socket.
   124|         If a socket with the given name already exists, it will be deleted.
   125|         If any other file with that name exists, an exception will be
   126|         raised.
   127|         Returns a socket object (not a list of socket objects like
   128|         `bind_sockets`)
   129|         """
   130|         sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
   131|         set_close_exec(sock.fileno())
   132|         sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
   133|         sock.setblocking(0)
   134|         try:
   135|             st = os.stat(file)
   136|         except OSError as err:
   137|             if errno_from_exception(err) != errno.ENOENT:
   138|                 raise
   139|         else:
   140|             if stat.S_ISSOCK(st.st_mode):
   141|                 os.remove(file)
   142|             else:
   143|                 raise ValueError("File %s exists and is not a socket", file)
   144|         sock.bind(file)
   145|         os.chmod(file, mode)
   146|         sock.listen(backlog)
   147|         return sock
   148| def add_accept_handler(sock, callback, io_loop=None):
   149|     """Adds an `.IOLoop` event handler to accept new connections on ``sock``.
   150|     When a connection is accepted, ``callback(connection, address)`` will
   151|     be run (``connection`` is a socket object, and ``address`` is the
   152|     address of the other end of the connection).  Note that this signature
   153|     is different from the ``callback(fd, events)`` signature used for
   154|     `.IOLoop` handlers.
   155|     .. versionchanged:: 4.1
   156|        The ``io_loop`` argument is deprecated.
   157|     """
   158|     if io_loop is None:
   159|         io_loop = IOLoop.current()
   160|     def accept_handler(fd, events):
   161|         for i in xrange(_DEFAULT_BACKLOG):
   162|             try:
   163|                 connection, address = sock.accept()
   164|             except socket.error as e:
   165|                 if errno_from_exception(e) in _ERRNO_WOULDBLOCK:
   166|                     return
   167|                 if errno_from_exception(e) == errno.ECONNABORTED:
   168|                     continue
   169|                 raise
   170|             set_close_exec(connection.fileno())
   171|             callback(connection, address)
   172|     io_loop.add_handler(sock, accept_handler, IOLoop.READ)
   173| def is_valid_ip(ip):
   174|     """Returns true if the given string is a well-formed IP address.
   175|     Supports IPv4 and IPv6.
   176|     """
   177|     if not ip or '\x00' in ip:
   178|         return False
   179|     try:
   180|         res = socket.getaddrinfo(ip, 0, socket.AF_UNSPEC,
   181|                                  socket.SOCK_STREAM,
   182|                                  0, socket.AI_NUMERICHOST)
   183|         return bool(res)
   184|     except socket.gaierror as e:
   185|         if e.args[0] == socket.EAI_NONAME:
   186|             return False
   187|         raise
   188|     return True
   189| class Resolver(Configurable):
   190|     """Configurable asynchronous DNS resolver interface.
   191|     By default, a blocking implementation is used (which simply calls
   192|     `socket.getaddrinfo`).  An alternative implementation can be
   193|     chosen with the `Resolver.configure <.Configurable.configure>`
   194|     class method::
   195|         Resolver.configure('tornado.netutil.ThreadedResolver')
   196|     The implementations of this interface included with Tornado are
   197|     * `tornado.netutil.BlockingResolver`
   198|     * `tornado.netutil.ThreadedResolver`
   199|     * `tornado.netutil.OverrideResolver`
   200|     * `tornado.platform.twisted.TwistedResolver`
   201|     * `tornado.platform.caresresolver.CaresResolver`
   202|     """
   203|     @classmethod
   204|     def configurable_base(cls):
   205|         return Resolver
   206|     @classmethod
   207|     def configurable_default(cls):
   208|         return BlockingResolver
   209|     def resolve(self, host, port, family=socket.AF_UNSPEC, callback=None):
   210|         """Resolves an address.
   211|         The ``host`` argument is a string which may be a hostname or a
   212|         literal IP address.
   213|         Returns a `.Future` whose result is a list of (family,
   214|         address) pairs, where address is a tuple suitable to pass to
   215|         `socket.connect <socket.socket.connect>` (i.e. a ``(host,
   216|         port)`` pair for IPv4; additional fields may be present for
   217|         IPv6). If a ``callback`` is passed, it will be run with the
   218|         result as an argument when it is complete.
   219|         :raises IOError: if the address cannot be resolved.
   220|         .. versionchanged:: 4.4
   221|            Standardized all implementations to raise `IOError`.
   222|         """
   223|         raise NotImplementedError()
   224|     def close(self):
   225|         """Closes the `Resolver`, freeing any resources used.
   226|         .. versionadded:: 3.1
   227|         """
   228|         pass
   229| class ExecutorResolver(Resolver):
   230|     """Resolver implementation using a `concurrent.futures.Executor`.
   231|     Use this instead of `ThreadedResolver` when you require additional
   232|     control over the executor being used.
   233|     The executor will be shut down when the resolver is closed unless
   234|     ``close_resolver=False``; use this if you want to reuse the same
   235|     executor elsewhere.
   236|     .. versionchanged:: 4.1
   237|        The ``io_loop`` argument is deprecated.
   238|     """
   239|     def initialize(self, io_loop=None, executor=None, close_executor=True):
   240|         self.io_loop = io_loop or IOLoop.current()
   241|         if executor is not None:
   242|             self.executor = executor
   243|             self.close_executor = close_executor
   244|         else:
   245|             self.executor = dummy_executor
   246|             self.close_executor = False
   247|     def close(self):
   248|         if self.close_executor:
   249|             self.executor.shutdown()
   250|         self.executor = None
   251|     @run_on_executor
   252|     def resolve(self, host, port, family=socket.AF_UNSPEC):
   253|         addrinfo = socket.getaddrinfo(host, port, family, socket.SOCK_STREAM)
   254|         results = []
   255|         for family, socktype, proto, canonname, address in addrinfo:
   256|             results.append((family, address))
   257|         return results
   258| class BlockingResolver(ExecutorResolver):
   259|     """Default `Resolver` implementation, using `socket.getaddrinfo`.
   260|     The `.IOLoop` will be blocked during the resolution, although the
   261|     callback will not be run until the next `.IOLoop` iteration.
   262|     """
   263|     def initialize(self, io_loop=None):
   264|         super(BlockingResolver, self).initialize(io_loop=io_loop)
   265| class ThreadedResolver(ExecutorResolver):
   266|     """Multithreaded non-blocking `Resolver` implementation.
   267|     Requires the `concurrent.futures` package to be installed
   268|     (available in the standard library since Python 3.2,
   269|     installable with ``pip install futures`` in older versions).
   270|     The thread pool size can be configured with::
   271|         Resolver.configure('tornado.netutil.ThreadedResolver',
   272|                            num_threads=10)
   273|     .. versionchanged:: 3.1
   274|        All ``ThreadedResolvers`` share a single thread pool, whose
   275|        size is set by the first one to be created.
   276|     """
   277|     _threadpool = None  # type: ignore
   278|     _threadpool_pid = None  # type: int
   279|     def initialize(self, io_loop=None, num_threads=10):
   280|         threadpool = ThreadedResolver._create_threadpool(num_threads)
   281|         super(ThreadedResolver, self).initialize(
   282|             io_loop=io_loop, executor=threadpool, close_executor=False)
   283|     @classmethod
   284|     def _create_threadpool(cls, num_threads):
   285|         pid = os.getpid()
   286|         if cls._threadpool_pid != pid:
   287|             cls._threadpool = None
   288|         if cls._threadpool is None:
   289|             from concurrent.futures import ThreadPoolExecutor
   290|             cls._threadpool = ThreadPoolExecutor(num_threads)
   291|             cls._threadpool_pid = pid
   292|         return cls._threadpool
   293| class OverrideResolver(Resolver):
   294|     """Wraps a resolver with a mapping of overrides.
   295|     This can be used to make local DNS changes (e.g. for testing)
   296|     without modifying system-wide settings.
   297|     The mapping can contain either host strings or host-port pairs.
   298|     """
   299|     def initialize(self, resolver, mapping):
   300|         self.resolver = resolver
   301|         self.mapping = mapping
   302|     def close(self):
   303|         self.resolver.close()
   304|     def resolve(self, host, port, *args, **kwargs):
   305|         if (host, port) in self.mapping:
   306|             host, port = self.mapping[(host, port)]
   307|         elif host in self.mapping:
   308|             host = self.mapping[host]
   309|         return self.resolver.resolve(host, port, *args, **kwargs)
   310| _SSL_CONTEXT_KEYWORDS = frozenset(['ssl_version', 'certfile', 'keyfile',
   311|                                    'cert_reqs', 'ca_certs', 'ciphers'])
   312| def ssl_options_to_context(ssl_options):
   313|     """Try to convert an ``ssl_options`` dictionary to an
   314|     `~ssl.SSLContext` object.
   315|     The ``ssl_options`` dictionary contains keywords to be passed to
   316|     `ssl.wrap_socket`.  In Python 2.7.9+, `ssl.SSLContext` objects can
   317|     be used instead.  This function converts the dict form to its
   318|     `~ssl.SSLContext` equivalent, and may be used when a component which
   319|     accepts both forms needs to upgrade to the `~ssl.SSLContext` version
   320|     to use features like SNI or NPN.
   321|     """
   322|     if isinstance(ssl_options, dict):
   323|         assert all(k in _SSL_CONTEXT_KEYWORDS for k in ssl_options), ssl_options
   324|     if (not hasattr(ssl, 'SSLContext') or
   325|             isinstance(ssl_options, ssl.SSLContext)):
   326|         return ssl_options
   327|     context = ssl.SSLContext(
   328|         ssl_options.get('ssl_version', ssl.PROTOCOL_SSLv23))
   329|     if 'certfile' in ssl_options:
   330|         context.load_cert_chain(ssl_options['certfile'], ssl_options.get('keyfile', None))
   331|     if 'cert_reqs' in ssl_options:
   332|         context.verify_mode = ssl_options['cert_reqs']
   333|     if 'ca_certs' in ssl_options:
   334|         context.load_verify_locations(ssl_options['ca_certs'])
   335|     if 'ciphers' in ssl_options:
   336|         context.set_ciphers(ssl_options['ciphers'])
   337|     if hasattr(ssl, 'OP_NO_COMPRESSION'):
   338|         context.options |= ssl.OP_NO_COMPRESSION
   339|     return context
   340| def ssl_wrap_socket(socket, ssl_options, server_hostname=None, **kwargs):
   341|     """Returns an ``ssl.SSLSocket`` wrapping the given socket.
   342|     ``ssl_options`` may be either an `ssl.SSLContext` object or a
   343|     dictionary (as accepted by `ssl_options_to_context`).  Additional
   344|     keyword arguments are passed to ``wrap_socket`` (either the
   345|     `~ssl.SSLContext` method or the `ssl` module function as
   346|     appropriate).
   347|     """
   348|     context = ssl_options_to_context(ssl_options)
   349|     if hasattr(ssl, 'SSLContext') and isinstance(context, ssl.SSLContext):
   350|         if server_hostname is not None and getattr(ssl, 'HAS_SNI'):
   351|             return context.wrap_socket(socket, server_hostname=server_hostname,
   352|                                        **kwargs)
   353|         else:
   354|             return context.wrap_socket(socket, **kwargs)
   355|     else:
   356|         return ssl.wrap_socket(socket, **dict(context, **kwargs))  # type: ignore


# ====================================================================
# FILE: salt/ext/tornado/options.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-451 ---
     1| """A command line parsing module that lets modules define their own options.
     2| Each module defines its own options which are added to the global
     3| option namespace, e.g.::
     4|     from salt.ext.tornado.options import define, options
     5|     define("mysql_host", default="127.0.0.1:3306", help="Main user DB")
     6|     define("memcache_hosts", default="127.0.0.1:11011", multiple=True,
     7|            help="Main user memcache servers")
     8|     def connect():
     9|         db = database.Connection(options.mysql_host)
    10|         ...
    11| The ``main()`` method of your application does not need to be aware of all of
    12| the options used throughout your program; they are all automatically loaded
    13| when the modules are loaded.  However, all modules that define options
    14| must have been imported before the command line is parsed.
    15| Your ``main()`` method can parse the command line or parse a config file with
    16| either::
    17|     tornado.options.parse_command_line()
    18|     tornado.options.parse_config_file("/etc/server.conf")
    19| .. note:
    20|    When using tornado.options.parse_command_line or
    21|    tornado.options.parse_config_file, the only options that are set are
    22|    ones that were previously defined with tornado.options.define.
    23| Command line formats are what you would expect (``--myoption=myvalue``).
    24| Config files are just Python files. Global names become options, e.g.::
    25|     myoption = "myvalue"
    26|     myotheroption = "myothervalue"
    27| We support `datetimes <datetime.datetime>`, `timedeltas
    28| <datetime.timedelta>`, ints, and floats (just pass a ``type`` kwarg to
    29| `define`). We also accept multi-value options. See the documentation for
    30| `define()` below.
    31| `tornado.options.options` is a singleton instance of `OptionParser`, and
    32| the top-level functions in this module (`define`, `parse_command_line`, etc)
    33| simply call methods on it.  You may create additional `OptionParser`
    34| instances to define isolated sets of options, such as for subcommands.
    35| .. note::
    36|    By default, several options are defined that will configure the
    37|    standard `logging` module when `parse_command_line` or `parse_config_file`
    38|    are called.  If you want Tornado to leave the logging configuration
    39|    alone so you can manage it yourself, either pass ``--logging=none``
    40|    on the command line or do the following to disable it in code::
    41|        from salt.ext.tornado.options import options, parse_command_line
    42|        options.logging = None
    43|        parse_command_line()
    44| .. versionchanged:: 4.3
    45|    Dashes and underscores are fully interchangeable in option names;
    46|    options can be defined, set, and read with any mix of the two.
    47|    Dashes are typical for command-line usage while config files require
    48|    underscores.
    49| """
    50| from __future__ import absolute_import, division, print_function
    51| import datetime
    52| import numbers
    53| import re
    54| import sys
    55| import os
    56| import textwrap
    57| from salt.ext.tornado.escape import _unicode, native_str
    58| from salt.ext.tornado.log import define_logging_options
    59| from salt.ext.tornado import stack_context
    60| from salt.ext.tornado.util import basestring_type, exec_in
    61| class Error(Exception):
    62|     """Exception raised by errors in the options module."""
    63|     pass
    64| class OptionParser(object):
    65|     """A collection of options, a dictionary with object-like access.
    66|     Normally accessed via static functions in the `tornado.options` module,
    67|     which reference a global instance.
    68|     """
    69|     def __init__(self):
    70|         self.__dict__['_options'] = {}
    71|         self.__dict__['_parse_callbacks'] = []
    72|         self.define("help", type=bool, help="show this help information",
    73|                     callback=self._help_callback)
    74|     def _normalize_name(self, name):
    75|         return name.replace('_', '-')
    76|     def __getattr__(self, name):
    77|         name = self._normalize_name(name)
    78|         if isinstance(self._options.get(name), _Option):
    79|             return self._options[name].value()
    80|         raise AttributeError("Unrecognized option %r" % name)
    81|     def __setattr__(self, name, value):
    82|         name = self._normalize_name(name)
    83|         if isinstance(self._options.get(name), _Option):
    84|             return self._options[name].set(value)
    85|         raise AttributeError("Unrecognized option %r" % name)
    86|     def __iter__(self):
    87|         return (opt.name for opt in self._options.values())
    88|     def __contains__(self, name):
    89|         name = self._normalize_name(name)
    90|         return name in self._options
    91|     def __getitem__(self, name):
    92|         return self.__getattr__(name)
    93|     def __setitem__(self, name, value):
    94|         return self.__setattr__(name, value)
    95|     def items(self):
    96|         """A sequence of (name, value) pairs.
    97|         .. versionadded:: 3.1
    98|         """
    99|         return [(opt.name, opt.value()) for name, opt in self._options.items()]
   100|     def groups(self):
   101|         """The set of option-groups created by ``define``.
   102|         .. versionadded:: 3.1
   103|         """
   104|         return set(opt.group_name for opt in self._options.values())
   105|     def group_dict(self, group):
   106|         """The names and values of options in a group.
   107|         Useful for copying options into Application settings::
   108|             from salt.ext.tornado.options import define, parse_command_line, options
   109|             define('template_path', group='application')
   110|             define('static_path', group='application')
   111|             parse_command_line()
   112|             application = Application(
   113|                 handlers, **options.group_dict('application'))
   114|         .. versionadded:: 3.1
   115|         """
   116|         return dict(
   117|             (opt.name, opt.value()) for name, opt in self._options.items()
   118|             if not group or group == opt.group_name)
   119|     def as_dict(self):
   120|         """The names and values of all options.
   121|         .. versionadded:: 3.1
   122|         """
   123|         return dict(
   124|             (opt.name, opt.value()) for name, opt in self._options.items())
   125|     def define(self, name, default=None, type=None, help=None, metavar=None,
   126|                multiple=False, group=None, callback=None):
   127|         """Defines a new command line option.
   128|         If ``type`` is given (one of str, float, int, datetime, or timedelta)
   129|         or can be inferred from the ``default``, we parse the command line
   130|         arguments based on the given type. If ``multiple`` is True, we accept
   131|         comma-separated values, and the option value is always a list.
   132|         For multi-value integers, we also accept the syntax ``x:y``, which
   133|         turns into ``range(x, y)`` - very useful for long integer ranges.
   134|         ``help`` and ``metavar`` are used to construct the
   135|         automatically generated command line help string. The help
   136|         message is formatted like::
   137|            --name=METAVAR      help string
   138|         ``group`` is used to group the defined options in logical
   139|         groups. By default, command line options are grouped by the
   140|         file in which they are defined.
   141|         Command line option names must be unique globally. They can be parsed
   142|         from the command line with `parse_command_line` or parsed from a
   143|         config file with `parse_config_file`.
   144|         If a ``callback`` is given, it will be run with the new value whenever
   145|         the option is changed.  This can be used to combine command-line
   146|         and file-based options::
   147|             define("config", type=str, help="path to config file",
   148|                    callback=lambda path: parse_config_file(path, final=False))
   149|         With this definition, options in the file specified by ``--config`` will
   150|         override options set earlier on the command line, but can be overridden
   151|         by later flags.
   152|         """
   153|         normalized = self._normalize_name(name)
   154|         if normalized in self._options:
   155|             raise Error("Option %r already defined in %s" %
   156|                         (normalized, self._options[normalized].file_name))
   157|         frame = sys._getframe(0)
   158|         options_file = frame.f_code.co_filename
   159|         if (frame.f_back.f_code.co_filename == options_file and
   160|                 frame.f_back.f_code.co_name == 'define'):
   161|             frame = frame.f_back
   162|         file_name = frame.f_back.f_code.co_filename
   163|         if file_name == options_file:
   164|             file_name = ""
   165|         if type is None:
   166|             if not multiple and default is not None:
   167|                 type = default.__class__
   168|             else:
   169|                 type = str
   170|         if group:
   171|             group_name = group
   172|         else:
   173|             group_name = file_name
   174|         option = _Option(name, file_name=file_name,
   175|                          default=default, type=type, help=help,
   176|                          metavar=metavar, multiple=multiple,
   177|                          group_name=group_name,
   178|                          callback=callback)
   179|         self._options[normalized] = option
   180|     def parse_command_line(self, args=None, final=True):
   181|         """Parses all options given on the command line (defaults to
   182|         `sys.argv`).
   183|         Note that ``args[0]`` is ignored since it is the program name
   184|         in `sys.argv`.
   185|         We return a list of all arguments that are not parsed as options.
   186|         If ``final`` is ``False``, parse callbacks will not be run.
   187|         This is useful for applications that wish to combine configurations
   188|         from multiple sources.
   189|         """
   190|         if args is None:
   191|             args = sys.argv
   192|         remaining = []
   193|         for i in range(1, len(args)):
   194|             if not args[i].startswith("-"):
   195|                 remaining = args[i:]
   196|                 break
   197|             if args[i] == "--":
   198|                 remaining = args[i + 1:]
   199|                 break
   200|             arg = args[i].lstrip("-")
   201|             name, equals, value = arg.partition("=")
   202|             name = self._normalize_name(name)
   203|             if name not in self._options:
   204|                 self.print_help()
   205|                 raise Error('Unrecognized command line option: %r' % name)
   206|             option = self._options[name]
   207|             if not equals:
   208|                 if option.type == bool:
   209|                     value = "true"
   210|                 else:
   211|                     raise Error('Option %r requires a value' % name)
   212|             option.parse(value)
   213|         if final:
   214|             self.run_parse_callbacks()
   215|         return remaining
   216|     def parse_config_file(self, path, final=True):
   217|         """Parses and loads the Python config file at the given path.
   218|         If ``final`` is ``False``, parse callbacks will not be run.
   219|         This is useful for applications that wish to combine configurations
   220|         from multiple sources.
   221|         .. versionchanged:: 4.1
   222|            Config files are now always interpreted as utf-8 instead of
   223|            the system default encoding.
   224|         .. versionchanged:: 4.4
   225|            The special variable ``__file__`` is available inside config
   226|            files, specifying the absolute path to the config file itself.
   227|         """
   228|         config = {'__file__': os.path.abspath(path)}
   229|         with open(path, 'rb') as f:
   230|             exec_in(native_str(f.read()), config, config)
   231|         for name in config:
   232|             normalized = self._normalize_name(name)
   233|             if normalized in self._options:
   234|                 self._options[normalized].set(config[name])
   235|         if final:
   236|             self.run_parse_callbacks()
   237|     def print_help(self, file=None):
   238|         """Prints all the command line options to stderr (or another file)."""
   239|         if file is None:
   240|             file = sys.stderr
   241|         print("Usage: %s [OPTIONS]" % sys.argv[0], file=file)
   242|         print("\nOptions:\n", file=file)
   243|         by_group = {}
   244|         for option in self._options.values():
   245|             by_group.setdefault(option.group_name, []).append(option)
   246|         for filename, o in sorted(by_group.items()):
   247|             if filename:
   248|                 print("\n%s options:\n" % os.path.normpath(filename), file=file)
   249|             o.sort(key=lambda option: option.name)
   250|             for option in o:
   251|                 prefix = self._normalize_name(option.name)
   252|                 if option.metavar:
   253|                     prefix += "=" + option.metavar
   254|                 description = option.help or ""
   255|                 if option.default is not None and option.default != '':
   256|                     description += " (default %s)" % option.default
   257|                 lines = textwrap.wrap(description, 79 - 35)
   258|                 if len(prefix) > 30 or len(lines) == 0:
   259|                     lines.insert(0, '')
   260|                 print("  --%-30s %s" % (prefix, lines[0]), file=file)
   261|                 for line in lines[1:]:
   262|                     print("%-34s %s" % (' ', line), file=file)
   263|         print(file=file)
   264|     def _help_callback(self, value):
   265|         if value:
   266|             self.print_help()
   267|             sys.exit(0)
   268|     def add_parse_callback(self, callback):
   269|         """Adds a parse callback, to be invoked when option parsing is done."""
   270|         self._parse_callbacks.append(stack_context.wrap(callback))
   271|     def run_parse_callbacks(self):
   272|         for callback in self._parse_callbacks:
   273|             callback()
   274|     def mockable(self):
   275|         """Returns a wrapper around self that is compatible with
   276|         `mock.patch <unittest.mock.patch>`.
   277|         The `mock.patch <unittest.mock.patch>` function (included in
   278|         the standard library `unittest.mock` package since Python 3.3,
   279|         or in the third-party ``mock`` package for older versions of
   280|         Python) is incompatible with objects like ``options`` that
   281|         override ``__getattr__`` and ``__setattr__``.  This function
   282|         returns an object that can be used with `mock.patch.object
   283|         <unittest.mock.patch.object>` to modify option values::
   284|             with mock.patch.object(options.mockable(), 'name', value):
   285|                 assert options.name == value
   286|         """
   287|         return _Mockable(self)
   288| class _Mockable(object):
   289|     """`mock.patch` compatible wrapper for `OptionParser`.
   290|     As of ``mock`` version 1.0.1, when an object uses ``__getattr__``
   291|     hooks instead of ``__dict__``, ``patch.__exit__`` tries to delete
   292|     the attribute it set instead of setting a new one (assuming that
   293|     the object does not catpure ``__setattr__``, so the patch
   294|     created a new attribute in ``__dict__``).
   295|     _Mockable's getattr and setattr pass through to the underlying
   296|     OptionParser, and delattr undoes the effect of a previous setattr.
   297|     """
   298|     def __init__(self, options):
   299|         self.__dict__['_options'] = options
   300|         self.__dict__['_originals'] = {}
   301|     def __getattr__(self, name):
   302|         return getattr(self._options, name)
   303|     def __setattr__(self, name, value):
   304|         assert name not in self._originals, "don't reuse mockable objects"
   305|         self._originals[name] = getattr(self._options, name)
   306|         setattr(self._options, name, value)
   307|     def __delattr__(self, name):
   308|         setattr(self._options, name, self._originals.pop(name))
   309| class _Option(object):
   310|     UNSET = object()
   311|     def __init__(self, name, default=None, type=basestring_type, help=None,
   312|                  metavar=None, multiple=False, file_name=None, group_name=None,
   313|                  callback=None):
   314|         if default is None and multiple:
   315|             default = []
   316|         self.name = name
   317|         self.type = type
   318|         self.help = help
   319|         self.metavar = metavar
   320|         self.multiple = multiple
   321|         self.file_name = file_name
   322|         self.group_name = group_name
   323|         self.callback = callback
   324|         self.default = default
   325|         self._value = _Option.UNSET
   326|     def value(self):
   327|         return self.default if self._value is _Option.UNSET else self._value
   328|     def parse(self, value):
   329|         _parse = {
   330|             datetime.datetime: self._parse_datetime,
   331|             datetime.timedelta: self._parse_timedelta,
   332|             bool: self._parse_bool,
   333|             basestring_type: self._parse_string,
   334|         }.get(self.type, self.type)
   335|         if self.multiple:
   336|             self._value = []
   337|             for part in value.split(","):
   338|                 if issubclass(self.type, numbers.Integral):
   339|                     lo, _, hi = part.partition(":")
   340|                     lo = _parse(lo)
   341|                     hi = _parse(hi) if hi else lo
   342|                     self._value.extend(range(lo, hi + 1))
   343|                 else:
   344|                     self._value.append(_parse(part))
   345|         else:
   346|             self._value = _parse(value)
   347|         if self.callback is not None:
   348|             self.callback(self._value)
   349|         return self.value()
   350|     def set(self, value):
   351|         if self.multiple:
   352|             if not isinstance(value, list):
   353|                 raise Error("Option %r is required to be a list of %s" %
   354|                             (self.name, self.type.__name__))
   355|             for item in value:
   356|                 if item is not None and not isinstance(item, self.type):
   357|                     raise Error("Option %r is required to be a list of %s" %
   358|                                 (self.name, self.type.__name__))
   359|         else:
   360|             if value is not None and not isinstance(value, self.type):
   361|                 raise Error("Option %r is required to be a %s (%s given)" %
   362|                             (self.name, self.type.__name__, type(value)))
   363|         self._value = value
   364|         if self.callback is not None:
   365|             self.callback(self._value)
   366|     _DATETIME_FORMATS = [
   367|         "%a %b %d %H:%M:%S %Y",
   368|         "%Y-%m-%d %H:%M:%S",
   369|         "%Y-%m-%d %H:%M",
   370|         "%Y-%m-%dT%H:%M",
   371|         "%Y%m%d %H:%M:%S",
   372|         "%Y%m%d %H:%M",
   373|         "%Y-%m-%d",
   374|         "%Y%m%d",
   375|         "%H:%M:%S",
   376|         "%H:%M",
   377|     ]
   378|     def _parse_datetime(self, value):
   379|         for format in self._DATETIME_FORMATS:
   380|             try:
   381|                 return datetime.datetime.strptime(value, format)
   382|             except ValueError:
   383|                 pass
   384|         raise Error('Unrecognized date/time format: %r' % value)
   385|     _TIMEDELTA_ABBREV_DICT = {
   386|         'h': 'hours',
   387|         'm': 'minutes',
   388|         'min': 'minutes',
   389|         's': 'seconds',
   390|         'sec': 'seconds',
   391|         'ms': 'milliseconds',
   392|         'us': 'microseconds',
   393|         'd': 'days',
   394|         'w': 'weeks',
   395|     }
   396|     _FLOAT_PATTERN = r'[-+]?(?:\d+(?:\.\d*)?|\.\d+)(?:[eE][-+]?\d+)?'
   397|     _TIMEDELTA_PATTERN = re.compile(
   398|         r'\s*(%s)\s*(\w*)\s*' % _FLOAT_PATTERN, re.IGNORECASE)
   399|     def _parse_timedelta(self, value):
   400|         try:
   401|             sum = datetime.timedelta()
   402|             start = 0
   403|             while start < len(value):
   404|                 m = self._TIMEDELTA_PATTERN.match(value, start)
   405|                 if not m:
   406|                     raise Exception()
   407|                 num = float(m.group(1))
   408|                 units = m.group(2) or 'seconds'
   409|                 units = self._TIMEDELTA_ABBREV_DICT.get(units, units)
   410|                 sum += datetime.timedelta(**{units: num})
   411|                 start = m.end()
   412|             return sum
   413|         except Exception:
   414|             raise
   415|     def _parse_bool(self, value):
   416|         return value.lower() not in ("false", "0", "f")
   417|     def _parse_string(self, value):
   418|         return _unicode(value)
   419| options = OptionParser()
   420| """Global options object.
   421| All defined options are available as attributes on this object.
   422| """
   423| def define(name, default=None, type=None, help=None, metavar=None,
   424|            multiple=False, group=None, callback=None):
   425|     """Defines an option in the global namespace.
   426|     See `OptionParser.define`.
   427|     """
   428|     return options.define(name, default=default, type=type, help=help,
   429|                           metavar=metavar, multiple=multiple, group=group,
   430|                           callback=callback)
   431| def parse_command_line(args=None, final=True):
   432|     """Parses global options from the command line.
   433|     See `OptionParser.parse_command_line`.
   434|     """
   435|     return options.parse_command_line(args, final=final)
   436| def parse_config_file(path, final=True):
   437|     """Parses global options from a config file.
   438|     See `OptionParser.parse_config_file`.
   439|     """
   440|     return options.parse_config_file(path, final=final)
   441| def print_help(file=None):
   442|     """Prints all the command line options to stderr (or another file).
   443|     See `OptionParser.print_help`.
   444|     """
   445|     return options.print_help(file)
   446| def add_parse_callback(callback):
   447|     """Adds a parse callback, to be invoked when option parsing is done.
   448|     See `OptionParser.add_parse_callback`
   449|     """
   450|     options.add_parse_callback(callback)
   451| define_logging_options(options)


# ====================================================================
# FILE: salt/ext/tornado/platform/asyncio.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-171 ---
     1| """Bridges between the `asyncio` module and Tornado IOLoop.
     2| .. versionadded:: 3.2
     3| This module integrates Tornado with the ``asyncio`` module introduced
     4| in Python 3.4 (and available `as a separate download
     5| <https://pypi.python.org/pypi/asyncio>`_ for Python 3.3).  This makes
     6| it possible to combine the two libraries on the same event loop.
     7| Most applications should use `AsyncIOMainLoop` to run Tornado on the
     8| default ``asyncio`` event loop.  Applications that need to run event
     9| loops on multiple threads may use `AsyncIOLoop` to create multiple
    10| loops.
    11| .. note::
    12|    Tornado requires the `~asyncio.AbstractEventLoop.add_reader` family of
    13|    methods, so it is not compatible with the `~asyncio.ProactorEventLoop` on
    14|    Windows. Use the `~asyncio.SelectorEventLoop` instead.
    15| """
    16| from __future__ import absolute_import, division, print_function
    17| import functools
    18| import salt.ext.tornado.concurrent
    19| from salt.ext.tornado.gen import convert_yielded
    20| from salt.ext.tornado.ioloop import IOLoop
    21| from salt.ext.tornado import stack_context
    22| try:
    23|     import asyncio  # type: ignore
    24| except ImportError as e:
    25|     try:
    26|         import trollius as asyncio  # type: ignore
    27|     except ImportError:
    28|         raise e
    29| class BaseAsyncIOLoop(IOLoop):
    30|     def initialize(self, asyncio_loop, close_loop=False, **kwargs):
    31|         super(BaseAsyncIOLoop, self).initialize(**kwargs)
    32|         self.asyncio_loop = asyncio_loop
    33|         self.close_loop = close_loop
    34|         self.handlers = {}
    35|         self.readers = set()
    36|         self.writers = set()
    37|         self.closing = False
    38|     def close(self, all_fds=False):
    39|         self.closing = True
    40|         for fd in list(self.handlers):
    41|             fileobj, handler_func = self.handlers[fd]
    42|             self.remove_handler(fd)
    43|             if all_fds:
    44|                 self.close_fd(fileobj)
    45|         if self.close_loop:
    46|             self.asyncio_loop.close()
    47|     def add_handler(self, fd, handler, events):
    48|         fd, fileobj = self.split_fd(fd)
    49|         if fd in self.handlers:
    50|             raise ValueError("fd %s added twice" % fd)
    51|         self.handlers[fd] = (fileobj, stack_context.wrap(handler))
    52|         if events & IOLoop.READ:
    53|             self.asyncio_loop.add_reader(
    54|                 fd, self._handle_events, fd, IOLoop.READ)
    55|             self.readers.add(fd)
    56|         if events & IOLoop.WRITE:
    57|             self.asyncio_loop.add_writer(
    58|                 fd, self._handle_events, fd, IOLoop.WRITE)
    59|             self.writers.add(fd)
    60|     def update_handler(self, fd, events):
    61|         fd, fileobj = self.split_fd(fd)
    62|         if events & IOLoop.READ:
    63|             if fd not in self.readers:
    64|                 self.asyncio_loop.add_reader(
    65|                     fd, self._handle_events, fd, IOLoop.READ)
    66|                 self.readers.add(fd)
    67|         else:
    68|             if fd in self.readers:
    69|                 self.asyncio_loop.remove_reader(fd)
    70|                 self.readers.remove(fd)
    71|         if events & IOLoop.WRITE:
    72|             if fd not in self.writers:
    73|                 self.asyncio_loop.add_writer(
    74|                     fd, self._handle_events, fd, IOLoop.WRITE)
    75|                 self.writers.add(fd)
    76|         else:
    77|             if fd in self.writers:
    78|                 self.asyncio_loop.remove_writer(fd)
    79|                 self.writers.remove(fd)
    80|     def remove_handler(self, fd):
    81|         fd, fileobj = self.split_fd(fd)
    82|         if fd not in self.handlers:
    83|             return
    84|         if fd in self.readers:
    85|             self.asyncio_loop.remove_reader(fd)
    86|             self.readers.remove(fd)
    87|         if fd in self.writers:
    88|             self.asyncio_loop.remove_writer(fd)
    89|             self.writers.remove(fd)
    90|         del self.handlers[fd]
    91|     def _handle_events(self, fd, events):
    92|         fileobj, handler_func = self.handlers[fd]
    93|         handler_func(fileobj, events)
    94|     def start(self):
    95|         old_current = IOLoop.current(instance=False)
    96|         try:
    97|             self._setup_logging()
    98|             self.make_current()
    99|             self.asyncio_loop.run_forever()
   100|         finally:
   101|             if old_current is None:
   102|                 IOLoop.clear_current()
   103|             else:
   104|                 old_current.make_current()
   105|     def stop(self):
   106|         self.asyncio_loop.stop()
   107|     def call_at(self, when, callback, *args, **kwargs):
   108|         return self.asyncio_loop.call_later(
   109|             max(0, when - self.time()), self._run_callback,
   110|             functools.partial(stack_context.wrap(callback), *args, **kwargs))
   111|     def remove_timeout(self, timeout):
   112|         timeout.cancel()
   113|     def add_callback(self, callback, *args, **kwargs):
   114|         if self.closing:
   115|             raise RuntimeError("IOLoop is closing")
   116|         self.asyncio_loop.call_soon_threadsafe(
   117|             self._run_callback,
   118|             functools.partial(stack_context.wrap(callback), *args, **kwargs))
   119|     add_callback_from_signal = add_callback
   120| class AsyncIOMainLoop(BaseAsyncIOLoop):
   121|     """``AsyncIOMainLoop`` creates an `.IOLoop` that corresponds to the
   122|     current ``asyncio`` event loop (i.e. the one returned by
   123|     ``asyncio.get_event_loop()``).  Recommended usage::
   124|         from salt.ext.tornado.platform.asyncio import AsyncIOMainLoop
   125|         import asyncio
   126|         AsyncIOMainLoop().install()
   127|         asyncio.get_event_loop().run_forever()
   128|     See also :meth:`tornado.ioloop.IOLoop.install` for general notes on
   129|     installing alternative IOLoops.
   130|     """
   131|     def initialize(self, **kwargs):
   132|         super(AsyncIOMainLoop, self).initialize(asyncio.get_event_loop(),
   133|                                                 close_loop=False, **kwargs)
   134| class AsyncIOLoop(BaseAsyncIOLoop):
   135|     """``AsyncIOLoop`` is an `.IOLoop` that runs on an ``asyncio`` event loop.
   136|     This class follows the usual Tornado semantics for creating new
   137|     ``IOLoops``; these loops are not necessarily related to the
   138|     ``asyncio`` default event loop.  Recommended usage::
   139|         from salt.ext.tornado.ioloop import IOLoop
   140|         IOLoop.configure('tornado.platform.asyncio.AsyncIOLoop')
   141|         IOLoop.current().start()
   142|     Each ``AsyncIOLoop`` creates a new ``asyncio.EventLoop``; this object
   143|     can be accessed with the ``asyncio_loop`` attribute.
   144|     """
   145|     def initialize(self, **kwargs):
   146|         loop = asyncio.new_event_loop()
   147|         try:
   148|             super(AsyncIOLoop, self).initialize(loop, close_loop=True, **kwargs)
   149|         except Exception:
   150|             loop.close()
   151|             raise
   152| def to_tornado_future(asyncio_future):
   153|     """Convert an `asyncio.Future` to a `tornado.concurrent.Future`.
   154|     .. versionadded:: 4.1
   155|     """
   156|     tf = salt.ext.tornado.concurrent.Future()
   157|     salt.ext.tornado.concurrent.chain_future(asyncio_future, tf)
   158|     return tf
   159| def to_asyncio_future(tornado_future):
   160|     """Convert a Tornado yieldable object to an `asyncio.Future`.
   161|     .. versionadded:: 4.1
   162|     .. versionchanged:: 4.3
   163|        Now accepts any yieldable object, not just
   164|        `tornado.concurrent.Future`.
   165|     """
   166|     tornado_future = convert_yielded(tornado_future)
   167|     af = asyncio.Future()
   168|     salt.ext.tornado.concurrent.chain_future(tornado_future, af)
   169|     return af
   170| if hasattr(convert_yielded, 'register'):
   171|     convert_yielded.register(asyncio.Future, to_tornado_future)  # type: ignore


# ====================================================================
# FILE: salt/ext/tornado/platform/auto.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-30 ---
     1| """Implementation of platform-specific functionality.
     2| For each function or class described in `tornado.platform.interface`,
     3| the appropriate platform-specific implementation exists in this module.
     4| Most code that needs access to this functionality should do e.g.::
     5|     from salt.ext.tornado.platform.auto import set_close_exec
     6| """
     7| from __future__ import absolute_import, division, print_function
     8| import os
     9| if 'APPENGINE_RUNTIME' in os.environ:
    10|     from salt.ext.tornado.platform.common import Waker
    11|     def set_close_exec(fd):
    12|         pass
    13| elif os.name == 'nt':
    14|     from salt.ext.tornado.platform.common import Waker
    15|     from salt.ext.tornado.platform.windows import set_close_exec
    16| else:
    17|     from salt.ext.tornado.platform.posix import set_close_exec, Waker
    18| try:
    19|     import monotime
    20|     monotime
    21| except ImportError:
    22|     pass
    23| try:
    24|     from monotonic import monotonic as monotonic_time
    25| except ImportError:
    26|     try:
    27|         from time import monotonic as monotonic_time
    28|     except ImportError:
    29|         monotonic_time = None
    30| __all__ = ['Waker', 'set_close_exec', 'monotonic_time']


# ====================================================================
# FILE: salt/ext/tornado/platform/caresresolver.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-69 ---
     1| from __future__ import absolute_import, division, print_function
     2| import pycares  # type: ignore
     3| import socket
     4| from salt.ext.tornado import gen
     5| from salt.ext.tornado.ioloop import IOLoop
     6| from salt.ext.tornado.netutil import Resolver, is_valid_ip
     7| class CaresResolver(Resolver):
     8|     """Name resolver based on the c-ares library.
     9|     This is a non-blocking and non-threaded resolver.  It may not produce
    10|     the same results as the system resolver, but can be used for non-blocking
    11|     resolution when threads cannot be used.
    12|     c-ares fails to resolve some names when ``family`` is ``AF_UNSPEC``,
    13|     so it is only recommended for use in ``AF_INET`` (i.e. IPv4).  This is
    14|     the default for ``tornado.simple_httpclient``, but other libraries
    15|     may default to ``AF_UNSPEC``.
    16|     .. versionchanged:: 4.1
    17|        The ``io_loop`` argument is deprecated.
    18|     """
    19|     def initialize(self, io_loop=None):
    20|         self.io_loop = io_loop or IOLoop.current()
    21|         self.channel = pycares.Channel(sock_state_cb=self._sock_state_cb)
    22|         self.fds = {}
    23|     def _sock_state_cb(self, fd, readable, writable):
    24|         state = ((IOLoop.READ if readable else 0) |
    25|                  (IOLoop.WRITE if writable else 0))
    26|         if not state:
    27|             self.io_loop.remove_handler(fd)
    28|             del self.fds[fd]
    29|         elif fd in self.fds:
    30|             self.io_loop.update_handler(fd, state)
    31|             self.fds[fd] = state
    32|         else:
    33|             self.io_loop.add_handler(fd, self._handle_events, state)
    34|             self.fds[fd] = state
    35|     def _handle_events(self, fd, events):
    36|         read_fd = pycares.ARES_SOCKET_BAD
    37|         write_fd = pycares.ARES_SOCKET_BAD
    38|         if events & IOLoop.READ:
    39|             read_fd = fd
    40|         if events & IOLoop.WRITE:
    41|             write_fd = fd
    42|         self.channel.process_fd(read_fd, write_fd)
    43|     @gen.coroutine
    44|     def resolve(self, host, port, family=0):
    45|         if is_valid_ip(host):
    46|             addresses = [host]
    47|         else:
    48|             self.channel.gethostbyname(host, family, (yield gen.Callback(1)))
    49|             callback_args = yield gen.Wait(1)
    50|             assert isinstance(callback_args, gen.Arguments)
    51|             assert not callback_args.kwargs
    52|             result, error = callback_args.args
    53|             if error:
    54|                 raise IOError('C-Ares returned error %s: %s while resolving %s' %
    55|                               (error, pycares.errno.strerror(error), host))
    56|             addresses = result.addresses
    57|         addrinfo = []
    58|         for address in addresses:
    59|             if '.' in address:
    60|                 address_family = socket.AF_INET
    61|             elif ':' in address:
    62|                 address_family = socket.AF_INET6
    63|             else:
    64|                 address_family = socket.AF_UNSPEC
    65|             if family != socket.AF_UNSPEC and family != address_family:
    66|                 raise IOError('Requested socket family %d but got %d' %
    67|                               (family, address_family))
    68|             addrinfo.append((address_family, (address, port)))
    69|         raise gen.Return(addrinfo)


# ====================================================================
# FILE: salt/ext/tornado/platform/common.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-73 ---
     1| """Lowest-common-denominator implementations of platform functionality."""
     2| from __future__ import absolute_import, division, print_function
     3| import errno
     4| import socket
     5| import time
     6| from salt.ext.tornado.platform import interface
     7| from salt.ext.tornado.util import errno_from_exception
     8| def try_close(f):
     9|     for i in range(10):
    10|         try:
    11|             f.close()
    12|         except IOError:
    13|             time.sleep(1e-3)
    14|         else:
    15|             break
    16|     f.close()
    17| class Waker(interface.Waker):
    18|     """Create an OS independent asynchronous pipe.
    19|     For use on platforms that don't have os.pipe() (or where pipes cannot
    20|     be passed to select()), but do have sockets.  This includes Windows
    21|     and Jython.
    22|     """
    23|     def __init__(self):
    24|         from .auto import set_close_exec
    25|         self.writer = socket.socket()
    26|         set_close_exec(self.writer.fileno())
    27|         self.writer.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
    28|         count = 0
    29|         while 1:
    30|             count += 1
    31|             a = socket.socket()
    32|             set_close_exec(a.fileno())
    33|             a.bind(("127.0.0.1", 0))
    34|             a.listen(1)
    35|             connect_address = a.getsockname()  # assigned (host, port) pair
    36|             try:
    37|                 self.writer.connect(connect_address)
    38|                 break    # success
    39|             except socket.error as detail:
    40|                 if (not hasattr(errno, 'WSAEADDRINUSE') or
    41|                         errno_from_exception(detail) != errno.WSAEADDRINUSE):
    42|                     raise
    43|                 if count >= 10:  # I've never seen it go above 2
    44|                     a.close()
    45|                     self.writer.close()
    46|                     raise socket.error("Cannot bind trigger!")
    47|                 a.close()
    48|         self.reader, addr = a.accept()
    49|         set_close_exec(self.reader.fileno())
    50|         self.reader.setblocking(0)
    51|         self.writer.setblocking(0)
    52|         a.close()
    53|         self.reader_fd = self.reader.fileno()
    54|     def fileno(self):
    55|         return self.reader.fileno()
    56|     def write_fileno(self):
    57|         return self.writer.fileno()
    58|     def wake(self):
    59|         try:
    60|             self.writer.send(b"x")
    61|         except (IOError, socket.error, ValueError):
    62|             pass
    63|     def consume(self):
    64|         try:
    65|             while True:
    66|                 result = self.reader.recv(1024)
    67|                 if not result:
    68|                     break
    69|         except (IOError, socket.error):
    70|             pass
    71|     def close(self):
    72|         self.reader.close()
    73|         try_close(self.writer)


# ====================================================================
# FILE: salt/ext/tornado/platform/epoll.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-7 ---
     1| """EPoll-based IOLoop implementation for Linux systems."""
     2| from __future__ import absolute_import, division, print_function
     3| import select
     4| from salt.ext.tornado.ioloop import PollIOLoop
     5| class EPollIOLoop(PollIOLoop):
     6|     def initialize(self, **kwargs):
     7|         super(EPollIOLoop, self).initialize(impl=select.epoll(), **kwargs)


# ====================================================================
# FILE: salt/ext/tornado/platform/interface.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| """Interfaces for platform-specific functionality.
     2| This module exists primarily for documentation purposes and as base classes
     3| for other tornado.platform modules.  Most code should import the appropriate
     4| implementation from `tornado.platform.auto`.
     5| """
     6| from __future__ import absolute_import, division, print_function
     7| def set_close_exec(fd):
     8|     """Sets the close-on-exec bit (``FD_CLOEXEC``)for a file descriptor."""
     9|     raise NotImplementedError()
    10| class Waker(object):
    11|     """A socket-like object that can wake another thread from ``select()``.
    12|     The `~tornado.ioloop.IOLoop` will add the Waker's `fileno()` to
    13|     its ``select`` (or ``epoll`` or ``kqueue``) calls.  When another
    14|     thread wants to wake up the loop, it calls `wake`.  Once it has woken
    15|     up, it will call `consume` to do any necessary per-wake cleanup.  When
    16|     the ``IOLoop`` is closed, it closes its waker too.
    17|     """
    18|     def fileno(self):
    19|         """Returns the read file descriptor for this waker.
    20|         Must be suitable for use with ``select()`` or equivalent on the
    21|         local platform.
    22|         """
    23|         raise NotImplementedError()
    24|     def write_fileno(self):
    25|         """Returns the write file descriptor for this waker."""
    26|         raise NotImplementedError()
    27|     def wake(self):
    28|         """Triggers activity on the waker's file descriptor."""
    29|         raise NotImplementedError()
    30|     def consume(self):
    31|         """Called after the listen has woken up to do any necessary cleanup."""
    32|         raise NotImplementedError()
    33|     def close(self):
    34|         """Closes the waker's file descriptor(s)."""
    35|         raise NotImplementedError()
    36| def monotonic_time():
    37|     raise NotImplementedError()


# ====================================================================
# FILE: salt/ext/tornado/platform/kqueue.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-53 ---
     1| """KQueue-based IOLoop implementation for BSD/Mac systems."""
     2| from __future__ import absolute_import, division, print_function
     3| import select
     4| from salt.ext.tornado.ioloop import IOLoop, PollIOLoop
     5| assert hasattr(select, 'kqueue'), 'kqueue not supported'
     6| class _KQueue(object):
     7|     """A kqueue-based event loop for BSD/Mac systems."""
     8|     def __init__(self):
     9|         self._kqueue = select.kqueue()
    10|         self._active = {}
    11|     def fileno(self):
    12|         return self._kqueue.fileno()
    13|     def close(self):
    14|         self._kqueue.close()
    15|     def register(self, fd, events):
    16|         if fd in self._active:
    17|             raise IOError("fd %s already registered" % fd)
    18|         self._control(fd, events, select.KQ_EV_ADD)
    19|         self._active[fd] = events
    20|     def modify(self, fd, events):
    21|         self.unregister(fd)
    22|         self.register(fd, events)
    23|     def unregister(self, fd):
    24|         events = self._active.pop(fd)
    25|         self._control(fd, events, select.KQ_EV_DELETE)
    26|     def _control(self, fd, events, flags):
    27|         kevents = []
    28|         if events & IOLoop.WRITE:
    29|             kevents.append(select.kevent(
    30|                 fd, filter=select.KQ_FILTER_WRITE, flags=flags))
    31|         if events & IOLoop.READ:
    32|             kevents.append(select.kevent(
    33|                 fd, filter=select.KQ_FILTER_READ, flags=flags))
    34|         for kevent in kevents:
    35|             self._kqueue.control([kevent], 0)
    36|     def poll(self, timeout):
    37|         kevents = self._kqueue.control(None, 1000, timeout)
    38|         events = {}
    39|         for kevent in kevents:
    40|             fd = kevent.ident
    41|             if kevent.filter == select.KQ_FILTER_READ:
    42|                 events[fd] = events.get(fd, 0) | IOLoop.READ
    43|             if kevent.filter == select.KQ_FILTER_WRITE:
    44|                 if kevent.flags & select.KQ_EV_EOF:
    45|                     events[fd] = IOLoop.ERROR
    46|                 else:
    47|                     events[fd] = events.get(fd, 0) | IOLoop.WRITE
    48|             if kevent.flags & select.KQ_EV_ERROR:
    49|                 events[fd] = events.get(fd, 0) | IOLoop.ERROR
    50|         return events.items()
    51| class KQueueIOLoop(PollIOLoop):
    52|     def initialize(self, **kwargs):
    53|         super(KQueueIOLoop, self).initialize(impl=_KQueue(), **kwargs)


# ====================================================================
# FILE: salt/ext/tornado/platform/posix.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-40 ---
     1| """Posix implementations of platform-specific functionality."""
     2| from __future__ import absolute_import, division, print_function
     3| import fcntl
     4| import os
     5| from salt.ext.tornado.platform import common, interface
     6| def set_close_exec(fd):
     7|     flags = fcntl.fcntl(fd, fcntl.F_GETFD)
     8|     fcntl.fcntl(fd, fcntl.F_SETFD, flags | fcntl.FD_CLOEXEC)
     9| def _set_nonblocking(fd):
    10|     flags = fcntl.fcntl(fd, fcntl.F_GETFL)
    11|     fcntl.fcntl(fd, fcntl.F_SETFL, flags | os.O_NONBLOCK)
    12| class Waker(interface.Waker):
    13|     def __init__(self):
    14|         r, w = os.pipe()
    15|         _set_nonblocking(r)
    16|         _set_nonblocking(w)
    17|         set_close_exec(r)
    18|         set_close_exec(w)
    19|         self.reader = os.fdopen(r, "rb", 0)
    20|         self.writer = os.fdopen(w, "wb", 0)
    21|     def fileno(self):
    22|         return self.reader.fileno()
    23|     def write_fileno(self):
    24|         return self.writer.fileno()
    25|     def wake(self):
    26|         try:
    27|             self.writer.write(b"x")
    28|         except (IOError, ValueError):
    29|             pass
    30|     def consume(self):
    31|         try:
    32|             while True:
    33|                 result = self.reader.read()
    34|                 if not result:
    35|                     break
    36|         except IOError:
    37|             pass
    38|     def close(self):
    39|         self.reader.close()
    40|         common.try_close(self.writer)


# ====================================================================
# FILE: salt/ext/tornado/platform/select.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-45 ---
     1| """Select-based IOLoop implementation.
     2| Used as a fallback for systems that don't support epoll or kqueue.
     3| """
     4| from __future__ import absolute_import, division, print_function
     5| import select
     6| from salt.ext.tornado.ioloop import IOLoop, PollIOLoop
     7| class _Select(object):
     8|     """A simple, select()-based IOLoop implementation for non-Linux systems"""
     9|     def __init__(self):
    10|         self.read_fds = set()
    11|         self.write_fds = set()
    12|         self.error_fds = set()
    13|         self.fd_sets = (self.read_fds, self.write_fds, self.error_fds)
    14|     def close(self):
    15|         pass
    16|     def register(self, fd, events):
    17|         if fd in self.read_fds or fd in self.write_fds or fd in self.error_fds:
    18|             raise IOError("fd %s already registered" % fd)
    19|         if events & IOLoop.READ:
    20|             self.read_fds.add(fd)
    21|         if events & IOLoop.WRITE:
    22|             self.write_fds.add(fd)
    23|         if events & IOLoop.ERROR:
    24|             self.error_fds.add(fd)
    25|     def modify(self, fd, events):
    26|         self.unregister(fd)
    27|         self.register(fd, events)
    28|     def unregister(self, fd):
    29|         self.read_fds.discard(fd)
    30|         self.write_fds.discard(fd)
    31|         self.error_fds.discard(fd)
    32|     def poll(self, timeout):
    33|         readable, writeable, errors = select.select(
    34|             self.read_fds, self.write_fds, self.error_fds, timeout)
    35|         events = {}
    36|         for fd in readable:
    37|             events[fd] = events.get(fd, 0) | IOLoop.READ
    38|         for fd in writeable:
    39|             events[fd] = events.get(fd, 0) | IOLoop.WRITE
    40|         for fd in errors:
    41|             events[fd] = events.get(fd, 0) | IOLoop.ERROR
    42|         return events.items()
    43| class SelectIOLoop(PollIOLoop):
    44|     def initialize(self, **kwargs):
    45|         super(SelectIOLoop, self).initialize(impl=_Select(), **kwargs)


# ====================================================================
# FILE: salt/ext/tornado/platform/twisted.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-440 ---
     1| """Bridges between the Twisted reactor and Tornado IOLoop.
     2| This module lets you run applications and libraries written for
     3| Twisted in a Tornado application.  It can be used in two modes,
     4| depending on which library's underlying event loop you want to use.
     5| This module has been tested with Twisted versions 11.0.0 and newer.
     6| """
     7| from __future__ import absolute_import, division, print_function
     8| import datetime
     9| import functools
    10| import numbers
    11| import socket
    12| import sys
    13| import twisted.internet.abstract  # type: ignore
    14| from twisted.internet.defer import Deferred  # type: ignore
    15| from twisted.internet.posixbase import PosixReactorBase  # type: ignore
    16| from twisted.internet.interfaces import IReactorFDSet, IDelayedCall, IReactorTime, IReadDescriptor, IWriteDescriptor  # type: ignore
    17| from twisted.python import failure, log  # type: ignore
    18| from twisted.internet import error  # type: ignore
    19| import twisted.names.cache  # type: ignore
    20| import twisted.names.client  # type: ignore
    21| import twisted.names.hosts  # type: ignore
    22| import twisted.names.resolve  # type: ignore
    23| from zope.interface import implementer  # type: ignore
    24| from salt.ext.tornado.concurrent import Future
    25| from salt.ext.tornado.escape import utf8
    26| from salt.ext.tornado import gen
    27| import salt.ext.tornado.ioloop
    28| from salt.ext.tornado.log import app_log
    29| from salt.ext.tornado.netutil import Resolver
    30| from salt.ext.tornado.stack_context import NullContext, wrap
    31| from salt.ext.tornado.ioloop import IOLoop
    32| from salt.ext.tornado.util import timedelta_to_seconds
    33| @implementer(IDelayedCall)
    34| class TornadoDelayedCall(object):
    35|     """DelayedCall object for Tornado."""
    36|     def __init__(self, reactor, seconds, f, *args, **kw):
    37|         self._reactor = reactor
    38|         self._func = functools.partial(f, *args, **kw)
    39|         self._time = self._reactor.seconds() + seconds
    40|         self._timeout = self._reactor._io_loop.add_timeout(self._time,
    41|                                                            self._called)
    42|         self._active = True
    43|     def _called(self):
    44|         self._active = False
    45|         self._reactor._removeDelayedCall(self)
    46|         try:
    47|             self._func()
    48|         except:
    49|             app_log.error("_called caught exception", exc_info=True)
    50|     def getTime(self):
    51|         return self._time
    52|     def cancel(self):
    53|         self._active = False
    54|         self._reactor._io_loop.remove_timeout(self._timeout)
    55|         self._reactor._removeDelayedCall(self)
    56|     def delay(self, seconds):
    57|         self._reactor._io_loop.remove_timeout(self._timeout)
    58|         self._time += seconds
    59|         self._timeout = self._reactor._io_loop.add_timeout(self._time,
    60|                                                            self._called)
    61|     def reset(self, seconds):
    62|         self._reactor._io_loop.remove_timeout(self._timeout)
    63|         self._time = self._reactor.seconds() + seconds
    64|         self._timeout = self._reactor._io_loop.add_timeout(self._time,
    65|                                                            self._called)
    66|     def active(self):
    67|         return self._active
    68| @implementer(IReactorTime, IReactorFDSet)
    69| class TornadoReactor(PosixReactorBase):
    70|     """Twisted reactor built on the Tornado IOLoop.
    71|     `TornadoReactor` implements the Twisted reactor interface on top of
    72|     the Tornado IOLoop.  To use it, simply call `install` at the beginning
    73|     of the application::
    74|         import tornado.platform.twisted
    75|         tornado.platform.twisted.install()
    76|         from twisted.internet import reactor
    77|     When the app is ready to start, call ``IOLoop.current().start()``
    78|     instead of ``reactor.run()``.
    79|     It is also possible to create a non-global reactor by calling
    80|     ``tornado.platform.twisted.TornadoReactor(io_loop)``.  However, if
    81|     the `.IOLoop` and reactor are to be short-lived (such as those used in
    82|     unit tests), additional cleanup may be required.  Specifically, it is
    83|     recommended to call::
    84|         reactor.fireSystemEvent('shutdown')
    85|         reactor.disconnectAll()
    86|     before closing the `.IOLoop`.
    87|     .. versionchanged:: 4.1
    88|        The ``io_loop`` argument is deprecated.
    89|     """
    90|     def __init__(self, io_loop=None):
    91|         if not io_loop:
    92|             io_loop = salt.ext.tornado.ioloop.IOLoop.current()
    93|         self._io_loop = io_loop
    94|         self._readers = {}  # map of reader objects to fd
    95|         self._writers = {}  # map of writer objects to fd
    96|         self._fds = {}  # a map of fd to a (reader, writer) tuple
    97|         self._delayedCalls = {}
    98|         PosixReactorBase.__init__(self)
    99|         self.addSystemEventTrigger('during', 'shutdown', self.crash)
   100|         def start_if_necessary():
   101|             if not self._started:
   102|                 self.fireSystemEvent('startup')
   103|         self._io_loop.add_callback(start_if_necessary)
   104|     def seconds(self):
   105|         return self._io_loop.time()
   106|     def callLater(self, seconds, f, *args, **kw):
   107|         dc = TornadoDelayedCall(self, seconds, f, *args, **kw)
   108|         self._delayedCalls[dc] = True
   109|         return dc
   110|     def getDelayedCalls(self):
   111|         return [x for x in self._delayedCalls if x._active]
   112|     def _removeDelayedCall(self, dc):
   113|         if dc in self._delayedCalls:
   114|             del self._delayedCalls[dc]
   115|     def callFromThread(self, f, *args, **kw):
   116|         assert callable(f), "%s is not callable" % f
   117|         with NullContext():
   118|             self._io_loop.add_callback(f, *args, **kw)
   119|     def installWaker(self):
   120|         pass
   121|     def wakeUp(self):
   122|         pass
   123|     def _invoke_callback(self, fd, events):
   124|         if fd not in self._fds:
   125|             return
   126|         (reader, writer) = self._fds[fd]
   127|         if reader:
   128|             err = None
   129|             if reader.fileno() == -1:
   130|                 err = error.ConnectionLost()
   131|             elif events & IOLoop.READ:
   132|                 err = log.callWithLogger(reader, reader.doRead)
   133|             if err is None and events & IOLoop.ERROR:
   134|                 err = error.ConnectionLost()
   135|             if err is not None:
   136|                 self.removeReader(reader)
   137|                 reader.readConnectionLost(failure.Failure(err))
   138|         if writer:
   139|             err = None
   140|             if writer.fileno() == -1:
   141|                 err = error.ConnectionLost()
   142|             elif events & IOLoop.WRITE:
   143|                 err = log.callWithLogger(writer, writer.doWrite)
   144|             if err is None and events & IOLoop.ERROR:
   145|                 err = error.ConnectionLost()
   146|             if err is not None:
   147|                 self.removeWriter(writer)
   148|                 writer.writeConnectionLost(failure.Failure(err))
   149|     def addReader(self, reader):
   150|         if reader in self._readers:
   151|             return
   152|         fd = reader.fileno()
   153|         self._readers[reader] = fd
   154|         if fd in self._fds:
   155|             (_, writer) = self._fds[fd]
   156|             self._fds[fd] = (reader, writer)
   157|             if writer:
   158|                 self._io_loop.update_handler(fd, IOLoop.READ | IOLoop.WRITE)
   159|         else:
   160|             with NullContext():
   161|                 self._fds[fd] = (reader, None)
   162|                 self._io_loop.add_handler(fd, self._invoke_callback,
   163|                                           IOLoop.READ)
   164|     def addWriter(self, writer):
   165|         if writer in self._writers:
   166|             return
   167|         fd = writer.fileno()
   168|         self._writers[writer] = fd
   169|         if fd in self._fds:
   170|             (reader, _) = self._fds[fd]
   171|             self._fds[fd] = (reader, writer)
   172|             if reader:
   173|                 self._io_loop.update_handler(fd, IOLoop.READ | IOLoop.WRITE)
   174|         else:
   175|             with NullContext():
   176|                 self._fds[fd] = (None, writer)
   177|                 self._io_loop.add_handler(fd, self._invoke_callback,
   178|                                           IOLoop.WRITE)
   179|     def removeReader(self, reader):
   180|         if reader in self._readers:
   181|             fd = self._readers.pop(reader)
   182|             (_, writer) = self._fds[fd]
   183|             if writer:
   184|                 self._fds[fd] = (None, writer)
   185|                 self._io_loop.update_handler(fd, IOLoop.WRITE)
   186|             else:
   187|                 del self._fds[fd]
   188|                 self._io_loop.remove_handler(fd)
   189|     def removeWriter(self, writer):
   190|         if writer in self._writers:
   191|             fd = self._writers.pop(writer)
   192|             (reader, _) = self._fds[fd]
   193|             if reader:
   194|                 self._fds[fd] = (reader, None)
   195|                 self._io_loop.update_handler(fd, IOLoop.READ)
   196|             else:
   197|                 del self._fds[fd]
   198|                 self._io_loop.remove_handler(fd)
   199|     def removeAll(self):
   200|         return self._removeAll(self._readers, self._writers)
   201|     def getReaders(self):
   202|         return self._readers.keys()
   203|     def getWriters(self):
   204|         return self._writers.keys()
   205|     def stop(self):
   206|         PosixReactorBase.stop(self)
   207|         fire_shutdown = functools.partial(self.fireSystemEvent, "shutdown")
   208|         self._io_loop.add_callback(fire_shutdown)
   209|     def crash(self):
   210|         PosixReactorBase.crash(self)
   211|         self._io_loop.stop()
   212|     def doIteration(self, delay):
   213|         raise NotImplementedError("doIteration")
   214|     def mainLoop(self):
   215|         self._io_loop.start()
   216| class _TestReactor(TornadoReactor):
   217|     """Subclass of TornadoReactor for use in unittests.
   218|     This can't go in the test.py file because of import-order dependencies
   219|     with the Twisted reactor test builder.
   220|     """
   221|     def __init__(self):
   222|         super(_TestReactor, self).__init__(IOLoop())
   223|     def listenTCP(self, port, factory, backlog=50, interface=''):
   224|         if not interface:
   225|             interface = '127.0.0.1'
   226|         return super(_TestReactor, self).listenTCP(
   227|             port, factory, backlog=backlog, interface=interface)
   228|     def listenUDP(self, port, protocol, interface='', maxPacketSize=8192):
   229|         if not interface:
   230|             interface = '127.0.0.1'
   231|         return super(_TestReactor, self).listenUDP(
   232|             port, protocol, interface=interface, maxPacketSize=maxPacketSize)
   233| def install(io_loop=None):
   234|     """Install this package as the default Twisted reactor.
   235|     ``install()`` must be called very early in the startup process,
   236|     before most other twisted-related imports. Conversely, because it
   237|     initializes the `.IOLoop`, it cannot be called before
   238|     `.fork_processes` or multi-process `~.TCPServer.start`. These
   239|     conflicting requirements make it difficult to use `.TornadoReactor`
   240|     in multi-process mode, and an external process manager such as
   241|     ``supervisord`` is recommended instead.
   242|     .. versionchanged:: 4.1
   243|        The ``io_loop`` argument is deprecated.
   244|     """
   245|     if not io_loop:
   246|         io_loop = salt.ext.tornado.ioloop.IOLoop.current()
   247|     reactor = TornadoReactor(io_loop)
   248|     from twisted.internet.main import installReactor  # type: ignore
   249|     installReactor(reactor)
   250|     return reactor
   251| @implementer(IReadDescriptor, IWriteDescriptor)
   252| class _FD(object):
   253|     def __init__(self, fd, fileobj, handler):
   254|         self.fd = fd
   255|         self.fileobj = fileobj
   256|         self.handler = handler
   257|         self.reading = False
   258|         self.writing = False
   259|         self.lost = False
   260|     def fileno(self):
   261|         return self.fd
   262|     def doRead(self):
   263|         if not self.lost:
   264|             self.handler(self.fileobj, salt.ext.tornado.ioloop.IOLoop.READ)
   265|     def doWrite(self):
   266|         if not self.lost:
   267|             self.handler(self.fileobj, salt.ext.tornado.ioloop.IOLoop.WRITE)
   268|     def connectionLost(self, reason):
   269|         if not self.lost:
   270|             self.handler(self.fileobj, salt.ext.tornado.ioloop.IOLoop.ERROR)
   271|             self.lost = True
   272|     def logPrefix(self):
   273|         return ''
   274| class TwistedIOLoop(salt.ext.tornado.ioloop.IOLoop):
   275|     """IOLoop implementation that runs on Twisted.
   276|     `TwistedIOLoop` implements the Tornado IOLoop interface on top of
   277|     the Twisted reactor. Recommended usage::
   278|         from salt.ext.tornado.platform.twisted import TwistedIOLoop
   279|         from twisted.internet import reactor
   280|         TwistedIOLoop().install()
   281|         reactor.run()
   282|     Uses the global Twisted reactor by default.  To create multiple
   283|     ``TwistedIOLoops`` in the same process, you must pass a unique reactor
   284|     when constructing each one.
   285|     Not compatible with `tornado.process.Subprocess.set_exit_callback`
   286|     because the ``SIGCHLD`` handlers used by Tornado and Twisted conflict
   287|     with each other.
   288|     See also :meth:`tornado.ioloop.IOLoop.install` for general notes on
   289|     installing alternative IOLoops.
   290|     """
   291|     def initialize(self, reactor=None, **kwargs):
   292|         super(TwistedIOLoop, self).initialize(**kwargs)
   293|         if reactor is None:
   294|             import twisted.internet.reactor  # type: ignore
   295|             reactor = twisted.internet.reactor
   296|         self.reactor = reactor
   297|         self.fds = {}
   298|     def close(self, all_fds=False):
   299|         fds = self.fds
   300|         self.reactor.removeAll()
   301|         for c in self.reactor.getDelayedCalls():
   302|             c.cancel()
   303|         if all_fds:
   304|             for fd in fds.values():
   305|                 self.close_fd(fd.fileobj)
   306|     def add_handler(self, fd, handler, events):
   307|         if fd in self.fds:
   308|             raise ValueError('fd %s added twice' % fd)
   309|         fd, fileobj = self.split_fd(fd)
   310|         self.fds[fd] = _FD(fd, fileobj, wrap(handler))
   311|         if events & salt.ext.tornado.ioloop.IOLoop.READ:
   312|             self.fds[fd].reading = True
   313|             self.reactor.addReader(self.fds[fd])
   314|         if events & salt.ext.tornado.ioloop.IOLoop.WRITE:
   315|             self.fds[fd].writing = True
   316|             self.reactor.addWriter(self.fds[fd])
   317|     def update_handler(self, fd, events):
   318|         fd, fileobj = self.split_fd(fd)
   319|         if events & salt.ext.tornado.ioloop.IOLoop.READ:
   320|             if not self.fds[fd].reading:
   321|                 self.fds[fd].reading = True
   322|                 self.reactor.addReader(self.fds[fd])
   323|         else:
   324|             if self.fds[fd].reading:
   325|                 self.fds[fd].reading = False
   326|                 self.reactor.removeReader(self.fds[fd])
   327|         if events & salt.ext.tornado.ioloop.IOLoop.WRITE:
   328|             if not self.fds[fd].writing:
   329|                 self.fds[fd].writing = True
   330|                 self.reactor.addWriter(self.fds[fd])
   331|         else:
   332|             if self.fds[fd].writing:
   333|                 self.fds[fd].writing = False
   334|                 self.reactor.removeWriter(self.fds[fd])
   335|     def remove_handler(self, fd):
   336|         fd, fileobj = self.split_fd(fd)
   337|         if fd not in self.fds:
   338|             return
   339|         self.fds[fd].lost = True
   340|         if self.fds[fd].reading:
   341|             self.reactor.removeReader(self.fds[fd])
   342|         if self.fds[fd].writing:
   343|             self.reactor.removeWriter(self.fds[fd])
   344|         del self.fds[fd]
   345|     def start(self):
   346|         old_current = IOLoop.current(instance=False)
   347|         try:
   348|             self._setup_logging()
   349|             self.make_current()
   350|             self.reactor.run()
   351|         finally:
   352|             if old_current is None:
   353|                 IOLoop.clear_current()
   354|             else:
   355|                 old_current.make_current()
   356|     def stop(self):
   357|         self.reactor.crash()
   358|     def add_timeout(self, deadline, callback, *args, **kwargs):
   359|         if isinstance(deadline, numbers.Real):
   360|             delay = max(deadline - self.time(), 0)
   361|         elif isinstance(deadline, datetime.timedelta):
   362|             delay = timedelta_to_seconds(deadline)
   363|         else:
   364|             raise TypeError("Unsupported deadline %r")
   365|         return self.reactor.callLater(
   366|             delay, self._run_callback,
   367|             functools.partial(wrap(callback), *args, **kwargs))
   368|     def remove_timeout(self, timeout):
   369|         if timeout.active():
   370|             timeout.cancel()
   371|     def add_callback(self, callback, *args, **kwargs):
   372|         self.reactor.callFromThread(
   373|             self._run_callback,
   374|             functools.partial(wrap(callback), *args, **kwargs))
   375|     def add_callback_from_signal(self, callback, *args, **kwargs):
   376|         self.add_callback(callback, *args, **kwargs)
   377| class TwistedResolver(Resolver):
   378|     """Twisted-based asynchronous resolver.
   379|     This is a non-blocking and non-threaded resolver.  It is
   380|     recommended only when threads cannot be used, since it has
   381|     limitations compared to the standard ``getaddrinfo``-based
   382|     `~tornado.netutil.Resolver` and
   383|     `~tornado.netutil.ThreadedResolver`.  Specifically, it returns at
   384|     most one result, and arguments other than ``host`` and ``family``
   385|     are ignored.  It may fail to resolve when ``family`` is not
   386|     ``socket.AF_UNSPEC``.
   387|     Requires Twisted 12.1 or newer.
   388|     .. versionchanged:: 4.1
   389|        The ``io_loop`` argument is deprecated.
   390|     """
   391|     def initialize(self, io_loop=None):
   392|         self.io_loop = io_loop or IOLoop.current()
   393|         self.reactor = salt.ext.tornado.platform.twisted.TornadoReactor(io_loop)
   394|         host_resolver = twisted.names.hosts.Resolver('/etc/hosts')
   395|         cache_resolver = twisted.names.cache.CacheResolver(reactor=self.reactor)
   396|         real_resolver = twisted.names.client.Resolver('/etc/resolv.conf',
   397|                                                       reactor=self.reactor)
   398|         self.resolver = twisted.names.resolve.ResolverChain(
   399|             [host_resolver, cache_resolver, real_resolver])
   400|     @gen.coroutine
   401|     def resolve(self, host, port, family=0):
   402|         if twisted.internet.abstract.isIPAddress(host):
   403|             resolved = host
   404|             resolved_family = socket.AF_INET
   405|         elif twisted.internet.abstract.isIPv6Address(host):
   406|             resolved = host
   407|             resolved_family = socket.AF_INET6
   408|         else:
   409|             deferred = self.resolver.getHostByName(utf8(host))
   410|             resolved = yield gen.Task(deferred.addBoth)
   411|             if isinstance(resolved, failure.Failure):
   412|                 try:
   413|                     resolved.raiseException()
   414|                 except twisted.names.error.DomainError as e:
   415|                     raise IOError(e)
   416|             elif twisted.internet.abstract.isIPAddress(resolved):
   417|                 resolved_family = socket.AF_INET
   418|             elif twisted.internet.abstract.isIPv6Address(resolved):
   419|                 resolved_family = socket.AF_INET6
   420|             else:
   421|                 resolved_family = socket.AF_UNSPEC
   422|         if family != socket.AF_UNSPEC and family != resolved_family:
   423|             raise Exception('Requested socket family %d but got %d' %
   424|                             (family, resolved_family))
   425|         result = [
   426|             (resolved_family, (resolved, port)),
   427|         ]
   428|         raise gen.Return(result)
   429| if hasattr(gen.convert_yielded, 'register'):
   430|     @gen.convert_yielded.register(Deferred)  # type: ignore
   431|     def _(d):
   432|         f = Future()
   433|         def errback(failure):
   434|             try:
   435|                 failure.raiseException()
   436|                 raise Exception("errback called without error")
   437|             except:
   438|                 f.set_exc_info(sys.exc_info())
   439|         d.addCallbacks(f.set_result, errback)
   440|         return f


# ====================================================================
# FILE: salt/ext/tornado/platform/windows.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-11 ---
     1| from __future__ import absolute_import, division, print_function
     2| import ctypes  # type: ignore
     3| import ctypes.wintypes  # type: ignore
     4| SetHandleInformation = ctypes.windll.kernel32.SetHandleInformation
     5| SetHandleInformation.argtypes = (ctypes.wintypes.HANDLE, ctypes.wintypes.DWORD, ctypes.wintypes.DWORD)
     6| SetHandleInformation.restype = ctypes.wintypes.BOOL
     7| HANDLE_FLAG_INHERIT = 0x00000001
     8| def set_close_exec(fd):
     9|     success = SetHandleInformation(fd, HANDLE_FLAG_INHERIT, 0)
    10|     if not success:
    11|         raise ctypes.WinError()


# ====================================================================
# FILE: salt/ext/tornado/process.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-277 ---
     1| """Utilities for working with multiple processes, including both forking
     2| the server into multiple processes and managing subprocesses.
     3| """
     4| from __future__ import absolute_import, division, print_function
     5| import errno
     6| import os
     7| import signal
     8| import subprocess
     9| import sys
    10| import time
    11| from binascii import hexlify
    12| from salt.ext.tornado.concurrent import Future
    13| from salt.ext.tornado import ioloop
    14| from salt.ext.tornado.iostream import PipeIOStream
    15| from salt.ext.tornado.log import gen_log
    16| from salt.ext.tornado.platform.auto import set_close_exec
    17| from salt.ext.tornado import stack_context
    18| from salt.ext.tornado.util import errno_from_exception, PY3
    19| try:
    20|     import multiprocessing
    21| except ImportError:
    22|     multiprocessing = None
    23| if PY3:
    24|     long = int
    25| try:
    26|     CalledProcessError = subprocess.CalledProcessError
    27| except AttributeError:
    28|     if 'APPENGINE_RUNTIME' not in os.environ:
    29|         raise
    30| def cpu_count():
    31|     """Returns the number of processors on this machine."""
    32|     if multiprocessing is None:
    33|         return 1
    34|     try:
    35|         return multiprocessing.cpu_count()
    36|     except NotImplementedError:
    37|         pass
    38|     try:
    39|         return os.sysconf("SC_NPROCESSORS_CONF")
    40|     except (AttributeError, ValueError):
    41|         pass
    42|     gen_log.error("Could not detect number of processors; assuming 1")
    43|     return 1
    44| def _reseed_random():
    45|     if 'random' not in sys.modules:
    46|         return
    47|     import random
    48|     try:
    49|         seed = long(hexlify(os.urandom(16)), 16)
    50|     except NotImplementedError:
    51|         seed = int(time.time() * 1000) ^ os.getpid()
    52|     random.seed(seed)
    53| def _pipe_cloexec():
    54|     r, w = os.pipe()
    55|     set_close_exec(r)
    56|     set_close_exec(w)
    57|     return r, w
    58| _task_id = None
    59| def fork_processes(num_processes, max_restarts=100):
    60|     """Starts multiple worker processes.
    61|     If ``num_processes`` is None or <= 0, we detect the number of cores
    62|     available on this machine and fork that number of child
    63|     processes. If ``num_processes`` is given and > 0, we fork that
    64|     specific number of sub-processes.
    65|     Since we use processes and not threads, there is no shared memory
    66|     between any server code.
    67|     Note that multiple processes are not compatible with the autoreload
    68|     module (or the ``autoreload=True`` option to `tornado.web.Application`
    69|     which defaults to True when ``debug=True``).
    70|     When using multiple processes, no IOLoops can be created or
    71|     referenced until after the call to ``fork_processes``.
    72|     In each child process, ``fork_processes`` returns its *task id*, a
    73|     number between 0 and ``num_processes``.  Processes that exit
    74|     abnormally (due to a signal or non-zero exit status) are restarted
    75|     with the same id (up to ``max_restarts`` times).  In the parent
    76|     process, ``fork_processes`` returns None if all child processes
    77|     have exited normally, but will otherwise only exit by throwing an
    78|     exception.
    79|     """
    80|     global _task_id
    81|     assert _task_id is None
    82|     if num_processes is None or num_processes <= 0:
    83|         num_processes = cpu_count()
    84|     if ioloop.IOLoop.initialized():
    85|         raise RuntimeError("Cannot run in multiple processes: IOLoop instance "
    86|                            "has already been initialized. You cannot call "
    87|                            "IOLoop.instance() before calling start_processes()")
    88|     gen_log.info("Starting %d processes", num_processes)
    89|     children = {}
    90|     def start_child(i):
    91|         pid = os.fork()
    92|         if pid == 0:
    93|             _reseed_random()
    94|             global _task_id
    95|             _task_id = i
    96|             return i
    97|         else:
    98|             children[pid] = i
    99|             return None
   100|     for i in range(num_processes):
   101|         id = start_child(i)
   102|         if id is not None:
   103|             return id
   104|     num_restarts = 0
   105|     while children:
   106|         try:
   107|             pid, status = os.wait()
   108|         except OSError as e:
   109|             if errno_from_exception(e) == errno.EINTR:
   110|                 continue
   111|             raise
   112|         if pid not in children:
   113|             continue
   114|         id = children.pop(pid)
   115|         if os.WIFSIGNALED(status):
   116|             gen_log.warning("child %d (pid %d) killed by signal %d, restarting",
   117|                             id, pid, os.WTERMSIG(status))
   118|         elif os.WEXITSTATUS(status) != 0:
   119|             gen_log.warning("child %d (pid %d) exited with status %d, restarting",
   120|                             id, pid, os.WEXITSTATUS(status))
   121|         else:
   122|             gen_log.info("child %d (pid %d) exited normally", id, pid)
   123|             continue
   124|         num_restarts += 1
   125|         if num_restarts > max_restarts:
   126|             raise RuntimeError("Too many child restarts, giving up")
   127|         new_id = start_child(id)
   128|         if new_id is not None:
   129|             return new_id
   130|     sys.exit(0)
   131| def task_id():
   132|     """Returns the current task id, if any.
   133|     Returns None if this process was not created by `fork_processes`.
   134|     """
   135|     global _task_id
   136|     return _task_id
   137| class Subprocess(object):
   138|     """Wraps ``subprocess.Popen`` with IOStream support.
   139|     The constructor is the same as ``subprocess.Popen`` with the following
   140|     additions:
   141|     * ``stdin``, ``stdout``, and ``stderr`` may have the value
   142|       ``tornado.process.Subprocess.STREAM``, which will make the corresponding
   143|       attribute of the resulting Subprocess a `.PipeIOStream`.
   144|     * A new keyword argument ``io_loop`` may be used to pass in an IOLoop.
   145|     The ``Subprocess.STREAM`` option and the ``set_exit_callback`` and
   146|     ``wait_for_exit`` methods do not work on Windows. There is
   147|     therefore no reason to use this class instead of
   148|     ``subprocess.Popen`` on that platform.
   149|     .. versionchanged:: 4.1
   150|        The ``io_loop`` argument is deprecated.
   151|     """
   152|     STREAM = object()
   153|     _initialized = False
   154|     _waiting = {}  # type: ignore
   155|     def __init__(self, *args, **kwargs):
   156|         self.io_loop = kwargs.pop('io_loop', None) or ioloop.IOLoop.current()
   157|         pipe_fds = []
   158|         to_close = []
   159|         if kwargs.get('stdin') is Subprocess.STREAM:
   160|             in_r, in_w = _pipe_cloexec()
   161|             kwargs['stdin'] = in_r
   162|             pipe_fds.extend((in_r, in_w))
   163|             to_close.append(in_r)
   164|             self.stdin = PipeIOStream(in_w, io_loop=self.io_loop)
   165|         if kwargs.get('stdout') is Subprocess.STREAM:
   166|             out_r, out_w = _pipe_cloexec()
   167|             kwargs['stdout'] = out_w
   168|             pipe_fds.extend((out_r, out_w))
   169|             to_close.append(out_w)
   170|             self.stdout = PipeIOStream(out_r, io_loop=self.io_loop)
   171|         if kwargs.get('stderr') is Subprocess.STREAM:
   172|             err_r, err_w = _pipe_cloexec()
   173|             kwargs['stderr'] = err_w
   174|             pipe_fds.extend((err_r, err_w))
   175|             to_close.append(err_w)
   176|             self.stderr = PipeIOStream(err_r, io_loop=self.io_loop)
   177|         try:
   178|             self.proc = subprocess.Popen(*args, **kwargs)
   179|         except:
   180|             for fd in pipe_fds:
   181|                 os.close(fd)
   182|             raise
   183|         for fd in to_close:
   184|             os.close(fd)
   185|         for attr in ['stdin', 'stdout', 'stderr', 'pid']:
   186|             if not hasattr(self, attr):  # don't clobber streams set above
   187|                 setattr(self, attr, getattr(self.proc, attr))
   188|         self._exit_callback = None
   189|         self.returncode = None
   190|     def set_exit_callback(self, callback):
   191|         """Runs ``callback`` when this process exits.
   192|         The callback takes one argument, the return code of the process.
   193|         This method uses a ``SIGCHLD`` handler, which is a global setting
   194|         and may conflict if you have other libraries trying to handle the
   195|         same signal.  If you are using more than one ``IOLoop`` it may
   196|         be necessary to call `Subprocess.initialize` first to designate
   197|         one ``IOLoop`` to run the signal handlers.
   198|         In many cases a close callback on the stdout or stderr streams
   199|         can be used as an alternative to an exit callback if the
   200|         signal handler is causing a problem.
   201|         """
   202|         self._exit_callback = stack_context.wrap(callback)
   203|         Subprocess.initialize(self.io_loop)
   204|         Subprocess._waiting[self.pid] = self
   205|         Subprocess._try_cleanup_process(self.pid)
   206|     def wait_for_exit(self, raise_error=True):
   207|         """Returns a `.Future` which resolves when the process exits.
   208|         Usage::
   209|             ret = yield proc.wait_for_exit()
   210|         This is a coroutine-friendly alternative to `set_exit_callback`
   211|         (and a replacement for the blocking `subprocess.Popen.wait`).
   212|         By default, raises `subprocess.CalledProcessError` if the process
   213|         has a non-zero exit status. Use ``wait_for_exit(raise_error=False)``
   214|         to suppress this behavior and return the exit status without raising.
   215|         .. versionadded:: 4.2
   216|         """
   217|         future = Future()
   218|         def callback(ret):
   219|             if ret != 0 and raise_error:
   220|                 future.set_exception(CalledProcessError(ret, None))
   221|             else:
   222|                 future.set_result(ret)
   223|         self.set_exit_callback(callback)
   224|         return future
   225|     @classmethod
   226|     def initialize(cls, io_loop=None):
   227|         """Initializes the ``SIGCHLD`` handler.
   228|         The signal handler is run on an `.IOLoop` to avoid locking issues.
   229|         Note that the `.IOLoop` used for signal handling need not be the
   230|         same one used by individual Subprocess objects (as long as the
   231|         ``IOLoops`` are each running in separate threads).
   232|         .. versionchanged:: 4.1
   233|            The ``io_loop`` argument is deprecated.
   234|         """
   235|         if cls._initialized:
   236|             return
   237|         if io_loop is None:
   238|             io_loop = ioloop.IOLoop.current()
   239|         cls._old_sigchld = signal.signal(
   240|             signal.SIGCHLD,
   241|             lambda sig, frame: io_loop.add_callback_from_signal(cls._cleanup))
   242|         cls._initialized = True
   243|     @classmethod
   244|     def uninitialize(cls):
   245|         """Removes the ``SIGCHLD`` handler."""
   246|         if not cls._initialized:
   247|             return
   248|         signal.signal(signal.SIGCHLD, cls._old_sigchld)
   249|         cls._initialized = False
   250|     @classmethod
   251|     def _cleanup(cls):
   252|         for pid in list(cls._waiting.keys()):  # make a copy
   253|             cls._try_cleanup_process(pid)
   254|     @classmethod
   255|     def _try_cleanup_process(cls, pid):
   256|         try:
   257|             ret_pid, status = os.waitpid(pid, os.WNOHANG)
   258|         except OSError as e:
   259|             if errno_from_exception(e) == errno.ECHILD:
   260|                 return
   261|         if ret_pid == 0:
   262|             return
   263|         assert ret_pid == pid
   264|         subproc = cls._waiting.pop(pid)
   265|         subproc.io_loop.add_callback_from_signal(
   266|             subproc._set_returncode, status)
   267|     def _set_returncode(self, status):
   268|         if os.WIFSIGNALED(status):
   269|             self.returncode = -os.WTERMSIG(status)
   270|         else:
   271|             assert os.WIFEXITED(status)
   272|             self.returncode = os.WEXITSTATUS(status)
   273|         self.proc.returncode = self.returncode
   274|         if self._exit_callback:
   275|             callback = self._exit_callback
   276|             self._exit_callback = None
   277|             callback(self.returncode)


# ====================================================================
# FILE: salt/ext/tornado/queues.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-266 ---
     1| """Asynchronous queues for coroutines.
     2| .. warning::
     3|    Unlike the standard library's `queue` module, the classes defined here
     4|    are *not* thread-safe. To use these queues from another thread,
     5|    use `.IOLoop.add_callback` to transfer control to the `.IOLoop` thread
     6|    before calling any queue methods.
     7| """
     8| from __future__ import absolute_import, division, print_function
     9| import collections
    10| import heapq
    11| from salt.ext.tornado import gen, ioloop
    12| from salt.ext.tornado.concurrent import Future
    13| from salt.ext.tornado.locks import Event
    14| __all__ = ['Queue', 'PriorityQueue', 'LifoQueue', 'QueueFull', 'QueueEmpty']
    15| class QueueEmpty(Exception):
    16|     """Raised by `.Queue.get_nowait` when the queue has no items."""
    17|     pass
    18| class QueueFull(Exception):
    19|     """Raised by `.Queue.put_nowait` when a queue is at its maximum size."""
    20|     pass
    21| def _set_timeout(future, timeout):
    22|     if timeout:
    23|         def on_timeout():
    24|             future.set_exception(gen.TimeoutError())
    25|         io_loop = ioloop.IOLoop.current()
    26|         timeout_handle = io_loop.add_timeout(timeout, on_timeout)
    27|         future.add_done_callback(
    28|             lambda _: io_loop.remove_timeout(timeout_handle))
    29| class _QueueIterator(object):
    30|     def __init__(self, q):
    31|         self.q = q
    32|     def __anext__(self):
    33|         return self.q.get()
    34| class Queue(object):
    35|     """Coordinate producer and consumer coroutines.
    36|     If maxsize is 0 (the default) the queue size is unbounded.
    37|     .. testcode::
    38|         from salt.ext.tornado import gen
    39|         from salt.ext.tornado.ioloop import IOLoop
    40|         from salt.ext.tornado.queues import Queue
    41|         q = Queue(maxsize=2)
    42|         @gen.coroutine
    43|         def consumer():
    44|             while True:
    45|                 item = yield q.get()
    46|                 try:
    47|                     print('Doing work on %s' % item)
    48|                     yield gen.sleep(0.01)
    49|                 finally:
    50|                     q.task_done()
    51|         @gen.coroutine
    52|         def producer():
    53|             for item in range(5):
    54|                 yield q.put(item)
    55|                 print('Put %s' % item)
    56|         @gen.coroutine
    57|         def main():
    58|             IOLoop.current().spawn_callback(consumer)
    59|             yield producer()     # Wait for producer to put all tasks.
    60|             yield q.join()       # Wait for consumer to finish all tasks.
    61|             print('Done')
    62|         IOLoop.current().run_sync(main)
    63|     .. testoutput::
    64|         Put 0
    65|         Put 1
    66|         Doing work on 0
    67|         Put 2
    68|         Doing work on 1
    69|         Put 3
    70|         Doing work on 2
    71|         Put 4
    72|         Doing work on 3
    73|         Doing work on 4
    74|         Done
    75|     In Python 3.5, `Queue` implements the async iterator protocol, so
    76|     ``consumer()`` could be rewritten as::
    77|         async def consumer():
    78|             async for item in q:
    79|                 try:
    80|                     print('Doing work on %s' % item)
    81|                     yield gen.sleep(0.01)
    82|                 finally:
    83|                     q.task_done()
    84|     .. versionchanged:: 4.3
    85|        Added ``async for`` support in Python 3.5.
    86|     """
    87|     def __init__(self, maxsize=0):
    88|         if maxsize is None:
    89|             raise TypeError("maxsize can't be None")
    90|         if maxsize < 0:
    91|             raise ValueError("maxsize can't be negative")
    92|         self._maxsize = maxsize
    93|         self._init()
    94|         self._getters = collections.deque([])  # Futures.
    95|         self._putters = collections.deque([])  # Pairs of (item, Future).
    96|         self._unfinished_tasks = 0
    97|         self._finished = Event()
    98|         self._finished.set()
    99|     @property
   100|     def maxsize(self):
   101|         """Number of items allowed in the queue."""
   102|         return self._maxsize
   103|     def qsize(self):
   104|         """Number of items in the queue."""
   105|         return len(self._queue)
   106|     def empty(self):
   107|         return not self._queue
   108|     def full(self):
   109|         if self.maxsize == 0:
   110|             return False
   111|         else:
   112|             return self.qsize() >= self.maxsize
   113|     def put(self, item, timeout=None):
   114|         """Put an item into the queue, perhaps waiting until there is room.
   115|         Returns a Future, which raises `tornado.gen.TimeoutError` after a
   116|         timeout.
   117|         """
   118|         try:
   119|             self.put_nowait(item)
   120|         except QueueFull:
   121|             future = Future()
   122|             self._putters.append((item, future))
   123|             _set_timeout(future, timeout)
   124|             return future
   125|         else:
   126|             return gen._null_future
   127|     def put_nowait(self, item):
   128|         """Put an item into the queue without blocking.
   129|         If no free slot is immediately available, raise `QueueFull`.
   130|         """
   131|         self._consume_expired()
   132|         if self._getters:
   133|             assert self.empty(), "queue non-empty, why are getters waiting?"
   134|             getter = self._getters.popleft()
   135|             self.__put_internal(item)
   136|             getter.set_result(self._get())
   137|         elif self.full():
   138|             raise QueueFull
   139|         else:
   140|             self.__put_internal(item)
   141|     def get(self, timeout=None):
   142|         """Remove and return an item from the queue.
   143|         Returns a Future which resolves once an item is available, or raises
   144|         `tornado.gen.TimeoutError` after a timeout.
   145|         """
   146|         future = Future()
   147|         try:
   148|             future.set_result(self.get_nowait())
   149|         except QueueEmpty:
   150|             self._getters.append(future)
   151|             _set_timeout(future, timeout)
   152|         return future
   153|     def get_nowait(self):
   154|         """Remove and return an item from the queue without blocking.
   155|         Return an item if one is immediately available, else raise
   156|         `QueueEmpty`.
   157|         """
   158|         self._consume_expired()
   159|         if self._putters:
   160|             assert self.full(), "queue not full, why are putters waiting?"
   161|             item, putter = self._putters.popleft()
   162|             self.__put_internal(item)
   163|             putter.set_result(None)
   164|             return self._get()
   165|         elif self.qsize():
   166|             return self._get()
   167|         else:
   168|             raise QueueEmpty
   169|     def task_done(self):
   170|         """Indicate that a formerly enqueued task is complete.
   171|         Used by queue consumers. For each `.get` used to fetch a task, a
   172|         subsequent call to `.task_done` tells the queue that the processing
   173|         on the task is complete.
   174|         If a `.join` is blocking, it resumes when all items have been
   175|         processed; that is, when every `.put` is matched by a `.task_done`.
   176|         Raises `ValueError` if called more times than `.put`.
   177|         """
   178|         if self._unfinished_tasks <= 0:
   179|             raise ValueError('task_done() called too many times')
   180|         self._unfinished_tasks -= 1
   181|         if self._unfinished_tasks == 0:
   182|             self._finished.set()
   183|     def join(self, timeout=None):
   184|         """Block until all items in the queue are processed.
   185|         Returns a Future, which raises `tornado.gen.TimeoutError` after a
   186|         timeout.
   187|         """
   188|         return self._finished.wait(timeout)
   189|     def __aiter__(self):
   190|         return _QueueIterator(self)
   191|     def _init(self):
   192|         self._queue = collections.deque()
   193|     def _get(self):
   194|         return self._queue.popleft()
   195|     def _put(self, item):
   196|         self._queue.append(item)
   197|     def __put_internal(self, item):
   198|         self._unfinished_tasks += 1
   199|         self._finished.clear()
   200|         self._put(item)
   201|     def _consume_expired(self):
   202|         while self._putters and self._putters[0][1].done():
   203|             self._putters.popleft()
   204|         while self._getters and self._getters[0].done():
   205|             self._getters.popleft()
   206|     def __repr__(self):
   207|         return '<%s at %s %s>' % (
   208|             type(self).__name__, hex(id(self)), self._format())
   209|     def __str__(self):
   210|         return '<%s %s>' % (type(self).__name__, self._format())
   211|     def _format(self):
   212|         result = 'maxsize=%r' % (self.maxsize, )
   213|         if getattr(self, '_queue', None):
   214|             result += ' queue=%r' % self._queue
   215|         if self._getters:
   216|             result += ' getters[%s]' % len(self._getters)
   217|         if self._putters:
   218|             result += ' putters[%s]' % len(self._putters)
   219|         if self._unfinished_tasks:
   220|             result += ' tasks=%s' % self._unfinished_tasks
   221|         return result
   222| class PriorityQueue(Queue):
   223|     """A `.Queue` that retrieves entries in priority order, lowest first.
   224|     Entries are typically tuples like ``(priority number, data)``.
   225|     .. testcode::
   226|         from salt.ext.tornado.queues import PriorityQueue
   227|         q = PriorityQueue()
   228|         q.put((1, 'medium-priority item'))
   229|         q.put((0, 'high-priority item'))
   230|         q.put((10, 'low-priority item'))
   231|         print(q.get_nowait())
   232|         print(q.get_nowait())
   233|         print(q.get_nowait())
   234|     .. testoutput::
   235|         (0, 'high-priority item')
   236|         (1, 'medium-priority item')
   237|         (10, 'low-priority item')
   238|     """
   239|     def _init(self):
   240|         self._queue = []
   241|     def _put(self, item):
   242|         heapq.heappush(self._queue, item)
   243|     def _get(self):
   244|         return heapq.heappop(self._queue)
   245| class LifoQueue(Queue):
   246|     """A `.Queue` that retrieves the most recently put items first.
   247|     .. testcode::
   248|         from salt.ext.tornado.queues import LifoQueue
   249|         q = LifoQueue()
   250|         q.put(3)
   251|         q.put(2)
   252|         q.put(1)
   253|         print(q.get_nowait())
   254|         print(q.get_nowait())
   255|         print(q.get_nowait())
   256|     .. testoutput::
   257|         1
   258|         2
   259|         3
   260|     """
   261|     def _init(self):
   262|         self._queue = []
   263|     def _put(self, item):
   264|         self._queue.append(item)
   265|     def _get(self):
   266|         return self._queue.pop()

