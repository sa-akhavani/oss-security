--- a/doc/_ext/vaultpolicylexer.py
+++ b//dev/null
@@ -1,21 +0,0 @@
-from pygments.lexer import bygroups, inherit
-from pygments.lexers.configs import TerraformLexer
-from pygments.token import Keyword, Name, Punctuation, Whitespace
-class VaultPolicyLexer(TerraformLexer):
-    aliases = ["vaultpolicy"]
-    filenames = ["*.hcl"]
-    mimetypes = ["application/x-hcl-policy"]
-    tokens = {
-        "basic": [
-            inherit,
-            (
-                r"(path)(\s+)(\".*\")(\s+)(\{)",
-                bygroups(
-                    Keyword.Reserved, Whitespace, Name.Variable, Whitespace, Punctuation
-                ),
-            ),
-        ],
-    }
-def setup(app):
-    app.add_lexer("vaultpolicy", VaultPolicyLexer)
-    return {"parallel_read_safe": True}

--- a/doc/conf.py
+++ b/doc/conf.py
@@ -115,21 +115,20 @@
     "sphinx.ext.autodoc",
     "sphinx.ext.napoleon",
     "sphinx.ext.autosummary",
     "sphinx.ext.extlinks",
     "sphinx.ext.imgconverter",
     "sphinx.ext.intersphinx",
     "sphinxcontrib.httpdomain",
     "saltrepo",
     "myst_parser",
     "sphinxcontrib.spelling",
-    "vaultpolicylexer",
 ]
 modindex_common_prefix = ["salt."]
 autosummary_generate = True
 autosummary_generate_overwrite = False
 autodoc_mock_imports = []
 stripped_release = re.sub(r"-\d+-g[0-9a-f]+$", "", release)
 rst_prolog = """\
 .. |current_release_doc| replace:: :doc:`/topics/releases/{release}`
 .. |saltrepo| replace:: https://github.com/saltstack/salt
 .. _`salt-users`: https://groups.google.com/forum/#!forum/salt-users
@@ -243,20 +242,21 @@
     r"http://localhost",
     r"https://groups.google.com/forum/#!forum/salt-users",
     r"https://www.elastic.co/logstash/docs/latest/inputs/udp",
     r"https://www.elastic.co/logstash/docs/latest/inputs/zeromq",
     r"http://www.youtube.com/saltstack",
     r"https://raven.readthedocs.io",
     r"https://getsentry.com",
     r"https://salt-cloud.readthedocs.io",
     r"https://salt.readthedocs.io",
     r"http://www.pip-installer.org/",
+    r"http://www.windowsazure.com/",
     r"https://github.com/watching",
     r"dash-feed://",
     r"https://github.com/saltstack/salt/",
     r"https://bootstrap.saltproject.io",
     r"https://raw.githubusercontent.com/saltstack/salt-bootstrap/stable/bootstrap-salt.sh",
     r"media.readthedocs.org/dash/salt/latest/salt.xml",
     r"https://portal.aws.amazon.com/gp/aws/securityCredentials",
     r"dash-feed://https%3A//media.readthedocs.org/dash/salt/latest/salt.xml",
     r"(?i)dns:.*",
     r"TCP:4506",

--- a/noxfile.py
+++ b/noxfile.py
@@ -62,21 +62,21 @@
     COVERAGE_FILE = str(COVERAGE_OUTPUT_DIR / ".coverage")
 IS_DARWIN = sys.platform.lower().startswith("darwin")
 IS_WINDOWS = sys.platform.lower().startswith("win")
 IS_FREEBSD = sys.platform.lower().startswith("freebsd")
 IS_LINUX = sys.platform.lower().startswith("linux")
 ONEDIR_ARTIFACT_PATH = ARTIFACTS_DIR / "salt"
 if IS_WINDOWS:
     ONEDIR_PYTHON_PATH = ONEDIR_ARTIFACT_PATH / "Scripts" / "python.exe"
 else:
     ONEDIR_PYTHON_PATH = ONEDIR_ARTIFACT_PATH / "bin" / "python3"
-_PYTHON_VERSIONS = ("3", "3.8", "3.9", "3.10", "3.11")
+_PYTHON_VERSIONS = ("3", "3.5", "3.6", "3.7", "3.8", "3.9", "3.10")
 nox.options.reuse_existing_virtualenvs = True
 os.chdir(str(REPO_ROOT))
 RUNTESTS_LOGFILE = ARTIFACTS_DIR.joinpath(
     "logs",
     "runtests-{}.log".format(datetime.datetime.now().strftime("%Y%m%d%H%M%S.%f")),
 )
 os.environ["PYTHONDONTWRITEBYTECODE"] = "1"
 def session_warn(session, message):
     try:
         session.warn(message)
@@ -137,22 +137,24 @@
         )
         version_info = tuple(
             int(part)
             for part in session_py_version.strip().split(".")
             if part.isdigit()
         )
         session._runner._real_python_version_info = version_info
     return version_info
 def _get_pydir(session):
     version_info = _get_session_python_version_info(session)
-    if version_info < (3, 8):
-        session.error("Only Python >= 3.8 is supported")
+    if version_info < (3, 5):
+        session.error("Only Python >= 3.5 is supported")
+    if IS_WINDOWS and version_info < (3, 6):
+        session.error("Only Python >= 3.6 is supported on Windows")
     return "py{}.{}".format(*version_info)
 def _get_pip_requirements_file(session, crypto=None, requirements_type="ci"):
     assert requirements_type in ("ci", "pkg")
     pydir = _get_pydir(session)
     if IS_WINDOWS:
         if crypto is None:
             _requirements_file = os.path.join(
                 "requirements", "static", requirements_type, pydir, "windows.txt"
             )
             if os.path.exists(_requirements_file):
@@ -769,33 +771,24 @@
 def test_cloud(session, coverage):
     """
     pytest cloud tests session
     """
     pydir = _get_pydir(session)
     if pydir == "py3.5":
         session.error(
             "Due to conflicting and unsupported requirements the cloud tests only run on Py3.6+"
         )
     if _upgrade_pip_setuptools_and_wheel(session):
-        linux_requirements_file = os.path.join(
-            "requirements", "static", "ci", pydir, "linux.txt"
-        )
-        cloud_requirements_file = os.path.join(
+        requirements_file = os.path.join(
             "requirements", "static", "ci", pydir, "cloud.txt"
         )
-        install_command = [
-            "--progress-bar=off",
-            "-r",
-            linux_requirements_file,
-            "-r",
-            cloud_requirements_file,
-        ]
+        install_command = ["--progress-bar=off", "-r", requirements_file]
         session.install(*install_command, silent=PIP_INSTALL_SILENT)
     cmd_args = [
         "--run-expensive",
         "-k",
         "cloud",
     ] + session.posargs
     _pytest(session, coverage=coverage, cmd_args=cmd_args)
 @nox.session(python=_PYTHON_VERSIONS, name="pytest-cloud")
 @nox.parametrize("coverage", [False, True])
 def pytest_cloud(session, coverage):
@@ -1105,31 +1098,33 @@
                         str(fixed_link.relative_to(REPO_ROOT)),
                     )
                     broken_link.unlink()
                     broken_link.symlink_to(fixed_link)
                 continue
             if not path.is_file():
                 continue
             if platform != "windows":
                 try:
                     fpath = pathlib.Path(path)
-                    contents = fpath.read_text().splitlines()
+                    contents = fpath.read_text(encoding="utf-8").splitlines()
                     if (
                         contents[0].startswith("#!")
                         and contents[0].endswith("python")
                         and contents[0] != fixed_shebang
                     ):
                         session.log(
                             "Fixing broken shebang in %r",
                             str(fpath.relative_to(REPO_ROOT)),
                         )
-                        fpath.write_text("\n".join([fixed_shebang] + contents[1:]))
+                        fpath.write_text(
+                            "\n".join([fixed_shebang] + contents[1:]), encoding="utf-8"
+                        )
                 except UnicodeDecodeError:
                     pass
 @nox.session(python=False, name="compress-dependencies")
 def compress_dependencies(session):
     if not session.posargs:
         session.error(
             "The 'compress-dependencies' session target needs "
             "two arguments, '<platform> <arch>'."
         )
     try:
@@ -1242,144 +1237,110 @@
     def __init__(self, first, second):
         self._first = first
         self._second = second
     def write(self, b):
         wrote = self._first.write(b)
         self._first.flush()
         self._second.write(b)
         self._second.flush()
     def fileno(self):
         return self._first.fileno()
-def _lint(
-    session, rcfile, flags, paths, tee_output=True, upgrade_setuptools_and_pip=True
-):
+def _lint(session, rcfile, flags, paths, upgrade_setuptools_and_pip=True):
     if _upgrade_pip_setuptools_and_wheel(session, upgrade=upgrade_setuptools_and_pip):
-        linux_requirements_file = os.path.join(
+        base_requirements_file = os.path.join(
             "requirements", "static", "ci", _get_pydir(session), "linux.txt"
         )
         lint_requirements_file = os.path.join(
             "requirements", "static", "ci", _get_pydir(session), "lint.txt"
         )
         install_command = [
             "--progress-bar=off",
             "-r",
-            linux_requirements_file,
+            base_requirements_file,
             "-r",
             lint_requirements_file,
         ]
         session.install(*install_command, silent=PIP_INSTALL_SILENT)
-    if tee_output:
-        session.run("pylint", "--version")
-        pylint_report_path = os.environ.get("PYLINT_REPORT")
     cmd_args = ["pylint", f"--rcfile={rcfile}"] + list(flags) + list(paths)
     cmd_kwargs = {"env": {"PYTHONUNBUFFERED": "1"}}
-    if tee_output:
-        stdout = tempfile.TemporaryFile(mode="w+b")
-        cmd_kwargs["stdout"] = Tee(stdout, sys.__stdout__)
-    lint_failed = False
-    try:
-        session.run(*cmd_args, **cmd_kwargs)
-    except CommandFailed:
-        lint_failed = True
-        raise
-    finally:
-        if tee_output:
-            stdout.seek(0)
-            contents = stdout.read()
-            if contents:
-                contents = contents.decode("utf-8")
-                sys.stdout.write(contents)
-                sys.stdout.flush()
-                if pylint_report_path:
-                    with open(pylint_report_path, "w") as wfh:
-                        wfh.write(contents)
-                    session.log("Report file written to %r", pylint_report_path)
-            stdout.close()
+    session.run(*cmd_args, **cmd_kwargs)
 def _lint_pre_commit(session, rcfile, flags, paths):
     if "VIRTUAL_ENV" not in os.environ:
         session.error(
             "This should be running from within a virtualenv and "
             "'VIRTUAL_ENV' was not found as an environment variable."
         )
     if "pre-commit" not in os.environ["VIRTUAL_ENV"]:
         session.error(
             "This should be running from within a pre-commit virtualenv and "
             "'VIRTUAL_ENV'({}) does not appear to be a pre-commit virtualenv.".format(
                 os.environ["VIRTUAL_ENV"]
             )
         )
     from nox.virtualenv import VirtualEnv
-    try:
-        session._runner.venv = VirtualEnv(  # pylint: disable=unexpected-keyword-arg
-            os.environ["VIRTUAL_ENV"],
-            interpreter=session._runner.func.python,
-            reuse_existing=True,
-            venv=True,
-        )
-    except TypeError:
-        session._runner.venv = VirtualEnv(
-            os.environ["VIRTUAL_ENV"],
-            interpreter=session._runner.func.python,
-            reuse_existing=True,
-        )
+    session._runner.venv = VirtualEnv(
+        os.environ["VIRTUAL_ENV"],
+        interpreter=session._runner.func.python,
+        reuse_existing=True,
+        venv=True,
+    )
     _lint(
         session,
         rcfile,
         flags,
         paths,
-        tee_output=False,
         upgrade_setuptools_and_pip=False,
     )
 @nox.session(python="3")
 def lint(session):
     """
-    Run PyLint against Salt and it's test suite. Set PYLINT_REPORT to a path to capture output.
+    Run PyLint against Salt and it's test suite.
     """
     session.notify(f"lint-salt-{session.python}")
     session.notify(f"lint-tests-{session.python}")
 @nox.session(python="3", name="lint-salt")
 def lint_salt(session):
     """
-    Run PyLint against Salt. Set PYLINT_REPORT to a path to capture output.
+    Run PyLint against Salt.
     """
     flags = ["--disable=I"]
     if session.posargs:
         paths = session.posargs
     else:
-        paths = ["setup.py", "noxfile.py", "salt/"]
+        paths = ["setup.py", "noxfile.py", "salt/", "tools/"]
     _lint(session, ".pylintrc", flags, paths)
 @nox.session(python="3", name="lint-tests")
 def lint_tests(session):
     """
-    Run PyLint against Salt and it's test suite. Set PYLINT_REPORT to a path to capture output.
+    Run PyLint against Salt and it's test suite.
     """
     flags = ["--disable=I"]
     if session.posargs:
         paths = session.posargs
     else:
         paths = ["tests/"]
     _lint(session, ".pylintrc", flags, paths)
 @nox.session(python=False, name="lint-salt-pre-commit")
 def lint_salt_pre_commit(session):
     """
-    Run PyLint against Salt. Set PYLINT_REPORT to a path to capture output.
+    Run PyLint against Salt.
     """
     flags = ["--disable=I"]
     if session.posargs:
         paths = session.posargs
     else:
-        paths = ["setup.py", "noxfile.py", "salt/"]
+        paths = ["setup.py", "noxfile.py", "salt/", "tools/"]
     _lint_pre_commit(session, ".pylintrc", flags, paths)
 @nox.session(python=False, name="lint-tests-pre-commit")
 def lint_tests_pre_commit(session):
     """
-    Run PyLint against Salt and it's test suite. Set PYLINT_REPORT to a path to capture output.
+    Run PyLint against Salt and it's test suite.
     """
     flags = ["--disable=I"]
     if session.posargs:
         paths = session.posargs
     else:
         paths = ["tests/"]
     _lint_pre_commit(session, ".pylintrc", flags, paths)
 @nox.session(python="3")
 @nox.parametrize("clean", [False, True])
 @nox.parametrize("update", [False, True])
@@ -1400,75 +1361,45 @@
         )
     )
 @nox.session(name="docs-html", python="3")
 @nox.parametrize("clean", [False, True])
 @nox.parametrize("compress", [False, True])
 def docs_html(session, compress, clean):
     """
     Build Salt's HTML Documentation
     """
     if _upgrade_pip_setuptools_and_wheel(session):
-        linux_requirements_file = os.path.join(
-            "requirements", "static", "ci", _get_pydir(session), "linux.txt"
-        )
-        base_requirements_file = os.path.join("requirements", "base.txt")
-        zeromq_requirements_file = os.path.join("requirements", "zeromq.txt")
-        docs_requirements_file = os.path.join(
+        requirements_file = os.path.join(
             "requirements", "static", "ci", _get_pydir(session), "docs.txt"
         )
-        install_command = [
-            "--progress-bar=off",
-            "--constraint",
-            linux_requirements_file,
-            "-r",
-            base_requirements_file,
-            "-r",
-            zeromq_requirements_file,
-            "-r",
-            docs_requirements_file,
-        ]
+        install_command = ["--progress-bar=off", "-r", requirements_file]
         session.install(*install_command, silent=PIP_INSTALL_SILENT)
     os.chdir("doc/")
     if clean:
         session.run("make", "clean", external=True)
     session.run("make", "html", "SPHINXOPTS=-W", external=True)
     if compress:
         session.run("tar", "-cJvf", "html-archive.tar.xz", "_build/html", external=True)
     os.chdir("..")
 @nox.session(name="docs-man", python="3")
 @nox.parametrize("clean", [False, True])
 @nox.parametrize("update", [False, True])
 @nox.parametrize("compress", [False, True])
 def docs_man(session, compress, update, clean):
     """
     Build Salt's Manpages Documentation
     """
     if _upgrade_pip_setuptools_and_wheel(session):
-        linux_requirements_file = os.path.join(
-            "requirements", "static", "ci", _get_pydir(session), "linux.txt"
-        )
-        base_requirements_file = os.path.join("requirements", "base.txt")
-        zeromq_requirements_file = os.path.join("requirements", "zeromq.txt")
-        docs_requirements_file = os.path.join(
+        requirements_file = os.path.join(
             "requirements", "static", "ci", _get_pydir(session), "docs.txt"
         )
-        install_command = [
-            "--progress-bar=off",
-            "--constraint",
-            linux_requirements_file,
-            "-r",
-            base_requirements_file,
-            "-r",
-            zeromq_requirements_file,
-            "-r",
-            docs_requirements_file,
-        ]
+        install_command = ["--progress-bar=off", "-r", requirements_file]
         session.install(*install_command, silent=PIP_INSTALL_SILENT)
     os.chdir("doc/")
     if clean:
         session.run("make", "clean", external=True)
     session.run("make", "man", "SPHINXOPTS=-W", external=True)
     if update:
         session.run("rm", "-rf", "man/", external=True)
         session.run("cp", "-Rp", "_build/man", "man/", external=True)
     if compress:
         session.run("tar", "-cJvf", "man-archive.tar.xz", "_build/man", external=True)
@@ -1519,21 +1450,21 @@
         """
         Re-compress the passed path.
         """
         tempd = pathlib.Path(tempfile.mkdtemp()).resolve()
         d_src = tempd.joinpath("src")
         d_src.mkdir()
         d_tar = tempd.joinpath(targz.stem)
         d_targz = tempd.joinpath(targz.name)
         with tarfile.open(d_tar, "w|") as wfile:
             with tarfile.open(targz, "r:gz") as rfile:
-                rfile.extractall(d_src)
+                rfile.extractall(d_src)  # nosec
                 extracted_dir = next(pathlib.Path(d_src).iterdir())
                 for name in sorted(extracted_dir.rglob("*")):
                     wfile.add(
                         str(name),
                         filter=self.tar_reset,
                         recursive=False,
                         arcname=str(name.relative_to(d_src)),
                     )
         with open(d_tar, "rb") as rfh:
             with gzip.GzipFile(
@@ -1618,136 +1549,134 @@
             )
         )
     common_pytest_args = [
         "--color=yes",
         "--sys-stats",
         "--run-destructive",
         f"--output-columns={os.environ.get('OUTPUT_COLUMNS') or 120}",
         "--pkg-system-service",
     ]
     chunks = {
-        "install": [
-            "tests/pytests/pkg/",
-        ],
+        "install": [],
         "upgrade": [
             "--upgrade",
             "--no-uninstall",
-            "tests/pytests/pkg/upgrade/",
-        ],
-        "upgrade-classic": [
-            "--upgrade",
-            "--no-uninstall",
-            "tests/pytests/pkg/upgrade/",
         ],
         "downgrade": [
             "--downgrade",
             "--no-uninstall",
-            "tests/pytests/pkg/downgrade/",
-        ],
-        "downgrade-classic": [
-            "--downgrade",
-            "--no-uninstall",
-            "tests/pytests/pkg/downgrade/",
         ],
         "download-pkgs": [
             "--download-pkgs",
-            "tests/pytests/pkg/download/",
         ],
     }
     if not session.posargs or session.posargs[0] not in chunks:
         chunk = "install"
         session.log("Choosing default 'install' test type")
     else:
         chunk = session.posargs.pop(0)
     cmd_args = chunks[chunk]
     for arg in session.posargs:
         if arg.startswith("tests/pytests/pkg/"):
             cmd_args.pop()
             break
     if IS_LINUX:
         session_run_always(session, "python3", "-m", "relenv", "toolchain", "fetch")
     if _upgrade_pip_setuptools_and_wheel(session):
         _install_requirements(session, "pyzmq")
     env = {
         "ONEDIR_TESTRUN": "1",
         "PKG_TEST_TYPE": chunk,
     }
-    if chunk in ("upgrade-classic", "downgrade-classic"):
-        cmd_args.append("--classic")
     pytest_args = (
         common_pytest_args[:]
         + cmd_args[:]
         + [
             f"--junitxml=artifacts/xml-unittests-output/test-results-{chunk}.xml",
             f"--log-file=artifacts/logs/runtests-{chunk}.log",
         ]
         + session.posargs
     )
+    append_tests_path = True
+    test_paths = (
+        "tests/pytests/pkg/",
+        str(REPO_ROOT / "tests" / "pytests" / "pkg"),
+    )
+    for arg in session.posargs:
+        if arg.startswith(test_paths):
+            append_tests_path = False
+            break
+    if append_tests_path:
+        pytest_args.append("tests/pytests/pkg/")
     try:
         _pytest(session, coverage=False, cmd_args=pytest_args, env=env)
     except CommandFailed:
         if os.environ.get("RERUN_FAILURES", "0") == "0":
             return
         global PRINT_TEST_SELECTION
         global PRINT_SYSTEM_INFO
         PRINT_TEST_SELECTION = False
         PRINT_SYSTEM_INFO = False
         pytest_args = (
             common_pytest_args[:]
             + cmd_args[:]
             + [
                 f"--junitxml=artifacts/xml-unittests-output/test-results-{chunk}-rerun.xml",
                 f"--log-file=artifacts/logs/runtests-{chunk}-rerun.log",
                 "--lf",
             ]
             + session.posargs
         )
+        if append_tests_path:
+            pytest_args.append("tests/pytests/pkg/")
         _pytest(
             session,
             coverage=False,
             cmd_args=pytest_args,
             env=env,
             on_rerun=True,
         )
     if chunk not in ("install", "download-pkgs"):
         cmd_args = chunks["install"]
         pytest_args = (
             common_pytest_args[:]
             + cmd_args[:]
             + [
                 "--no-install",
-                f"--junitxml=artifacts/xml-unittests-output/test-results-install.xml",
-                f"--log-file=artifacts/logs/runtests-install.log",
+                "--junitxml=artifacts/xml-unittests-output/test-results-install.xml",
+                "--log-file=artifacts/logs/runtests-install.log",
             ]
             + session.posargs
         )
         if "downgrade" in chunk:
             pytest_args.append("--use-prev-version")
-        if chunk in ("upgrade-classic", "downgrade-classic"):
-            pytest_args.append("--classic")
+        if append_tests_path:
+            pytest_args.append("tests/pytests/pkg/")
         try:
             _pytest(session, coverage=False, cmd_args=pytest_args, env=env)
         except CommandFailed:
+            if os.environ.get("RERUN_FAILURES", "0") == "0":
+                return
             cmd_args = chunks["install"]
             pytest_args = (
                 common_pytest_args[:]
                 + cmd_args[:]
                 + [
                     "--no-install",
-                    f"--junitxml=artifacts/xml-unittests-output/test-results-install-rerun.xml",
-                    f"--log-file=artifacts/logs/runtests-install-rerun.log",
+                    "--junitxml=artifacts/xml-unittests-output/test-results-install-rerun.xml",
+                    "--log-file=artifacts/logs/runtests-install-rerun.log",
                     "--lf",
                 ]
                 + session.posargs
             )
             if "downgrade" in chunk:
                 pytest_args.append("--use-prev-version")
-            if chunk in ("upgrade-classic", "downgrade-classic"):
-                pytest_args.append("--classic")
+            if append_tests_path:
+                pytest_args.append("tests/pytests/pkg/")
             _pytest(
                 session,
                 coverage=False,
                 cmd_args=pytest_args,
                 env=env,
                 on_rerun=True,
             )
     sys.exit(0)

--- a//dev/null
+++ b/run.py
@@ -0,0 +1,96 @@
+import contextlib
+import multiprocessing
+import os
+import pathlib
+import sys
+import _pyio
+import tiamatpip.cli
+import tiamatpip.configure
+import tiamatpip.utils
+import salt.scripts
+import salt.utils.platform
+AVAIL = (
+    "minion",
+    "master",
+    "call",
+    "api",
+    "cloud",
+    "cp",
+    "extend",
+    "key",
+    "proxy",
+    "pip",
+    "run",
+    "shell",
+    "spm",
+    "ssh",
+    "support",
+    "syndic",
+    "python",
+)
+if "TIAMAT_PIP_PYPATH" in os.environ:
+    PIP_PATH = pathlib.Path(os.environ["TIAMAT_PIP_PYPATH"]).resolve()
+elif not sys.platform.startswith("win"):
+    PIP_PATH = pathlib.Path(f"{os.sep}opt", "saltstack", "salt", "pypath")
+else:
+    PIP_PATH = pathlib.Path(os.getenv("LocalAppData"), "salt", "pypath")
+with contextlib.suppress(PermissionError):
+    PIP_PATH.mkdir(mode=0o755, parents=True, exist_ok=True)
+tiamatpip.configure.set_user_base_path(PIP_PATH)
+def py_shell():
+    if not sys.platform.startswith("win"):
+        import readline
+    import code
+    variables = globals().copy()
+    variables.update(locals())
+    shell = code.InteractiveConsole(variables)
+    shell.interact()
+def python_runtime():
+    import traceback
+    script = pathlib.Path(sys.argv[2]).expanduser().resolve()
+    sys.path.insert(0, str(script.parent))
+    sys.argv[:] = sys.argv[2:]
+    exec_locals = {"__name__": "__main__", "__file__": str(script), "__doc__": None}
+    with open(script, encoding="utf-8") as rfh:
+        try:
+            exec(rfh.read(), exec_locals)
+        except Exception:
+            traceback.print_exc()
+            sys.exit(1)
+def redirect(argv):
+    """
+    Change the args and redirect to another salt script
+    """
+    if len(argv) < 2:
+        msg = "Must pass in a salt command, available commands are:"
+        for cmd in AVAIL:
+            msg += f"\n{cmd}"
+        print(msg, file=sys.stderr, flush=True)
+        sys.exit(1)
+    cmd = sys.argv[1]
+    if cmd == "shell":
+        py_shell()
+        return
+    if cmd == "python":
+        if len(argv) < 3:
+            msg = "Must pass script location to this command"
+            print(msg, file=sys.stderr, flush=True)
+            sys.exit(1)
+        python_runtime()
+        return
+    if tiamatpip.cli.should_redirect_argv(argv):
+        tiamatpip.cli.process_pip_argv(argv)
+        return
+    if cmd not in AVAIL:
+        args = ["salt"]
+        s_fun = salt.scripts.salt_main
+    else:
+        args = [f"salt-{cmd}"]
+        sys.argv.pop(1)
+        s_fun = getattr(salt.scripts, f"salt_{cmd}")
+    args.extend(argv[1:])
+    with tiamatpip.utils.patched_sys_argv(args):
+        s_fun()
+if __name__ == "__main__":
+    multiprocessing.freeze_support()
+    redirect(sys.argv)

--- a/salt/__init__.py
+++ b/salt/__init__.py
@@ -1,26 +1,36 @@
 """
 Salt package
 """
-import asyncio
 import importlib
-import locale
 import os
 import sys
 import warnings
-if sys.platform.startswith("win"):
-    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
-if sys.version_info < (3,):  # pragma: no cover
-    sys.stderr.write(
-        "\n\nAfter the Sodium release, 3001, Salt no longer supports Python 2. Exiting.\n\n"
-    )
-    sys.stderr.flush()
+USE_VENDORED_TORNADO = True
+class TornadoImporter:
+    def find_module(self, module_name, package_path=None):
+        if USE_VENDORED_TORNADO:
+            if module_name.startswith("tornado"):
+                return self
+        else:  # pragma: no cover
+            if module_name.startswith("salt.ext.tornado"):
+                return self
+        return None
+    def create_module(self, spec):
+        if USE_VENDORED_TORNADO:
+            mod = importlib.import_module(f"salt.ext.{spec.name}")
+        else:  # pragma: no cover
+            mod = importlib.import_module(spec.name[9:])
+        sys.modules[spec.name] = mod
+        return mod
+    def exec_module(self, module):
+        return None
 class NaclImporter:
     """
     Import hook to force PyNaCl to perform dlopen on libsodium with the
     RTLD_DEEPBIND flag. This is to work around an issue where pyzmq does a dlopen
     with RTLD_GLOBAL which then causes calls to libsodium to resolve to
     tweetnacl when it's been bundled with pyzmq.
     See:  https://github.com/zeromq/pyzmq/issues/1878
     """
     loading = False
     def find_module(self, module_name, package_path=None):
@@ -40,21 +50,21 @@
         try:
             mod = importlib.import_module(spec.name)
         finally:
             if dlopen:
                 sys.setdlopenflags(dlflags)
         NaclImporter.loading = False
         sys.modules[spec.name] = mod
         return mod
     def exec_module(self, module):
         return None
-sys.meta_path = [NaclImporter()] + sys.meta_path
+sys.meta_path = [TornadoImporter(), NaclImporter()] + sys.meta_path
 warnings.filterwarnings(
     "once",  # Show once
     "",  # No deprecation message match
     DeprecationWarning,  # This filter is for DeprecationWarnings
     r"^(salt|salt\.(.*))$",  # Match module(s) 'salt' and 'salt.<whatever>'
 )
 warnings.filterwarnings(
     "ignore",
     "^Module backports was already imported from (.*), but (.*) is being added to sys.path$",
     UserWarning,

--- a/salt/_logging/handlers.py
+++ b/salt/_logging/handlers.py
@@ -63,21 +63,21 @@
 class SysLogHandler(ExcInfoOnLogLevelFormatMixin, logging.handlers.SysLogHandler):
     """
     Syslog handler which properly handles exc_info on a per handler basis
     """
     def handleError(self, record):
         """
         Override the default error handling mechanism for py3
         Deal with syslog os errors when the log file does not exist
         """
         handled = False
-        if sys.stderr:
+        if sys.stderr and sys.version_info >= (3, 5, 4):
             exc_type, exc, exc_traceback = sys.exc_info()
             try:
                 if exc_type.__name__ in "FileNotFoundError":
                     sys.stderr.write(
                         "[WARNING ] The log_file does not exist. Logging not "
                         "setup correctly or syslog service not started.\n"
                     )
                     handled = True
             finally:
                 del exc_type, exc, exc_traceback

--- a/salt/_logging/impl.py
+++ b/salt/_logging/impl.py
@@ -12,28 +12,28 @@
 import socket
 import sys
 import traceback
 import types
 import urllib.parse
 PROFILE = logging.PROFILE = 15
 TRACE = logging.TRACE = 5
 GARBAGE = logging.GARBAGE = 1
 QUIET = logging.QUIET = 1000
 import salt.defaults.exitcodes  # isort:skip  pylint: disable=unused-import
-import salt.utils.ctx
 from salt._logging.handlers import DeferredStreamHandler  # isort:skip
 from salt._logging.handlers import RotatingFileHandler  # isort:skip
 from salt._logging.handlers import StreamHandler  # isort:skip
 from salt._logging.handlers import SysLogHandler  # isort:skip
 from salt._logging.handlers import WatchedFileHandler  # isort:skip
 from salt._logging.mixins import LoggingMixinMeta  # isort:skip
 from salt.exceptions import LoggingRuntimeError  # isort:skip
+from salt.utils.ctx import RequestContext  # isort:skip
 from salt.utils.immutabletypes import freeze, ImmutableDict  # isort:skip
 from salt.utils.textformat import TextFormat  # isort:skip
 LOG_LEVELS = {
     "all": logging.NOTSET,
     "debug": logging.DEBUG,
     "error": logging.ERROR,
     "critical": logging.CRITICAL,
     "garbage": GARBAGE,
     "info": logging.INFO,
     "profile": PROFILE,
@@ -128,21 +128,21 @@
 class SaltLoggingClass(LOGGING_LOGGER_CLASS, metaclass=LoggingMixinMeta):
     def __new__(cls, *args):
         """
         We override `__new__` in our logging logger class in order to provide
         some additional features like expand the module name padding if length
         is being used, and also some Unicode fixes.
         This code overhead will only be executed when the class is
         instantiated, i.e.:
             logging.getLogger(__name__)
         """
-        instance = super().__new__(cls)
+        instance = super().__new__(cls)  # pylint: disable=no-value-for-parameter
         try:
             max_logger_length = len(
                 max(list(logging.Logger.manager.loggerDict), key=len)
             )
             if max_logger_length > 80:
                 max_logger_length = 80
             for handler in logging.root.handlers:
                 if handler is get_temp_handler():
                     continue
                 formatter = handler.formatter
@@ -179,28 +179,22 @@
         msg,
         args,
         exc_info=None,
         extra=None,  # pylint: disable=arguments-differ
         stack_info=False,
         stacklevel=1,
         exc_info_on_loglevel=None,
     ):
         if extra is None:
             extra = {}
-        current_jid = (
-            salt.utils.ctx.get_request_context().get("data", {}).get("jid", None)
-        )
-        log_fmt_jid = (
-            salt.utils.ctx.get_request_context()
-            .get("opts", {})
-            .get("log_fmt_jid", None)
-        )
+        current_jid = RequestContext.current.get("data", {}).get("jid", None)
+        log_fmt_jid = RequestContext.current.get("opts", {}).get("log_fmt_jid", None)
         if current_jid is not None:
             extra["jid"] = current_jid
         if log_fmt_jid is not None:
             extra["log_fmt_jid"] = log_fmt_jid
         if exc_info and exc_info_on_loglevel:
             raise LoggingRuntimeError(
                 "Only one of 'exc_info' and 'exc_info_on_loglevel' is permitted"
             )
         if exc_info_on_loglevel is not None:
             if isinstance(exc_info_on_loglevel, str):
@@ -211,40 +205,40 @@
                 raise RuntimeError(
                     "The value of 'exc_info_on_loglevel' needs to be a "
                     "logging level or a logging level name, not '{}'".format(
                         exc_info_on_loglevel
                     )
                 )
         if extra is None:
             extra = {"exc_info_on_loglevel": exc_info_on_loglevel}
         else:
             extra["exc_info_on_loglevel"] = exc_info_on_loglevel
-        try:
+        if sys.version_info < (3, 8):
+            LOGGING_LOGGER_CLASS._log(
+                self,
+                level,
+                msg,
+                args,
+                exc_info=exc_info,
+                extra=extra,
+                stack_info=stack_info,
+            )
+        else:
             LOGGING_LOGGER_CLASS._log(
                 self,
                 level,
                 msg,
                 args,
                 exc_info=exc_info,
                 extra=extra,
                 stack_info=stack_info,
                 stacklevel=stacklevel,
-            )
-        except TypeError:
-            LOGGING_LOGGER_CLASS._log(
-                self,
-                level,
-                msg,
-                args,
-                exc_info=exc_info,
-                extra=extra,
-                stack_info=stack_info,
             )
     def makeRecord(
         self,
         name,
         level,
         fn,
         lno,
         msg,
         args,
         exc_info,
@@ -367,26 +361,21 @@
         log_level = logging.WARNING
     log_level = get_logging_level_from_string(log_level)
     handler = None
     for handler in logging.root.handlers:
         if not hasattr(handler, "stream"):
             continue
         if handler.stream is sys.stderr:
             break
     else:
         handler = DeferredStreamHandler(sys.stderr)
-        def tryflush():
-            try:
-                handler.flush()
-            except ValueError:
-                pass
-        atexit.register(tryflush)
+        atexit.register(handler.flush)
     handler.setLevel(log_level)
     formatter = logging.Formatter(DFLT_LOG_FMT_CONSOLE, datefmt=DFLT_LOG_DATEFMT)
     handler.setFormatter(formatter)
     logging.root.addHandler(handler)
     setup_temp_handler.__handler__ = handler
 def shutdown_temp_handler():
     """
     Shutdown the temporary deferred stream handler
     """
     temp_handler = get_temp_handler()

--- a/salt/auth/__init__.py
+++ b/salt/auth/__init__.py
@@ -216,21 +216,21 @@
                 return {}
             rm_tok = False
         if tdata.get("expire", 0) < time.time():
             rm_tok = True
         if rm_tok:
             self.rm_token(tok)
             return {}
         return tdata
     def list_tokens(self):
         """
-        List all tokens in eauth_tokens storage.
+        List all tokens in eauth_tokn storage.
         """
         return self.tokens["{}.list_tokens".format(self.opts["eauth_tokens"])](
             self.opts
         )
     def rm_token(self, tok):
         """
         Remove the given token from token storage.
         """
         self.tokens["{}.rm_token".format(self.opts["eauth_tokens"])](self.opts, tok)
     def authenticate_token(self, load):
@@ -499,21 +499,21 @@
         load["cmd"] = "get_token"
         tdata = self._send_token_request(load)
         return tdata
 class AuthUser:
     """
     Represents a user requesting authentication to the salt master
     """
     def __init__(self, user):
         """
         Instantiate an AuthUser object.
-        Takes a user to represent, as a string.
+        Takes a user to reprsent, as a string.
         """
         self.user = user
     def is_sudo(self):
         """
         Determines if the user is running with sudo
         Returns True if the user is running with sudo and False if the
         user is not running with sudo
         """
         return self.user.startswith("sudo_")
     def is_running_user(self):

--- a/salt/beacons/__init__.py
+++ b/salt/beacons/__init__.py
@@ -289,21 +289,21 @@
                 "because it is configured in pillar.".format(name)
             )
             complete = False
         else:
             if name in self.opts["beacons"]:
                 comment = f"Updating settings for beacon item: {name}"
             else:
                 comment = f"Added new beacon item: {name}"
             complete = True
             self.opts["beacons"].update(data)
-        with salt.utils.event.get_event("minion", opts=self.opts, listen=False) as evt:
+        with salt.utils.event.get_event("minion", opts=self.opts) as evt:
             evt.fire_event(
                 {
                     "complete": complete,
                     "comment": comment,
                     "beacons": self.opts["beacons"],
                 },
                 tag="/salt/minion/minion_beacon_add_complete",
             )
         return True
     def modify_beacon(self, name, beacon_data):

--- a/salt/beacons/twilio_txt_msg.py
+++ b/salt/beacons/twilio_txt_msg.py
@@ -2,21 +2,21 @@
 Beacon to emit Twilio text messages
 """
 import logging
 import salt.utils.beacons
 try:
     import twilio
     twilio_version = tuple(int(x) for x in twilio.__version_info__)
     if twilio_version > (5,):
         from twilio.rest import Client as TwilioRestClient
     else:
-        from twilio.rest import TwilioRestClient  # pylint: disable=no-name-in-module
+        from twilio.rest import TwilioRestClient
     HAS_TWILIO = True
 except ImportError:
     HAS_TWILIO = False
 log = logging.getLogger(__name__)
 __virtualname__ = "twilio_txt_msg"
 def __virtual__():
     if HAS_TWILIO:
         return __virtualname__
     else:
         err_msg = "twilio library is missing."

--- a/salt/cache/mysql_cache.py
+++ b/salt/cache/mysql_cache.py
@@ -15,46 +15,42 @@
     pip install pymysql
 Optionally, depending on the MySQL agent configuration, the following values
 could be set in the master config. These are the defaults:
 .. code-block:: yaml
     mysql.host: 127.0.0.1
     mysql.port: 2379
     mysql.user: None
     mysql.password: None
     mysql.database: salt_cache
     mysql.table_name: cache
-    mysql.fresh_connection: false
 Related docs can be found in the `python-mysql documentation`_.
 To use the mysql as a minion data cache backend, set the master ``cache`` config
 value to ``mysql``:
 .. code-block:: yaml
     cache: mysql
 .. _`MySQL documentation`: https://github.com/coreos/mysql
 .. _`python-mysql documentation`: http://python-mysql.readthedocs.io/en/latest/
 """
 import copy
 import logging
 import time
 import salt.payload
 import salt.utils.stringutils
 from salt.exceptions import SaltCacheError
 try:
     import MySQLdb
     import MySQLdb.converters
     import MySQLdb.cursors
     from MySQLdb.connections import OperationalError
-    class InterfaceError(Exception):
-        pass
 except ImportError:
     try:
         import pymysql
-        from pymysql.err import InterfaceError
         pymysql.install_as_MySQLdb()
         import MySQLdb
         import MySQLdb.converters
         import MySQLdb.cursors
         from MySQLdb.err import OperationalError
     except ImportError:
         MySQLdb = None
 _DEFAULT_DATABASE_NAME = "salt_cache"
 _DEFAULT_CACHE_TABLE_NAME = "cache"
 _RECONNECT_INTERVAL_SEC = 0.050
@@ -70,37 +66,34 @@
     """
     Force a reconnection to the MySQL database, by removing the client from
     Salt's __context__.
     """
     __context__.pop("mysql_client", None)
 def run_query(conn, query, args=None, retries=3):
     """
     Get a cursor and run a query. Reconnect up to ``retries`` times if
     needed.
     Returns: cursor, affected rows counter
-    Raises: SaltCacheError, AttributeError, OperationalError, InterfaceError
-    """
-    if __context__.get("mysql_fresh_connection"):
-        conn = MySQLdb.connect(**__context__["mysql_kwargs"])
-        __context__["mysql_client"] = conn
+    Raises: SaltCacheError, AttributeError, OperationalError
+    """
     if conn is None:
         conn = __context__.get("mysql_client")
     try:
         cur = conn.cursor()
         if not args:
             log.debug("Doing query: %s", query)
             out = cur.execute(query)
         else:
             log.debug("Doing query: %s args: %s ", query, repr(args))
             out = cur.execute(query, args)
         return cur, out
-    except (AttributeError, OperationalError, InterfaceError) as e:
+    except (AttributeError, OperationalError) as e:
         if retries == 0:
             raise
         time.sleep(_RECONNECT_INTERVAL_SEC)
         if conn is None:
             log.debug("mysql_cache: creating db connection")
         else:
             log.info("mysql_cache: recreating db connection due to: %r", e)
         __context__["mysql_client"] = MySQLdb.connect(**__context__["mysql_kwargs"])
         return run_query(
             conn=__context__.get("mysql_client"),
@@ -182,21 +175,20 @@
         "host": opts.pop("mysql.host", "127.0.0.1"),
         "user": opts.pop("mysql.user", None),
         "passwd": opts.pop("mysql.password", None),
         "db": opts.pop("mysql.database", _DEFAULT_DATABASE_NAME),
         "port": opts.pop("mysql.port", 3306),
         "unix_socket": opts.pop("mysql.unix_socket", None),
         "connect_timeout": opts.pop("mysql.connect_timeout", None),
     }
     mysql_kwargs["autocommit"] = True
     __context__["mysql_table_name"] = opts.pop("mysql.table_name", "salt")
-    __context__["mysql_fresh_connection"] = opts.pop("mysql.fresh_connection", False)
     for k in opts:
         if k.startswith("mysql."):
             _key = k.split(".")[1]
             mysql_kwargs[_key] = opts.get(k)
     for k, v in copy.deepcopy(mysql_kwargs).items():
         if v is None:
             mysql_kwargs.pop(k)
     kwargs_copy = mysql_kwargs.copy()
     kwargs_copy["passwd"] = "<hidden>"
     log.info("mysql_cache: Setting up client with params: %r", kwargs_copy)
@@ -234,21 +226,21 @@
     """
     Remove the key from the cache bank with all the key content.
     """
     _init_client()
     query = "DELETE FROM {} WHERE bank=%s".format(__context__["mysql_table_name"])
     if key is None:
         data = (bank,)
     else:
         data = (bank, key)
         query += " AND etcd_key=%s"
-    cur, _ = run_query(__context__.get("mysql_client"), query, args=data)
+    cur, _ = run_query(__context__["mysql_client"], query, args=data)
     cur.close()
 def ls(bank):
     """
     Return an iterable object containing all entries stored in the specified
     bank.
     """
     _init_client()
     query = "SELECT etcd_key FROM {} WHERE bank=%s".format(
         __context__["mysql_table_name"]
     )
@@ -279,14 +271,14 @@
     """
     Return the integer Unix epoch update timestamp of the specified bank and
     key.
     """
     _init_client()
     query = (
         "SELECT UNIX_TIMESTAMP(last_update) FROM {} WHERE bank=%s "
         "AND etcd_key=%s".format(__context__["mysql_table_name"])
     )
     data = (bank, key)
-    cur, _ = run_query(__context__.get("mysql_client"), query=query, args=data)
+    cur, _ = run_query(__context__["mysql_client"], query=query, args=data)
     r = cur.fetchone()
     cur.close()
     return int(r[0]) if r else r

--- a/salt/channel/client.py
+++ b/salt/channel/client.py
@@ -1,43 +1,30 @@
 """
 Encapsulate the different transports available to Salt.
 This includes client side transport, for the ReqServer and the Publisher
 """
 import logging
 import os
 import time
 import uuid
-import tornado.gen
-import tornado.ioloop
 import salt.crypt
 import salt.exceptions
+import salt.ext.tornado.gen
+import salt.ext.tornado.ioloop
 import salt.payload
 import salt.transport.frame
 import salt.utils.event
 import salt.utils.files
 import salt.utils.minions
 import salt.utils.stringutils
 import salt.utils.verify
-import salt.utils.versions
 from salt.utils.asynchronous import SyncWrapper
-try:
-    from M2Crypto import RSA
-    HAS_M2 = True
-except ImportError:
-    HAS_M2 = False
-    try:
-        from Cryptodome.Cipher import PKCS1_OAEP
-    except ImportError:
-        try:
-            from Crypto.Cipher import PKCS1_OAEP  # nosec
-        except ImportError:
-            pass
 log = logging.getLogger(__name__)
 REQUEST_CHANNEL_TIMEOUT = 60
 REQUEST_CHANNEL_TRIES = 3
 class ReqChannel:
     """
     Factory class to create a sychronous communication channels to the master's
     ReqServer. ReqChannels use transports to connect to the ReqServer.
     """
     @staticmethod
     def factory(opts, **kwargs):
@@ -90,21 +77,21 @@
     def factory(cls, opts, **kwargs):
         ttype = "zeromq"
         if "transport" in opts:
             ttype = opts["transport"]
         elif "transport" in opts.get("pillar", {}).get("master", {}):
             ttype = opts["pillar"]["master"]["transport"]
         if "master_uri" not in opts and "master_uri" in kwargs:
             opts["master_uri"] = kwargs["master_uri"]
         io_loop = kwargs.get("io_loop")
         if io_loop is None:
-            io_loop = tornado.ioloop.IOLoop.current()
+            io_loop = salt.ext.tornado.ioloop.IOLoop.current()
         timeout = opts.get("request_channel_timeout", REQUEST_CHANNEL_TIMEOUT)
         tries = opts.get("request_channel_tries", REQUEST_CHANNEL_TRIES)
         crypt = kwargs.get("crypt", "aes")
         if crypt != "clear":
             auth = salt.crypt.AsyncAuth(opts, io_loop=io_loop)
         else:
             auth = None
         transport = salt.transport.request_client(opts, io_loop=io_loop)
         return cls(opts, transport, auth, tries=tries, timeout=timeout)
     def __init__(
@@ -127,44 +114,48 @@
         self.tries = tries
     @property
     def crypt(self):
         if self.auth:
             return "aes"
         return "clear"
     @property
     def ttype(self):
         return self.transport.ttype
     def _package_load(self, load):
-        return {
+        ret = {
             "enc": self.crypt,
             "load": load,
             "version": 2,
         }
-    @tornado.gen.coroutine
+        if self.crypt == "aes":
+            ret["enc_algo"] = self.opts["encryption_algorithm"]
+            ret["sig_algo"] = self.opts["signing_algorithm"]
+        return ret
+    @salt.ext.tornado.gen.coroutine
     def _send_with_retry(self, load, tries, timeout):
         _try = 1
         while True:
             try:
                 ret = yield self.transport.send(
                     load,
                     timeout=timeout,
                 )
                 break
             except Exception as exc:  # pylint: disable=broad-except
                 log.trace("Failed to send msg %r", exc)
                 if _try >= tries:
                     raise
                 else:
                     _try += 1
                     continue
-        raise tornado.gen.Return(ret)
-    @tornado.gen.coroutine
+        raise salt.ext.tornado.gen.Return(ret)
+    @salt.ext.tornado.gen.coroutine
     def crypted_transfer_decode_dictentry(
         self,
         load,
         dictkey=None,
         timeout=None,
         tries=None,
     ):
         if timeout is None:
             timeout = self.timeout
         if tries is None:
@@ -179,87 +170,86 @@
             timeout,
         )
         key = self.auth.get_keys()
         if "key" not in ret:
             yield self.auth.authenticate()
             ret = yield self._send_with_retry(
                 self._package_load(self.auth.crypticle.dumps(load)),
                 tries,
                 timeout,
             )
-        if HAS_M2:
-            aes = key.private_decrypt(ret["key"], RSA.pkcs1_oaep_padding)
-        else:
-            cipher = PKCS1_OAEP.new(key)
-            aes = cipher.decrypt(ret["key"])
+        aes = key.decrypt(ret["key"], self.opts["encryption_algorithm"])
         pcrypt = salt.crypt.Crypticle(self.opts, aes)
         signed_msg = pcrypt.loads(ret[dictkey])
         if not self.verify_signature(signed_msg["data"], signed_msg["sig"]):
             raise salt.crypt.AuthenticationError(
                 "Pillar payload signature failed to validate."
             )
         data = salt.payload.loads(signed_msg["data"])
         if data["key"] != ret["key"]:
             raise salt.crypt.AuthenticationError("Key verification failed.")
         if data["nonce"] != nonce:
             raise salt.crypt.AuthenticationError("Pillar nonce verification failed.")
-        raise tornado.gen.Return(data["pillar"])
+        raise salt.ext.tornado.gen.Return(data["pillar"])
     def verify_signature(self, data, sig):
-        return salt.crypt.verify_signature(self.master_pubkey_path, data, sig)
-    @tornado.gen.coroutine
+        return salt.crypt.PublicKey(self.master_pubkey_path).verify(
+            data, sig, self.opts["signing_algorithm"]
+        )
+    @salt.ext.tornado.gen.coroutine
     def _crypted_transfer(self, load, timeout, raw=False):
         """
         Send a load across the wire, with encryption
         In case of authentication errors, try to renegotiate authentication
         and retry the method.
         Indeed, we can fail too early in case of a master restart during a
         minion state execution call
         :param dict load: A load to send across the wire
         :param int timeout: The number of seconds on a response before failing
         """
         nonce = uuid.uuid4().hex
         if load and isinstance(load, dict):
             load["nonce"] = nonce
-        @tornado.gen.coroutine
+        @salt.ext.tornado.gen.coroutine
         def _do_transfer():
             data = yield self.transport.send(
                 self._package_load(self.auth.crypticle.dumps(load)),
                 timeout=timeout,
             )
             if data:
                 data = self.auth.crypticle.loads(data, raw, nonce=nonce)
             if not raw or self.ttype == "tcp":  # XXX Why is this needed for tcp
                 data = salt.transport.frame.decode_embedded_strs(data)
-            raise tornado.gen.Return(data)
+            raise salt.ext.tornado.gen.Return(data)
         if not self.auth.authenticated:
             yield self.auth.authenticate()
         try:
             ret = yield _do_transfer()
         except salt.crypt.AuthenticationError:
             yield self.auth.authenticate()
             ret = yield _do_transfer()
-        raise tornado.gen.Return(ret)
-    @tornado.gen.coroutine
+        raise salt.ext.tornado.gen.Return(ret)
+    @salt.ext.tornado.gen.coroutine
     def _uncrypted_transfer(self, load, timeout):
         """
         Send a load across the wire in cleartext
         :param dict load: A load to send across the wire
         :param int timeout: The number of seconds on a response before failing
         """
         ret = yield self.transport.send(
             self._package_load(load),
             timeout=timeout,
         )
-        raise tornado.gen.Return(ret)
-    async def connect(self):
-        await self.transport.connect()
-    @tornado.gen.coroutine
+        raise salt.ext.tornado.gen.Return(ret)
+    @salt.ext.tornado.gen.coroutine
+    def connect(self):
+        yield self.transport.connect()
+    @salt.ext.tornado.gen.coroutine
     def send(self, load, tries=None, timeout=None, raw=False):
         """
         Send a request, return a future which will complete when we send the message
         :param dict load: A load to send across the wire
         :param int tries: The number of times to make before failure
         :param int timeout: The number of seconds on a response before failing
         """
         if timeout is None:
             timeout = self.timeout
         if tries is None:
@@ -274,39 +264,34 @@
                     log.trace("ReqChannel send crypt load=%r", load)
                     ret = yield self._crypted_transfer(load, timeout=timeout, raw=raw)
                 break
             except Exception as exc:  # pylint: disable=broad-except
                 log.trace("Failed to send msg %r", exc)
                 if _try >= tries:
                     raise
                 else:
                     _try += 1
                     continue
-        raise tornado.gen.Return(ret)
+        raise salt.ext.tornado.gen.Return(ret)
     def close(self):
         """
         Since the message_client creates sockets and assigns them to the IOLoop we have to
         specifically destroy them, since we aren't the only ones with references to the FDs
         """
         if self._closing:
             return
         log.debug("Closing %s instance", self.__class__.__name__)
         self._closing = True
         self.transport.close()
     def __enter__(self):
         return self
     def __exit__(self, *args):
-        self.close()
-    async def __aenter__(self):
-        await self.transport.connect()
-        return self
-    async def __aexit__(self, *_):
         self.close()
 class AsyncPubChannel:
     """
     Factory class to create subscription channels to the master's Publisher
     """
     async_methods = [
         "connect",
         "_decode_messages",
     ]
     close_methods = [
@@ -319,40 +304,38 @@
             ttype = opts["transport"]
         elif "transport" in opts.get("pillar", {}).get("master", {}):
             ttype = opts["pillar"]["master"]["transport"]
         if "master_uri" not in opts and "master_uri" in kwargs:
             opts["master_uri"] = kwargs["master_uri"]
         if ttype == "detect":
             opts["detect_mode"] = True
             log.info("Transport is set to detect; using %s", ttype)
         io_loop = kwargs.get("io_loop")
         if io_loop is None:
-            io_loop = tornado.ioloop.IOLoop.current()
+            io_loop = salt.ext.tornado.ioloop.IOLoop.current()
         auth = salt.crypt.AsyncAuth(opts, io_loop=io_loop)
-        host = opts.get("master_ip", "127.0.0.1")
-        port = int(opts.get("publish_port", 4506))
-        transport = salt.transport.publish_client(opts, io_loop, host=host, port=port)
+        transport = salt.transport.publish_client(opts, io_loop)
         return cls(opts, transport, auth, io_loop)
     def __init__(self, opts, transport, auth, io_loop=None):
         self.opts = opts
         self.io_loop = io_loop
         self.auth = auth
         self.token = self.auth.gen_token(b"salt")
         self.transport = transport
         self._closing = False
         self._reconnected = False
         self.event = salt.utils.event.get_event("minion", opts=self.opts, listen=False)
         self.master_pubkey_path = os.path.join(self.opts["pki_dir"], self.auth.mpub)
     @property
     def crypt(self):
         return "aes" if self.auth else "clear"
-    @tornado.gen.coroutine
+    @salt.ext.tornado.gen.coroutine
     def connect(self):
         """
         Return a future which completes when connected to the remote publisher
         """
         try:
             if not self.auth.authenticated:
                 yield self.auth.authenticate()
             if int(self.opts.get("publish_port", 4506)) != 4506:
                 publish_port = self.opts.get("publish_port")
             else:
@@ -374,68 +357,69 @@
         self.transport.close()
         if self.event is not None:
             self.event.destroy()
             self.event = None
     def on_recv(self, callback=None):
         """
         When jobs are received pass them (decoded) to callback
         """
         if callback is None:
             return self.transport.on_recv(None)
-        async def wrap_callback(messages):
-            payload = self.transport._decode_messages(messages)
-            decoded = await self._decode_payload(payload)
-            log.debug("PubChannel received: %r %r", decoded, callback)
-            if decoded is not None and callback is not None:
-                await callback(decoded)
+        @salt.ext.tornado.gen.coroutine
+        def wrap_callback(messages):
+            payload = yield self.transport._decode_messages(messages)
+            decoded = yield self._decode_payload(payload)
+            log.debug("PubChannel received: %r", decoded)
+            if decoded is not None:
+                callback(decoded)
         return self.transport.on_recv(wrap_callback)
     def _package_load(self, load):
         return {
             "enc": self.crypt,
             "load": load,
             "version": 2,
         }
-    @tornado.gen.coroutine
+    @salt.ext.tornado.gen.coroutine
     def send_id(self, tok, force_auth):
         """
         Send the minion id to the master so that the master may better
         track the connection state of the minion.
         In case of authentication errors, try to renegotiate authentication
         and retry the method.
         """
         load = {"id": self.opts["id"], "tok": tok}
-        @tornado.gen.coroutine
+        @salt.ext.tornado.gen.coroutine
         def _do_transfer():
             msg = self._package_load(self.auth.crypticle.dumps(load))
             package = salt.transport.frame.frame_msg(msg, header=None)
             yield self.transport.send(package)
-            raise tornado.gen.Return(True)
+            raise salt.ext.tornado.gen.Return(True)
         if force_auth or not self.auth.authenticated:
             count = 0
             while (
                 count <= self.opts["tcp_authentication_retries"]
                 or self.opts["tcp_authentication_retries"] < 0
             ):
                 try:
                     yield self.auth.authenticate()
                     break
                 except salt.exceptions.SaltClientError as exc:
                     log.debug(exc)
                     count += 1
         try:
             ret = yield _do_transfer()
-            raise tornado.gen.Return(ret)
+            raise salt.ext.tornado.gen.Return(ret)
         except salt.crypt.AuthenticationError:
             yield self.auth.authenticate()
             ret = yield _do_transfer()
-            raise tornado.gen.Return(ret)
-    @tornado.gen.coroutine
+            raise salt.ext.tornado.gen.Return(ret)
+    @salt.ext.tornado.gen.coroutine
     def connect_callback(self, result):
         if self._closing:
             return
         try:
             yield self.send_id(self.token, self._reconnected)
             self.connected = True
             self.event.fire_event({"master": self.opts["master"]}, "__master_connected")
             if self._reconnected:
                 if self.opts.get("__role") == "syndic":
                     data = "Syndic {} started at {}".format(
@@ -476,73 +460,64 @@
             return
         self.connected = False
         self.event.fire_event({"master": self.opts["master"]}, "__master_disconnected")
     def _verify_master_signature(self, payload):
         if self.opts.get("sign_pub_messages"):
             if not payload.get("sig", False):
                 raise salt.crypt.AuthenticationError(
                     "Message signing is enabled but the payload has no signature."
                 )
             if not salt.crypt.verify_signature(
-                self.master_pubkey_path, payload["load"], payload.get("sig")
+                self.master_pubkey_path,
+                payload["load"],
+                payload.get("sig"),
+                algorithm=payload["sig_algo"],
             ):
                 raise salt.crypt.AuthenticationError(
                     "Message signature failed to validate."
                 )
-    @tornado.gen.coroutine
+    @salt.ext.tornado.gen.coroutine
     def _decode_payload(self, payload):
         log.trace("Decoding payload: %s", payload)
         reauth = False
         if payload["enc"] == "aes":
             self._verify_master_signature(payload)
             try:
                 payload["load"] = self.auth.crypticle.loads(payload["load"])
             except salt.crypt.AuthenticationError:
                 reauth = True
             if reauth:
                 try:
                     yield self.auth.authenticate()
                     payload["load"] = self.auth.crypticle.loads(payload["load"])
                 except salt.crypt.AuthenticationError:
                     log.error(
                         "Payload decryption failed even after re-authenticating with master %s",
                         self.opts["master_ip"],
                     )
-        raise tornado.gen.Return(payload)
+        raise salt.ext.tornado.gen.Return(payload)
     def __enter__(self):
         return self
     def __exit__(self, *args):
-        self.io_loop.spawn_callback(self.close)
-    async def __aenter__(self):
-        return self
-    async def __aexit__(self, *_):
-        await self.close()
+        self.close()
 class AsyncPushChannel:
     """
     Factory class to create IPC Push channels
     """
     @staticmethod
     def factory(opts, **kwargs):
         """
         If we have additional IPC transports other than UxD and TCP, add them here
         """
-        salt.utils.versions.warn_until(
-            3009,
-            "AsyncPushChannel is deprecated. Use zeromq or tcp transport instead.",
-        )
         import salt.transport.ipc
         return salt.transport.ipc.IPCMessageClient(opts, **kwargs)
 class AsyncPullChannel:
     """
     Factory class to create IPC pull channels
     """
     @staticmethod
     def factory(opts, **kwargs):
         """
         If we have additional IPC transports other than UXD and TCP, add them here
         """
-        salt.utils.versions.warn_until(
-            3009,
-            "AsyncPullChannel is deprecated. Use zeromq or tcp transport instead.",
-        )
         import salt.transport.ipc
         return salt.transport.ipc.IPCMessageServer(opts, **kwargs)

--- a/salt/channel/server.py
+++ b/salt/channel/server.py
@@ -1,45 +1,33 @@
 """
 Encapsulate the different transports available to Salt.
 This includes server side transport, for the ReqServer and the Publisher
 """
-import asyncio
 import binascii
-import collections
 import hashlib
 import logging
 import os
-import pathlib
 import shutil
-import tornado.gen
 import salt.crypt
+import salt.ext.tornado.gen
 import salt.master
 import salt.payload
 import salt.transport.frame
 import salt.utils.channel
 import salt.utils.event
 import salt.utils.files
 import salt.utils.minions
 import salt.utils.platform
 import salt.utils.stringutils
 import salt.utils.verify
-from salt.exceptions import SaltDeserializationError
+from salt.exceptions import SaltDeserializationError, UnsupportedAlgorithm
 from salt.utils.cache import CacheCli
-try:
-    from M2Crypto import RSA
-    HAS_M2 = True
-except ImportError:
-    HAS_M2 = False
-    try:
-        from Cryptodome.Cipher import PKCS1_OAEP
-    except ImportError:
-        from Crypto.Cipher import PKCS1_OAEP  # nosec
 log = logging.getLogger(__name__)
 class ReqServerChannel:
     """
     ReqServerChannel handles request/reply messages from ReqChannels.
     """
     @classmethod
     def factory(cls, opts, **kwargs):
         if "master_uri" not in opts and "master_uri" in kwargs:
             opts["master_uri"] = kwargs["master_uri"]
         transport = salt.transport.request_server(opts, **kwargs)
@@ -48,29 +36,22 @@
     def compare_keys(cls, key1, key2):
         """
         Normalize and compare two keys
         Returns:
             bool: ``True`` if the keys match, otherwise ``False``
         """
         return salt.crypt.clean_key(key1) == salt.crypt.clean_key(key2)
     def __init__(self, opts, transport):
         self.opts = opts
         self.transport = transport
-        self.event = salt.utils.event.get_master_event(
-            self.opts, self.opts["sock_dir"], listen=False
-        )
-        self.master_key = salt.crypt.MasterKeys(self.opts)
-    @property
-    def aes_key(self):
-        if self.opts.get("cluster_id", None):
-            return salt.master.SMaster.secrets["cluster_aes"]["secret"].value
-        return salt.master.SMaster.secrets["aes"]["secret"].value
+        self.event = None
+        self.master_key = None
     def pre_fork(self, process_manager):
         """
         Do anything necessary pre-fork. Since this is on the master side this will
         primarily be bind and listen (or the equivalent for your network library)
         """
         if hasattr(self.transport, "pre_fork"):
             self.transport.pre_fork(process_manager)
     def post_fork(self, payload_handler, io_loop):
         """
         Do anything you need post-fork. This should handle all incoming payloads
@@ -78,155 +59,179 @@
         asynchronous needs
         """
         import salt.master
         if self.opts["pub_server_niceness"] and not salt.utils.platform.is_windows():
             log.info(
                 "setting Publish daemon niceness to %i",
                 self.opts["pub_server_niceness"],
             )
             os.nice(self.opts["pub_server_niceness"])
         self.io_loop = io_loop
-        self.crypticle = salt.crypt.Crypticle(self.opts, self.aes_key)
+        self.crypticle = salt.crypt.Crypticle(
+            self.opts, salt.master.SMaster.secrets["aes"]["secret"].value
+        )
         self.event = salt.utils.event.get_master_event(
             self.opts, self.opts["sock_dir"], listen=False
         )
         self.auto_key = salt.daemons.masterapi.AutoKey(self.opts)
         if self.opts["con_cache"]:
             self.cache_cli = CacheCli(self.opts)
         else:
             self.cache_cli = False
             self.ckminions = salt.utils.minions.CkMinions(self.opts)
         self.master_key = salt.crypt.MasterKeys(self.opts)
         self.payload_handler = payload_handler
         if hasattr(self.transport, "post_fork"):
             self.transport.post_fork(self.handle_message, io_loop)
-    @tornado.gen.coroutine
+    @salt.ext.tornado.gen.coroutine
     def handle_message(self, payload):
         try:
             payload = self._decode_payload(payload)
         except Exception as exc:  # pylint: disable=broad-except
             exc_type = type(exc).__name__
             if exc_type == "AuthenticationError":
                 log.debug(
                     "Minion failed to auth to master. Since the payload is "
                     "encrypted, it is not known which minion failed to "
                     "authenticate. It is likely that this is a transient "
                     "failure due to the master rotating its public key."
                 )
             else:
                 log.error("Bad load from minion: %s: %s", exc_type, exc)
-            raise tornado.gen.Return("bad load")
+            raise salt.ext.tornado.gen.Return("bad load")
         if not isinstance(payload, dict) or not isinstance(payload.get("load"), dict):
             log.error(
                 "payload and load must be a dict. Payload was: %s and load was %s",
                 payload,
                 payload.get("load"),
             )
-            raise tornado.gen.Return("payload and load must be a dict")
+            raise salt.ext.tornado.gen.Return("payload and load must be a dict")
         try:
             id_ = payload["load"].get("id", "")
             if "\0" in id_:
                 log.error("Payload contains an id with a null byte: %s", payload)
-                raise tornado.gen.Return("bad load: id contains a null byte")
+                raise salt.ext.tornado.gen.Return("bad load: id contains a null byte")
         except TypeError:
             log.error("Payload contains non-string id: %s", payload)
-            raise tornado.gen.Return(f"bad load: id {id_} is not a string")
+            raise salt.ext.tornado.gen.Return(f"bad load: id {id_} is not a string")
         version = 0
         if "version" in payload:
             version = payload["version"]
         sign_messages = False
         if version > 1:
             sign_messages = True
         if payload["enc"] == "clear" and payload.get("load", {}).get("cmd") == "_auth":
-            raise tornado.gen.Return(self._auth(payload["load"], sign_messages))
+            raise salt.ext.tornado.gen.Return(
+                self._auth(payload["load"], sign_messages)
+            )
         nonce = None
         if version > 1:
             nonce = payload["load"].pop("nonce", None)
         try:
             ret, req_opts = yield self.payload_handler(payload)
         except Exception as e:  # pylint: disable=broad-except
             log.error("Some exception handling a payload from minion", exc_info=True)
-            raise tornado.gen.Return("Some exception handling minion payload")
+            raise salt.ext.tornado.gen.Return("Some exception handling minion payload")
         req_fun = req_opts.get("fun", "send")
         if req_fun == "send_clear":
-            raise tornado.gen.Return(ret)
+            raise salt.ext.tornado.gen.Return(ret)
         elif req_fun == "send":
-            raise tornado.gen.Return(self.crypticle.dumps(ret, nonce))
+            raise salt.ext.tornado.gen.Return(self.crypticle.dumps(ret, nonce))
         elif req_fun == "send_private":
-            raise tornado.gen.Return(
+            raise salt.ext.tornado.gen.Return(
                 self._encrypt_private(
                     ret,
                     req_opts["key"],
                     req_opts["tgt"],
                     nonce,
                     sign_messages,
+                    payload.get("enc_algo", salt.crypt.OAEP_SHA1),
+                    payload.get("sig_algo", salt.crypt.PKCS1v15_SHA1),
                 ),
             )
         log.error("Unknown req_fun %s", req_fun)
-        raise tornado.gen.Return("Server-side exception handling payload")
-    def _encrypt_private(self, ret, dictkey, target, nonce=None, sign_messages=True):
+        raise salt.ext.tornado.gen.Return("Server-side exception handling payload")
+    def _encrypt_private(
+        self,
+        ret,
+        dictkey,
+        target,
+        nonce=None,
+        sign_messages=True,
+        encryption_algorithm=salt.crypt.OAEP_SHA1,
+        signing_algorithm=salt.crypt.PKCS1v15_SHA1,
+    ):
         """
         The server equivalent of ReqChannel.crypted_transfer_decode_dictentry
         """
-        if self.master_key.cluster_key:
-            pubfn = os.path.join(self.opts["cluster_pki_dir"], "minions", target)
-        else:
-            pubfn = os.path.join(self.opts["pki_dir"], "minions", target)
+        pubfn = os.path.join(self.opts["pki_dir"], "minions", target)
         key = salt.crypt.Crypticle.generate_key_string()
         pcrypt = salt.crypt.Crypticle(self.opts, key)
         try:
             pub = salt.crypt.PublicKey(pubfn)
         except (ValueError, IndexError, TypeError):
             return self.crypticle.dumps({})
         except OSError:
             log.error("AES key not found")
             return {"error": "AES key not found"}
         pret = {}
-        pret["key"] = pub.encrypt(key)
+        pret["key"] = pub.encrypt(
+            salt.utils.stringutils.to_bytes(key), encryption_algorithm
+        )
         if ret is False:
             ret = {}
         if sign_messages:
             if nonce is None:
                 return {"error": "Nonce not included in request"}
             tosign = salt.payload.dumps(
                 {"key": pret["key"], "pillar": ret, "nonce": nonce}
             )
+            master_pem_path = os.path.join(self.opts["pki_dir"], "master.pem")
             signed_msg = {
                 "data": tosign,
-                "sig": salt.crypt.PrivateKey(self.master_key.rsa_path).sign(tosign),
+                "sig": salt.crypt.PrivateKey(master_pem_path).sign(
+                    tosign, algorithm=signing_algorithm
+                ),
             }
             pret[dictkey] = pcrypt.dumps(signed_msg)
         else:
             pret[dictkey] = pcrypt.dumps(ret)
         return pret
-    def _clear_signed(self, load):
-        tosign = salt.payload.dumps(load)
-        return {
-            "enc": "clear",
-            "load": tosign,
-            "sig": salt.crypt.sign_message(self.master_key.rsa_path, tosign),
-        }
+    def _clear_signed(self, load, algorithm):
+        try:
+            master_pem_path = os.path.join(self.opts["pki_dir"], "master.pem")
+            tosign = salt.payload.dumps(load)
+            return {
+                "enc": "clear",
+                "load": tosign,
+                "sig": salt.crypt.PrivateKey(master_pem_path).sign(
+                    tosign, algorithm=algorithm
+                ),
+            }
+        except UnsupportedAlgorithm:
+            log.info(
+                "Minion tried to authenticate with unsupported signing algorithm: %s",
+                algorithm,
+            )
+            return {"enc": "clear", "load": {"ret": "bad sig algo"}}
     def _update_aes(self):
         """
         Check to see if a fresh AES key is available and update the components
         of the worker
         """
         import salt.master
-        key = "aes"
-        if self.opts.get("cluster_id", None):
-            key = "cluster_aes"
         if (
-            salt.master.SMaster.secrets[key]["secret"].value
+            salt.master.SMaster.secrets["aes"]["secret"].value
             != self.crypticle.key_string
         ):
             self.crypticle = salt.crypt.Crypticle(
-                self.opts, salt.master.SMaster.secrets[key]["secret"].value
+                self.opts, salt.master.SMaster.secrets["aes"]["secret"].value
             )
             return True
         return False
     def _decode_payload(self, payload):
         if (
             not isinstance(payload, dict)
             or "enc" not in payload
             or "load" not in payload
         ):
             raise SaltDeserializationError("bad load received on socket!")
@@ -245,24 +250,28 @@
         This method fires an event over the master event manager. The event is
         tagged "auth" and returns a dict with information about the auth
         event
             - Verify that the key we are receiving matches the stored key
             - Store the key if it is not there
             - Make an RSA key with the pub key
             - Encrypt the AES key as an encrypted salt.payload
             - Package the return and return it
         """
         import salt.master
+        enc_algo = load.get("enc_algo", salt.crypt.OAEP_SHA1)
+        sig_algo = load.get("sig_algo", salt.crypt.PKCS1v15_SHA1)
         if not salt.utils.verify.valid_id(self.opts, load["id"]):
             log.info("Authentication request from invalid id %s", load["id"])
             if sign_messages:
-                return self._clear_signed({"ret": False, "nonce": load["nonce"]})
+                return self._clear_signed(
+                    {"ret": False, "nonce": load["nonce"]}, sig_algo
+                )
             else:
                 return {"enc": "clear", "load": {"ret": False}}
         log.info("Authentication request from %s", load["id"])
         if self.opts["max_minions"] > 0:
             if self.cache_cli:
                 minions = self.cache_cli.get_cached()
             else:
                 minions = self.ckminions.connected_ids()
                 if len(minions) > 1000:
                     log.info(
@@ -283,48 +292,48 @@
                         "act": "full",
                         "id": load["id"],
                         "pub": load["pub"],
                     }
                     if self.opts.get("auth_events") is True:
                         self.event.fire_event(
                             eload, salt.utils.event.tagify(prefix="auth")
                         )
                     if sign_messages:
                         return self._clear_signed(
-                            {"ret": "full", "nonce": load["nonce"]}
+                            {"ret": "full", "nonce": load["nonce"]}, sig_algo
                         )
                     else:
                         return {"enc": "clear", "load": {"ret": "full"}}
-        pki_dir = self.opts["pki_dir"]
-        if self.opts["cluster_id"]:
-            if self.opts["cluster_pki_dir"]:
-                pki_dir = self.opts["cluster_pki_dir"]
         auto_reject = self.auto_key.check_autoreject(load["id"])
         auto_sign = self.auto_key.check_autosign(
             load["id"], load.get("autosign_grains", None)
         )
-        pubfn = os.path.join(pki_dir, "minions", load["id"])
-        pubfn_pend = os.path.join(pki_dir, "minions_pre", load["id"])
-        pubfn_rejected = os.path.join(pki_dir, "minions_rejected", load["id"])
-        pubfn_denied = os.path.join(pki_dir, "minions_denied", load["id"])
+        pubfn = os.path.join(self.opts["pki_dir"], "minions", load["id"])
+        pubfn_pend = os.path.join(self.opts["pki_dir"], "minions_pre", load["id"])
+        pubfn_rejected = os.path.join(
+            self.opts["pki_dir"], "minions_rejected", load["id"]
+        )
+        pubfn_denied = os.path.join(self.opts["pki_dir"], "minions_denied", load["id"])
         if self.opts["open_mode"]:
             pass
         elif os.path.isfile(pubfn_rejected):
             log.info(
                 "Public key rejected for %s. Key is present in rejection key dir.",
                 load["id"],
             )
             eload = {"result": False, "id": load["id"], "pub": load["pub"]}
             if self.opts.get("auth_events") is True:
                 self.event.fire_event(eload, salt.utils.event.tagify(prefix="auth"))
             if sign_messages:
-                return self._clear_signed({"ret": False, "nonce": load["nonce"]})
+                return self._clear_signed(
+                    {"ret": False, "nonce": load["nonce"]}, sig_algo
+                )
             else:
                 return {"enc": "clear", "load": {"ret": False}}
         elif os.path.isfile(pubfn):
             with salt.utils.files.fopen(pubfn, "r") as pubfn_handle:
                 if not self.compare_keys(pubfn_handle.read(), load["pub"]):
                     log.error(
                         "Authentication attempt from %s failed, the public "
                         "keys did not match. This may be an attempt to compromise "
                         "the Salt cluster.",
                         load["id"],
@@ -336,32 +345,34 @@
                         "id": load["id"],
                         "act": "denied",
                         "pub": load["pub"],
                     }
                     if self.opts.get("auth_events") is True:
                         self.event.fire_event(
                             eload, salt.utils.event.tagify(prefix="auth")
                         )
                     if sign_messages:
                         return self._clear_signed(
-                            {"ret": False, "nonce": load["nonce"]}
+                            {"ret": False, "nonce": load["nonce"]}, sig_algo
                         )
                     else:
                         return {"enc": "clear", "load": {"ret": False}}
         elif not os.path.isfile(pubfn_pend):
             if os.path.isdir(pubfn_pend):
                 log.info("New public key %s is a directory", load["id"])
                 eload = {"result": False, "id": load["id"], "pub": load["pub"]}
                 if self.opts.get("auth_events") is True:
                     self.event.fire_event(eload, salt.utils.event.tagify(prefix="auth"))
                 if sign_messages:
-                    return self._clear_signed({"ret": False, "nonce": load["nonce"]})
+                    return self._clear_signed(
+                        {"ret": False, "nonce": load["nonce"]}, sig_algo
+                    )
                 else:
                     return {"enc": "clear", "load": {"ret": False}}
             if auto_reject:
                 key_path = pubfn_rejected
                 log.info(
                     "New public key for %s rejected via autoreject_file", load["id"]
                 )
                 key_act = "reject"
                 key_result = False
             elif not auto_sign:
@@ -377,21 +388,22 @@
                 eload = {
                     "result": key_result,
                     "act": key_act,
                     "id": load["id"],
                     "pub": load["pub"],
                 }
                 if self.opts.get("auth_events") is True:
                     self.event.fire_event(eload, salt.utils.event.tagify(prefix="auth"))
                 if sign_messages:
                     return self._clear_signed(
-                        {"ret": key_result, "nonce": load["nonce"]}
+                        {"ret": key_result, "nonce": load["nonce"]},
+                        sig_algo,
                     )
                 else:
                     return {"enc": "clear", "load": {"ret": key_result}}
         elif os.path.isfile(pubfn_pend):
             if auto_reject:
                 try:
                     shutil.move(pubfn_pend, pubfn_rejected)
                 except OSError:
                     pass
                 log.info(
@@ -400,21 +412,23 @@
                 )
                 eload = {
                     "result": False,
                     "act": "reject",
                     "id": load["id"],
                     "pub": load["pub"],
                 }
                 if self.opts.get("auth_events") is True:
                     self.event.fire_event(eload, salt.utils.event.tagify(prefix="auth"))
                 if sign_messages:
-                    return self._clear_signed({"ret": False, "nonce": load["nonce"]})
+                    return self._clear_signed(
+                        {"ret": False, "nonce": load["nonce"]}, sig_algo
+                    )
                 else:
                     return {"enc": "clear", "load": {"ret": False}}
             elif not auto_sign:
                 with salt.utils.files.fopen(pubfn_pend, "r") as pubfn_handle:
                     if not self.compare_keys(pubfn_handle.read(), load["pub"]):
                         log.error(
                             "Authentication attempt from %s failed, the public "
                             "key in pending did not match. This may be an "
                             "attempt to compromise the Salt cluster.",
                             load["id"],
@@ -426,21 +440,21 @@
                             "id": load["id"],
                             "act": "denied",
                             "pub": load["pub"],
                         }
                         if self.opts.get("auth_events") is True:
                             self.event.fire_event(
                                 eload, salt.utils.event.tagify(prefix="auth")
                             )
                         if sign_messages:
                             return self._clear_signed(
-                                {"ret": False, "nonce": load["nonce"]}
+                                {"ret": False, "nonce": load["nonce"]}, sig_algo
                             )
                         else:
                             return {"enc": "clear", "load": {"ret": False}}
                     else:
                         log.info(
                             "Authentication failed from host %s, the key is in "
                             "pending and needs to be accepted with salt-key "
                             "-a %s",
                             load["id"],
                             load["id"],
@@ -450,21 +464,21 @@
                             "act": "pend",
                             "id": load["id"],
                             "pub": load["pub"],
                         }
                         if self.opts.get("auth_events") is True:
                             self.event.fire_event(
                                 eload, salt.utils.event.tagify(prefix="auth")
                             )
                         if sign_messages:
                             return self._clear_signed(
-                                {"ret": True, "nonce": load["nonce"]}
+                                {"ret": True, "nonce": load["nonce"]}, sig_algo
                             )
                         else:
                             return {"enc": "clear", "load": {"ret": True}}
             else:
                 with salt.utils.files.fopen(pubfn_pend, "r") as pubfn_handle:
                     if not self.compare_keys(pubfn_handle.read(), load["pub"]):
                         log.error(
                             "Authentication attempt from %s failed, the public "
                             "keys in pending did not match. This may be an "
                             "attempt to compromise the Salt cluster.",
@@ -472,134 +486,136 @@
                         )
                         with salt.utils.files.fopen(pubfn_denied, "w+") as fp_:
                             fp_.write(load["pub"])
                         eload = {"result": False, "id": load["id"], "pub": load["pub"]}
                         if self.opts.get("auth_events") is True:
                             self.event.fire_event(
                                 eload, salt.utils.event.tagify(prefix="auth")
                             )
                         if sign_messages:
                             return self._clear_signed(
-                                {"ret": False, "nonce": load["nonce"]}
+                                {"ret": False, "nonce": load["nonce"]}, sig_algo
                             )
                         else:
                             return {"enc": "clear", "load": {"ret": False}}
                     else:
                         os.remove(pubfn_pend)
         else:
             log.warning("Unaccounted for authentication failure")
             eload = {"result": False, "id": load["id"], "pub": load["pub"]}
             if self.opts.get("auth_events") is True:
                 self.event.fire_event(eload, salt.utils.event.tagify(prefix="auth"))
             if sign_messages:
-                return self._clear_signed({"ret": False, "nonce": load["nonce"]})
+                return self._clear_signed(
+                    {"ret": False, "nonce": load["nonce"]}, sig_algo
+                )
             else:
                 return {"enc": "clear", "load": {"ret": False}}
         log.info("Authentication accepted from %s", load["id"])
         if not os.path.isfile(pubfn) and not self.opts["open_mode"]:
             with salt.utils.files.fopen(pubfn, "w+") as fp_:
                 fp_.write(load["pub"])
         elif self.opts["open_mode"]:
             disk_key = ""
             if os.path.isfile(pubfn):
                 with salt.utils.files.fopen(pubfn, "r") as fp_:
                     disk_key = fp_.read()
             if load["pub"] and load["pub"] != disk_key:
                 log.debug("Host key change detected in open mode.")
                 with salt.utils.files.fopen(pubfn, "w+") as fp_:
                     fp_.write(load["pub"])
             elif not load["pub"]:
                 log.error("Public key is empty: %s", load["id"])
                 if sign_messages:
-                    return self._clear_signed({"ret": False, "nonce": load["nonce"]})
+                    return self._clear_signed(
+                        {"ret": False, "nonce": load["nonce"]}, sig_algo
+                    )
                 else:
                     return {"enc": "clear", "load": {"ret": False}}
         pub = None
         if self.cache_cli:
             self.cache_cli.put_cache([load["id"]])
         try:
-            pub = salt.crypt.get_rsa_pub_key(pubfn)
+            pub = salt.crypt.PublicKey(pubfn)
         except salt.crypt.InvalidKeyError as err:
             log.error('Corrupt public key "%s": %s', pubfn, err)
             if sign_messages:
-                return self._clear_signed({"ret": False, "nonce": load["nonce"]})
+                return self._clear_signed(
+                    {"ret": False, "nonce": load["nonce"]}, sig_algo
+                )
             else:
                 return {"enc": "clear", "load": {"ret": False}}
-        if not HAS_M2:
-            cipher = PKCS1_OAEP.new(pub)
         ret = {
             "enc": "pub",
             "pub_key": self.master_key.get_pub_str(),
             "publish_port": self.opts["publish_port"],
         }
         if self.opts["master_sign_pubkey"]:
             if self.master_key.pubkey_signature():
                 log.debug("Adding pubkey signature to auth-reply")
                 log.debug(self.master_key.pubkey_signature())
                 ret.update({"pub_sig": self.master_key.pubkey_signature()})
             else:
                 key_pass = salt.utils.sdb.sdb_get(
                     self.opts["signing_key_pass"], self.opts
                 )
                 log.debug("Signing master public key before sending")
                 pub_sign = salt.crypt.sign_message(
-                    self.master_key.get_sign_paths()[1], ret["pub_key"], key_pass
+                    self.master_key.get_sign_paths()[1],
+                    ret["pub_key"],
+                    key_pass,
+                    algorithm=sig_algo,
                 )
                 ret.update({"pub_sig": binascii.b2a_base64(pub_sign)})
-        if not HAS_M2:
-            mcipher = PKCS1_OAEP.new(self.master_key.key)
         if self.opts["auth_mode"] >= 2:
             if "token" in load:
                 try:
-                    if HAS_M2:
-                        mtoken = self.master_key.key.private_decrypt(
-                            load["token"], RSA.pkcs1_oaep_padding
-                        )
-                    else:
-                        mtoken = mcipher.decrypt(load["token"])
-                    aes = f"{self.aes_key}_|-{mtoken}"
-                except Exception:  # pylint: disable=broad-except
-                    pass
-            else:
-                aes = self.aes_key
-            if HAS_M2:
-                ret["aes"] = pub.public_encrypt(aes, RSA.pkcs1_oaep_padding)
-            else:
-                ret["aes"] = cipher.encrypt(aes)
+                    mtoken = self.master_key.key.decrypt(load["token"], enc_algo)
+                    aes = "{}_|-{}".format(
+                        salt.master.SMaster.secrets["aes"]["secret"].value, mtoken
+                    )
+                except UnsupportedAlgorithm as exc:
+                    log.info(
+                        "Minion %s tried to authenticate with unsupported encryption algorithm: %s",
+                        load["id"],
+                        enc_algo,
+                    )
+                    return {"enc": "clear", "load": {"ret": "bad enc algo"}}
+                except Exception as exc:  # pylint: disable=broad-except
+                    log.warning("Token failed to decrypt %s", exc)
+            else:
+                aes = salt.master.SMaster.secrets["aes"]["secret"].value
+            ret["aes"] = pub.encrypt(aes, enc_algo)
         else:
             if "token" in load:
                 try:
-                    if HAS_M2:
-                        mtoken = self.master_key.key.private_decrypt(
-                            load["token"], RSA.pkcs1_oaep_padding
-                        )
-                        ret["token"] = pub.public_encrypt(
-                            mtoken, RSA.pkcs1_oaep_padding
-                        )
-                    else:
-                        mtoken = mcipher.decrypt(load["token"])
-                        ret["token"] = cipher.encrypt(mtoken)
-                except Exception:  # pylint: disable=broad-except
-                    pass
-            aes = self.aes_key
-            if HAS_M2:
-                ret["aes"] = pub.public_encrypt(aes, RSA.pkcs1_oaep_padding)
-            else:
-                ret["aes"] = cipher.encrypt(aes)
+                    mtoken = self.master_key.key.decrypt(load["token"], enc_algo)
+                    ret["token"] = pub.encrypt(mtoken, enc_algo)
+                except UnsupportedAlgorithm as exc:
+                    log.info(
+                        "Minion %s tried to authenticate with unsupported encryption algorithm: %s",
+                        load["id"],
+                        enc_algo,
+                    )
+                    return {"enc": "clear", "load": {"ret": "bad enc algo"}}
+                except Exception as exc:  # pylint: disable=broad-except
+                    log.warning("Token failed to decrypt: %r", exc)
+            aes = salt.master.SMaster.secrets["aes"]["secret"].value
+            ret["aes"] = pub.encrypt(aes, enc_algo)
         digest = salt.utils.stringutils.to_bytes(hashlib.sha256(aes).hexdigest())
         ret["sig"] = salt.crypt.private_encrypt(self.master_key.key, digest)
         eload = {"result": True, "act": "accept", "id": load["id"], "pub": load["pub"]}
         if self.opts.get("auth_events") is True:
             self.event.fire_event(eload, salt.utils.event.tagify(prefix="auth"))
         if sign_messages:
             ret["nonce"] = load["nonce"]
-            return self._clear_signed(ret)
+            return self._clear_signed(ret, sig_algo)
         return ret
     def close(self):
         self.transport.close()
         if self.event is not None:
             self.event.destroy()
 class PubServerChannel:
     """
     Factory class to create subscription channels to the master's Publisher
     """
     @classmethod
@@ -617,74 +633,70 @@
         transport = salt.transport.publish_server(opts, **kwargs)
         return cls(opts, transport, presence_events=presence_events)
     def __init__(self, opts, transport, presence_events=False):
         self.opts = opts
         self.ckminions = salt.utils.minions.CkMinions(self.opts)
         self.transport = transport
         self.aes_funcs = salt.master.AESFuncs(self.opts)
         self.present = {}
         self.presence_events = presence_events
         self.event = salt.utils.event.get_event("master", opts=self.opts, listen=False)
-    @property
-    def aes_key(self):
-        if self.opts.get("cluster_id", None):
-            return salt.master.SMaster.secrets["cluster_aes"]["secret"].value
-        return salt.master.SMaster.secrets["aes"]["secret"].value
     def __getstate__(self):
         return {
             "opts": self.opts,
             "transport": self.transport,
             "presence_events": self.presence_events,
         }
     def __setstate__(self, state):
         self.opts = state["opts"]
         self.state = state["presence_events"]
         self.transport = state["transport"]
         self.event = salt.utils.event.get_event("master", opts=self.opts, listen=False)
         self.ckminions = salt.utils.minions.CkMinions(self.opts)
         self.present = {}
-        self.master_key = salt.crypt.MasterKeys(self.opts)
     def close(self):
         self.transport.close()
         if self.event is not None:
             self.event.destroy()
             self.event = None
         if self.aes_funcs is not None:
             self.aes_funcs.destroy()
             self.aes_funcs = None
     def pre_fork(self, process_manager, kwargs=None):
         """
         Do anything necessary pre-fork. Since this is on the master side this will
         primarily be used to create IPC channels and create our daemon process to
         do the actual publishing
         :param func process_manager: A ProcessManager, from salt.utils.process.ProcessManager
         """
         if hasattr(self.transport, "publish_daemon"):
             process_manager.add_process(self._publish_daemon, kwargs=kwargs)
     def _publish_daemon(self, **kwargs):
         if self.opts["pub_server_niceness"] and not salt.utils.platform.is_windows():
-            log.debug(
+            log.info(
                 "setting Publish daemon niceness to %i",
                 self.opts["pub_server_niceness"],
             )
             os.nice(self.opts["pub_server_niceness"])
         secrets = kwargs.get("secrets", None)
         if secrets is not None:
             salt.master.SMaster.secrets = secrets
         self.master_key = salt.crypt.MasterKeys(self.opts)
         self.transport.publish_daemon(
             self.publish_payload, self.presence_callback, self.remove_presence_callback
         )
     def presence_callback(self, subscriber, msg):
         if msg["enc"] != "aes":
             return
-        crypticle = salt.crypt.Crypticle(self.opts, self.aes_key)
+        crypticle = salt.crypt.Crypticle(
+            self.opts, salt.master.SMaster.secrets["aes"]["secret"].value
+        )
         load = crypticle.loads(msg["load"])
         load = salt.transport.frame.decode_embedded_strs(load)
         if not self.aes_funcs.verify_minion(load["id"], load["tok"]):
             return
         subscriber.id_ = load["id"]
         self._add_client_present(subscriber)
     def remove_presence_callback(self, subscriber):
         self._remove_client_present(subscriber)
     def _add_client_present(self, client):
         id_ = client.id_
@@ -714,291 +726,63 @@
             del self.present[id_]
             if self.presence_events:
                 data = {"new": [], "lost": [id_]}
                 self.event.fire_event(
                     data, salt.utils.event.tagify("change", "presence")
                 )
                 data = {"present": list(self.present.keys())}
                 self.event.fire_event(
                     data, salt.utils.event.tagify("present", "presence")
                 )
-    async def publish_payload(self, load, *args):
-        load = salt.payload.loads(load)
+    @salt.ext.tornado.gen.coroutine
+    def publish_payload(self, load, *args):
         unpacked_package = self.wrap_payload(load)
         try:
             payload = salt.payload.loads(unpacked_package["payload"])
         except KeyError:
             log.error("Invalid package %r", unpacked_package)
             raise
-        payload = salt.payload.dumps(payload)
         if "topic_lst" in unpacked_package:
             topic_list = unpacked_package["topic_lst"]
-            ret = await self.transport.publish_payload(payload, topic_list)
+            ret = yield self.transport.publish_payload(payload, topic_list)
         else:
-            ret = await self.transport.publish_payload(payload)
-        return ret
+            ret = yield self.transport.publish_payload(payload)
+        raise salt.ext.tornado.gen.Return(ret)
     def wrap_payload(self, load):
         payload = {"enc": "aes"}
-        if not self.opts.get("cluster_id", None):
-            load["serial"] = salt.master.SMaster.get_serial()
-        crypticle = salt.crypt.Crypticle(self.opts, self.aes_key)
+        load["serial"] = salt.master.SMaster.get_serial()
+        crypticle = salt.crypt.Crypticle(
+            self.opts, salt.master.SMaster.secrets["aes"]["secret"].value
+        )
         payload["load"] = crypticle.dumps(load)
         if self.opts["sign_pub_messages"]:
+            master_pem_path = os.path.join(self.opts["pki_dir"], "master.pem")
             log.debug("Signing data packet")
+            payload["sig_algo"] = self.opts["publish_signing_algorithm"]
             payload["sig"] = salt.crypt.PrivateKey(
                 self.master_key.rsa_path,
-            ).sign(payload["load"])
+            ).sign(payload["load"], self.opts["publish_signing_algorithm"])
         int_payload = {"payload": salt.payload.dumps(payload)}
         match_targets = ["pcre", "glob", "list"]
         if self.transport.topic_support and load["tgt_type"] in match_targets:
             if load["tgt_type"] == "list":
                 int_payload["topic_lst"] = load["tgt"]
             if isinstance(load["tgt"], str):
                 _res = self.ckminions.check_minions(
                     load["tgt"], tgt_type=load["tgt_type"]
                 )
                 match_ids = _res["minions"]
                 log.debug("Publish Side Match: %s", match_ids)
                 int_payload["topic_lst"] = match_ids
             else:
                 int_payload["topic_lst"] = load["tgt"]
         return int_payload
-    async def publish(self, load):
+    def publish(self, load):
         """
         Publish "load" to minions
         """
         log.debug(
             "Sending payload to publish daemon. jid=%s load=%s",
             load.get("jid", None),
             repr(load)[:40],
         )
-        payload = salt.payload.dumps(load)
-        await self.transport.publish(payload)
-class MasterPubServerChannel:
-    """ """
-    @classmethod
-    def factory(cls, opts, **kwargs):
-        transport = salt.transport.ipc_publish_server("master", opts)
-        return cls(opts, transport)
-    def __init__(self, opts, transport, presence_events=False):
-        self.opts = opts
-        self.transport = transport
-        self.io_loop = tornado.ioloop.IOLoop.current()
-        self.master_key = salt.crypt.MasterKeys(self.opts)
-        self.peer_keys = {}
-    def send_aes_key_event(self):
-        data = {"peer_id": self.opts["id"], "peers": {}}
-        for peer in self.opts.get("cluster_peers", []):
-            peer_pub = (
-                pathlib.Path(self.opts["cluster_pki_dir"]) / "peers" / f"{peer}.pub"
-            )
-            if peer_pub.exists():
-                pub = salt.crypt.PublicKey(peer_pub)
-                aes = salt.master.SMaster.secrets["aes"]["secret"].value
-                digest = salt.utils.stringutils.to_bytes(
-                    hashlib.sha256(aes).hexdigest()
-                )
-                data["peers"][peer] = {
-                    "aes": pub.encrypt(aes),
-                    "sig": salt.crypt.private_encrypt(
-                        self.master_key.master_key, digest
-                    ),
-                }
-            else:
-                log.warning("Peer key missing %r", peer_pub)
-                data["peers"][peer] = {}
-        with salt.utils.event.get_master_event(
-            self.opts, self.opts["sock_dir"], listen=False
-        ) as event:
-            success = event.fire_event(
-                data,
-                salt.utils.event.tagify(self.opts["id"], "peer", "cluster"),
-                timeout=30000,  # 30 second timeout
-            )
-            if not success:
-                log.error("Unable to send aes key event")
-    def __getstate__(self):
-        return {
-            "opts": self.opts,
-            "transport": self.transport,
-        }
-    def __setstate__(self, state):
-        self.opts = state["opts"]
-        self.transport = state["transport"]
-    def close(self):
-        self.transport.close()
-    def pre_fork(self, process_manager, kwargs=None):
-        """
-        Do anything necessary pre-fork. Since this is on the master side this will
-        primarily be used to create IPC channels and create our daemon process to
-        do the actual publishing
-        :param func process_manager: A ProcessManager, from salt.utils.process.ProcessManager
-        """
-        if hasattr(self.transport, "publish_daemon"):
-            process_manager.add_process(
-                self._publish_daemon, kwargs=kwargs, name="EventPublisher"
-            )
-    def _publish_daemon(self, **kwargs):
-        if (
-            self.opts["event_publisher_niceness"]
-            and not salt.utils.platform.is_windows()
-        ):
-            log.info(
-                "setting EventPublisher niceness to %i",
-                self.opts["event_publisher_niceness"],
-            )
-            os.nice(self.opts["event_publisher_niceness"])
-        self.io_loop = tornado.ioloop.IOLoop.current()
-        tcp_master_pool_port = 4520
-        self.pushers = []
-        self.auth_errors = {}
-        for peer in self.opts.get("cluster_peers", []):
-            pusher = salt.transport.tcp.TCPPublishServer(
-                self.opts,
-                pull_host=peer,
-                pull_port=tcp_master_pool_port,
-            )
-            self.auth_errors[peer] = collections.deque()
-            self.pushers.append(pusher)
-        if self.opts.get("cluster_id", None):
-            self.pool_puller = salt.transport.tcp.TCPPuller(
-                host=self.opts["interface"],
-                port=tcp_master_pool_port,
-                io_loop=self.io_loop,
-                payload_handler=self.handle_pool_publish,
-            )
-            self.pool_puller.start()
-        self.io_loop.add_callback(
-            self.transport.publisher,
-            self.publish_payload,
-            io_loop=self.io_loop,
-        )
-        try:
-            self.io_loop.start()
-        except (KeyboardInterrupt, SystemExit):
-            pass
-        finally:
-            self.close()
-    async def handle_pool_publish(self, payload, _):
-        """
-        Handle incomming events from cluster peer.
-        """
-        try:
-            tag, data = salt.utils.event.SaltEvent.unpack(payload)
-            if tag.startswith("cluster/peer"):
-                peer = data["peer_id"]
-                aes = data["peers"][self.opts["id"]]["aes"]
-                sig = data["peers"][self.opts["id"]]["sig"]
-                key_str = self.master_key.master_private_decrypt(aes)
-                digest = salt.utils.stringutils.to_bytes(
-                    hashlib.sha256(key_str).hexdigest()
-                )
-                pub_path = (
-                    pathlib.Path(self.opts["cluster_pki_dir"]) / "peers" / f"{peer}.pub"
-                )
-                key = salt.crypt.PublicKey(pub_path)
-                m_digest = key.decrypt(sig)
-                if m_digest != digest:
-                    log.error("Invalid aes signature from peer: %s", peer)
-                    return
-                log.error("Received new key from peer %s", peer)
-                if peer in self.peer_keys:
-                    if self.peer_keys[peer] != key_str:
-                        self.peer_keys[peer] = key_str
-                        self.send_aes_key_event()
-                        while self.auth_errors[peer]:
-                            key, data = self.auth_errors[peer].popleft()
-                            peer_id, parsed_tag = self.parse_cluster_tag(tag)
-                            try:
-                                event_data = self.extract_cluster_event(peer_id, data)
-                            except salt.exceptions.AuthenticationError:
-                                log.error(
-                                    "Event from peer failed authentication: %s", peer_id
-                                )
-                            else:
-                                await self.transport.publish_payload(
-                                    salt.utils.event.SaltEvent.pack(
-                                        parsed_tag, event_data
-                                    )
-                                )
-                else:
-                    self.peer_keys[peer] = key_str
-                    self.send_aes_key_event()
-                    while self.auth_errors[peer]:
-                        key, data = self.auth_errors[peer].popleft()
-                        peer_id, parsed_tag = self.parse_cluster_tag(tag)
-                        try:
-                            event_data = self.extract_cluster_event(peer_id, data)
-                        except salt.exceptions.AuthenticationError:
-                            log.error(
-                                "Event from peer failed authentication: %s", peer_id
-                            )
-                        else:
-                            await self.transport.publish_payload(
-                                salt.utils.event.SaltEvent.pack(parsed_tag, event_data)
-                            )
-            elif tag.startswith("cluster/event"):
-                peer_id, parsed_tag = self.parse_cluster_tag(tag)
-                try:
-                    event_data = self.extract_cluster_event(peer_id, data)
-                except salt.exceptions.AuthenticationError:
-                    self.auth_errors[peer_id].append((tag, data))
-                else:
-                    await self.transport.publish_payload(
-                        salt.utils.event.SaltEvent.pack(parsed_tag, event_data)
-                    )
-            else:
-                log.error("This cluster tag not valid %s", tag)
-        except Exception:  # pylint: disable=broad-except
-            log.critical("Unhandled error while polling master events", exc_info=True)
-            return None
-    def parse_cluster_tag(self, tag):
-        peer_id = tag.replace("cluster/event/", "").split("/")[0]
-        stripped_tag = tag.replace(f"cluster/event/{peer_id}/", "")
-        return peer_id, stripped_tag
-    def extract_cluster_event(self, peer_id, data):
-        if peer_id in self.peer_keys:
-            crypticle = salt.crypt.Crypticle(self.opts, self.peer_keys[peer_id])
-            event_data = crypticle.loads(data)["event_payload"]
-            event_data["__peer_id"] = peer_id
-            return event_data
-        raise salt.exceptions.AuthenticationError("Peer aes key not available")
-    async def publish_payload(self, load, *args):
-        tag, data = salt.utils.event.SaltEvent.unpack(load)
-        tasks = []
-        if not tag.startswith("cluster/peer"):
-            tasks = [
-                asyncio.create_task(
-                    self.transport.publish_payload(load), name=self.opts["id"]
-                )
-            ]
-        for pusher in self.pushers:
-            log.debug("Publish event to peer %s:%s", pusher.pull_host, pusher.pull_port)
-            if tag.startswith("cluster/peer"):
-                tasks.append(
-                    asyncio.create_task(pusher.publish(load), name=pusher.pull_host)
-                )
-                continue
-            crypticle = salt.crypt.Crypticle(
-                self.opts, salt.master.SMaster.secrets["aes"]["secret"].value
-            )
-            load = {"event_payload": data}
-            event_data = salt.utils.event.SaltEvent.pack(
-                salt.utils.event.tagify(tag, self.opts["id"], "cluster/event"),
-                crypticle.dumps(load),
-            )
-            tasks.append(asyncio.create_task(pusher.publish(event_data)))
-        await asyncio.gather(*tasks, return_exceptions=True)
-        for task in tasks:
-            try:
-                task.result()
-            except tornado.iostream.StreamClosedError:
-                if task.get_name() == self.opts["id"]:
-                    log.error("Unable to forward event to local ipc bus")
-                else:
-                    log.warning(
-                        "Unable to forward event to cluster peer %s", task.get_name()
-                    )
-            except Exception as exc:  # pylint: disable=broad-except
-                log.error(
-                    "Unhandled error sending task %s", task.get_name(), exc_info=True
-                )
+        self.transport.publish(load)

--- a/salt/cli/daemons.py
+++ b/salt/cli/daemons.py
@@ -87,72 +87,53 @@
 class Master(
     salt.utils.parsers.MasterOptionParser, DaemonsMixin
 ):  # pylint: disable=no-init
     """
     Creates a master server
     """
     def _handle_signals(self, signum, sigframe):
         if hasattr(self.master, "process_manager"):
             self.master.process_manager._handle_signals(signum, sigframe)
         super()._handle_signals(signum, sigframe)
-    def verify_environment(self):
-        if not self.config["verify_env"]:
-            return
-        v_dirs = [
-            self.config["pki_dir"],
-            os.path.join(self.config["pki_dir"], "minions"),
-            os.path.join(self.config["pki_dir"], "minions_pre"),
-            os.path.join(self.config["pki_dir"], "minions_denied"),
-            os.path.join(self.config["pki_dir"], "minions_autosign"),
-            os.path.join(self.config["pki_dir"], "minions_rejected"),
-            self.config["cachedir"],
-            os.path.join(self.config["cachedir"], "jobs"),
-            os.path.join(self.config["cachedir"], "proc"),
-            self.config["sock_dir"],
-            self.config["token_dir"],
-            self.config["syndic_dir"],
-            self.config["sqlite_queue_dir"],
-        ]
-        pki_dir = self.config["pki_dir"]
-        if (
-            self.config["cluster_id"]
-            and self.config["cluster_pki_dir"]
-            and self.config["cluster_pki_dir"] != self.config["pki_dir"]
-        ):
-            v_dirs.extend(
-                [
-                    self.config["cluster_pki_dir"],
-                    os.path.join(self.config["cluster_pki_dir"], "peers"),
-                    os.path.join(self.config["cluster_pki_dir"], "minions"),
-                    os.path.join(self.config["cluster_pki_dir"], "minions_pre"),
-                    os.path.join(self.config["cluster_pki_dir"], "minions_denied"),
-                    os.path.join(self.config["cluster_pki_dir"], "minions_autosign"),
-                    os.path.join(self.config["cluster_pki_dir"], "minions_rejected"),
+    def prepare(self):
+        """
+        Run the preparation sequence required to start a salt master server.
+        If sub-classed, don't **ever** forget to run:
+            super(YourSubClass, self).prepare()
+        """
+        super().prepare()
+        try:
+            if self.config["verify_env"]:
+                v_dirs = [
+                    self.config["pki_dir"],
+                    os.path.join(self.config["pki_dir"], "minions"),
+                    os.path.join(self.config["pki_dir"], "minions_pre"),
+                    os.path.join(self.config["pki_dir"], "minions_denied"),
+                    os.path.join(self.config["pki_dir"], "minions_autosign"),
+                    os.path.join(self.config["pki_dir"], "minions_rejected"),
+                    self.config["cachedir"],
+                    os.path.join(self.config["cachedir"], "jobs"),
+                    os.path.join(self.config["cachedir"], "proc"),
+                    self.config["sock_dir"],
+                    self.config["token_dir"],
+                    self.config["syndic_dir"],
+                    self.config["sqlite_queue_dir"],
                 ]
-            )
-            pki_dir = [self.config["pki_dir"], self.config["cluster_pki_dir"]]
-        verify_env(
-            v_dirs,
-            self.config["user"],
-            permissive=self.config["permissive_pki_access"],
-            root_dir=self.config["root_dir"],
-            pki_dir=pki_dir,
-        )
-    def prepare(self):
-        """
-        Run the preparation sequence required to start a salt master server.
-        If sub-classed, don't **ever** forget to run:
-            super(YourSubClass, self).prepare()
-        """
-        super().prepare()
-        try:
-            self.verify_environment()
+                verify_env(
+                    v_dirs,
+                    self.config["user"],
+                    permissive=self.config["permissive_pki_access"],
+                    root_dir=self.config["root_dir"],
+                    pki_dir=self.config["pki_dir"],
+                )
+                for syndic_file in os.listdir(self.config["syndic_dir"]):
+                    os.remove(os.path.join(self.config["syndic_dir"], syndic_file))
         except OSError as error:
             self.environment_failure(error)
         self.action_log_info("Setting up")
         if not verify_socket(
             self.config["interface"],
             self.config["publish_port"],
             self.config["ret_port"],
         ):
             self.shutdown(4, "The ports are not available to bind")
         self.config["interface"] = ip_bracket(self.config["interface"])

--- a/salt/cli/salt.py
+++ b/salt/cli/salt.py
@@ -163,21 +163,22 @@
             ):
                 sys.stderr.write("ERROR: Minions returned with non-zero exit code\n")
                 sys.exit(salt.defaults.exitcodes.EX_GENERIC)
         except (
             AuthenticationError,
             AuthorizationError,
             SaltInvocationError,
             EauthAuthenticationError,
             SaltClientError,
         ) as exc:
-            self._output_ret(str(exc), "", retcode=1)
+            ret = str(exc)
+            self._output_ret(ret, "", retcode=1)
         finally:
             self.local_client.destroy()
     def _preview_target(self):
         """
         Return a list of minions from a given target
         """
         return self.local_client.gather_minions(
             self.config["tgt"], self.selected_target_option or "glob"
         )
     def _run_batch(self):

--- a/salt/client/__init__.py
+++ b/salt/client/__init__.py
@@ -6,25 +6,25 @@
               'arg':, ('arg1', 'arg2', ...),
               'tgt': '<glob or id>',
               'key': '<read in the key file>'}
 """
 import logging
 import os
 import random
 import sys
 import time
 from datetime import datetime
-import tornado.gen
 import salt.cache
 import salt.channel.client
 import salt.config
 import salt.defaults.exitcodes
+import salt.ext.tornado.gen
 import salt.loader
 import salt.payload
 import salt.syspaths as syspaths
 import salt.utils.args
 import salt.utils.event
 import salt.utils.files
 import salt.utils.jid
 import salt.utils.minions
 import salt.utils.network
 import salt.utils.platform
@@ -318,21 +318,21 @@
         except AuthorizationError as err:
             raise
         except Exception as general_exception:  # pylint: disable=broad-except
             raise SaltClientError(general_exception)
         return self._check_pub_data(pub_data, listen=listen)
     def gather_minions(self, tgt, expr_form):
         _res = salt.utils.minions.CkMinions(self.opts).check_minions(
             tgt, tgt_type=expr_form
         )
         return _res["minions"]
-    @tornado.gen.coroutine
+    @salt.ext.tornado.gen.coroutine
     def run_job_async(
         self,
         tgt,
         fun,
         arg=(),
         tgt_type="glob",
         ret="",
         timeout=None,
         jid="",
         kwarg=None,
@@ -367,21 +367,21 @@
         except SaltClientError:
             raise SaltClientError(
                 "The salt master could not be contacted. Is master running?"
             )
         except AuthenticationError as err:
             raise AuthenticationError(err)
         except AuthorizationError as err:
             raise AuthorizationError(err)
         except Exception as general_exception:  # pylint: disable=broad-except
             raise SaltClientError(general_exception)
-        raise tornado.gen.Return(self._check_pub_data(pub_data, listen=listen))
+        raise salt.ext.tornado.gen.Return(self._check_pub_data(pub_data, listen=listen))
     def cmd_async(
         self, tgt, fun, arg=(), tgt_type="glob", ret="", jid="", kwarg=None, **kwargs
     ):
         """
         Asynchronously send a command to connected minions
         The function signature is the same as :py:meth:`cmd` with the
         following exceptions.
         :returns: A job ID or 0 on failure.
         .. code-block:: python
             >>> local.cmd_async('*', 'test.sleep', [300])
@@ -1042,21 +1042,21 @@
                 if raw is None:
                     break
                 try:
                     if raw["data"]["retcode"] > 0:
                         log.error(
                             "saltutil returning errors on minion %s", raw["data"]["id"]
                         )
                         minions.remove(raw["data"]["id"])
                         break
                 except KeyError as exc:
-                    missing_key = exc.__str__().strip("'\"")
+                    missing_key = str(exc).strip("'\"")
                     if missing_key == "retcode":
                         log.debug("retcode missing from client return")
                     else:
                         log.debug(
                             "Passing on saltutil error. Key '%s' missing "
                             "from client return. This may be an error in "
                             "the client.",
                             missing_key,
                         )
                 open_jids.add(jinfo["jid"])
@@ -1564,21 +1564,21 @@
                     err_name = error.get("name", "")
                     err_msg = error.get("message", "")
                     if err_name == "AuthenticationError":
                         raise AuthenticationError(err_msg)
                     elif err_name == "AuthorizationError":
                         raise AuthorizationError(err_msg)
                 raise PublishError(error)
             if not payload:
                 return payload
         return {"jid": payload["load"]["jid"], "minions": payload["load"]["minions"]}
-    @tornado.gen.coroutine
+    @salt.ext.tornado.gen.coroutine
     def pub_async(
         self,
         tgt,
         fun,
         arg=(),
         tgt_type="glob",
         ret="",
         jid="",
         timeout=5,
         io_loop=None,
@@ -1636,37 +1636,37 @@
                     "may need to run your command with `--async` in order to "
                     "bypass the congested event bus. With `--async`, the CLI tool "
                     "will print the job id (jid) and exit immediately without "
                     "listening for responses. You can then use "
                     "`salt-run jobs.lookup_jid` to look up the results of the job "
                     "in the job cache later."
                 )
             if not payload:
                 key = self.__read_master_key()
                 if key == self.key:
-                    raise tornado.gen.Return(payload)
+                    raise salt.ext.tornado.gen.Return(payload)
                 self.key = key
                 payload_kwargs["key"] = self.key
                 payload = yield channel.send(payload_kwargs)
             error = payload.pop("error", None)
             if error is not None:
                 if isinstance(error, dict):
                     err_name = error.get("name", "")
                     err_msg = error.get("message", "")
                     if err_name == "AuthenticationError":
                         raise AuthenticationError(err_msg)
                     elif err_name == "AuthorizationError":
                         raise AuthorizationError(err_msg)
                 raise PublishError(error)
             if not payload:
-                raise tornado.gen.Return(payload)
-        raise tornado.gen.Return(
+                raise salt.ext.tornado.gen.Return(payload)
+        raise salt.ext.tornado.gen.Return(
             {"jid": payload["load"]["jid"], "minions": payload["load"]["minions"]}
         )
     def __del__(self):
         self.destroy()
     def _clean_up_subscriptions(self, job_id):
         if self.opts.get("order_masters"):
             self.event.unsubscribe(f"syndic/.*/{job_id}", "regex")
         self.event.unsubscribe(f"salt/job/{job_id}")
     def destroy(self):
         if self.event is not None:

--- a/salt/client/mixins.py
+++ b/salt/client/mixins.py
@@ -5,20 +5,21 @@
 import fnmatch
 import logging
 import os
 import signal
 import traceback
 import weakref
 from collections.abc import Mapping, MutableMapping
 import salt._logging
 import salt.channel.client
 import salt.exceptions
+import salt.ext.tornado.stack_context
 import salt.minion
 import salt.output
 import salt.utils.args
 import salt.utils.doc
 import salt.utils.error
 import salt.utils.event
 import salt.utils.jid
 import salt.utils.job
 import salt.utils.lazy
 import salt.utils.platform
@@ -292,37 +293,40 @@
                 else:
                     f_call = salt.utils.args.format_call(
                         self.functions[fun],
                         low,
                         expected_extra_kws=CLIENT_INTERNAL_KEYWORDS,
                     )
                     args = f_call.get("args", ())
                     kwargs = f_call.get("kwargs", {})
                 data["fun_args"] = list(args) + ([kwargs] if kwargs else [])
                 func_globals["__jid_event__"].fire_event(data, "new")
-                func = self.functions[fun]
-                try:
-                    data["return"] = func(*args, **kwargs)
-                except TypeError as exc:
-                    data[
-                        "return"
-                    ] = "\nPassed invalid arguments: {}\n\nUsage:\n{}".format(
-                        exc, func.__doc__
-                    )
-                try:
-                    data["success"] = self.context.get("retcode", 0) == 0
-                except AttributeError:
-                    data["success"] = True
-                if isinstance(data["return"], dict) and "data" in data["return"]:
-                    data["success"] = salt.utils.state.check_result(
-                        data["return"]["data"]
-                    )
+                with salt.ext.tornado.stack_context.StackContext(
+                    self.functions.context_dict.clone
+                ):
+                    func = self.functions[fun]
+                    try:
+                        data["return"] = func(*args, **kwargs)
+                    except TypeError as exc:
+                        data["return"] = (
+                            "\nPassed invalid arguments: {}\n\nUsage:\n{}".format(
+                                exc, func.__doc__
+                            )
+                        )
+                    try:
+                        data["success"] = self.context.get("retcode", 0) == 0
+                    except AttributeError:
+                        data["success"] = True
+                    if isinstance(data["return"], dict) and "data" in data["return"]:
+                        data["success"] = salt.utils.state.check_result(
+                            data["return"]["data"]
+                        )
             except (Exception, SystemExit) as ex:  # pylint: disable=broad-except
                 if isinstance(ex, salt.exceptions.NotImplemented):
                     data["return"] = str(ex)
                 else:
                     data["return"] = "Exception occurred in {} {}: {}".format(
                         self.client,
                         fun,
                         traceback.format_exc(),
                     )
                 data["success"] = False

--- a/salt/client/ssh/__init__.py
+++ b/salt/client/ssh/__init__.py
@@ -370,35 +370,38 @@
         with salt.utils.files.fopen(pub, "r") as fp_:
             return f"{fp_.read().split()[1]} rsa root@master"
     def key_deploy(self, host, ret):
         """
         Deploy the SSH key if the minions don't auth
         """
         if not isinstance(ret[host], dict) or self.opts.get("ssh_key_deploy"):
             target = self.targets[host]
             if target.get("passwd", False) or self.opts["ssh_passwd"]:
                 self._key_deploy_run(host, target, False)
-            return ret, None
-        if "_error" in ret[host] and ret[host]["_error"] == "Permission denied":
+            return ret
+        stderr = ret[host].get("stderr", "")
+        ignore_err = ["failed to upload file"]
+        check_err = [x for x in ignore_err if stderr.count(x)]
+        if "Permission denied" in stderr and not check_err:
             target = self.targets[host]
             print(
                 "Permission denied for host {}, do you want to deploy "
                 "the salt-ssh key? (password required):".format(host)
             )
             deploy = input("[Y/n] ")
             if deploy.startswith(("n", "N")):
                 return ret
             target["passwd"] = getpass.getpass(
                 "Password for {}@{}: ".format(target["user"], host)
             )
             return self._key_deploy_run(host, target, True)
-        return ret, None
+        return ret
     def _key_deploy_run(self, host, target, re_run=True):
         """
         The ssh-copy-id routine
         """
         argv = [
             "ssh.set_auth_key",
             target.get("user", "root"),
             self.get_pubkey(),
         ]
         single = Single(
@@ -420,118 +423,89 @@
                 self.opts,
                 self.opts["argv"],
                 host,
                 mods=self.mods,
                 fsclient=self.fsclient,
                 thin=self.thin,
                 **target,
             )
             stdout, stderr, retcode = single.cmd_block()
             try:
-                retcode = int(retcode)
-            except (TypeError, ValueError):
-                log.warning(f"Got an invalid retcode for host '{host}': '{retcode}'")
-                retcode = 1
-            try:
-                ret = (
-                    salt.client.ssh.wrapper.parse_ret(stdout, stderr, retcode),
-                    salt.defaults.exitcodes.EX_OK,
-                )
-            except (
-                salt.client.ssh.wrapper.SSHPermissionDeniedError,
-                salt.client.ssh.wrapper.SSHCommandExecutionError,
-            ) as err:
-                ret = err.to_ret()
-                retcode = max(retcode, err.retcode, 1)
-            except salt.client.ssh.wrapper.SSHException as err:
-                ret = err.to_ret()
-                if not self.opts.get("raw_shell"):
-                    retcode = max(retcode, err.retcode, 1)
-                else:
-                    ret.pop("_error", None)
-            except Exception as err:  # pylint: disable=broad-except
-                log.error(
-                    f"Error while parsing the command output: {err}",
-                    exc_info_on_loglevel=logging.DEBUG,
-                )
-                ret = {
-                    "_error": f"Internal error while parsing the command output: {err}",
-                    "stdout": stdout,
-                    "stderr": stderr,
-                    "retcode": retcode,
-                    "data": None,
-                }
-                retcode = max(retcode, 1)
-            return {host: ret}, retcode
-        if retcode != salt.defaults.exitcodes.EX_OK:
-            return {host: stderr}, retcode
-        return {host: stdout}, retcode
+                data = salt.utils.json.find_json(stdout)
+                return {host: data.get("local", data)}
+            except Exception:  # pylint: disable=broad-except
+                if stderr:
+                    return {host: stderr}
+                return {host: "Bad Return"}
+        if salt.defaults.exitcodes.EX_OK != retcode:
+            return {host: stderr}
+        return {host: stdout}
     def handle_routine(self, que, opts, host, target, mine=False):
         """
         Run the routine in a "Thread", put a dict on the queue
         """
         opts = copy.deepcopy(opts)
         single = Single(
             opts,
             opts["argv"],
             host,
             mods=self.mods,
             fsclient=self.fsclient,
             thin=self.thin,
             mine=mine,
             **target,
         )
         ret = {"id": single.id}
-        stdout = stderr = ""
-        retcode = salt.defaults.exitcodes.EX_OK
+        stdout, stderr, retcode = single.run()
         try:
-            stdout, stderr, retcode = single.run()
-            try:
-                retcode = int(retcode)
-            except (TypeError, ValueError):
-                log.warning(f"Got an invalid retcode for host '{host}': '{retcode}'")
-                retcode = 1
-            ret["ret"] = salt.client.ssh.wrapper.parse_ret(stdout, stderr, retcode)
-        except (
-            salt.client.ssh.wrapper.SSHPermissionDeniedError,
-            salt.client.ssh.wrapper.SSHCommandExecutionError,
-        ) as err:
-            ret["ret"] = err.to_ret()
-            retcode = max(retcode, err.retcode, 1)
-        except salt.client.ssh.wrapper.SSHException as err:
-            ret["ret"] = err.to_ret()
-            if not self.opts.get("raw_shell"):
-                retcode = max(retcode, err.retcode, 1)
+            retcode = int(retcode)
+        except (TypeError, ValueError):
+            log.warning("Got an invalid retcode for host '%s': '%s'", host, retcode)
+            retcode = 1
+        try:
+            data = salt.utils.json.find_json(stdout)
+            if len(data) < 2 and "local" in data:
+                ret["ret"] = data["local"]
+                try:
+                    remote_retcode = data["local"]["retcode"]
+                    try:
+                        retcode = int(remote_retcode)
+                    except (TypeError, ValueError):
+                        log.warning(
+                            "Host '%s' reported an invalid retcode: '%s'",
+                            host,
+                            remote_retcode,
+                        )
+                        retcode = max(retcode, 1)
+                except (KeyError, TypeError):
+                    pass
             else:
-                ret["ret"].pop("_error", None)
-        except Exception as err:  # pylint: disable=broad-except
-            log.error(
-                f"Error while parsing the command output: {err}",
-                exc_info_on_loglevel=logging.DEBUG,
-            )
+                ret["ret"] = {
+                    "stdout": stdout,
+                    "stderr": stderr,
+                    "retcode": retcode,
+                }
+        except Exception:  # pylint: disable=broad-except
             ret["ret"] = {
-                "_error": f"Internal error while parsing the command output: {err}",
                 "stdout": stdout,
                 "stderr": stderr,
                 "retcode": retcode,
-                "data": None,
             }
-            retcode = max(retcode, 1)
         que.put((ret, retcode))
     def handle_ssh(self, mine=False):
         """
         Spin up the needed threads or processes and execute the subsequent
         routines
         """
         que = multiprocessing.Queue()
         running = {}
-        target_iter = self.targets.__iter__()
+        target_iter = iter(self.targets)
         returned = set()
         rets = set()
         init = False
         while True:
             if not self.targets:
                 log.error("No matching targets found in roster.")
                 break
             if len(running) < self.opts.get("ssh_max_procs", 25) and not init:
                 try:
                     host = next(target_iter)
@@ -566,21 +540,21 @@
                     self.opts,
                     host,
                     self.targets[host],
                     mine,
                 )
                 routine = Process(target=self.handle_routine, args=args)
                 routine.start()
                 running[host] = {"thread": routine}
                 continue
             ret = {}
-            retcode = salt.defaults.exitcodes.EX_OK
+            retcode = 0
             try:
                 ret, retcode = que.get(False)
                 if "id" in ret:
                     returned.add(ret["id"])
                     yield {ret["id"]: ret["ret"]}, retcode
             except queue.Empty:
                 pass
             for host in running:
                 if not running[host]["thread"].is_alive():
                     if host not in returned:
@@ -637,45 +611,34 @@
             "arg": args,
         }
         if self.opts["master_job_cache"] == "local_cache":
             self.returners["{}.save_load".format(self.opts["master_job_cache"])](
                 jid, job_load, minions=self.targets.keys()
             )
         else:
             self.returners["{}.save_load".format(self.opts["master_job_cache"])](
                 jid, job_load
             )
-        for ret, retcode in self.handle_ssh(mine=mine):
+        for ret, _ in self.handle_ssh(mine=mine):
             host = next(iter(ret))
             self.cache_job(jid, host, ret[host], fun)
             if self.event:
                 id_, data = next(iter(ret.items()))
-                if not isinstance(data, dict):
+                if isinstance(data, str):
                     data = {"return": data}
                 if "id" not in data:
                     data["id"] = id_
                 if "fun" not in data:
                     data["fun"] = fun
-                if "fun_args" not in data:
-                    data["fun_args"] = args
-                if "retcode" not in data:
-                    data["retcode"] = retcode
-                if "success" not in data:
-                    data["success"] = data["retcode"] == salt.defaults.exitcodes.EX_OK
-                if "return" not in data:
-                    if data["success"]:
-                        data["return"] = data.get("stdout")
-                    else:
-                        data["return"] = data.get("stderr", data.get("stdout"))
-                data[
-                    "jid"
-                ] = jid  # make the jid in the payload the same as the jid in the tag
+                data["jid"] = (
+                    jid  # make the jid in the payload the same as the jid in the tag
+                )
                 self.event.fire_event(
                     data, salt.utils.event.tagify([jid, "ret", host], "job")
                 )
             yield ret
     def cache_job(self, jid, id_, ret, fun):
         """
         Cache the job information
         """
         self.returners["{}.returner".format(self.opts["master_job_cache"])](
             {"jid": jid, "id": id_, "return": ret, "fun": fun}
@@ -731,78 +694,55 @@
                 exc,
                 exc_info=True,
             )
         if self.opts.get("verbose"):
             msg = f"Executing job with jid {jid}"
             print(msg)
             print("-" * len(msg) + "\n")
             print("")
         sret = {}
         outputter = self.opts.get("output", "nested")
-        final_exit = salt.defaults.exitcodes.EX_OK
+        final_exit = 0
         for ret, retcode in self.handle_ssh():
             host = next(iter(ret))
             if not isinstance(retcode, int):
-                log.warning(f"Host '{host}' returned an invalid retcode: {retcode}")
+                log.warning("Host '%s' returned an invalid retcode: %s", host, retcode)
                 retcode = 1
             final_exit = max(final_exit, retcode)
             self.cache_job(jid, host, ret[host], fun)
-            ret, deploy_retcode = self.key_deploy(host, ret)
-            if deploy_retcode is not None:
-                try:
-                    retcode = int(deploy_retcode)
-                except (TypeError, ValueError):
-                    log.warning(
-                        f"Got an invalid deploy retcode for host '{host}': '{retcode}'"
-                    )
-                    retcode = 1
-            final_exit = max(final_exit, retcode)
+            ret = self.key_deploy(host, ret)
             if isinstance(ret[host], dict) and (
                 ret[host].get("stderr") or ""
             ).startswith("ssh:"):
                 ret[host] = ret[host]["stderr"]
             if not isinstance(ret[host], dict):
                 p_data = {host: ret[host]}
             elif "return" not in ret[host]:
-                if ret[host].get("_error") == "Permission denied":
-                    p_data = {host: ret[host]["stderr"]}
-                else:
-                    p_data = ret
+                p_data = ret
             else:
                 outputter = ret[host].get("out", self.opts.get("output", "nested"))
                 p_data = {host: ret[host].get("return", {})}
             if self.opts.get("static"):
                 sret.update(p_data)
             else:
                 salt.output.display_output(p_data, outputter, self.opts)
             if self.event:
                 id_, data = next(iter(ret.items()))
-                if not isinstance(data, dict):
+                if isinstance(data, str):
                     data = {"return": data}
                 if "id" not in data:
                     data["id"] = id_
                 if "fun" not in data:
                     data["fun"] = fun
-                if "fun_args" not in data:
-                    data["fun_args"] = args
-                if "retcode" not in data:
-                    data["retcode"] = retcode
-                if "success" not in data:
-                    data["success"] = data["retcode"] == salt.defaults.exitcodes.EX_OK
-                if "return" not in data:
-                    if data["success"]:
-                        data["return"] = data.get("stdout")
-                    else:
-                        data["return"] = data.get("stderr", data.get("stdout"))
-                data[
-                    "jid"
-                ] = jid  # make the jid in the payload the same as the jid in the tag
+                data["jid"] = (
+                    jid  # make the jid in the payload the same as the jid in the tag
+                )
                 self.event.fire_event(
                     data, salt.utils.event.tagify([jid, "ret", host], "job")
                 )
         if self.event is not None:
             self.event.destroy()
         if self.opts.get("static"):
             salt.output.display_output(sret, outputter, self.opts)
         if final_exit:
             sys.exit(salt.defaults.exitcodes.EX_AGGREGATE)
 class Single:
@@ -938,21 +878,21 @@
         if self.winrm:
             return arg
         return "".join(["\\" + char if re.match(r"\W", char) else char for char in arg])
     def run_ssh_pre_flight(self):
         """
         Run our pre_flight script before running any ssh commands
         """
         with tempfile.NamedTemporaryFile() as temp:
             try:
                 shutil.copyfile(self.ssh_pre_flight, temp.name)
-            except OSError:
+            except OSError as err:
                 return (
                     "",
                     "Could not copy pre flight script to temporary path",
                     1,
                 )
             target_script = f".{pathlib.Path(temp.name).name}"
             log.trace("Copying the pre flight script to target")
             stdout, stderr, retcode = self.shell.send(temp.name, target_script)
             if retcode != 0:
                 log.error("Could not copy the pre flight script to target")
@@ -992,35 +932,34 @@
     def run(self, deploy_attempted=False):
         """
         Execute the routine, the routine can be either:
         1. Execute a raw shell command
         2. Execute a wrapper func
         3. Execute a remote Salt command
         If a (re)deploy is needed, then retry the operation after a deploy
         attempt
         Returns tuple of (stdout, stderr, retcode)
         """
-        stdout = stderr = ""
-        retcode = salt.defaults.exitcodes.EX_OK
+        stdout = stderr = retcode = None
         if self.ssh_pre_flight:
             if not self.opts.get("ssh_run_pre_flight", False) and self.check_thin_dir():
                 log.info(
                     "%s thin dir already exists. Not running ssh_pre_flight script",
                     self.thin_dir,
                 )
             elif not os.path.exists(self.ssh_pre_flight):
                 log.error(
                     "The ssh_pre_flight script %s does not exist", self.ssh_pre_flight
                 )
             else:
                 stdout, stderr, retcode = self.run_ssh_pre_flight()
-                if retcode != salt.defaults.exitcodes.EX_OK:
+                if retcode != 0:
                     log.error(
                         "Error running ssh_pre_flight script %s", self.ssh_pre_file
                     )
                     return stdout, stderr, retcode
                 log.info(
                     "Successfully ran the ssh_pre_flight script: %s", self.ssh_pre_file
                 )
         if self.opts.get("raw_shell", False):
             cmd_str = " ".join([self._escape_arg(arg) for arg in self.argv])
             stdout, stderr, retcode = self.shell.exec_cmd(cmd_str)
@@ -1056,38 +995,42 @@
             refresh = True
         if refresh:
             pre_wrapper = salt.client.ssh.wrapper.FunctionWrapper(
                 self.opts,
                 self.id,
                 fsclient=self.fsclient,
                 minion_opts=self.minion_opts,
                 **self.target,
             )
             opts_pkg = pre_wrapper["test.opts_pkg"]()  # pylint: disable=E1102
+            if "_error" in opts_pkg:
+                retcode = opts_pkg["retcode"]
+                ret = salt.utils.json.dumps({"local": opts_pkg})
+                return ret, retcode
             opts_pkg["file_roots"] = self.opts["file_roots"]
             opts_pkg["pillar_roots"] = self.opts["pillar_roots"]
             opts_pkg["ext_pillar"] = self.opts["ext_pillar"]
             opts_pkg["extension_modules"] = self.opts["extension_modules"]
             opts_pkg["module_dirs"] = self.opts["module_dirs"]
             opts_pkg["_ssh_version"] = self.opts["_ssh_version"]
             opts_pkg["thin_dir"] = self.opts["thin_dir"]
             opts_pkg["master_tops"] = self.opts["master_tops"]
             opts_pkg["extra_filerefs"] = self.opts.get("extra_filerefs", "")
             opts_pkg["__master_opts__"] = self.context["master_opts"]
             if "known_hosts_file" in self.opts:
                 opts_pkg["known_hosts_file"] = self.opts["known_hosts_file"]
             if "_caller_cachedir" in self.opts:
                 opts_pkg["_caller_cachedir"] = self.opts["_caller_cachedir"]
             else:
                 opts_pkg["_caller_cachedir"] = self.opts["cachedir"]
             opts_pkg["id"] = self.id
-            retcode = salt.defaults.exitcodes.EX_OK
+            retcode = 0
             for grain in conf_grains:
                 opts_pkg["grains"][grain] = conf_grains[grain]
             if "grains" in self.target:
                 for grain in self.target["grains"]:
                     opts_pkg["grains"][grain] = self.target["grains"][grain]
             popts = {}
             popts.update(opts_pkg)
             popts.update(opts_pkg["__master_opts__"])
             pillar = salt.pillar.Pillar(
                 popts,
@@ -1153,45 +1096,33 @@
                 self.args = []
                 self.kwargs = mine_args
             elif isinstance(mine_args, list):
                 self.args = mine_args
                 self.kwargs = {}
         try:
             if self.mine:
                 result = wrapper[mine_fun](*self.args, **self.kwargs)
             else:
                 result = self.wfuncs[self.fun](*self.args, **self.kwargs)
-        except salt.client.ssh.wrapper.SSHException:
-            raise
         except TypeError as exc:
-            result = {"local": f"TypeError encountered executing {self.fun}: {exc}"}
+            result = f"TypeError encountered executing {self.fun}: {exc}"
             log.error(result, exc_info_on_loglevel=logging.DEBUG)
             retcode = 1
         except Exception as exc:  # pylint: disable=broad-except
-            result = {
-                "local": f"An Exception occurred while executing {self.fun}: {exc}"
-            }
+            result = "An Exception occurred while executing {}: {}".format(
+                self.fun, exc
+            )
             log.error(result, exc_info_on_loglevel=logging.DEBUG)
             retcode = 1
-        try:
-            retcode = max(
-                retcode, self.context.get("retcode", salt.defaults.exitcodes.EX_OK)
-            )
-        except (TypeError, ValueError):
-            log.warning(
-                f"Wrapper module set invalid value for retcode: '{self.context['retcode']}"
-            )
-            retcode = max(retcode, 1)
+        retcode = max(retcode, self.context.get("retcode", 0))
         if isinstance(result, dict) and "local" in result:
             ret = salt.utils.json.dumps({"local": result["local"]})
-        elif self.context.get("retcode"):
-            ret = salt.utils.json.dumps({"local": result})
         else:
             ret = salt.utils.json.dumps({"local": {"return": result}})
         return ret, retcode
     def _cmd_str(self):
         """
         Prepare the command string
         """
         if self.target.get("sudo"):
             sudo = (
                 f"sudo -p '{salt.client.ssh.shell.SUDO_PROMPT}'"
@@ -1282,21 +1213,21 @@
         execute it there
         """
         if not self.tty and not self.winrm:
             return self.shell.exec_cmd(cmd_str)
         with tempfile.NamedTemporaryFile(mode="w+b", delete=False) as shim_tmp_file:
             shim_tmp_file.write(salt.utils.stringutils.to_bytes(cmd_str))
         target_shim_file = f".{pathlib.Path(shim_tmp_file.name).name}"
         if self.winrm:
             target_shim_file = saltwinshell.get_target_shim_file(self, target_shim_file)
         stdout, stderr, retcode = self.shell.send(
-            shim_tmp_file.name, target_shim_file, makedirs=self.winrm
+            shim_tmp_file.name, target_shim_file, makedirs=True
         )
         if retcode != 0:
             log.error("Could not copy the shim script to target")
             return stdout, stderr, retcode
         try:
             os.remove(shim_tmp_file.name)
         except OSError:
             pass
         ret = self.execute_script(script=target_shim_file, extension=extension)
         return ret
@@ -1521,21 +1452,20 @@
 def mod_data(fsclient):
     """
     Generate the module arguments for the shim data
     """
     sync_refs = [
         "modules",
         "states",
         "grains",
         "renderers",
         "returners",
-        "utils",
     ]
     ret = {}
     with fsclient:
         envs = fsclient.envs()
         ver_base = ""
         for env in envs:
             files = fsclient.file_list(env)
             for ref in sync_refs:
                 mods_data = {}
                 pref = f"_{ref}"
@@ -1573,21 +1503,21 @@
         tfp.close()
         return mods
 def ssh_version():
     """
     Returns the version of the installed ssh command
     """
     ret = subprocess.Popen(
         ["ssh", "-V"], stdout=subprocess.PIPE, stderr=subprocess.PIPE
     ).communicate()
     try:
-        version_parts = ret[1].split(b",")[0].split(b"_")[1]
+        version_parts = ret[1].split(b",", maxsplit=1)[0].split(b"_")[1]
         parts = []
         for part in version_parts:
             try:
                 parts.append(int(part))
             except ValueError:
                 return tuple(parts)
         return tuple(parts)
     except IndexError:
         return (2, 0)
 def _convert_args(args):

--- a/salt/client/ssh/shell.py
+++ b/salt/client/ssh/shell.py
@@ -4,37 +4,41 @@
 import logging
 import os
 import re
 import shlex
 import subprocess
 import sys
 import time
 import salt.defaults.exitcodes
 import salt.utils.json
 import salt.utils.nb_popen
+import salt.utils.path
 import salt.utils.vt
 log = logging.getLogger(__name__)
 SSH_PASSWORD_PROMPT_RE = re.compile(r"(?:.*)[Pp]assword(?: for .*)?:\s*$", re.M)
 KEY_VALID_RE = re.compile(r".*\(yes\/no\).*")
 SSH_PRIVATE_KEY_PASSWORD_PROMPT_RE = re.compile(r"Enter passphrase for key", re.M)
 SUDO_PROMPT = "[salt:sudo:d11bd4221135c33324a6bdc09674146fbfdf519989847491e34a689369bbce23]passwd:"
 SUDO_PROMPT_RE = re.compile(
     r"\[salt:sudo:d11bd4221135c33324a6bdc09674146fbfdf519989847491e34a689369bbce23\]passwd:",
     re.M,
 )
 RSTR = "_edbc7885e4f9aac9b83b35999b68d015148caf467b78fa39c05f669c0ff89878"
 RSTR_RE = re.compile(r"(?:^|\r?\n)" + RSTR + r"(?:\r?\n|$)")
+SSH_KEYGEN_PATH = salt.utils.path.which("ssh-keygen") or "ssh-keygen"
+SSH_PATH = salt.utils.path.which("ssh") or "ssh"
+SCP_PATH = salt.utils.path.which("scp") or "scp"
 def gen_key(path):
     """
     Generate a key for use with salt-ssh
     """
-    cmd = ["ssh-keygen", "-P", "", "-f", path, "-t", "rsa", "-q"]
+    cmd = [SSH_KEYGEN_PATH, "-P", "", "-f", path, "-t", "rsa", "-q"]
     dirname = os.path.dirname(path)
     if dirname and not os.path.isdir(dirname):
         os.makedirs(os.path.dirname(path))
     subprocess.call(cmd)
 def gen_shell(opts, **kwargs):
     """
     Return the correct shell interface for the target system
     """
     if kwargs["winrm"]:
         try:
@@ -200,32 +204,32 @@
             )
         return None
     def copy_id(self):
         """
         Execute ssh-copy-id to plant the id file on the target
         """
         stdout, stderr, retcode = self._run_cmd(self._copy_id_str_old())
         if salt.defaults.exitcodes.EX_OK != retcode and "Usage" in stderr:
             stdout, stderr, retcode = self._run_cmd(self._copy_id_str_new())
         return stdout, stderr, retcode
-    def _cmd_str(self, cmd, ssh="ssh"):
+    def _cmd_str(self, cmd, ssh=SSH_PATH):
         """
         Return the cmd string to execute
         """
         command = [ssh]
-        if ssh != "scp":
+        if ssh != SCP_PATH:
             command.append(self.host)
-        if self.tty and ssh == "ssh":
+        if self.tty and ssh == SSH_PATH:
             command.append("-t -t")
         if self.passwd or self.priv:
             command.append(self.priv and self._key_opts() or self._passwd_opts())
-        if ssh != "scp" and self.remote_port_forwards:
+        if ssh != SCP_PATH and self.remote_port_forwards:
             command.append(
                 " ".join(
                     [f"-R {item}" for item in self.remote_port_forwards.split(",")]
                 )
             )
         if self.ssh_options:
             command.append(self._ssh_opts())
         command.append(cmd)
         return " ".join(command)
     def _run_nb_cmd(self, cmd):
@@ -282,34 +286,26 @@
             log.trace(logmsg)
         else:
             log.debug(logmsg)
         ret = self._run_cmd(cmd)
         return ret
     def send(self, local, remote, makedirs=False):
         """
         scp a file or files to a remote system
         """
         if makedirs:
-            pardir = os.path.dirname(remote)
-            if not pardir:
-                log.warning(
-                    f"Makedirs called on relative filename: '{remote}'. Skipping."
-                )
-            else:
-                ret = self.exec_cmd("mkdir -p " + shlex.quote(pardir))
-                if ret[2]:
-                    return ret
+            self.exec_cmd(f"mkdir -p {os.path.dirname(remote)}")
         host = self.host
         if ":" in host:
             host = f"[{host}]"
         cmd = f"{local} {host}:{remote}"
-        cmd = self._cmd_str(cmd, ssh="scp")
+        cmd = self._cmd_str(cmd, ssh=SCP_PATH)
         logmsg = f"Executing command: {cmd}"
         if self.passwd:
             logmsg = logmsg.replace(self.passwd, ("*" * 6))
         log.debug(logmsg)
         return self._run_cmd(cmd)
     def _split_cmd(self, cmd):
         """
         Split a command string so that it is suitable to pass to Popen without
         shell=True. This prevents shell injection attacks in the options passed
         to ssh or some other command.

--- a/salt/client/ssh/state.py
+++ b/salt/client/ssh/state.py
@@ -92,34 +92,56 @@
         self.client = fsclient
         salt.state.BaseHighState.__init__(self, opts)
         self.state = SSHState(
             opts,
             pillar_override,
             wrapper,
             context=context,
             initial_pillar=initial_pillar,
         )
         self.matchers = salt.loader.matchers(self.opts)
+        self.tops = salt.loader.tops(self.opts)
         self._pydsl_all_decls = {}
         self._pydsl_render_stack = []
     def push_active(self):
         salt.state.HighState.stack.append(self)
     def load_dynamic(self, matches):
         """
         Stub out load_dynamic
         """
         return
     def _master_tops(self):
         """
         Evaluate master_tops locally
         """
-        return self._local_master_tops()
+        if "id" not in self.opts:
+            log.error("Received call for external nodes without an id")
+            return {}
+        if not salt.utils.verify.valid_id(self.opts, self.opts["id"]):
+            return {}
+        grains = {}
+        ret = {}
+        if "grains" in self.opts:
+            grains = self.opts["grains"]
+        for fun in self.tops:
+            if fun not in self.opts.get("master_tops", {}):
+                continue
+            try:
+                ret.update(self.tops[fun](opts=self.opts, grains=grains))
+            except Exception as exc:  # pylint: disable=broad-except
+                log.error(
+                    "Top function %s failed with error %s for minion %s",
+                    fun,
+                    exc,
+                    self.opts["id"],
+                )
+        return ret
     def destroy(self):
         if self.client:
             self.client.destroy()
     def __enter__(self):
         return self
     def __exit__(self, *_):
         self.destroy()
 def lowstate_file_refs(chunks, extras=""):
     """
     Create a list of file ref objects to reconcile

--- a/salt/client/ssh/wrapper/__init__.py
+++ b/salt/client/ssh/wrapper/__init__.py
@@ -1,100 +1,21 @@
 """
 The ssh client wrapper system contains the routines that are used to alter
 how executions are run in the salt-ssh system, this allows for state routines
 to be easily rewritten to execute in a way that makes them do the same tasks
 as ZeroMQ salt, but via ssh.
 """
 import copy
-import logging
 import salt.client.ssh
 import salt.loader
 import salt.utils.data
 import salt.utils.json
-from salt.defaults import NOT_SET
-from salt.exceptions import CommandExecutionError, SaltException
-log = logging.getLogger(__name__)
-class SSHException(SaltException):
-    """
-    Indicates general command failure via salt-ssh.
-    """
-    _error = ""
-    def __init__(
-        self, stdout, stderr, retcode, result=NOT_SET, parsed=None, *args, **kwargs
-    ):
-        super().__init__(stderr, *args, **kwargs)
-        self.stdout = stdout
-        self.stderr = self._filter_stderr(stderr)
-        self.result = result
-        self.parsed = parsed
-        self.retcode = retcode
-        if args:
-            self._error = args.pop(0)
-        super().__init__(self._error)
-    def _filter_stderr(self, stderr):
-        stderr_lines = []
-        skip_next = False
-        for line in stderr.splitlines():
-            if skip_next:
-                skip_next = False
-                continue
-            parts = line.split(":")
-            if len(parts) > 2 and "DeprecationWarning" in parts[2]:
-                skip_next = True
-                continue
-            stderr_lines.append(line)
-        return "\n".join(stderr_lines)
-    def to_ret(self):
-        ret = {
-            "stdout": self.stdout,
-            "stderr": self.stderr,
-            "retcode": self.retcode,
-            "parsed": self.parsed,
-        }
-        if self._error:
-            ret["_error"] = self._error
-        if self.result is not NOT_SET:
-            ret["return"] = self.result
-        return ret
-class SSHCommandExecutionError(SSHException, CommandExecutionError):
-    """
-    Thrown whenever a non-zero exit code is returned.
-    This was introduced to make the salt-ssh FunctionWrapper behave
-    more like the usual one, in particular to force template rendering
-    to stop when a function call results in an exception.
-    """
-    _error = "The command resulted in a non-zero exit code"
-    def to_ret(self):
-        if self.parsed and "local" in self.parsed:
-            return self.parsed["local"]
-        return super().to_ret()
-    def __str__(self):
-        ret = self.to_ret()
-        if self.retcode > 0:
-            return f"{self._error}: {self.stderr or self.stdout}"
-        return self._error
-class SSHPermissionDeniedError(SSHException):
-    """
-    Thrown when "Permission denied" is found in stderr
-    """
-    _error = "Permission denied"
-class SSHReturnDecodeError(SSHException):
-    """
-    Thrown when JSON-decoding stdout fails and the retcode is 0 otherwise
-    """
-    _error = "Failed to return clean data"
-class SSHMalformedReturnError(SSHException):
-    """
-    Thrown when a decoded return dict is not formed as
-    {"local": {"return": ...}}
-    """
-    _error = "Return dict was malformed"
 class FunctionWrapper:
     """
     Create an object that acts like the salt function dict and makes function
     calls remotely via the SSH shell system
     """
     def __init__(
         self,
         opts,
         id_,
         host,
@@ -172,91 +93,50 @@
             single = salt.client.ssh.Single(
                 self.opts,
                 argv,
                 mods=self.mods,
                 disable_wipe=True,
                 fsclient=self.fsclient,
                 minion_opts=self.minion_opts,
                 **self.kwargs,
             )
             stdout, stderr, retcode = single.cmd_block()
-            return parse_ret(stdout, stderr, retcode, result_only=True)
+            if stderr.count("Permission Denied"):
+                return {
+                    "_error": "Permission Denied",
+                    "stdout": stdout,
+                    "stderr": stderr,
+                    "retcode": retcode,
+                }
+            try:
+                ret = salt.utils.json.loads(stdout)
+                if len(ret) < 2 and "local" in ret:
+                    ret = ret["local"]
+                ret = ret.get("return", {})
+            except ValueError:
+                ret = {
+                    "_error": "Failed to return clean data",
+                    "stderr": stderr,
+                    "stdout": stdout,
+                    "retcode": retcode,
+                }
+            return ret
         return caller
     def __setitem__(self, cmd, value):
         """
         Set aliases for functions
         """
         if "." not in cmd and not self.cmd_prefix:
             raise KeyError(f"Cannot assign to module key {cmd} in the FunctionWrapper")
         if self.cmd_prefix:
             cmd = f"{self.cmd_prefix}.{cmd}"
         if cmd in self.wfuncs:
             self.wfuncs[cmd] = value
         self.aliases[cmd] = value
     def get(self, cmd, default):
         """
         Mirrors behavior of dict.get
         """
         if cmd in self:
             return self[cmd]
         else:
             return default
-def parse_ret(stdout, stderr, retcode, result_only=False):
-    """
-    Parse the output of a remote or local command and return its
-    result. Raise exceptions if the command has a non-zero exitcode
-    or its output is not valid JSON or is not in the expected format,
-    usually ``{"local": {"return": value}}`` (+ optional keys in the "local" dict).
-    """
-    try:
-        retcode = int(retcode)
-    except (TypeError, ValueError):
-        log.warning(f"Got an invalid retcode for host: '{retcode}'")
-        retcode = 1
-    if "Permission denied" in stderr:
-        ignore_err = ["failed to upload file"]
-        check_err = [x for x in ignore_err if stderr.count(x)]
-        if not check_err:
-            raise SSHPermissionDeniedError(
-                stdout=stdout, stderr=stderr, retcode=retcode
-            )
-    result = NOT_SET
-    error = None
-    data = None
-    try:
-        data = salt.utils.json.find_json(stdout)
-    except ValueError:
-        error = SSHReturnDecodeError
-    else:
-        if isinstance(data, dict) and len(data) < 2 and "local" in data:
-            result = data["local"]
-            try:
-                remote_retcode = result["retcode"]
-            except (KeyError, TypeError):
-                pass
-            else:
-                try:
-                    retcode = max(retcode, remote_retcode)
-                except (TypeError, ValueError):
-                    log.warning(f"Host reported an invalid retcode: '{remote_retcode}'")
-                    retcode = max(retcode, 1)
-            if not isinstance(result, dict):
-                error = SSHCommandExecutionError
-            elif result_only:
-                try:
-                    result = result["return"]
-                except KeyError:
-                    error = SSHMalformedReturnError
-                    result = NOT_SET
-        else:
-            error = SSHMalformedReturnError
-    if retcode:
-        error = SSHCommandExecutionError
-    if error is not None:
-        raise error(
-            stdout=stdout,
-            stderr=stderr,
-            retcode=retcode,
-            result=result,
-            parsed=data,
-        )
-    return result

--- a/salt/client/ssh/wrapper/cmdmod.py
+++ b//dev/null
@@ -1,418 +0,0 @@
-"""
-SSH wrapper module for the ``cmdmod`` execution module.
-.. note::
-    For consistency reasons, this wrapper currently does
-    not behave the same as the execution module regarding ``saltenv``.
-    The parameter defaults to ``base``, regardless of the current
-    value of the minion setting.
-    This is the same for the ``state`` and `cp`` wrappers.
-"""
-import logging
-import os.path
-import shlex
-import salt.utils.url
-from salt.exceptions import CommandExecutionError, SaltInvocationError
-from salt.modules.cmdmod import _python_shell_default
-log = logging.getLogger(__name__)
-__virtualname__ = "cmd"
-def __virtual__():
-    return __virtualname__
-def script(
-    source,
-    args=None,
-    cwd=None,
-    stdin=None,
-    runas=None,
-    group=None,
-    shell=None,
-    python_shell=None,
-    env=None,
-    template=None,
-    umask=None,
-    output_encoding=None,
-    output_loglevel="debug",
-    log_callback=None,
-    hide_output=False,
-    timeout=None,
-    reset_system_locale=True,
-    saltenv="base",
-    use_vt=False,
-    bg=False,
-    password=None,
-    success_retcodes=None,
-    success_stdout=None,
-    success_stderr=None,
-    **kwargs
-):
-    """
-    Download a script from a remote location and execute the script locally.
-    The script can be located on the salt master file server or on an HTTP/FTP
-    server.
-    The script will be executed directly, so it can be written in any available
-    programming language.
-    :param str source: The location of the script to download. If the file is
-        located on the master in the directory named spam, and is called eggs,
-        the source string is salt://spam/eggs
-    :param str args: String of command line args to pass to the script. Only
-        used if no args are specified as part of the `name` argument. To pass a
-        string containing spaces in YAML, you will need to doubly-quote it:
-        .. code-block:: bash
-            salt myminion cmd.script salt://foo.sh "arg1 'arg two' arg3"
-    :param str cwd: The directory from which to execute the command. Defaults
-        to the directory returned from Python's tempfile.mkstemp.
-    :param str stdin: A string of standard input can be specified for the
-        command to be run using the ``stdin`` parameter. This can be useful in
-        cases where sensitive information must be read from standard input.
-    :param str runas: Specify an alternate user to run the command. The default
-        behavior is to run as the user under which Salt is running. If running
-        on a Windows minion you must also use the ``password`` argument, and
-        the target user account must be in the Administrators group.
-        .. note::
-            For Window's users, specifically Server users, it may be necessary
-            to specify your runas user using the User Logon Name instead of the
-            legacy logon name. Traditionally, logons would be in the following
-            format.
-                ``Domain/user``
-            In the event this causes issues when executing scripts, use the UPN
-            format which looks like the following.
-                ``user@domain.local``
-            More information <https://github.com/saltstack/salt/issues/55080>
-    :param str password: Windows only. Required when specifying ``runas``. This
-        parameter will be ignored on non-Windows platforms.
-        .. versionadded:: 2016.3.0
-    :param str group: Group to run script as. Not currently supported
-      on Windows.
-    :param str shell: Specify an alternate shell. Defaults to the system's
-        default shell.
-    :param bool python_shell: If False, let python handle the positional
-        arguments. Set to True to use shell features, such as pipes or
-        redirection.
-    :param bool bg: If True, run script in background and do not await or
-        deliver its results
-    :param dict env: Environment variables to be set prior to execution.
-        .. note::
-            When passing environment variables on the CLI, they should be
-            passed as the string representation of a dictionary.
-            .. code-block:: bash
-                salt myminion cmd.script 'some command' env='{"FOO": "bar"}'
-        .. note::
-            When using environment variables on Window's, case-sensitivity
-            matters, i.e. Window's uses `Path` as opposed to `PATH` for other
-            systems.
-    :param str template: If this setting is applied then the named templating
-        engine will be used to render the downloaded file. Currently jinja,
-        mako, and wempy are supported.
-    :param str umask: The umask (in octal) to use when running the command.
-    :param str output_encoding: Control the encoding used to decode the
-        command's output.
-        .. note::
-            This should not need to be used in most cases. By default, Salt
-            will try to use the encoding detected from the system locale, and
-            will fall back to UTF-8 if this fails. This should only need to be
-            used in cases where the output of the command is encoded in
-            something other than the system locale or UTF-8.
-            To see the encoding Salt has detected from the system locale, check
-            the `locale` line in the output of :py:func:`test.versions_report
-            <salt.modules.test.versions_report>`.
-        .. versionadded:: 2018.3.0
-    :param str output_loglevel: Control the loglevel at which the output from
-        the command is logged to the minion log.
-        .. note::
-            The command being run will still be logged at the ``debug``
-            loglevel regardless, unless ``quiet`` is used for this value.
-    :param bool ignore_retcode: If the exit code of the command is nonzero,
-        this is treated as an error condition, and the output from the command
-        will be logged to the minion log. However, there are some cases where
-        programs use the return code for signaling and a nonzero exit code
-        doesn't necessarily mean failure. Pass this argument as ``True`` to
-        skip logging the output if the command has a nonzero exit code.
-    :param bool hide_output: If ``True``, suppress stdout and stderr in the
-        return data.
-        .. note::
-            This is separate from ``output_loglevel``, which only handles how
-            Salt logs to the minion log.
-        .. versionadded:: 2018.3.0
-    :param int timeout: If the command has not terminated after timeout
-        seconds, send the subprocess sigterm, and if sigterm is ignored, follow
-        up with sigkill
-    :param bool use_vt: Not supported via salt-ssh.
-    :param list success_retcodes: This parameter will allow a list of
-        non-zero return codes that should be considered a success.  If the
-        return code returned from the run matches any in the provided list,
-        the return code will be overridden with zero.
-      .. versionadded:: 2019.2.0
-    :param list success_stdout: This parameter will allow a list of
-        strings that when found in standard out should be considered a success.
-        If stdout returned from the run matches any in the provided list,
-        the return code will be overridden with zero.
-      .. versionadded:: 3004
-    :param list success_stderr: This parameter will allow a list of
-        strings that when found in standard error should be considered a success.
-        If stderr returned from the run matches any in the provided list,
-        the return code will be overridden with zero.
-      .. versionadded:: 3004
-    :param bool stdin_raw_newlines: False
-        If ``True``, Salt will not automatically convert the characters ``\\n``
-        present in the ``stdin`` value to newlines.
-      .. versionadded:: 2019.2.0
-    CLI Example:
-    .. code-block:: bash
-        salt '*' cmd.script salt://scripts/runme.sh
-        salt '*' cmd.script salt://scripts/runme.sh 'arg1 arg2 "arg 3"'
-        salt '*' cmd.script salt://scripts/windows_task.ps1 args=' -Input c:\\tmp\\infile.txt' shell='powershell'
-    .. code-block:: bash
-        salt '*' cmd.script salt://scripts/runme.sh stdin='one\\ntwo\\nthree\\nfour\\nfive\\n'
-    """
-    def _cleanup_tempfile(path):
-        try:
-            __salt__["file.remove"](path)
-        except (SaltInvocationError, CommandExecutionError) as exc:
-            log.error(
-                "cmd.script: Unable to clean tempfile '%s': %s",
-                path,
-                exc,
-                exc_info_on_loglevel=logging.DEBUG,
-            )
-    if shell is None:
-        shell = __grains__.get("shell", "/bin/sh")
-    python_shell = _python_shell_default(python_shell, kwargs.get("__pub_jid", ""))
-    if "__env__" in kwargs:
-        kwargs.pop("__env__")
-    path = __salt__["temp.file"](
-        suffix=os.path.splitext(salt.utils.url.split_env(source)[0])[1], parent=cwd
-    )
-    try:
-        if template:
-            if "pillarenv" in kwargs or "pillar" in kwargs:
-                pillarenv = kwargs.get("pillarenv", __opts__.get("pillarenv"))
-                kwargs["pillar"] = _gather_pillar(pillarenv, kwargs.get("pillar"))
-            fn_ = __salt__["cp.get_template"](source, path, template, saltenv, **kwargs)
-            if not fn_:
-                _cleanup_tempfile(path)
-                return {
-                    "pid": 0,
-                    "retcode": 1,
-                    "stdout": "",
-                    "stderr": "",
-                    "cache_error": True,
-                }
-        else:
-            fn_ = __salt__["cp.cache_file"](source, saltenv)
-            if not fn_:
-                _cleanup_tempfile(path)
-                return {
-                    "pid": 0,
-                    "retcode": 1,
-                    "stdout": "",
-                    "stderr": "",
-                    "cache_error": True,
-                }
-            __salt__["file.copy"](fn_, path)
-        __salt__["file.set_mode"](path, "0500")
-        __salt__["file.chown"](path, runas, -1)
-        cmd_path = shlex.quote(path)
-        kwargs.pop("pillar", None)
-        return __salt__["cmd.run_all"](
-            cmd_path + " " + str(args) if args else cmd_path,
-            cwd=cwd,
-            stdin=stdin,
-            output_encoding=output_encoding,
-            output_loglevel=output_loglevel,
-            log_callback=log_callback,
-            runas=runas,
-            group=group,
-            shell=shell,
-            python_shell=python_shell,
-            env=env,
-            umask=umask,
-            timeout=timeout,
-            reset_system_locale=reset_system_locale,
-            saltenv=saltenv,
-            use_vt=False,
-            bg=bg,
-            password=password,
-            success_retcodes=success_retcodes,
-            success_stdout=success_stdout,
-            success_stderr=success_stderr,
-            hide_output=hide_output,
-            **kwargs
-        )
-    finally:
-        _cleanup_tempfile(path)
-def script_retcode(
-    source,
-    args=None,
-    cwd=None,
-    stdin=None,
-    runas=None,
-    group=None,
-    shell=None,
-    python_shell=None,
-    env=None,
-    template="jinja",
-    umask=None,
-    timeout=None,
-    reset_system_locale=True,
-    saltenv="base",
-    output_encoding=None,
-    output_loglevel="debug",
-    log_callback=None,
-    use_vt=False,
-    password=None,
-    success_retcodes=None,
-    success_stdout=None,
-    success_stderr=None,
-    **kwargs
-):
-    """
-    Download a script from a remote location and execute the script locally.
-    The script can be located on the salt master file server or on an HTTP/FTP
-    server.
-    The script will be executed directly, so it can be written in any available
-    programming language.
-    The script can also be formatted as a template, the default is jinja.
-    Only evaluate the script return code and do not block for terminal output
-    :param str source: The location of the script to download. If the file is
-        located on the master in the directory named spam, and is called eggs,
-        the source string is salt://spam/eggs
-    :param str args: String of command line args to pass to the script. Only
-        used if no args are specified as part of the `name` argument. To pass a
-        string containing spaces in YAML, you will need to doubly-quote it:
-        "arg1 'arg two' arg3"
-    :param str cwd: The directory from which to execute the command. Defaults
-        to the home directory of the user specified by ``runas`` (or the user
-        under which Salt is running if ``runas`` is not specified).
-    :param str stdin: A string of standard input can be specified for the
-        command to be run using the ``stdin`` parameter. This can be useful in
-        cases where sensitive information must be read from standard input.
-    :param str runas: Specify an alternate user to run the command. The default
-        behavior is to run as the user under which Salt is running. If running
-        on a Windows minion you must also use the ``password`` argument, and
-        the target user account must be in the Administrators group.
-    :param str password: Windows only. Required when specifying ``runas``. This
-        parameter will be ignored on non-Windows platforms.
-        .. versionadded:: 2016.3.0
-    :param str group: Group to run script as. Not currently supported
-      on Windows.
-    :param str shell: Specify an alternate shell. Defaults to the system's
-        default shell.
-    :param bool python_shell: If False, let python handle the positional
-        arguments. Set to True to use shell features, such as pipes or
-        redirection.
-    :param dict env: Environment variables to be set prior to execution.
-        .. note::
-            When passing environment variables on the CLI, they should be
-            passed as the string representation of a dictionary.
-            .. code-block:: bash
-                salt myminion cmd.script_retcode 'some command' env='{"FOO": "bar"}'
-        .. note::
-            When using environment variables on Window's, case-sensitivity
-            matters, i.e. Window's uses `Path` as opposed to `PATH` for other
-            systems.
-    :param str template: If this setting is applied then the named templating
-        engine will be used to render the downloaded file. Currently jinja,
-        mako, and wempy are supported.
-    :param str umask: The umask (in octal) to use when running the command.
-    :param str output_encoding: Control the encoding used to decode the
-        command's output.
-        .. note::
-            This should not need to be used in most cases. By default, Salt
-            will try to use the encoding detected from the system locale, and
-            will fall back to UTF-8 if this fails. This should only need to be
-            used in cases where the output of the command is encoded in
-            something other than the system locale or UTF-8.
-            To see the encoding Salt has detected from the system locale, check
-            the `locale` line in the output of :py:func:`test.versions_report
-            <salt.modules.test.versions_report>`.
-        .. versionadded:: 2018.3.0
-    :param str output_loglevel: Control the loglevel at which the output from
-        the command is logged to the minion log.
-        .. note::
-            The command being run will still be logged at the ``debug``
-            loglevel regardless, unless ``quiet`` is used for this value.
-    :param bool ignore_retcode: If the exit code of the command is nonzero,
-        this is treated as an error condition, and the output from the command
-        will be logged to the minion log. However, there are some cases where
-        programs use the return code for signaling and a nonzero exit code
-        doesn't necessarily mean failure. Pass this argument as ``True`` to
-        skip logging the output if the command has a nonzero exit code.
-    :param int timeout: If the command has not terminated after timeout
-        seconds, send the subprocess sigterm, and if sigterm is ignored, follow
-        up with sigkill
-    :param bool use_vt: Use VT utils (saltstack) to stream the command output
-        more interactively to the console and the logs. This is experimental.
-    :param list success_retcodes: This parameter will allow a list of
-        non-zero return codes that should be considered a success.  If the
-        return code returned from the run matches any in the provided list,
-        the return code will be overridden with zero.
-      .. versionadded:: 2019.2.0
-    :param list success_stdout: This parameter will allow a list of
-        strings that when found in standard out should be considered a success.
-        If stdout returned from the run matches any in the provided list,
-        the return code will be overridden with zero.
-      .. versionadded:: 3004
-    :param list success_stderr: This parameter will allow a list of
-        strings that when found in standard error should be considered a success.
-        If stderr returned from the run matches any in the provided list,
-        the return code will be overridden with zero.
-      .. versionadded:: 3004
-    :param bool stdin_raw_newlines: False
-        If ``True``, Salt will not automatically convert the characters ``\\n``
-        present in the ``stdin`` value to newlines.
-      .. versionadded:: 2019.2.0
-    CLI Example:
-    .. code-block:: bash
-        salt '*' cmd.script_retcode salt://scripts/runme.sh
-        salt '*' cmd.script_retcode salt://scripts/runme.sh 'arg1 arg2 "arg 3"'
-        salt '*' cmd.script_retcode salt://scripts/windows_task.ps1 args=' -Input c:\\tmp\\infile.txt' shell='powershell'
-    A string of standard input can be specified for the command to be run using
-    the ``stdin`` parameter. This can be useful in cases where sensitive
-    information must be read from standard input.
-    .. code-block:: bash
-        salt '*' cmd.script_retcode salt://scripts/runme.sh stdin='one\\ntwo\\nthree\\nfour\\nfive\\n'
-    """
-    if "__env__" in kwargs:
-        kwargs.pop("__env__")
-    return script(
-        source=source,
-        args=args,
-        cwd=cwd,
-        stdin=stdin,
-        runas=runas,
-        group=group,
-        shell=shell,
-        python_shell=python_shell,
-        env=env,
-        template=template,
-        umask=umask,
-        timeout=timeout,
-        reset_system_locale=reset_system_locale,
-        saltenv=saltenv,
-        output_encoding=output_encoding,
-        output_loglevel=output_loglevel,
-        log_callback=log_callback,
-        use_vt=use_vt,
-        password=password,
-        success_retcodes=success_retcodes,
-        success_stdout=success_stdout,
-        success_stderr=success_stderr,
-        **kwargs
-    )["retcode"]
-def _gather_pillar(pillarenv, pillar_override):
-    """
-    The opts used during pillar rendering should contain the master
-    opts in the root namespace. self.opts is the modified minion opts,
-    containing the original master opts in __master_opts__.
-    """
-    popts = {}
-    popts.update(__opts__)
-    popts.update(__opts__.get("__master_opts__", {}))
-    pillar = salt.pillar.get_pillar(
-        popts,
-        __grains__.value(),
-        __salt__.kwargs["id_"],
-        __opts__["saltenv"] or "base",
-        pillar_override=pillar_override,
-        pillarenv=pillarenv,
-    )
-    return pillar.compile_pillar()

--- a/salt/client/ssh/wrapper/cp.py
+++ b/salt/client/ssh/wrapper/cp.py
@@ -1,609 +1,94 @@
 """
-Wrap the ``cp`` module allowing for managed SSH file transfers.
-This module works by keeping a cachedir per SSH minion on the master.
-Requested files are first cached there and afterwards replicated to
-the minion using ``scp``. The returned paths will point to files in
-the remote cachedir. You can convert these paths to the local ones
-by calling ``cp.convert_cache_path``, which is a function unique
-to the wrapper.
-.. note::
-    This wrapper currently has several limitations:
-    * Replication will always be performed, even if the file exists
-      in the minion cache dir in the correct state (no hash checks).
-    * Even non-``salt://`` URIs will be fetched by the master node
-      first in order for other wrappers to be able to employ this
-      one for fetching remotes.
-    * When replicating directories, they are currently not sent as
-      a tar archive, but file per file, which is very inefficient.
-    * You cannot transfer files from the minion to the master-side
-      SSH minion cache, they will only be available on the remote.
-.. note::
-    For backwards-compatibility reasons, this wrapper currently does
-    not behave the same as the execution module regarding ``saltenv``.
-    The parameter defaults to ``base``, regardless of the current
-    value of the minion setting.
+Wrap the cp module allowing for managed ssh file transfers
 """
 import logging
 import os
-import shlex
-import urllib.parse
-from pathlib import Path
 import salt.client.ssh
-import salt.fileclient
 import salt.utils.files
 import salt.utils.stringutils
 import salt.utils.templates
 from salt.exceptions import CommandExecutionError
 log = logging.getLogger(__name__)
-def _client():
-    ckey = "_cp_shell"
-    if ckey not in __context__:
-        single = salt.client.ssh.Single(__opts__, "", **__salt__.kwargs)
-        __context__[ckey] = single.shell
-    return SSHCpClient(
-        __context__["fileclient"].opts, __context__[ckey], __salt__.kwargs["id_"]
-    )
-def get_file(path, dest, saltenv="base", makedirs=False, template=None, **kwargs):
+def get_file(path, dest, saltenv="base", makedirs=False, template=None, gzip=None):
     """
-    Send a file from the fileserver to the specified location.
-    CLI Example:
-    .. code-block:: bash
-        salt-ssh '*' cp.get_file salt://path/to/file /minion/dest
-    path
-        The path on the fileserver, like ``salt://foo/bar.conf``. It is possible
-        to specify the ``saltenv`` using the querystring syntax:
-        ``salt://foo/bar.conf?saltenv=config``
-    dest
-        The absolute path to transfer the file to on the minion. If empty,
-        the file will be cached in the minion's cache dir, under
-        ``files/<saltenv>/<path>``.
-    saltenv
-        Salt fileserver environment from which to retrieve the file.
-        Defaults to ``base``.
-    makedirs
-        Whether to create the parent directories for ``dest`` as needed.
-        Defaults to false.
-    template
-        If ``path`` and ``dest`` parameters should be interpreted as templates,
-        the name of the renderer to use.
-        Template rendering can be enabled on both ``path`` and
-        ``dest`` file paths like so:
-        .. code-block:: bash
-            salt-ssh '*' cp.get_file "salt://{{grains.os}}/vimrc" /etc/vimrc template=jinja
-    Additional keyword arguments are passed through to the renderer, otherwise discarded.
-    .. note::
-        It may be necessary to quote the URL when using the querystring method,
-        depending on the shell being used to run the command.
+    Send a file from the master to the location in specified
     .. note::
         gzip compression is not supported in the salt-ssh version of
-        ``cp.get_file``.
+        cp.get_file. The argument is only accepted for interface compatibility.
     """
-    gzip = kwargs.pop("gzip", None)
     if gzip is not None:
         log.warning("The gzip argument to cp.get_file in salt-ssh is unsupported")
-    (path, dest) = _render_filenames(path, dest, saltenv, template, **kwargs)
-    path, senv = salt.utils.url.split_env(path)
-    if senv:
-        saltenv = senv
-    if not hash_file(path, saltenv):
-        return ""
-    else:
-        with _client() as client:
-            ret = client.get_file(path, dest, makedirs, saltenv)
-            if not ret:
-                return ret
-            return client.target_map[ret]
-def envs():
+    if template is not None:
+        (path, dest) = _render_filenames(path, dest, saltenv, template)
+    src = __context__["fileclient"].cache_file(
+        path, saltenv, cachedir=os.path.join("salt-ssh", __salt__.kwargs["id_"])
+    )
+    single = salt.client.ssh.Single(__opts__, "", **__salt__.kwargs)
+    ret = single.shell.send(src, dest, makedirs)
+    return not ret[2]
+def get_dir(path, dest, saltenv="base"):
     """
-    List available fileserver environments.
-    CLI Example:
-    .. code-block:: bash
-        salt-ssh '*' cp.envs
+    Transfer a directory down
     """
-    return __context__["fileclient"].envs()
-def get_template(
-    path, dest, template="jinja", saltenv="base", makedirs=False, **kwargs
-):
+    src = __context__["fileclient"].cache_dir(
+        path, saltenv, cachedir=os.path.join("salt-ssh", __salt__.kwargs["id_"])
+    )
+    src = " ".join(src)
+    single = salt.client.ssh.Single(__opts__, "", **__salt__.kwargs)
+    ret = single.shell.send(src, dest)
+    return not ret[2]
+def get_url(path, dest, saltenv="base"):
     """
-    Render a file as a template before writing it.
-    CLI Example:
-    .. code-block:: bash
-        salt '*' cp.get_template salt://path/to/template /minion/dest
-    path
-        The path on the fileserver, like ``salt://foo/bar.conf``. It is possible
-        to specify the ``saltenv`` using the querystring syntax:
-        ``salt://foo/bar.conf?saltenv=config``
-    dest
-        The absolute path to transfer the file to on the minion. If empty,
-        the rendered template will be cached in the minion's cache dir,
-        under ``extrn_files/<saltenv>/<path>``.
-    template
-        The renderer to use for rendering the template. Defaults to ``jinja``.
-    saltenv
-        The saltenv the template should be pulled from. Defaults to ``base``.
-    makedirs
-        Whether to create the parent directories for ``dest`` as needed.
-        Defaults to false.
-    Additional keyword arguments are passed verbatim to the renderer.
+    retrieve a URL
     """
-    if "salt" not in kwargs:
-        kwargs["salt"] = __salt__.value()
-    if "pillar" not in kwargs:
-        kwargs["pillar"] = __pillar__.value()
-    if "grains" not in kwargs:
-        kwargs["grains"] = __grains__.value()
-    if "opts" not in kwargs:
-        kwargs["opts"] = __opts__
-    with _client() as client:
-        ret = client.get_template(path, dest, template, makedirs, saltenv, **kwargs)
-        if not ret:
-            return ret
-        return client.target_map[ret]
-def get_dir(path, dest, saltenv="base", template=None, **kwargs):
-    """
-    Recursively transfer a directory from the fileserver to the minion.
-    .. note::
-        This can take a long time since each file is transferred separately
-        currently.
-    CLI Example:
-    .. code-block:: bash
-        salt-ssh '*' cp.get_dir salt://path/to/dir/ /minion/dest
-    path
-        The path on the fileserver, like ``salt://foo/bar/``. It is possible
-        to specify the ``saltenv`` using the querystring syntax:
-        ``salt://foo/bar?saltenv=config``
-    dest
-        The absolute path to transfer the directory to on the minion. If empty,
-        the directory will be cached in the minion's cache dir, under
-        ``files/<saltenv>/<path>``. Note that parent directories will
-        be created as required automatically.
-    saltenv
-        Salt fileserver environment from which to retrieve the directory.
-        Defaults to ``base``.
-    template
-        If ``path`` and ``dest`` parameters should be interpreted as templates,
-        the name of the renderer to use.
-    .. note::
-        gzip compression is not supported in the salt-ssh version of
-        cp.get_dir. The argument is only accepted for interface compatibility.
-    """
-    gzip = kwargs.pop("gzip", None)
-    if gzip is not None:
-        log.warning("The gzip argument to cp.get_dir in salt-ssh is unsupported")
-    (path, dest) = _render_filenames(path, dest, saltenv, template, **kwargs)
-    with _client() as client:
-        ret = client.get_dir(path, dest, saltenv, gzip)
-        if not ret:
-            return ret
-        return [client.target_map[x] for x in ret]
-def get_url(path, dest="", saltenv="base", makedirs=False, source_hash=None):
-    """
-    Retrieve a single file from a URL.
-    path
-        A URL to download a file from. Supported URL schemes are: ``salt://``,
-        ``http://``, ``https://``, ``ftp://``, ``s3://``, ``swift://`` and
-        ``file://`` (local filesystem). If no scheme was specified, this is
-        equivalent of using ``file://``.
-        If a ``file://`` URL is given, the function just returns absolute path
-        to that file on a local filesystem.
-        The function returns ``False`` if Salt was unable to fetch a file from
-        a ``salt://`` URL.
-        .. note::
-            The file:// scheme is currently only partially supported in salt-ssh.
-            It behaves the same as the unwrapped ``cp.get_url`` if dest is not
-            ``None``, but returning its contents will fail. Use ``get_file_str``
-            as a workaround for text files.
-    dest
-        The destination to write the cached file to. If empty, will cache the file
-        in the minion's cache dir under ``extrn_files/<saltenv>/<hostname>/<path>``.
-        Defaults to empty (i.e. caching the file).
-        .. note::
-            To simply return the file contents instead, set destination to
-            ``None``. This works with ``salt://``, ``http://`` and ``https://``
-            URLs. The files fetched by ``http://`` and ``https://`` will not
-            be cached.
-    saltenv
-        Salt fileserver environment from which to retrieve the file. Ignored if
-        ``path`` is not a ``salt://`` URL. Defaults to ``base``.
-    makedirs
-        Whether to create the parent directories for ``dest`` as needed.
-        Defaults to false.
-    source_hash
-        If ``path`` is an http(s) or ftp URL and the file exists in the
-        minion's file cache, this option can be passed to keep the minion from
-        re-downloading the file if the cached copy matches the specified hash.
-    """
-    with _client() as client:
-        if isinstance(dest, str):
-            result = client.get_url(
-                path, dest, makedirs, saltenv, source_hash=source_hash
-            )
-        else:
-            result = client.get_url(
-                path, None, makedirs, saltenv, no_cache=True, source_hash=source_hash
-            )
-        if not result:
-            log.error(
-                "Unable to fetch file %s from saltenv %s.",
-                salt.utils.url.redact_http_basic_auth(path),
-                saltenv,
-            )
-            return result
-        if isinstance(dest, str):
-            result = client.target_map[result]
-        return salt.utils.stringutils.to_unicode(result)
-def get_file_str(path, saltenv="base"):
-    """
-    Download a file from a URL to the Minion cache directory and return the
-    contents of that file.
-    Returns ``False`` if Salt was unable to cache a file from a URL.
-    CLI Example:
-    .. code-block:: bash
-        salt-ssh '*' cp.get_file_str salt://my/file
-    path
-        The path on the fileserver, like ``salt://foo/bar/``. It is possible
-        to specify the ``saltenv`` using the querystring syntax:
-        ``salt://foo/bar?saltenv=config``
-    saltenv
-        Salt fileserver environment from which to retrieve the file.
-    """
-    fn_ = cache_file(path, saltenv)
-    if isinstance(fn_, str):
-        try:
-            with salt.utils.files.fopen(fn_, "r") as fp_:
-                return salt.utils.stringutils.to_unicode(fp_.read())
-        except OSError:
-            return False
-    return fn_
-def cache_file(path, saltenv="base", source_hash=None, verify_ssl=True, use_etag=False):
-    """
-    Cache a single file on the Minion.
-    Returns the location of the new cached file on the Minion.
-    If the path being cached is a ``salt://`` URI, and the path does not exist,
-    then ``False`` will be returned.
-    If the path refers to a fileserver path (``salt://`` URI) and this is a state run,
-    the file will also be added to the package of files that's sent to the minion
-    for executing the state run (this behaves like ``extra_filerefs``).
-    CLI Example:
-    .. code-block:: bash
-        salt-ssh '*' cp.cache_file salt://path/to/file
-    path
-        The path on the fileserver, like ``salt://foo/bar/``. It is possible
-        to specify the ``saltenv`` using the querystring syntax:
-        ``salt://foo/bar?saltenv=config``
-    saltenv
-        Salt fileserver environment from which to retrieve the file. Ignored if
-        ``path`` is not a ``salt://`` URL. Defaults to ``base``.
-    source_hash
-        If ``name`` is an http(s) or ftp URL and the file exists in the
-        minion's file cache, this option can be passed to keep the minion from
-        re-downloading the file if the cached copy matches the specified hash.
-        .. versionadded:: 2018.3.0
-    verify_ssl
-        If ``False``, remote https file sources (``https://``) and source_hash
-        will not attempt to validate the servers certificate. Default is True.
-        .. versionadded:: 3002
-    use_etag
-        If ``True``, remote http/https file sources will attempt to use the
-        ETag header to determine if the remote file needs to be downloaded.
-        This provides a lightweight mechanism for promptly refreshing files
-        changed on a web server without requiring a full hash comparison via
-        the ``source_hash`` parameter.
-        .. versionadded:: 3005
-    .. note::
-        You can instrumentalize this function in your ``sls`` files to workaround a
-        limitation in how ``salt-ssh`` handles Jinja imports:
-        Imports in templates that will be rendered on the minion (usually during
-        ``file.managed`` calls) will fail since the corresponding file is not
-        sent to the minion by default.
-        By caching it explicitly in your states, you can ensure it will be included
-        in the filerefs that will be sent to the minion.
-        .. code-block:: jinja
-            {%- do salt["cp.cache_file"]("salt://my/map.jinja") %}
-            Serialize config:
-              file.managed:
-                - name: /etc/my/config.conf
-                - source: salt://my/files/config.conf.j2
-                - template: jinja
-            {%- from "my/map.jinja" import mapdata with context %}
-            myconf = {{ mapdata["foo"] }}
-            {%- set mapdata = {"foo": "bar"} %}
-    """
-    path = salt.utils.data.decode(path)
-    saltenv = salt.utils.data.decode(saltenv)
-    url_data = urllib.parse.urlparse(path)
-    if url_data.scheme in ("file", ""):
-        return __salt__["cp.cache_file_ssh"](
-            path,
-            saltenv=saltenv,
-            source_hash=source_hash,
-            verify_ssl=verify_ssl,
-            use_etag=use_etag,
-        )
-    contextkey = "{}_|-{}_|-{}".format("cp.cache_file", path, saltenv)
-    filerefs_ckey = "_cp_extra_filerefs"
-    url_data = urllib.parse.urlparse(path)
-    path_is_remote = url_data.scheme in salt.utils.files.REMOTE_PROTOS
-    def _check_return(result):
-        if result and url_data.scheme == "salt":
-            if filerefs_ckey not in __context__:
-                __context__[filerefs_ckey] = []
-            if path not in __context__[filerefs_ckey]:
-                __context__[filerefs_ckey].append(path)
-        return result
-    with _client() as client:
-        try:
-            if path_is_remote and contextkey in __context__:
-                if client._path_exists(__context__[contextkey]):
-                    return _check_return(__context__[contextkey])
-                else:
-                    __context__.pop(contextkey)
-        except AttributeError:
-            pass
-        result = client.cache_file(
-            path,
-            saltenv,
-            source_hash=source_hash,
-            verify_ssl=verify_ssl,
-            use_etag=use_etag,
-        )
-        if not result and not use_etag:
-            log.error("Unable to cache file '%s' from saltenv '%s'.", path, saltenv)
-        if result:
-            result = client.target_map[result]
-        if path_is_remote:
-            __context__[contextkey] = result
-        return _check_return(result)
-def cache_files(paths, saltenv="base"):
-    """
-    Used to gather many files from the Master, the gathered files will be
-    saved in the minion cachedir reflective to the paths retrieved from the
-    Master.
-    .. note::
-        This can take a long time since each file is transferred separately.
-    CLI Example:
-    .. code-block:: bash
-        salt-ssh '*' cp.cache_files salt://pathto/file1,salt://pathto/file1
-    There are two ways of defining the fileserver environment (a.k.a.
-    ``saltenv``) from which to cache the files. One is to use the ``saltenv``
-    parameter, and the other is to use a querystring syntax in the ``salt://``
-    URL. The below two examples are equivalent:
-    .. code-block:: bash
-        salt '*' cp.cache_files salt://foo/bar.conf,salt://foo/baz.conf saltenv=config
-        salt '*' cp.cache_files salt://foo/bar.conf?saltenv=config,salt://foo/baz.conf?saltenv=config
-    The querystring method is less useful when all files are being cached from
-    the same environment, but is a good way of caching files from multiple
-    different environments in the same command. For example, the below command
-    will cache the first file from the ``config1`` environment, and the second
-    one from the ``config2`` environment.
-    .. code-block:: bash
-        salt '*' cp.cache_files salt://foo/bar.conf?saltenv=config1,salt://foo/bar.conf?saltenv=config2
-    .. note::
-        It may be necessary to quote the URL when using the querystring method,
-        depending on the shell being used to run the command.
-    """
-    ret = []
-    if isinstance(paths, str):
-        paths = paths.split(",")
-    for path in paths:
-        ret.append(cache_file(path, saltenv))
-    return ret
-def cache_dir(
-    path, saltenv="base", include_empty=False, include_pat=None, exclude_pat=None
-):
-    """
-    Download and cache everything under a directory from the master.
-    .. note::
-        This can take a long time since each file is transferred separately.
-    CLI Example:
-    .. code-block:: bash
-        salt '*' cp.cache_dir salt://path/to/dir
-        salt '*' cp.cache_dir salt://path/to/dir include_pat='E@*.py$'
-    path
-        The path on the fileserver, like ``salt://foo/bar/``. It is possible
-        to specify the ``saltenv`` using the querystring syntax:
-        ``salt://foo/bar?saltenv=config``
-    saltenv
-        Salt fileserver environment from which to retrieve the directory.
-        Defaults to ``base``.
-    include_empty
-        Whether to cache empty directories as well. Defaults to false.
-    include_pat : None
-        Glob or regex to narrow down the files cached from the given path. If
-        matching with a regex, the regex must be prefixed with ``E@``,
-        otherwise the expression will be interpreted as a glob.
-        .. versionadded:: 2014.7.0
-    exclude_pat : None
-        Glob or regex to exclude certain files from being cached from the given
-        path. If matching with a regex, the regex must be prefixed with ``E@``,
-        otherwise the expression will be interpreted as a glob.
-        .. note::
-            If used with ``include_pat``, files matching this pattern will be
-            excluded from the subset of files defined by ``include_pat``.
-        .. versionadded:: 2014.7.0
-    """
-    with _client() as client:
-        ret = client.cache_dir(path, saltenv, include_empty, include_pat, exclude_pat)
-        if not ret:
-            return ret
-        return [client.target_map[x] for x in ret]
-def cache_master(saltenv="base"):
-    """
-    Retrieve all of the files on the master and cache them locally.
-    .. note::
-        This can take a long time since each file is transferred separately.
-    CLI Example:
-    .. code-block:: bash
-        salt-ssh '*' cp.cache_master
-    """
-    with _client() as client:
-        ret = client.cache_master(saltenv)
-        if not ret:
-            return ret
-        parsed = []
-        for file in ret:
-            try:
-                parsed.append(client.target_map[file])
-            except KeyError:
-                log.error("Failed transferring a file")
-        return parsed
+    src = __context__["fileclient"].cache_file(
+        path, saltenv, cachedir=os.path.join("salt-ssh", __salt__.kwargs["id_"])
+    )
+    single = salt.client.ssh.Single(__opts__, "", **__salt__.kwargs)
+    ret = single.shell.send(src, dest)
+    return not ret[2]
 def list_states(saltenv="base"):
     """
-    List all of the available state files in an environment.
-    CLI Example:
-    .. code-block:: bash
-        salt-ssh '*' cp.list_states
-    saltenv
-        Salt fileserver environment from which to list states.
-        Defaults to ``base``.
+    List all the available state modules in an environment
     """
     return __context__["fileclient"].list_states(saltenv)
 def list_master(saltenv="base", prefix=""):
     """
-    List all of the files stored on the master.
-    CLI Example:
-    .. code-block:: bash
-        salt-ssh '*' cp.list_master
-    saltenv
-        Salt fileserver environment from which to list files.
-        Defaults to ``base``.
-    prefix
-        Only list files under this prefix. Defaults to empty.
+    List all of the files stored on the master
     """
     return __context__["fileclient"].file_list(saltenv, prefix)
 def list_master_dirs(saltenv="base", prefix=""):
     """
-    List all of the directories stored on the master.
-    CLI Example:
-    .. code-block:: bash
-        salt-ssh '*' cp.list_master_dirs
-    saltenv
-        Salt fileserver environment from which to list directories.
-        Defaults to ``base``.
-    prefix
-        Only list directories under this prefix. Defaults to empty.
+    List all of the directories stored on the master
     """
     return __context__["fileclient"].dir_list(saltenv, prefix)
 def list_master_symlinks(saltenv="base", prefix=""):
     """
-    List all of the symlinks stored on the master.
-    Will return a mapping of symlink names to absolute paths.
-    CLI Example:
-    .. code-block:: bash
-        salt-ssh '*' cp.list_master_symlinks
-    saltenv
-        Salt fileserver environment from which to list symlinks.
-        Defaults to ``base``.
-    prefix
-        Only list symlinks under this prefix. Defaults to empty.
+    List all of the symlinks stored on the master
     """
     return __context__["fileclient"].symlink_list(saltenv, prefix)
-def is_cached(path, saltenv="base"):
-    """
-    Returns the full path to a file if it is cached locally on the minion
-    as well as the SSH master-minion, otherwise returns a blank string.
-    CLI Example:
-    .. code-block:: bash
-        salt-ssh '*' cp.is_cached salt://path/to/file
-    path
-        The path to check.
-    saltenv
-        Salt fileserver environment the file was cached from.
-        Defaults to ``base``.
-    """
-    with _client() as client:
-        ret = client.is_cached(path, saltenv)
-        if not ret:
-            return ret
-        return str(client.convert_path(ret))
-def hash_file(path, saltenv="base"):
-    """
-    Return the hash of a file. Supports ``salt://`` URIs and local files.
-    Local files should be specified with their absolute paths, without the
-    ``file://`` scheme.
-    CLI Example:
-    .. code-block:: bash
-        salt-ssh '*' cp.hash_file salt://path/to/file
-        salt-ssh '*' cp.hash_file /path/to/file
-    path
-        The path to return the hash for.
-    saltenv
-        Salt fileserver environment from which the file should be hashed.
-        Defaults to ``base``.
-    """
-    path, senv = salt.utils.url.split_env(path)
-    if senv:
-        saltenv = senv
-    url_data = urllib.parse.urlparse(path)
-    if url_data.scheme in ("file", ""):
-        return __salt__["cp.hash_file_ssh"](path, saltenv)
-    with _client() as client:
-        return client.hash_file(path, saltenv)
-def convert_cache_path(path, cachedir=None, master=True):
-    """
-    It converts a path received by caching a file to the minion cache to the
-    corresponding one in the local master cache (or the other way around).
-    .. note::
-        This function is exclusive to the SSH wrapper module and is mostly
-        intended for other wrapper modules to use, not on the CLI.
-    CLI Example:
-    .. code-block:: bash
-        salt-ssh '*' cp.convert_cache_path /var/tmp/.root_abc123_salt/running_data/var/cache/salt/minion/files/base/foo.txt
-    path
-        The path to convert. It has to be in one of the (remote or SSH master-minion)
-        cachedirs to be converted, otherwise will be returned verbatim.
-    cachedir
-        An optional cachedir override that was used when caching the file.
-    master
-        Whether to convert the path to the master-side path. Defaults
-        to true (since this module returns the minion paths otherwise).
-    """
-    with _client() as client:
-        return str(client.convert_path(path, cachedir, master))
-def _gather_pillar(pillarenv, pillar_override):
-    """
-    The opts used during pillar rendering should contain the master
-    opts in the root namespace. self.opts is the modified minion opts,
-    containing the original master opts in __master_opts__.
-    """
-    popts = {}
-    popts.update(__opts__)
-    popts.update(__opts__.get("__master_opts__", {}))
-    pillar = salt.pillar.get_pillar(
-        popts,
-        __grains__.value(),
-        __salt__.kwargs["id_"],
-        __opts__["saltenv"] or "base",
-        pillar_override=pillar_override,
-        pillarenv=pillarenv,
-    )
-    return pillar.compile_pillar()
-def _render_filenames(path, dest, saltenv, template, **kw):
+def _render_filenames(path, dest, saltenv, template):
     """
     Process markup in the :param:`path` and :param:`dest` variables (NOT the
     files under the paths they ultimately point to) according to the markup
     format provided by :param:`template`.
     """
     if not template:
         return (path, dest)
     if template not in salt.utils.templates.TEMPLATE_REGISTRY:
         raise CommandExecutionError(
             f"Attempted to render file paths with unavailable engine {template}"
         )
     kwargs = {}
     kwargs["salt"] = __salt__.value()
-    if "pillarenv" in kw or "pillar" in kw:
-        pillarenv = kw.get("pillarenv", __opts__.get("pillarenv"))
-        kwargs["pillar"] = _gather_pillar(pillarenv, kw.get("pillar"))
-    else:
-        kwargs["pillar"] = __pillar__.value()
+    kwargs["pillar"] = __pillar__.value()
     kwargs["grains"] = __grains__.value()
     kwargs["opts"] = __opts__
     kwargs["saltenv"] = saltenv
     def _render(contents):
         """
         Render :param:`contents` into a literal pathname by writing it to a
         temp file, rendering that file, and returning the result.
         """
         tmp_path_fn = salt.utils.files.mkstemp()
         with salt.utils.files.fopen(tmp_path_fn, "w+") as fp_:
@@ -614,296 +99,10 @@
         salt.utils.files.safe_rm(tmp_path_fn)
         if not data["result"]:
             raise CommandExecutionError(
                 "Failed to render file path with error: {}".format(data["data"])
             )
         else:
             return data["data"]
     path = _render(path)
     dest = _render(dest)
     return (path, dest)
-class SSHCpClient(salt.fileclient.FSClient):
-    """
-    A FileClient that replicates between SSH master-minion and remote minion caches
-    """
-    def __init__(self, opts, shell, tgt):  # pylint: disable=W0231
-        salt.fileclient.FSClient.__init__(self, opts)  # pylint: disable=W0233
-        self.shell = shell
-        self.tgt = tgt
-        self.target_map = {}
-    def _local_path_exists(self, path):
-        file = self.convert_path(path, master=True)
-        return file.exists()
-    def _remote_path_exists(self, path):
-        path = self.convert_path(path)
-        _, _, retcode = self.shell.exec_cmd("test -e " + shlex.quote(str(path)))
-        return not retcode
-    def _path_exists(self, path):
-        return self._local_path_exists(path) and self._remote_path_exists(path)
-    def cache_local_file(self, path, **kwargs):
-        raise CommandExecutionError("Cannot cache local files via salt-ssh")
-    def is_cached(self, path, saltenv="base", cachedir=None):
-        """
-        Returns the full path to a file if it is cached both locally on the
-        SSH master-minion and the minion, otherwise returns a blank string
-        """
-        if path.startswith("salt://"):
-            path, senv = salt.utils.url.parse(path)
-            if senv:
-                saltenv = senv
-        escaped = True if salt.utils.url.is_escaped(path) else False
-        localsfilesdest = os.path.join(
-            self.opts["cachedir"], "localfiles", path.lstrip("|/")
-        )
-        filesdest = os.path.join(
-            self.opts["cachedir"], "files", saltenv, path.lstrip("|/")
-        )
-        extrndest = self._extrn_path(path, saltenv, cachedir=cachedir)
-        if self._path_exists(filesdest):
-            return salt.utils.url.escape(filesdest) if escaped else filesdest
-        if self._remote_path_exists(localsfilesdest):
-            return (
-                salt.utils.url.escape(localsfilesdest) if escaped else localsfilesdest
-            )
-        if self._path_exists(extrndest):
-            return extrndest
-        return ""
-    def get_cachedir(
-        self, cachedir=None, master=True
-    ):  # pylint: disable=arguments-differ
-        prefix = []
-        if master:
-            prefix = ["salt-ssh", self.tgt]
-        if cachedir is None:
-            cachedir = os.path.join(self.opts["cachedir"], *prefix)
-        elif not os.path.isabs(cachedir):
-            cachedir = os.path.join(self.opts["cachedir"], *prefix, cachedir)
-        elif master:
-            cachedir = os.path.join(
-                self.opts["cachedir"],
-                *prefix,
-                "absolute_root",
-                str(Path(*cachedir.split(os.sep)[1:])),
-            )
-        return cachedir
-    def convert_path(self, path, cachedir=None, master=False):
-        """
-        Convert a cache path from master/minion to the other.
-        Both use the same cachedir in salt-ssh, but our fileclient
-        here caches to a subdir on the master. Remove/add it from/to
-        the path.
-        """
-        path = Path(path)
-        master_cachedir = Path(self.get_cachedir(cachedir, master=True))
-        minion_cachedir = Path(self.get_cachedir(cachedir, master=False))
-        if master:
-            if master_cachedir in path.parents:
-                return path
-            return master_cachedir / path.relative_to(minion_cachedir)
-        if master_cachedir not in path.parents:
-            return path
-        return minion_cachedir / path.relative_to(master_cachedir)
-    def _send_file(self, src, dest, makedirs, cachedir):
-        def _error(stdout, stderr):
-            log.error(f"Failed sending file: {stderr or stdout}")
-            if Path(self.get_cachedir(cachedir)) in Path(src).parents:
-                Path(src).unlink(missing_ok=True)
-            return False
-        for path in (src, dest):
-            if not Path(path).is_absolute():
-                raise ValueError(
-                    f"Paths must be absolute, got '{path}' as {'src' if path == src else 'dest'}"
-                )
-        src, dest = str(src), str(dest)  # ensure we're using strings
-        stdout, stderr, retcode = self.shell.send(src, dest, makedirs)
-        if retcode and makedirs and "Not a directory" in stderr:
-            minion_cachedir = Path(self.get_cachedir(cachedir, master=False))
-            dest = cur = Path(dest)
-            while minion_cachedir in cur.parents:
-                if self._isfile(cur):
-                    if not self._rmpath(cur):
-                        return _error(stdout, stderr)
-                    dest = str(dest)
-                    break
-                cur = cur.parent
-            else:
-                return _error(stdout, stderr)
-            stdout, stderr, retcode = self.shell.send(src, dest, makedirs)
-        if retcode:
-            return _error(stdout, stderr)
-        self.target_map[src] = dest
-        return src
-    def _isdir(self, path):
-        _, _, retcode = self.shell.exec_cmd("test -d " + shlex.quote(str(path)))
-        return not retcode
-    def _isfile(self, path):
-        _, _, retcode = self.shell.exec_cmd("test -f " + shlex.quote(str(path)))
-        return not retcode
-    def _rmpath(self, path, cachedir=None):
-        path = Path(path)
-        if not path or not path.is_absolute() or str(path) == "/":
-            raise ValueError(
-                f"Not deleting unspecified, relative or root path: '{path}'"
-            )
-        minion_cachedir = Path(self.get_cachedir(cachedir, master=False))
-        if minion_cachedir not in path.parents and path != minion_cachedir:
-            raise ValueError(
-                f"Not recursively deleting a path outside of the cachedir. Path: '{path}'"
-            )
-        stdout, stderr, retcode = self.shell.exec_cmd(
-            "rm -rf " + shlex.quote(str(path))
-        )
-        if retcode:
-            log.error(f"Failed deleting path '{path}': {stderr or stdout}")
-        return not retcode
-    def get_url(
-        self,
-        url,
-        dest,
-        makedirs=False,
-        saltenv="base",
-        no_cache=False,
-        cachedir=None,
-        source_hash=None,
-        verify_ssl=True,
-        use_etag=False,
-    ):
-        url_data = urllib.parse.urlparse(url)
-        if url_data.scheme in ("file", ""):
-            log.error("The file:// scheme is not supported via the salt-ssh cp wrapper")
-            return False
-        if url_data.scheme == "salt":
-            result = self.get_file(url, dest, makedirs, saltenv, cachedir=cachedir)
-            if result and dest is None:
-                with salt.utils.files.fopen(result, "rb") as fp_:
-                    data = fp_.read()
-                return data
-            return result
-        cached = super().get_url(
-            url,
-            "",
-            makedirs=True,
-            saltenv=saltenv,
-            no_cache=no_cache,
-            cachedir=cachedir,
-            source_hash=source_hash,
-            verify_ssl=verify_ssl,
-            use_etag=use_etag,
-        )
-        if not cached:
-            return cached
-        if not isinstance(dest, str) and no_cache:
-            return cached
-        strict = False
-        if not dest:
-            makedirs = True
-            dest = str(self.convert_path(cached, cachedir))
-            strict = True
-        if dest.endswith("/") or self._isdir(dest):
-            if not dest.endswith("/"):
-                if (
-                    strict
-                    or self.get_cachedir(cachedir, master=False) in Path(dest).parents
-                ):
-                    strict = True
-                    if not self._rmpath(dest):
-                        Path(cached).unlink(missing_ok=True)
-                        return False
-            if not strict:
-                if (
-                    url_data.query
-                    or len(url_data.path) > 1
-                    and not url_data.path.endswith("/")
-                ):
-                    strpath = url.split("/")[-1]
-                else:
-                    strpath = "index.html"
-                dest = os.path.join(dest, strpath)
-        return self._send_file(cached, dest, makedirs, cachedir)
-    def get_file(
-        self, path, dest="", makedirs=False, saltenv="base", gzip=None, cachedir=None
-    ):
-        """
-        Get a single file from the salt-master
-        path must be a salt server location, aka, salt://path/to/file, if
-        dest is omitted, then the downloaded file will be placed in the minion
-        cache
-        """
-        src = super().get_file(
-            path,
-            "",
-            makedirs=True,
-            saltenv=saltenv,
-            cachedir=cachedir,
-        )
-        if not src:
-            return src
-        strict = False
-        if not dest:
-            makedirs = True
-            dest = str(self.convert_path(src, cachedir))
-            strict = True
-        if dest.endswith("/") or self._isdir(dest):
-            if not dest.endswith("/"):
-                if (
-                    strict
-                    or self.get_cachedir(cachedir, master=False) in Path(dest).parents
-                ):
-                    strict = True
-                    if not self._rmpath(dest):
-                        Path(src).unlink(missing_ok=True)
-                        return ""
-            if not strict:
-                dest = os.path.join(dest, os.path.basename(src))
-        return self._send_file(src, dest, makedirs, cachedir)
-    def get_template(
-        self,
-        url,
-        dest,
-        template="jinja",
-        makedirs=False,
-        saltenv="base",
-        cachedir=None,
-        **kwargs,
-    ):
-        """
-        Cache a file then process it as a template
-        """
-        res = super().get_template(
-            url,
-            "",
-            template=template,
-            makedirs=makedirs,
-            saltenv=saltenv,
-            cachedir=cachedir,
-            **kwargs,
-        )
-        if not res:
-            return res
-        strict = False
-        if not dest:
-            makedirs = True
-            dest = str(self.convert_path(res, cachedir))
-            strict = True
-        if dest.endswith("/") or self._isdir(dest):
-            if not dest.endswith("/"):
-                if (
-                    strict
-                    or self.get_cachedir(cachedir, master=False) in Path(dest).parents
-                ):
-                    strict = True
-                    if not self._rmpath(dest):
-                        Path(res).unlink(missing_ok=True)
-                        return ""
-            if not strict:
-                dest = os.path.join(dest, os.path.basename(res))
-        return self._send_file(res, dest, makedirs, cachedir)
-    def _extrn_path(self, url, saltenv, cachedir=None):
-        res = super()._extrn_path(url, saltenv, cachedir=cachedir)
-        return str(self.convert_path(res, cachedir, master=True))
-    def cache_dest(self, url, saltenv="base", cachedir=None):
-        """
-        Return the expected cache location for the specified URL and
-        environment.
-        """
-        res = super().cache_dest(url, saltenv=saltenv, cachedir=cachedir)
-        return str(self.convert_path(res, cachedir, master=True))

--- a/salt/client/ssh/wrapper/grains.py
+++ b/salt/client/ssh/wrapper/grains.py
@@ -7,31 +7,34 @@
 import salt.utils.json
 from salt.defaults import DEFAULT_TARGET_DELIM
 __grains__ = {}
 def _serial_sanitizer(instr):
     """
     Replaces the last 1/4 of a string with X's
     """
     length = len(instr)
     index = int(math.floor(length * 0.75))
     return "{}{}".format(instr[:index], "X" * (length - index))
-_FQDN_SANITIZER = lambda x: "MINION.DOMAINNAME"
-_HOSTNAME_SANITIZER = lambda x: "MINION"
-_DOMAINNAME_SANITIZER = lambda x: "DOMAINNAME"
+def _fqdn_sanitizer(x):
+    return "MINION.DOMAINNAME"
+def _hostname_sanitizer(x):
+    return "MINION"
+def _domainname_sanitizer(x):
+    return "DOMAINNAME"
 _SANITIZERS = {
     "serialnumber": _serial_sanitizer,
-    "domain": _DOMAINNAME_SANITIZER,
-    "fqdn": _FQDN_SANITIZER,
-    "id": _FQDN_SANITIZER,
-    "host": _HOSTNAME_SANITIZER,
-    "localhost": _HOSTNAME_SANITIZER,
-    "nodename": _HOSTNAME_SANITIZER,
+    "domain": _domainname_sanitizer,
+    "fqdn": _fqdn_sanitizer,
+    "id": _fqdn_sanitizer,
+    "host": _hostname_sanitizer,
+    "localhost": _hostname_sanitizer,
+    "nodename": _hostname_sanitizer,
 }
 def get(key, default="", delimiter=DEFAULT_TARGET_DELIM, ordered=True):
     """
     Attempt to retrieve the named value from grains, if the named value is not
     available return the passed default. The default return is an empty string.
     The value can also represent a value in a nested dict using a ":" delimiter
     for the dict. This means that if a dict in grains looks like this::
         {'pkg': {'apache': 'httpd'}}
     To retrieve the value associated with the apache key in the pkg dict this
     key can be passed::

--- a/salt/client/ssh/wrapper/mine.py
+++ b/salt/client/ssh/wrapper/mine.py
@@ -1,95 +1,41 @@
 """
 Wrapper function for mine operations for salt-ssh
 .. versionadded:: 2015.5.0
-.. versionchanged:: 3007.0
-    In addition to mine returns from roster targets, this wrapper now supports
-    accessing the regular mine as well.
 """
 import copy
-import logging
 import salt.client.ssh
-import salt.daemons.masterapi
-log = logging.getLogger(__name__)
-def get(
-    tgt, fun, tgt_type="glob", roster="flat", ssh_minions=True, regular_minions=False
-):
+def get(tgt, fun, tgt_type="glob", roster="flat"):
     """
     Get data from the mine based on the target, function and tgt_type
-    This will actually run the function on all targeted SSH minions (like
+    This will actually run the function on all targeted minions (like
     publish.publish), as salt-ssh clients can't update the mine themselves.
     We will look for mine_functions in the roster, pillar, and master config,
-    in that order, looking for a match for the defined function.
+    in that order, looking for a match for the defined function
     Targets can be matched based on any standard matching system that can be
-    matched on the defined roster (in salt-ssh).
-    Regular mine data will be fetched as usual and can be targeted as usual.
+    matched on the defined roster (in salt-ssh) via these keywords::
     CLI Example:
     .. code-block:: bash
         salt-ssh '*' mine.get '*' network.interfaces
         salt-ssh '*' mine.get 'myminion' network.interfaces roster=flat
         salt-ssh '*' mine.get '192.168.5.0' network.ipaddrs roster=scan
-        salt-ssh myminion mine.get '*' network.interfaces ssh_minions=False regular_minions=True
-        salt-ssh myminion mine.get '*' network.interfaces ssh_minions=True regular_minions=True
-    tgt
-        Target whose mine data to get.
-    fun
-        Function to get the mine data of. You can specify multiple functions
-        to retrieve using either a list or a comma-separated string of functions.
-    tgt_type
-        Target type to use with ``tgt``. Defaults to ``glob``.
-        See :ref:`targeting` for more information for regular minion targets, above
-        for SSH ones.
-    roster
-        The roster module to use. Defaults to ``flat``.
-    ssh_minions
-        .. versionadded:: 3007.0
-        Target minions from the roster. Defaults to true.
-    regular_minions
-        .. versionadded:: 3007.0
-        Target regular minions of the master running salt-ssh. Defaults to false.
     """
+    opts = copy.deepcopy(__context__["master_opts"])
+    minopts = copy.deepcopy(__opts__)
+    opts.update(minopts)
+    if roster:
+        opts["roster"] = roster
+    opts["argv"] = [fun]
+    opts["selected_target_option"] = tgt_type
+    opts["tgt"] = tgt
+    opts["arg"] = []
+    ssh = salt.client.ssh.SSH(opts)
     rets = {}
-    if regular_minions:
-        masterapi = salt.daemons.masterapi.RemoteFuncs(__context__["master_opts"])
-        load = {
-            "id": __opts__["id"],
-            "fun": fun,
-            "tgt": tgt,
-            "tgt_type": tgt_type,
-        }
-        ret = masterapi._mine_get(load)
+    for ret in ssh.run_iter(mine=True):
         rets.update(ret)
-    if ssh_minions:
-        opts = copy.deepcopy(__context__["master_opts"])
-        minopts = copy.deepcopy(__opts__)
-        opts.update(minopts)
-        if roster:
-            opts["roster"] = roster
-        opts["argv"] = [fun]
-        opts["selected_target_option"] = tgt_type
-        opts["tgt"] = tgt
-        opts["arg"] = []
-        ssh = salt.client.ssh.SSH(opts)
-        mrets = {}
-        for ret in ssh.run_iter(mine=True):
-            mrets.update(ret)
-        for host, data in mrets.items():
-            if not isinstance(data, dict):
-                log.error(
-                    f"Error executing mine func {fun} on {host}: {data}."
-                    " Excluding minion from mine."
-                )
-            elif "_error" in data:
-                log.error(
-                    f"Error executing mine func {fun} on {host}: {data['_error']}."
-                    " Excluding minion from mine. Full output in debug log."
-                )
-                log.debug(f"Return was: {salt.utils.json.dumps(data)}")
-            elif "return" not in data:
-                log.error(
-                    f"Error executing mine func {fun} on {host}: No return was specified."
-                    " Excluding minion from mine. Full output in debug log."
-                )
-                log.debug(f"Return was: {salt.utils.json.dumps(data)}")
-            else:
-                rets[host] = data["return"]
-    return rets
+    cret = {}
+    for host in rets:
+        if "return" in rets[host]:
+            cret[host] = rets[host]["return"]
+        else:
+            cret[host] = rets[host]
+    return cret

--- a/salt/client/ssh/wrapper/publish.py
+++ b/salt/client/ssh/wrapper/publish.py
@@ -1,39 +1,23 @@
 """
 .. versionadded:: 2015.5.0
 Salt-ssh wrapper functions for the publish module.
 Publish will never actually execute on the minions, so we just create new
 salt-ssh calls and return the data from them.
 No access control is needed because calls cannot originate from the minions.
-.. versionchanged:: 3007.0
-    In addition to SSH minions, this module can now also target regular ones.
 """
 import copy
 import logging
-import time
 import salt.client.ssh
-import salt.daemons.masterapi
 import salt.runner
 import salt.utils.args
-import salt.utils.json
 log = logging.getLogger(__name__)
-def _parse_args(arg):
-    """
-    yamlify `arg` and ensure its outermost datatype is a list
-    """
-    yaml_args = salt.utils.args.yamlify_arg(arg)
-    if yaml_args is None:
-        return []
-    elif not isinstance(yaml_args, list):
-        return [yaml_args]
-    else:
-        return yaml_args
 def _publish(
     tgt,
     fun,
     arg=None,
     tgt_type="glob",
     returner="",
     timeout=None,
     form="clean",
     roster=None,
 ):
@@ -55,26 +39,23 @@
         salt-ssh system.example.com publish.publish '*' cmd.run 'ls -la /tmp'
     """
     if fun.startswith("publish."):
         log.info("Cannot publish publish calls. Returning {}")
         return {}
     if returner:
         log.warning("Returners currently not supported in salt-ssh publish")
     if arg is None:
         arg = []
     elif not isinstance(arg, list):
-        arg = [salt.utils.json.dumps(salt.utils.args.yamlify_arg(arg))]
+        arg = [salt.utils.args.yamlify_arg(arg)]
     else:
-        arg = [
-            salt.utils.json.dumps(y)
-            for y in (salt.utils.args.yamlify_arg(x) for x in arg)
-        ]
+        arg = [salt.utils.args.yamlify_arg(x) for x in arg]
     if len(arg) == 1 and arg[0] is None:
         arg = []
     opts = copy.deepcopy(__context__["master_opts"])
     minopts = copy.deepcopy(__opts__)
     opts.update(minopts)
     if roster:
         opts["roster"] = roster
     if timeout:
         opts["timeout"] = timeout
     opts["argv"] = [fun] + arg
@@ -86,128 +67,36 @@
     for ret in ssh.run_iter():
         rets.update(ret)
     if form == "clean":
         cret = {}
         for host in rets:
             if "return" in rets[host]:
                 cret[host] = rets[host]["return"]
             else:
                 cret[host] = rets[host]
         return cret
-    for host in rets:
-        if "return" in rets[host]:
-            rets[host]["ret"] = rets[host]["return"]
-    return rets
-def _publish_regular(
-    tgt,
-    fun,
-    arg=None,
-    tgt_type="glob",
-    returner="",
-    timeout=5,
-    form="clean",
-    wait=False,
-):
-    if fun.startswith("publish."):
-        log.info("Cannot publish publish calls. Returning {}")
-        return {}
-    arg = _parse_args(arg)
-    masterapi = salt.daemons.masterapi.RemoteFuncs(__context__["master_opts"])
-    log.info("Publishing '%s'", fun)
-    load = {
-        "cmd": "minion_pub",
-        "fun": fun,
-        "arg": arg,
-        "tgt": tgt,
-        "tgt_type": tgt_type,
-        "ret": returner,
-        "tmo": timeout,
-        "form": form,
-        "id": __opts__["id"],
-        "no_parse": __opts__.get("no_parse", []),
-    }
-    peer_data = masterapi.minion_pub(load)
-    if not peer_data:
-        return {}
-    if wait:
-        loop_interval = 0.3
-        matched_minions = set(peer_data["minions"])
-        returned_minions = set()
-        loop_counter = 0
-        while returned_minions ^ matched_minions:
-            load = {
-                "cmd": "pub_ret",
-                "id": __opts__["id"],
-                "jid": peer_data["jid"],
-            }
-            ret = masterapi.pub_ret(load)
-            returned_minions = set(ret.keys())
-            end_loop = False
-            if returned_minions >= matched_minions:
-                end_loop = True
-            elif (loop_interval * loop_counter) > timeout:
-                if not returned_minions:
-                    return {}
-                end_loop = True
-            if end_loop:
-                if form == "clean":
-                    cret = {}
-                    for host in ret:
-                        cret[host] = ret[host]["ret"]
-                    return cret
-                else:
-                    return ret
-            loop_counter = loop_counter + 1
-            time.sleep(loop_interval)
     else:
-        time.sleep(float(timeout))
-        load = {
-            "cmd": "pub_ret",
-            "id": __opts__["id"],
-            "jid": peer_data["jid"],
-        }
-        ret = masterapi.pub_ret(load)
-        if form == "clean":
-            cret = {}
-            for host in ret:
-                cret[host] = ret[host]["ret"]
-            return cret
-        else:
-            return ret
-    return ret
-def publish(
-    tgt,
-    fun,
-    arg=None,
-    tgt_type="glob",
-    returner="",
-    timeout=5,
-    roster=None,
-    ssh_minions=True,
-    regular_minions=False,
-):
+        return rets
+def publish(tgt, fun, arg=None, tgt_type="glob", returner="", timeout=5, roster=None):
     """
-    Publish a command from the minion out to other minions. In reality, the
+    Publish a command "from the minion out to other minions". In reality, the
     minion does not execute this function, it is executed by the master. Thus,
     no access control is enabled, as minions cannot initiate publishes
     themselves.
     Salt-ssh publishes will default to whichever roster was used for the
     initiating salt-ssh call, and can be overridden using the ``roster``
-    argument.
+    argument
     Returners are not currently supported
     The tgt_type argument is used to pass a target other than a glob into
-    the execution, the available options for SSH minions are:
+    the execution, the available options are:
     - glob
     - pcre
-    - nodegroup
-    - range
-    Regular minions support all usual ones.
     .. versionchanged:: 2017.7.0
         The ``expr_form`` argument has been renamed to ``tgt_type``, earlier
         releases must use ``expr_form``.
     The arguments sent to the minion publish function are separated with
     commas. This means that for a minion executing a command with multiple
     args it will look like this:
     .. code-block:: bash
         salt-ssh system.example.com publish.publish '*' user.add 'foo,1020,1020'
         salt-ssh system.example.com publish.publish '127.0.0.1' network.interfaces '' roster=scan
     CLI Example:
@@ -215,125 +104,58 @@
         salt-ssh system.example.com publish.publish '*' cmd.run 'ls -la /tmp'
     .. admonition:: Attention
         If you need to pass a value to a function argument and that value
         contains an equal sign, you **must** include the argument name.
         For example:
         .. code-block:: bash
             salt-ssh '*' publish.publish test.kwarg arg='cheese=spam'
         Multiple keyword arguments should be passed as a list.
         .. code-block:: bash
             salt-ssh '*' publish.publish test.kwarg arg="['cheese=spam','spam=cheese']"
-    tgt
-        The target specification.
-    fun
-        The execution module to run.
-    arg
-        A list of arguments to pass to the module.
-    tgt_type
-        The matcher to use. Defaults to ``glob``.
-    returner
-        A returner to use.
-    timeout
-        Timeout in seconds. Defaults to 5.
-    roster
-        Override the roster for SSH minion targets. Defaults to the one
-        used for initiating the salt-ssh call.
-    ssh_minions
-        .. versionadded:: 3007.0
-        Include SSH minions in the possible targets. Defaults to true.
-    regular_minions
-        .. versionadded:: 3007.0
-        Include regular minions in the possible targets. Defaults to false.
     """
-    rets = {}
-    if regular_minions:
-        rets.update(
-            _publish_regular(
-                tgt,
-                fun,
-                arg=arg,
-                tgt_type=tgt_type,
-                returner=returner,
-                timeout=timeout,
-                form="clean",
-                wait=True,
-            )
-        )
-    if ssh_minions:
-        rets.update(
-            _publish(
-                tgt,
-                fun,
-                arg=arg,
-                tgt_type=tgt_type,
-                returner=returner,
-                timeout=timeout,
-                form="clean",
-                roster=roster,
-            )
-        )
-    return rets
-def full_data(
-    tgt,
-    fun,
-    arg=None,
-    tgt_type="glob",
-    returner="",
-    timeout=5,
-    roster=None,
-    ssh_minions=True,
-    regular_minions=False,
-):
+    return _publish(
+        tgt,
+        fun,
+        arg=arg,
+        tgt_type=tgt_type,
+        returner=returner,
+        timeout=timeout,
+        form="clean",
+        roster=roster,
+    )
+def full_data(tgt, fun, arg=None, tgt_type="glob", returner="", timeout=5, roster=None):
     """
     Return the full data about the publication, this is invoked in the same
     way as the publish function
     CLI Example:
     .. code-block:: bash
         salt-ssh system.example.com publish.full_data '*' cmd.run 'ls -la /tmp'
     .. admonition:: Attention
         If you need to pass a value to a function argument and that value
         contains an equal sign, you **must** include the argument name.
         For example:
         .. code-block:: bash
             salt-ssh '*' publish.full_data test.kwarg arg='cheese=spam'
     """
-    rets = {}
-    if regular_minions:
-        rets.update(
-            _publish_regular(
-                tgt,
-                fun,
-                arg=arg,
-                tgt_type=tgt_type,
-                returner=returner,
-                timeout=timeout,
-                form="full",
-                wait=True,
-            )
-        )
-    if ssh_minions:
-        rets.update(
-            _publish(
-                tgt,
-                fun,
-                arg=arg,
-                tgt_type=tgt_type,
-                returner=returner,
-                timeout=timeout,
-                form="full",
-                roster=roster,
-            )
-        )
-    return rets
+    return _publish(
+        tgt,
+        fun,
+        arg=arg,
+        tgt_type=tgt_type,
+        returner=returner,
+        timeout=timeout,
+        form="full",
+        roster=roster,
+    )
 def runner(fun, arg=None, timeout=5):
     """
-    Execute a runner on the master and return the data from the runner function
+    Execute a runner on the master and return the data from the runnr function
     CLI Example:
     .. code-block:: bash
         salt-ssh '*' publish.runner jobs.lookup_jid 20140916125524463507
     """
     if not isinstance(arg, list):
         arg = [salt.utils.args.yamlify_arg(arg)]
     else:
         arg = [salt.utils.args.yamlify_arg(x) for x in arg]
     if len(arg) == 1 and arg[0] is None:
         arg = []

--- a/salt/client/ssh/wrapper/state.py
+++ b/salt/client/ssh/wrapper/state.py
@@ -23,23 +23,21 @@
 from salt.exceptions import SaltInvocationError
 __func_alias__ = {"apply_": "apply"}
 log = logging.getLogger(__name__)
 def _ssh_state(chunks, st_kwargs, kwargs, pillar, test=False):
     """
     Function to run a state with the given chunk via salt-ssh
     """
     file_refs = salt.client.ssh.state.lowstate_file_refs(
         chunks,
         _merge_extra_filerefs(
-            kwargs.get("extra_filerefs", ""),
-            __opts__.get("extra_filerefs", ""),
-            __context__.get("_cp_extra_filerefs", ""),
+            kwargs.get("extra_filerefs", ""), __opts__.get("extra_filerefs", "")
         ),
     )
     trans_tar = salt.client.ssh.state.prep_trans_tar(
         __context__["fileclient"],
         chunks,
         file_refs,
         pillar,
         st_kwargs["id_"],
     )
     trans_tar_sum = salt.utils.hashutils.get_hash(trans_tar, __opts__["hash_type"])
@@ -47,26 +45,43 @@
         __opts__["thin_dir"], test, trans_tar_sum, __opts__["hash_type"]
     )
     single = salt.client.ssh.Single(
         __opts__,
         cmd,
         fsclient=__context__["fileclient"],
         minion_opts=__salt__.minion_opts,
         **st_kwargs,
     )
     single.shell.send(trans_tar, "{}/salt_state.tgz".format(__opts__["thin_dir"]))
-    stdout, stderr, retcode = single.cmd_block()
+    stdout, stderr, _ = single.cmd_block()
     try:
         os.remove(trans_tar)
     except OSError:
         pass
-    return {"local": salt.client.ssh.wrapper.parse_ret(stdout, stderr, retcode)}
+    try:
+        return salt.utils.data.decode(
+            salt.utils.json.loads(stdout, object_hook=salt.utils.data.encode_dict)
+        )
+    except Exception as e:  # pylint: disable=broad-except
+        log.error("JSON Render failed for: %s\n%s", stdout, stderr)
+        log.error(str(e))
+    return salt.utils.data.decode(stdout)
+def _set_retcode(ret, highstate=None):
+    """
+    Set the return code based on the data back from the state system
+    """
+    __context__["retcode"] = salt.defaults.exitcodes.EX_OK
+    if isinstance(ret, list):
+        __context__["retcode"] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR
+        return
+    if not salt.utils.state.check_result(ret, highstate=highstate):
+        __context__["retcode"] = salt.defaults.exitcodes.EX_STATE_FAILURE
 def _check_pillar(kwargs, pillar=None):
     """
     Check the pillar for errors, refuse to run the state if there are errors
     in the pillar and return the pillar errors
     """
     if kwargs.get("force"):
         return True
     pillar_dict = pillar if pillar is not None else __pillar__.value()
     if "_errors" in pillar_dict:
         return False
@@ -169,23 +184,21 @@
         high_data, req_in_errors = st_.state.requisite_in(high_data)
         errors += req_in_errors
         high_data = st_.state.apply_exclude(high_data)
         if errors:
             __context__["retcode"] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR
             return errors
         chunks = st_.state.compile_high_data(high_data)
         file_refs = salt.client.ssh.state.lowstate_file_refs(
             chunks,
             _merge_extra_filerefs(
-                kwargs.get("extra_filerefs", ""),
-                opts.get("extra_filerefs", ""),
-                __context__.get("_cp_extra_filerefs", ""),
+                kwargs.get("extra_filerefs", ""), opts.get("extra_filerefs", "")
             ),
         )
         roster = salt.roster.Roster(opts, opts.get("roster", "flat"))
         roster_grains = roster.opts["grains"]
         _cleanup_slsmod_low_data(chunks)
         trans_tar = salt.client.ssh.state.prep_trans_tar(
             __context__["fileclient"],
             chunks,
             file_refs,
             pillar,
@@ -197,26 +210,31 @@
             opts["thin_dir"], test, trans_tar_sum, opts["hash_type"]
         )
         single = salt.client.ssh.Single(
             opts,
             cmd,
             fsclient=__context__["fileclient"],
             minion_opts=__salt__.minion_opts,
             **st_kwargs,
         )
         single.shell.send(trans_tar, "{}/salt_state.tgz".format(opts["thin_dir"]))
-        stdout, stderr, retcode = single.cmd_block()
+        stdout, stderr, _ = single.cmd_block()
         try:
             os.remove(trans_tar)
         except OSError:
             pass
-        return {"local": salt.client.ssh.wrapper.parse_ret(stdout, stderr, retcode)}
+        try:
+            return salt.utils.json.loads(stdout)
+        except Exception as e:  # pylint: disable=broad-except
+            log.error("JSON Render failed for: %s\n%s", stdout, stderr)
+            log.error(str(e))
+        return stdout
 def running(concurrent=False):
     """
     Return a list of strings that contain state return data if a state function
     is already running. This function is used to prevent multiple state calls
     from being run at the same time.
     CLI Example:
     .. code-block:: bash
         salt '*' state.running
     """
     ret = []
@@ -286,23 +304,21 @@
         for chunk in chunks:
             chunk["__id__"] = (
                 chunk["name"] if not chunk.get("__id__") else chunk["__id__"]
             )
         err = st_.state.verify_data(data)
         if err:
             return err
         file_refs = salt.client.ssh.state.lowstate_file_refs(
             chunks,
             _merge_extra_filerefs(
-                kwargs.get("extra_filerefs", ""),
-                __opts__.get("extra_filerefs", ""),
-                __context__.get("_cp_extra_filerefs", ""),
+                kwargs.get("extra_filerefs", ""), __opts__.get("extra_filerefs", "")
             ),
         )
         roster = salt.roster.Roster(__opts__, __opts__.get("roster", "flat"))
         roster_grains = roster.opts["grains"]
         trans_tar = salt.client.ssh.state.prep_trans_tar(
             __context__["fileclient"],
             chunks,
             file_refs,
             __pillar__.value(),
             st_kwargs["id_"],
@@ -313,26 +329,31 @@
             __opts__["thin_dir"], trans_tar_sum, __opts__["hash_type"]
         )
         single = salt.client.ssh.Single(
             __opts__,
             cmd,
             fsclient=__context__["fileclient"],
             minion_opts=__salt__.minion_opts,
             **st_kwargs,
         )
         single.shell.send(trans_tar, "{}/salt_state.tgz".format(__opts__["thin_dir"]))
-        stdout, stderr, retcode = single.cmd_block()
+        stdout, stderr, _ = single.cmd_block()
         try:
             os.remove(trans_tar)
         except OSError:
             pass
-        return {"local": salt.client.ssh.wrapper.parse_ret(stdout, stderr, retcode)}
+        try:
+            return salt.utils.json.loads(stdout)
+        except Exception as e:  # pylint: disable=broad-except
+            log.error("JSON Render failed for: %s\n%s", stdout, stderr)
+            log.error(str(e))
+        return stdout
 def _get_test_value(test=None, **kwargs):
     """
     Determine the correct value for the test flag.
     """
     ret = True
     if test is None:
         if salt.utils.args.test_mode(test=test, **kwargs):
             ret = True
         else:
             ret = __opts__.get("test", None)
@@ -364,23 +385,21 @@
             pillar = st_.opts["pillar"].value()
         except AttributeError:
             pillar = st_.opts["pillar"]
         if pillar_override is not None or initial_pillar is None:
             __pillar__.update(pillar)
         st_.push_active()
         chunks = st_.state.compile_high_data(data)
         file_refs = salt.client.ssh.state.lowstate_file_refs(
             chunks,
             _merge_extra_filerefs(
-                kwargs.get("extra_filerefs", ""),
-                opts.get("extra_filerefs", ""),
-                __context__.get("_cp_extra_filerefs", ""),
+                kwargs.get("extra_filerefs", ""), opts.get("extra_filerefs", "")
             ),
         )
         roster = salt.roster.Roster(opts, opts.get("roster", "flat"))
         roster_grains = roster.opts["grains"]
         _cleanup_slsmod_low_data(chunks)
         trans_tar = salt.client.ssh.state.prep_trans_tar(
             __context__["fileclient"],
             chunks,
             file_refs,
             pillar,
@@ -392,26 +411,31 @@
             opts["thin_dir"], trans_tar_sum, opts["hash_type"]
         )
         single = salt.client.ssh.Single(
             opts,
             cmd,
             fsclient=__context__["fileclient"],
             minion_opts=__salt__.minion_opts,
             **st_kwargs,
         )
         single.shell.send(trans_tar, "{}/salt_state.tgz".format(opts["thin_dir"]))
-        stdout, stderr, retcode = single.cmd_block()
+        stdout, stderr, _ = single.cmd_block()
         try:
             os.remove(trans_tar)
         except OSError:
             pass
-        return {"local": salt.client.ssh.wrapper.parse_ret(stdout, stderr, retcode)}
+        try:
+            return salt.utils.json.loads(stdout)
+        except Exception as e:  # pylint: disable=broad-except
+            log.error("JSON Render failed for: %s\n%s", stdout, stderr)
+            log.error(str(e))
+        return stdout
 def apply_(mods=None, **kwargs):
     """
     .. versionadded:: 2015.5.3
     Apply states! This function will call highstate or state.sls based on the
     arguments passed in, state.apply is intended to be the main gateway for
     all state executions.
     CLI Example:
     .. code-block:: bash
         salt '*' state.apply
         salt '*' state.apply test
@@ -563,23 +587,21 @@
             pillar = st_.opts["pillar"].value()
         except AttributeError:
             pillar = st_.opts["pillar"]
         if pillar_override is not None or initial_pillar is None:
             __pillar__.update(pillar)
         st_.push_active()
         chunks = st_.compile_low_chunks(context=__context__.value())
         file_refs = salt.client.ssh.state.lowstate_file_refs(
             chunks,
             _merge_extra_filerefs(
-                kwargs.get("extra_filerefs", ""),
-                opts.get("extra_filerefs", ""),
-                __context__.get("_cp_extra_filerefs", ""),
+                kwargs.get("extra_filerefs", ""), opts.get("extra_filerefs", "")
             ),
         )
         for chunk in chunks:
             if not isinstance(chunk, dict):
                 __context__["retcode"] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR
                 return chunks
         roster = salt.roster.Roster(opts, opts.get("roster", "flat"))
         roster_grains = roster.opts["grains"]
         _cleanup_slsmod_low_data(chunks)
         trans_tar = salt.client.ssh.state.prep_trans_tar(
@@ -595,26 +617,31 @@
             opts["thin_dir"], test, trans_tar_sum, opts["hash_type"]
         )
         single = salt.client.ssh.Single(
             opts,
             cmd,
             fsclient=__context__["fileclient"],
             minion_opts=__salt__.minion_opts,
             **st_kwargs,
         )
         single.shell.send(trans_tar, "{}/salt_state.tgz".format(opts["thin_dir"]))
-        stdout, stderr, retcode = single.cmd_block()
+        stdout, stderr, _ = single.cmd_block()
         try:
             os.remove(trans_tar)
         except OSError:
             pass
-        return {"local": salt.client.ssh.wrapper.parse_ret(stdout, stderr, retcode)}
+        try:
+            return salt.utils.json.loads(stdout)
+        except Exception as e:  # pylint: disable=broad-except
+            log.error("JSON Render failed for: %s\n%s", stdout, stderr)
+            log.error(str(e))
+        return stdout
 def top(topfn, test=None, **kwargs):
     """
     Execute a specific top file instead of the default
     CLI Example:
     .. code-block:: bash
         salt '*' state.top reverse_top.sls
         salt '*' state.top reverse_top.sls exclude=sls_to_exclude
         salt '*' state.top reverse_top.sls exclude="[{'id': 'id_to_exclude'}, {'sls': 'sls_to_exclude'}]"
     """
     st_kwargs = __salt__.kwargs
@@ -645,23 +672,21 @@
         st_.opts["state_top"] = os.path.join("salt://", topfn)
         st_.push_active()
         chunks = st_.compile_low_chunks(context=__context__.value())
         for chunk in chunks:
             if not isinstance(chunk, dict):
                 __context__["retcode"] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR
                 return chunks
         file_refs = salt.client.ssh.state.lowstate_file_refs(
             chunks,
             _merge_extra_filerefs(
-                kwargs.get("extra_filerefs", ""),
-                opts.get("extra_filerefs", ""),
-                __context__.get("_cp_extra_filerefs", ""),
+                kwargs.get("extra_filerefs", ""), opts.get("extra_filerefs", "")
             ),
         )
         roster = salt.roster.Roster(opts, opts.get("roster", "flat"))
         roster_grains = roster.opts["grains"]
         _cleanup_slsmod_low_data(chunks)
         trans_tar = salt.client.ssh.state.prep_trans_tar(
             __context__["fileclient"],
             chunks,
             file_refs,
             pillar,
@@ -673,26 +698,31 @@
             opts["thin_dir"], test, trans_tar_sum, opts["hash_type"]
         )
         single = salt.client.ssh.Single(
             opts,
             cmd,
             fsclient=__context__["fileclient"],
             minion_opts=__salt__.minion_opts,
             **st_kwargs,
         )
         single.shell.send(trans_tar, "{}/salt_state.tgz".format(opts["thin_dir"]))
-        stdout, stderr, retcode = single.cmd_block()
+        stdout, stderr, _ = single.cmd_block()
         try:
             os.remove(trans_tar)
         except OSError:
             pass
-        return {"local": salt.client.ssh.wrapper.parse_ret(stdout, stderr, retcode)}
+        try:
+            return salt.utils.json.loads(stdout)
+        except Exception as e:  # pylint: disable=broad-except
+            log.error("JSON Render failed for: %s\n%s", stdout, stderr)
+            log.error(str(e))
+        return stdout
 def show_highstate(**kwargs):
     """
     Retrieve the highstate data from the salt master and display it
     CLI Example:
     .. code-block:: bash
         salt '*' state.show_highstate
     """
     __opts__["grains"] = __grains__.value()
     opts = salt.utils.state.get_sls_opts(__opts__, **kwargs)
     pillar_override = kwargs.get("pillar")
@@ -815,20 +845,21 @@
             return errors
         chunks = st_.state.compile_high_data(high_)
         chunk = [x for x in chunks if x.get("__id__", "") == id_]
         if not chunk:
             raise SaltInvocationError(
                 "No matches for ID '{}' found in SLS '{}' within saltenv '{}'".format(
                     id_, mods, opts["saltenv"]
                 )
             )
         ret = _ssh_state(chunk, st_kwargs, kwargs, pillar, test=test)
+        _set_retcode(ret, highstate=highstate)
         __opts__["test"] = orig_test
         return ret
 def show_sls(mods, saltenv="base", test=None, **kwargs):
     """
     Display the state data from a specific sls or list of sls files on the
     master
     CLI Example:
     .. code-block:: bash
         salt '*' state.show_sls core,edit.vim dev
     """
@@ -984,23 +1015,21 @@
     except AttributeError:
         pillar = st_.opts["pillar"]
     err = st_.verify_data(kwargs)
     if err:
         __context__["retcode"] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR
         return err
     chunks = [kwargs]
     file_refs = salt.client.ssh.state.lowstate_file_refs(
         chunks,
         _merge_extra_filerefs(
-            kwargs.get("extra_filerefs", ""),
-            opts.get("extra_filerefs", ""),
-            __context__.get("_cp_extra_filerefs", ""),
+            kwargs.get("extra_filerefs", ""), opts.get("extra_filerefs", "")
         ),
     )
     roster = salt.roster.Roster(opts, opts.get("roster", "flat"))
     roster_grains = roster.opts["grains"]
     trans_tar = salt.client.ssh.state.prep_trans_tar(
         __context__["fileclient"],
         chunks,
         file_refs,
         pillar,
         st_kwargs["id_"],
@@ -1011,26 +1040,31 @@
         opts["thin_dir"], test, trans_tar_sum, opts["hash_type"]
     )
     single = salt.client.ssh.Single(
         opts,
         cmd,
         fsclient=__context__["fileclient"],
         minion_opts=__salt__.minion_opts,
         **st_kwargs,
     )
     single.shell.send(trans_tar, "{}/salt_state.tgz".format(opts["thin_dir"]))
-    stdout, stderr, retcode = single.cmd_block()
+    stdout, stderr, _ = single.cmd_block()
     try:
         os.remove(trans_tar)
     except OSError:
         pass
-    return {"local": salt.client.ssh.wrapper.parse_ret(stdout, stderr, retcode)}
+    try:
+        return salt.utils.json.loads(stdout)
+    except Exception as e:  # pylint: disable=broad-except
+        log.error("JSON Render failed for: %s\n%s", stdout, stderr)
+        log.error(str(e))
+    return stdout
 def test(*args, **kwargs):
     """
     .. versionadded:: 3001
     Alias for `state.apply` with the kwarg `test` forced to `True`.
     This is a nicety to avoid the need to type out `test=True` and the possibility of
     a typo causing changes you do not intend.
     """
     kwargs["test"] = True
     ret = apply_(*args, **kwargs)
     return ret

--- a/salt/cloud/__init__.py
+++ b/salt/cloud/__init__.py
@@ -25,27 +25,20 @@
 import salt.utils.user
 import salt.utils.verify
 import salt.utils.yaml
 from salt.exceptions import (
     SaltCloudConfigError,
     SaltCloudException,
     SaltCloudNotFound,
     SaltCloudSystemExit,
 )
 from salt.template import compile_template
-try:
-    import Cryptodome.Random
-except ImportError:
-    try:
-        import Crypto.Random  # nosec
-    except ImportError:
-        pass  # pycrypto < 2.1
 log = logging.getLogger(__name__)
 def communicator(func):
     """Warning, this is a picklable decorator !"""
     def _call(queue, args, kwargs):
         """called with [queue, args, kwargs] as first optional arg"""
         kwargs["queue"] = queue
         ret = None
         try:
             ret = func(*args, **kwargs)
             queue.put("END")
@@ -1064,26 +1057,26 @@
             output["ret"] = action_out
         return output
     @staticmethod
     def vm_config(name, main, provider, profile, overrides):
         """
         Create vm config.
         :param str name: The name of the vm
         :param dict main: The main cloud config
         :param dict provider: The provider config
         :param dict profile: The profile config
-        :param dict overrides: a special dict that carries per-node options overrides (see CloudConfig:profile() documentation)
+        :param dict overrides: The vm's config overrides
         """
         vm = main.copy()
         vm = salt.utils.dictupdate.update(vm, provider)
         vm = salt.utils.dictupdate.update(vm, profile)
-        vm = salt.utils.dictupdate.update(vm, overrides.get(name, {}))
+        vm.update(overrides)
         vm["name"] = name
         return vm
     def extras(self, extra_):
         """
         Extra actions
         """
         output = {}
         alias, driver = extra_["provider"].split(":")
         fun = "{}.{}".format(driver, extra_["action"])
         if fun not in self.clouds:
@@ -1862,21 +1855,20 @@
     """
     Make every worker ignore KeyboarInterrup's since it will be handled by the
     parent process.
     """
     signal.signal(signal.SIGINT, signal.SIG_IGN)
 def create_multiprocessing(parallel_data, queue=None):
     """
     This function will be called from another process when running a map in
     parallel mode. The result from the create is always a json object.
     """
-    salt.utils.crypt.reinit_crypto()
     parallel_data["opts"]["output"] = "json"
     cloud = Cloud(parallel_data["opts"])
     try:
         output = cloud.create(
             parallel_data["profile"], local_master=parallel_data["local_master"]
         )
     except SaltCloudException as exc:
         log.error(
             "Failed to deploy '%s'. Error: %s",
             parallel_data["name"],
@@ -1887,21 +1879,20 @@
     if parallel_data["opts"].get("show_deploy_args", False) is False and isinstance(
         output, dict
     ):
         output.pop("deploy_kwargs", None)
     return {parallel_data["name"]: salt.utils.data.simple_types_filter(output)}
 def destroy_multiprocessing(parallel_data, queue=None):
     """
     This function will be called from another process when running a map in
     parallel mode. The result from the destroy is always a json object.
     """
-    salt.utils.crypt.reinit_crypto()
     parallel_data["opts"]["output"] = "json"
     clouds = salt.loader.clouds(parallel_data["opts"])
     try:
         fun = clouds["{}.destroy".format(parallel_data["driver"])]
         with salt.utils.context.func_globals_inject(
             fun,
             __active_provider_name__=":".join(
                 [parallel_data["alias"], parallel_data["driver"]]
             ),
         ):
@@ -1913,21 +1904,20 @@
             exc,
             exc_info_on_loglevel=logging.DEBUG,
         )
         return {parallel_data["name"]: {"Error": str(exc)}}
     return {parallel_data["name"]: salt.utils.data.simple_types_filter(output)}
 def run_parallel_map_providers_query(data, queue=None):
     """
     This function will be called from another process when building the
     providers map.
     """
-    salt.utils.crypt.reinit_crypto()
     cloud = Cloud(data["opts"])
     try:
         with salt.utils.context.func_globals_inject(
             cloud.clouds[data["fun"]],
             __active_provider_name__=":".join([data["alias"], data["driver"]]),
         ):
             return (
                 data["alias"],
                 data["driver"],
                 salt.utils.data.simple_types_filter(cloud.clouds[data["fun"]]()),

--- a//dev/null
+++ b/salt/cloud/clouds/azurearm.py
@@ -0,0 +1,1707 @@
+"""
+Azure ARM Cloud Module
+======================
+.. versionadded:: 2016.11.0
+.. versionchanged:: 2019.2.0
+The Azure ARM cloud module is used to control access to Microsoft Azure Resource Manager
+.. warning::
+    This cloud provider will be removed from Salt in version 3007 in favor of
+    the `saltext.azurerm Salt Extension
+    <https://github.com/salt-extensions/saltext-azurerm>`_
+:maintainer: <devops@eitr.tech>
+:depends:
+    * `azure <https://pypi.python.org/pypi/azure>`_ >= 2.0.0rc6
+    * `azure-common <https://pypi.python.org/pypi/azure-common>`_ >= 1.1.4
+    * `azure-mgmt <https://pypi.python.org/pypi/azure-mgmt>`_ >= 0.30.0rc6
+    * `azure-mgmt-compute <https://pypi.python.org/pypi/azure-mgmt-compute>`_ >= 0.33.0
+    * `azure-mgmt-network <https://pypi.python.org/pypi/azure-mgmt-network>`_ >= 0.30.0rc6
+    * `azure-mgmt-resource <https://pypi.python.org/pypi/azure-mgmt-resource>`_ >= 0.30.0
+    * `azure-mgmt-storage <https://pypi.python.org/pypi/azure-mgmt-storage>`_ >= 0.30.0rc6
+    * `azure-mgmt-web <https://pypi.python.org/pypi/azure-mgmt-web>`_ >= 0.30.0rc6
+    * `azure-storage <https://pypi.python.org/pypi/azure-storage>`_ >= 0.32.0
+    * `msrestazure <https://pypi.python.org/pypi/msrestazure>`_ >= 0.4.21
+:configuration:
+    Required provider parameters:
+    if using username and password:
+      * ``subscription_id``
+      * ``username``
+      * ``password``
+    if using a service principal:
+      * ``subscription_id``
+      * ``tenant``
+      * ``client_id``
+      * ``secret``
+    if using Managed Service Identity authentication:
+      * ``subscription_id``
+    Optional provider parameters:
+    **cloud_environment**: Used to point the cloud driver to different API endpoints, such as Azure GovCloud. Possible values:
+      * ``AZURE_PUBLIC_CLOUD`` (default)
+      * ``AZURE_CHINA_CLOUD``
+      * ``AZURE_US_GOV_CLOUD``
+      * ``AZURE_GERMAN_CLOUD``
+      * HTTP base URL for a custom endpoint, such as Azure Stack. The ``/metadata/endpoints`` path will be added to the URL.
+    **userdata** and **userdata_file**:
+      Azure Resource Manager uses a separate VirtualMachineExtension object to pass userdata scripts to the virtual
+      machine. Arbitrary shell commands can be passed via the ``userdata`` parameter, or via a file local to the Salt
+      Cloud system using the ``userdata_file`` parameter. Note that the local file is not treated as a script by the
+      extension, so "one-liners" probably work best. If greater functionality is desired, a web-hosted script file can
+      be specified via ``userdata_file: https://raw.githubusercontent.com/account/repo/master/azure-script.py``, which
+      will be executed on the system after VM creation. For Windows systems, script files ending in ``.ps1`` will be
+      executed with ``powershell.exe``. The ``userdata`` parameter takes precedence over the ``userdata_file`` parameter
+      when creating the custom script extension.
+    **win_installer**:
+      This parameter, which holds the local path to the Salt Minion installer package, is used to determine if the
+      virtual machine type will be "Windows". Only set this parameter on profiles which install Windows operating systems.
+Example ``/etc/salt/cloud.providers`` or
+``/etc/salt/cloud.providers.d/azure.conf`` configuration:
+.. code-block:: yaml
+    my-azure-config with username and password:
+      driver: azurearm
+      subscription_id: 3287abc8-f98a-c678-3bde-326766fd3617
+      username: larry
+      password: 123pass
+    Or my-azure-config with service principal:
+      driver: azurearm
+      subscription_id: 3287abc8-f98a-c678-3bde-326766fd3617
+      tenant: ABCDEFAB-1234-ABCD-1234-ABCDEFABCDEF
+      client_id: ABCDEFAB-1234-ABCD-1234-ABCDEFABCDEF
+      secret: XXXXXXXXXXXXXXXXXXXXXXXX
+      cloud_environment: AZURE_US_GOV_CLOUD
+      The Service Principal can be created with the new Azure CLI (https://github.com/Azure/azure-cli) with:
+      az ad sp create-for-rbac -n "http://<yourappname>" --role <role> --scopes <scope>
+      For example, this creates a service principal with 'owner' role for the whole subscription:
+      az ad sp create-for-rbac -n "http://mysaltapp" --role owner --scopes /subscriptions/3287abc8-f98a-c678-3bde-326766fd3617
+      *Note: review the details of Service Principals. Owner role is more than you normally need, and you can restrict
+      scope to a resource group or individual resources.
+"""
+import importlib
+import logging
+import os
+import os.path
+import pprint
+import string
+import time
+from functools import wraps
+from multiprocessing import cpu_count
+from multiprocessing.pool import ThreadPool
+import salt.cache
+import salt.config as config
+import salt.loader
+import salt.utils.azurearm
+import salt.utils.cloud
+import salt.utils.files
+import salt.utils.stringutils
+import salt.utils.yaml
+import salt.version
+from salt.exceptions import (
+    SaltCloudConfigError,
+    SaltCloudExecutionFailure,
+    SaltCloudExecutionTimeout,
+    SaltCloudSystemExit,
+)
+HAS_LIBS = False
+try:
+    import azure.mgmt.compute.models as compute_models
+    import azure.mgmt.network.models as network_models
+    from azure.storage.blob.blockblobservice import BlockBlobService
+    from msrestazure.azure_exceptions import CloudError
+    HAS_LIBS = True
+except ImportError:
+    pass
+__virtualname__ = "azurearm"
+log = logging.getLogger(__name__)
+def __virtual__():
+    """
+    Check for Azure configurations.
+    """
+    if get_configured_provider() is False:
+        return False
+    if get_dependencies() is False:
+        return (
+            False,
+            "The following dependencies are required to use the AzureARM driver: "
+            "Microsoft Azure SDK for Python >= 2.0rc6, "
+            "Microsoft Azure Storage SDK for Python >= 0.32, "
+            "MS REST Azure (msrestazure) >= 0.4",
+        )
+    return __virtualname__
+def _get_active_provider_name():
+    try:
+        return __active_provider_name__.value()
+    except AttributeError:
+        return __active_provider_name__
+def _deprecation_message(function):
+    """
+    Decorator wrapper to warn about msazure deprecation
+    """
+    @wraps(function)
+    def wrapped(*args, **kwargs):
+        salt.utils.versions.warn_until(
+            "Chlorine",
+            "This cloud provider will be removed from Salt in version 3007 due to "
+            "the deprecation of the 'Classic' API for Azure. Please migrate to "
+            "Azure Resource Manager by March 1, 2023 "
+            "(https://docs.microsoft.com/en-us/azure/virtual-machines/classic-vm-deprecation)",
+            category=FutureWarning,
+        )
+        ret = function(*args, **salt.utils.args.clean_kwargs(**kwargs))
+        return ret
+    return wrapped
+@_deprecation_message
+def get_api_versions(call=None, kwargs=None):  # pylint: disable=unused-argument
+    """
+    Get a resource type api versions
+    """
+    if kwargs is None:
+        kwargs = {}
+    if "resource_provider" not in kwargs:
+        raise SaltCloudSystemExit("A resource_provider must be specified")
+    if "resource_type" not in kwargs:
+        raise SaltCloudSystemExit("A resource_type must be specified")
+    api_versions = []
+    try:
+        resconn = get_conn(client_type="resource")
+        provider_query = resconn.providers.get(
+            resource_provider_namespace=kwargs["resource_provider"]
+        )
+        for resource in provider_query.resource_types:
+            if str(resource.resource_type) == kwargs["resource_type"]:
+                resource_dict = resource.as_dict()
+                api_versions = resource_dict["api_versions"]
+    except CloudError as exc:
+        salt.utils.azurearm.log_cloud_error("resource", exc.message)
+    return api_versions
+@_deprecation_message
+def get_resource_by_id(resource_id, api_version, extract_value=None):
+    """
+    Get an AzureARM resource by id
+    """
+    ret = {}
+    try:
+        resconn = get_conn(client_type="resource")
+        resource_query = resconn.resources.get_by_id(
+            resource_id=resource_id, api_version=api_version
+        )
+        resource_dict = resource_query.as_dict()
+        if extract_value is not None:
+            ret = resource_dict[extract_value]
+        else:
+            ret = resource_dict
+    except CloudError as exc:
+        salt.utils.azurearm.log_cloud_error("resource", exc.message)
+        ret = {"Error": exc.message}
+    return ret
+def get_configured_provider():
+    """
+    Return the first configured provider instance.
+    """
+    key_combos = [
+        ("subscription_id", "tenant", "client_id", "secret"),
+        ("subscription_id", "username", "password"),
+        ("subscription_id",),
+    ]
+    for combo in key_combos:
+        provider = config.is_provider_configured(
+            __opts__,
+            _get_active_provider_name() or __virtualname__,
+            combo,
+        )
+        if provider:
+            return provider
+    return provider
+@_deprecation_message
+def get_dependencies():
+    """
+    Warn if dependencies aren't met.
+    """
+    return config.check_driver_dependencies(__virtualname__, {"azurearm": HAS_LIBS})
+@_deprecation_message
+def get_conn(client_type):
+    """
+    Return a connection object for a client type.
+    """
+    conn_kwargs = {}
+    conn_kwargs["subscription_id"] = salt.utils.stringutils.to_str(
+        config.get_cloud_config_value(
+            "subscription_id", get_configured_provider(), __opts__, search_global=False
+        )
+    )
+    cloud_env = config.get_cloud_config_value(
+        "cloud_environment", get_configured_provider(), __opts__, search_global=False
+    )
+    if cloud_env is not None:
+        conn_kwargs["cloud_environment"] = cloud_env
+    tenant = config.get_cloud_config_value(
+        "tenant", get_configured_provider(), __opts__, search_global=False
+    )
+    if tenant is not None:
+        client_id = config.get_cloud_config_value(
+            "client_id", get_configured_provider(), __opts__, search_global=False
+        )
+        secret = config.get_cloud_config_value(
+            "secret", get_configured_provider(), __opts__, search_global=False
+        )
+        conn_kwargs.update({"client_id": client_id, "secret": secret, "tenant": tenant})
+    username = config.get_cloud_config_value(
+        "username", get_configured_provider(), __opts__, search_global=False
+    )
+    if username:
+        password = config.get_cloud_config_value(
+            "password", get_configured_provider(), __opts__, search_global=False
+        )
+        conn_kwargs.update({"username": username, "password": password})
+    client = salt.utils.azurearm.get_client(client_type=client_type, **conn_kwargs)
+    return client
+@_deprecation_message
+def get_location(call=None, kwargs=None):  # pylint: disable=unused-argument
+    """
+    Return the location that is configured for this provider
+    """
+    if not kwargs:
+        kwargs = {}
+    vm_dict = get_configured_provider()
+    vm_dict.update(kwargs)
+    return config.get_cloud_config_value(
+        "location", vm_dict, __opts__, search_global=False
+    )
+@_deprecation_message
+def avail_locations(call=None):
+    """
+    Return a dict of all available regions.
+    """
+    if call == "action":
+        raise SaltCloudSystemExit(
+            "The avail_locations function must be called with "
+            "-f or --function, or with the --list-locations option"
+        )
+    ret = {}
+    ret["locations"] = []
+    try:
+        resconn = get_conn(client_type="resource")
+        provider_query = resconn.providers.get(
+            resource_provider_namespace="Microsoft.Compute"
+        )
+        locations = []
+        for resource in provider_query.resource_types:
+            if str(resource.resource_type) == "virtualMachines":
+                resource_dict = resource.as_dict()
+                locations = resource_dict["locations"]
+        for location in locations:
+            lowercase = location.lower().replace(" ", "")
+            ret["locations"].append(lowercase)
+    except CloudError as exc:
+        salt.utils.azurearm.log_cloud_error("resource", exc.message)
+        ret = {"Error": exc.message}
+    return ret
+@_deprecation_message
+def avail_images(call=None):
+    """
+    Return a dict of all available images on the provider
+    """
+    if call == "action":
+        raise SaltCloudSystemExit(
+            "The avail_images function must be called with "
+            "-f or --function, or with the --list-images option"
+        )
+    compconn = get_conn(client_type="compute")
+    region = get_location()
+    publishers = []
+    ret = {}
+    def _get_publisher_images(publisher):
+        """
+        Get all images from a specific publisher
+        """
+        data = {}
+        try:
+            offers = compconn.virtual_machine_images.list_offers(
+                location=region,
+                publisher_name=publisher,
+            )
+            for offer_obj in offers:
+                offer = offer_obj.as_dict()
+                skus = compconn.virtual_machine_images.list_skus(
+                    location=region,
+                    publisher_name=publisher,
+                    offer=offer["name"],
+                )
+                for sku_obj in skus:
+                    sku = sku_obj.as_dict()
+                    results = compconn.virtual_machine_images.list(
+                        location=region,
+                        publisher_name=publisher,
+                        offer=offer["name"],
+                        skus=sku["name"],
+                    )
+                    for version_obj in results:
+                        version = version_obj.as_dict()
+                        name = "|".join(
+                            (
+                                publisher,
+                                offer["name"],
+                                sku["name"],
+                                version["name"],
+                            )
+                        )
+                        data[name] = {
+                            "publisher": publisher,
+                            "offer": offer["name"],
+                            "sku": sku["name"],
+                            "version": version["name"],
+                        }
+        except CloudError as exc:
+            salt.utils.azurearm.log_cloud_error("compute", exc.message)
+            data = {publisher: exc.message}
+        return data
+    try:
+        publishers_query = compconn.virtual_machine_images.list_publishers(
+            location=region
+        )
+        for publisher_obj in publishers_query:
+            publisher = publisher_obj.as_dict()
+            publishers.append(publisher["name"])
+    except CloudError as exc:
+        salt.utils.azurearm.log_cloud_error("compute", exc.message)
+    pool = ThreadPool(cpu_count() * 6)
+    results = pool.map_async(_get_publisher_images, publishers)
+    results.wait()
+    ret = {k: v for result in results.get() for k, v in result.items()}
+    return ret
+@_deprecation_message
+def avail_sizes(call=None):
+    """
+    Return a list of sizes available from the provider
+    """
+    if call == "action":
+        raise SaltCloudSystemExit(
+            "The avail_sizes function must be called with "
+            "-f or --function, or with the --list-sizes option"
+        )
+    compconn = get_conn(client_type="compute")
+    ret = {}
+    location = get_location()
+    try:
+        sizes = compconn.virtual_machine_sizes.list(location=location)
+        for size_obj in sizes:
+            size = size_obj.as_dict()
+            ret[size["name"]] = size
+    except CloudError as exc:
+        salt.utils.azurearm.log_cloud_error("compute", exc.message)
+        ret = {"Error": exc.message}
+    return ret
+@_deprecation_message
+def list_nodes(call=None):
+    """
+    List VMs on this Azure account
+    """
+    if call == "action":
+        raise SaltCloudSystemExit(
+            "The list_nodes function must be called with -f or --function."
+        )
+    ret = {}
+    nodes = list_nodes_full()
+    for node in nodes:
+        ret[node] = {"name": node}
+        for prop in ("id", "image", "size", "state", "private_ips", "public_ips"):
+            ret[node][prop] = nodes[node].get(prop)
+    return ret
+@_deprecation_message
+def list_nodes_full(call=None):
+    """
+    List all VMs on the subscription with full information
+    """
+    if call == "action":
+        raise SaltCloudSystemExit(
+            "The list_nodes_full function must be called with -f or --function."
+        )
+    netapi_versions = get_api_versions(
+        kwargs={
+            "resource_provider": "Microsoft.Network",
+            "resource_type": "networkInterfaces",
+        }
+    )
+    netapi_version = netapi_versions[0]
+    compconn = get_conn(client_type="compute")
+    ret = {}
+    def _get_node_info(node):
+        """
+        Get node info.
+        """
+        node_ret = {}
+        node["id"] = node["vm_id"]
+        node["size"] = node["hardware_profile"]["vm_size"]
+        node["state"] = node["provisioning_state"]
+        node["public_ips"] = []
+        node["private_ips"] = []
+        node_ret[node["name"]] = node
+        try:
+            image_ref = node["storage_profile"]["image_reference"]
+            node["image"] = "|".join(
+                [
+                    image_ref["publisher"],
+                    image_ref["offer"],
+                    image_ref["sku"],
+                    image_ref["version"],
+                ]
+            )
+        except (TypeError, KeyError):
+            try:
+                node["image"] = node["storage_profile"]["os_disk"]["image"]["uri"]
+            except (TypeError, KeyError):
+                node["image"] = (
+                    node.get("storage_profile", {}).get("image_reference", {}).get("id")
+                )
+        try:
+            netifaces = node["network_profile"]["network_interfaces"]
+            for index, netiface in enumerate(netifaces):
+                netiface_name = get_resource_by_id(
+                    netiface["id"], netapi_version, "name"
+                )
+                netiface, pubips, privips = _get_network_interface(
+                    netiface_name, node["resource_group"]
+                )
+                node["network_profile"]["network_interfaces"][index].update(netiface)
+                node["public_ips"].extend(pubips)
+                node["private_ips"].extend(privips)
+        except Exception:  # pylint: disable=broad-except
+            pass
+        node_ret[node["name"]] = node
+        return node_ret
+    for group in list_resource_groups():
+        nodes = []
+        nodes_query = compconn.virtual_machines.list(resource_group_name=group)
+        for node_obj in nodes_query:
+            node = node_obj.as_dict()
+            node["resource_group"] = group
+            nodes.append(node)
+        pool = ThreadPool(cpu_count() * 6)
+        results = pool.map_async(_get_node_info, nodes)
+        results.wait()
+        group_ret = {k: v for result in results.get() for k, v in result.items()}
+        ret.update(group_ret)
+    return ret
+@_deprecation_message
+def list_resource_groups(call=None):
+    """
+    List resource groups associated with the subscription
+    """
+    if call == "action":
+        raise SaltCloudSystemExit(
+            "The list_hosted_services function must be called with -f or --function"
+        )
+    resconn = get_conn(client_type="resource")
+    ret = {}
+    try:
+        groups = resconn.resource_groups.list()
+        for group_obj in groups:
+            group = group_obj.as_dict()
+            ret[group["name"]] = group
+    except CloudError as exc:
+        salt.utils.azurearm.log_cloud_error("resource", exc.message)
+        ret = {"Error": exc.message}
+    return ret
+@_deprecation_message
+def show_instance(name, call=None):
+    """
+    Show the details from AzureARM concerning an instance
+    """
+    if call != "action":
+        raise SaltCloudSystemExit(
+            "The show_instance action must be called with -a or --action."
+        )
+    try:
+        node = list_nodes_full("function")[name]
+    except KeyError:
+        log.debug("Failed to get data for node '%s'", name)
+        node = {}
+    __utils__["cloud.cache_node"](node, _get_active_provider_name(), __opts__)
+    return node
+@_deprecation_message
+def delete_interface(call=None, kwargs=None):  # pylint: disable=unused-argument
+    """
+    Delete a network interface.
+    """
+    if kwargs is None:
+        kwargs = {}
+    netconn = get_conn(client_type="network")
+    if kwargs.get("resource_group") is None:
+        kwargs["resource_group"] = config.get_cloud_config_value(
+            "resource_group", {}, __opts__, search_global=True
+        )
+    ips = []
+    iface = netconn.network_interfaces.get(
+        kwargs["resource_group"],
+        kwargs["iface_name"],
+    )
+    iface_name = iface.name
+    for ip_ in iface.ip_configurations:
+        ips.append(ip_.name)
+    poller = netconn.network_interfaces.delete(
+        kwargs["resource_group"],
+        kwargs["iface_name"],
+    )
+    poller.wait()
+    for ip_ in ips:
+        poller = netconn.public_ip_addresses.delete(kwargs["resource_group"], ip_)
+        poller.wait()
+    return {iface_name: ips}
+def _get_public_ip(name, resource_group):
+    """
+    Get the public ip address details by name.
+    """
+    netconn = get_conn(client_type="network")
+    try:
+        pubip_query = netconn.public_ip_addresses.get(
+            resource_group_name=resource_group, public_ip_address_name=name
+        )
+        pubip = pubip_query.as_dict()
+    except CloudError as exc:
+        salt.utils.azurearm.log_cloud_error("network", exc.message)
+        pubip = {"error": exc.message}
+    return pubip
+def _get_network_interface(name, resource_group):
+    """
+    Get a network interface.
+    """
+    public_ips = []
+    private_ips = []
+    netapi_versions = get_api_versions(
+        kwargs={
+            "resource_provider": "Microsoft.Network",
+            "resource_type": "publicIPAddresses",
+        }
+    )
+    netapi_version = netapi_versions[0]
+    netconn = get_conn(client_type="network")
+    netiface_query = netconn.network_interfaces.get(
+        resource_group_name=resource_group, network_interface_name=name
+    )
+    netiface = netiface_query.as_dict()
+    for index, ip_config in enumerate(netiface["ip_configurations"]):
+        if ip_config.get("private_ip_address") is not None:
+            private_ips.append(ip_config["private_ip_address"])
+        if "id" in ip_config.get("public_ip_address", {}):
+            public_ip_name = get_resource_by_id(
+                ip_config["public_ip_address"]["id"], netapi_version, "name"
+            )
+            public_ip = _get_public_ip(public_ip_name, resource_group)
+            public_ips.append(public_ip["ip_address"])
+            netiface["ip_configurations"][index]["public_ip_address"].update(public_ip)
+    return netiface, public_ips, private_ips
+@_deprecation_message
+def create_network_interface(call=None, kwargs=None):
+    """
+    Create a network interface.
+    """
+    if call != "action":
+        raise SaltCloudSystemExit(
+            "The create_network_interface action must be called with -a or --action."
+        )
+    IPAllocationMethod = getattr(network_models, "IPAllocationMethod")
+    NetworkInterface = getattr(network_models, "NetworkInterface")
+    NetworkInterfaceIPConfiguration = getattr(
+        network_models, "NetworkInterfaceIPConfiguration"
+    )
+    PublicIPAddress = getattr(network_models, "PublicIPAddress")
+    if not isinstance(kwargs, dict):
+        kwargs = {}
+    vm_ = kwargs
+    netconn = get_conn(client_type="network")
+    if kwargs.get("location") is None:
+        kwargs["location"] = get_location()
+    if kwargs.get("network") is None:
+        kwargs["network"] = config.get_cloud_config_value(
+            "network", vm_, __opts__, search_global=False
+        )
+    if kwargs.get("subnet") is None:
+        kwargs["subnet"] = config.get_cloud_config_value(
+            "subnet", vm_, __opts__, search_global=False
+        )
+    if kwargs.get("network_resource_group") is None:
+        kwargs["network_resource_group"] = config.get_cloud_config_value(
+            "resource_group", vm_, __opts__, search_global=False
+        )
+    if kwargs.get("iface_name") is None:
+        kwargs["iface_name"] = "{}-iface0".format(vm_["name"])
+    try:
+        subnet_obj = netconn.subnets.get(
+            resource_group_name=kwargs["network_resource_group"],
+            virtual_network_name=kwargs["network"],
+            subnet_name=kwargs["subnet"],
+        )
+    except CloudError as exc:
+        raise SaltCloudSystemExit(
+            '{} (Resource Group: "{}", VNET: "{}", Subnet: "{}")'.format(
+                exc.message,
+                kwargs["network_resource_group"],
+                kwargs["network"],
+                kwargs["subnet"],
+            )
+        )
+    ip_kwargs = {}
+    ip_configurations = None
+    if "load_balancer_backend_address_pools" in kwargs:
+        pool_dicts = kwargs["load_balancer_backend_address_pools"]
+        if isinstance(pool_dicts, dict):
+            pool_ids = []
+            for load_bal, be_pools in pool_dicts.items():
+                for pool in be_pools:
+                    try:
+                        lbbep_data = netconn.load_balancer_backend_address_pools.get(
+                            kwargs["resource_group"],
+                            load_bal,
+                            pool,
+                        )
+                        pool_ids.append({"id": lbbep_data.as_dict()["id"]})
+                    except CloudError as exc:
+                        log.error("There was a cloud error: %s", str(exc))
+                    except KeyError as exc:
+                        log.error(
+                            "There was an error getting the Backend Pool ID: %s",
+                            str(exc),
+                        )
+            ip_kwargs["load_balancer_backend_address_pools"] = pool_ids
+    if "private_ip_address" in kwargs.keys():
+        ip_kwargs["private_ip_address"] = kwargs["private_ip_address"]
+        ip_kwargs["private_ip_allocation_method"] = IPAllocationMethod.static
+    else:
+        ip_kwargs["private_ip_allocation_method"] = IPAllocationMethod.dynamic
+    if kwargs.get("allocate_public_ip") is True:
+        pub_ip_name = "{}-ip".format(kwargs["iface_name"])
+        poller = netconn.public_ip_addresses.create_or_update(
+            resource_group_name=kwargs["resource_group"],
+            public_ip_address_name=pub_ip_name,
+            parameters=PublicIPAddress(
+                location=kwargs["location"],
+                public_ip_allocation_method=IPAllocationMethod.static,
+            ),
+        )
+        count = 0
+        poller.wait()
+        while True:
+            try:
+                pub_ip_data = netconn.public_ip_addresses.get(
+                    kwargs["resource_group"],
+                    pub_ip_name,
+                )
+                if pub_ip_data.ip_address:  # pylint: disable=no-member
+                    ip_kwargs["public_ip_address"] = PublicIPAddress(
+                        id=str(pub_ip_data.id),  # pylint: disable=no-member
+                    )
+                    ip_configurations = [
+                        NetworkInterfaceIPConfiguration(
+                            name="{}-ip".format(kwargs["iface_name"]),
+                            subnet=subnet_obj,
+                            **ip_kwargs,
+                        )
+                    ]
+                    break
+            except CloudError as exc:
+                log.error("There was a cloud error: %s", exc)
+            count += 1
+            if count > 120:
+                raise ValueError("Timed out waiting for public IP Address.")
+            time.sleep(5)
+    else:
+        priv_ip_name = "{}-ip".format(kwargs["iface_name"])
+        ip_configurations = [
+            NetworkInterfaceIPConfiguration(
+                name=priv_ip_name, subnet=subnet_obj, **ip_kwargs
+            )
+        ]
+    network_security_group = None
+    if kwargs.get("security_group") is not None:
+        network_security_group = netconn.network_security_groups.get(
+            resource_group_name=kwargs["resource_group"],
+            network_security_group_name=kwargs["security_group"],
+        )
+    iface_params = NetworkInterface(
+        location=kwargs["location"],
+        network_security_group=network_security_group,
+        ip_configurations=ip_configurations,
+    )
+    poller = netconn.network_interfaces.create_or_update(
+        kwargs["resource_group"], kwargs["iface_name"], iface_params
+    )
+    try:
+        poller.wait()
+    except Exception as exc:  # pylint: disable=broad-except
+        log.warning(
+            "Network interface creation could not be polled. "
+            "It is likely that we are reusing an existing interface. (%s)",
+            exc,
+        )
+    count = 0
+    while True:
+        try:
+            return _get_network_interface(
+                kwargs["iface_name"], kwargs["resource_group"]
+            )
+        except CloudError:
+            count += 1
+            if count > 120:
+                raise ValueError("Timed out waiting for operation to complete.")
+            time.sleep(5)
+def request_instance(vm_, kwargs=None):
+    """
+    Request a VM from Azure.
+    """
+    compconn = get_conn(client_type="compute")
+    CachingTypes = getattr(compute_models, "CachingTypes")
+    DataDisk = getattr(compute_models, "DataDisk")
+    DiskCreateOptionTypes = getattr(compute_models, "DiskCreateOptionTypes")
+    HardwareProfile = getattr(compute_models, "HardwareProfile")
+    ImageReference = getattr(compute_models, "ImageReference")
+    LinuxConfiguration = getattr(compute_models, "LinuxConfiguration")
+    SshConfiguration = getattr(compute_models, "SshConfiguration")
+    SshPublicKey = getattr(compute_models, "SshPublicKey")
+    NetworkInterfaceReference = getattr(compute_models, "NetworkInterfaceReference")
+    NetworkProfile = getattr(compute_models, "NetworkProfile")
+    OSDisk = getattr(compute_models, "OSDisk")
+    OSProfile = getattr(compute_models, "OSProfile")
+    StorageProfile = getattr(compute_models, "StorageProfile")
+    VirtualHardDisk = getattr(compute_models, "VirtualHardDisk")
+    VirtualMachine = getattr(compute_models, "VirtualMachine")
+    VirtualMachineSizeTypes = getattr(compute_models, "VirtualMachineSizeTypes")
+    subscription_id = config.get_cloud_config_value(
+        "subscription_id", get_configured_provider(), __opts__, search_global=False
+    )
+    if vm_.get("driver") is None:
+        vm_["driver"] = "azurearm"
+    if vm_.get("location") is None:
+        vm_["location"] = get_location()
+    if vm_.get("resource_group") is None:
+        vm_["resource_group"] = config.get_cloud_config_value(
+            "resource_group", vm_, __opts__, search_global=True
+        )
+    if vm_.get("name") is None:
+        vm_["name"] = config.get_cloud_config_value(
+            "name", vm_, __opts__, search_global=True
+        )
+    iface_data, public_ips, private_ips = create_network_interface(
+        call="action", kwargs=vm_
+    )
+    vm_["iface_id"] = iface_data["id"]
+    disk_name = "{}-vol0".format(vm_["name"])
+    vm_username = config.get_cloud_config_value(
+        "ssh_username",
+        vm_,
+        __opts__,
+        search_global=True,
+        default=config.get_cloud_config_value(
+            "win_username", vm_, __opts__, search_global=True
+        ),
+    )
+    ssh_publickeyfile_contents = None
+    ssh_publickeyfile = config.get_cloud_config_value(
+        "ssh_publickeyfile", vm_, __opts__, search_global=False, default=None
+    )
+    if ssh_publickeyfile is not None:
+        try:
+            with salt.utils.files.fopen(ssh_publickeyfile, "r") as spkc_:
+                ssh_publickeyfile_contents = spkc_.read()
+        except Exception as exc:  # pylint: disable=broad-except
+            raise SaltCloudConfigError(
+                "Failed to read ssh publickey file '{}': {}".format(
+                    ssh_publickeyfile, exc.args[-1]
+                )
+            )
+    disable_password_authentication = config.get_cloud_config_value(
+        "disable_password_authentication",
+        vm_,
+        __opts__,
+        search_global=False,
+        default=False,
+    )
+    os_kwargs = {}
+    win_installer = config.get_cloud_config_value(
+        "win_installer", vm_, __opts__, search_global=True
+    )
+    if not win_installer and ssh_publickeyfile_contents is not None:
+        sshpublickey = SshPublicKey(
+            key_data=ssh_publickeyfile_contents,
+            path=f"/home/{vm_username}/.ssh/authorized_keys",
+        )
+        sshconfiguration = SshConfiguration(
+            public_keys=[sshpublickey],
+        )
+        linuxconfiguration = LinuxConfiguration(
+            disable_password_authentication=disable_password_authentication,
+            ssh=sshconfiguration,
+        )
+        os_kwargs["linux_configuration"] = linuxconfiguration
+        vm_password = None
+    else:
+        vm_password = salt.utils.stringutils.to_str(
+            config.get_cloud_config_value(
+                "ssh_password",
+                vm_,
+                __opts__,
+                search_global=True,
+                default=config.get_cloud_config_value(
+                    "win_password", vm_, __opts__, search_global=True
+                ),
+            )
+        )
+    if win_installer or (
+        vm_password is not None and not disable_password_authentication
+    ):
+        if not isinstance(vm_password, str):
+            raise SaltCloudSystemExit("The admin password must be a string.")
+        if len(vm_password) < 8 or len(vm_password) > 123:
+            raise SaltCloudSystemExit(
+                "The admin password must be between 8-123 characters long."
+            )
+        complexity = 0
+        if any(char.isdigit() for char in vm_password):
+            complexity += 1
+        if any(char.isupper() for char in vm_password):
+            complexity += 1
+        if any(char.islower() for char in vm_password):
+            complexity += 1
+        if any(char in string.punctuation for char in vm_password):
+            complexity += 1
+        if complexity < 3:
+            raise SaltCloudSystemExit(
+                "The admin password must contain at least 3 of the following types: "
+                "upper, lower, digits, special characters"
+            )
+        os_kwargs["admin_password"] = vm_password
+    availability_set = config.get_cloud_config_value(
+        "availability_set", vm_, __opts__, search_global=False, default=None
+    )
+    if availability_set is not None and isinstance(availability_set, str):
+        availability_set = {
+            "id": "/subscriptions/{}/resourceGroups/{}/providers/Microsoft.Compute/availabilitySets/{}".format(
+                subscription_id, vm_["resource_group"], availability_set
+            )
+        }
+    else:
+        availability_set = None
+    cloud_env = _get_cloud_environment()
+    storage_endpoint_suffix = cloud_env.suffixes.storage_endpoint
+    if isinstance(vm_.get("volumes"), str):
+        volumes = salt.utils.yaml.safe_load(vm_["volumes"])
+    else:
+        volumes = vm_.get("volumes")
+    data_disks = None
+    if isinstance(volumes, list):
+        data_disks = []
+    else:
+        volumes = []
+    lun = 0
+    luns = []
+    for volume in volumes:
+        if isinstance(volume, str):
+            volume = {"name": volume}
+        volume.setdefault(
+            "name",
+            volume.get(
+                "name",
+                volume.get("name", "{}-datadisk{}".format(vm_["name"], str(lun))),
+            ),
+        )
+        volume.setdefault(
+            "disk_size_gb",
+            volume.get("logical_disk_size_in_gb", volume.get("size", 100)),
+        )
+        volume.setdefault("caching", volume.get("host_caching", "ReadOnly"))
+        while lun in luns:
+            lun += 1
+            if lun > 15:
+                log.error("Maximum lun count has been reached")
+                break
+        volume.setdefault("lun", lun)
+        lun += 1
+        if "media_link" in volume:
+            volume["vhd"] = VirtualHardDisk(uri=volume["media_link"])
+            del volume["media_link"]
+        elif volume.get("vhd") == "unmanaged":
+            volume["vhd"] = VirtualHardDisk(
+                uri="https://{}.blob.{}/vhds/{}-datadisk{}.vhd".format(
+                    vm_["storage_account"],
+                    storage_endpoint_suffix,
+                    vm_["name"],
+                    volume["lun"],
+                ),
+            )
+        elif "vhd" in volume:
+            volume["vhd"] = VirtualHardDisk(uri=volume["vhd"])
+        if "image" in volume:
+            volume["create_option"] = "from_image"
+        elif "attach" in volume:
+            volume["create_option"] = "attach"
+        else:
+            volume["create_option"] = "empty"
+        data_disks.append(DataDisk(**volume))
+    img_ref = None
+    if vm_["image"].startswith("http") or vm_.get("vhd") == "unmanaged":
+        if vm_["image"].startswith("http"):
+            source_image = VirtualHardDisk(uri=vm_["image"])
+        else:
+            source_image = None
+            if "|" in vm_["image"]:
+                img_pub, img_off, img_sku, img_ver = vm_["image"].split("|")
+                img_ref = ImageReference(
+                    publisher=img_pub,
+                    offer=img_off,
+                    sku=img_sku,
+                    version=img_ver,
+                )
+            elif vm_["image"].startswith("/subscriptions"):
+                img_ref = ImageReference(id=vm_["image"])
+        if win_installer:
+            os_type = "Windows"
+        else:
+            os_type = "Linux"
+        os_disk = OSDisk(
+            caching=CachingTypes.none,
+            create_option=DiskCreateOptionTypes.from_image,
+            name=disk_name,
+            vhd=VirtualHardDisk(
+                uri="https://{}.blob.{}/vhds/{}.vhd".format(
+                    vm_["storage_account"],
+                    storage_endpoint_suffix,
+                    disk_name,
+                ),
+            ),
+            os_type=os_type,
+            image=source_image,
+            disk_size_gb=vm_.get("os_disk_size_gb"),
+        )
+    else:
+        source_image = None
+        os_type = None
+        os_disk = OSDisk(
+            create_option=DiskCreateOptionTypes.from_image,
+            disk_size_gb=vm_.get("os_disk_size_gb"),
+        )
+        if "|" in vm_["image"]:
+            img_pub, img_off, img_sku, img_ver = vm_["image"].split("|")
+            img_ref = ImageReference(
+                publisher=img_pub,
+                offer=img_off,
+                sku=img_sku,
+                version=img_ver,
+            )
+        elif vm_["image"].startswith("/subscriptions"):
+            img_ref = ImageReference(id=vm_["image"])
+    userdata_file = config.get_cloud_config_value(
+        "userdata_file", vm_, __opts__, search_global=False, default=None
+    )
+    userdata = config.get_cloud_config_value(
+        "userdata", vm_, __opts__, search_global=False, default=None
+    )
+    userdata_template = config.get_cloud_config_value(
+        "userdata_template", vm_, __opts__, search_global=False, default=None
+    )
+    if userdata_file:
+        if os.path.exists(userdata_file):
+            with salt.utils.files.fopen(userdata_file, "r") as fh_:
+                userdata = fh_.read()
+    if userdata and userdata_template:
+        userdata_sendkeys = config.get_cloud_config_value(
+            "userdata_sendkeys", vm_, __opts__, search_global=False, default=None
+        )
+        if userdata_sendkeys:
+            vm_["priv_key"], vm_["pub_key"] = salt.utils.cloud.gen_keys(
+                config.get_cloud_config_value("keysize", vm_, __opts__)
+            )
+            key_id = vm_.get("name")
+            if "append_domain" in vm_:
+                key_id = ".".join([key_id, vm_["append_domain"]])
+            salt.utils.cloud.accept_key(__opts__["pki_dir"], vm_["pub_key"], key_id)
+        userdata = salt.utils.cloud.userdata_template(__opts__, vm_, userdata)
+    custom_extension = None
+    if userdata is not None or userdata_file is not None:
+        try:
+            if win_installer:
+                publisher = "Microsoft.Compute"
+                virtual_machine_extension_type = "CustomScriptExtension"
+                type_handler_version = "1.8"
+                if userdata_file and userdata_file.endswith(".ps1"):
+                    command_prefix = "powershell -ExecutionPolicy Unrestricted -File "
+                else:
+                    command_prefix = ""
+            else:
+                publisher = "Microsoft.Azure.Extensions"
+                virtual_machine_extension_type = "CustomScript"
+                type_handler_version = "2.0"
+                command_prefix = ""
+            settings = {}
+            if userdata:
+                settings["commandToExecute"] = userdata
+            elif userdata_file.startswith("http"):
+                settings["fileUris"] = [userdata_file]
+                settings["commandToExecute"] = (
+                    command_prefix
+                    + "./"
+                    + userdata_file[userdata_file.rfind("/") + 1 :]
+                )
+            custom_extension = {
+                "resource_group": vm_["resource_group"],
+                "virtual_machine_name": vm_["name"],
+                "extension_name": vm_["name"] + "_custom_userdata_script",
+                "location": vm_["location"],
+                "publisher": publisher,
+                "virtual_machine_extension_type": virtual_machine_extension_type,
+                "type_handler_version": type_handler_version,
+                "auto_upgrade_minor_version": True,
+                "settings": settings,
+                "protected_settings": None,
+            }
+        except Exception as exc:  # pylint: disable=broad-except
+            log.exception("Failed to encode userdata: %s", exc)
+    params = VirtualMachine(
+        location=vm_["location"],
+        plan=None,
+        hardware_profile=HardwareProfile(
+            vm_size=getattr(VirtualMachineSizeTypes, vm_["size"].lower(), kwargs),
+        ),
+        storage_profile=StorageProfile(
+            os_disk=os_disk,
+            data_disks=data_disks,
+            image_reference=img_ref,
+        ),
+        os_profile=OSProfile(
+            admin_username=vm_username, computer_name=vm_["name"], **os_kwargs
+        ),
+        network_profile=NetworkProfile(
+            network_interfaces=[NetworkInterfaceReference(id=vm_["iface_id"])],
+        ),
+        availability_set=availability_set,
+    )
+    __utils__["cloud.fire_event"](
+        "event",
+        "requesting instance",
+        "salt/cloud/{}/requesting".format(vm_["name"]),
+        args=__utils__["cloud.filter_event"](
+            "requesting", vm_, ["name", "profile", "provider", "driver"]
+        ),
+        sock_dir=__opts__["sock_dir"],
+        transport=__opts__["transport"],
+    )
+    try:
+        vm_create = compconn.virtual_machines.create_or_update(
+            resource_group_name=vm_["resource_group"],
+            vm_name=vm_["name"],
+            parameters=params,
+        )
+        vm_create.wait()
+        vm_result = vm_create.result()
+        vm_result = vm_result.as_dict()
+        if custom_extension:
+            create_or_update_vmextension(kwargs=custom_extension)
+    except CloudError as exc:
+        salt.utils.azurearm.log_cloud_error("compute", exc.message)
+        vm_result = {}
+    return vm_result
+@_deprecation_message
+def create(vm_):
+    """
+    Create a single VM from a data dict.
+    """
+    try:
+        if (
+            vm_["profile"]
+            and config.is_profile_configured(
+                __opts__,
+                _get_active_provider_name() or "azurearm",
+                vm_["profile"],
+                vm_=vm_,
+            )
+            is False
+        ):
+            return False
+    except AttributeError:
+        pass
+    if vm_.get("bootstrap_interface") is None:
+        vm_["bootstrap_interface"] = "public"
+    __utils__["cloud.fire_event"](
+        "event",
+        "starting create",
+        "salt/cloud/{}/creating".format(vm_["name"]),
+        args=__utils__["cloud.filter_event"](
+            "creating", vm_, ["name", "profile", "provider", "driver"]
+        ),
+        sock_dir=__opts__["sock_dir"],
+        transport=__opts__["transport"],
+    )
+    __utils__["cloud.cachedir_index_add"](
+        vm_["name"], vm_["profile"], "azurearm", vm_["driver"]
+    )
+    if not vm_.get("location"):
+        vm_["location"] = get_location(kwargs=vm_)
+    log.info("Creating Cloud VM %s in %s", vm_["name"], vm_["location"])
+    vm_request = request_instance(vm_=vm_)
+    if not vm_request or "error" in vm_request:
+        err_message = "Error creating VM {}! ({})".format(vm_["name"], str(vm_request))
+        log.error(err_message)
+        raise SaltCloudSystemExit(err_message)
+    def _query_node_data(name, bootstrap_interface):
+        """
+        Query node data.
+        """
+        data = show_instance(name, call="action")
+        if not data:
+            return False
+        ip_address = None
+        if bootstrap_interface == "public":
+            ip_address = data["public_ips"][0]
+        if bootstrap_interface == "private":
+            ip_address = data["private_ips"][0]
+        if ip_address is None:
+            return False
+        return ip_address
+    try:
+        data = salt.utils.cloud.wait_for_ip(
+            _query_node_data,
+            update_args=(
+                vm_["name"],
+                vm_["bootstrap_interface"],
+            ),
+            timeout=config.get_cloud_config_value(
+                "wait_for_ip_timeout", vm_, __opts__, default=10 * 60
+            ),
+            interval=config.get_cloud_config_value(
+                "wait_for_ip_interval", vm_, __opts__, default=10
+            ),
+            interval_multiplier=config.get_cloud_config_value(
+                "wait_for_ip_interval_multiplier", vm_, __opts__, default=1
+            ),
+        )
+    except (
+        SaltCloudExecutionTimeout,
+        SaltCloudExecutionFailure,
+        SaltCloudSystemExit,
+    ) as exc:
+        try:
+            log.warning(exc)
+        finally:
+            raise SaltCloudSystemExit(str(exc))
+    vm_["ssh_host"] = data
+    if not vm_.get("ssh_username"):
+        vm_["ssh_username"] = config.get_cloud_config_value(
+            "ssh_username", vm_, __opts__
+        )
+    vm_["password"] = config.get_cloud_config_value("ssh_password", vm_, __opts__)
+    ret = __utils__["cloud.bootstrap"](vm_, __opts__)
+    data = show_instance(vm_["name"], call="action")
+    log.info("Created Cloud VM '%s'", vm_["name"])
+    log.debug("'%s' VM creation details:\n%s", vm_["name"], pprint.pformat(data))
+    ret.update(data)
+    __utils__["cloud.fire_event"](
+        "event",
+        "created instance",
+        "salt/cloud/{}/created".format(vm_["name"]),
+        args=__utils__["cloud.filter_event"](
+            "created", vm_, ["name", "profile", "provider", "driver"]
+        ),
+        sock_dir=__opts__["sock_dir"],
+        transport=__opts__["transport"],
+    )
+    return ret
+@_deprecation_message
+def destroy(name, call=None, kwargs=None):  # pylint: disable=unused-argument
+    """
+    Destroy a VM.
+    CLI Examples:
+    .. code-block:: bash
+        salt-cloud -d myminion
+        salt-cloud -a destroy myminion service_name=myservice
+    """
+    if kwargs is None:
+        kwargs = {}
+    if call == "function":
+        raise SaltCloudSystemExit(
+            "The destroy action must be called with -d, --destroy, -a or --action."
+        )
+    compconn = get_conn(client_type="compute")
+    node_data = show_instance(name, call="action")
+    if node_data["storage_profile"]["os_disk"].get("managed_disk"):
+        vhd = node_data["storage_profile"]["os_disk"]["managed_disk"]["id"]
+    else:
+        vhd = node_data["storage_profile"]["os_disk"]["vhd"]["uri"]
+    ret = {name: {}}
+    log.debug("Deleting VM")
+    result = compconn.virtual_machines.delete(node_data["resource_group"], name)
+    result.wait()
+    if __opts__.get("update_cachedir", False) is True:
+        __utils__["cloud.delete_minion_cachedir"](
+            name, _get_active_provider_name().split(":")[0], __opts__
+        )
+    cleanup_disks = config.get_cloud_config_value(
+        "cleanup_disks",
+        get_configured_provider(),
+        __opts__,
+        search_global=False,
+        default=False,
+    )
+    if cleanup_disks:
+        cleanup_vhds = kwargs.get(
+            "delete_vhd",
+            config.get_cloud_config_value(
+                "cleanup_vhds",
+                get_configured_provider(),
+                __opts__,
+                search_global=False,
+                default=False,
+            ),
+        )
+        if cleanup_vhds:
+            log.debug("Deleting vhd")
+            comps = vhd.split("/")
+            container = comps[-2]
+            blob = comps[-1]
+            ret[name]["delete_disk"] = {
+                "delete_disks": cleanup_disks,
+                "delete_vhd": cleanup_vhds,
+                "container": container,
+                "blob": blob,
+            }
+            if vhd.startswith("http"):
+                ret[name]["data"] = delete_blob(
+                    kwargs={"container": container, "blob": blob}, call="function"
+                )
+            else:
+                ret[name]["data"] = delete_managed_disk(
+                    kwargs={
+                        "resource_group": node_data["resource_group"],
+                        "container": container,
+                        "blob": blob,
+                    },
+                    call="function",
+                )
+        cleanup_data_disks = kwargs.get(
+            "delete_data_disks",
+            config.get_cloud_config_value(
+                "cleanup_data_disks",
+                get_configured_provider(),
+                __opts__,
+                search_global=False,
+                default=False,
+            ),
+        )
+        if cleanup_data_disks:
+            log.debug("Deleting data_disks")
+            ret[name]["data_disks"] = {}
+            for disk in node_data["storage_profile"]["data_disks"]:
+                datavhd = disk.get("managed_disk", {}).get("id") or disk.get(
+                    "vhd", {}
+                ).get("uri")
+                comps = datavhd.split("/")
+                container = comps[-2]
+                blob = comps[-1]
+                ret[name]["data_disks"][disk["name"]] = {
+                    "delete_disks": cleanup_disks,
+                    "delete_vhd": cleanup_vhds,
+                    "container": container,
+                    "blob": blob,
+                }
+                if datavhd.startswith("http"):
+                    ret[name]["data"] = delete_blob(
+                        kwargs={"container": container, "blob": blob}, call="function"
+                    )
+                else:
+                    ret[name]["data"] = delete_managed_disk(
+                        kwargs={
+                            "resource_group": node_data["resource_group"],
+                            "container": container,
+                            "blob": blob,
+                        },
+                        call="function",
+                    )
+    cleanup_interfaces = config.get_cloud_config_value(
+        "cleanup_interfaces",
+        get_configured_provider(),
+        __opts__,
+        search_global=False,
+        default=False,
+    )
+    if cleanup_interfaces:
+        ret[name]["cleanup_network"] = {
+            "cleanup_interfaces": cleanup_interfaces,
+            "resource_group": node_data["resource_group"],
+            "data": [],
+        }
+        ifaces = node_data["network_profile"]["network_interfaces"]
+        for iface in ifaces:
+            resource_group = iface["id"].split("/")[4]
+            ret[name]["cleanup_network"]["data"].append(
+                delete_interface(
+                    kwargs={
+                        "resource_group": resource_group,
+                        "iface_name": iface["name"],
+                    },
+                    call="function",
+                )
+            )
+    return ret
+@_deprecation_message
+def list_storage_accounts(call=None):
+    """
+    List storage accounts within the subscription.
+    """
+    if call == "action":
+        raise SaltCloudSystemExit(
+            "The list_storage_accounts function must be called with -f or --function"
+        )
+    storconn = get_conn(client_type="storage")
+    ret = {}
+    try:
+        accounts_query = storconn.storage_accounts.list()
+        accounts = salt.utils.azurearm.paged_object_to_list(accounts_query)
+        for account in accounts:
+            ret[account["name"]] = account
+    except CloudError as exc:
+        salt.utils.azurearm.log_cloud_error("storage", exc.message)
+        ret = {"Error": exc.message}
+    return ret
+def _get_cloud_environment():
+    """
+    Get the cloud environment object.
+    """
+    cloud_environment = config.get_cloud_config_value(
+        "cloud_environment", get_configured_provider(), __opts__, search_global=False
+    )
+    try:
+        cloud_env_module = importlib.import_module("msrestazure.azure_cloud")
+        cloud_env = getattr(cloud_env_module, cloud_environment or "AZURE_PUBLIC_CLOUD")
+    except (AttributeError, ImportError):
+        raise SaltCloudSystemExit(
+            f"The azure {cloud_environment} cloud environment is not available."
+        )
+    return cloud_env
+def _get_block_blob_service(kwargs=None):
+    """
+    Get the block blob storage service.
+    """
+    resource_group = kwargs.get("resource_group") or config.get_cloud_config_value(
+        "resource_group", get_configured_provider(), __opts__, search_global=False
+    )
+    sas_token = kwargs.get("sas_token") or config.get_cloud_config_value(
+        "sas_token", get_configured_provider(), __opts__, search_global=False
+    )
+    storage_account = kwargs.get("storage_account") or config.get_cloud_config_value(
+        "storage_account", get_configured_provider(), __opts__, search_global=False
+    )
+    storage_key = kwargs.get("storage_key") or config.get_cloud_config_value(
+        "storage_key", get_configured_provider(), __opts__, search_global=False
+    )
+    if not resource_group:
+        raise SaltCloudSystemExit("A resource group must be specified")
+    if not storage_account:
+        raise SaltCloudSystemExit("A storage account must be specified")
+    if not storage_key:
+        storconn = get_conn(client_type="storage")
+        storage_keys = storconn.storage_accounts.list_keys(
+            resource_group, storage_account
+        )
+        storage_keys = {v.key_name: v.value for v in storage_keys.keys}
+        storage_key = next(iter(storage_keys.values()))
+    cloud_env = _get_cloud_environment()
+    endpoint_suffix = cloud_env.suffixes.storage_endpoint
+    return BlockBlobService(
+        storage_account,
+        storage_key,
+        sas_token=sas_token,
+        endpoint_suffix=endpoint_suffix,
+    )
+@_deprecation_message
+def list_blobs(call=None, kwargs=None):  # pylint: disable=unused-argument
+    """
+    List blobs.
+    """
+    if kwargs is None:
+        kwargs = {}
+    if "container" not in kwargs:
+        raise SaltCloudSystemExit("A container must be specified")
+    storageservice = _get_block_blob_service(kwargs)
+    ret = {}
+    try:
+        for blob in storageservice.list_blobs(kwargs["container"]).items:
+            ret[blob.name] = {
+                "blob_type": blob.properties.blob_type,
+                "last_modified": blob.properties.last_modified.isoformat(),
+                "server_encrypted": blob.properties.server_encrypted,
+            }
+    except Exception as exc:  # pylint: disable=broad-except
+        log.warning(str(exc))
+    return ret
+@_deprecation_message
+def delete_blob(call=None, kwargs=None):  # pylint: disable=unused-argument
+    """
+    Delete a blob from a container.
+    """
+    if kwargs is None:
+        kwargs = {}
+    if "container" not in kwargs:
+        raise SaltCloudSystemExit("A container must be specified")
+    if "blob" not in kwargs:
+        raise SaltCloudSystemExit("A blob must be specified")
+    storageservice = _get_block_blob_service(kwargs)
+    storageservice.delete_blob(kwargs["container"], kwargs["blob"])
+    return True
+@_deprecation_message
+def delete_managed_disk(call=None, kwargs=None):  # pylint: disable=unused-argument
+    """
+    Delete a managed disk from a resource group.
+    """
+    compconn = get_conn(client_type="compute")
+    try:
+        compconn.disks.delete(kwargs["resource_group"], kwargs["blob"])
+    except Exception as exc:  # pylint: disable=broad-except
+        log.error(
+            "Error deleting managed disk %s - %s",
+            kwargs.get("blob"),
+            str(exc),
+        )
+        return False
+    return True
+@_deprecation_message
+def list_virtual_networks(call=None, kwargs=None):
+    """
+    List virtual networks.
+    """
+    if kwargs is None:
+        kwargs = {}
+    if call == "action":
+        raise SaltCloudSystemExit(
+            "The avail_sizes function must be called with -f or --function"
+        )
+    netconn = get_conn(client_type="network")
+    resource_groups = list_resource_groups()
+    ret = {}
+    for group in resource_groups:
+        try:
+            networks = netconn.virtual_networks.list(resource_group_name=group)
+        except CloudError:
+            networks = {}
+        for network_obj in networks:
+            network = network_obj.as_dict()
+            ret[network["name"]] = network
+            ret[network["name"]]["subnets"] = list_subnets(
+                kwargs={"resource_group": group, "network": network["name"]}
+            )
+    return ret
+@_deprecation_message
+def list_subnets(call=None, kwargs=None):
+    """
+    List subnets in a virtual network.
+    """
+    if kwargs is None:
+        kwargs = {}
+    if call == "action":
+        raise SaltCloudSystemExit(
+            "The avail_sizes function must be called with -f or --function"
+        )
+    netconn = get_conn(client_type="network")
+    resource_group = kwargs.get("resource_group") or config.get_cloud_config_value(
+        "resource_group", get_configured_provider(), __opts__, search_global=False
+    )
+    if not resource_group and "group" in kwargs and "resource_group" not in kwargs:
+        resource_group = kwargs["group"]
+    if not resource_group:
+        raise SaltCloudSystemExit("A resource group must be specified")
+    if kwargs.get("network") is None:
+        kwargs["network"] = config.get_cloud_config_value(
+            "network", get_configured_provider(), __opts__, search_global=False
+        )
+    if "network" not in kwargs or kwargs["network"] is None:
+        raise SaltCloudSystemExit('A "network" must be specified')
+    ret = {}
+    subnets = netconn.subnets.list(resource_group, kwargs["network"])
+    for subnet in subnets:
+        ret[subnet.name] = subnet.as_dict()
+        ret[subnet.name]["ip_configurations"] = {}
+        for ip_ in subnet.ip_configurations:
+            comps = ip_.id.split("/")
+            name = comps[-1]
+            ret[subnet.name]["ip_configurations"][name] = ip_.as_dict()
+            ret[subnet.name]["ip_configurations"][name]["subnet"] = subnet.name
+        ret[subnet.name]["resource_group"] = resource_group
+    return ret
+@_deprecation_message
+def create_or_update_vmextension(
+    call=None, kwargs=None
+):  # pylint: disable=unused-argument
+    """
+    .. versionadded:: 2019.2.0
+    Create or update a VM extension object "inside" of a VM object.
+    required kwargs:
+      .. code-block:: yaml
+        extension_name: myvmextension
+        virtual_machine_name: myvm
+        settings: {"commandToExecute": "hostname"}
+    optional kwargs:
+      .. code-block:: yaml
+        resource_group: < inferred from cloud configs >
+        location: < inferred from cloud configs >
+        publisher: < default: Microsoft.Azure.Extensions >
+        virtual_machine_extension_type: < default: CustomScript >
+        type_handler_version: < default: 2.0 >
+        auto_upgrade_minor_version: < default: True >
+        protected_settings: < default: None >
+    """
+    if kwargs is None:
+        kwargs = {}
+    if "extension_name" not in kwargs:
+        raise SaltCloudSystemExit("An extension name must be specified")
+    if "virtual_machine_name" not in kwargs:
+        raise SaltCloudSystemExit("A virtual machine name must be specified")
+    compconn = get_conn(client_type="compute")
+    VirtualMachineExtension = getattr(compute_models, "VirtualMachineExtension")
+    resource_group = kwargs.get("resource_group") or config.get_cloud_config_value(
+        "resource_group", get_configured_provider(), __opts__, search_global=False
+    )
+    if not resource_group:
+        raise SaltCloudSystemExit("A resource group must be specified")
+    location = kwargs.get("location") or get_location()
+    if not location:
+        raise SaltCloudSystemExit("A location must be specified")
+    publisher = kwargs.get("publisher", "Microsoft.Azure.Extensions")
+    virtual_machine_extension_type = kwargs.get(
+        "virtual_machine_extension_type", "CustomScript"
+    )
+    type_handler_version = kwargs.get("type_handler_version", "2.0")
+    auto_upgrade_minor_version = kwargs.get("auto_upgrade_minor_version", True)
+    settings = kwargs.get("settings", {})
+    protected_settings = kwargs.get("protected_settings")
+    if not isinstance(settings, dict):
+        raise SaltCloudSystemExit("VM extension settings are not valid")
+    elif "commandToExecute" not in settings and "script" not in settings:
+        raise SaltCloudSystemExit(
+            "VM extension settings are not valid. Either commandToExecute or script"
+            " must be specified."
+        )
+    log.info("Creating VM extension %s", kwargs["extension_name"])
+    ret = {}
+    try:
+        params = VirtualMachineExtension(
+            location=location,
+            publisher=publisher,
+            virtual_machine_extension_type=virtual_machine_extension_type,
+            type_handler_version=type_handler_version,
+            auto_upgrade_minor_version=auto_upgrade_minor_version,
+            settings=settings,
+            protected_settings=protected_settings,
+        )
+        poller = compconn.virtual_machine_extensions.create_or_update(
+            resource_group,
+            kwargs["virtual_machine_name"],
+            kwargs["extension_name"],
+            params,
+        )
+        ret = poller.result()
+        ret = ret.as_dict()
+    except CloudError as exc:
+        salt.utils.azurearm.log_cloud_error(
+            "compute",
+            f"Error attempting to create the VM extension: {exc.message}",
+        )
+        ret = {"error": exc.message}
+    return ret
+@_deprecation_message
+def stop(name, call=None):
+    """
+    .. versionadded:: 2019.2.0
+    Stop (deallocate) a VM
+    CLI Examples:
+    .. code-block:: bash
+         salt-cloud -a stop myminion
+    """
+    if call == "function":
+        raise SaltCloudSystemExit("The stop action must be called with -a or --action.")
+    compconn = get_conn(client_type="compute")
+    resource_group = config.get_cloud_config_value(
+        "resource_group", get_configured_provider(), __opts__, search_global=False
+    )
+    ret = {}
+    if not resource_group:
+        groups = list_resource_groups()
+        for group in groups:
+            try:
+                instance = compconn.virtual_machines.deallocate(
+                    vm_name=name, resource_group_name=group
+                )
+                instance.wait()
+                vm_result = instance.result()
+                ret = vm_result.as_dict()
+                break
+            except CloudError as exc:
+                if "was not found" in exc.message:
+                    continue
+                else:
+                    ret = {"error": exc.message}
+        if not ret:
+            salt.utils.azurearm.log_cloud_error(
+                "compute", f"Unable to find virtual machine with name: {name}"
+            )
+            ret = {"error": f"Unable to find virtual machine with name: {name}"}
+    else:
+        try:
+            instance = compconn.virtual_machines.deallocate(
+                vm_name=name, resource_group_name=resource_group
+            )
+            instance.wait()
+            vm_result = instance.result()
+            ret = vm_result.as_dict()
+        except CloudError as exc:
+            salt.utils.azurearm.log_cloud_error(
+                "compute", f"Error attempting to stop {name}: {exc.message}"
+            )
+            ret = {"error": exc.message}
+    return ret
+@_deprecation_message
+def start(name, call=None):
+    """
+    .. versionadded:: 2019.2.0
+    Start a VM
+    CLI Examples:
+    .. code-block:: bash
+         salt-cloud -a start myminion
+    """
+    if call == "function":
+        raise SaltCloudSystemExit(
+            "The start action must be called with -a or --action."
+        )
+    compconn = get_conn(client_type="compute")
+    resource_group = config.get_cloud_config_value(
+        "resource_group", get_configured_provider(), __opts__, search_global=False
+    )
+    ret = {}
+    if not resource_group:
+        groups = list_resource_groups()
+        for group in groups:
+            try:
+                instance = compconn.virtual_machines.start(
+                    vm_name=name, resource_group_name=group
+                )
+                instance.wait()
+                vm_result = instance.result()
+                ret = vm_result.as_dict()
+                break
+            except CloudError as exc:
+                if "was not found" in exc.message:
+                    continue
+                else:
+                    ret = {"error": exc.message}
+        if not ret:
+            salt.utils.azurearm.log_cloud_error(
+                "compute", f"Unable to find virtual machine with name: {name}"
+            )
+            ret = {"error": f"Unable to find virtual machine with name: {name}"}
+    else:
+        try:
+            instance = compconn.virtual_machines.start(
+                vm_name=name, resource_group_name=resource_group
+            )
+            instance.wait()
+            vm_result = instance.result()
+            ret = vm_result.as_dict()
+        except CloudError as exc:
+            salt.utils.azurearm.log_cloud_error(
+                "compute",
+                f"Error attempting to start {name}: {exc.message}",
+            )
+            ret = {"error": exc.message}
+    return ret

--- a/salt/cloud/clouds/clc.py
+++ b/salt/cloud/clouds/clc.py
@@ -236,21 +236,21 @@
     use templates for this
     """
     return {"Sizes": "Sizes are built into templates. Choose appropriate template"}
 def get_build_status(req_id, nodename):
     """
     get the build status from CLC to make sure we don't return to early
     """
     counter = 0
     req_id = str(req_id)
     while counter < 10:
-        queue = clc.v1.Blueprint.GetStatus(request_id=(req_id))
+        queue = clc.v1.Blueprint.GetStatus(request_id=req_id)
         if queue["PercentComplete"] == 100:
             server_name = queue["Servers"][0]
             creds = get_creds()
             clc.v2.SetCredentials(creds["user"], creds["password"])
             ip_addresses = clc.v2.Server(server_name).ip_addresses
             internal_ip_address = ip_addresses[0]["internal"]
             return internal_ip_address
         else:
             counter = counter + 1
             log.info(

--- a/salt/cloud/clouds/digitalocean.py
+++ b/salt/cloud/clouds/digitalocean.py
@@ -270,52 +270,38 @@
         "ssh_interface", vm_, __opts__, search_global=False, default="public"
     )
     if ssh_interface in ["private", "public"]:
         log.info("ssh_interface: Setting interface for ssh to %s", ssh_interface)
         kwargs["ssh_interface"] = ssh_interface
     else:
         raise SaltCloudConfigError(
             "The DigitalOcean driver requires ssh_interface to be defined as 'public'"
             " or 'private'."
         )
-    vpc_name = config.get_cloud_config_value(
-        "vpc_name",
+    private_networking = config.get_cloud_config_value(
+        "private_networking",
         vm_,
         __opts__,
         search_global=False,
         default=None,
     )
-    if vpc_name is not None:
-        vpc = _get_vpc_by_name(vpc_name)
-        if vpc is None:
-            raise SaltCloudConfigError("Invalid VPC name provided")
-        else:
-            kwargs["vpc_uuid"] = vpc[vpc_name]["id"]
-    else:
-        private_networking = config.get_cloud_config_value(
-            "private_networking",
-            vm_,
-            __opts__,
-            search_global=False,
-            default=None,
-        )
-        if private_networking is not None:
-            if not isinstance(private_networking, bool):
-                raise SaltCloudConfigError(
-                    "'private_networking' should be a boolean value."
-                )
-            kwargs["private_networking"] = private_networking
-        if not private_networking and ssh_interface == "private":
+    if private_networking is not None:
+        if not isinstance(private_networking, bool):
             raise SaltCloudConfigError(
-                "The DigitalOcean driver requires ssh_interface if defined as 'private' "
-                "then private_networking should be set as 'True'."
+                "'private_networking' should be a boolean value."
             )
+        kwargs["private_networking"] = private_networking
+    if not private_networking and ssh_interface == "private":
+        raise SaltCloudConfigError(
+            "The DigitalOcean driver requires ssh_interface if defined as 'private' "
+            "then private_networking should be set as 'True'."
+        )
     backups_enabled = config.get_cloud_config_value(
         "backups_enabled",
         vm_,
         __opts__,
         search_global=False,
         default=None,
     )
     if backups_enabled is not None:
         if not isinstance(backups_enabled, bool):
             raise SaltCloudConfigError("'backups_enabled' should be a boolean value.")
@@ -390,23 +376,27 @@
             __opts__,
             search_global=False,
             default=default_dns_domain,
         )
         if dns_hostname and dns_domain:
             log.info(
                 'create_dns_record: using dns_hostname="%s", dns_domain="%s"',
                 dns_hostname,
                 dns_domain,
             )
-            __add_dns_addr__ = lambda t, d: post_dns_record(
-                dns_domain=dns_domain, name=dns_hostname, record_type=t, record_data=d
-            )
+            def __add_dns_addr__(t, d):
+                return post_dns_record(
+                    dns_domain=dns_domain,
+                    name=dns_hostname,
+                    record_type=t,
+                    record_data=d,
+                )
             log.debug("create_dns_record: %s", __add_dns_addr__)
         else:
             log.error(
                 "create_dns_record: could not determine dns_hostname and/or dns_domain"
             )
             raise SaltCloudConfigError(
                 "'create_dns_record' must be a dict specifying \"domain\" "
                 'and "hostname" or the minion name must be an FQDN.'
             )
     __utils__["cloud.fire_event"](
@@ -507,24 +497,21 @@
     """
     base_path = str(
         config.get_cloud_config_value(
             "api_root",
             get_configured_provider(),
             __opts__,
             search_global=False,
             default="https://api.digitalocean.com/v2",
         )
     )
-    if method == "vpcs":
-        path = f"{base_path}/{method}"
-    else:
-        path = f"{base_path}/{method}/"
+    path = f"{base_path}/{method}/"
     if droplet_id:
         path += f"{droplet_id}/"
     if command:
         path += command
     if not isinstance(args, dict):
         args = {}
     personal_access_token = config.get_cloud_config_value(
         "personal_access_token",
         get_configured_provider(),
         __opts__,
@@ -532,20 +519,21 @@
     )
     data = salt.utils.json.dumps(args)
     requester = getattr(requests, http_method)
     request = requester(
         path,
         data=data,
         headers={
             "Authorization": "Bearer " + personal_access_token,
             "Content-Type": "application/json",
         },
+        timeout=120,
     )
     if request.status_code > 299:
         raise SaltCloudSystemExit(
             "An error occurred while querying DigitalOcean. HTTP Code: {}  "
             "Error: '{}'".format(
                 request.status_code,
                 request.text,
             )
         )
     log.debug(request.url)
@@ -783,21 +771,21 @@
     domain = ".".join(fqdn.split(".")[-2:])
     hostname = ".".join(fqdn.split(".")[:-2])
     try:
         response = query(method="domains", droplet_id=domain, command="records")
     except SaltCloudSystemExit:
         log.debug("Failed to find domains.")
         return False
     log.debug("found DNS records: %s", pprint.pformat(response))
     records = response["domain_records"]
     if records:
-        record_ids = [r["id"] for r in records if r["name"] == hostname]
+        record_ids = [r["id"] for r in records if r["name"].decode() == hostname]
         log.debug("deleting DNS record IDs: %s", record_ids)
         for id_ in record_ids:
             try:
                 log.info("deleting DNS record %s", id_)
                 ret = query(
                     method="domains",
                     droplet_id=domain,
                     command=f"records/{id_}",
                     http_method="delete",
                 )
@@ -981,61 +969,29 @@
     if "floating_ip" not in kwargs:
         log.error("A floating IP is required.")
         return False
     result = query(
         method="floating_ips",
         command=kwargs["floating_ip"] + "/actions",
         args={"type": "unassign"},
         http_method="post",
     )
     return result
-def _get_vpc_by_name(name):
-    """
-    Helper function to format and parse vpc data. It's pretty expensive as it
-    retrieves a list of vpcs and iterates through them till it finds the correct
-    vpc by name.
-    """
-    fetch = True
-    page = 1
-    ret = {}
-    log.debug("Matching vpc name with: %s", name)
-    while fetch:
-        items = query(method="vpcs", command=f"?page={str(page)}&per_page=200")
-        for node in items["vpcs"]:
-            log.debug("Node returned : %s", node["name"])
-            if name == node["name"]:
-                log.debug("Matched VPC node")
-                ret[name] = {
-                    "id": node["id"],
-                    "urn": node["urn"],
-                    "name": name,
-                    "description": node["description"],
-                    "region": node["region"],
-                    "ip_range": node["ip_range"],
-                    "default": node["default"],
-                }
-                return ret
-        page += 1
-        try:
-            fetch = "next" in items["links"]["pages"]
-        except KeyError:
-            fetch = False
-    return None
 def _list_nodes(full=False, for_output=False):
     """
     Helper function to format and parse node data.
     """
     fetch = True
     page = 1
     ret = {}
     while fetch:
-        items = query(method="droplets", command=f"?page={str(page)}&per_page=200")
+        items = query(method="droplets", command="?page=" + str(page) + "&per_page=200")
         for node in items["droplets"]:
             name = node["name"]
             ret[name] = {}
             if full:
                 ret[name] = _get_full_output(node, for_output=for_output)
             else:
                 public_ips, private_ips = _get_ips(node["networks"])
                 ret[name] = {
                     "id": node["id"],
                     "image": node["image"]["name"],

--- a/salt/cloud/clouds/linode.py
+++ b/salt/cloud/clouds/linode.py
@@ -1,128 +1,169 @@
 r"""
 The Linode Cloud Module
 =======================
 The Linode cloud module is used to interact with the Linode Cloud.
+You can target a specific version of the Linode API with the ``api_version`` parameter. The default is ``v3``.
 Provider
 --------
 The following provider parameters are supported:
 - **apikey**: (required) The key to use to authenticate with the Linode API.
 - **password**: (required) The default password to set on new VMs. Must be 8 characters with at least one lowercase, uppercase, and numeric.
+- **api_version**: (optional) The version of the Linode API to interact with. Defaults to ``v3``.
 - **poll_interval**: (optional) The rate of time in milliseconds to poll the Linode API for changes. Defaults to ``500``.
 - **ratelimit_sleep**: (optional) The time in seconds to wait before retrying after a ratelimit has been enforced. Defaults to ``0``.
 .. note::
-    APIv3 usage has been removed in favor of APIv4. To move to APIv4 now,
-    See the full migration guide
+    APIv3 usage is deprecated and will be removed in a future release in favor of APIv4. To move to APIv4 now,
+    set the ``api_version`` parameter in your provider configuration to ``v4``. See the full migration guide
     here https://docs.saltproject.io/en/latest/topics/cloud/linode.html#migrating-to-apiv4.
 Set up the provider configuration at ``/etc/salt/cloud.providers`` or ``/etc/salt/cloud.providers.d/linode.conf``:
 .. code-block:: yaml
     my-linode-provider:
         driver: linode
+        api_version: v4
         apikey: f4ZsmwtB1c7f85Jdu43RgXVDFlNjuJaeIYV8QMftTqKScEB2vSosFSr...
-        password: F00barbazverylongp@ssword
+        password: F00barbaz
+For use with APIv3 (deprecated):
+.. code-block:: yaml
+    my-linode-provider-v3:
+        driver: linode
+        apikey: f4ZsmwtB1c7f85Jdu43RgXVDFlNjuJaeIYV8QMftTqKScEB2vSosFSr...
+        password: F00barbaz
 Profile
 -------
 The following profile parameters are supported:
-- **size**: (required) The size of the VM. This should be a Linode instance type ID (i.e. ``g6-standard-2``). Run ``salt-cloud -f avail_sizes my-linode-provider`` for options.
-- **location**: (required) The location of the VM. This should be a Linode region (e.g. ``us-east``). Run ``salt-cloud -f avail_locations my-linode-provider`` for options.
-- **image**: (required) The image to deploy the boot disk from. This should be an image ID (e.g. ``linode/ubuntu22.04``); official images start with ``linode/``. Run ``salt-cloud -f avail_images my-linode-provider`` for more options.
+- **size**: (required) The size of the VM. This should be a Linode instance type ID (i.e. ``g6-standard-2``). For APIv3, this would be a plan ID (i.e. ``Linode 2GB``). Run ``salt-cloud -f avail_sizes my-linode-provider`` for options.
+- **location**: (required) The location of the VM. This should be a Linode region (e.g. ``us-east``). For APIv3, this would be a datacenter location (i.e. ``Newark, NJ, USA``). Run ``salt-cloud -f avail_locations my-linode-provider`` for options.
+- **image**: (required) The image to deploy the boot disk from. This should be an image ID (e.g. ``linode/ubuntu16.04``); official images start with ``linode/``. For APIv3, this would be an image label (i.e. Ubuntu 16.04). Run ``salt-cloud -f avail_images my-linode-provider`` for more options.
 - **password**: (\*required) The default password for the VM. Must be provided at the profile or provider level.
-- **assign_private_ip**: (optional) Whether or not to assign a private IP to the VM. Defaults to ``False``.
-- **backups_enabled**: (optional) Whether or not to enable the backup for this VM. Backup can be configured in your Linode account Defaults to ``False``.
+- **assign_private_ip**: (optional) Whether or not to assign a private key to the VM. Defaults to ``False``.
 - **ssh_interface**: (optional) The interface with which to connect over SSH. Valid options are ``private_ips`` or ``public_ips``. Defaults to ``public_ips``.
 - **ssh_pubkey**: (optional) The public key to authorize for SSH with the VM.
 - **swap**: (optional) The amount of disk space to allocate for the swap partition. Defaults to ``256``.
 - **clonefrom**: (optional) The name of the Linode to clone from.
+- **disk_size**: (deprecated, optional) The amount of disk space to allocate for the OS disk. This has no effect with APIv4; the size of the boot disk will be the remainder of disk space after the swap parition is allocated.
 Set up a profile configuration in ``/etc/salt/cloud.profiles.d/``:
 .. code-block:: yaml
     my-linode-profile:
         provider: my-linode-provider
         size: g6-standard-1
-        image: linode/ubuntu22.04
+        image: linode/alpine3.12
         location: us-east
     my-linode-profile-advanced:
         provider: my-linode-provider
         size: g6-standard-3
-        image: linode/ubuntu22.04
+        image: linode/alpine3.10
         location: eu-west
         password: bogus123X
         assign_private_ip: true
         ssh_interface: private_ips
         ssh_pubkey: ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB...
         swap_size: 512
+    my-linode-profile-v3:
+        provider: my-linode-provider-v3
+        size: Nanode 1GB
+        image: Alpine 3.12
+        location: Fremont, CA, USA
 Migrating to APIv4
 ------------------
-You will need to generate a new token for your account. See https://www.linode.com/docs/products/tools/api/get-started/#create-an-api-token
+In order to target APIv4, ensure your provider configuration has ``api_version`` set to ``v4``.
+You will also need to generate a new token for your account. See https://www.linode.com/docs/platform/api/getting-started-with-the-linode-api/#create-an-api-token
 There are a few changes to note:
 - There has been a general move from label references to ID references. The profile configuration parameters ``location``, ``size``, and ``image`` have moved from being label based references to IDs. See the profile section for more information. In addition to these inputs being changed, ``avail_sizes``, ``avail_locations``, and ``avail_images`` now output options sorted by ID instead of label.
 - The ``disk_size`` profile configuration parameter has been deprecated and will not be taken into account when creating new VMs while targeting APIv4.
-:maintainer: Linode Developer Tools and Experience Team <dev-dx@linode.com>
+:maintainer: Charles Kenney <ckenney@linode.com>
+:maintainer: Phillip Campbell <pcampbell@linode.com>
 :depends: requests
 """
+import abc
 import datetime
 import json
 import logging
 import pprint
 import re
 import time
-from abc import ABC, abstractmethod
 from pathlib import Path
 import salt.config as config
 from salt._compat import ipaddress
-from salt.exceptions import SaltCloudException, SaltCloudNotFound, SaltCloudSystemExit
+from salt.exceptions import (
+    SaltCloudConfigError,
+    SaltCloudException,
+    SaltCloudNotFound,
+    SaltCloudSystemExit,
+)
 try:
     import requests
     HAS_REQUESTS = True
 except ImportError:
     HAS_REQUESTS = False
 log = logging.getLogger(__name__)
+HAS_WARNED_FOR_API_V3 = False
 LASTCALL = int(time.mktime(datetime.datetime.now().timetuple()))
+LINODE_STATUS = {
+    "boot_failed": {"code": -2, "descr": "Boot Failed (not in use)"},
+    "beeing_created": {"code": -1, "descr": "Being Created"},
+    "brand_new": {"code": 0, "descr": "Brand New"},
+    "running": {"code": 1, "descr": "Running"},
+    "poweroff": {"code": 2, "descr": "Powered Off"},
+    "shutdown": {"code": 3, "descr": "Shutting Down (not in use)"},
+    "save_to_disk": {"code": 4, "descr": "Saved to Disk (not in use)"},
+}
 __virtualname__ = "linode"
 def __virtual__():
     """
     Check for Linode configs.
     """
     if get_configured_provider() is False:
         return False
     if _get_dependencies() is False:
         return False
     return __virtualname__
 def _get_active_provider_name():
     try:
         return __active_provider_name__.value()
     except AttributeError:
         return __active_provider_name__
-def _get_backup_enabled(vm_):
-    """
-    Return True if a backup is set to enabled
-    """
-    return config.get_cloud_config_value(
-        "backups_enabled",
-        vm_,
-        __opts__,
-        default=False,
-    )
 def get_configured_provider():
     """
     Return the first configured instance.
     """
     return config.is_provider_configured(
         __opts__,
         _get_active_provider_name() or __virtualname__,
         ("apikey", "password"),
     )
 def _get_dependencies():
     """
     Warn if dependencies aren't met.
     """
     deps = {"requests": HAS_REQUESTS}
     return config.check_driver_dependencies(__virtualname__, deps)
+def _get_api_version():
+    """
+    Return the configured Linode API version.
+    """
+    return config.get_cloud_config_value(
+        "api_version",
+        get_configured_provider(),
+        __opts__,
+        search_global=False,
+        default="v3",
+    )
+def _is_api_v3():
+    """
+    Return whether the configured Linode API version is ``v3``.
+    """
+    return _get_api_version() == "v3"
+def _get_cloud_interface():
+    if _is_api_v3():
+        return LinodeAPIv3()
+    return LinodeAPIv4()
 def _get_api_key():
     """
     Returned the configured Linode API key.
     """
     val = config.get_cloud_config_value(
         "api_key",
         get_configured_provider(),
         __opts__,
         search_global=False,
         default=config.get_cloud_config_value(
@@ -159,20 +200,27 @@
         The configuration to obtain the password from.
     """
     return config.get_cloud_config_value(
         "password",
         vm_,
         __opts__,
         default=config.get_cloud_config_value(
             "passwd", vm_, __opts__, search_global=False
         ),
         search_global=False,
+    )
+def _get_root_disk_size(vm_):
+    """
+    Return the specified size of the data partition.
+    """
+    return config.get_cloud_config_value(
+        "disk_size", vm_, __opts__, search_global=False
     )
 def _get_private_ip(vm_):
     """
     Return True if a private ip address is requested
     """
     return config.get_cloud_config_value(
         "assign_private_ip", vm_, __opts__, default=False
     )
 def _get_ssh_key_files(vm_):
     """
@@ -235,121 +283,134 @@
         ret = False
     else:
         ret = True
     if ret is False:
         log.warning(
             "A Linode label may only contain ASCII letters or numbers, dashes, and "
             "underscores, must begin and end with letters or numbers, and be at least "
             "three characters in length."
         )
     return ret
-class LinodeAPI(ABC):
-    @abstractmethod
+def _warn_for_api_v3():
+    global HAS_WARNED_FOR_API_V3
+    if not HAS_WARNED_FOR_API_V3:
+        log.warning(
+            "Linode APIv3 has been deprecated and support will be removed "
+            "in future releases. Please plan to upgrade to APIv4. For more "
+            "information, see"
+            " https://docs.saltproject.io/en/latest/topics/cloud/linode.html#migrating-to-apiv4."
+        )
+        HAS_WARNED_FOR_API_V3 = True
+class LinodeAPI:
+    @abc.abstractmethod
     def avail_images(self):
         """avail_images implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def avail_locations(self):
         """avail_locations implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def avail_sizes(self):
         """avail_sizes implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def boot(self, name=None, kwargs=None):
         """boot implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def clone(self, kwargs=None):
         """clone implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def create_config(self, kwargs=None):
         """create_config implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def create(self, vm_):
         """create implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def destroy(self, name):
         """destroy implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def get_config_id(self, kwargs=None):
         """get_config_id implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def list_nodes(self):
         """list_nodes implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def list_nodes_full(self):
         """list_nodes_full implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def list_nodes_min(self):
         """list_nodes_min implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def reboot(self, name):
         """reboot implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def show_instance(self, name):
         """show_instance implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def show_pricing(self, kwargs=None):
         """show_pricing implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def start(self, name):
         """start implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def stop(self, name):
         """stop implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def _get_linode_by_name(self, name):
         """_get_linode_by_name implementation"""
-    @abstractmethod
+    @abc.abstractmethod
     def _get_linode_by_id(self, linode_id):
         """_get_linode_by_id implementation"""
+    def get_plan_id(self, kwargs=None):
+        """get_plan_id implementation"""
+        raise SaltCloudSystemExit(
+            "The get_plan_id is not supported by this api_version."
+        )
     def get_linode(self, kwargs=None):
         name = kwargs.get("name", None)
         linode_id = kwargs.get("linode_id", None)
         if linode_id is not None:
             return self._get_linode_by_id(linode_id)
         elif name is not None:
             return self._get_linode_by_name(name)
         raise SaltCloudSystemExit(
             "The get_linode function requires either a 'name' or a 'linode_id'."
         )
     def list_nodes_select(self, call):
         return __utils__["cloud.list_nodes_select"](
             self.list_nodes_full(),
             __opts__["query.selection"],
             call,
         )
 class LinodeAPIv4(LinodeAPI):
-    @classmethod
-    def get_api_instance(cls):
-        if not hasattr(cls, "api_instance"):
-            cls.api_instance = cls()
-        return cls.api_instance
-    def _query(self, path, method="GET", data=None, headers=None):
+    def _query(self, path=None, method="GET", data=None, headers=None):
         """
         Make a call to the Linode API.
         """
+        api_version = _get_api_version()
         api_key = _get_api_key()
         ratelimit_sleep = _get_ratelimit_sleep()
         if headers is None:
             headers = {}
         headers["Authorization"] = f"Bearer {api_key}"
         headers["Content-Type"] = "application/json"
         headers["User-Agent"] = "salt-cloud-linode"
-        url = f"https://api.linode.com/v4{path}"
+        url = f"https://api.linode.com/{api_version}{path}"
         decode = method != "DELETE"
         result = None
         log.debug("Linode API request: %s %s", method, url)
         if data is not None:
             log.trace("Linode API request body: %s", data)
         attempt = 0
         while True:
             try:
-                result = requests.request(method, url, json=data, headers=headers)
+                result = requests.request(
+                    method, url, json=data, headers=headers, timeout=120
+                )
                 log.debug("Linode API response status code: %d", result.status_code)
                 log.trace("Linode API response body: %s", result.text)
                 result.raise_for_status()
                 break
             except requests.exceptions.HTTPError as exc:
                 err_response = exc.response
                 err_data = self._get_response_json(err_response)
                 status_code = err_response.status_code
                 if status_code == 429:
                     log.debug(
@@ -394,60 +455,55 @@
         ret = {}
         for region in response["data"]:
             ret[region["id"]] = region
         return ret
     def avail_sizes(self):
         response = self._query(path="/linode/types")
         ret = {}
         for instance_type in response["data"]:
             ret[instance_type["id"]] = instance_type
         return ret
-    def set_backup_schedule(self, label, linode_id, day, window, auto_enable=False):
-        instance = self.get_linode(kwargs={"linode_id": linode_id, "name": label})
-        linode_id = instance.get("id", None)
-        if auto_enable:
-            backups = instance.get("backups")
-            if backups and not backups.get("enabled"):
-                self._query(
-                    f"/linode/instances/{linode_id}/backups/enable",
-                    method="POST",
-                )
-        self._query(
-            f"/linode/instances/{linode_id}",
-            method="PUT",
-            data={"backups": {"schedule": {"day": day, "window": window}}},
-        )
     def boot(self, name=None, kwargs=None):
         instance = self.get_linode(
             kwargs={"linode_id": kwargs.get("linode_id", None), "name": name}
         )
         config_id = kwargs.get("config_id", None)
         check_running = kwargs.get("check_running", True)
         linode_id = instance.get("id", None)
         name = instance.get("label", None)
         if check_running:
             if instance["status"] == "running":
                 raise SaltCloudSystemExit(
                     "Cannot boot Linode {0} ({1}). "
                     "Linode {0} is already running.".format(name, linode_id)
                 )
-        self._query(
+        response = self._query(
             f"/linode/instances/{linode_id}/boot",
             method="POST",
             data={"config_id": config_id},
         )
         self._wait_for_linode_status(linode_id, "running")
         return True
     def clone(self, kwargs=None):
         linode_id = kwargs.get("linode_id", None)
         location = kwargs.get("location", None)
         size = kwargs.get("size", None)
+        if "datacenter_id" in kwargs:
+            log.warning(
+                "The 'datacenter_id' argument has been deprecated and will be "
+                "removed in future releases. Please use 'location' instead."
+            )
+        if "plan_id" in kwargs:
+            log.warning(
+                "The 'plan_id' argument has been deprecated and will be "
+                "removed in future releases. Please use 'size' instead."
+            )
         for item in [linode_id, location, size]:
             if item is None:
                 raise SaltCloudSystemExit(
                     "The clone function requires a 'linode_id', 'location',"
                     "and 'size' to be provided."
                 )
         return self._query(
             f"/linode/instances/{linode_id}/clone",
             method="POST",
             data={"region": location, "type": size},
@@ -494,21 +550,20 @@
             transport=__opts__["transport"],
         )
         log.info("Creating Cloud VM %s", name)
         result = None
         pub_ssh_keys = _get_ssh_keys(vm_)
         ssh_interface = _get_ssh_interface(vm_)
         use_private_ip = ssh_interface == "private_ips"
         assign_private_ip = _get_private_ip(vm_) or use_private_ip
         password = _get_password(vm_)
         swap_size = _get_swap_size(vm_)
-        backups_enabled = _get_backup_enabled(vm_)
         clonefrom_name = vm_.get("clonefrom", None)
         instance_type = vm_.get("size", None)
         image = vm_.get("image", None)
         should_clone = True if clonefrom_name else False
         if should_clone:
             clone_linode = self.get_linode(kwargs={"name": clonefrom_name})
             result = clone(
                 {
                     "linode_id": clone_linode["id"],
                     "location": clone_linode["region"],
@@ -519,21 +574,20 @@
                 self._query(
                     "/networking/ips",
                     method="POST",
                     data={"type": "ipv4", "public": False, "linode_id": result["id"]},
                 )
         else:
             result = self._query(
                 "/linode/instances",
                 method="POST",
                 data={
-                    "backups_enabled": backups_enabled,
                     "label": name,
                     "type": instance_type,
                     "region": vm_.get("location", None),
                     "private_ip": assign_private_ip,
                     "booted": True,
                     "root_pass": password,
                     "authorized_keys": pub_ssh_keys,
                     "image": image,
                     "swap_size": swap_size,
                 },
@@ -791,21 +845,22 @@
     def _wait_for_event(self, action, entity, entity_id, status, timeout=None):
         event_filter = {
             "+order_by": "created",
             "+order": "desc",
             "seen": False,
             "action": action,
             "entity.id": entity_id,
             "entity.type": entity,
         }
         last_event = None
-        condition = lambda event: self._check_event_status(event, status)
+        def condition(event):
+            return self._check_event_status(event, status)
         while True:
             if last_event is not None:
                 event_filter["+gt"] = last_event
             filter_json = json.dumps(event_filter, separators=(",", ":"))
             result = self._query("/account/events", headers={"X-Filter": filter_json})
             events = result.get("data", [])
             if len(events) == 0:
                 break
             for event in events:
                 event_id = event.get("id")
@@ -828,126 +883,844 @@
                     timeout=timeout,
                 )
         return False
     def _get_response_json(self, response):
         json = None
         try:
             json = response.json()
         except ValueError:
             pass
         return json
+class LinodeAPIv3(LinodeAPI):
+    def __init__(self):
+        _warn_for_api_v3()
+    def _query(
+        self,
+        action=None,
+        command=None,
+        args=None,
+        method="GET",
+        header_dict=None,
+        data=None,
+        url="https://api.linode.com/",
+    ):
+        """
+        Make a web call to the Linode API.
+        """
+        global LASTCALL
+        ratelimit_sleep = _get_ratelimit_sleep()
+        apikey = _get_api_key()
+        if not isinstance(args, dict):
+            args = {}
+        if "api_key" not in args.keys():
+            args["api_key"] = apikey
+        if action and "api_action" not in args.keys():
+            args["api_action"] = f"{action}.{command}"
+        if header_dict is None:
+            header_dict = {}
+        if method != "POST":
+            header_dict["Accept"] = "application/json"
+        decode = True
+        if method == "DELETE":
+            decode = False
+        now = int(time.mktime(datetime.datetime.now().timetuple()))
+        if LASTCALL >= now:
+            time.sleep(ratelimit_sleep)
+        result = __utils__["http.query"](
+            url,
+            method,
+            params=args,
+            data=data,
+            header_dict=header_dict,
+            decode=decode,
+            decode_type="json",
+            text=True,
+            status=True,
+            hide_fields=["api_key", "rootPass"],
+            opts=__opts__,
+        )
+        if "ERRORARRAY" in result["dict"]:
+            if result["dict"]["ERRORARRAY"]:
+                error_list = []
+                for error in result["dict"]["ERRORARRAY"]:
+                    msg = error["ERRORMESSAGE"]
+                    if msg == "Authentication failed":
+                        raise SaltCloudSystemExit(
+                            "Linode API Key is expired or invalid"
+                        )
+                    else:
+                        error_list.append(msg)
+                raise SaltCloudException(
+                    "Linode API reported error(s): {}".format(", ".join(error_list))
+                )
+        LASTCALL = int(time.mktime(datetime.datetime.now().timetuple()))
+        log.debug("Linode Response Status Code: %s", result["status"])
+        return result["dict"]
+    def avail_images(self):
+        response = self._query("avail", "distributions")
+        ret = {}
+        for item in response["DATA"]:
+            name = item["LABEL"]
+            ret[name] = item
+        return ret
+    def avail_locations(self):
+        response = self._query("avail", "datacenters")
+        ret = {}
+        for item in response["DATA"]:
+            name = item["LOCATION"]
+            ret[name] = item
+        return ret
+    def avail_sizes(self):
+        response = self._query("avail", "LinodePlans")
+        ret = {}
+        for item in response["DATA"]:
+            name = item["LABEL"]
+            ret[name] = item
+        return ret
+    def boot(self, name=None, kwargs=None):
+        linode_id = kwargs.get("linode_id", None)
+        config_id = kwargs.get("config_id", None)
+        check_running = kwargs.get("check_running", True)
+        if config_id is None:
+            raise SaltCloudSystemExit("The boot function requires a 'config_id'.")
+        if linode_id is None:
+            linode_id = self._get_linode_id_from_name(name)
+            linode_item = name
+        else:
+            linode_item = linode_id
+        if check_running:
+            status = get_linode(kwargs={"linode_id": linode_id})["STATUS"]
+            if status == "1":
+                raise SaltCloudSystemExit(
+                    "Cannot boot Linode {0}. "
+                    + f"Linode {linode_item} is already running."
+                )
+        response = self._query(
+            "linode", "boot", args={"LinodeID": linode_id, "ConfigID": config_id}
+        )["DATA"]
+        boot_job_id = response["JobID"]
+        if not self._wait_for_job(linode_id, boot_job_id):
+            log.error("Boot failed for Linode %s.", linode_item)
+            return False
+        return True
+    def clone(self, kwargs=None):
+        linode_id = kwargs.get("linode_id", None)
+        datacenter_id = kwargs.get("datacenter_id", kwargs.get("location"))
+        plan_id = kwargs.get("plan_id", kwargs.get("size"))
+        required_params = [linode_id, datacenter_id, plan_id]
+        for item in required_params:
+            if item is None:
+                raise SaltCloudSystemExit(
+                    "The clone function requires a 'linode_id', 'datacenter_id', "
+                    "and 'plan_id' to be provided."
+                )
+        clone_args = {
+            "LinodeID": linode_id,
+            "DatacenterID": datacenter_id,
+            "PlanID": plan_id,
+        }
+        return self._query("linode", "clone", args=clone_args)
+    def create(self, vm_):
+        name = vm_["name"]
+        if not _validate_name(name):
+            return False
+        __utils__["cloud.fire_event"](
+            "event",
+            "starting create",
+            f"salt/cloud/{name}/creating",
+            args=__utils__["cloud.filter_event"](
+                "creating", vm_, ["name", "profile", "provider", "driver"]
+            ),
+            sock_dir=__opts__["sock_dir"],
+            transport=__opts__["transport"],
+        )
+        log.info("Creating Cloud VM %s", name)
+        data = {}
+        kwargs = {"name": name}
+        plan_id = None
+        size = vm_.get("size")
+        if size:
+            kwargs["size"] = size
+            plan_id = self.get_plan_id(kwargs={"label": size})
+        datacenter_id = None
+        location = vm_.get("location")
+        if location:
+            try:
+                datacenter_id = self._get_datacenter_id(location)
+            except KeyError:
+                datacenter_id = 2
+        clonefrom_name = vm_.get("clonefrom")
+        cloning = True if clonefrom_name else False
+        if cloning:
+            linode_id = self._get_linode_id_from_name(clonefrom_name)
+            clone_source = get_linode(kwargs={"linode_id": linode_id})
+            kwargs = {
+                "clonefrom": clonefrom_name,
+                "image": f"Clone of {clonefrom_name}",
+            }
+            if size is None:
+                size = clone_source["TOTALRAM"]
+                kwargs["size"] = size
+                plan_id = clone_source["PLANID"]
+            if location is None:
+                datacenter_id = clone_source["DATACENTERID"]
+            try:
+                result = clone(
+                    kwargs={
+                        "linode_id": linode_id,
+                        "datacenter_id": datacenter_id,
+                        "plan_id": plan_id,
+                    }
+                )
+            except Exception as err:  # pylint: disable=broad-except
+                log.error(
+                    "Error cloning '%s' on Linode.\n\n"
+                    "The following exception was thrown by Linode when trying to "
+                    "clone the specified machine:\n%s",
+                    clonefrom_name,
+                    err,
+                    exc_info_on_loglevel=logging.DEBUG,
+                )
+                return False
+        else:
+            kwargs["image"] = vm_["image"]
+            try:
+                result = self._query(
+                    "linode",
+                    "create",
+                    args={"PLANID": plan_id, "DATACENTERID": datacenter_id},
+                )
+            except Exception as err:  # pylint: disable=broad-except
+                log.error(
+                    "Error creating %s on Linode\n\n"
+                    "The following exception was thrown by Linode when trying to "
+                    "run the initial deployment:\n%s",
+                    name,
+                    err,
+                    exc_info_on_loglevel=logging.DEBUG,
+                )
+                return False
+        if "ERRORARRAY" in result:
+            for error_data in result["ERRORARRAY"]:
+                log.error(
+                    "Error creating %s on Linode\n\n"
+                    "The Linode API returned the following: %s\n",
+                    name,
+                    error_data["ERRORMESSAGE"],
+                )
+                return False
+        __utils__["cloud.fire_event"](
+            "event",
+            "requesting instance",
+            f"salt/cloud/{name}/requesting",
+            args=__utils__["cloud.filter_event"](
+                "requesting", vm_, ["name", "profile", "provider", "driver"]
+            ),
+            sock_dir=__opts__["sock_dir"],
+            transport=__opts__["transport"],
+        )
+        node_id = self._clean_data(result)["LinodeID"]
+        data["id"] = node_id
+        if not self._wait_for_status(
+            node_id, status=(self._get_status_id_by_name("brand_new"))
+        ):
+            log.error(
+                "Error creating %s on LINODE\n\nwhile waiting for initial ready status",
+                name,
+                exc_info_on_loglevel=logging.DEBUG,
+            )
+        self._update_linode(node_id, update_args={"Label": name})
+        log.debug("Set name for %s - was linode%s.", name, node_id)
+        private_ip_assignment = _get_private_ip(vm_)
+        if private_ip_assignment:
+            self._create_private_ip(node_id)
+        ssh_interface = _get_ssh_interface(vm_)
+        if ssh_interface == "private_ips" and private_ip_assignment is False:
+            self._create_private_ip(node_id)
+            private_ip_assignment = True
+        if cloning:
+            config_id = get_config_id(kwargs={"linode_id": node_id})["config_id"]
+        else:
+            log.debug("Creating disks for %s", name)
+            root_disk_id = self._create_disk_from_distro(vm_, node_id)["DiskID"]
+            swap_disk_id = self._create_swap_disk(vm_, node_id)["DiskID"]
+            config_id = create_config(
+                kwargs={
+                    "name": name,
+                    "linode_id": node_id,
+                    "root_disk_id": root_disk_id,
+                    "swap_disk_id": swap_disk_id,
+                }
+            )["ConfigID"]
+        self.boot(
+            kwargs={
+                "linode_id": node_id,
+                "config_id": config_id,
+                "check_running": False,
+            }
+        )
+        node_data = get_linode(kwargs={"linode_id": node_id})
+        ips = self._get_ips(node_id)
+        state = int(node_data["STATUS"])
+        data["image"] = kwargs["image"]
+        data["name"] = name
+        data["size"] = size
+        data["state"] = self._get_status_descr_by_id(state)
+        data["private_ips"] = ips["private_ips"]
+        data["public_ips"] = ips["public_ips"]
+        if ssh_interface == "private_ips":
+            vm_["ssh_host"] = data["private_ips"][0]
+        else:
+            vm_["ssh_host"] = data["public_ips"][0]
+        vm_["password"] = _get_password(vm_)
+        vm_["public_ips"] = ips["public_ips"]
+        vm_["private_ips"] = ips["private_ips"]
+        __utils__["cloud.fire_event"](
+            "event",
+            "waiting for ssh",
+            f"salt/cloud/{name}/waiting_for_ssh",
+            sock_dir=__opts__["sock_dir"],
+            args={"ip_address": vm_["ssh_host"]},
+            transport=__opts__["transport"],
+        )
+        ret = __utils__["cloud.bootstrap"](vm_, __opts__)
+        ret.update(data)
+        log.info("Created Cloud VM '%s'", name)
+        log.debug("'%s' VM creation details:\n%s", name, pprint.pformat(data))
+        __utils__["cloud.fire_event"](
+            "event",
+            "created instance",
+            f"salt/cloud/{name}/created",
+            args=__utils__["cloud.filter_event"](
+                "created", vm_, ["name", "profile", "provider", "driver"]
+            ),
+            sock_dir=__opts__["sock_dir"],
+            transport=__opts__["transport"],
+        )
+        return ret
+    def create_config(self, kwargs=None):
+        name = kwargs.get("name", None)
+        linode_id = kwargs.get("linode_id", None)
+        root_disk_id = kwargs.get("root_disk_id", None)
+        swap_disk_id = kwargs.get("swap_disk_id", None)
+        data_disk_id = kwargs.get("data_disk_id", None)
+        kernel_id = kwargs.get("kernel_id", None)
+        if kernel_id is None:
+            kernel_id = 138
+        required_params = [name, linode_id, root_disk_id, swap_disk_id]
+        for item in required_params:
+            if item is None:
+                raise SaltCloudSystemExit(
+                    "The create_config functions requires a 'name', 'linode_id', "
+                    "'root_disk_id', and 'swap_disk_id'."
+                )
+        if kernel_id is None:
+            kernel_id = 138
+        if not linode_id:
+            instance = self._get_linode_by_name(name)
+            linode_id = instance.get("id", None)
+        disklist = f"{root_disk_id},{swap_disk_id}"
+        if data_disk_id is not None:
+            disklist = f"{root_disk_id},{swap_disk_id},{data_disk_id}"
+        config_args = {
+            "LinodeID": int(linode_id),
+            "KernelID": int(kernel_id),
+            "Label": name,
+            "DiskList": disklist,
+        }
+        result = self._query("linode", "config.create", args=config_args)
+        return result.get("DATA", None)
+    def _create_disk_from_distro(self, vm_, linode_id):
+        kwargs = {}
+        swap_size = _get_swap_size(vm_)
+        pub_key = _get_ssh_key(vm_)
+        root_password = _get_password(vm_)
+        if pub_key:
+            kwargs.update({"rootSSHKey": pub_key})
+        if root_password:
+            kwargs.update({"rootPass": root_password})
+        else:
+            raise SaltCloudConfigError("The Linode driver requires a password.")
+        kwargs.update(
+            {
+                "LinodeID": linode_id,
+                "DistributionID": self._get_distribution_id(vm_),
+                "Label": vm_["name"],
+                "Size": self._get_disk_size(vm_, swap_size, linode_id),
+            }
+        )
+        result = self._query("linode", "disk.createfromdistribution", args=kwargs)
+        return self._clean_data(result)
+    def _create_swap_disk(self, vm_, linode_id, swap_size=None):
+        r"""
+        Creates the disk for the specified Linode.
+        vm\_
+            The VM profile to create the swap disk for.
+        linode_id
+            The ID of the Linode to create the swap disk for.
+        swap_size
+            The size of the disk, in MB.
+        """
+        kwargs = {}
+        if not swap_size:
+            swap_size = _get_swap_size(vm_)
+        kwargs.update(
+            {
+                "LinodeID": linode_id,
+                "Label": vm_["name"],
+                "Type": "swap",
+                "Size": swap_size,
+            }
+        )
+        result = self._query("linode", "disk.create", args=kwargs)
+        return self._clean_data(result)
+    def _create_data_disk(self, vm_=None, linode_id=None, data_size=None):
+        kwargs = {}
+        kwargs.update(
+            {
+                "LinodeID": linode_id,
+                "Label": vm_["name"] + "_data",
+                "Type": "ext4",
+                "Size": data_size,
+            }
+        )
+        result = self._query("linode", "disk.create", args=kwargs)
+        return self._clean_data(result)
+    def _create_private_ip(self, linode_id):
+        r"""
+        Creates a private IP for the specified Linode.
+        linode_id
+            The ID of the Linode to create the IP address for.
+        """
+        kwargs = {"LinodeID": linode_id}
+        result = self._query("linode", "ip.addprivate", args=kwargs)
+        return self._clean_data(result)
+    def destroy(self, name):
+        __utils__["cloud.fire_event"](
+            "event",
+            "destroying instance",
+            f"salt/cloud/{name}/destroying",
+            args={"name": name},
+            sock_dir=__opts__["sock_dir"],
+            transport=__opts__["transport"],
+        )
+        linode_id = self._get_linode_id_from_name(name)
+        response = self._query(
+            "linode", "delete", args={"LinodeID": linode_id, "skipChecks": True}
+        )
+        __utils__["cloud.fire_event"](
+            "event",
+            "destroyed instance",
+            f"salt/cloud/{name}/destroyed",
+            args={"name": name},
+            sock_dir=__opts__["sock_dir"],
+            transport=__opts__["transport"],
+        )
+        if __opts__.get("update_cachedir", False) is True:
+            __utils__["cloud.delete_minion_cachedir"](
+                name, _get_active_provider_name().split(":")[0], __opts__
+            )
+        return response
+    def _decode_linode_plan_label(self, label):
+        """
+        Attempts to decode a user-supplied Linode plan label
+        into the format in Linode API output
+        label
+            The label, or name, of the plan to decode.
+        Example:
+            `Linode 2048` will decode to `Linode 2GB`
+        """
+        sizes = self.avail_sizes()
+        if label not in sizes:
+            if "GB" in label:
+                raise SaltCloudException(
+                    "Invalid Linode plan ({}) specified - call avail_sizes() for all"
+                    " available options".format(label)
+                )
+            else:
+                plan = label.split()
+                if len(plan) != 2:
+                    raise SaltCloudException(
+                        "Invalid Linode plan ({}) specified - call avail_sizes() for"
+                        " all available options".format(label)
+                    )
+                plan_type = plan[0]
+                try:
+                    plan_size = int(plan[1])
+                except TypeError:
+                    plan_size = 0
+                    log.debug(
+                        "Failed to decode Linode plan label in Cloud Profile: %s", label
+                    )
+                if plan_type == "Linode" and plan_size == 1024:
+                    plan_type = "Nanode"
+                plan_size = plan_size / 1024
+                new_label = f"{plan_type} {plan_size}GB"
+                if new_label not in sizes:
+                    raise SaltCloudException(
+                        "Invalid Linode plan ({}) specified - call avail_sizes() for"
+                        " all available options".format(new_label)
+                    )
+                log.warning(
+                    "An outdated Linode plan label was detected in your Cloud "
+                    "Profile (%s). Please update the profile to use the new "
+                    "label format (%s) for the requested Linode plan size.",
+                    label,
+                    new_label,
+                )
+                label = new_label
+        return sizes[label]["PLANID"]
+    def get_config_id(self, kwargs=None):
+        name = kwargs.get("name", None)
+        linode_id = kwargs.get("linode_id", None)
+        if name is None and linode_id is None:
+            raise SaltCloudSystemExit(
+                "The get_config_id function requires either a 'name' or a 'linode_id' "
+                "to be provided."
+            )
+        if linode_id is None:
+            linode_id = self._get_linode_id_from_name(name)
+        response = self._query("linode", "config.list", args={"LinodeID": linode_id})[
+            "DATA"
+        ]
+        config_id = {"config_id": response[0]["ConfigID"]}
+        return config_id
+    def _get_datacenter_id(self, location):
+        """
+        Returns the Linode Datacenter ID.
+        location
+            The location, or name, of the datacenter to get the ID from.
+        """
+        return avail_locations()[location]["DATACENTERID"]
+    def _get_disk_size(self, vm_, swap, linode_id):
+        r"""
+        Returns the size of of the root disk in MB.
+        vm\_
+            The VM to get the disk size for.
+        """
+        disk_size = get_linode(kwargs={"linode_id": linode_id})["TOTALHD"]
+        return config.get_cloud_config_value(
+            "disk_size", vm_, __opts__, default=disk_size - swap
+        )
+    def _get_distribution_id(self, vm_):
+        r"""
+        Returns the distribution ID for a VM
+        vm\_
+            The VM to get the distribution ID for
+        """
+        distributions = self._query("avail", "distributions")["DATA"]
+        vm_image_name = config.get_cloud_config_value("image", vm_, __opts__)
+        distro_id = ""
+        for distro in distributions:
+            if vm_image_name == distro["LABEL"]:
+                distro_id = distro["DISTRIBUTIONID"]
+                return distro_id
+        if not distro_id:
+            raise SaltCloudNotFound(
+                "The DistributionID for the '{}' profile could not be found.\nThe '{}'"
+                " instance could not be provisioned. The following distributions are"
+                " available:\n{}".format(
+                    vm_image_name,
+                    vm_["name"],
+                    pprint.pprint(
+                        sorted(
+                            distro["LABEL"].encode(__salt_system_encoding__)
+                            for distro in distributions
+                        )
+                    ),
+                )
+            )
+    def get_plan_id(self, kwargs=None):
+        label = kwargs.get("label", None)
+        if label is None:
+            raise SaltCloudException("The get_plan_id function requires a 'label'.")
+        return self._decode_linode_plan_label(label)
+    def _get_ips(self, linode_id=None):
+        """
+        Returns public and private IP addresses.
+        linode_id
+            Limits the IP addresses returned to the specified Linode ID.
+        """
+        if linode_id:
+            ips = self._query("linode", "ip.list", args={"LinodeID": linode_id})
+        else:
+            ips = self._query("linode", "ip.list")
+        ips = ips["DATA"]
+        ret = {}
+        for item in ips:
+            node_id = str(item["LINODEID"])
+            if item["ISPUBLIC"] == 1:
+                key = "public_ips"
+            else:
+                key = "private_ips"
+            if ret.get(node_id) is None:
+                ret.update({node_id: {"public_ips": [], "private_ips": []}})
+            ret[node_id][key].append(item["IPADDRESS"])
+        if linode_id:
+            _all_ips = {"public_ips": [], "private_ips": []}
+            matching_id = ret.get(str(linode_id))
+            if matching_id:
+                _all_ips["private_ips"] = matching_id["private_ips"]
+                _all_ips["public_ips"] = matching_id["public_ips"]
+            ret = _all_ips
+        return ret
+    def _wait_for_job(self, linode_id, job_id, timeout=300, quiet=True):
+        """
+        Wait for a Job to return.
+        linode_id
+            The ID of the Linode to wait on. Required.
+        job_id
+            The ID of the job to wait for.
+        timeout
+            The amount of time to wait for a status to update.
+        quiet
+            Log status updates to debug logs when True. Otherwise, logs to info.
+        """
+        interval = 5
+        iterations = int(timeout / interval)
+        for i in range(0, iterations):
+            jobs_result = self._query(
+                "linode", "job.list", args={"LinodeID": linode_id}
+            )["DATA"]
+            if (
+                jobs_result[0]["JOBID"] == job_id
+                and jobs_result[0]["HOST_SUCCESS"] == 1
+            ):
+                return True
+            time.sleep(interval)
+            log.log(
+                logging.INFO if not quiet else logging.DEBUG,
+                "Still waiting on Job %s for Linode %s.",
+                job_id,
+                linode_id,
+            )
+        return False
+    def _wait_for_status(self, linode_id, status=None, timeout=300, quiet=True):
+        """
+        Wait for a certain status from Linode.
+        linode_id
+            The ID of the Linode to wait on. Required.
+        status
+            The status to look for to update.
+        timeout
+            The amount of time to wait for a status to update.
+        quiet
+            Log status updates to debug logs when False. Otherwise, logs to info.
+        """
+        if status is None:
+            status = self._get_status_id_by_name("brand_new")
+        status_desc_waiting = self._get_status_descr_by_id(status)
+        interval = 5
+        iterations = int(timeout / interval)
+        for i in range(0, iterations):
+            result = get_linode(kwargs={"linode_id": linode_id})
+            if result["STATUS"] == status:
+                return True
+            status_desc_result = self._get_status_descr_by_id(result["STATUS"])
+            time.sleep(interval)
+            log.log(
+                logging.INFO if not quiet else logging.DEBUG,
+                "Status for Linode %s is '%s', waiting for '%s'.",
+                linode_id,
+                status_desc_result,
+                status_desc_waiting,
+            )
+        return False
+    def _list_linodes(self, full=False):
+        nodes = self._query("linode", "list")["DATA"]
+        ips = self._get_ips()
+        ret = {}
+        for node in nodes:
+            this_node = {}
+            linode_id = str(node["LINODEID"])
+            this_node["id"] = linode_id
+            this_node["image"] = node["DISTRIBUTIONVENDOR"]
+            this_node["name"] = node["LABEL"]
+            this_node["size"] = node["TOTALRAM"]
+            state = int(node["STATUS"])
+            this_node["state"] = self._get_status_descr_by_id(state)
+            for key, val in ips.items():
+                if key == linode_id:
+                    this_node["private_ips"] = val[1]
+                    this_node["public_ips"] = val[0]
+            if full:
+                this_node["extra"] = node
+            ret[node["LABEL"]] = this_node
+        return ret
+    def list_nodes(self):
+        return self._list_linodes()
+    def list_nodes_full(self):
+        return self._list_linodes(full=True)
+    def list_nodes_min(self):
+        ret = {}
+        nodes = self._query("linode", "list")["DATA"]
+        for node in nodes:
+            name = node["LABEL"]
+            ret[name] = {
+                "id": str(node["LINODEID"]),
+                "state": self._get_status_descr_by_id(int(node["STATUS"])),
+            }
+        return ret
+    def show_instance(self, name):
+        node_id = self._get_linode_id_from_name(name)
+        node_data = get_linode(kwargs={"linode_id": node_id})
+        ips = self._get_ips(node_id)
+        state = int(node_data["STATUS"])
+        return {
+            "id": node_data["LINODEID"],
+            "image": node_data["DISTRIBUTIONVENDOR"],
+            "name": node_data["LABEL"],
+            "size": node_data["TOTALRAM"],
+            "state": self._get_status_descr_by_id(state),
+            "private_ips": ips["private_ips"],
+            "public_ips": ips["public_ips"],
+        }
+    def show_pricing(self, kwargs=None):
+        profile = __opts__["profiles"].get(kwargs["profile"], {})
+        if not profile:
+            raise SaltCloudNotFound("The requested profile was not found.")
+        provider = profile.get("provider", "0:0")
+        comps = provider.split(":")
+        if len(comps) < 2 or comps[1] != "linode":
+            raise SaltCloudException("The requested profile does not belong to Linode.")
+        plan_id = self.get_plan_id(kwargs={"label": profile["size"]})
+        response = self._query("avail", "linodeplans", args={"PlanID": plan_id})[
+            "DATA"
+        ][0]
+        ret = {}
+        ret["per_hour"] = response["HOURLY"]
+        ret["per_day"] = ret["per_hour"] * 24
+        ret["per_week"] = ret["per_day"] * 7
+        ret["per_month"] = response["PRICE"]
+        ret["per_year"] = ret["per_month"] * 12
+        return {profile["profile"]: ret}
+    def _update_linode(self, linode_id, update_args=None):
+        update_args.update({"LinodeID": linode_id})
+        result = self._query("linode", "update", args=update_args)
+        return self._clean_data(result)
+    def _get_linode_id_from_name(self, name):
+        node = self._get_linode_by_name(name)
+        return node.get("LINODEID", None)
+    def _get_linode_by_name(self, name):
+        nodes = self._query("linode", "list")["DATA"]
+        for node in nodes:
+            if name == node["LABEL"]:
+                return node
+        raise SaltCloudNotFound(f"The specified name, {name}, could not be found.")
+    def _get_linode_by_id(self, linode_id):
+        result = self._query("linode", "list", args={"LinodeID": linode_id})
+        return result["DATA"][0]
+    def start(self, name):
+        node_id = self._get_linode_id_from_name(name)
+        node = get_linode(kwargs={"linode_id": node_id})
+        if node["STATUS"] == 1:
+            return {
+                "success": True,
+                "action": "start",
+                "state": "Running",
+                "msg": "Machine already running",
+            }
+        response = self._query("linode", "boot", args={"LinodeID": node_id})["DATA"]
+        if self._wait_for_job(node_id, response["JobID"]):
+            return {"state": "Running", "action": "start", "success": True}
+        else:
+            return {"action": "start", "success": False}
+    def stop(self, name):
+        node_id = self._get_linode_id_from_name(name)
+        node = get_linode(kwargs={"linode_id": node_id})
+        if node["STATUS"] == 2:
+            return {
+                "success": True,
+                "state": "Stopped",
+                "msg": "Machine already stopped",
+            }
+        response = self._query("linode", "shutdown", args={"LinodeID": node_id})["DATA"]
+        if self._wait_for_job(node_id, response["JobID"]):
+            return {"state": "Stopped", "action": "stop", "success": True}
+        return {"action": "stop", "success": False}
+    def reboot(self, name):
+        node_id = self._get_linode_id_from_name(name)
+        response = self._query("linode", "reboot", args={"LinodeID": node_id})
+        data = self._clean_data(response)
+        reboot_jid = data["JobID"]
+        if not self._wait_for_job(node_id, reboot_jid):
+            log.error("Reboot failed for %s.", name)
+            return False
+        return data
+    def _clean_data(self, api_response):
+        """
+        Returns the DATA response from a Linode API query as a single pre-formatted dictionary
+        api_response
+            The query to be cleaned.
+        """
+        data = {}
+        data.update(api_response["DATA"])
+        if not data:
+            response_data = api_response["DATA"]
+            data.update(response_data)
+        return data
+    def _get_status_descr_by_id(self, status_id):
+        """
+        Return linode status by ID
+        status_id
+            linode VM status ID
+        """
+        for status_name, status_data in LINODE_STATUS.items():
+            if status_data["code"] == int(status_id):
+                return status_data["descr"]
+        return LINODE_STATUS.get(status_id, None)
+    def _get_status_id_by_name(self, status_name):
+        """
+        Return linode status description by internalstatus name
+        status_name
+            internal linode VM status name
+        """
+        return LINODE_STATUS.get(status_name, {}).get("code", None)
 def avail_images(call=None):
     """
     Return available Linode images.
     CLI Example:
     .. code-block:: bash
         salt-cloud --list-images my-linode-config
         salt-cloud -f avail_images my-linode-config
     """
     if call == "action":
         raise SaltCloudException(
             "The avail_images function must be called with -f or --function."
         )
-    return LinodeAPIv4.get_api_instance().avail_images()
+    return _get_cloud_interface().avail_images()
 def avail_locations(call=None):
     """
     Return available Linode datacenter locations.
     CLI Example:
     .. code-block:: bash
         salt-cloud --list-locations my-linode-config
         salt-cloud -f avail_locations my-linode-config
     """
     if call == "action":
         raise SaltCloudException(
             "The avail_locations function must be called with -f or --function."
         )
-    return LinodeAPIv4.get_api_instance().avail_locations()
+    return _get_cloud_interface().avail_locations()
 def avail_sizes(call=None):
     """
     Return available Linode sizes.
     CLI Example:
     .. code-block:: bash
         salt-cloud --list-sizes my-linode-config
         salt-cloud -f avail_sizes my-linode-config
     """
     if call == "action":
         raise SaltCloudException(
             "The avail_locations function must be called with -f or --function."
         )
-    return LinodeAPIv4.get_api_instance().avail_sizes()
-def set_backup_schedule(name=None, kwargs=None, call=None):
-    """
-    Set the backup schedule for a Linode.
-    name
-        The name (label) of the Linode. Can be used instead of
-        ``linode_id``.
-    linode_id
-        The ID of the Linode instance to set the backup schedule for.
-        If provided, will be used as an alternative to ``name`` and
-        reduces the number of API calls to Linode by one. Will be
-        preferred over ``name``.
-    auto_enable
-        If ``True``, automatically enable the backup feature for the Linode
-        if it wasn't already enabled. Optional parameter, default to ``False``.
-    day
-        Possible values:
-        ``Sunday``, ``Monday``, ``Tuesday``, ``Wednesday``,
-        ``Thursday``, ``Friday``, ``Saturday``
-        The day of the week that your Linode's weekly Backup is taken.
-        If not set manually, a day will be chosen for you. Backups are
-        taken every day, but backups taken on this day are preferred
-        when selecting backups to retain for a longer period.
-        If not set manually, then when backups are initially enabled,
-        this may come back as ``Scheduling`` until the day is automatically
-        selected.
-    window
-        Possible values:
-        ``W0``, ``W2``, ``W4``, ``W6``, ``W8``, ``W10``,
-        ``W12``, ``W14``, ``W16``, ``W18``, ``W20``, ``W22``
-        The window in which your backups will be taken, in UTC. A backups
-        window is a two-hour span of time in which the backup may occur.
-        For example, ``W10`` indicates that your backups should be taken
-        between 10:00 and 12:00. If you do not choose a backup window, one
-        will be selected for you automatically.
-        If not set manually, when backups are initially enabled this may come
-        back as ``Scheduling`` until the window is automatically selected.
-    Can be called as an action (which requires a name):
-    .. code-block:: bash
-        salt-cloud -a set_backup_schedule my-linode-instance day=Monday window=W20 auto_enable=True
-    ...or as a function (which requires either a name or linode_id):
-    .. code-block:: bash
-        salt-cloud -f set_backup_schedule my-linode-provider name=my-linode-instance day=Monday window=W20 auto_enable=True
-        salt-cloud -f set_backup_schedule my-linode-provider linode_id=1225876 day=Monday window=W20 auto_enable=True
-    """
-    if name is None and call == "action":
-        raise SaltCloudSystemExit(
-            "The set_backup_schedule backup schedule "
-            "action requires the name of the Linode.",
-        )
-    if kwargs is None:
-        kwargs = {}
-    if call == "function":
-        name = kwargs.get("name", None)
-    linode_id = kwargs.get("linode_id")
-    auto_enable = str(kwargs.get("auto_enable")).lower() == "true"
-    if name is None and linode_id is None:
-        raise SaltCloudSystemExit(
-            "The set_backup_schedule function requires "
-            "either a 'name' or a 'linode_id'."
-        )
-    return LinodeAPIv4.get_api_instance().set_backup_schedule(
-        day=kwargs.get("day"),
-        window=kwargs.get("window"),
-        label=name,
-        linode_id=linode_id,
-        auto_enable=auto_enable,
-    )
+    return _get_cloud_interface().avail_sizes()
 def boot(name=None, kwargs=None, call=None):
     """
     Boot a Linode.
     name
         The name of the Linode to boot. Can be used instead of ``linode_id``.
     linode_id
         The ID of the Linode to boot. If provided, will be used as an
         alternative to ``name`` and reduces the number of API calls to
         Linode by one. Will be preferred over ``name``.
     config_id
@@ -968,57 +1741,64 @@
     if name is None and call == "action":
         raise SaltCloudSystemExit("The boot action requires a 'name'.")
     linode_id = kwargs.get("linode_id", None)
     config_id = kwargs.get("config_id", None)
     if call == "function":
         name = kwargs.get("name", None)
     if name is None and linode_id is None:
         raise SaltCloudSystemExit(
             "The boot function requires either a 'name' or a 'linode_id'."
         )
-    return LinodeAPIv4.get_api_instance().boot(name=name, kwargs=kwargs)
+    return _get_cloud_interface().boot(name=name, kwargs=kwargs)
 def clone(kwargs=None, call=None):
     """
     Clone a Linode.
     linode_id
         The ID of the Linode to clone. Required.
     location
         The location of the new Linode. Required.
     size
         The size of the new Linode (must be greater than or equal to the clone source). Required.
+    datacenter_id
+        The ID of the Datacenter where the Linode will be placed. Required for APIv3 usage.
+        Deprecated. Use ``location`` instead.
+    plan_id
+        The ID of the plan (size) of the Linode. Required. Required for APIv3 usage.
+        Deprecated. Use ``size`` instead.
     CLI Example:
     .. code-block:: bash
-        salt-cloud -f clone my-linode-config linode_id=1234567 location=us-central size=g6-standard-1
+        salt-cloud -f clone my-linode-config linode_id=1234567 datacenter_id=2 plan_id=5
     """
     if call == "action":
         raise SaltCloudSystemExit(
             "The clone function must be called with -f or --function."
         )
-    return LinodeAPIv4.get_api_instance().clone(kwargs=kwargs)
+    return _get_cloud_interface().clone(kwargs=kwargs)
 def create(vm_):
     """
     Create a single Linode VM.
     """
     try:
         if (
             vm_["profile"]
             and config.is_profile_configured(
                 __opts__,
                 _get_active_provider_name() or "linode",
                 vm_["profile"],
                 vm_=vm_,
             )
-        ) is False:
+            is False
+        ):
             return False
     except AttributeError:
         pass
-    return LinodeAPIv4.get_api_instance().create(vm_)
+    return _get_cloud_interface().create(vm_)
 def create_config(kwargs=None, call=None):
     """
     Creates a Linode Configuration Profile.
     name
         The name of the VM to create the config for.
     linode_id
         The ID of the Linode to create the configuration for.
     root_disk_id
         The Root Disk ID to be used for this config.
     swap_disk_id
@@ -1026,195 +1806,210 @@
     data_disk_id
         The Data Disk ID to be used for this config.
     .. versionadded:: 2016.3.0
     kernel_id
         The ID of the kernel to use for this configuration profile.
     """
     if call == "action":
         raise SaltCloudSystemExit(
             "The create_config function must be called with -f or --function."
         )
-    return LinodeAPIv4.get_api_instance().create_config(kwargs=kwargs)
+    return _get_cloud_interface().create_config(kwargs=kwargs)
 def destroy(name, call=None):
     """
     Destroys a Linode by name.
     name
         The name of VM to be be destroyed.
     CLI Example:
     .. code-block:: bash
         salt-cloud -d vm_name
     """
     if call == "function":
         raise SaltCloudException(
             "The destroy action must be called with -d, --destroy, -a or --action."
         )
-    return LinodeAPIv4.get_api_instance().destroy(name)
+    return _get_cloud_interface().destroy(name)
 def get_config_id(kwargs=None, call=None):
     """
     Returns a config_id for a given linode.
     .. versionadded:: 2015.8.0
     name
         The name of the Linode for which to get the config_id. Can be used instead
         of ``linode_id``.
     linode_id
         The ID of the Linode for which to get the config_id. Can be used instead
         of ``name``.
     CLI Example:
     .. code-block:: bash
         salt-cloud -f get_config_id my-linode-config name=my-linode
         salt-cloud -f get_config_id my-linode-config linode_id=1234567
     """
     if call == "action":
         raise SaltCloudException(
             "The get_config_id function must be called with -f or --function."
         )
-    return LinodeAPIv4.get_api_instance().get_config_id(kwargs=kwargs)
+    return _get_cloud_interface().get_config_id(kwargs=kwargs)
 def get_linode(kwargs=None, call=None):
     """
     Returns data for a single named Linode.
     name
         The name of the Linode for which to get data. Can be used instead
         ``linode_id``. Note this will induce an additional API call
         compared to using ``linode_id``.
     linode_id
         The ID of the Linode for which to get data. Can be used instead of
         ``name``.
     CLI Example:
     .. code-block:: bash
         salt-cloud -f get_linode my-linode-config name=my-instance
         salt-cloud -f get_linode my-linode-config linode_id=1234567
     """
     if call == "action":
         raise SaltCloudSystemExit(
             "The get_linode function must be called with -f or --function."
         )
-    return LinodeAPIv4.get_api_instance().get_linode(kwargs=kwargs)
+    return _get_cloud_interface().get_linode(kwargs=kwargs)
+def get_plan_id(kwargs=None, call=None):
+    """
+    Returns the Linode Plan ID.
+    label
+        The label, or name, of the plan to get the ID from.
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f get_plan_id linode label="Nanode 1GB"
+        salt-cloud -f get_plan_id linode label="Linode 2GB"
+    """
+    if call == "action":
+        raise SaltCloudException(
+            "The show_instance action must be called with -f or --function."
+        )
+    return _get_cloud_interface().get_plan_id(kwargs=kwargs)
 def list_nodes(call=None):
     """
     Returns a list of linodes, keeping only a brief listing.
     CLI Example:
     .. code-block:: bash
         salt-cloud -Q
         salt-cloud --query
         salt-cloud -f list_nodes my-linode-config
     .. note::
         The ``image`` label only displays information about the VM's distribution vendor,
         such as "Debian" or "RHEL" and does not display the actual image name. This is
         due to a limitation of the Linode API.
     """
     if call == "action":
         raise SaltCloudException(
             "The list_nodes function must be called with -f or --function."
         )
-    return LinodeAPIv4.get_api_instance().list_nodes()
+    return _get_cloud_interface().list_nodes()
 def list_nodes_full(call=None):
     """
     List linodes, with all available information.
     CLI Example:
     .. code-block:: bash
         salt-cloud -F
         salt-cloud --full-query
         salt-cloud -f list_nodes_full my-linode-config
     .. note::
         The ``image`` label only displays information about the VM's distribution vendor,
         such as "Debian" or "RHEL" and does not display the actual image name. This is
         due to a limitation of the Linode API.
     """
     if call == "action":
         raise SaltCloudException(
             "The list_nodes_full function must be called with -f or --function."
         )
-    return LinodeAPIv4.get_api_instance().list_nodes_full()
+    return _get_cloud_interface().list_nodes_full()
 def list_nodes_min(call=None):
     """
     Return a list of the VMs that are on the provider. Only a list of VM names and
     their state is returned. This is the minimum amount of information needed to
     check for existing VMs.
     .. versionadded:: 2015.8.0
     CLI Example:
     .. code-block:: bash
         salt-cloud -f list_nodes_min my-linode-config
         salt-cloud --function list_nodes_min my-linode-config
     """
     if call == "action":
         raise SaltCloudSystemExit(
             "The list_nodes_min function must be called with -f or --function."
         )
-    return LinodeAPIv4.get_api_instance().list_nodes_min()
+    return _get_cloud_interface().list_nodes_min()
 def list_nodes_select(call=None):
     """
     Return a list of the VMs that are on the provider, with select fields.
     """
-    return LinodeAPIv4.get_api_instance().list_nodes_select(call)
+    return _get_cloud_interface().list_nodes_select(call)
 def reboot(name, call=None):
     """
     Reboot a linode.
     .. versionadded:: 2015.8.0
     name
         The name of the VM to reboot.
     CLI Example:
     .. code-block:: bash
         salt-cloud -a reboot vm_name
     """
     if call != "action":
         raise SaltCloudException(
             "The show_instance action must be called with -a or --action."
         )
-    return LinodeAPIv4.get_api_instance().reboot(name)
+    return _get_cloud_interface().reboot(name)
 def show_instance(name, call=None):
     """
     Displays details about a particular Linode VM. Either a name or a linode_id must
     be provided.
     .. versionadded:: 2015.8.0
     name
         The name of the VM for which to display details.
     CLI Example:
     .. code-block:: bash
         salt-cloud -a show_instance vm_name
     .. note::
         The ``image`` label only displays information about the VM's distribution vendor,
         such as "Debian" or "RHEL" and does not display the actual image name. This is
         due to a limitation of the Linode API.
     """
     if call != "action":
         raise SaltCloudException(
             "The show_instance action must be called with -a or --action."
         )
-    return LinodeAPIv4.get_api_instance().show_instance(name)
+    return _get_cloud_interface().show_instance(name)
 def show_pricing(kwargs=None, call=None):
     """
     Show pricing for a particular profile. This is only an estimate, based on
     unofficial pricing sources.
     .. versionadded:: 2015.8.0
     CLI Example:
     .. code-block:: bash
         salt-cloud -f show_pricing my-linode-config profile=my-linode-profile
     """
     if call != "function":
         raise SaltCloudException(
             "The show_instance action must be called with -f or --function."
         )
-    return LinodeAPIv4.get_api_instance().show_pricing(kwargs=kwargs)
+    return _get_cloud_interface().show_pricing(kwargs=kwargs)
 def start(name, call=None):
     """
     Start a VM in Linode.
     name
         The name of the VM to start.
     CLI Example:
     .. code-block:: bash
         salt-cloud -a stop vm_name
     """
     if call != "action":
         raise SaltCloudException("The start action must be called with -a or --action.")
-    return LinodeAPIv4.get_api_instance().start(name)
+    return _get_cloud_interface().start(name)
 def stop(name, call=None):
     """
     Stop a VM in Linode.
     name
         The name of the VM to stop.
     CLI Example:
     .. code-block:: bash
         salt-cloud -a stop vm_name
     """
     if call != "action":
         raise SaltCloudException("The stop action must be called with -a or --action.")
-    return LinodeAPIv4.get_api_instance().stop(name)
+    return _get_cloud_interface().stop(name)

--- a//dev/null
+++ b/salt/cloud/clouds/msazure.py
@@ -0,0 +1,2826 @@
+"""
+Azure Cloud Module
+==================
+The Azure cloud module is used to control access to Microsoft Azure
+.. warning::
+    This cloud provider will be removed from Salt in version 3007 due to
+    the deprecation of the "Classic" API for Azure. Please migrate to
+    `Azure Resource Manager by March 1, 2023
+    <https://docs.microsoft.com/en-us/azure/virtual-machines/classic-vm-deprecation>`_
+:depends:
+    * `Microsoft Azure SDK for Python <https://pypi.python.org/pypi/azure/1.0.2>`_ >= 1.0.2
+    * python-requests, for Python < 2.7.9
+:configuration:
+    Required provider parameters:
+    * ``apikey``
+    * ``certificate_path``
+    * ``subscription_id``
+    * ``backend``
+    A Management Certificate (.pem and .crt files) must be created and the .pem
+    file placed on the same machine that salt-cloud is run from. Information on
+    creating the pem file to use, and uploading the associated cer file can be
+    found at:
+    http://www.windowsazure.com/en-us/develop/python/how-to-guides/service-management/
+    For users with Python < 2.7.9, ``backend`` must currently be set to ``requests``.
+Example ``/etc/salt/cloud.providers`` or
+``/etc/salt/cloud.providers.d/azure.conf`` configuration:
+.. code-block:: yaml
+    my-azure-config:
+      driver: azure
+      subscription_id: 3287abc8-f98a-c678-3bde-326766fd3617
+      certificate_path: /etc/salt/azure.pem
+      management_host: management.core.windows.net
+"""
+import copy
+import logging
+import pprint
+import time
+from functools import wraps
+import salt.config as config
+import salt.utils.args
+import salt.utils.cloud
+import salt.utils.stringutils
+import salt.utils.versions
+import salt.utils.yaml
+from salt.exceptions import SaltCloudSystemExit
+HAS_LIBS = False
+try:
+    import azure
+    import azure.servicemanagement
+    import azure.storage
+    from azure.common import (
+        AzureConflictHttpError,
+        AzureException,
+        AzureMissingResourceHttpError,
+    )
+    import salt.utils.msazure
+    from salt.utils.msazure import object_to_dict
+    HAS_LIBS = True
+except ImportError:
+    pass
+__virtualname__ = "azure"
+log = logging.getLogger(__name__)
+def __virtual__():
+    """
+    Check for Azure configurations.
+    """
+    if get_configured_provider() is False:
+        return False
+    if get_dependencies() is False:
+        return False
+    return __virtualname__
+def _get_active_provider_name():
+    try:
+        return __active_provider_name__.value()
+    except AttributeError:
+        return __active_provider_name__
+def _deprecation_message(function):
+    """
+    Decorator wrapper to warn about msazure deprecation
+    """
+    @wraps(function)
+    def wrapped(*args, **kwargs):
+        salt.utils.versions.warn_until(
+            "Chlorine",
+            "This cloud provider will be removed from Salt in version 3007 due to "
+            "the deprecation of the 'Classic' API for Azure. Please migrate to "
+            "Azure Resource Manager by March 1, 2023 "
+            "(https://docs.microsoft.com/en-us/azure/virtual-machines/classic-vm-deprecation)",
+            category=FutureWarning,
+        )
+        ret = function(*args, **salt.utils.args.clean_kwargs(**kwargs))
+        return ret
+    return wrapped
+def get_configured_provider():
+    """
+    Return the first configured instance.
+    """
+    return config.is_provider_configured(
+        __opts__,
+        _get_active_provider_name() or __virtualname__,
+        ("subscription_id", "certificate_path"),
+    )
+@_deprecation_message
+def get_dependencies():
+    """
+    Warn if dependencies aren't met.
+    """
+    return config.check_driver_dependencies(__virtualname__, {"azure": HAS_LIBS})
+@_deprecation_message
+def get_conn():
+    """
+    Return a conn object for the passed VM data
+    """
+    certificate_path = config.get_cloud_config_value(
+        "certificate_path", get_configured_provider(), __opts__, search_global=False
+    )
+    subscription_id = salt.utils.stringutils.to_str(
+        config.get_cloud_config_value(
+            "subscription_id", get_configured_provider(), __opts__, search_global=False
+        )
+    )
+    management_host = config.get_cloud_config_value(
+        "management_host",
+        get_configured_provider(),
+        __opts__,
+        search_global=False,
+        default="management.core.windows.net",
+    )
+    return azure.servicemanagement.ServiceManagementService(
+        subscription_id, certificate_path, management_host
+    )
+@_deprecation_message
+def script(vm_):
+    """
+    Return the script deployment object
+    """
+    return salt.utils.cloud.os_script(
+        config.get_cloud_config_value("script", vm_, __opts__),
+        vm_,
+        __opts__,
+        salt.utils.cloud.salt_config_to_yaml(
+            salt.utils.cloud.minion_config(__opts__, vm_)
+        ),
+    )
+@_deprecation_message
+def avail_locations(conn=None, call=None):
+    """
+    List available locations for Azure
+    """
+    if call == "action":
+        raise SaltCloudSystemExit(
+            "The avail_locations function must be called with "
+            "-f or --function, or with the --list-locations option"
+        )
+    if not conn:
+        conn = get_conn()
+    ret = {}
+    locations = conn.list_locations()
+    for location in locations:
+        ret[location.name] = {
+            "name": location.name,
+            "display_name": location.display_name,
+            "available_services": location.available_services,
+        }
+    return ret
+@_deprecation_message
+def avail_images(conn=None, call=None):
+    """
+    List available images for Azure
+    """
+    if call == "action":
+        raise SaltCloudSystemExit(
+            "The avail_images function must be called with "
+            "-f or --function, or with the --list-images option"
+        )
+    if not conn:
+        conn = get_conn()
+    ret = {}
+    for item in conn.list_os_images():
+        ret[item.name] = object_to_dict(item)
+    for item in conn.list_vm_images():
+        ret[item.name] = object_to_dict(item)
+    return ret
+@_deprecation_message
+def avail_sizes(call=None):
+    """
+    Return a list of sizes from Azure
+    """
+    if call == "action":
+        raise SaltCloudSystemExit(
+            "The avail_sizes function must be called with "
+            "-f or --function, or with the --list-sizes option"
+        )
+    conn = get_conn()
+    data = conn.list_role_sizes()
+    ret = {}
+    for item in data.role_sizes:
+        ret[item.name] = object_to_dict(item)
+    return ret
+@_deprecation_message
+def list_nodes(conn=None, call=None):
+    """
+    List VMs on this Azure account
+    """
+    if call == "action":
+        raise SaltCloudSystemExit(
+            "The list_nodes function must be called with -f or --function."
+        )
+    ret = {}
+    nodes = list_nodes_full(conn, call)
+    for node in nodes:
+        ret[node] = {"name": node}
+        for prop in ("id", "image", "size", "state", "private_ips", "public_ips"):
+            ret[node][prop] = nodes[node].get(prop)
+    return ret
+@_deprecation_message
+def list_nodes_full(conn=None, call=None):
+    """
+    List VMs on this Azure account, with full information
+    """
+    if call == "action":
+        raise SaltCloudSystemExit(
+            "The list_nodes_full function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    ret = {}
+    services = list_hosted_services(conn=conn, call=call)
+    for service in services:
+        for deployment in services[service]["deployments"]:
+            deploy_dict = services[service]["deployments"][deployment]
+            deploy_dict_no_role_info = copy.deepcopy(deploy_dict)
+            del deploy_dict_no_role_info["role_list"]
+            del deploy_dict_no_role_info["role_instance_list"]
+            roles = deploy_dict["role_list"]
+            for role in roles:
+                role_instances = deploy_dict["role_instance_list"]
+                ret[role] = roles[role]
+                ret[role].update(role_instances[role])
+                ret[role]["id"] = role
+                ret[role]["hosted_service"] = service
+                if role_instances[role]["power_state"] == "Started":
+                    ret[role]["state"] = "running"
+                elif role_instances[role]["power_state"] == "Stopped":
+                    ret[role]["state"] = "stopped"
+                else:
+                    ret[role]["state"] = "pending"
+                ret[role]["private_ips"] = []
+                ret[role]["public_ips"] = []
+                ret[role]["deployment"] = deploy_dict_no_role_info
+                ret[role]["url"] = deploy_dict["url"]
+                ip_address = role_instances[role]["ip_address"]
+                if ip_address:
+                    if salt.utils.cloud.is_public_ip(ip_address):
+                        ret[role]["public_ips"].append(ip_address)
+                    else:
+                        ret[role]["private_ips"].append(ip_address)
+                ret[role]["size"] = role_instances[role]["instance_size"]
+                ret[role]["image"] = roles[role]["role_info"]["os_virtual_hard_disk"][
+                    "source_image_name"
+                ]
+    return ret
+@_deprecation_message
+def list_hosted_services(conn=None, call=None):
+    """
+    List VMs on this Azure account, with full information
+    """
+    if call == "action":
+        raise SaltCloudSystemExit(
+            "The list_hosted_services function must be called with -f or --function"
+        )
+    if not conn:
+        conn = get_conn()
+    ret = {}
+    services = conn.list_hosted_services()
+    for service in services:
+        props = service.hosted_service_properties
+        ret[service.service_name] = {
+            "name": service.service_name,
+            "url": service.url,
+            "affinity_group": props.affinity_group,
+            "date_created": props.date_created,
+            "date_last_modified": props.date_last_modified,
+            "description": props.description,
+            "extended_properties": props.extended_properties,
+            "label": props.label,
+            "location": props.location,
+            "status": props.status,
+            "deployments": {},
+        }
+        deployments = conn.get_hosted_service_properties(
+            service_name=service.service_name, embed_detail=True
+        )
+        for deployment in deployments.deployments:
+            ret[service.service_name]["deployments"][deployment.name] = {
+                "configuration": deployment.configuration,
+                "created_time": deployment.created_time,
+                "deployment_slot": deployment.deployment_slot,
+                "extended_properties": deployment.extended_properties,
+                "input_endpoint_list": deployment.input_endpoint_list,
+                "label": deployment.label,
+                "last_modified_time": deployment.last_modified_time,
+                "locked": deployment.locked,
+                "name": deployment.name,
+                "persistent_vm_downtime_info": deployment.persistent_vm_downtime_info,
+                "private_id": deployment.private_id,
+                "role_instance_list": {},
+                "role_list": {},
+                "rollback_allowed": deployment.rollback_allowed,
+                "sdk_version": deployment.sdk_version,
+                "status": deployment.status,
+                "upgrade_domain_count": deployment.upgrade_domain_count,
+                "upgrade_status": deployment.upgrade_status,
+                "url": deployment.url,
+            }
+            for role_instance in deployment.role_instance_list:
+                ret[service.service_name]["deployments"][deployment.name][
+                    "role_instance_list"
+                ][role_instance.role_name] = {
+                    "fqdn": role_instance.fqdn,
+                    "instance_error_code": role_instance.instance_error_code,
+                    "instance_fault_domain": role_instance.instance_fault_domain,
+                    "instance_name": role_instance.instance_name,
+                    "instance_size": role_instance.instance_size,
+                    "instance_state_details": role_instance.instance_state_details,
+                    "instance_status": role_instance.instance_status,
+                    "instance_upgrade_domain": role_instance.instance_upgrade_domain,
+                    "ip_address": role_instance.ip_address,
+                    "power_state": role_instance.power_state,
+                    "role_name": role_instance.role_name,
+                }
+            for role in deployment.role_list:
+                ret[service.service_name]["deployments"][deployment.name]["role_list"][
+                    role.role_name
+                ] = {
+                    "role_name": role.role_name,
+                    "os_version": role.os_version,
+                }
+                role_info = conn.get_role(
+                    service_name=service.service_name,
+                    deployment_name=deployment.name,
+                    role_name=role.role_name,
+                )
+                ret[service.service_name]["deployments"][deployment.name]["role_list"][
+                    role.role_name
+                ]["role_info"] = {
+                    "availability_set_name": role_info.availability_set_name,
+                    "configuration_sets": role_info.configuration_sets,
+                    "data_virtual_hard_disks": role_info.data_virtual_hard_disks,
+                    "os_version": role_info.os_version,
+                    "role_name": role_info.role_name,
+                    "role_size": role_info.role_size,
+                    "role_type": role_info.role_type,
+                }
+                ret[service.service_name]["deployments"][deployment.name]["role_list"][
+                    role.role_name
+                ]["role_info"]["os_virtual_hard_disk"] = {
+                    "disk_label": role_info.os_virtual_hard_disk.disk_label,
+                    "disk_name": role_info.os_virtual_hard_disk.disk_name,
+                    "host_caching": role_info.os_virtual_hard_disk.host_caching,
+                    "media_link": role_info.os_virtual_hard_disk.media_link,
+                    "os": role_info.os_virtual_hard_disk.os,
+                    "source_image_name": role_info.os_virtual_hard_disk.source_image_name,
+                }
+    return ret
+@_deprecation_message
+def list_nodes_select(conn=None, call=None):
+    """
+    Return a list of the VMs that are on the provider, with select fields
+    """
+    if not conn:
+        conn = get_conn()
+    return salt.utils.cloud.list_nodes_select(
+        list_nodes_full(conn, "function"),
+        __opts__["query.selection"],
+        call,
+    )
+@_deprecation_message
+def show_instance(name, call=None):
+    """
+    Show the details from the provider concerning an instance
+    """
+    if call != "action":
+        raise SaltCloudSystemExit(
+            "The show_instance action must be called with -a or --action."
+        )
+    nodes = list_nodes_full()
+    if name not in nodes:
+        return {}
+    if "name" not in nodes[name]:
+        nodes[name]["name"] = nodes[name]["id"]
+    try:
+        __utils__["cloud.cache_node"](
+            nodes[name], _get_active_provider_name(), __opts__
+        )
+    except TypeError:
+        log.warning(
+            "Unable to show cache node data; this may be because the node has been"
+            " deleted"
+        )
+    return nodes[name]
+@_deprecation_message
+def create(vm_):
+    """
+    Create a single VM from a data dict
+    """
+    try:
+        if (
+            vm_["profile"]
+            and config.is_profile_configured(
+                __opts__,
+                _get_active_provider_name() or "azure",
+                vm_["profile"],
+                vm_=vm_,
+            )
+            is False
+        ):
+            return False
+    except AttributeError:
+        pass
+    __utils__["cloud.fire_event"](
+        "event",
+        "starting create",
+        "salt/cloud/{}/creating".format(vm_["name"]),
+        args=__utils__["cloud.filter_event"](
+            "creating", vm_, ["name", "profile", "provider", "driver"]
+        ),
+        sock_dir=__opts__["sock_dir"],
+        transport=__opts__["transport"],
+    )
+    log.info("Creating Cloud VM %s", vm_["name"])
+    conn = get_conn()
+    label = vm_.get("label", vm_["name"])
+    service_name = vm_.get("service_name", vm_["name"])
+    service_kwargs = {
+        "service_name": service_name,
+        "label": label,
+        "description": vm_.get("desc", vm_["name"]),
+    }
+    loc_error = False
+    if "location" in vm_:
+        if "affinity_group" in vm_:
+            loc_error = True
+        else:
+            service_kwargs["location"] = vm_["location"]
+    elif "affinity_group" in vm_:
+        service_kwargs["affinity_group"] = vm_["affinity_group"]
+    else:
+        loc_error = True
+    if loc_error:
+        raise SaltCloudSystemExit(
+            "Either a location or affinity group must be specified, but not both"
+        )
+    ssh_port = config.get_cloud_config_value(
+        "port", vm_, __opts__, default=22, search_global=True
+    )
+    ssh_endpoint = azure.servicemanagement.ConfigurationSetInputEndpoint(
+        name="SSH",
+        protocol="TCP",
+        port=ssh_port,
+        local_port=22,
+    )
+    network_config = azure.servicemanagement.ConfigurationSet()
+    network_config.input_endpoints.input_endpoints.append(ssh_endpoint)
+    network_config.configuration_set_type = "NetworkConfiguration"
+    if "win_username" in vm_:
+        system_config = azure.servicemanagement.WindowsConfigurationSet(
+            computer_name=vm_["name"],
+            admin_username=vm_["win_username"],
+            admin_password=vm_["win_password"],
+        )
+        smb_port = "445"
+        if "smb_port" in vm_:
+            smb_port = vm_["smb_port"]
+        smb_endpoint = azure.servicemanagement.ConfigurationSetInputEndpoint(
+            name="SMB",
+            protocol="TCP",
+            port=smb_port,
+            local_port=smb_port,
+        )
+        network_config.input_endpoints.input_endpoints.append(smb_endpoint)
+        system_config.domain_join = None
+        system_config.win_rm = None
+    else:
+        system_config = azure.servicemanagement.LinuxConfigurationSet(
+            host_name=vm_["name"],
+            user_name=vm_["ssh_username"],
+            user_password=vm_["ssh_password"],
+            disable_ssh_password_authentication=False,
+        )
+    media_link = vm_["media_link"]
+    media_link += "/{}.vhd".format(vm_["name"])
+    os_hd = azure.servicemanagement.OSVirtualHardDisk(vm_["image"], media_link)
+    vm_kwargs = {
+        "service_name": service_name,
+        "deployment_name": service_name,
+        "deployment_slot": vm_["slot"],
+        "label": label,
+        "role_name": vm_["name"],
+        "system_config": system_config,
+        "os_virtual_hard_disk": os_hd,
+        "role_size": vm_["size"],
+        "network_config": network_config,
+    }
+    if "virtual_network_name" in vm_:
+        vm_kwargs["virtual_network_name"] = vm_["virtual_network_name"]
+        if "subnet_name" in vm_:
+            network_config.subnet_names.append(vm_["subnet_name"])
+    log.debug("vm_kwargs: %s", vm_kwargs)
+    event_kwargs = {
+        "service_kwargs": service_kwargs.copy(),
+        "vm_kwargs": vm_kwargs.copy(),
+    }
+    del event_kwargs["vm_kwargs"]["system_config"]
+    del event_kwargs["vm_kwargs"]["os_virtual_hard_disk"]
+    del event_kwargs["vm_kwargs"]["network_config"]
+    __utils__["cloud.fire_event"](
+        "event",
+        "requesting instance",
+        "salt/cloud/{}/requesting".format(vm_["name"]),
+        args=__utils__["cloud.filter_event"](
+            "requesting", event_kwargs, list(event_kwargs)
+        ),
+        sock_dir=__opts__["sock_dir"],
+        transport=__opts__["transport"],
+    )
+    log.debug("vm_kwargs: %s", vm_kwargs)
+    try:
+        conn.create_hosted_service(**service_kwargs)
+    except AzureConflictHttpError:
+        log.debug("Cloud service already exists")
+    except Exception as exc:  # pylint: disable=broad-except
+        error = "The hosted service name is invalid."
+        if error in str(exc):
+            log.error(
+                "Error creating %s on Azure.\n\n"
+                "The hosted service name is invalid. The name can contain "
+                "only letters, numbers, and hyphens. The name must start with "
+                "a letter and must end with a letter or a number.",
+                vm_["name"],
+                exc_info_on_loglevel=logging.DEBUG,
+            )
+        else:
+            log.error(
+                "Error creating %s on Azure\n\n"
+                "The following exception was thrown when trying to "
+                "run the initial deployment: \n%s",
+                vm_["name"],
+                exc,
+                exc_info_on_loglevel=logging.DEBUG,
+            )
+        return False
+    try:
+        result = conn.create_virtual_machine_deployment(**vm_kwargs)
+        log.debug("Request ID for machine: %s", result.request_id)
+        _wait_for_async(conn, result.request_id)
+    except AzureConflictHttpError:
+        log.debug("Conflict error. The deployment may already exist, trying add_role")
+        del vm_kwargs["deployment_slot"]
+        del vm_kwargs["label"]
+        del vm_kwargs["virtual_network_name"]
+        result = conn.add_role(**vm_kwargs)  # pylint: disable=unexpected-keyword-arg
+        _wait_for_async(conn, result.request_id)
+    except Exception as exc:  # pylint: disable=broad-except
+        error = "The hosted service name is invalid."
+        if error in str(exc):
+            log.error(
+                "Error creating %s on Azure.\n\n"
+                "The VM name is invalid. The name can contain "
+                "only letters, numbers, and hyphens. The name must start with "
+                "a letter and must end with a letter or a number.",
+                vm_["name"],
+                exc_info_on_loglevel=logging.DEBUG,
+            )
+        else:
+            log.error(
+                "Error creating %s on Azure.\n\n"
+                "The Virtual Machine could not be created. If you "
+                "are using an already existing Cloud Service, "
+                "make sure you set up the `port` variable corresponding "
+                "to the SSH port exists and that the port number is not "
+                "already in use.\nThe following exception was thrown when trying to "
+                "run the initial deployment: \n%s",
+                vm_["name"],
+                exc,
+                exc_info_on_loglevel=logging.DEBUG,
+            )
+        return False
+    def wait_for_hostname():
+        """
+        Wait for the IP address to become available
+        """
+        try:
+            conn.get_role(service_name, service_name, vm_["name"])
+            data = show_instance(vm_["name"], call="action")
+            if "url" in data and data["url"] != "":
+                return data["url"]
+        except AzureMissingResourceHttpError:
+            pass
+        time.sleep(1)
+        return False
+    hostname = salt.utils.cloud.wait_for_fun(
+        wait_for_hostname,
+        timeout=config.get_cloud_config_value(
+            "wait_for_fun_timeout", vm_, __opts__, default=15 * 60
+        ),
+    )
+    if not hostname:
+        log.error("Failed to get a value for the hostname.")
+        return False
+    vm_["ssh_host"] = hostname.replace("http://", "").replace("/", "")
+    vm_["password"] = config.get_cloud_config_value("ssh_password", vm_, __opts__)
+    ret = __utils__["cloud.bootstrap"](vm_, __opts__)
+    volumes = config.get_cloud_config_value(
+        "volumes", vm_, __opts__, search_global=True
+    )
+    if volumes:
+        __utils__["cloud.fire_event"](
+            "event",
+            "attaching volumes",
+            "salt/cloud/{}/attaching_volumes".format(vm_["name"]),
+            args=__utils__["cloud.filter_event"]("attaching_volumes", vm_, ["volumes"]),
+            sock_dir=__opts__["sock_dir"],
+            transport=__opts__["transport"],
+        )
+        log.info("Create and attach volumes to node %s", vm_["name"])
+        created = create_attach_volumes(
+            vm_["name"],
+            {
+                "volumes": volumes,
+                "service_name": service_name,
+                "deployment_name": vm_["name"],
+                "media_link": media_link,
+                "role_name": vm_["name"],
+                "del_all_vols_on_destroy": vm_.get(
+                    "set_del_all_vols_on_destroy", False
+                ),
+            },
+            call="action",
+        )
+        ret["Attached Volumes"] = created
+    data = show_instance(vm_["name"], call="action")
+    log.info("Created Cloud VM '%s'", vm_)
+    log.debug("'%s' VM creation details:\n%s", vm_["name"], pprint.pformat(data))
+    ret.update(data)
+    __utils__["cloud.fire_event"](
+        "event",
+        "created instance",
+        "salt/cloud/{}/created".format(vm_["name"]),
+        args=__utils__["cloud.filter_event"](
+            "created", vm_, ["name", "profile", "provider", "driver"]
+        ),
+        sock_dir=__opts__["sock_dir"],
+        transport=__opts__["transport"],
+    )
+    return ret
+@_deprecation_message
+def create_attach_volumes(name, kwargs, call=None, wait_to_finish=True):
+    """
+    Create and attach volumes to created node
+    """
+    if call != "action":
+        raise SaltCloudSystemExit(
+            "The create_attach_volumes action must be called with -a or --action."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if isinstance(kwargs["volumes"], str):
+        volumes = salt.utils.yaml.safe_load(kwargs["volumes"])
+    else:
+        volumes = kwargs["volumes"]
+    conn = get_conn()
+    ret = []
+    for volume in volumes:
+        if "disk_name" in volume:
+            log.error("You cannot specify a disk_name. Only new volumes are allowed")
+            return False
+        volume.setdefault("logical_disk_size_in_gb", volume.get("size", 100))
+        volume.setdefault("host_caching", "ReadOnly")
+        volume.setdefault("lun", 0)
+        volume.setdefault(
+            "media_link",
+            kwargs["media_link"][:-4] + "-disk-{}.vhd".format(volume["lun"]),
+        )
+        volume.setdefault(
+            "disk_label", kwargs["role_name"] + "-disk-{}".format(volume["lun"])
+        )
+        volume_dict = {"volume_name": volume["lun"], "disk_label": volume["disk_label"]}
+        kwargs_add_data_disk = [
+            "lun",
+            "host_caching",
+            "media_link",
+            "disk_label",
+            "disk_name",
+            "logical_disk_size_in_gb",
+            "source_media_link",
+        ]
+        for key in set(volume.keys()) - set(kwargs_add_data_disk):
+            del volume[key]
+        attach = conn.add_data_disk(
+            kwargs["service_name"],
+            kwargs["deployment_name"],
+            kwargs["role_name"],
+            **volume,
+        )
+        log.debug(attach)
+        if attach:
+            msg = "{} attached to {} (aka {})".format(
+                volume_dict["volume_name"],
+                kwargs["role_name"],
+                name,
+            )
+            log.info(msg)
+            ret.append(msg)
+        else:
+            log.error("Error attaching %s on Azure", volume_dict)
+    return ret
+@_deprecation_message
+def create_attach_volumes(name, kwargs, call=None, wait_to_finish=True):
+    """
+    Create and attach volumes to created node
+    """
+    if call != "action":
+        raise SaltCloudSystemExit(
+            "The create_attach_volumes action must be called with -a or --action."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if isinstance(kwargs["volumes"], str):
+        volumes = salt.utils.yaml.safe_load(kwargs["volumes"])
+    else:
+        volumes = kwargs["volumes"]
+    conn = get_conn()
+    ret = []
+    for volume in volumes:
+        if "disk_name" in volume:
+            log.error("You cannot specify a disk_name. Only new volumes are allowed")
+            return False
+        volume.setdefault("logical_disk_size_in_gb", volume.get("size", 100))
+        volume.setdefault("host_caching", "ReadOnly")
+        volume.setdefault("lun", 0)
+        volume.setdefault(
+            "media_link",
+            kwargs["media_link"][:-4] + "-disk-{}.vhd".format(volume["lun"]),
+        )
+        volume.setdefault(
+            "disk_label", kwargs["role_name"] + "-disk-{}".format(volume["lun"])
+        )
+        volume_dict = {"volume_name": volume["lun"], "disk_label": volume["disk_label"]}
+        kwargs_add_data_disk = [
+            "lun",
+            "host_caching",
+            "media_link",
+            "disk_label",
+            "disk_name",
+            "logical_disk_size_in_gb",
+            "source_media_link",
+        ]
+        for key in set(volume.keys()) - set(kwargs_add_data_disk):
+            del volume[key]
+        result = conn.add_data_disk(
+            kwargs["service_name"],
+            kwargs["deployment_name"],
+            kwargs["role_name"],
+            **volume,
+        )
+        _wait_for_async(conn, result.request_id)
+        msg = "{} attached to {} (aka {})".format(
+            volume_dict["volume_name"], kwargs["role_name"], name
+        )
+        log.info(msg)
+        ret.append(msg)
+    return ret
+def _wait_for_async(conn, request_id):
+    """
+    Helper function for azure tests
+    """
+    count = 0
+    log.debug("Waiting for asynchronous operation to complete")
+    result = conn.get_operation_status(request_id)
+    while result.status == "InProgress":
+        count = count + 1
+        if count > 120:
+            raise ValueError(
+                "Timed out waiting for asynchronous operation to complete."
+            )
+        time.sleep(5)
+        result = conn.get_operation_status(request_id)
+    if result.status != "Succeeded":
+        raise AzureException(
+            "Operation failed. {message} ({code})".format(
+                message=result.error.message, code=result.error.code
+            )
+        )
+@_deprecation_message
+def destroy(name, conn=None, call=None, kwargs=None):
+    """
+    Destroy a VM
+    CLI Examples:
+    .. code-block:: bash
+        salt-cloud -d myminion
+        salt-cloud -a destroy myminion service_name=myservice
+    """
+    if call == "function":
+        raise SaltCloudSystemExit(
+            "The destroy action must be called with -d, --destroy, -a or --action."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    instance_data = show_instance(name, call="action")
+    service_name = instance_data["deployment"]["name"]
+    disk_name = instance_data["role_info"]["os_virtual_hard_disk"]["disk_name"]
+    ret = {}
+    try:
+        log.debug("Deleting role")
+        result = conn.delete_role(service_name, service_name, name)
+        delete_type = "delete_role"
+    except AzureException:
+        log.debug("Failed to delete role, deleting deployment")
+        try:
+            result = conn.delete_deployment(service_name, service_name)
+        except AzureConflictHttpError as exc:
+            log.error(exc.message)
+            raise SaltCloudSystemExit(f"{name}: {exc.message}")
+        delete_type = "delete_deployment"
+    _wait_for_async(conn, result.request_id)
+    ret[name] = {
+        delete_type: {"request_id": result.request_id},
+    }
+    if __opts__.get("update_cachedir", False) is True:
+        __utils__["cloud.delete_minion_cachedir"](
+            name, _get_active_provider_name().split(":")[0], __opts__
+        )
+    cleanup_disks = config.get_cloud_config_value(
+        "cleanup_disks",
+        get_configured_provider(),
+        __opts__,
+        search_global=False,
+        default=False,
+    )
+    if cleanup_disks:
+        cleanup_vhds = kwargs.get(
+            "delete_vhd",
+            config.get_cloud_config_value(
+                "cleanup_vhds",
+                get_configured_provider(),
+                __opts__,
+                search_global=False,
+                default=False,
+            ),
+        )
+        log.debug("Deleting disk %s", disk_name)
+        if cleanup_vhds:
+            log.debug("Deleting vhd")
+        def wait_for_destroy():
+            """
+            Wait for the VM to be deleted
+            """
+            try:
+                data = delete_disk(
+                    kwargs={"name": disk_name, "delete_vhd": cleanup_vhds},
+                    call="function",
+                )
+                return data
+            except AzureConflictHttpError:
+                log.debug("Waiting for VM to be destroyed...")
+            time.sleep(5)
+            return False
+        data = salt.utils.cloud.wait_for_fun(
+            wait_for_destroy,
+            timeout=config.get_cloud_config_value(
+                "wait_for_fun_timeout", {}, __opts__, default=15 * 60
+            ),
+        )
+        ret[name]["delete_disk"] = {
+            "name": disk_name,
+            "delete_vhd": cleanup_vhds,
+            "data": data,
+        }
+        cleanup_services = config.get_cloud_config_value(
+            "cleanup_services",
+            get_configured_provider(),
+            __opts__,
+            search_global=False,
+            default=False,
+        )
+        if cleanup_services:
+            log.debug("Deleting service %s", service_name)
+            def wait_for_disk_delete():
+                """
+                Wait for the disk to be deleted
+                """
+                try:
+                    data = delete_service(
+                        kwargs={"name": service_name}, call="function"
+                    )
+                    return data
+                except AzureConflictHttpError:
+                    log.debug("Waiting for disk to be deleted...")
+                time.sleep(5)
+                return False
+            data = salt.utils.cloud.wait_for_fun(
+                wait_for_disk_delete,
+                timeout=config.get_cloud_config_value(
+                    "wait_for_fun_timeout", {}, __opts__, default=15 * 60
+                ),
+            )
+            ret[name]["delete_services"] = {"name": service_name, "data": data}
+    return ret
+@_deprecation_message
+def list_storage_services(conn=None, call=None):
+    """
+    List VMs on this Azure account, with full information
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The list_storage_services function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    ret = {}
+    accounts = conn.list_storage_accounts()
+    for service in accounts.storage_services:
+        ret[service.service_name] = {
+            "capabilities": service.capabilities,
+            "service_name": service.service_name,
+            "storage_service_properties": service.storage_service_properties,
+            "extended_properties": service.extended_properties,
+            "storage_service_keys": service.storage_service_keys,
+            "url": service.url,
+        }
+    return ret
+@_deprecation_message
+def get_operation_status(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Get Operation Status, based on a request ID
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f get_operation_status my-azure id=0123456789abcdef0123456789abcdef
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The show_instance function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "id" not in kwargs:
+        raise SaltCloudSystemExit('A request ID must be specified as "id"')
+    if not conn:
+        conn = get_conn()
+    data = conn.get_operation_status(kwargs["id"])
+    ret = {
+        "http_status_code": data.http_status_code,
+        "id": kwargs["id"],
+        "status": data.status,
+    }
+    if hasattr(data.error, "code"):
+        ret["error"] = {
+            "code": data.error.code,
+            "message": data.error.message,
+        }
+    return ret
+@_deprecation_message
+def list_storage(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    List storage accounts associated with the account
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f list_storage my-azure
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The list_storage function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    data = conn.list_storage_accounts()
+    pprint.pprint(dir(data))
+    ret = {}
+    for item in data.storage_services:
+        ret[item.service_name] = object_to_dict(item)
+    return ret
+@_deprecation_message
+def show_storage(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    List storage service properties
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f show_storage my-azure name=my_storage
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The show_storage function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    data = conn.get_storage_account_properties(
+        kwargs["name"],
+    )
+    return object_to_dict(data)
+get_storage = show_storage
+@_deprecation_message
+def show_storage_keys(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Show storage account keys
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f show_storage_keys my-azure name=my_storage
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The show_storage_keys function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    try:
+        data = conn.get_storage_account_keys(
+            kwargs["name"],
+        )
+    except AzureMissingResourceHttpError as exc:
+        storage_data = show_storage(kwargs={"name": kwargs["name"]}, call="function")
+        if storage_data["storage_service_properties"]["status"] == "Creating":
+            raise SaltCloudSystemExit(
+                "The storage account keys have not yet been created."
+            )
+        else:
+            raise SaltCloudSystemExit("{}: {}".format(kwargs["name"], exc.message))
+    return object_to_dict(data)
+get_storage_keys = show_storage_keys
+@_deprecation_message
+def create_storage(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Create a new storage account
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f create_storage my-azure name=my_storage label=my_storage location='West US'
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The show_storage function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if not conn:
+        conn = get_conn()
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    if "description" not in kwargs:
+        raise SaltCloudSystemExit('A description must be specified as "description"')
+    if "label" not in kwargs:
+        raise SaltCloudSystemExit('A label must be specified as "label"')
+    if "location" not in kwargs and "affinity_group" not in kwargs:
+        raise SaltCloudSystemExit(
+            "Either a location or an affinity_group must be specified (but not both)"
+        )
+    try:
+        data = conn.create_storage_account(
+            service_name=kwargs["name"],
+            label=kwargs["label"],
+            description=kwargs.get("description", None),
+            location=kwargs.get("location", None),
+            affinity_group=kwargs.get("affinity_group", None),
+            extended_properties=kwargs.get("extended_properties", None),
+            geo_replication_enabled=kwargs.get("geo_replication_enabled", None),
+            account_type=kwargs.get("account_type", "Standard_GRS"),
+        )
+        return {"Success": "The storage account was successfully created"}
+    except AzureConflictHttpError:
+        raise SaltCloudSystemExit(
+            "There was a conflict. This usually means that the storage account already"
+            " exists."
+        )
+@_deprecation_message
+def update_storage(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Update a storage account's properties
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f update_storage my-azure name=my_storage label=my_storage
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The show_storage function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    data = conn.update_storage_account(
+        service_name=kwargs["name"],
+        label=kwargs.get("label", None),
+        description=kwargs.get("description", None),
+        extended_properties=kwargs.get("extended_properties", None),
+        geo_replication_enabled=kwargs.get("geo_replication_enabled", None),
+        account_type=kwargs.get("account_type", "Standard_GRS"),
+    )
+    return show_storage(kwargs={"name": kwargs["name"]}, call="function")
+@_deprecation_message
+def regenerate_storage_keys(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Regenerate storage account keys. Requires a key_type ("primary" or
+    "secondary") to be specified.
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f regenerate_storage_keys my-azure name=my_storage key_type=primary
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The show_storage function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    if "key_type" not in kwargs or kwargs["key_type"] not in ("primary", "secondary"):
+        raise SaltCloudSystemExit(
+            'A key_type must be specified ("primary" or "secondary")'
+        )
+    try:
+        data = conn.regenerate_storage_account_keys(
+            service_name=kwargs["name"],
+            key_type=kwargs["key_type"],
+        )
+        return show_storage_keys(kwargs={"name": kwargs["name"]}, call="function")
+    except AzureConflictHttpError:
+        raise SaltCloudSystemExit(
+            "There was a conflict. This usually means that the storage account already"
+            " exists."
+        )
+@_deprecation_message
+def delete_storage(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Delete a specific storage account
+    CLI Examples:
+    .. code-block:: bash
+        salt-cloud -f delete_storage my-azure name=my_storage
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The delete_storage function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    if not conn:
+        conn = get_conn()
+    try:
+        data = conn.delete_storage_account(kwargs["name"])
+        return {"Success": "The storage account was successfully deleted"}
+    except AzureMissingResourceHttpError as exc:
+        raise SaltCloudSystemExit("{}: {}".format(kwargs["name"], exc.message))
+@_deprecation_message
+def list_services(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    List hosted services associated with the account
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f list_services my-azure
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The list_services function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    data = conn.list_hosted_services()
+    ret = {}
+    for item in data.hosted_services:
+        ret[item.service_name] = object_to_dict(item)
+        ret[item.service_name]["name"] = item.service_name
+    return ret
+@_deprecation_message
+def show_service(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    List hosted service properties
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f show_service my-azure name=my_service
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The show_service function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    data = conn.get_hosted_service_properties(
+        kwargs["name"], kwargs.get("details", False)
+    )
+    ret = object_to_dict(data)
+    return ret
+@_deprecation_message
+def create_service(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Create a new hosted service
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f create_service my-azure name=my_service label=my_service location='West US'
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The create_service function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    if "label" not in kwargs:
+        raise SaltCloudSystemExit('A label must be specified as "label"')
+    if "location" not in kwargs and "affinity_group" not in kwargs:
+        raise SaltCloudSystemExit(
+            "Either a location or an affinity_group must be specified (but not both)"
+        )
+    try:
+        data = conn.create_hosted_service(
+            kwargs["name"],
+            kwargs["label"],
+            kwargs.get("description", None),
+            kwargs.get("location", None),
+            kwargs.get("affinity_group", None),
+            kwargs.get("extended_properties", None),
+        )
+        return {"Success": "The service was successfully created"}
+    except AzureConflictHttpError:
+        raise SaltCloudSystemExit(
+            "There was a conflict. This usually means that the service already exists."
+        )
+@_deprecation_message
+def delete_service(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Delete a specific service associated with the account
+    CLI Examples:
+    .. code-block:: bash
+        salt-cloud -f delete_service my-azure name=my_service
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The delete_service function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    if not conn:
+        conn = get_conn()
+    try:
+        conn.delete_hosted_service(kwargs["name"])
+        return {"Success": "The service was successfully deleted"}
+    except AzureMissingResourceHttpError as exc:
+        raise SaltCloudSystemExit("{}: {}".format(kwargs["name"], exc.message))
+@_deprecation_message
+def list_disks(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    List disks associated with the account
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f list_disks my-azure
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The list_disks function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    data = conn.list_disks()
+    ret = {}
+    for item in data.disks:
+        ret[item.name] = object_to_dict(item)
+    return ret
+@_deprecation_message
+def show_disk(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Return information about a disk
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f show_disk my-azure name=my_disk
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The get_disk function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    data = conn.get_disk(kwargs["name"])
+    return object_to_dict(data)
+get_disk = show_disk
+@_deprecation_message
+def cleanup_unattached_disks(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Cleans up all disks associated with the account, which are not attached.
+    *** CAUTION *** This is a destructive function with no undo button, and no
+    "Are you sure?" confirmation!
+    CLI Examples:
+    .. code-block:: bash
+        salt-cloud -f cleanup_unattached_disks my-azure name=my_disk
+        salt-cloud -f cleanup_unattached_disks my-azure name=my_disk delete_vhd=True
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The delete_disk function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    disks = list_disks(kwargs=kwargs, conn=conn, call="function")
+    for disk in disks:
+        if disks[disk]["attached_to"] is None:
+            del_kwargs = {
+                "name": disks[disk]["name"],
+                "delete_vhd": kwargs.get("delete_vhd", False),
+            }
+            log.info(
+                "Deleting disk %s, deleting VHD: %s",
+                del_kwargs["name"],
+                del_kwargs["delete_vhd"],
+            )
+            data = delete_disk(kwargs=del_kwargs, call="function")
+    return True
+@_deprecation_message
+def delete_disk(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Delete a specific disk associated with the account
+    CLI Examples:
+    .. code-block:: bash
+        salt-cloud -f delete_disk my-azure name=my_disk
+        salt-cloud -f delete_disk my-azure name=my_disk delete_vhd=True
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The delete_disk function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    if not conn:
+        conn = get_conn()
+    try:
+        data = conn.delete_disk(kwargs["name"], kwargs.get("delete_vhd", False))
+        return {"Success": "The disk was successfully deleted"}
+    except AzureMissingResourceHttpError as exc:
+        raise SaltCloudSystemExit("{}: {}".format(kwargs["name"], exc.message))
+@_deprecation_message
+def update_disk(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Update a disk's properties
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f update_disk my-azure name=my_disk label=my_disk
+        salt-cloud -f update_disk my-azure name=my_disk new_name=another_disk
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The show_disk function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    old_data = show_disk(kwargs={"name": kwargs["name"]}, call="function")
+    data = conn.update_disk(
+        disk_name=kwargs["name"],
+        has_operating_system=kwargs.get(
+            "has_operating_system", old_data["has_operating_system"]
+        ),
+        label=kwargs.get("label", old_data["label"]),
+        media_link=kwargs.get("media_link", old_data["media_link"]),
+        name=kwargs.get("new_name", old_data["name"]),
+        os=kwargs.get("os", old_data["os"]),
+    )
+    return show_disk(kwargs={"name": kwargs["name"]}, call="function")
+@_deprecation_message
+def list_service_certificates(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    List certificates associated with the service
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f list_service_certificates my-azure name=my_service
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The list_service_certificates function must be called with -f or"
+            " --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A service name must be specified as "name"')
+    if not conn:
+        conn = get_conn()
+    data = conn.list_service_certificates(service_name=kwargs["name"])
+    ret = {}
+    for item in data.certificates:
+        ret[item.thumbprint] = object_to_dict(item)
+    return ret
+@_deprecation_message
+def show_service_certificate(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Return information about a service certificate
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f show_service_certificate my-azure name=my_service_certificate \\
+            thumbalgorithm=sha1 thumbprint=0123456789ABCDEF
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The get_service_certificate function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A service name must be specified as "name"')
+    if "thumbalgorithm" not in kwargs:
+        raise SaltCloudSystemExit(
+            'A thumbalgorithm must be specified as "thumbalgorithm"'
+        )
+    if "thumbprint" not in kwargs:
+        raise SaltCloudSystemExit('A thumbprint must be specified as "thumbprint"')
+    data = conn.get_service_certificate(
+        kwargs["name"],
+        kwargs["thumbalgorithm"],
+        kwargs["thumbprint"],
+    )
+    return object_to_dict(data)
+get_service_certificate = show_service_certificate
+@_deprecation_message
+def add_service_certificate(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Add a new service certificate
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f add_service_certificate my-azure name=my_service_certificate \\
+            data='...CERT_DATA...' certificate_format=sha1 password=verybadpass
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The add_service_certificate function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    if "data" not in kwargs:
+        raise SaltCloudSystemExit('Certificate data must be specified as "data"')
+    if "certificate_format" not in kwargs:
+        raise SaltCloudSystemExit(
+            'A certificate_format must be specified as "certificate_format"'
+        )
+    if "password" not in kwargs:
+        raise SaltCloudSystemExit('A password must be specified as "password"')
+    try:
+        data = conn.add_service_certificate(
+            kwargs["name"],
+            kwargs["data"],
+            kwargs["certificate_format"],
+            kwargs["password"],
+        )
+        return {"Success": "The service certificate was successfully added"}
+    except AzureConflictHttpError:
+        raise SaltCloudSystemExit(
+            "There was a conflict. This usually means that the "
+            "service certificate already exists."
+        )
+@_deprecation_message
+def delete_service_certificate(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Delete a specific certificate associated with the service
+    CLI Examples:
+    .. code-block:: bash
+        salt-cloud -f delete_service_certificate my-azure name=my_service_certificate \\
+            thumbalgorithm=sha1 thumbprint=0123456789ABCDEF
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The delete_service_certificate function must be called with -f or"
+            " --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    if "thumbalgorithm" not in kwargs:
+        raise SaltCloudSystemExit(
+            'A thumbalgorithm must be specified as "thumbalgorithm"'
+        )
+    if "thumbprint" not in kwargs:
+        raise SaltCloudSystemExit('A thumbprint must be specified as "thumbprint"')
+    if not conn:
+        conn = get_conn()
+    try:
+        data = conn.delete_service_certificate(
+            kwargs["name"],
+            kwargs["thumbalgorithm"],
+            kwargs["thumbprint"],
+        )
+        return {"Success": "The service certificate was successfully deleted"}
+    except AzureMissingResourceHttpError as exc:
+        raise SaltCloudSystemExit("{}: {}".format(kwargs["name"], exc.message))
+@_deprecation_message
+def list_management_certificates(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    List management certificates associated with the subscription
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f list_management_certificates my-azure name=my_management
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The list_management_certificates function must be called with -f or"
+            " --function."
+        )
+    if not conn:
+        conn = get_conn()
+    data = conn.list_management_certificates()
+    ret = {}
+    for item in data.subscription_certificates:
+        ret[item.subscription_certificate_thumbprint] = object_to_dict(item)
+    return ret
+@_deprecation_message
+def show_management_certificate(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Return information about a management_certificate
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f get_management_certificate my-azure name=my_management_certificate \\
+            thumbalgorithm=sha1 thumbprint=0123456789ABCDEF
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The get_management_certificate function must be called with -f or"
+            " --function."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    if "thumbprint" not in kwargs:
+        raise SaltCloudSystemExit('A thumbprint must be specified as "thumbprint"')
+    data = conn.get_management_certificate(kwargs["thumbprint"])
+    return object_to_dict(data)
+get_management_certificate = show_management_certificate
+@_deprecation_message
+def add_management_certificate(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Add a new management certificate
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f add_management_certificate my-azure public_key='...PUBKEY...' \\
+            thumbprint=0123456789ABCDEF data='...CERT_DATA...'
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The add_management_certificate function must be called with -f or"
+            " --function."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    if "public_key" not in kwargs:
+        raise SaltCloudSystemExit('A public_key must be specified as "public_key"')
+    if "thumbprint" not in kwargs:
+        raise SaltCloudSystemExit('A thumbprint must be specified as "thumbprint"')
+    if "data" not in kwargs:
+        raise SaltCloudSystemExit('Certificate data must be specified as "data"')
+    try:
+        conn.add_management_certificate(
+            kwargs["name"],
+            kwargs["thumbprint"],
+            kwargs["data"],
+        )
+        return {"Success": "The management certificate was successfully added"}
+    except AzureConflictHttpError:
+        raise SaltCloudSystemExit(
+            "There was a conflict. "
+            "This usually means that the management certificate already exists."
+        )
+@_deprecation_message
+def delete_management_certificate(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Delete a specific certificate associated with the management
+    CLI Examples:
+    .. code-block:: bash
+        salt-cloud -f delete_management_certificate my-azure name=my_management_certificate \\
+            thumbalgorithm=sha1 thumbprint=0123456789ABCDEF
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The delete_management_certificate function must be called with -f or"
+            " --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "thumbprint" not in kwargs:
+        raise SaltCloudSystemExit('A thumbprint must be specified as "thumbprint"')
+    if not conn:
+        conn = get_conn()
+    try:
+        conn.delete_management_certificate(kwargs["thumbprint"])
+        return {"Success": "The management certificate was successfully deleted"}
+    except AzureMissingResourceHttpError as exc:
+        raise SaltCloudSystemExit("{}: {}".format(kwargs["thumbprint"], exc.message))
+@_deprecation_message
+def list_virtual_networks(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    List input endpoints associated with the deployment
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f list_virtual_networks my-azure service=myservice deployment=mydeployment
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The list_virtual_networks function must be called with -f or --function."
+        )
+    path = "services/networking/virtualnetwork"
+    data = query(path)
+    return data
+@_deprecation_message
+def list_input_endpoints(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    List input endpoints associated with the deployment
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f list_input_endpoints my-azure service=myservice deployment=mydeployment
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The list_input_endpoints function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "service" not in kwargs:
+        raise SaltCloudSystemExit('A service name must be specified as "service"')
+    if "deployment" not in kwargs:
+        raise SaltCloudSystemExit('A deployment name must be specified as "deployment"')
+    path = "services/hostedservices/{}/deployments/{}".format(
+        kwargs["service"],
+        kwargs["deployment"],
+    )
+    data = query(path)
+    if data is None:
+        raise SaltCloudSystemExit(
+            "There was an error listing endpoints with the {} service on the {}"
+            " deployment.".format(kwargs["service"], kwargs["deployment"])
+        )
+    ret = {}
+    for item in data:
+        if "Role" in item:
+            role = item["Role"]
+            if not isinstance(role, dict):
+                return ret
+            input_endpoint = (
+                role["ConfigurationSets"]["ConfigurationSet"]
+                .get("InputEndpoints", {})
+                .get("InputEndpoint")
+            )
+            if not input_endpoint:
+                continue
+            if not isinstance(input_endpoint, list):
+                input_endpoint = [input_endpoint]
+            for endpoint in input_endpoint:
+                ret[endpoint["Name"]] = endpoint
+            return ret
+    return ret
+@_deprecation_message
+def show_input_endpoint(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Show an input endpoint associated with the deployment
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f show_input_endpoint my-azure service=myservice \\
+            deployment=mydeployment name=SSH
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The show_input_endpoint function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('An endpoint name must be specified as "name"')
+    data = list_input_endpoints(kwargs=kwargs, call="function")
+    return data.get(kwargs["name"], None)
+get_input_endpoint = show_input_endpoint
+@_deprecation_message
+def update_input_endpoint(kwargs=None, conn=None, call=None, activity="update"):
+    """
+    .. versionadded:: 2015.8.0
+    Update an input endpoint associated with the deployment. Please note that
+    there may be a delay before the changes show up.
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f update_input_endpoint my-azure service=myservice \\
+            deployment=mydeployment role=myrole name=HTTP local_port=80 \\
+            port=80 protocol=tcp enable_direct_server_return=False \\
+            timeout_for_tcp_idle_connection=4
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The update_input_endpoint function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "service" not in kwargs:
+        raise SaltCloudSystemExit('A service name must be specified as "service"')
+    if "deployment" not in kwargs:
+        raise SaltCloudSystemExit('A deployment name must be specified as "deployment"')
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('An endpoint name must be specified as "name"')
+    if "role" not in kwargs:
+        raise SaltCloudSystemExit('An role name must be specified as "role"')
+    if activity != "delete":
+        if "port" not in kwargs:
+            raise SaltCloudSystemExit('An endpoint port must be specified as "port"')
+        if "protocol" not in kwargs:
+            raise SaltCloudSystemExit(
+                'An endpoint protocol (tcp or udp) must be specified as "protocol"'
+            )
+        if "local_port" not in kwargs:
+            kwargs["local_port"] = kwargs["port"]
+        if "enable_direct_server_return" not in kwargs:
+            kwargs["enable_direct_server_return"] = False
+        kwargs["enable_direct_server_return"] = str(
+            kwargs["enable_direct_server_return"]
+        ).lower()
+        if "timeout_for_tcp_idle_connection" not in kwargs:
+            kwargs["timeout_for_tcp_idle_connection"] = 4
+    old_endpoints = list_input_endpoints(kwargs, call="function")
+    endpoints_xml = ""
+    endpoint_xml = """
+        <InputEndpoint>
+          <LocalPort>{local_port}</LocalPort>
+          <Name>{name}</Name>
+          <Port>{port}</Port>
+          <Protocol>{protocol}</Protocol>
+          <EnableDirectServerReturn>{enable_direct_server_return}</EnableDirectServerReturn>
+          <IdleTimeoutInMinutes>{timeout_for_tcp_idle_connection}</IdleTimeoutInMinutes>
+        </InputEndpoint>"""
+    if activity == "add":
+        old_endpoints[kwargs["name"]] = kwargs
+        old_endpoints[kwargs["name"]]["Name"] = kwargs["name"]
+    for endpoint in old_endpoints:
+        if old_endpoints[endpoint]["Name"] == kwargs["name"]:
+            if activity != "delete":
+                this_endpoint_xml = endpoint_xml.format(**kwargs)
+                endpoints_xml += this_endpoint_xml
+        else:
+            this_endpoint_xml = endpoint_xml.format(
+                local_port=old_endpoints[endpoint]["LocalPort"],
+                name=old_endpoints[endpoint]["Name"],
+                port=old_endpoints[endpoint]["Port"],
+                protocol=old_endpoints[endpoint]["Protocol"],
+                enable_direct_server_return=old_endpoints[endpoint][
+                    "EnableDirectServerReturn"
+                ],
+                timeout_for_tcp_idle_connection=old_endpoints[endpoint].get(
+                    "IdleTimeoutInMinutes", 4
+                ),
+            )
+            endpoints_xml += this_endpoint_xml
+    request_xml = """<PersistentVMRole xmlns="http://schemas.microsoft.com/windowsazure"
+xmlns:i="http://www.w3.org/2001/XMLSchema-instance">
+  <ConfigurationSets>
+    <ConfigurationSet>
+      <ConfigurationSetType>NetworkConfiguration</ConfigurationSetType>
+      <InputEndpoints>{}
+      </InputEndpoints>
+    </ConfigurationSet>
+  </ConfigurationSets>
+  <OSVirtualHardDisk>
+  </OSVirtualHardDisk>
+</PersistentVMRole>""".format(
+        endpoints_xml
+    )
+    path = "services/hostedservices/{}/deployments/{}/roles/{}".format(
+        kwargs["service"],
+        kwargs["deployment"],
+        kwargs["role"],
+    )
+    query(
+        path=path,
+        method="PUT",
+        header_dict={"Content-Type": "application/xml"},
+        data=request_xml,
+        decode=False,
+    )
+    return True
+@_deprecation_message
+def add_input_endpoint(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Add an input endpoint to the deployment. Please note that
+    there may be a delay before the changes show up.
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f add_input_endpoint my-azure service=myservice \\
+            deployment=mydeployment role=myrole name=HTTP local_port=80 \\
+            port=80 protocol=tcp enable_direct_server_return=False \\
+            timeout_for_tcp_idle_connection=4
+    """
+    return update_input_endpoint(
+        kwargs=kwargs,
+        conn=conn,
+        call="function",
+        activity="add",
+    )
+@_deprecation_message
+def delete_input_endpoint(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Delete an input endpoint from the deployment. Please note that
+    there may be a delay before the changes show up.
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f delete_input_endpoint my-azure service=myservice \\
+            deployment=mydeployment role=myrole name=HTTP
+    """
+    return update_input_endpoint(
+        kwargs=kwargs,
+        conn=conn,
+        call="function",
+        activity="delete",
+    )
+@_deprecation_message
+def show_deployment(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Return information about a deployment
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f show_deployment my-azure name=my_deployment
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The get_deployment function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    if "service_name" not in kwargs:
+        raise SaltCloudSystemExit('A service name must be specified as "service_name"')
+    if "deployment_name" not in kwargs:
+        raise SaltCloudSystemExit(
+            'A deployment name must be specified as "deployment_name"'
+        )
+    data = conn.get_deployment_by_name(
+        service_name=kwargs["service_name"],
+        deployment_name=kwargs["deployment_name"],
+    )
+    return object_to_dict(data)
+get_deployment = show_deployment
+@_deprecation_message
+def list_affinity_groups(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    List input endpoints associated with the deployment
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f list_affinity_groups my-azure
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The list_affinity_groups function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    data = conn.list_affinity_groups()
+    ret = {}
+    for item in data.affinity_groups:
+        ret[item.name] = object_to_dict(item)
+    return ret
+@_deprecation_message
+def show_affinity_group(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Show an affinity group associated with the account
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f show_affinity_group my-azure service=myservice \\
+            deployment=mydeployment name=SSH
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The show_affinity_group function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('An affinity group name must be specified as "name"')
+    data = conn.get_affinity_group_properties(affinity_group_name=kwargs["name"])
+    return object_to_dict(data)
+get_affinity_group = show_affinity_group
+@_deprecation_message
+def create_affinity_group(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Create a new affinity group
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f create_affinity_group my-azure name=my_affinity_group
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The create_affinity_group function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    if "label" not in kwargs:
+        raise SaltCloudSystemExit('A label must be specified as "label"')
+    if "location" not in kwargs:
+        raise SaltCloudSystemExit('A location must be specified as "location"')
+    try:
+        conn.create_affinity_group(
+            kwargs["name"],
+            kwargs["label"],
+            kwargs["location"],
+            kwargs.get("description", None),
+        )
+        return {"Success": "The affinity group was successfully created"}
+    except AzureConflictHttpError:
+        raise SaltCloudSystemExit(
+            "There was a conflict. This usually means that the affinity group already"
+            " exists."
+        )
+@_deprecation_message
+def update_affinity_group(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Update an affinity group's properties
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f update_affinity_group my-azure name=my_group label=my_group
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The update_affinity_group function must be called with -f or --function."
+        )
+    if not conn:
+        conn = get_conn()
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    if "label" not in kwargs:
+        raise SaltCloudSystemExit('A label must be specified as "label"')
+    conn.update_affinity_group(
+        affinity_group_name=kwargs["name"],
+        label=kwargs["label"],
+        description=kwargs.get("description", None),
+    )
+    return show_affinity_group(kwargs={"name": kwargs["name"]}, call="function")
+@_deprecation_message
+def delete_affinity_group(kwargs=None, conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Delete a specific affinity group associated with the account
+    CLI Examples:
+    .. code-block:: bash
+        salt-cloud -f delete_affinity_group my-azure name=my_affinity_group
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The delete_affinity_group function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('A name must be specified as "name"')
+    if not conn:
+        conn = get_conn()
+    try:
+        conn.delete_affinity_group(kwargs["name"])
+        return {"Success": "The affinity group was successfully deleted"}
+    except AzureMissingResourceHttpError as exc:
+        raise SaltCloudSystemExit("{}: {}".format(kwargs["name"], exc.message))
+@_deprecation_message
+def get_storage_conn(storage_account=None, storage_key=None, conn_kwargs=None):
+    """
+    .. versionadded:: 2015.8.0
+    Return a storage_conn object for the storage account
+    """
+    if conn_kwargs is None:
+        conn_kwargs = {}
+    if not storage_account:
+        storage_account = config.get_cloud_config_value(
+            "storage_account",
+            get_configured_provider(),
+            __opts__,
+            search_global=False,
+            default=conn_kwargs.get("storage_account", None),
+        )
+    if not storage_key:
+        storage_key = config.get_cloud_config_value(
+            "storage_key",
+            get_configured_provider(),
+            __opts__,
+            search_global=False,
+            default=conn_kwargs.get("storage_key", None),
+        )
+    return azure.storage.BlobService(storage_account, storage_key)
+@_deprecation_message
+def make_blob_url(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Creates the URL to access a blob
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f make_blob_url my-azure container=mycontainer blob=myblob
+    container:
+        Name of the container.
+    blob:
+        Name of the blob.
+    account:
+        Name of the storage account. If not specified, derives the host base
+        from the provider configuration.
+    protocol:
+        Protocol to use: 'http' or 'https'. If not specified, derives the host
+        base from the provider configuration.
+    host_base:
+        Live host base URL.  If not specified, derives the host base from the
+        provider configuration.
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The make_blob_url function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "container" not in kwargs:
+        raise SaltCloudSystemExit('A container name must be specified as "container"')
+    if "blob" not in kwargs:
+        raise SaltCloudSystemExit('A blob name must be specified as "blob"')
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    data = storage_conn.make_blob_url(
+        kwargs["container"],
+        kwargs["blob"],
+        kwargs.get("account", None),
+        kwargs.get("protocol", None),
+        kwargs.get("host_base", None),
+    )
+    ret = {}
+    for item in data.containers:
+        ret[item.name] = object_to_dict(item)
+    return ret
+@_deprecation_message
+def list_storage_containers(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    List containers associated with the storage account
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f list_storage_containers my-azure
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The list_storage_containers function must be called with -f or --function."
+        )
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    data = storage_conn.list_containers()
+    ret = {}
+    for item in data.containers:
+        ret[item.name] = object_to_dict(item)
+    return ret
+@_deprecation_message
+def create_storage_container(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Create a storage container
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f create_storage_container my-azure name=mycontainer
+    name:
+        Name of container to create.
+    meta_name_values:
+        Optional. A dict with name_value pairs to associate with the
+        container as metadata. Example:{'Category':'test'}
+    blob_public_access:
+        Optional. Possible values include: container, blob
+    fail_on_exist:
+        Specify whether to throw an exception when the container exists.
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The create_storage_container function must be called with -f or"
+            " --function."
+        )
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    try:
+        storage_conn.create_container(
+            container_name=kwargs["name"],
+            x_ms_meta_name_values=kwargs.get("meta_name_values", None),
+            x_ms_blob_public_access=kwargs.get("blob_public_access", None),
+            fail_on_exist=kwargs.get("fail_on_exist", False),
+        )
+        return {"Success": "The storage container was successfully created"}
+    except AzureConflictHttpError:
+        raise SaltCloudSystemExit(
+            "There was a conflict. This usually means that the storage container"
+            " already exists."
+        )
+@_deprecation_message
+def show_storage_container(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Show a container associated with the storage account
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f show_storage_container my-azure name=myservice
+    name:
+        Name of container to show.
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The show_storage_container function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit(
+            'An storage container name must be specified as "name"'
+        )
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    data = storage_conn.get_container_properties(
+        container_name=kwargs["name"],
+        x_ms_lease_id=kwargs.get("lease_id", None),
+    )
+    return data
+get_storage_container = show_storage_container
+@_deprecation_message
+def show_storage_container_metadata(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Show a storage container's metadata
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f show_storage_container_metadata my-azure name=myservice
+    name:
+        Name of container to show.
+    lease_id:
+        If specified, show_storage_container_metadata only succeeds if the
+        container's lease is active and matches this ID.
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The show_storage_container function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit(
+            'An storage container name must be specified as "name"'
+        )
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    data = storage_conn.get_container_metadata(
+        container_name=kwargs["name"],
+        x_ms_lease_id=kwargs.get("lease_id", None),
+    )
+    return data
+get_storage_container_metadata = show_storage_container_metadata
+@_deprecation_message
+def set_storage_container_metadata(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Set a storage container's metadata
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f set_storage_container my-azure name=mycontainer \\
+            x_ms_meta_name_values='{"my_name": "my_value"}'
+    name:
+        Name of existing container.
+    meta_name_values:
+        A dict containing name, value for metadata.
+        Example: {'category':'test'}
+    lease_id:
+        If specified, set_storage_container_metadata only succeeds if the
+        container's lease is active and matches this ID.
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The create_storage_container function must be called with -f or"
+            " --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit(
+            'An storage container name must be specified as "name"'
+        )
+    x_ms_meta_name_values = salt.utils.yaml.safe_load(
+        kwargs.get("meta_name_values", "")
+    )
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    try:
+        storage_conn.set_container_metadata(
+            container_name=kwargs["name"],
+            x_ms_meta_name_values=x_ms_meta_name_values,
+            x_ms_lease_id=kwargs.get("lease_id", None),
+        )
+        return {"Success": "The storage container was successfully updated"}
+    except AzureConflictHttpError:
+        raise SaltCloudSystemExit("There was a conflict.")
+@_deprecation_message
+def show_storage_container_acl(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Show a storage container's acl
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f show_storage_container_acl my-azure name=myservice
+    name:
+        Name of existing container.
+    lease_id:
+        If specified, show_storage_container_acl only succeeds if the
+        container's lease is active and matches this ID.
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The show_storage_container function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit(
+            'An storage container name must be specified as "name"'
+        )
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    data = storage_conn.get_container_acl(
+        container_name=kwargs["name"],
+        x_ms_lease_id=kwargs.get("lease_id", None),
+    )
+    return data
+get_storage_container_acl = show_storage_container_acl
+@_deprecation_message
+def set_storage_container_acl(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Set a storage container's acl
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f set_storage_container my-azure name=mycontainer
+    name:
+        Name of existing container.
+    signed_identifiers:
+        SignedIdentifers instance
+    blob_public_access:
+        Optional. Possible values include: container, blob
+    lease_id:
+        If specified, set_storage_container_acl only succeeds if the
+        container's lease is active and matches this ID.
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The create_storage_container function must be called with -f or"
+            " --function."
+        )
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    try:
+        data = storage_conn.set_container_acl(
+            container_name=kwargs["name"],
+            signed_identifiers=kwargs.get("signed_identifiers", None),
+            x_ms_blob_public_access=kwargs.get("blob_public_access", None),
+            x_ms_lease_id=kwargs.get("lease_id", None),
+        )
+        return {"Success": "The storage container was successfully updated"}
+    except AzureConflictHttpError:
+        raise SaltCloudSystemExit("There was a conflict.")
+@_deprecation_message
+def delete_storage_container(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Delete a container associated with the storage account
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f delete_storage_container my-azure name=mycontainer
+    name:
+        Name of container to create.
+    fail_not_exist:
+        Specify whether to throw an exception when the container exists.
+    lease_id:
+        If specified, delete_storage_container only succeeds if the
+        container's lease is active and matches this ID.
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The delete_storage_container function must be called with -f or"
+            " --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit(
+            'An storage container name must be specified as "name"'
+        )
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    data = storage_conn.delete_container(
+        container_name=kwargs["name"],
+        fail_not_exist=kwargs.get("fail_not_exist", None),
+        x_ms_lease_id=kwargs.get("lease_id", None),
+    )
+    return data
+@_deprecation_message
+def lease_storage_container(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Lease a container associated with the storage account
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f lease_storage_container my-azure name=mycontainer
+    name:
+        Name of container to create.
+    lease_action:
+        Required. Possible values: acquire|renew|release|break|change
+    lease_id:
+        Required if the container has an active lease.
+    lease_duration:
+        Specifies the duration of the lease, in seconds, or negative one
+        (-1) for a lease that never expires. A non-infinite lease can be
+        between 15 and 60 seconds. A lease duration cannot be changed
+        using renew or change. For backwards compatibility, the default is
+        60, and the value is only used on an acquire operation.
+    lease_break_period:
+        Optional. For a break operation, this is the proposed duration of
+        seconds that the lease should continue before it is broken, between
+        0 and 60 seconds. This break period is only used if it is shorter
+        than the time remaining on the lease. If longer, the time remaining
+        on the lease is used. A new lease will not be available before the
+        break period has expired, but the lease may be held for longer than
+        the break period. If this header does not appear with a break
+        operation, a fixed-duration lease breaks after the remaining lease
+        period elapses, and an infinite lease breaks immediately.
+    proposed_lease_id:
+        Optional for acquire, required for change. Proposed lease ID, in a
+        GUID string format.
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The lease_storage_container function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit(
+            'An storage container name must be specified as "name"'
+        )
+    lease_actions = ("acquire", "renew", "release", "break", "change")
+    if kwargs.get("lease_action", None) not in lease_actions:
+        raise SaltCloudSystemExit(
+            "A lease_action must be one of: {}".format(", ".join(lease_actions))
+        )
+    if kwargs["lease_action"] != "acquire" and "lease_id" not in kwargs:
+        raise SaltCloudSystemExit(
+            'A lease ID must be specified for the "{}" lease action '
+            'as "lease_id"'.format(kwargs["lease_action"])
+        )
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    data = storage_conn.lease_container(
+        container_name=kwargs["name"],
+        x_ms_lease_action=kwargs["lease_action"],
+        x_ms_lease_id=kwargs.get("lease_id", None),
+        x_ms_lease_duration=kwargs.get("lease_duration", 60),
+        x_ms_lease_break_period=kwargs.get("lease_break_period", None),
+        x_ms_proposed_lease_id=kwargs.get("proposed_lease_id", None),
+    )
+    return data
+@_deprecation_message
+def list_blobs(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    List blobs associated with the container
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f list_blobs my-azure container=mycontainer
+    container:
+        The name of the storage container
+    prefix:
+        Optional. Filters the results to return only blobs whose names
+        begin with the specified prefix.
+    marker:
+        Optional. A string value that identifies the portion of the list
+        to be returned with the next list operation. The operation returns
+        a marker value within the response body if the list returned was
+        not complete. The marker value may then be used in a subsequent
+        call to request the next set of list items. The marker value is
+        opaque to the client.
+    maxresults:
+        Optional. Specifies the maximum number of blobs to return,
+        including all BlobPrefix elements. If the request does not specify
+        maxresults or specifies a value greater than 5,000, the server will
+        return up to 5,000 items. Setting maxresults to a value less than
+        or equal to zero results in error response code 400 (Bad Request).
+    include:
+        Optional. Specifies one or more datasets to include in the
+        response. To specify more than one of these options on the URI,
+        you must separate each option with a comma. Valid values are:
+        snapshots:
+            Specifies that snapshots should be included in the
+            enumeration. Snapshots are listed from oldest to newest in
+            the response.
+        metadata:
+            Specifies that blob metadata be returned in the response.
+        uncommittedblobs:
+            Specifies that blobs for which blocks have been uploaded,
+            but which have not been committed using Put Block List
+            (REST API), be included in the response.
+        copy:
+            Version 2012-02-12 and newer. Specifies that metadata
+            related to any current or previous Copy Blob operation
+            should be included in the response.
+    delimiter:
+        Optional. When the request includes this parameter, the operation
+        returns a BlobPrefix element in the response body that acts as a
+        placeholder for all blobs whose names begin with the same
+        substring up to the appearance of the delimiter character. The
+        delimiter may be a single character or a string.
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The list_blobs function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "container" not in kwargs:
+        raise SaltCloudSystemExit(
+            'An storage container name must be specified as "container"'
+        )
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    return salt.utils.msazure.list_blobs(storage_conn=storage_conn, **kwargs)
+@_deprecation_message
+def show_blob_service_properties(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Show a blob's service properties
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f show_blob_service_properties my-azure
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The show_blob_service_properties function must be called with -f or"
+            " --function."
+        )
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    data = storage_conn.get_blob_service_properties(
+        timeout=kwargs.get("timeout", None),
+    )
+    return data
+get_blob_service_properties = show_blob_service_properties
+@_deprecation_message
+def set_blob_service_properties(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Sets the properties of a storage account's Blob service, including
+    Windows Azure Storage Analytics. You can also use this operation to
+    set the default request version for all incoming requests that do not
+    have a version specified.
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f set_blob_service_properties my-azure
+    properties:
+        a StorageServiceProperties object.
+    timeout:
+        Optional. The timeout parameter is expressed in seconds.
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The set_blob_service_properties function must be called with -f or"
+            " --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "properties" not in kwargs:
+        raise SaltCloudSystemExit(
+            'The blob service properties name must be specified as "properties"'
+        )
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    data = storage_conn.get_blob_service_properties(
+        storage_service_properties=kwargs["properties"],
+        timeout=kwargs.get("timeout", None),
+    )
+    return data
+@_deprecation_message
+def show_blob_properties(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Returns all user-defined metadata, standard HTTP properties, and
+    system properties for the blob.
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f show_blob_properties my-azure container=mycontainer blob=myblob
+    container:
+        Name of existing container.
+    blob:
+        Name of existing blob.
+    lease_id:
+        Required if the blob has an active lease.
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The show_blob_properties function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "container" not in kwargs:
+        raise SaltCloudSystemExit('The container name must be specified as "container"')
+    if "blob" not in kwargs:
+        raise SaltCloudSystemExit('The blob name must be specified as "blob"')
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    try:
+        data = storage_conn.get_blob_properties(
+            container_name=kwargs["container"],
+            blob_name=kwargs["blob"],
+            x_ms_lease_id=kwargs.get("lease_id", None),
+        )
+    except AzureMissingResourceHttpError:
+        raise SaltCloudSystemExit("The specified blob does not exist.")
+    return data
+get_blob_properties = show_blob_properties
+@_deprecation_message
+def set_blob_properties(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Set a blob's properties
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f set_blob_properties my-azure
+    container:
+        Name of existing container.
+    blob:
+        Name of existing blob.
+    blob_cache_control:
+        Optional. Modifies the cache control string for the blob.
+    blob_content_type:
+        Optional. Sets the blob's content type.
+    blob_content_md5:
+        Optional. Sets the blob's MD5 hash.
+    blob_content_encoding:
+        Optional. Sets the blob's content encoding.
+    blob_content_language:
+        Optional. Sets the blob's content language.
+    lease_id:
+        Required if the blob has an active lease.
+    blob_content_disposition:
+        Optional. Sets the blob's Content-Disposition header.
+        The Content-Disposition response header field conveys additional
+        information about how to process the response payload, and also can
+        be used to attach additional metadata. For example, if set to
+        attachment, it indicates that the user-agent should not display the
+        response, but instead show a Save As dialog with a filename other
+        than the blob name specified.
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The set_blob_properties function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "container" not in kwargs:
+        raise SaltCloudSystemExit(
+            'The blob container name must be specified as "container"'
+        )
+    if "blob" not in kwargs:
+        raise SaltCloudSystemExit('The blob name must be specified as "blob"')
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    data = storage_conn.get_blob_properties(
+        container_name=kwargs["container"],
+        blob_name=kwargs["blob"],
+        x_ms_blob_cache_control=kwargs.get("blob_cache_control", None),
+        x_ms_blob_content_type=kwargs.get("blob_content_type", None),
+        x_ms_blob_content_md5=kwargs.get("blob_content_md5", None),
+        x_ms_blob_content_encoding=kwargs.get("blob_content_encoding", None),
+        x_ms_blob_content_language=kwargs.get("blob_content_language", None),
+        x_ms_lease_id=kwargs.get("lease_id", None),
+        x_ms_blob_content_disposition=kwargs.get("blob_content_disposition", None),
+    )
+    return data
+@_deprecation_message
+def put_blob(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Upload a blob
+    CLI Examples:
+    .. code-block:: bash
+        salt-cloud -f put_blob my-azure container=base name=top.sls blob_path=/srv/salt/top.sls
+        salt-cloud -f put_blob my-azure container=base name=content.txt blob_content='Some content'
+    container:
+        Name of existing container.
+    name:
+        Name of existing blob.
+    blob_path:
+        The path on the local machine of the file to upload as a blob. Either
+        this or blob_content must be specified.
+    blob_content:
+        The actual content to be uploaded as a blob. Either this or blob_path
+        must me specified.
+    cache_control:
+        Optional. The Blob service stores this value but does not use or
+        modify it.
+    content_language:
+        Optional. Specifies the natural languages used by this resource.
+    content_md5:
+        Optional. An MD5 hash of the blob content. This hash is used to
+        verify the integrity of the blob during transport. When this header
+        is specified, the storage service checks the hash that has arrived
+        with the one that was sent. If the two hashes do not match, the
+        operation will fail with error code 400 (Bad Request).
+    blob_content_type:
+        Optional. Set the blob's content type.
+    blob_content_encoding:
+        Optional. Set the blob's content encoding.
+    blob_content_language:
+        Optional. Set the blob's content language.
+    blob_content_md5:
+        Optional. Set the blob's MD5 hash.
+    blob_cache_control:
+        Optional. Sets the blob's cache control.
+    meta_name_values:
+        A dict containing name, value for metadata.
+    lease_id:
+        Required if the blob has an active lease.
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The put_blob function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "container" not in kwargs:
+        raise SaltCloudSystemExit(
+            'The blob container name must be specified as "container"'
+        )
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('The blob name must be specified as "name"')
+    if "blob_path" not in kwargs and "blob_content" not in kwargs:
+        raise SaltCloudSystemExit(
+            'Either a path to a file needs to be passed in as "blob_path" or '
+            'the contents of a blob as "blob_content."'
+        )
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    return salt.utils.msazure.put_blob(storage_conn=storage_conn, **kwargs)
+@_deprecation_message
+def get_blob(kwargs=None, storage_conn=None, call=None):
+    """
+    .. versionadded:: 2015.8.0
+    Download a blob
+    CLI Example:
+    .. code-block:: bash
+        salt-cloud -f get_blob my-azure container=base name=top.sls local_path=/srv/salt/top.sls
+        salt-cloud -f get_blob my-azure container=base name=content.txt return_content=True
+    container:
+        Name of existing container.
+    name:
+        Name of existing blob.
+    local_path:
+        The path on the local machine to download the blob to. Either this or
+        return_content must be specified.
+    return_content:
+        Whether or not to return the content directly from the blob. If
+        specified, must be True or False. Either this or the local_path must
+        be specified.
+    snapshot:
+        Optional. The snapshot parameter is an opaque DateTime value that,
+        when present, specifies the blob snapshot to retrieve.
+    lease_id:
+        Required if the blob has an active lease.
+    progress_callback:
+        callback for progress with signature function(current, total) where
+        current is the number of bytes transferred so far, and total is the
+        size of the blob.
+    max_connections:
+        Maximum number of parallel connections to use when the blob size
+        exceeds 64MB.
+        Set to 1 to download the blob chunks sequentially.
+        Set to 2 or more to download the blob chunks in parallel. This uses
+        more system resources but will download faster.
+    max_retries:
+        Number of times to retry download of blob chunk if an error occurs.
+    retry_wait:
+        Sleep time in secs between retries.
+    """
+    if call != "function":
+        raise SaltCloudSystemExit(
+            "The get_blob function must be called with -f or --function."
+        )
+    if kwargs is None:
+        kwargs = {}
+    if "container" not in kwargs:
+        raise SaltCloudSystemExit(
+            'The blob container name must be specified as "container"'
+        )
+    if "name" not in kwargs:
+        raise SaltCloudSystemExit('The blob name must be specified as "name"')
+    if "local_path" not in kwargs and "return_content" not in kwargs:
+        raise SaltCloudSystemExit(
+            'Either a local path needs to be passed in as "local_path" or '
+            '"return_content" to return the blob contents directly'
+        )
+    if not storage_conn:
+        storage_conn = get_storage_conn(conn_kwargs=kwargs)
+    return salt.utils.msazure.get_blob(storage_conn=storage_conn, **kwargs)
+@_deprecation_message
+def query(path, method="GET", data=None, params=None, header_dict=None, decode=True):
+    """
+    Perform a query directly against the Azure REST API
+    """
+    certificate_path = config.get_cloud_config_value(
+        "certificate_path", get_configured_provider(), __opts__, search_global=False
+    )
+    subscription_id = salt.utils.stringutils.to_str(
+        config.get_cloud_config_value(
+            "subscription_id", get_configured_provider(), __opts__, search_global=False
+        )
+    )
+    management_host = config.get_cloud_config_value(
+        "management_host",
+        get_configured_provider(),
+        __opts__,
+        search_global=False,
+        default="management.core.windows.net",
+    )
+    backend = config.get_cloud_config_value(
+        "backend", get_configured_provider(), __opts__, search_global=False
+    )
+    url = "https://{management_host}/{subscription_id}/{path}".format(
+        management_host=management_host,
+        subscription_id=subscription_id,
+        path=path,
+    )
+    if header_dict is None:
+        header_dict = {}
+    header_dict["x-ms-version"] = "2014-06-01"
+    result = salt.utils.http.query(
+        url,
+        method=method,
+        params=params,
+        data=data,
+        header_dict=header_dict,
+        port=443,
+        text=True,
+        cert=certificate_path,
+        backend=backend,
+        decode=decode,
+        decode_type="xml",
+    )
+    if "dict" in result:
+        return result["dict"]
+    return

--- a/salt/cloud/clouds/proxmox.py
+++ b/salt/cloud/clouds/proxmox.py
@@ -7,24 +7,20 @@
 Set up the cloud configuration at ``/etc/salt/cloud.providers`` or
  ``/etc/salt/cloud.providers.d/proxmox.conf``:
 .. code-block:: yaml
     my-proxmox-config:
       user: myuser@pam or myuser@pve
       password: mypassword
       url: hypervisor.domain.tld
       port: 8006
       driver: proxmox
       verify_ssl: True
-.. warning::
-    This cloud provider will be removed from Salt in version 3009.0 in favor of
-    the `saltext.proxmox Salt Extension
-    <https://github.com/salt-extensions/saltext-proxmox>`_
 :maintainer: Frank Klaassen <frank@cloudright.nl>
 :depends: requests >= 2.2.1
 :depends: IPy >= 0.81
 """
 import logging
 import pprint
 import re
 import socket
 import time
 import urllib
@@ -41,25 +37,20 @@
     HAS_REQUESTS = True
 except ImportError:
     HAS_REQUESTS = False
 try:
     from IPy import IP
     HAS_IPY = True
 except ImportError:
     HAS_IPY = False
 log = logging.getLogger(__name__)
 __virtualname__ = "proxmox"
-__deprecated__ = (
-    3009,
-    "proxmox",
-    "https://github.com/salt-extensions/saltext-proxmox",
-)
 def __virtual__():
     """
     Check for PROXMOX configurations
     """
     if get_configured_provider() is False:
         return False
     if get_dependencies() is False:
         return False
     return __virtualname__
 def _get_active_provider_name():
@@ -107,21 +98,23 @@
     )
     verify_ssl = config.get_cloud_config_value(
         "verify_ssl",
         get_configured_provider(),
         __opts__,
         default=True,
         search_global=False,
     )
     connect_data = {"username": username, "password": passwd}
     full_url = f"https://{url}:{port}/api2/json/access/ticket"
-    response = requests.post(full_url, verify=verify_ssl, data=connect_data)
+    response = requests.post(
+        full_url, verify=verify_ssl, data=connect_data, timeout=120
+    )
     response.raise_for_status()
     returned_data = response.json()
     ticket = {"PVEAuthCookie": returned_data["data"]["ticket"]}
     csrf = str(returned_data["data"]["CSRFPreventionToken"])
 def query(conn_type, option, post_data=None):
     """
     Execute the HTTP request to the API
     """
     if ticket is None or csrf is None or url is None:
         log.debug("Not authenticated yet, doing that now..")
@@ -134,41 +127,46 @@
         "User-Agent": "salt-cloud-proxmox",
     }
     if conn_type == "post":
         httpheaders["CSRFPreventionToken"] = csrf
         response = requests.post(
             full_url,
             verify=verify_ssl,
             data=post_data,
             cookies=ticket,
             headers=httpheaders,
+            timeout=120,
         )
     elif conn_type == "put":
         httpheaders["CSRFPreventionToken"] = csrf
         response = requests.put(
             full_url,
             verify=verify_ssl,
             data=post_data,
             cookies=ticket,
             headers=httpheaders,
+            timeout=120,
         )
     elif conn_type == "delete":
         httpheaders["CSRFPreventionToken"] = csrf
         response = requests.delete(
             full_url,
             verify=verify_ssl,
             data=post_data,
             cookies=ticket,
             headers=httpheaders,
+            timeout=120,
         )
     elif conn_type == "get":
-        response = requests.get(full_url, verify=verify_ssl, cookies=ticket)
+        response = requests.get(
+            full_url, verify=verify_ssl, cookies=ticket, timeout=120
+        )
     try:
         response.raise_for_status()
     except requests.exceptions.RequestException:
         log.error("Error in %s query to %s:\n%s", conn_type, full_url, response.text)
         raise
     try:
         returned_data = response.json()
         if "data" not in returned_data:
             raise SaltCloudExecutionFailure
         return returned_data["data"]
@@ -644,21 +642,21 @@
         return preferred_ip(vm_, ips)
     raise SaltCloudExecutionFailure
 def _import_api():
     """
     Download https://<url>/pve-docs/api-viewer/apidoc.js
     Extract content of pveapi var (json formatted)
     Load this json content into global variable "api"
     """
     global api
     full_url = f"https://{url}:{port}/pve-docs/api-viewer/apidoc.js"
-    returned_data = requests.get(full_url, verify=verify_ssl)
+    returned_data = requests.get(full_url, verify=verify_ssl, timeout=120)
     re_filter = re.compile(" (?:pveapi|apiSchema) = (.*)^;", re.DOTALL | re.MULTILINE)
     api_json = re_filter.findall(returned_data.text)[0]
     api = salt.utils.json.loads(api_json)
 def _get_properties(path="", method="GET", forced_params=None):
     """
     Return the parameter list from api for defined path and HTTP method
     """
     if api is None:
         _import_api()
     sub = api

--- a/salt/config/__init__.py
+++ b/salt/config/__init__.py
@@ -4,20 +4,21 @@
 import codecs
 import glob
 import logging
 import os
 import re
 import sys
 import time
 import types
 import urllib.parse
 from copy import deepcopy
+import salt.crypt
 import salt.defaults.exitcodes
 import salt.exceptions
 import salt.features
 import salt.syspaths
 import salt.utils.data
 import salt.utils.dictupdate
 import salt.utils.files
 import salt.utils.immutabletypes as immutabletypes
 import salt.utils.network
 import salt.utils.path
@@ -30,22 +31,20 @@
 import salt.utils.yaml
 from salt._logging import (
     DFLT_LOG_DATEFMT,
     DFLT_LOG_DATEFMT_LOGFILE,
     DFLT_LOG_FMT_CONSOLE,
     DFLT_LOG_FMT_JID,
     DFLT_LOG_FMT_LOGFILE,
 )
 try:
     import psutil
-    if not hasattr(psutil, "virtual_memory"):
-        raise ImportError("Version of psutil too old.")
     HAS_PSUTIL = True
 except ImportError:
     HAS_PSUTIL = False
 log = logging.getLogger(__name__)
 _DFLT_REFSPECS = ["+refs/heads/*:refs/remotes/origin/*", "+refs/tags/*:refs/tags/*"]
 DEFAULT_INTERVAL = 60
 DEFAULT_HASH_TYPE = "sha256"
 if salt.utils.platform.is_windows():
     _DFLT_IPC_MODE = "tcp"
     _DFLT_FQDNS_GRAINS = False
@@ -65,21 +64,21 @@
     _DFLT_IPC_MODE = "ipc"
     _DFLT_FQDNS_GRAINS = False
     _MASTER_TRIES = 1
     _MASTER_USER = salt.utils.user.get_user()
 def _gather_buffer_space():
     """
     Gather some system data and then calculate
     buffer space.
     Result is in bytes.
     """
-    if HAS_PSUTIL and psutil.version_info >= (0, 6, 0):
+    if HAS_PSUTIL:
         total_mem = psutil.virtual_memory().total
     else:
         import platform
         import salt.grains.core
         os_data = {"kernel": platform.system()}
         grains = salt.grains.core._memdata(os_data)
         total_mem = grains["mem_total"]
     return max([total_mem * 0.05, 10 << 20])
 _DFLT_IPC_WBUFFER = int(_gather_buffer_space() * 0.5)
 _DFLT_IPC_RBUFFER = int(_gather_buffer_space() * 0.5)
@@ -105,23 +104,20 @@
         "master_pubkey_signature": str,
         "master_use_pubkey_signature": bool,
         "master_stats": bool,
         "master_stats_event_iter": int,
         "syndic_finger": str,
         "key_cache": str,
         "user": str,
         "root_dir": str,
         "pki_dir": str,
         "id": str,
-        "cluster_id": str,
-        "cluster_peers": list,
-        "cluster_pki_dir": str,
         "id_function": (dict, str),
         "cachedir": str,
         "append_minionid_config_dirs": list,
         "cache_jobs": bool,
         "conf_file": str,
         "sock_dir": str,
         "sock_pool_size": int,
         "backup_mode": str,
         "renderer": str,
         "renderer_whitelist": list,
@@ -209,21 +205,20 @@
         "state_auto_order": bool,
         "state_events": bool,
         "acceptance_wait_time": float,
         "acceptance_wait_time_max": float,
         "rejected_retry": bool,
         "loop_interval": float,
         "verify_env": bool,
         "grains": dict,
         "permissive_pki_access": bool,
         "key_pass": (type(None), str),
-        "cluster_key_pass": (type(None), str),
         "signing_key_pass": (type(None), str),
         "default_include": str,
         "update_url": (bool, str),
         "update_restart_services": list,
         "retry_dns": float,
         "retry_dns_count": (type(None), int),
         "resolve_dns_fallback": bool,
         "recon_max": float,
         "recon_default": float,
         "recon_randomize": bool,
@@ -265,20 +260,21 @@
         "master_roots": dict,
         "add_proxymodule_to_opts": bool,
         "proxy_merge_pillar_in_opts": bool,
         "proxy_deep_merge_pillar_in_opts": bool,
         "proxy_merge_pillar_in_opts_strategy": str,
         "proxy_mines_pillar": bool,
         "proxy_always_alive": bool,
         "proxy_keep_alive": bool,
         "proxy_keep_alive_interval": int,
         "roots_update_interval": int,
+        "azurefs_update_interval": int,
         "gitfs_update_interval": int,
         "git_pillar_update_interval": int,
         "hgfs_update_interval": int,
         "minionfs_update_interval": int,
         "s3fs_update_interval": int,
         "svnfs_update_interval": int,
         "git_pillar_ssl_verify": bool,
         "git_pillar_global_lock": bool,
         "git_pillar_user": str,
         "git_pillar_password": str,
@@ -510,20 +506,23 @@
         "detect_remote_minions": bool,
         "remote_minions_port": int,
         "pass_variable_prefix": str,
         "pass_strict_fetch": bool,
         "pass_gnupghome": str,
         "pass_dir": str,
         "maintenance_interval": int,
         "fileserver_interval": int,
         "request_channel_timeout": int,
         "request_channel_tries": int,
+        "encryption_algorithm": str,
+        "signing_algorithm": str,
+        "publish_signing_algorithm": str,
     }
 )
 DEFAULT_MINION_OPTS = immutabletypes.freeze(
     {
         "interface": "0.0.0.0",
         "master": "salt",
         "master_type": "str",
         "master_uri_format": "default",
         "source_interface_name": "",
         "source_address": "",
@@ -606,22 +605,23 @@
         "fileserver_followsymlinks": True,
         "fileserver_ignoresymlinks": False,
         "pillar_roots": {
             "base": [salt.syspaths.BASE_PILLAR_ROOTS_DIR, salt.syspaths.SPM_PILLAR_PATH]
         },
         "on_demand_ext_pillar": ["libvirt", "virtkey"],
         "decrypt_pillar": [],
         "decrypt_pillar_delimiter": ":",
         "decrypt_pillar_default": "gpg",
         "decrypt_pillar_renderers": ["gpg"],
-        "gpg_decrypt_must_succeed": True,
+        "gpg_decrypt_must_succeed": False,
         "roots_update_interval": DEFAULT_INTERVAL,
+        "azurefs_update_interval": DEFAULT_INTERVAL,
         "gitfs_update_interval": DEFAULT_INTERVAL,
         "git_pillar_update_interval": DEFAULT_INTERVAL,
         "hgfs_update_interval": DEFAULT_INTERVAL,
         "minionfs_update_interval": DEFAULT_INTERVAL,
         "s3fs_update_interval": DEFAULT_INTERVAL,
         "svnfs_update_interval": DEFAULT_INTERVAL,
         "git_pillar_base": "master",
         "git_pillar_branch": "master",
         "git_pillar_env": "",
         "git_pillar_fallback": "",
@@ -811,20 +811,22 @@
         "extmod_blacklist": {},
         "minion_sign_messages": False,
         "discovery": False,
         "schedule": {},
         "ssh_merge_pillar": True,
         "disabled_requisites": [],
         "global_state_conditions": None,
         "reactor_niceness": None,
         "fips_mode": False,
         "features": {},
+        "encryption_algorithm": "OAEP-SHA1",
+        "signing_algorithm": "PKCS1v15-SHA1",
     }
 )
 DEFAULT_MASTER_OPTS = immutabletypes.freeze(
     {
         "interface": "0.0.0.0",
         "publish_port": 4505,
         "zmq_backlog": 1000,
         "pub_hwm": 1000,
         "auth_mode": 1,
         "user": _MASTER_USER,
@@ -845,34 +847,35 @@
         },
         "master_roots": {"base": [salt.syspaths.BASE_MASTER_ROOTS_DIR]},
         "pillar_roots": {
             "base": [salt.syspaths.BASE_PILLAR_ROOTS_DIR, salt.syspaths.SPM_PILLAR_PATH]
         },
         "on_demand_ext_pillar": ["libvirt", "virtkey"],
         "decrypt_pillar": [],
         "decrypt_pillar_delimiter": ":",
         "decrypt_pillar_default": "gpg",
         "decrypt_pillar_renderers": ["gpg"],
-        "gpg_decrypt_must_succeed": True,
+        "gpg_decrypt_must_succeed": False,
         "thoriumenv": None,
         "thorium_top": "top.sls",
         "thorium_interval": 0.5,
         "thorium_roots": {"base": [salt.syspaths.BASE_THORIUM_ROOTS_DIR]},
         "top_file_merging_strategy": "merge",
         "env_order": [],
         "saltenv": None,
         "lock_saltenv": False,
         "pillarenv": None,
         "default_top": "base",
         "file_client": "local",
         "local": True,
         "roots_update_interval": DEFAULT_INTERVAL,
+        "azurefs_update_interval": DEFAULT_INTERVAL,
         "gitfs_update_interval": DEFAULT_INTERVAL,
         "git_pillar_update_interval": DEFAULT_INTERVAL,
         "hgfs_update_interval": DEFAULT_INTERVAL,
         "minionfs_update_interval": DEFAULT_INTERVAL,
         "s3fs_update_interval": DEFAULT_INTERVAL,
         "svnfs_update_interval": DEFAULT_INTERVAL,
         "git_pillar_base": "master",
         "git_pillar_branch": "master",
         "git_pillar_env": "",
         "git_pillar_fallback": "",
@@ -1053,21 +1056,20 @@
         "loop_interval": 60,
         "nodegroups": {},
         "ssh_list_nodegroups": {},
         "ssh_use_home_key": False,
         "cython_enable": False,
         "enable_gpu_grains": False,
         "key_logfile": os.path.join(salt.syspaths.LOGS_DIR, "key"),
         "verify_env": True,
         "permissive_pki_access": False,
         "key_pass": None,
-        "cluster_key_pass": None,
         "signing_key_pass": None,
         "default_include": "master.d/*.conf",
         "winrepo_dir": os.path.join(salt.syspaths.BASE_FILE_ROOTS_DIR, "win", "repo"),
         "winrepo_dir_ng": os.path.join(
             salt.syspaths.BASE_FILE_ROOTS_DIR, "win", "repo-ng"
         ),
         "winrepo_cachefile": "winrepo.p",
         "winrepo_remotes": ["https://github.com/saltstack/salt-winrepo.git"],
         "winrepo_remotes_ng": ["https://github.com/saltstack/salt-winrepo-ng.git"],
         "winrepo_branch": "master",
@@ -1152,24 +1154,22 @@
         "fips_mode": False,
         "detect_remote_minions": False,
         "remote_minions_port": 22,
         "pass_variable_prefix": "",
         "pass_strict_fetch": False,
         "pass_gnupghome": "",
         "pass_dir": "",
         "netapi_enable_clients": [],
         "maintenance_interval": 3600,
         "fileserver_interval": 3600,
-        "cluster_id": None,
-        "cluster_peers": [],
-        "cluster_pki_dir": None,
         "features": {},
+        "publish_signing_algorithm": "PKCS1v15-SHA1",
     }
 )
 DEFAULT_PROXY_MINION_OPTS = immutabletypes.freeze(
     {
         "conf_file": os.path.join(salt.syspaths.CONFIG_DIR, "proxy"),
         "log_file": os.path.join(salt.syspaths.LOGS_DIR, "proxy"),
         "add_proxymodule_to_opts": False,
         "proxy_merge_grains_in_module": True,
         "extension_modules": os.path.join(salt.syspaths.CACHE_DIR, "proxy", "extmods"),
         "append_minionid_config_dirs": [
@@ -1580,20 +1580,30 @@
                 else:
                     opts = {}
             schedule = opts.get("schedule", {})
             if schedule and "schedule" in configuration:
                 configuration["schedule"].update(schedule)
             include = opts.get("include", [])
             if include:
                 opts.update(include_config(include, fn_, verbose))
             salt.utils.dictupdate.update(configuration, opts, True, True)
     return configuration
+def should_prepend_root_dir(key, opts):
+    """
+    Prepend root dir only when the key exists, has a value, and that value is
+    not a URI.
+    """
+    return (
+        key in opts
+        and opts[key] is not None
+        and urllib.parse.urlparse(os.path.splitdrive(opts[key])[1]).scheme == ""
+    )
 def prepend_root_dir(opts, path_options):
     """
     Prepends the options that represent filesystem paths with value of the
     'root_dir' option.
     """
     root_dir = os.path.abspath(opts["root_dir"])
     def_root_dir = salt.syspaths.ROOT_DIR.rstrip(os.sep)
     for path_option in path_options:
         if path_option in opts:
             path = opts[path_option]
@@ -1811,21 +1821,21 @@
         "cachedir",
         "pidfile",
         "sock_dir",
         "extension_modules",
         "autosign_file",
         "autoreject_file",
         "token_dir",
         "autosign_grains_dir",
     ]
     for config_key in ("log_file", "key_logfile", "syndic_log_file"):
-        if urllib.parse.urlparse(opts.get(config_key, "")).scheme == "":
+        if should_prepend_root_dir(config_key, opts):
             prepend_root_dirs.append(config_key)
     prepend_root_dir(opts, prepend_root_dirs)
     salt.features.setup_features(opts)
     return opts
 def apply_sdb(opts, sdb_opts=None):
     """
     Recurse for sdb:// links for opts
     """
     import salt.utils.sdb
     if sdb_opts is None:
@@ -1974,22 +1984,22 @@
                 )
         providers_config = opts["providers"]
     elif providers_config_path is not None:
         providers_config = cloud_providers_config(providers_config_path)
     opts["providers"] = providers_config
     if profiles_config is None:
         profiles_config = vm_profiles_config(profiles_config_path, providers_config)
     opts["profiles"] = profiles_config
     apply_sdb(opts)
     prepend_root_dirs = ["cachedir"]
-    if "log_file" in opts and urllib.parse.urlparse(opts["log_file"]).scheme == "":
-        prepend_root_dirs.append(opts["log_file"])
+    if should_prepend_root_dir("log_file", opts):
+        prepend_root_dirs.append("log_file")
     prepend_root_dir(opts, prepend_root_dirs)
     salt.features.setup_features(opts)
     return opts
 def apply_cloud_config(overrides, defaults=None):
     """
     Return a cloud config
     """
     if defaults is None:
         defaults = DEFAULT_CLOUD_OPTS.copy()
     config = defaults.copy()
@@ -2743,23 +2753,21 @@
                 opts["environment"],
             )
             opts["saltenv"] = opts["environment"]
     for idx, val in enumerate(opts["fileserver_backend"]):
         if val in ("git", "hg", "svn", "minion"):
             new_val = val + "fs"
             log.debug(
                 "Changed %s to %s in minion opts' fileserver_backend list", val, new_val
             )
             opts["fileserver_backend"][idx] = new_val
-    opts["__cli"] = salt.utils.stringutils.to_unicode(
-        os.path.basename(salt.utils.path.expand(sys.argv[0]))
-    )
+    opts["__cli"] = salt.utils.stringutils.to_unicode(os.path.basename(sys.argv[0]))
     using_ip_for_id = False
     if not opts.get("id"):
         if minion_id:
             opts["id"] = minion_id
         else:
             opts["id"], using_ip_for_id = get_id(opts, cache_minion_id=cache_minion_id)
     if not using_ip_for_id and "append_domain" in opts:
         opts["id"] = _append_domain(opts)
     for directory in opts.get("append_minionid_config_dirs", []):
         if directory in ("pki_dir", "cachedir", "extension_modules"):
@@ -2789,32 +2797,42 @@
     ]
     insert_system_path(opts, opts["utils_dirs"])
     prepend_root_dirs = [
         "pki_dir",
         "cachedir",
         "sock_dir",
         "extension_modules",
         "pidfile",
     ]
     for config_key in ("log_file", "key_logfile"):
-        if urllib.parse.urlparse(opts.get(config_key, "")).scheme == "":
+        if should_prepend_root_dir(config_key, opts):
             prepend_root_dirs.append(config_key)
     prepend_root_dir(opts, prepend_root_dirs)
     if "beacons" not in opts:
         opts["beacons"] = {}
     if overrides.get("ipc_write_buffer", "") == "dynamic":
         opts["ipc_write_buffer"] = _DFLT_IPC_WBUFFER
     if "ipc_write_buffer" not in overrides:
         opts["ipc_write_buffer"] = 0
     opts["hash_type"] = opts["hash_type"].lower()
     _update_ssl_config(opts)
     _update_discovery_config(opts)
+    if opts["encryption_algorithm"] not in salt.crypt.VALID_ENCRYPTION_ALGORITHMS:
+        raise salt.exceptions.SaltConfigurationError(
+            f"The encryption algorithm '{opts['encryption_algorithm']}' is not valid. "
+            f"Please specify one of {','.join(salt.crypt.VALID_ENCRYPTION_ALGORITHMS)}."
+        )
+    if opts["signing_algorithm"] not in salt.crypt.VALID_SIGNING_ALGORITHMS:
+        raise salt.exceptions.SaltConfigurationError(
+            f"The signging algorithm '{opts['signing_algorithm']}' is not valid. "
+            f"Please specify one of {','.join(salt.crypt.VALID_SIGNING_ALGORITHMS)}."
+        )
     return opts
 def _update_discovery_config(opts):
     """
     Update discovery config for all instances.
     :param opts:
     :return:
     """
     if opts.get("discovery") not in (None, False):
         if opts["discovery"] is True:
             opts["discovery"] = {}
@@ -2893,23 +2911,21 @@
     _adjust_log_file_override(overrides, defaults["log_file"])
     if overrides:
         opts.update(overrides)
     if "rest" in opts.get("external_auth", {}):
         if opts["keep_acl_in_token"] is False:
             log.warning(
                 "The 'rest' external_auth backend requires 'keep_acl_in_token' to be True. "
                 "Setting 'keep_acl_in_token' to True."
             )
         opts["keep_acl_in_token"] = True
-    opts["__cli"] = salt.utils.stringutils.to_unicode(
-        os.path.basename(salt.utils.path.expand(sys.argv[0]))
-    )
+    opts["__cli"] = salt.utils.stringutils.to_unicode(os.path.basename(sys.argv[0]))
     if "environment" in opts:
         if opts["saltenv"] is not None:
             log.warning(
                 "The 'saltenv' and 'environment' master config options "
                 "cannot both be used. Ignoring 'environment' in favor of "
                 "'saltenv'."
             )
             opts["environment"] = opts["saltenv"]
         else:
             log.warning(
@@ -2956,44 +2972,23 @@
         "sock_dir",
         "extension_modules",
         "autosign_file",
         "autoreject_file",
         "token_dir",
         "syndic_dir",
         "sqlite_queue_dir",
         "autosign_grains_dir",
     ]
     for config_key in ("log_file", "key_logfile", "ssh_log_file"):
-        log_setting = opts.get(config_key, "")
-        if log_setting is None:
-            continue
-        if urllib.parse.urlparse(log_setting).scheme == "":
+        if should_prepend_root_dir(config_key, opts):
             prepend_root_dirs.append(config_key)
     prepend_root_dir(opts, prepend_root_dirs)
-    if "cluster_id" not in opts:
-        opts["cluster_id"] = None
-    if opts["cluster_id"] is not None:
-        if not opts.get("cluster_peers", None):
-            log.warning("Cluster id defined without defining cluster peers")
-            opts["cluster_peers"] = []
-        if not opts.get("cluster_pki_dir", None):
-            log.warning(
-                "Cluster id defined without defining cluster pki, falling back to pki_dir"
-            )
-            opts["cluster_pki_dir"] = opts["pki_dir"]
-    else:
-        if opts.get("cluster_peers", None):
-            log.warning("Cluster peers defined without a cluster_id, ignoring.")
-            opts["cluster_peers"] = []
-        if opts.get("cluster_pki_dir", None):
-            log.warning("Cluster pki defined without a cluster_id, ignoring.")
-            opts["cluster_pki_dir"] = None
     opts["open_mode"] = opts["open_mode"] is True
     opts["auto_accept"] = opts["auto_accept"] is True
     opts["file_roots"] = _validate_file_roots(opts["file_roots"])
     opts["pillar_roots"] = _validate_file_roots(opts["pillar_roots"])
     if opts["file_ignore_regex"]:
         if isinstance(opts["file_ignore_regex"], str):
             ignore_regex = [opts["file_ignore_regex"]]
         elif isinstance(opts["file_ignore_regex"], list):
             ignore_regex = opts["file_ignore_regex"]
         opts["file_ignore_regex"] = []
@@ -3010,20 +3005,25 @@
         log.warning(
             "The 'worker_threads' setting in '%s' cannot be lower than "
             "3. Resetting it to the default value of 3.",
             opts["conf_file"],
         )
         opts["worker_threads"] = 3
     opts.setdefault("pillar_source_merging_strategy", "smart")
     opts["hash_type"] = opts["hash_type"].lower()
     _update_ssl_config(opts)
     _update_discovery_config(opts)
+    if opts["publish_signing_algorithm"] not in salt.crypt.VALID_SIGNING_ALGORITHMS:
+        raise salt.exceptions.SaltConfigurationError(
+            f"The  publish signging algorithm '{opts['publish_signing_algorithm']}' is not valid. "
+            f"Please specify one of {','.join(salt.crypt.VALID_SIGNING_ALGORITHMS)}."
+        )
     return opts
 def client_config(path, env_var="SALT_CLIENT_CONFIG", defaults=None):
     """
     Load Master configuration data
     Usage:
     .. code-block:: python
         import salt.config
         master_opts = salt.config.client_config('/etc/salt/master')
     Returns a dictionary of the Salt Master configuration file with necessary
     options needed to communicate with a locally-running Salt Master daemon.
@@ -3113,17 +3113,14 @@
     if overrides:
         opts.update(overrides)
     prepend_root_dirs = [
         "formula_path",
         "pillar_path",
         "reactor_path",
         "spm_cache_dir",
         "spm_build_dir",
     ]
     for config_key in ("spm_logfile",):
-        log_setting = opts.get(config_key, "")
-        if log_setting is None:
-            continue
-        if urllib.parse.urlparse(log_setting).scheme == "":
+        if should_prepend_root_dir(config_key, opts):
             prepend_root_dirs.append(config_key)
     prepend_root_dir(opts, prepend_root_dirs)
     return opts

--- a/salt/crypt.py
+++ b/salt/crypt.py
@@ -4,322 +4,323 @@
 authenticating peers
 """
 import base64
 import binascii
 import copy
 import getpass
 import hashlib
 import hmac
 import logging
 import os
-import pathlib
 import random
 import stat
 import sys
-import tempfile
 import time
 import traceback
 import uuid
 import weakref
-import tornado.gen
 import salt.channel.client
 import salt.defaults.exitcodes
+import salt.ext.tornado.gen
 import salt.payload
 import salt.utils.crypt
 import salt.utils.decorators
 import salt.utils.event
 import salt.utils.files
 import salt.utils.rsax931
 import salt.utils.sdb
 import salt.utils.stringutils
 import salt.utils.user
 import salt.utils.verify
 import salt.version
 from salt.exceptions import (
     AuthenticationError,
     InvalidKeyError,
     MasterExit,
     SaltClientError,
     SaltReqTimeoutError,
+    UnsupportedAlgorithm,
 )
 try:
-    from M2Crypto import BIO, EVP, RSA
-    HAS_M2 = True
+    import cryptography.exceptions
+    from cryptography.hazmat.primitives import hashes, serialization
+    from cryptography.hazmat.primitives.asymmetric import padding, rsa
+    from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
+    HAS_CRYPTOGRAPHY = True
 except ImportError:
-    HAS_M2 = False
-if not HAS_M2:
-    try:
-        from Cryptodome import Random
-        from Cryptodome.Cipher import AES, PKCS1_OAEP
-        from Cryptodome.Cipher import PKCS1_v1_5 as PKCS1_v1_5_CIPHER
-        from Cryptodome.Hash import SHA
-        from Cryptodome.PublicKey import RSA
-        from Cryptodome.Signature import PKCS1_v1_5
-        HAS_CRYPTO = True
-    except ImportError:
-        HAS_CRYPTO = False
-if not HAS_M2 and not HAS_CRYPTO:
-    try:
-        from Crypto import Random  # nosec
-        from Crypto.Cipher import AES, PKCS1_OAEP  # nosec
-        from Crypto.Cipher import PKCS1_v1_5 as PKCS1_v1_5_CIPHER  # nosec
-        from Crypto.Hash import SHA  # nosec
-        from Crypto.PublicKey import RSA  # nosec
-        from Crypto.Signature import PKCS1_v1_5  # nosec
-        HAS_CRYPTO = True
-    except ImportError:
-        HAS_CRYPTO = False
+    HAS_CRYPTOGRAPHY = False
 log = logging.getLogger(__name__)
+OAEP = "OAEP"
+PKCS1v15 = "PKCS1v15"
+SHA1 = "SHA1"
+SHA224 = "SHA224"
+OAEP_SHA1 = f"{OAEP}-{SHA1}"
+OAEP_SHA224 = f"{OAEP}-{SHA224}"
+PKCS1v15_SHA1 = f"{PKCS1v15}-{SHA1}"
+PKCS1v15_SHA224 = f"{PKCS1v15}-{SHA224}"
+VALID_HASHES = (
+    SHA1,
+    SHA224,
+)
+VALID_PADDING_FOR_SIGNING = (PKCS1v15,)
+VALID_PADDING_FOR_ENCRYPTION = (OAEP,)
+VALID_ENCRYPTION_ALGORITHMS = (
+    OAEP_SHA1,
+    OAEP_SHA224,
+)
+VALID_SIGNING_ALGORITHMS = (
+    PKCS1v15_SHA1,
+    PKCS1v15_SHA224,
+)
+def fips_enabled():
+    if HAS_CRYPTOGRAPHY:
+        import cryptography.hazmat.backends.openssl.backend
+        return cryptography.hazmat.backends.openssl.backend._fips_enabled
 def clean_key(key):
     """
     Clean the key so that it only has unix style line endings (\\n)
     """
     return "\n".join(key.strip().splitlines())
-def read_dropfile(cachedir):
-    dfn = os.path.join(cachedir, ".dfn")
-    try:
-        with salt.utils.files.fopen(dfn, "r") as fp:
-            return fp.read()
-    except FileNotFoundError:
-        pass
-def dropfile(cachedir, user=None, master_id=""):
+def dropfile(cachedir, user=None):
     """
     Set an AES dropfile to request the master update the publish session key
     """
-    dfn_next = os.path.join(cachedir, ".dfn-next")
     dfn = os.path.join(cachedir, ".dfn")
     with salt.utils.files.set_umask(0o277):
         log.info("Rotating AES key")
         if os.path.isfile(dfn):
             log.info("AES key rotation already requested")
             return
         if os.path.isfile(dfn) and not os.access(dfn, os.W_OK):
             os.chmod(dfn, stat.S_IRUSR | stat.S_IWUSR)
-        with salt.utils.files.fopen(dfn_next, "w+") as fp_:
-            fp_.write(master_id)
-        os.chmod(dfn_next, stat.S_IRUSR)
+        with salt.utils.files.fopen(dfn, "wb+") as fp_:
+            fp_.write(b"")
+        os.chmod(dfn, stat.S_IRUSR)
         if user:
             try:
                 import pwd
                 uid = pwd.getpwnam(user).pw_uid
-                os.chown(dfn_next, uid, -1)
+                os.chown(dfn, uid, -1)
             except (KeyError, ImportError, OSError):
                 pass
-        os.rename(dfn_next, dfn)
-def gen_keys(keydir, keyname, keysize, user=None, passphrase=None):
+def gen_keys(keydir, keyname, keysize, user=None, passphrase=None, e=65537):
     """
     Generate a RSA public keypair for use with salt
     :param str keydir: The directory to write the keypair to
     :param str keyname: The type of salt server for whom this key should be written. (i.e. 'master' or 'minion')
     :param int keysize: The number of bits in the key
     :param str user: The user on the system who should own this keypair
     :param str passphrase: The passphrase which should be used to encrypt the private key
     :rtype: str
     :return: Path on the filesystem to the RSA private key
     """
     base = os.path.join(keydir, keyname)
     priv = f"{base}.pem"
     pub = f"{base}.pub"
-    if HAS_M2:
-        gen = RSA.gen_key(keysize, 65537, lambda: None)
-    else:
-        salt.utils.crypt.reinit_crypto()
-        gen = RSA.generate(bits=keysize, e=65537)
+    gen = rsa.generate_private_key(e, keysize)
     if os.path.isfile(priv):
         return priv
     if not os.access(keydir, os.W_OK):
         raise OSError(
             'Write access denied to "{}" for user "{}".'.format(
                 os.path.abspath(keydir), getpass.getuser()
             )
         )
     with salt.utils.files.set_umask(0o277):
-        if HAS_M2:
-            if not passphrase:
-                gen.save_pem(priv, cipher=None)
+        with salt.utils.files.fopen(priv, "wb+") as f:
+            if passphrase:
+                enc = serialization.BestAvailableEncryption(passphrase.encode())
+                _format = serialization.PrivateFormat.TraditionalOpenSSL
+                if fips_enabled():
+                    _format = serialization.PrivateFormat.PKCS8
             else:
-                gen.save_pem(
-                    priv,
-                    cipher="des_ede3_cbc",
-                    callback=lambda x: salt.utils.stringutils.to_bytes(passphrase),
-                )
-        else:
-            with salt.utils.files.fopen(priv, "wb+") as f:
-                f.write(gen.exportKey("PEM", passphrase))
-    if HAS_M2:
-        gen.save_pub_key(pub)
-    else:
-        with salt.utils.files.fopen(pub, "wb+") as f:
-            f.write(gen.publickey().exportKey("PEM"))
+                enc = serialization.NoEncryption()
+                _format = serialization.PrivateFormat.TraditionalOpenSSL
+            pem = gen.private_bytes(
+                encoding=serialization.Encoding.PEM,
+                format=_format,
+                encryption_algorithm=enc,
+            )
+            f.write(pem)
+    pubkey = gen.public_key()
+    with salt.utils.files.fopen(pub, "wb+") as f:
+        pem = pubkey.public_bytes(
+            encoding=serialization.Encoding.PEM,
+            format=serialization.PublicFormat.SubjectPublicKeyInfo,
+        )
+        f.write(pem)
     os.chmod(priv, 0o400)
     if user:
         try:
             import pwd
             uid = pwd.getpwnam(user).pw_uid
             os.chown(priv, uid, -1)
             os.chown(pub, uid, -1)
         except (KeyError, ImportError, OSError):
             pass
     return priv
-class PrivateKey:
+class BaseKey:
+    @staticmethod
+    def parse_padding_for_signing(algorithm):
+        if algorithm not in VALID_SIGNING_ALGORITHMS:
+            raise UnsupportedAlgorithm(f"Invalid signing algorithm: {algorithm}")
+        _pad, _hash = algorithm.split("-", 1)
+        if _pad not in VALID_PADDING_FOR_SIGNING:
+            raise UnsupportedAlgorithm(f"Invalid padding algorithm: {_pad}")
+        return getattr(padding, _pad)
+    @staticmethod
+    def parse_padding_for_encryption(algorithm):
+        if algorithm not in VALID_ENCRYPTION_ALGORITHMS:
+            raise UnsupportedAlgorithm(f"Invalid encryption algorithm: {algorithm}")
+        _pad, _hash = algorithm.split("-", 1)
+        if _pad not in VALID_PADDING_FOR_ENCRYPTION:
+            raise UnsupportedAlgorithm(f"Invalid padding algorithm: {_pad}")
+        return getattr(padding, _pad)
+    @staticmethod
+    def parse_hash(algorithm):
+        if "-" not in algorithm:
+            raise UnsupportedAlgorithm(f"Invalid encryption algorithm: {algorithm}")
+        _pad, _hash = algorithm.split("-", 1)
+        if _hash not in VALID_HASHES:
+            raise Exception("Invalid hashing algorithm")
+        return getattr(hashes, _hash)
+class PrivateKey(BaseKey):
     def __init__(self, path, passphrase=None):
-        if HAS_M2:
-            self.key = RSA.load_key(path, lambda x: bytes(passphrase))
-        else:
-            with salt.utils.files.fopen(path) as f:
-                self.key = RSA.importKey(f.read(), passphrase)
+        self.key = get_rsa_key(path, passphrase)
     def encrypt(self, data):
-        if HAS_M2:
-            return self.key.private_encrypt(data, salt.utils.rsax931.RSA_X931_PADDING)
-        else:
-            return salt.utils.rsax931.RSAX931Signer(self.key.exportKey("PEM")).sign(
-                data
-            )
-    def sign(self, data):
-        if HAS_M2:
-            md = EVP.MessageDigest("sha1")
-            md.update(salt.utils.stringutils.to_bytes(data))
-            digest = md.final()
-            return self.key.sign(digest)
-        else:
-            signer = PKCS1_v1_5.new(self.key)
-            return signer.sign(SHA.new(salt.utils.stringutils.to_bytes(data)))
-class PublicKey:
-    def __init__(self, path, _HAS_M2=HAS_M2):
-        self._HAS_M2 = _HAS_M2
-        if self._HAS_M2:
-            with salt.utils.files.fopen(path, "rb") as f:
-                data = f.read().replace(b"RSA ", b"")
-            bio = BIO.MemoryBuffer(data)
+        pem = self.key.private_bytes(
+            encoding=serialization.Encoding.PEM,
+            format=serialization.PrivateFormat.TraditionalOpenSSL,
+            encryption_algorithm=serialization.NoEncryption(),
+        )
+        return salt.utils.rsax931.RSAX931Signer(pem).sign(data)
+    def sign(self, data, algorithm=PKCS1v15_SHA1):
+        _padding = self.parse_padding_for_signing(algorithm)
+        _hash = self.parse_hash(algorithm)
+        try:
+            return self.key.sign(
+                salt.utils.stringutils.to_bytes(data), _padding(), _hash()
+            )
+        except cryptography.exceptions.UnsupportedAlgorithm:
+            raise UnsupportedAlgorithm(f"Unsupported algorithm: {algorithm}")
+    def decrypt(self, data, algorithm=OAEP_SHA1):
+        _padding = self.parse_padding_for_encryption(algorithm)
+        _hash = self.parse_hash(algorithm)
+        try:
+            return self.key.decrypt(
+                data,
+                _padding(
+                    mgf=padding.MGF1(algorithm=_hash()),
+                    algorithm=_hash(),
+                    label=None,
+                ),
+            )
+        except cryptography.exceptions.UnsupportedAlgorithm:
+            raise UnsupportedAlgorithm(f"Unsupported algorithm: {algorithm}")
+class PublicKey(BaseKey):
+    def __init__(self, path):
+        with salt.utils.files.fopen(path, "rb") as fp:
             try:
-                self.key = RSA.load_pub_key_bio(bio)
-            except RSA.RSAError:
-                raise InvalidKeyError("Encountered bad RSA public key")
-        else:
-            with salt.utils.files.fopen(path) as f:
-                try:
-                    self.key = RSA.importKey(f.read())
-                except (ValueError, IndexError, TypeError):
-                    raise InvalidKeyError("Encountered bad RSA public key")
-    def encrypt(self, data):
+                self.key = serialization.load_pem_public_key(fp.read())
+            except ValueError as exc:
+                raise InvalidKeyError("Invalid key")
+    def encrypt(self, data, algorithm=OAEP_SHA1):
+        _padding = self.parse_padding_for_encryption(algorithm)
+        _hash = self.parse_hash(algorithm)
         bdata = salt.utils.stringutils.to_bytes(data)
-        if self._HAS_M2:
-            return self.key.public_encrypt(bdata, salt.crypt.RSA.pkcs1_oaep_padding)
-        else:
-            return salt.crypt.PKCS1_OAEP.new(self.key).encrypt(bdata)
-    def verify(self, data, signature):
-        if self._HAS_M2:
-            md = EVP.MessageDigest("sha1")
-            md.update(salt.utils.stringutils.to_bytes(data))
-            digest = md.final()
-            try:
-                return self.key.verify(digest, signature)
-            except RSA.RSAError as exc:
-                log.debug("Signature verification failed: %s", exc.args[0])
-                return False
-        else:
-            verifier = PKCS1_v1_5.new(self.key)
-            return verifier.verify(
-                SHA.new(salt.utils.stringutils.to_bytes(data)), signature
-            )
+        try:
+            return self.key.encrypt(
+                bdata,
+                _padding(
+                    mgf=padding.MGF1(algorithm=_hash()),
+                    algorithm=_hash(),
+                    label=None,
+                ),
+            )
+        except cryptography.exceptions.UnsupportedAlgorithm:
+            raise UnsupportedAlgorithm(f"Unsupported algorithm: {algorithm}")
+    def verify(self, data, signature, algorithm=PKCS1v15_SHA1):
+        _padding = self.parse_padding_for_signing(algorithm)
+        _hash = self.parse_hash(algorithm)
+        try:
+            self.key.verify(
+                salt.utils.stringutils.to_bytes(signature),
+                salt.utils.stringutils.to_bytes(data),
+                _padding(),
+                _hash(),
+            )
+        except cryptography.exceptions.InvalidSignature:
+            return False
+        return True
     def decrypt(self, data):
-        data = salt.utils.stringutils.to_bytes(data)
-        if HAS_M2:
-            return self.key.public_decrypt(data, salt.utils.rsax931.RSA_X931_PADDING)
-        else:
-            verifier = salt.utils.rsax931.RSAX931Verifier(self.key.exportKey("PEM"))
-            return verifier.verify(data)
+        pem = self.key.public_bytes(
+            encoding=serialization.Encoding.PEM,
+            format=serialization.PublicFormat.SubjectPublicKeyInfo,
+        )
+        verifier = salt.utils.rsax931.RSAX931Verifier(pem)
+        return verifier.verify(data)
 @salt.utils.decorators.memoize
 def _get_key_with_evict(path, timestamp, passphrase):
     """
     Load a private key from disk.  `timestamp` above is intended to be the
     timestamp of the file's last modification. This fn is memoized so if it is
     called with the same path and timestamp (the file's last modified time) the
     second time the result is returned from the memoization.  If the file gets
     modified then the params are different and the key is loaded from disk.
     """
     log.debug("salt.crypt._get_key_with_evict: Loading private key")
-    if HAS_M2:
-        key = RSA.load_key(path, lambda x: bytes(passphrase))
+    if passphrase:
+        password = passphrase.encode()
     else:
-        with salt.utils.files.fopen(path) as f:
-            key = RSA.importKey(f.read(), passphrase)
-    return key
+        password = None
+    with salt.utils.files.fopen(path, "rb") as f:
+        return serialization.load_pem_private_key(
+            f.read(),
+            password=password,
+        )
 def get_rsa_key(path, passphrase):
     """
     Read a private key off the disk.  Poor man's simple cache in effect here,
     we memoize the result of calling _get_rsa_with_evict.  This means the first
     time _get_key_with_evict is called with a path and a timestamp the result
     is cached.  If the file (the private key) does not change then its
     timestamp will not change and the next time the result is returned from the
     cache.  If the key DOES change the next time _get_rsa_with_evict is called
     it is called with different parameters and the fn is run fully to retrieve
     the key from disk.
     """
     log.debug("salt.crypt.get_rsa_key: Loading private key")
     return _get_key_with_evict(path, str(os.path.getmtime(path)), passphrase)
 def get_rsa_pub_key(path):
     """
     Read a public key off the disk.
     """
     log.debug("salt.crypt.get_rsa_pub_key: Loading public key")
-    if HAS_M2:
-        with salt.utils.files.fopen(path, "rb") as f:
-            data = f.read().replace(b"RSA ", b"")
-        bio = BIO.MemoryBuffer(data)
-        try:
-            key = RSA.load_pub_key_bio(bio)
-        except RSA.RSAError:
-            raise InvalidKeyError("Encountered bad RSA public key")
-    else:
-        with salt.utils.files.fopen(path) as f:
-            try:
-                key = RSA.importKey(f.read())
-            except (ValueError, IndexError, TypeError):
-                raise InvalidKeyError("Encountered bad RSA public key")
-    return key
-def sign_message(privkey_path, message, passphrase=None):
+    try:
+        with salt.utils.files.fopen(path, "rb") as fp:
+            return serialization.load_pem_public_key(fp.read())
+    except ValueError:
+        raise InvalidKeyError("Encountered bad RSA public key")
+    except cryptography.exceptions.UnsupportedAlgorithm:
+        raise InvalidKeyError("Unsupported key algorithm")
+def sign_message(privkey_path, message, passphrase=None, algorithm=PKCS1v15_SHA1):
     """
     Use Crypto.Signature.PKCS1_v1_5 to sign a message. Returns the signature.
     """
-    key = get_rsa_key(privkey_path, passphrase)
-    log.debug("salt.crypt.sign_message: Signing message.")
-    if HAS_M2:
-        md = EVP.MessageDigest("sha1")
-        md.update(salt.utils.stringutils.to_bytes(message))
-        digest = md.final()
-        return key.sign(digest)
-    else:
-        signer = PKCS1_v1_5.new(key)
-        return signer.sign(SHA.new(salt.utils.stringutils.to_bytes(message)))
-def verify_signature(pubkey_path, message, signature):
+    return PrivateKey(privkey_path, passphrase).sign(message, algorithm)
+def verify_signature(pubkey_path, message, signature, algorithm=PKCS1v15_SHA1):
     """
     Use Crypto.Signature.PKCS1_v1_5 to verify the signature on a message.
     Returns True for valid signature.
     """
     log.debug("salt.crypt.verify_signature: Loading public key")
-    pubkey = get_rsa_pub_key(pubkey_path)
-    log.debug("salt.crypt.verify_signature: Verifying signature")
-    if HAS_M2:
-        md = EVP.MessageDigest("sha1")
-        md.update(salt.utils.stringutils.to_bytes(message))
-        digest = md.final()
-        try:
-            return pubkey.verify(digest, signature)
-        except RSA.RSAError as exc:
-            log.debug("Signature verification failed: %s", exc.args[0])
-            return False
-    else:
-        verifier = PKCS1_v1_5.new(pubkey)
-        return verifier.verify(
-            SHA.new(salt.utils.stringutils.to_bytes(message)), signature
-        )
+    return PublicKey(pubkey_path).verify(message, signature, algorithm)
 def gen_signature(priv_path, pub_path, sign_path, passphrase=None):
     """
     creates a signature for the given public-key with
     the given private key and writes it to sign_path
     """
     with salt.utils.files.fopen(pub_path) as fp_:
         mpub_64 = fp_.read()
     mpub_sig = sign_message(priv_path, mpub_64, passphrase)
     mpub_sig_64 = binascii.b2a_base64(mpub_sig)
     if os.path.isfile(sign_path):
@@ -340,85 +341,41 @@
         log.trace("Wrote signature to %s", sign_path)
     return True
 def private_encrypt(key, message):
     """
     Generate an M2Crypto-compatible signature
     :param Crypto.PublicKey.RSA._RSAobj key: The RSA key object
     :param str message: The message to sign
     :rtype: str
     :return: The signature, or an empty string if the signature operation failed
     """
-    if HAS_M2:
-        return key.private_encrypt(message, salt.utils.rsax931.RSA_X931_PADDING)
-    else:
-        signer = salt.utils.rsax931.RSAX931Signer(key.exportKey("PEM"))
-        return signer.sign(message)
-def public_decrypt(pub, message):
-    """
-    Verify an M2Crypto-compatible signature
-    :param Crypto.PublicKey.RSA._RSAobj key: The RSA public key object
-    :param str message: The signed message to verify
-    :rtype: str
-    :return: The message (or digest) recovered from the signature, or an
-        empty string if the verification failed
-    """
-    if HAS_M2:
-        return pub.public_decrypt(message, salt.utils.rsax931.RSA_X931_PADDING)
-    else:
-        verifier = salt.utils.rsax931.RSAX931Verifier(pub.exportKey("PEM"))
-        return verifier.verify(message)
+    return key.encrypt(message)
 def pwdata_decrypt(rsa_key, pwdata):
-    if HAS_M2:
-        key = RSA.load_key_string(salt.utils.stringutils.to_bytes(rsa_key, "ascii"))
-        password = key.private_decrypt(pwdata, RSA.pkcs1_padding)
-    else:
-        dsize = SHA.digest_size
-        sentinel = Random.new().read(15 + dsize)
-        key_obj = RSA.importKey(rsa_key)
-        key_obj = PKCS1_v1_5_CIPHER.new(key_obj)
-        password = key_obj.decrypt(pwdata, sentinel)
+    key = serialization.load_pem_private_key(rsa_key.encode(), password=None)
+    password = key.decrypt(
+        pwdata,
+        padding.PKCS1v15(),
+    )
     return salt.utils.stringutils.to_unicode(password)
 class MasterKeys(dict):
     """
     The Master Keys class is used to manage the RSA public key pair used for
     authentication by the master.
     It also generates a signing key-pair if enabled with master_sign_key_name.
     """
     def __init__(self, opts):
         super().__init__()
         self.opts = opts
-        self.master_pub_path = os.path.join(self.opts["pki_dir"], "master.pub")
-        self.master_rsa_path = os.path.join(self.opts["pki_dir"], "master.pem")
+        self.pub_path = os.path.join(self.opts["pki_dir"], "master.pub")
+        self.rsa_path = os.path.join(self.opts["pki_dir"], "master.pem")
         key_pass = salt.utils.sdb.sdb_get(self.opts["key_pass"], self.opts)
-        self.master_key = self.__get_keys(passphrase=key_pass)
-        self.cluster_pub_path = None
-        self.cluster_rsa_path = None
-        self.cluster_key = None
-        if self.opts["cluster_id"]:
-            self.cluster_pub_path = os.path.join(
-                self.opts["cluster_pki_dir"], "cluster.pub"
-            )
-            self.cluster_rsa_path = os.path.join(
-                self.opts["cluster_pki_dir"], "cluster.pem"
-            )
-            self.cluster_shared_path = os.path.join(
-                self.opts["cluster_pki_dir"],
-                "peers",
-                f"{self.opts['id']}.pub",
-            )
-            self.check_master_shared_pub()
-            key_pass = salt.utils.sdb.sdb_get(self.opts["cluster_key_pass"], self.opts)
-            self.cluster_key = self.__get_keys(
-                name="cluster",
-                passphrase=key_pass,
-                pki_dir=self.opts["cluster_pki_dir"],
-            )
+        self.key = self.__get_keys(passphrase=key_pass)
         self.pub_signature = None
         if opts["master_sign_pubkey"]:
             if opts["master_use_pubkey_signature"]:
                 self.sig_path = os.path.join(
                     self.opts["pki_dir"], opts["master_pubkey_signature"]
                 )
                 if os.path.isfile(self.sig_path):
                     with salt.utils.files.fopen(self.sig_path) as fp_:
                         self.pub_signature = clean_key(fp_.read())
                     log.info(
@@ -446,134 +403,87 @@
                     self.opts["pki_dir"], opts["master_sign_key_name"] + ".pub"
                 )
                 self.rsa_sign_path = os.path.join(
                     self.opts["pki_dir"], opts["master_sign_key_name"] + ".pem"
                 )
                 self.sign_key = self.__get_keys(name=opts["master_sign_key_name"])
     def __setstate__(self, state):
         self.__init__(state["opts"])
     def __getstate__(self):
         return {"opts": self.opts}
-    @property
-    def key(self):
-        if self.cluster_key:
-            return self.cluster_key
-        return self.master_key
-    @property
-    def pub_path(self):
-        if self.cluster_pub_path:
-            return self.cluster_pub_path
-        return self.master_pub_path
-    @property
-    def rsa_path(self):
-        if self.cluster_rsa_path:
-            return self.cluster_rsa_path
-        return self.master_rsa_path
-    def __key_exists(self, name="master", passphrase=None, pki_dir=None):
-        if pki_dir is None:
-            pki_dir = self.opts["pki_dir"]
-        path = os.path.join(pki_dir, name + ".pem")
-        return os.path.exists(path)
-    def __get_keys(self, name="master", passphrase=None, pki_dir=None):
+    def __get_keys(self, name="master", passphrase=None):
         """
         Returns a key object for a key in the pki-dir
         """
-        if pki_dir is None:
-            pki_dir = self.opts["pki_dir"]
-        path = os.path.join(pki_dir, name + ".pem")
-        if not self.__key_exists(name, passphrase, pki_dir):
-            log.info("Generating %s keys: %s", name, pki_dir)
+        path = os.path.join(self.opts["pki_dir"], name + ".pem")
+        if not os.path.exists(path):
+            log.info("Generating %s keys: %s", name, self.opts["pki_dir"])
             gen_keys(
-                pki_dir,
+                self.opts["pki_dir"],
                 name,
                 self.opts["keysize"],
                 self.opts.get("user"),
                 passphrase,
             )
-        if HAS_M2:
-            key_error = RSA.RSAError
-        else:
-            key_error = ValueError
         try:
-            key = get_rsa_key(path, passphrase)
-        except key_error as e:
+            key = PrivateKey(path, passphrase)
+        except ValueError as e:
+            message = f"Unable to read key: {path}; file may be corrupt"
+        except TypeError as e:
             message = f"Unable to read key: {path}; passphrase may be incorrect"
-            log.error(message)
-            raise MasterExit(message)
-        log.debug("Loaded %s key: %s", name, path)
-        return key
+        except InvalidKeyError as e:
+            message = f"Unable to read key: {path}; key contains unsupported algorithm"
+        except cryptography.exceptions.UnsupportedAlgorithm as e:
+            message = f"Unable to read key: {path}; key contains unsupported algorithm"
+        else:
+            log.debug("Loaded %s key: %s", name, path)
+            return key
+        log.error(message)
+        raise MasterExit(message)
     def get_pub_str(self, name="master"):
         """
         Return the string representation of a public key
         in the pki-directory
         """
-        if self.cluster_pub_path:
-            path = self.cluster_pub_path
-        else:
-            path = self.master_pub_path
+        path = os.path.join(self.opts["pki_dir"], name + ".pub")
         if not os.path.isfile(path):
-            key = self.__get_keys()
-            if HAS_M2:
-                key.save_pub_key(path)
-            else:
-                with salt.utils.files.fopen(path, "wb+") as wfh:
-                    wfh.write(key.publickey().exportKey("PEM"))
+            pubkey = self.key.public_key()
+            with salt.utils.files.fopen(path, "wb+") as f:
+                f.write(
+                    pubkey.public_bytes(
+                        encoding=serialization.Encoding.PEM,
+                        format=serialization.PublicFormat.SubjectPublicKeyInfo,
+                    )
+                )
         with salt.utils.files.fopen(path) as rfh:
             return clean_key(rfh.read())
-    def get_ckey_paths(self):
-        return self.cluster_pub_path, self.cluster_rsa_path
     def get_mkey_paths(self):
         return self.pub_path, self.rsa_path
     def get_sign_paths(self):
         return self.pub_sign_path, self.rsa_sign_path
     def pubkey_signature(self):
         """
         returns the base64 encoded signature from the signature file
         or None if the master has its own signing keys
         """
         return self.pub_signature
-    def check_master_shared_pub(self):
-        """
-        Check the status of the master's shared public key.
-        If the shared master key does not exist, write this master's public key
-        to the shared location. Otherwise validate the shared key matches our
-        key. Failed validation raises MasterExit
-        """
-        shared_pub = pathlib.Path(self.cluster_shared_path)
-        master_pub = pathlib.Path(self.master_pub_path)
-        if shared_pub.exists():
-            if shared_pub.read_bytes() != master_pub.read_bytes():
-                message = (
-                    f"Shared key does not match, remove it to continue: {shared_pub}"
-                )
-                log.error(message)
-                raise MasterExit(message)
-        else:
-            log.debug("Writing shared key %s", shared_pub)
-            shared_pub.write_bytes(master_pub.read_bytes())
-    def master_private_decrypt(self, data):
-        if HAS_M2:
-            return self.master_key.private_decrypt(data, RSA.pkcs1_oaep_padding)
-        else:
-            cipher = PKCS1_OAEP.new(self.master_key)
-            return cipher.decrypt(data)
 class AsyncAuth:
     """
     Set up an Async object to maintain authentication with the salt master
     """
     instance_map = weakref.WeakKeyDictionary()
     creds_map = {}
     def __new__(cls, opts, io_loop=None):
         """
         Only create one instance of AsyncAuth per __key()
         """
-        io_loop = io_loop or tornado.ioloop.IOLoop.current()
+        io_loop = io_loop or salt.ext.tornado.ioloop.IOLoop.current()
         if io_loop not in AsyncAuth.instance_map:
             AsyncAuth.instance_map[io_loop] = weakref.WeakValueDictionary()
         loop_instance_map = AsyncAuth.instance_map[io_loop]
         key = cls.__key(opts)
         auth = loop_instance_map.get(key)
         if auth is None:
             log.debug("Initializing new AsyncAuth for %s", key)
             auth = object.__new__(cls)
             auth.__singleton_init__(opts, io_loop=io_loop)
             loop_instance_map[key] = auth
@@ -599,29 +509,30 @@
         self.opts = opts
         self.token = salt.utils.stringutils.to_bytes(Crypticle.generate_key_string())
         self.pub_path = os.path.join(self.opts["pki_dir"], "minion.pub")
         self.rsa_path = os.path.join(self.opts["pki_dir"], "minion.pem")
         if self.opts["__role"] == "syndic":
             self.mpub = "syndic_master.pub"
         else:
             self.mpub = "minion_master.pub"
         if not os.path.isfile(self.pub_path):
             self.get_keys()
-        self.io_loop = io_loop or tornado.ioloop.IOLoop.current()
-        salt.utils.crypt.reinit_crypto()
+        self.io_loop = io_loop or salt.ext.tornado.ioloop.IOLoop.current()
         key = self.__key(self.opts)
         if key in AsyncAuth.creds_map:
             creds = AsyncAuth.creds_map[key]
             self._creds = creds
             self._crypticle = Crypticle(self.opts, creds["aes"])
-            self._authenticate_future = tornado.concurrent.Future()
+            self._authenticate_future = salt.ext.tornado.concurrent.Future()
             self._authenticate_future.set_result(True)
+        else:
+            self.authenticate()
     def __deepcopy__(self, memo):
         cls = self.__class__
         result = cls.__new__(cls, copy.deepcopy(self.opts, memo))
         memo[id(self)] = result
         for key in self.__dict__:
             if key in ("io_loop",):
                 continue
             setattr(result, key, copy.deepcopy(self.__dict__[key], memo))
         return result
     @property
@@ -648,30 +559,30 @@
         Ask for this client to reconnect to the origin
         This function will de-dupe all calls here and return a *single* future
         for the sign-in-- whis way callers can all assume there aren't others
         """
         if (
             hasattr(self, "_authenticate_future")
             and not self._authenticate_future.done()
         ):
             future = self._authenticate_future
         else:
-            future = tornado.concurrent.Future()
+            future = salt.ext.tornado.concurrent.Future()
             self._authenticate_future = future
             self.io_loop.add_callback(self._authenticate)
         if callback is not None:
             def handle_future(future):
                 response = future.result()
                 self.io_loop.add_callback(callback, response)
             future.add_done_callback(handle_future)
         return future
-    @tornado.gen.coroutine
+    @salt.ext.tornado.gen.coroutine
     def _authenticate(self):
         """
         Authenticate with the master, this method breaks the functional
         paradigm, it will update the master information from a fresh sign
         in, signing in can occur as often as needed to keep up with the
         revolving master AES key.
         :rtype: Crypticle
         :returns: A crypticle used for encryption operations
         """
         acceptance_wait_time = self.opts["acceptance_wait_time"]
@@ -704,27 +615,39 @@
                         else:
                             print(
                                 "Minion failed to authenticate with the master, "
                                 "has the minion key been accepted?"
                             )
                             sys.exit(2)
                     if acceptance_wait_time:
                         log.info(
                             "Waiting %s seconds before retry.", acceptance_wait_time
                         )
-                        yield tornado.gen.sleep(acceptance_wait_time)
+                        yield salt.ext.tornado.gen.sleep(acceptance_wait_time)
                     if acceptance_wait_time < acceptance_wait_time_max:
                         acceptance_wait_time += acceptance_wait_time
                         log.debug(
                             "Authentication wait time is %s", acceptance_wait_time
                         )
                     continue
+                elif creds == "bad enc algo":
+                    log.error(
+                        "This minion is using a encryption algorithm that is "
+                        "not supported by it's Master. Please check your minion configutation."
+                    )
+                    break
+                elif creds == "bad sig algo":
+                    log.error(
+                        "This minion is using a signing algorithm that is "
+                        "not supported by it's Master. Please check your minion configutation."
+                    )
+                    break
                 break
             if not isinstance(creds, dict) or "aes" not in creds:
                 if self.opts.get("detect_mode") is True:
                     error = SaltClientError("-|RETRY|-")
                 try:
                     del AsyncAuth.creds_map[self.__key(self.opts)]
                 except KeyError:
                     pass
                 if not error:
                     error = SaltClientError(
@@ -747,21 +670,21 @@
                     True
                 )  # mark the sign-in as complete
                 if self.opts.get("auth_events") is True:
                     with salt.utils.event.get_event(
                         self.opts.get("__role"), opts=self.opts, listen=False
                     ) as event:
                         event.fire_event(
                             {"key": key, "creds": creds},
                             salt.utils.event.tagify(prefix="auth", suffix="creds"),
                         )
-    @tornado.gen.coroutine
+    @salt.ext.tornado.gen.coroutine
     def sign_in(self, timeout=60, safe=True, tries=1, channel=None):
         """
         Send a sign in request to the master, sets the key information and
         returns a dict containing the master publish interface to bind to
         and the decrypted aes key for transport decryption.
         :param int timeout: Number of seconds to wait before timing out the sign-in request
         :param bool safe: If True, do not raise an exception on timeout. Retry instead.
         :param int tries: The number of times to try to authenticate before giving up.
         :raises SaltReqTimeoutError: If the sign-in request has timed out and :param safe: is not set
         :return: Return a string on failure indicating the reason for failure. On success, return a dictionary
@@ -781,40 +704,47 @@
             close_channel = True
             channel = salt.channel.client.AsyncReqChannel.factory(
                 self.opts, crypt="clear", io_loop=self.io_loop
             )
         sign_in_payload = self.minion_sign_in_payload()
         try:
             payload = yield channel.send(sign_in_payload, tries=tries, timeout=timeout)
         except SaltReqTimeoutError as e:
             if safe:
                 log.warning("SaltReqTimeoutError: %s", e)
-                raise tornado.gen.Return("retry")
+                raise salt.ext.tornado.gen.Return("retry")
             if self.opts.get("detect_mode") is True:
-                raise tornado.gen.Return("retry")
+                raise salt.ext.tornado.gen.Return("retry")
             else:
                 raise SaltClientError(
                     "Attempt to authenticate with the salt master failed with timeout"
                     " error"
                 )
         finally:
             if close_channel:
                 channel.close()
         ret = self.handle_signin_response(sign_in_payload, payload)
-        raise tornado.gen.Return(ret)
+        raise salt.ext.tornado.gen.Return(ret)
     def handle_signin_response(self, sign_in_payload, payload):
         auth = {}
         m_pub_fn = os.path.join(self.opts["pki_dir"], self.mpub)
         auth["master_uri"] = self.opts["master_uri"]
         if not isinstance(payload, dict) or "load" not in payload:
             log.error("Sign-in attempt failed: %s", payload)
             return False
+        elif isinstance(payload["load"], dict) and "ret" in payload["load"]:
+            if payload["load"]["ret"] == "bad enc algo":
+                log.error("Sign-in attempt failed: %s", payload)
+                return "bad enc algo"
+            elif payload["load"]["ret"] == "bad sig algo":
+                log.error("Sign-in attempt failed: %s", payload)
+                return "bad sig algo"
         clear_signed_data = payload["load"]
         clear_signature = payload["sig"]
         payload = salt.payload.loads(clear_signed_data)
         if "pub_key" in payload:
             auth["aes"] = self.verify_master(
                 payload, master_pub="token" in sign_in_payload
             )
             if not auth["aes"]:
                 log.critical(
                     "The Salt Master server's public key did not authenticate!\n"
@@ -822,23 +752,26 @@
                     "lower than %s, or\n"
                     "If you are confident that you are connecting to a valid Salt "
                     "Master, then remove the master public key and restart the "
                     "Salt Minion.\nThe master public key can be found "
                     "at:\n%s",
                     salt.version.__version__,
                     m_pub_fn,
                 )
                 raise SaltClientError("Invalid master key")
         master_pubkey_path = os.path.join(self.opts["pki_dir"], self.mpub)
-        if os.path.exists(master_pubkey_path) and not PublicKey(
-            master_pubkey_path
-        ).verify(clear_signed_data, clear_signature):
+        if os.path.exists(master_pubkey_path) and not verify_signature(
+            master_pubkey_path,
+            clear_signed_data,
+            clear_signature,
+            algorithm=self.opts["signing_algorithm"],
+        ):
             log.critical("The payload signature did not validate.")
             raise SaltClientError("Invalid signature")
         if payload["nonce"] != sign_in_payload["nonce"]:
             log.critical("The payload nonce did not validate.")
             raise SaltClientError("Invalid nonce")
         if "ret" in payload:
             if not payload["ret"]:
                 if self.opts["rejected_retry"]:
                     log.error(
                         "The Salt Master has rejected this minion's public "
@@ -899,21 +832,21 @@
         user = self.opts.get("user", "root")
         salt.utils.verify.check_path_traversal(self.opts["pki_dir"], user)
         if not os.path.exists(self.rsa_path):
             log.info("Generating keys: %s", self.opts["pki_dir"])
             gen_keys(
                 self.opts["pki_dir"],
                 "minion",
                 self.opts["keysize"],
                 self.opts.get("user"),
             )
-        key = get_rsa_key(self.rsa_path, None)
+        key = PrivateKey(self.rsa_path, None)
         log.debug("Loaded minion key: %s", self.rsa_path)
         return key
     def gen_token(self, clear_tok):
         """
         Encrypt a string with the minion private key to verify identity
         with the master.
         :param str clear_tok: A plaintext token to encrypt
         :return: Encrypted token
         :rtype: str
         """
@@ -923,37 +856,37 @@
         Generates the payload used to authenticate with the master
         server. This payload consists of the passed in id_ and the ssh
         public key to encrypt the AES key sent back from the master.
         :return: Payload dictionary
         :rtype: dict
         """
         payload = {}
         payload["cmd"] = "_auth"
         payload["id"] = self.opts["id"]
         payload["nonce"] = uuid.uuid4().hex
+        payload["enc_algo"] = self.opts["encryption_algorithm"]
+        payload["sig_algo"] = self.opts["signing_algorithm"]
         if "autosign_grains" in self.opts:
             autosign_grains = {}
             for grain in self.opts["autosign_grains"]:
                 autosign_grains[grain] = self.opts["grains"].get(grain, None)
             payload["autosign_grains"] = autosign_grains
         try:
             pubkey_path = os.path.join(self.opts["pki_dir"], self.mpub)
-            pub = get_rsa_pub_key(pubkey_path)
-            if HAS_M2:
-                payload["token"] = pub.public_encrypt(
-                    self.token, RSA.pkcs1_oaep_padding
-                )
-            else:
-                cipher = PKCS1_OAEP.new(pub)
-                payload["token"] = cipher.encrypt(self.token)
-        except Exception:  # pylint: disable=broad-except
-            pass
+            pub = PublicKey(pubkey_path)
+            payload["token"] = pub.encrypt(
+                self.token, self.opts["encryption_algorithm"]
+            )
+        except FileNotFoundError:
+            log.debug("Master public key not found")
+        except Exception as exc:  # pylint: disable=broad-except
+            log.debug("Exception while encrypting token %s", exc)
         with salt.utils.files.fopen(self.pub_path) as f:
             payload["pub"] = clean_key(f.read())
         return payload
     def decrypt_aes(self, payload, master_pub=True):
         """
         This function is used to decrypt the AES seed phrase returned from
         the master server. The seed phrase is decrypted with the SSH RSA
         host key.
         Pass in the encrypted AES key.
         Returns the decrypted AES seed key, a string
@@ -967,70 +900,63 @@
         :rtype: str
         :return: The decrypted token that was provided, with padding.
         :rtype: str
         :return: The decrypted AES seed key
         """
         if self.opts.get("auth_trb", False):
             log.warning("Auth Called: %s", "".join(traceback.format_stack()))
         else:
             log.debug("Decrypting the current master AES key")
         key = self.get_keys()
-        if HAS_M2:
-            key_str = key.private_decrypt(payload["aes"], RSA.pkcs1_oaep_padding)
-        else:
-            cipher = PKCS1_OAEP.new(key)
-            key_str = cipher.decrypt(payload["aes"])
+        key_str = key.decrypt(payload["aes"], self.opts["encryption_algorithm"])
         if "sig" in payload:
             m_path = os.path.join(self.opts["pki_dir"], self.mpub)
             if os.path.exists(m_path):
                 try:
-                    mkey = get_rsa_pub_key(m_path)
+                    mkey = PublicKey(m_path)
                 except Exception:  # pylint: disable=broad-except
                     return "", ""
                 digest = hashlib.sha256(key_str).hexdigest()
                 digest = salt.utils.stringutils.to_bytes(digest)
-                if HAS_M2:
-                    m_digest = public_decrypt(mkey, payload["sig"])
-                else:
-                    m_digest = public_decrypt(mkey.publickey(), payload["sig"])
+                m_digest = mkey.decrypt(payload["sig"])
                 if m_digest != digest:
                     return "", ""
         else:
             return "", ""
         key_str = salt.utils.stringutils.to_str(key_str)
         if "_|-" in key_str:
             return key_str.split("_|-")
         else:
             if "token" in payload:
-                if HAS_M2:
-                    token = key.private_decrypt(
-                        payload["token"], RSA.pkcs1_oaep_padding
-                    )
-                else:
-                    token = cipher.decrypt(payload["token"])
+                token = key.decrypt(payload["token"], self.opts["encryption_algorithm"])
                 return key_str, token
             elif not master_pub:
                 return key_str, ""
         return "", ""
     def verify_pubkey_sig(self, message, sig):
         """
         Wraps the verify_signature method so we have
         additional checks.
         :rtype: bool
         :return: Success or failure of public key verification
         """
         if self.opts["master_sign_key_name"]:
             path = os.path.join(
                 self.opts["pki_dir"], self.opts["master_sign_key_name"] + ".pub"
             )
             if os.path.isfile(path):
-                res = verify_signature(path, message, binascii.a2b_base64(sig))
+                res = verify_signature(
+                    path,
+                    message,
+                    binascii.a2b_base64(sig),
+                    algorithm=self.opts["signing_algorithm"],
+                )
             else:
                 log.error(
                     "Verification public key %s does not exist. You need to "
                     "copy it from the master to the minions pki directory",
                     os.path.basename(path),
                 )
                 return False
             if res:
                 log.debug(
                     "Successfully verified signature of master public key "
@@ -1363,63 +1289,42 @@
     """
     PICKLE_PAD = b"pickle::"
     AES_BLOCK_SIZE = 16
     SIG_SIZE = hashlib.sha256().digest_size
     def __init__(self, opts, key_string, key_size=192, serial=0):
         self.key_string = key_string
         self.keys = self.extract_keys(self.key_string, key_size)
         self.key_size = key_size
         self.serial = serial
     @classmethod
-    def generate_key_string(cls, key_size=192, **kwargs):
+    def generate_key_string(cls, key_size=192):
         key = os.urandom(key_size // 8 + cls.SIG_SIZE)
         b64key = base64.b64encode(key)
         b64key = b64key.decode("utf-8")
         return b64key.replace("\n", "")
-    @classmethod
-    def write_key(cls, path, key_size=192):
-        directory = pathlib.Path(path).parent
-        with salt.utils.files.set_umask(0o177):
-            fd, tmp = tempfile.mkstemp(dir=directory, prefix="aes")
-            os.close(fd)
-            with salt.utils.files.fopen(tmp, "w") as fp:
-                fp.write(cls.generate_key_string(key_size))
-            os.rename(tmp, path)
-    @classmethod
-    def read_key(cls, path):
-        try:
-            with salt.utils.files.fopen(path, "r") as fp:
-                return fp.read()
-        except FileNotFoundError:
-            pass
     @classmethod
     def extract_keys(cls, key_string, key_size):
         key = salt.utils.stringutils.to_bytes(base64.b64decode(key_string))
         assert len(key) == key_size / 8 + cls.SIG_SIZE, "invalid key"
         return key[: -cls.SIG_SIZE], key[-cls.SIG_SIZE :]
     def encrypt(self, data):
         """
         encrypt data with AES-CBC and sign it with HMAC-SHA256
         """
         aes_key, hmac_key = self.keys
         pad = self.AES_BLOCK_SIZE - len(data) % self.AES_BLOCK_SIZE
         data = data + salt.utils.stringutils.to_bytes(pad * chr(pad))
         iv_bytes = os.urandom(self.AES_BLOCK_SIZE)
-        if HAS_M2:
-            cypher = EVP.Cipher(
-                alg="aes_192_cbc", key=aes_key, iv=iv_bytes, op=1, padding=False
-            )
-            encr = cypher.update(data)
-            encr += cypher.final()
-        else:
-            cypher = AES.new(aes_key, AES.MODE_CBC, iv_bytes)
-            encr = cypher.encrypt(data)
+        cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv_bytes))
+        encryptor = cipher.encryptor()
+        encr = encryptor.update(data)
+        encr += encryptor.finalize()
         data = iv_bytes + encr
         sig = hmac.new(hmac_key, data, hashlib.sha256).digest()
         return data + sig
     def decrypt(self, data):
         """
         verify HMAC-SHA256 signature and decrypt data with AES-CBC
         """
         aes_key, hmac_key = self.keys
         sig = data[-self.SIG_SIZE :]
         data = data[: -self.SIG_SIZE]
@@ -1430,29 +1335,23 @@
             log.debug("Failed to authenticate message")
             raise AuthenticationError("message authentication failed")
         result = 0
         for zipped_x, zipped_y in zip(mac_bytes, sig):
             result |= zipped_x ^ zipped_y
         if result != 0:
             log.debug("Failed to authenticate message")
             raise AuthenticationError("message authentication failed")
         iv_bytes = data[: self.AES_BLOCK_SIZE]
         data = data[self.AES_BLOCK_SIZE :]
-        if HAS_M2:
-            cypher = EVP.Cipher(
-                alg="aes_192_cbc", key=aes_key, iv=iv_bytes, op=0, padding=False
-            )
-            encr = cypher.update(data)
-            data = encr + cypher.final()
-        else:
-            cypher = AES.new(aes_key, AES.MODE_CBC, iv_bytes)
-            data = cypher.decrypt(data)
+        cipher = Cipher(algorithms.AES(aes_key), modes.CBC(iv_bytes))
+        decryptor = cipher.decryptor()
+        data = decryptor.update(data) + decryptor.finalize()
         return data[: -data[-1]]
     def dumps(self, obj, nonce=None):
         """
         Serialize and encrypt a python object
         """
         if nonce:
             toencrypt = self.PICKLE_PAD + nonce.encode() + salt.payload.dumps(obj)
         else:
             toencrypt = self.PICKLE_PAD + salt.payload.dumps(obj)
         return self.encrypt(toencrypt)
@@ -1461,21 +1360,21 @@
         Decrypt and un-serialize a python object
         """
         data = self.decrypt(data)
         if not data.startswith(self.PICKLE_PAD):
             return {}
         data = data[len(self.PICKLE_PAD) :]
         if nonce:
             ret_nonce = data[:32].decode()
             data = data[32:]
             if ret_nonce != nonce:
-                raise SaltClientError(f"Nonce verification error {ret_nonce} {nonce}")
+                raise SaltClientError("Nonce verification error")
         payload = salt.payload.loads(data, raw=raw)
         if isinstance(payload, dict):
             if "serial" in payload:
                 serial = payload.pop("serial")
                 if serial <= self.serial:
                     log.critical(
                         "A message with an invalid serial was received.\n"
                         "this serial: %d\n"
                         "last serial: %d\n"
                         "The minion will not honor this request.",

--- a/salt/daemons/masterapi.py
+++ b/salt/daemons/masterapi.py
@@ -106,21 +106,21 @@
     for tok in loadauth.list_tokens():
         token_data = loadauth.get_tok(tok)
         if "expire" not in token_data or token_data.get("expire", 0) < time.time():
             loadauth.rm_token(tok)
 def clean_pub_auth(opts):
     try:
         auth_cache = os.path.join(opts["cachedir"], "publish_auth")
         if not os.path.exists(auth_cache):
             return
         else:
-            for (dirpath, dirnames, filenames) in salt.utils.path.os_walk(auth_cache):
+            for dirpath, dirnames, filenames in salt.utils.path.os_walk(auth_cache):
                 for auth_file in filenames:
                     auth_file_path = os.path.join(dirpath, auth_file)
                     if not os.path.isfile(auth_file_path):
                         continue
                     if time.time() - os.path.getmtime(auth_file_path) > (
                         salt.utils.job.get_keep_jobs_seconds(opts)
                     ):
                         os.remove(auth_file_path)
     except OSError:
         log.error("Unable to delete pub auth file")
@@ -172,40 +172,31 @@
     clients are required to run as root.
     """
     keys = {}
     publisher_acl = opts["publisher_acl"]
     acl_users = set(publisher_acl.keys())
     if opts.get("user"):
         acl_users.add(opts["user"])
     acl_users.add(salt.utils.user.get_user())
     for user in acl_users:
         log.info("Preparing the %s key for local communication", user)
-        keyfile = os.path.join(opts["cachedir"], f".{user}_key")
-        if os.path.exists(keyfile):
-            with salt.utils.files.fopen(keyfile, "r") as fp:
-                key = salt.utils.stringutils.to_unicode(fp.read())
-        else:
-            key = mk_key(opts, user)
+        key = mk_key(opts, user)
         if key is not None:
             keys[user] = key
     if opts["client_acl_verify"] and HAS_PWD:
         log.profile("Beginning pwd.getpwall() call in masterapi access_keys function")
         for user in pwd.getpwall():
             user = user.pw_name
             if user not in keys and salt.utils.stringutils.check_whitelist_blacklist(
                 user, whitelist=acl_users
             ):
-                if os.path.exists(keyfile):
-                    with salt.utils.files.fopen(keyfile, "r") as fp:
-                        keys[user] = salt.utils.stringutils.to_unicode(fp.read())
-                else:
-                    keys[user] = mk_key(opts, user)
+                keys[user] = mk_key(opts, user)
         log.profile("End pwd.getpwall() call in masterapi access_keys function")
     return keys
 def fileserver_update(fileserver):
     """
     Update the fileserver backends, requires that a salt.fileserver.Fileserver
     object be passed in
     """
     try:
         if not fileserver.servers:
             log.error(
@@ -262,25 +253,21 @@
                     if not entry.strip().startswith("#")
                 ]
         return any(
             salt.utils.stringutils.expr_match(keyid, line)
             for line in self.signing_files[signing_file].get("data", [])
         )
     def check_autosign_dir(self, keyid):
         """
         Check a keyid for membership in a autosign directory.
         """
-        if self.opts["cluster_id"]:
-            pki_dir = self.opts["cluster_pki_dir"]
-        else:
-            pki_dir = self.opts["pki_dir"]
-        autosign_dir = os.path.join(pki_dir, "minions_autosign")
+        autosign_dir = os.path.join(self.opts["pki_dir"], "minions_autosign")
         expire_minutes = self.opts.get("autosign_timeout", 120)
         if expire_minutes > 0:
             min_time = time.time() - (60 * int(expire_minutes))
             for root, dirs, filenames in salt.utils.path.os_walk(autosign_dir):
                 for f in filenames:
                     stub_file = os.path.join(autosign_dir, f)
                     mtime = os.path.getmtime(stub_file)
                     if mtime < min_time:
                         log.warning("Autosign keyid expired %s", stub_file)
                         os.remove(stub_file)
@@ -303,21 +290,21 @@
                     if not self.check_permissions(grain_file):
                         log.warning(
                             "Wrong permissions for %s, ignoring content", grain_file
                         )
                         continue
                     with salt.utils.files.fopen(grain_file, "r") as f:
                         for line in f:
                             line = salt.utils.stringutils.to_unicode(line).strip()
                             if line.startswith("#"):
                                 continue
-                            if str(autosign_grains[grain]) == line:
+                            if autosign_grains[grain] == line:
                                 return True
         return False
     def check_autoreject(self, keyid):
         """
         Checks if the specified keyid should automatically be rejected.
         """
         return self.check_signing_file(keyid, self.opts.get("autoreject_file", None))
     def check_autosign(self, keyid, autosign_grains=None):
         """
         Checks if the specified keyid should automatically be signed.
@@ -616,21 +603,21 @@
         if ":" in normpath:
             normpath = normpath.replace("\\", "/")
             normpath = os.path.normpath(normpath)
         cpath = os.path.join(
             self.opts["cachedir"], "minions", load["id"], "files", normpath
         )
         cdir = os.path.dirname(cpath)
         if not os.path.isdir(cdir):
             try:
                 os.makedirs(cdir)
-            except os.error:
+            except OSError:
                 pass
         if os.path.isfile(cpath) and load["loc"] != 0:
             mode = "ab"
         else:
             mode = "wb"
         with salt.utils.files.fopen(cpath, mode) as fp_:
             if load["loc"]:
                 fp_.seek(load["loc"])
             fp_.write(salt.utils.stringutils.to_str(load["data"]))
         return True

--- a/salt/engines/docker_events.py
+++ b/salt/engines/docker_events.py
@@ -8,25 +8,20 @@
 import salt.utils.json
 try:
     import docker  # pylint: disable=import-error,no-name-in-module
     import docker.utils  # pylint: disable=import-error,no-name-in-module
     HAS_DOCKER_PY = True
 except ImportError:
     HAS_DOCKER_PY = False
 log = logging.getLogger(__name__)  # pylint: disable=invalid-name
 CLIENT_TIMEOUT = 60
 __virtualname__ = "docker_events"
-__deprecated__ = (
-    3009,
-    "docker",
-    "https://github.com/saltstack/saltext-docker",
-)
 def __virtual__():
     """
     Only load if docker libs are present
     """
     if not HAS_DOCKER_PY:
         return (False, "Docker_events engine could not be imported")
     return True
 def start(
     docker_url="unix://var/run/docker.sock",
     timeout=CLIENT_TIMEOUT,

--- a/salt/engines/ircbot.py
+++ b/salt/engines/ircbot.py
@@ -38,22 +38,22 @@
     08:34:17   gtmanbot > gtmanfred: TaDa!
 .. code-block:: text
     [DEBUG   ] Sending event: tag = salt/engines/ircbot/test/tag/ircbot; data = {'_stamp': '2016-11-28T14:34:16.633623', 'data': ['irc', 'is', 'useful']}
 """
 import base64
 import logging
 import re
 import socket
 import ssl
 from collections import namedtuple
-import tornado.ioloop
-import tornado.iostream
+import salt.ext.tornado.ioloop
+import salt.ext.tornado.iostream
 import salt.utils.event
 log = logging.getLogger(__name__)
 Event = namedtuple("Event", "source code line")
 PrivEvent = namedtuple("PrivEvent", "source nick user host code channel command line")
 class IRCClient:
     def __init__(
         self,
         nick,
         host,
         port=6667,
@@ -72,30 +72,31 @@
         self.port = port
         self.username = username or nick
         self.password = password
         self.channels = channels or []
         self.ssl = use_ssl
         self.sasl = use_sasl
         self.char = char
         self.allow_hosts = allow_hosts
         self.allow_nicks = allow_nicks
         self.disable_query = disable_query
-        self.io_loop = tornado.ioloop.IOLoop()
+        self.io_loop = salt.ext.tornado.ioloop.IOLoop(make_current=False)
+        self.io_loop.make_current()
         self._connect()
     def _connect(self):
         _sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)
         if self.ssl is True:
-            self._stream = tornado.iostream.SSLIOStream(
+            self._stream = salt.ext.tornado.iostream.SSLIOStream(
                 _sock, ssl_options={"cert_reqs": ssl.CERT_NONE}
             )
         else:
-            self._stream = tornado.iostream.IOStream(_sock)
+            self._stream = salt.ext.tornado.iostream.IOStream(_sock)
         self._stream.set_close_callback(self.on_closed)
         self._stream.connect((self.host, self.port), self.on_connect)
     def read_messages(self):
         self._stream.read_until("\r\n", self._message)
     @staticmethod
     def _event(line):
         log.debug("Received: %s", line)
         search = re.match(
             "^(?:(?P<source>:[^ ]+) )?(?P<code>[^ ]+)(?: (?P<line>.*))?$", line
         )
@@ -137,28 +138,28 @@
                 search.group("line"),
             )
             if self.disable_query is True and not channel.startswith("#"):
                 return
             if channel == self.nick:
                 channel = nick
             privevent = PrivEvent(
                 event.source, nick, user, host, event.code, channel, command, line
             )
             if (self._allow_nick(nick) or self._allow_host(host)) and hasattr(
-                self, "_command_{}".format(command)
+                self, f"_command_{command}"
             ):
-                getattr(self, "_command_{}".format(command))(privevent)
+                getattr(self, f"_command_{command}")(privevent)
     def _command_echo(self, event):
-        message = "PRIVMSG {} :{}".format(event.channel, event.line)
+        message = f"PRIVMSG {event.channel} :{event.line}"
         self.send_message(message)
     def _command_ping(self, event):
-        message = "PRIVMSG {} :{}: pong".format(event.channel, event.nick)
+        message = f"PRIVMSG {event.channel} :{event.nick}: pong"
         self.send_message(message)
     def _command_event(self, event):
         if __opts__.get("__role") == "master":
             fire_master = salt.utils.event.get_master_event(
                 __opts__, __opts__["sock_dir"]
             ).fire_event
         else:
             fire_master = None
         def fire(tag, msg):
             """
@@ -168,49 +169,51 @@
                 fire_master(msg, tag)
             else:
                 __salt__["event.send"](tag, msg)
         args = event.line.split(" ")
         tag = args[0]
         if len(args) > 1:
             payload = {"data": args[1:]}
         else:
             payload = {"data": []}
         fire("salt/engines/ircbot/" + tag, payload)
-        message = "PRIVMSG {} :{}: TaDa!".format(event.channel, event.nick)
+        message = f"PRIVMSG {event.channel} :{event.nick}: TaDa!"
         self.send_message(message)
     def _message(self, raw):
         raw = raw.rstrip(b"\r\n").decode("utf-8")
         event = self._event(raw)
         if event.code == "PING":
-            tornado.ioloop.IOLoop.current().spawn_callback(
-                self.send_message, "PONG {}".format(event.line)
+            salt.ext.tornado.ioloop.IOLoop.current().spawn_callback(
+                self.send_message, f"PONG {event.line}"
             )
         elif event.code == "PRIVMSG":
-            tornado.ioloop.IOLoop.current().spawn_callback(self._privmsg, event)
+            salt.ext.tornado.ioloop.IOLoop.current().spawn_callback(
+                self._privmsg, event
+            )
         self.read_messages()
     def join_channel(self, channel):
         if not channel.startswith("#"):
             channel = "#" + channel
-        self.send_message("JOIN {}".format(channel))
+        self.send_message(f"JOIN {channel}")
     def on_connect(self):
         logging.info("on_connect")
         if self.sasl is True:
             self.send_message("CAP REQ :sasl")
-        self.send_message("NICK {}".format(self.nick))
+        self.send_message(f"NICK {self.nick}")
         self.send_message("USER saltstack 0 * :saltstack")
         if self.password:
             if self.sasl is True:
                 authstring = base64.b64encode(
                     "{0}\x00{0}\x00{1}".format(self.username, self.password).encode()
                 )
                 self.send_message("AUTHENTICATE PLAIN")
-                self.send_message("AUTHENTICATE {}".format(authstring))
+                self.send_message(f"AUTHENTICATE {authstring}")
                 self.send_message("CAP END")
             else:
                 self.send_message(
                     "PRIVMSG NickServ :IDENTIFY {} {}".format(
                         self.username, self.password
                     )
                 )
         for channel in self.channels:
             self.join_channel(channel)
         self.read_messages()

--- a/salt/engines/slack.py
+++ b/salt/engines/slack.py
@@ -652,23 +652,23 @@
                     log.trace("msg is done")
                     break
                 if fire_all:
                     log.debug("Firing message to the bus with tag: %s", tag)
                     log.debug("%s %s", tag, msg)
                     self.fire("{}/{}".format(tag, msg["message_data"].get("type")), msg)
                 if control and (len(msg) > 1) and msg.get("cmdline"):
                     channel = self.sc.server.channels.find(msg["channel"])
                     jid = self.run_command_async(msg)
                     log.debug("Submitted a job and got jid: %s", jid)
-                    outstanding[
-                        jid
-                    ] = msg  # record so we can return messages to the caller
+                    outstanding[jid] = (
+                        msg  # record so we can return messages to the caller
+                    )
                     channel.send_message(
                         "@{}'s job is submitted as salt jid {}".format(
                             msg["user_name"], jid
                         )
                     )
                 count += 1
             start_time = time.time()
             job_status = self.get_jobs_from_runner(
                 outstanding.keys()
             )  # dict of job_ids:results are returned
@@ -756,21 +756,21 @@
     trigger="!",
     groups=None,
     groups_pillar_name=None,
     fire_all=False,
     tag="salt/engines/slack",
 ):
     """
     Listen to slack events and forward them to salt, new version
     """
     salt.utils.versions.warn_until(
-        3008,
+        "Argon",
         "This 'slack' engine will be deprecated and "
         "will be replace by the slack_bolt engine. This new "
         "engine will use the new Bolt library from Slack and requires "
         "a Slack app and a Slack bot account.",
     )
     if (not token) or (not token.startswith("xoxb")):
         time.sleep(2)  # don't respawn too quickly
         log.error("Slack bot token not found, bailing...")
         raise UserWarning("Slack Engine bot token not configured")
     try:

--- a/salt/engines/webhook.py
+++ b/salt/engines/webhook.py
@@ -1,16 +1,16 @@
 """
 Send events from webhook api
 """
-import tornado.httpserver
-import tornado.ioloop
-import tornado.web
+import salt.ext.tornado.httpserver
+import salt.ext.tornado.ioloop
+import salt.ext.tornado.web
 import salt.utils.event
 def start(address=None, port=5000, ssl_crt=None, ssl_key=None):
     """
     Api to listen for webhooks to send to the reactor.
     Implement the webhook behavior in an engine.
     :py:class:`rest_cherrypy Webhook docs <salt.netapi.rest_cherrypy.app.Webhook>`
     Unlike the rest_cherrypy Webhook, this is only an unauthenticated webhook
     endpoint.  If an authenticated webhook endpoint is needed, use the salt-api
     webhook which runs on the master and authenticates through eauth.
     .. note: This is really meant to be used on the minion, because salt-api
@@ -40,27 +40,32 @@
     else:
         fire_master = None
     def fire(tag, msg):
         """
         How to fire the event
         """
         if fire_master:
             fire_master(msg, tag)
         else:
             __salt__["event.send"](tag, msg)
-    class WebHook(tornado.web.RequestHandler):  # pylint: disable=abstract-method
+    class WebHook(
+        salt.ext.tornado.web.RequestHandler
+    ):  # pylint: disable=abstract-method
         def post(self, tag):  # pylint: disable=arguments-differ
             body = self.request.body
             headers = self.request.headers
             payload = {
                 "headers": headers if isinstance(headers, dict) else dict(headers),
                 "body": body,
             }
             fire("salt/engines/hook/" + tag, payload)
-    application = tornado.web.Application([(r"/(.*)", WebHook)])
+    application = salt.ext.tornado.web.Application([(r"/(.*)", WebHook)])
     ssl_options = None
     if all([ssl_crt, ssl_key]):
         ssl_options = {"certfile": ssl_crt, "keyfile": ssl_key}
-    io_loop = tornado.ioloop.IOLoop()
-    http_server = tornado.httpserver.HTTPServer(application, ssl_options=ssl_options)
+    io_loop = salt.ext.tornado.ioloop.IOLoop(make_current=False)
+    io_loop.make_current()
+    http_server = salt.ext.tornado.httpserver.HTTPServer(
+        application, ssl_options=ssl_options
+    )
     http_server.listen(port, address=address)
     io_loop.start()

--- a/salt/executors/docker.py
+++ b/salt/executors/docker.py
@@ -2,25 +2,20 @@
 Docker executor module
 .. versionadded:: 2019.2.0
 Used with the docker proxy minion.
 """
 __virtualname__ = "docker"
 DOCKER_MOD_MAP = {
     "state.sls": "docker.sls",
     "state.apply": "docker.apply",
     "state.highstate": "docker.highstate",
 }
-__deprecated__ = (
-    3009,
-    "docker",
-    "https://github.com/saltstack/saltext-docker",
-)
 def __virtual__():
     if "proxy" not in __opts__:
         return (
             False,
             "Docker executor is only meant to be used with Docker Proxy Minions",
         )
     if __opts__.get("proxy", {}).get("proxytype") != __virtualname__:
         return False, f"Proxytype does not match: {__virtualname__}"
     return True
 def execute(opts, data, func, args, kwargs):

--- a//dev/null
+++ b/salt/ext/tornado/__init__.py
@@ -0,0 +1,4 @@
+"""The Tornado web server and tools."""
+from __future__ import absolute_import, division, print_function
+version = "4.5.3"
+version_info = (4, 5, 3, 0)

--- a//dev/null
+++ b/salt/ext/tornado/_locale_data.py
@@ -0,0 +1,66 @@
+"""Data used by the tornado.locale module."""
+from __future__ import absolute_import, division, print_function
+LOCALE_NAMES = {
+    "af_ZA": {"name_en": u"Afrikaans", "name": u"Afrikaans"},
+    "am_ET": {"name_en": u"Amharic", "name": u""},
+    "ar_AR": {"name_en": u"Arabic", "name": u""},
+    "bg_BG": {"name_en": u"Bulgarian", "name": u""},
+    "bn_IN": {"name_en": u"Bengali", "name": u""},
+    "bs_BA": {"name_en": u"Bosnian", "name": u"Bosanski"},
+    "ca_ES": {"name_en": u"Catalan", "name": u"Catal"},
+    "cs_CZ": {"name_en": u"Czech", "name": u"etina"},
+    "cy_GB": {"name_en": u"Welsh", "name": u"Cymraeg"},
+    "da_DK": {"name_en": u"Danish", "name": u"Dansk"},
+    "de_DE": {"name_en": u"German", "name": u"Deutsch"},
+    "el_GR": {"name_en": u"Greek", "name": u""},
+    "en_GB": {"name_en": u"English (UK)", "name": u"English (UK)"},
+    "en_US": {"name_en": u"English (US)", "name": u"English (US)"},
+    "es_ES": {"name_en": u"Spanish (Spain)", "name": u"Espaol (Espaa)"},
+    "es_LA": {"name_en": u"Spanish", "name": u"Espaol"},
+    "et_EE": {"name_en": u"Estonian", "name": u"Eesti"},
+    "eu_ES": {"name_en": u"Basque", "name": u"Euskara"},
+    "fa_IR": {"name_en": u"Persian", "name": u""},
+    "fi_FI": {"name_en": u"Finnish", "name": u"Suomi"},
+    "fr_CA": {"name_en": u"French (Canada)", "name": u"Franais (Canada)"},
+    "fr_FR": {"name_en": u"French", "name": u"Franais"},
+    "ga_IE": {"name_en": u"Irish", "name": u"Gaeilge"},
+    "gl_ES": {"name_en": u"Galician", "name": u"Galego"},
+    "he_IL": {"name_en": u"Hebrew", "name": u""},
+    "hi_IN": {"name_en": u"Hindi", "name": u""},
+    "hr_HR": {"name_en": u"Croatian", "name": u"Hrvatski"},
+    "hu_HU": {"name_en": u"Hungarian", "name": u"Magyar"},
+    "id_ID": {"name_en": u"Indonesian", "name": u"Bahasa Indonesia"},
+    "is_IS": {"name_en": u"Icelandic", "name": u"slenska"},
+    "it_IT": {"name_en": u"Italian", "name": u"Italiano"},
+    "ja_JP": {"name_en": u"Japanese", "name": u""},
+    "ko_KR": {"name_en": u"Korean", "name": u""},
+    "lt_LT": {"name_en": u"Lithuanian", "name": u"Lietuvi"},
+    "lv_LV": {"name_en": u"Latvian", "name": u"Latvieu"},
+    "mk_MK": {"name_en": u"Macedonian", "name": u""},
+    "ml_IN": {"name_en": u"Malayalam", "name": u""},
+    "ms_MY": {"name_en": u"Malay", "name": u"Bahasa Melayu"},
+    "nb_NO": {"name_en": u"Norwegian (bokmal)", "name": u"Norsk (bokml)"},
+    "nl_NL": {"name_en": u"Dutch", "name": u"Nederlands"},
+    "nn_NO": {"name_en": u"Norwegian (nynorsk)", "name": u"Norsk (nynorsk)"},
+    "pa_IN": {"name_en": u"Punjabi", "name": u""},
+    "pl_PL": {"name_en": u"Polish", "name": u"Polski"},
+    "pt_BR": {"name_en": u"Portuguese (Brazil)", "name": u"Portugus (Brasil)"},
+    "pt_PT": {"name_en": u"Portuguese (Portugal)", "name": u"Portugus (Portugal)"},
+    "ro_RO": {"name_en": u"Romanian", "name": u"Romn"},
+    "ru_RU": {"name_en": u"Russian", "name": u""},
+    "sk_SK": {"name_en": u"Slovak", "name": u"Slovenina"},
+    "sl_SI": {"name_en": u"Slovenian", "name": u"Slovenina"},
+    "sq_AL": {"name_en": u"Albanian", "name": u"Shqip"},
+    "sr_RS": {"name_en": u"Serbian", "name": u""},
+    "sv_SE": {"name_en": u"Swedish", "name": u"Svenska"},
+    "sw_KE": {"name_en": u"Swahili", "name": u"Kiswahili"},
+    "ta_IN": {"name_en": u"Tamil", "name": u""},
+    "te_IN": {"name_en": u"Telugu", "name": u""},
+    "th_TH": {"name_en": u"Thai", "name": u""},
+    "tl_PH": {"name_en": u"Filipino", "name": u"Filipino"},
+    "tr_TR": {"name_en": u"Turkish", "name": u"Trke"},
+    "uk_UA": {"name_en": u"Ukraini ", "name": u""},
+    "vi_VN": {"name_en": u"Vietnamese", "name": u"Ting Vit"},
+    "zh_CN": {"name_en": u"Chinese (Simplified)", "name": u"()"},
+    "zh_TW": {"name_en": u"Chinese (Traditional)", "name": u"()"},
+}

--- a//dev/null
+++ b/salt/ext/tornado/auth.py
@@ -0,0 +1,933 @@
+"""This module contains implementations of various third-party
+authentication schemes.
+All the classes in this file are class mixins designed to be used with
+the `tornado.web.RequestHandler` class.  They are used in two ways:
+* On a login handler, use methods such as ``authenticate_redirect()``,
+  ``authorize_redirect()``, and ``get_authenticated_user()`` to
+  establish the user's identity and store authentication tokens to your
+  database and/or cookies.
+* In non-login handlers, use methods such as ``facebook_request()``
+  or ``twitter_request()`` to use the authentication tokens to make
+  requests to the respective services.
+They all take slightly different arguments due to the fact all these
+services implement authentication and authorization slightly differently.
+See the individual service classes below for complete documentation.
+Example usage for Google OAuth:
+.. testcode::
+    class GoogleOAuth2LoginHandler(tornado.web.RequestHandler,
+                                   tornado.auth.GoogleOAuth2Mixin):
+        @tornado.gen.coroutine
+        def get(self):
+            if self.get_argument('code', False):
+                user = yield self.get_authenticated_user(
+                    redirect_uri='http://your.site.com/auth/google',
+                    code=self.get_argument('code'))
+            else:
+                yield self.authorize_redirect(
+                    redirect_uri='http://your.site.com/auth/google',
+                    client_id=self.settings['google_oauth']['key'],
+                    scope=['profile', 'email'],
+                    response_type='code',
+                    extra_params={'approval_prompt': 'auto'})
+.. testoutput::
+   :hide:
+.. versionchanged:: 4.0
+   All of the callback interfaces in this module are now guaranteed
+   to run their callback with an argument of ``None`` on error.
+   Previously some functions would do this while others would simply
+   terminate the request on their own.  This change also ensures that
+   errors are more consistently reported through the ``Future`` interfaces.
+"""
+from __future__ import absolute_import, division, print_function
+import base64
+import binascii
+import functools
+import hashlib
+import hmac
+import time
+import uuid
+from salt.ext.tornado.concurrent import TracebackFuture, return_future, chain_future
+from salt.ext.tornado import gen
+from salt.ext.tornado import httpclient
+from salt.ext.tornado import escape
+from salt.ext.tornado.httputil import url_concat
+from salt.ext.tornado.log import gen_log
+from salt.ext.tornado.stack_context import ExceptionStackContext
+from salt.ext.tornado.util import unicode_type, ArgReplacer, PY3
+if PY3:
+    import urllib.parse as urlparse
+    import urllib.parse as urllib_parse
+    long = int
+else:
+    import urlparse
+    import urllib as urllib_parse
+class AuthError(Exception):
+    pass
+def _auth_future_to_callback(callback, future):
+    try:
+        result = future.result()
+    except AuthError as e:
+        gen_log.warning(str(e))
+        result = None
+    callback(result)
+def _auth_return_future(f):
+    """Similar to tornado.concurrent.return_future, but uses the auth
+    module's legacy callback interface.
+    Note that when using this decorator the ``callback`` parameter
+    inside the function will actually be a future.
+    """
+    replacer = ArgReplacer(f, 'callback')
+    @functools.wraps(f)
+    def wrapper(*args, **kwargs):
+        future = TracebackFuture()
+        callback, args, kwargs = replacer.replace(future, args, kwargs)
+        if callback is not None:
+            future.add_done_callback(
+                functools.partial(_auth_future_to_callback, callback))
+        def handle_exception(typ, value, tb):
+            if future.done():
+                return False
+            else:
+                future.set_exc_info((typ, value, tb))
+                return True
+        with ExceptionStackContext(handle_exception):
+            f(*args, **kwargs)
+        return future
+    return wrapper
+class OpenIdMixin(object):
+    """Abstract implementation of OpenID and Attribute Exchange.
+    Class attributes:
+    * ``_OPENID_ENDPOINT``: the identity provider's URI.
+    """
+    @return_future
+    def authenticate_redirect(self, callback_uri=None,
+                              ax_attrs=["name", "email", "language", "username"],
+                              callback=None):
+        """Redirects to the authentication URL for this service.
+        After authentication, the service will redirect back to the given
+        callback URI with additional parameters including ``openid.mode``.
+        We request the given attributes for the authenticated user by
+        default (name, email, language, and username). If you don't need
+        all those attributes for your app, you can request fewer with
+        the ax_attrs keyword argument.
+        .. versionchanged:: 3.1
+           Returns a `.Future` and takes an optional callback.  These are
+           not strictly necessary as this method is synchronous,
+           but they are supplied for consistency with
+           `OAuthMixin.authorize_redirect`.
+        """
+        callback_uri = callback_uri or self.request.uri
+        args = self._openid_args(callback_uri, ax_attrs=ax_attrs)
+        self.redirect(self._OPENID_ENDPOINT + "?" + urllib_parse.urlencode(args))
+        callback()
+    @_auth_return_future
+    def get_authenticated_user(self, callback, http_client=None):
+        """Fetches the authenticated user data upon redirect.
+        This method should be called by the handler that receives the
+        redirect from the `authenticate_redirect()` method (which is
+        often the same as the one that calls it; in that case you would
+        call `get_authenticated_user` if the ``openid.mode`` parameter
+        is present and `authenticate_redirect` if it is not).
+        The result of this method will generally be used to set a cookie.
+        """
+        args = dict((k, v[-1]) for k, v in self.request.arguments.items())
+        args["openid.mode"] = u"check_authentication"
+        url = self._OPENID_ENDPOINT
+        if http_client is None:
+            http_client = self.get_auth_http_client()
+        http_client.fetch(url, functools.partial(
+            self._on_authentication_verified, callback),
+            method="POST", body=urllib_parse.urlencode(args))
+    def _openid_args(self, callback_uri, ax_attrs=[], oauth_scope=None):
+        url = urlparse.urljoin(self.request.full_url(), callback_uri)
+        args = {
+            "openid.ns": "http://specs.openid.net/auth/2.0",
+            "openid.claimed_id":
+            "http://specs.openid.net/auth/2.0/identifier_select",
+            "openid.identity":
+            "http://specs.openid.net/auth/2.0/identifier_select",
+            "openid.return_to": url,
+            "openid.realm": urlparse.urljoin(url, '/'),
+            "openid.mode": "checkid_setup",
+        }
+        if ax_attrs:
+            args.update({
+                "openid.ns.ax": "http://openid.net/srv/ax/1.0",
+                "openid.ax.mode": "fetch_request",
+            })
+            ax_attrs = set(ax_attrs)
+            required = []
+            if "name" in ax_attrs:
+                ax_attrs -= set(["name", "firstname", "fullname", "lastname"])
+                required += ["firstname", "fullname", "lastname"]
+                args.update({
+                    "openid.ax.type.firstname":
+                    "http://axschema.org/namePerson/first",
+                    "openid.ax.type.fullname":
+                    "http://axschema.org/namePerson",
+                    "openid.ax.type.lastname":
+                    "http://axschema.org/namePerson/last",
+                })
+            known_attrs = {
+                "email": "http://axschema.org/contact/email",
+                "language": "http://axschema.org/pref/language",
+                "username": "http://axschema.org/namePerson/friendly",
+            }
+            for name in ax_attrs:
+                args["openid.ax.type." + name] = known_attrs[name]
+                required.append(name)
+            args["openid.ax.required"] = ",".join(required)
+        if oauth_scope:
+            args.update({
+                "openid.ns.oauth":
+                "http://specs.openid.net/extensions/oauth/1.0",
+                "openid.oauth.consumer": self.request.host.split(":")[0],
+                "openid.oauth.scope": oauth_scope,
+            })
+        return args
+    def _on_authentication_verified(self, future, response):
+        if response.error or b"is_valid:true" not in response.body:
+            future.set_exception(AuthError(
+                "Invalid OpenID response: %s" % (response.error or
+                                                 response.body)))
+            return
+        ax_ns = None
+        for name in self.request.arguments:
+            if name.startswith("openid.ns.") and \
+                    self.get_argument(name) == u"http://openid.net/srv/ax/1.0":
+                ax_ns = name[10:]
+                break
+        def get_ax_arg(uri):
+            if not ax_ns:
+                return u""
+            prefix = "openid." + ax_ns + ".type."
+            ax_name = None
+            for name in self.request.arguments.keys():
+                if self.get_argument(name) == uri and name.startswith(prefix):
+                    part = name[len(prefix):]
+                    ax_name = "openid." + ax_ns + ".value." + part
+                    break
+            if not ax_name:
+                return u""
+            return self.get_argument(ax_name, u"")
+        email = get_ax_arg("http://axschema.org/contact/email")
+        name = get_ax_arg("http://axschema.org/namePerson")
+        first_name = get_ax_arg("http://axschema.org/namePerson/first")
+        last_name = get_ax_arg("http://axschema.org/namePerson/last")
+        username = get_ax_arg("http://axschema.org/namePerson/friendly")
+        locale = get_ax_arg("http://axschema.org/pref/language").lower()
+        user = dict()
+        name_parts = []
+        if first_name:
+            user["first_name"] = first_name
+            name_parts.append(first_name)
+        if last_name:
+            user["last_name"] = last_name
+            name_parts.append(last_name)
+        if name:
+            user["name"] = name
+        elif name_parts:
+            user["name"] = u" ".join(name_parts)
+        elif email:
+            user["name"] = email.split("@")[0]
+        if email:
+            user["email"] = email
+        if locale:
+            user["locale"] = locale
+        if username:
+            user["username"] = username
+        claimed_id = self.get_argument("openid.claimed_id", None)
+        if claimed_id:
+            user["claimed_id"] = claimed_id
+        future.set_result(user)
+    def get_auth_http_client(self):
+        """Returns the `.AsyncHTTPClient` instance to be used for auth requests.
+        May be overridden by subclasses to use an HTTP client other than
+        the default.
+        """
+        return httpclient.AsyncHTTPClient()
+class OAuthMixin(object):
+    """Abstract implementation of OAuth 1.0 and 1.0a.
+    See `TwitterMixin` below for an example implementation.
+    Class attributes:
+    * ``_OAUTH_AUTHORIZE_URL``: The service's OAuth authorization url.
+    * ``_OAUTH_ACCESS_TOKEN_URL``: The service's OAuth access token url.
+    * ``_OAUTH_VERSION``: May be either "1.0" or "1.0a".
+    * ``_OAUTH_NO_CALLBACKS``: Set this to True if the service requires
+      advance registration of callbacks.
+    Subclasses must also override the `_oauth_get_user_future` and
+    `_oauth_consumer_token` methods.
+    """
+    @return_future
+    def authorize_redirect(self, callback_uri=None, extra_params=None,
+                           http_client=None, callback=None):
+        """Redirects the user to obtain OAuth authorization for this service.
+        The ``callback_uri`` may be omitted if you have previously
+        registered a callback URI with the third-party service.  For
+        some services (including Friendfeed), you must use a
+        previously-registered callback URI and cannot specify a
+        callback via this method.
+        This method sets a cookie called ``_oauth_request_token`` which is
+        subsequently used (and cleared) in `get_authenticated_user` for
+        security purposes.
+        Note that this method is asynchronous, although it calls
+        `.RequestHandler.finish` for you so it may not be necessary
+        to pass a callback or use the `.Future` it returns.  However,
+        if this method is called from a function decorated with
+        `.gen.coroutine`, you must call it with ``yield`` to keep the
+        response from being closed prematurely.
+        .. versionchanged:: 3.1
+           Now returns a `.Future` and takes an optional callback, for
+           compatibility with `.gen.coroutine`.
+        """
+        if callback_uri and getattr(self, "_OAUTH_NO_CALLBACKS", False):
+            raise Exception("This service does not support oauth_callback")
+        if http_client is None:
+            http_client = self.get_auth_http_client()
+        if getattr(self, "_OAUTH_VERSION", "1.0a") == "1.0a":
+            http_client.fetch(
+                self._oauth_request_token_url(callback_uri=callback_uri,
+                                              extra_params=extra_params),
+                functools.partial(
+                    self._on_request_token,
+                    self._OAUTH_AUTHORIZE_URL,
+                    callback_uri,
+                    callback))
+        else:
+            http_client.fetch(
+                self._oauth_request_token_url(),
+                functools.partial(
+                    self._on_request_token, self._OAUTH_AUTHORIZE_URL,
+                    callback_uri,
+                    callback))
+    @_auth_return_future
+    def get_authenticated_user(self, callback, http_client=None):
+        """Gets the OAuth authorized user and access token.
+        This method should be called from the handler for your
+        OAuth callback URL to complete the registration process. We run the
+        callback with the authenticated user dictionary.  This dictionary
+        will contain an ``access_key`` which can be used to make authorized
+        requests to this service on behalf of the user.  The dictionary will
+        also contain other fields such as ``name``, depending on the service
+        used.
+        """
+        future = callback
+        request_key = escape.utf8(self.get_argument("oauth_token"))
+        oauth_verifier = self.get_argument("oauth_verifier", None)
+        request_cookie = self.get_cookie("_oauth_request_token")
+        if not request_cookie:
+            future.set_exception(AuthError(
+                "Missing OAuth request token cookie"))
+            return
+        self.clear_cookie("_oauth_request_token")
+        cookie_key, cookie_secret = [base64.b64decode(escape.utf8(i)) for i in request_cookie.split("|")]
+        if cookie_key != request_key:
+            future.set_exception(AuthError(
+                "Request token does not match cookie"))
+            return
+        token = dict(key=cookie_key, secret=cookie_secret)
+        if oauth_verifier:
+            token["verifier"] = oauth_verifier
+        if http_client is None:
+            http_client = self.get_auth_http_client()
+        http_client.fetch(self._oauth_access_token_url(token),
+                          functools.partial(self._on_access_token, callback))
+    def _oauth_request_token_url(self, callback_uri=None, extra_params=None):
+        consumer_token = self._oauth_consumer_token()
+        url = self._OAUTH_REQUEST_TOKEN_URL
+        args = dict(
+            oauth_consumer_key=escape.to_basestring(consumer_token["key"]),
+            oauth_signature_method="HMAC-SHA1",
+            oauth_timestamp=str(int(time.time())),
+            oauth_nonce=escape.to_basestring(binascii.b2a_hex(uuid.uuid4().bytes)),
+            oauth_version="1.0",
+        )
+        if getattr(self, "_OAUTH_VERSION", "1.0a") == "1.0a":
+            if callback_uri == "oob":
+                args["oauth_callback"] = "oob"
+            elif callback_uri:
+                args["oauth_callback"] = urlparse.urljoin(
+                    self.request.full_url(), callback_uri)
+            if extra_params:
+                args.update(extra_params)
+            signature = _oauth10a_signature(consumer_token, "GET", url, args)
+        else:
+            signature = _oauth_signature(consumer_token, "GET", url, args)
+        args["oauth_signature"] = signature
+        return url + "?" + urllib_parse.urlencode(args)
+    def _on_request_token(self, authorize_url, callback_uri, callback,
+                          response):
+        if response.error:
+            raise Exception("Could not get request token: %s" % response.error)
+        request_token = _oauth_parse_response(response.body)
+        data = (base64.b64encode(escape.utf8(request_token["key"])) + b"|" +
+                base64.b64encode(escape.utf8(request_token["secret"])))
+        self.set_cookie("_oauth_request_token", data)
+        args = dict(oauth_token=request_token["key"])
+        if callback_uri == "oob":
+            self.finish(authorize_url + "?" + urllib_parse.urlencode(args))
+            callback()
+            return
+        elif callback_uri:
+            args["oauth_callback"] = urlparse.urljoin(
+                self.request.full_url(), callback_uri)
+        self.redirect(authorize_url + "?" + urllib_parse.urlencode(args))
+        callback()
+    def _oauth_access_token_url(self, request_token):
+        consumer_token = self._oauth_consumer_token()
+        url = self._OAUTH_ACCESS_TOKEN_URL
+        args = dict(
+            oauth_consumer_key=escape.to_basestring(consumer_token["key"]),
+            oauth_token=escape.to_basestring(request_token["key"]),
+            oauth_signature_method="HMAC-SHA1",
+            oauth_timestamp=str(int(time.time())),
+            oauth_nonce=escape.to_basestring(binascii.b2a_hex(uuid.uuid4().bytes)),
+            oauth_version="1.0",
+        )
+        if "verifier" in request_token:
+            args["oauth_verifier"] = request_token["verifier"]
+        if getattr(self, "_OAUTH_VERSION", "1.0a") == "1.0a":
+            signature = _oauth10a_signature(consumer_token, "GET", url, args,
+                                            request_token)
+        else:
+            signature = _oauth_signature(consumer_token, "GET", url, args,
+                                         request_token)
+        args["oauth_signature"] = signature
+        return url + "?" + urllib_parse.urlencode(args)
+    def _on_access_token(self, future, response):
+        if response.error:
+            future.set_exception(AuthError("Could not fetch access token"))
+            return
+        access_token = _oauth_parse_response(response.body)
+        self._oauth_get_user_future(access_token).add_done_callback(
+            functools.partial(self._on_oauth_get_user, access_token, future))
+    def _oauth_consumer_token(self):
+        """Subclasses must override this to return their OAuth consumer keys.
+        The return value should be a `dict` with keys ``key`` and ``secret``.
+        """
+        raise NotImplementedError()
+    @return_future
+    def _oauth_get_user_future(self, access_token, callback):
+        """Subclasses must override this to get basic information about the
+        user.
+        Should return a `.Future` whose result is a dictionary
+        containing information about the user, which may have been
+        retrieved by using ``access_token`` to make a request to the
+        service.
+        The access token will be added to the returned dictionary to make
+        the result of `get_authenticated_user`.
+        For backwards compatibility, the callback-based ``_oauth_get_user``
+        method is also supported.
+        """
+        self._oauth_get_user(access_token, callback)
+    def _oauth_get_user(self, access_token, callback):
+        raise NotImplementedError()
+    def _on_oauth_get_user(self, access_token, future, user_future):
+        if user_future.exception() is not None:
+            future.set_exception(user_future.exception())
+            return
+        user = user_future.result()
+        if not user:
+            future.set_exception(AuthError("Error getting user"))
+            return
+        user["access_token"] = access_token
+        future.set_result(user)
+    def _oauth_request_parameters(self, url, access_token, parameters={},
+                                  method="GET"):
+        """Returns the OAuth parameters as a dict for the given request.
+        parameters should include all POST arguments and query string arguments
+        that will be sent with the request.
+        """
+        consumer_token = self._oauth_consumer_token()
+        base_args = dict(
+            oauth_consumer_key=escape.to_basestring(consumer_token["key"]),
+            oauth_token=escape.to_basestring(access_token["key"]),
+            oauth_signature_method="HMAC-SHA1",
+            oauth_timestamp=str(int(time.time())),
+            oauth_nonce=escape.to_basestring(binascii.b2a_hex(uuid.uuid4().bytes)),
+            oauth_version="1.0",
+        )
+        args = {}
+        args.update(base_args)
+        args.update(parameters)
+        if getattr(self, "_OAUTH_VERSION", "1.0a") == "1.0a":
+            signature = _oauth10a_signature(consumer_token, method, url, args,
+                                            access_token)
+        else:
+            signature = _oauth_signature(consumer_token, method, url, args,
+                                         access_token)
+        base_args["oauth_signature"] = escape.to_basestring(signature)
+        return base_args
+    def get_auth_http_client(self):
+        """Returns the `.AsyncHTTPClient` instance to be used for auth requests.
+        May be overridden by subclasses to use an HTTP client other than
+        the default.
+        """
+        return httpclient.AsyncHTTPClient()
+class OAuth2Mixin(object):
+    """Abstract implementation of OAuth 2.0.
+    See `FacebookGraphMixin` or `GoogleOAuth2Mixin` below for example
+    implementations.
+    Class attributes:
+    * ``_OAUTH_AUTHORIZE_URL``: The service's authorization url.
+    * ``_OAUTH_ACCESS_TOKEN_URL``:  The service's access token url.
+    """
+    @return_future
+    def authorize_redirect(self, redirect_uri=None, client_id=None,
+                           client_secret=None, extra_params=None,
+                           callback=None, scope=None, response_type="code"):
+        """Redirects the user to obtain OAuth authorization for this service.
+        Some providers require that you register a redirect URL with
+        your application instead of passing one via this method. You
+        should call this method to log the user in, and then call
+        ``get_authenticated_user`` in the handler for your
+        redirect URL to complete the authorization process.
+        .. versionchanged:: 3.1
+           Returns a `.Future` and takes an optional callback.  These are
+           not strictly necessary as this method is synchronous,
+           but they are supplied for consistency with
+           `OAuthMixin.authorize_redirect`.
+        """
+        args = {
+            "redirect_uri": redirect_uri,
+            "client_id": client_id,
+            "response_type": response_type
+        }
+        if extra_params:
+            args.update(extra_params)
+        if scope:
+            args['scope'] = ' '.join(scope)
+        self.redirect(
+            url_concat(self._OAUTH_AUTHORIZE_URL, args))
+        callback()
+    def _oauth_request_token_url(self, redirect_uri=None, client_id=None,
+                                 client_secret=None, code=None,
+                                 extra_params=None):
+        url = self._OAUTH_ACCESS_TOKEN_URL
+        args = dict(
+            redirect_uri=redirect_uri,
+            code=code,
+            client_id=client_id,
+            client_secret=client_secret,
+        )
+        if extra_params:
+            args.update(extra_params)
+        return url_concat(url, args)
+    @_auth_return_future
+    def oauth2_request(self, url, callback, access_token=None,
+                       post_args=None, **args):
+        """Fetches the given URL auth an OAuth2 access token.
+        If the request is a POST, ``post_args`` should be provided. Query
+        string arguments should be given as keyword arguments.
+        Example usage:
+        ..testcode::
+            class MainHandler(tornado.web.RequestHandler,
+                              tornado.auth.FacebookGraphMixin):
+                @tornado.web.authenticated
+                @tornado.gen.coroutine
+                def get(self):
+                    new_entry = yield self.oauth2_request(
+                        "https://graph.facebook.com/me/feed",
+                        post_args={"message": "I am posting from my Tornado application!"},
+                        access_token=self.current_user["access_token"])
+                    if not new_entry:
+                        yield self.authorize_redirect()
+                        return
+                    self.finish("Posted a message!")
+        .. testoutput::
+           :hide:
+        .. versionadded:: 4.3
+        """
+        all_args = {}
+        if access_token:
+            all_args["access_token"] = access_token
+            all_args.update(args)
+        if all_args:
+            url += "?" + urllib_parse.urlencode(all_args)
+        callback = functools.partial(self._on_oauth2_request, callback)
+        http = self.get_auth_http_client()
+        if post_args is not None:
+            http.fetch(url, method="POST", body=urllib_parse.urlencode(post_args),
+                       callback=callback)
+        else:
+            http.fetch(url, callback=callback)
+    def _on_oauth2_request(self, future, response):
+        if response.error:
+            future.set_exception(AuthError("Error response %s fetching %s" %
+                                           (response.error, response.request.url)))
+            return
+        future.set_result(escape.json_decode(response.body))
+    def get_auth_http_client(self):
+        """Returns the `.AsyncHTTPClient` instance to be used for auth requests.
+        May be overridden by subclasses to use an HTTP client other than
+        the default.
+        .. versionadded:: 4.3
+        """
+        return httpclient.AsyncHTTPClient()
+class TwitterMixin(OAuthMixin):
+    """Twitter OAuth authentication.
+    To authenticate with Twitter, register your application with
+    Twitter at http://twitter.com/apps. Then copy your Consumer Key
+    and Consumer Secret to the application
+    `~tornado.web.Application.settings` ``twitter_consumer_key`` and
+    ``twitter_consumer_secret``. Use this mixin on the handler for the
+    URL you registered as your application's callback URL.
+    When your application is set up, you can use this mixin like this
+    to authenticate the user with Twitter and get access to their stream:
+    .. testcode::
+        class TwitterLoginHandler(tornado.web.RequestHandler,
+                                  tornado.auth.TwitterMixin):
+            @tornado.gen.coroutine
+            def get(self):
+                if self.get_argument("oauth_token", None):
+                    user = yield self.get_authenticated_user()
+                else:
+                    yield self.authorize_redirect()
+    .. testoutput::
+       :hide:
+    The user object returned by `~OAuthMixin.get_authenticated_user`
+    includes the attributes ``username``, ``name``, ``access_token``,
+    and all of the custom Twitter user attributes described at
+    https://dev.twitter.com/docs/api/1.1/get/users/show
+    """
+    _OAUTH_REQUEST_TOKEN_URL = "https://api.twitter.com/oauth/request_token"
+    _OAUTH_ACCESS_TOKEN_URL = "https://api.twitter.com/oauth/access_token"
+    _OAUTH_AUTHORIZE_URL = "https://api.twitter.com/oauth/authorize"
+    _OAUTH_AUTHENTICATE_URL = "https://api.twitter.com/oauth/authenticate"
+    _OAUTH_NO_CALLBACKS = False
+    _TWITTER_BASE_URL = "https://api.twitter.com/1.1"
+    @return_future
+    def authenticate_redirect(self, callback_uri=None, callback=None):
+        """Just like `~OAuthMixin.authorize_redirect`, but
+        auto-redirects if authorized.
+        This is generally the right interface to use if you are using
+        Twitter for single-sign on.
+        .. versionchanged:: 3.1
+           Now returns a `.Future` and takes an optional callback, for
+           compatibility with `.gen.coroutine`.
+        """
+        http = self.get_auth_http_client()
+        http.fetch(self._oauth_request_token_url(callback_uri=callback_uri),
+                   functools.partial(
+                       self._on_request_token, self._OAUTH_AUTHENTICATE_URL,
+                       None, callback))
+    @_auth_return_future
+    def twitter_request(self, path, callback=None, access_token=None,
+                        post_args=None, **args):
+        """Fetches the given API path, e.g., ``statuses/user_timeline/btaylor``
+        The path should not include the format or API version number.
+        (we automatically use JSON format and API version 1).
+        If the request is a POST, ``post_args`` should be provided. Query
+        string arguments should be given as keyword arguments.
+        All the Twitter methods are documented at http://dev.twitter.com/
+        Many methods require an OAuth access token which you can
+        obtain through `~OAuthMixin.authorize_redirect` and
+        `~OAuthMixin.get_authenticated_user`. The user returned through that
+        process includes an 'access_token' attribute that can be used
+        to make authenticated requests via this method. Example
+        usage:
+        .. testcode::
+            class MainHandler(tornado.web.RequestHandler,
+                              tornado.auth.TwitterMixin):
+                @tornado.web.authenticated
+                @tornado.gen.coroutine
+                def get(self):
+                    new_entry = yield self.twitter_request(
+                        "/statuses/update",
+                        post_args={"status": "Testing Tornado Web Server"},
+                        access_token=self.current_user["access_token"])
+                    if not new_entry:
+                        yield self.authorize_redirect()
+                        return
+                    self.finish("Posted a message!")
+        .. testoutput::
+           :hide:
+        """
+        if path.startswith('http:') or path.startswith('https:'):
+            url = path
+        else:
+            url = self._TWITTER_BASE_URL + path + ".json"
+        if access_token:
+            all_args = {}
+            all_args.update(args)
+            all_args.update(post_args or {})
+            method = "POST" if post_args is not None else "GET"
+            oauth = self._oauth_request_parameters(
+                url, access_token, all_args, method=method)
+            args.update(oauth)
+        if args:
+            url += "?" + urllib_parse.urlencode(args)
+        http = self.get_auth_http_client()
+        http_callback = functools.partial(self._on_twitter_request, callback)
+        if post_args is not None:
+            http.fetch(url, method="POST", body=urllib_parse.urlencode(post_args),
+                       callback=http_callback)
+        else:
+            http.fetch(url, callback=http_callback)
+    def _on_twitter_request(self, future, response):
+        if response.error:
+            future.set_exception(AuthError(
+                "Error response %s fetching %s" % (response.error,
+                                                   response.request.url)))
+            return
+        future.set_result(escape.json_decode(response.body))
+    def _oauth_consumer_token(self):
+        self.require_setting("twitter_consumer_key", "Twitter OAuth")
+        self.require_setting("twitter_consumer_secret", "Twitter OAuth")
+        return dict(
+            key=self.settings["twitter_consumer_key"],
+            secret=self.settings["twitter_consumer_secret"])
+    @gen.coroutine
+    def _oauth_get_user_future(self, access_token):
+        user = yield self.twitter_request(
+            "/account/verify_credentials",
+            access_token=access_token)
+        if user:
+            user["username"] = user["screen_name"]
+        raise gen.Return(user)
+class GoogleOAuth2Mixin(OAuth2Mixin):
+    """Google authentication using OAuth2.
+    In order to use, register your application with Google and copy the
+    relevant parameters to your application settings.
+    * Go to the Google Dev Console at http://console.developers.google.com
+    * Select a project, or create a new one.
+    * In the sidebar on the left, select APIs & Auth.
+    * In the list of APIs, find the Google+ API service and set it to ON.
+    * In the sidebar on the left, select Credentials.
+    * In the OAuth section of the page, select Create New Client ID.
+    * Set the Redirect URI to point to your auth handler
+    * Copy the "Client secret" and "Client ID" to the application settings as
+      {"google_oauth": {"key": CLIENT_ID, "secret": CLIENT_SECRET}}
+    .. versionadded:: 3.2
+    """
+    _OAUTH_AUTHORIZE_URL = "https://accounts.google.com/o/oauth2/auth"
+    _OAUTH_ACCESS_TOKEN_URL = "https://accounts.google.com/o/oauth2/token"
+    _OAUTH_USERINFO_URL = "https://www.googleapis.com/oauth2/v1/userinfo"
+    _OAUTH_NO_CALLBACKS = False
+    _OAUTH_SETTINGS_KEY = 'google_oauth'
+    @_auth_return_future
+    def get_authenticated_user(self, redirect_uri, code, callback):
+        """Handles the login for the Google user, returning an access token.
+        The result is a dictionary containing an ``access_token`` field
+        ([among others](https://developers.google.com/identity/protocols/OAuth2WebServer#handlingtheresponse)).
+        Unlike other ``get_authenticated_user`` methods in this package,
+        this method does not return any additional information about the user.
+        The returned access token can be used with `OAuth2Mixin.oauth2_request`
+        to request additional information (perhaps from
+        ``https://www.googleapis.com/oauth2/v2/userinfo``)
+        Example usage:
+        .. testcode::
+            class GoogleOAuth2LoginHandler(tornado.web.RequestHandler,
+                                           tornado.auth.GoogleOAuth2Mixin):
+                @tornado.gen.coroutine
+                def get(self):
+                    if self.get_argument('code', False):
+                        access = yield self.get_authenticated_user(
+                            redirect_uri='http://your.site.com/auth/google',
+                            code=self.get_argument('code'))
+                        user = yield self.oauth2_request(
+                            "https://www.googleapis.com/oauth2/v1/userinfo",
+                            access_token=access["access_token"])
+                    else:
+                        yield self.authorize_redirect(
+                            redirect_uri='http://your.site.com/auth/google',
+                            client_id=self.settings['google_oauth']['key'],
+                            scope=['profile', 'email'],
+                            response_type='code',
+                            extra_params={'approval_prompt': 'auto'})
+        .. testoutput::
+           :hide:
+        """
+        http = self.get_auth_http_client()
+        body = urllib_parse.urlencode({
+            "redirect_uri": redirect_uri,
+            "code": code,
+            "client_id": self.settings[self._OAUTH_SETTINGS_KEY]['key'],
+            "client_secret": self.settings[self._OAUTH_SETTINGS_KEY]['secret'],
+            "grant_type": "authorization_code",
+        })
+        http.fetch(self._OAUTH_ACCESS_TOKEN_URL,
+                   functools.partial(self._on_access_token, callback),
+                   method="POST", headers={'Content-Type': 'application/x-www-form-urlencoded'}, body=body)
+    def _on_access_token(self, future, response):
+        """Callback function for the exchange to the access token."""
+        if response.error:
+            future.set_exception(AuthError('Google auth error: %s' % str(response)))
+            return
+        args = escape.json_decode(response.body)
+        future.set_result(args)
+class FacebookGraphMixin(OAuth2Mixin):
+    """Facebook authentication using the new Graph API and OAuth2."""
+    _OAUTH_ACCESS_TOKEN_URL = "https://graph.facebook.com/oauth/access_token?"
+    _OAUTH_AUTHORIZE_URL = "https://www.facebook.com/dialog/oauth?"
+    _OAUTH_NO_CALLBACKS = False
+    _FACEBOOK_BASE_URL = "https://graph.facebook.com"
+    @_auth_return_future
+    def get_authenticated_user(self, redirect_uri, client_id, client_secret,
+                               code, callback, extra_fields=None):
+        """Handles the login for the Facebook user, returning a user object.
+        Example usage:
+        .. testcode::
+            class FacebookGraphLoginHandler(tornado.web.RequestHandler,
+                                            tornado.auth.FacebookGraphMixin):
+              @tornado.gen.coroutine
+              def get(self):
+                  if self.get_argument("code", False):
+                      user = yield self.get_authenticated_user(
+                          redirect_uri='/auth/facebookgraph/',
+                          client_id=self.settings["facebook_api_key"],
+                          client_secret=self.settings["facebook_secret"],
+                          code=self.get_argument("code"))
+                  else:
+                      yield self.authorize_redirect(
+                          redirect_uri='/auth/facebookgraph/',
+                          client_id=self.settings["facebook_api_key"],
+                          extra_params={"scope": "read_stream,offline_access"})
+        .. testoutput::
+           :hide:
+        This method returns a dictionary which may contain the following fields:
+        * ``access_token``, a string which may be passed to `facebook_request`
+        * ``session_expires``, an integer encoded as a string representing
+          the time until the access token expires in seconds. This field should
+          be used like ``int(user['session_expires'])``; in a future version of
+          Tornado it will change from a string to an integer.
+        * ``id``, ``name``, ``first_name``, ``last_name``, ``locale``, ``picture``,
+          ``link``, plus any fields named in the ``extra_fields`` argument. These
+          fields are copied from the Facebook graph API `user object <https://developers.facebook.com/docs/graph-api/reference/user>`_
+        .. versionchanged:: 4.5
+           The ``session_expires`` field was updated to support changes made to the
+           Facebook API in March 2017.
+        """
+        http = self.get_auth_http_client()
+        args = {
+            "redirect_uri": redirect_uri,
+            "code": code,
+            "client_id": client_id,
+            "client_secret": client_secret,
+        }
+        fields = set(['id', 'name', 'first_name', 'last_name',
+                      'locale', 'picture', 'link'])
+        if extra_fields:
+            fields.update(extra_fields)
+        http.fetch(self._oauth_request_token_url(**args),
+                   functools.partial(self._on_access_token, redirect_uri, client_id,
+                                     client_secret, callback, fields))
+    def _on_access_token(self, redirect_uri, client_id, client_secret,
+                         future, fields, response):
+        if response.error:
+            future.set_exception(AuthError('Facebook auth error: %s' % str(response)))
+            return
+        args = escape.json_decode(response.body)
+        session = {
+            "access_token": args.get("access_token"),
+            "expires_in": args.get("expires_in")
+        }
+        self.facebook_request(
+            path="/me",
+            callback=functools.partial(
+                self._on_get_user_info, future, session, fields),
+            access_token=session["access_token"],
+            appsecret_proof=hmac.new(key=client_secret.encode('utf8'),
+                                     msg=session["access_token"].encode('utf8'),
+                                     digestmod=hashlib.sha256).hexdigest(),
+            fields=",".join(fields)
+        )
+    def _on_get_user_info(self, future, session, fields, user):
+        if user is None:
+            future.set_result(None)
+            return
+        fieldmap = {}
+        for field in fields:
+            fieldmap[field] = user.get(field)
+        fieldmap.update({"access_token": session["access_token"],
+                         "session_expires": str(session.get("expires_in"))})
+        future.set_result(fieldmap)
+    @_auth_return_future
+    def facebook_request(self, path, callback, access_token=None,
+                         post_args=None, **args):
+        """Fetches the given relative API path, e.g., "/btaylor/picture"
+        If the request is a POST, ``post_args`` should be provided. Query
+        string arguments should be given as keyword arguments.
+        An introduction to the Facebook Graph API can be found at
+        http://developers.facebook.com/docs/api
+        Many methods require an OAuth access token which you can
+        obtain through `~OAuth2Mixin.authorize_redirect` and
+        `get_authenticated_user`. The user returned through that
+        process includes an ``access_token`` attribute that can be
+        used to make authenticated requests via this method.
+        Example usage:
+        ..testcode::
+            class MainHandler(tornado.web.RequestHandler,
+                              tornado.auth.FacebookGraphMixin):
+                @tornado.web.authenticated
+                @tornado.gen.coroutine
+                def get(self):
+                    new_entry = yield self.facebook_request(
+                        "/me/feed",
+                        post_args={"message": "I am posting from my Tornado application!"},
+                        access_token=self.current_user["access_token"])
+                    if not new_entry:
+                        yield self.authorize_redirect()
+                        return
+                    self.finish("Posted a message!")
+        .. testoutput::
+           :hide:
+        The given path is relative to ``self._FACEBOOK_BASE_URL``,
+        by default "https://graph.facebook.com".
+        This method is a wrapper around `OAuth2Mixin.oauth2_request`;
+        the only difference is that this method takes a relative path,
+        while ``oauth2_request`` takes a complete url.
+        .. versionchanged:: 3.1
+           Added the ability to override ``self._FACEBOOK_BASE_URL``.
+        """
+        url = self._FACEBOOK_BASE_URL + path
+        oauth_future = self.oauth2_request(url, access_token=access_token,
+                                           post_args=post_args, **args)
+        chain_future(oauth_future, callback)
+def _oauth_signature(consumer_token, method, url, parameters={}, token=None):
+    """Calculates the HMAC-SHA1 OAuth signature for the given request.
+    See http://oauth.net/core/1.0/#signing_process
+    """
+    parts = urlparse.urlparse(url)
+    scheme, netloc, path = parts[:3]
+    normalized_url = scheme.lower() + "://" + netloc.lower() + path
+    base_elems = []
+    base_elems.append(method.upper())
+    base_elems.append(normalized_url)
+    base_elems.append("&".join("%s=%s" % (k, _oauth_escape(str(v)))
+                               for k, v in sorted(parameters.items())))
+    base_string = "&".join(_oauth_escape(e) for e in base_elems)
+    key_elems = [escape.utf8(consumer_token["secret"])]
+    key_elems.append(escape.utf8(token["secret"] if token else ""))
+    key = b"&".join(key_elems)
+    hash = hmac.new(key, escape.utf8(base_string), hashlib.sha1)
+    return binascii.b2a_base64(hash.digest())[:-1]
+def _oauth10a_signature(consumer_token, method, url, parameters={}, token=None):
+    """Calculates the HMAC-SHA1 OAuth 1.0a signature for the given request.
+    See http://oauth.net/core/1.0a/#signing_process
+    """
+    parts = urlparse.urlparse(url)
+    scheme, netloc, path = parts[:3]
+    normalized_url = scheme.lower() + "://" + netloc.lower() + path
+    base_elems = []
+    base_elems.append(method.upper())
+    base_elems.append(normalized_url)
+    base_elems.append("&".join("%s=%s" % (k, _oauth_escape(str(v)))
+                               for k, v in sorted(parameters.items())))
+    base_string = "&".join(_oauth_escape(e) for e in base_elems)
+    key_elems = [escape.utf8(urllib_parse.quote(consumer_token["secret"], safe='~'))]
+    key_elems.append(escape.utf8(urllib_parse.quote(token["secret"], safe='~') if token else ""))
+    key = b"&".join(key_elems)
+    hash = hmac.new(key, escape.utf8(base_string), hashlib.sha1)
+    return binascii.b2a_base64(hash.digest())[:-1]
+def _oauth_escape(val):
+    if isinstance(val, unicode_type):
+        val = val.encode("utf-8")
+    return urllib_parse.quote(val, safe="~")
+def _oauth_parse_response(body):
+    body = escape.native_str(body)
+    p = urlparse.parse_qs(body, keep_blank_values=False)
+    token = dict(key=p["oauth_token"][0], secret=p["oauth_token_secret"][0])
+    special = ("oauth_token", "oauth_token_secret")
+    token.update((k, p[k][0]) for k in p if k not in special)
+    return token

--- a//dev/null
+++ b/salt/ext/tornado/autoreload.py
@@ -0,0 +1,198 @@
+"""Automatically restart the server when a source file is modified.
+Most applications should not access this module directly.  Instead,
+pass the keyword argument ``autoreload=True`` to the
+`tornado.web.Application` constructor (or ``debug=True``, which
+enables this setting and several others).  This will enable autoreload
+mode as well as checking for changes to templates and static
+resources.  Note that restarting is a destructive operation and any
+requests in progress will be aborted when the process restarts.  (If
+you want to disable autoreload while using other debug-mode features,
+pass both ``debug=True`` and ``autoreload=False``).
+This module can also be used as a command-line wrapper around scripts
+such as unit test runners.  See the `main` method for details.
+The command-line wrapper and Application debug modes can be used together.
+This combination is encouraged as the wrapper catches syntax errors and
+other import-time failures, while debug mode catches changes once
+the server has started.
+This module depends on `.IOLoop`, so it will not work in WSGI applications
+and Google App Engine.  It also will not work correctly when `.HTTPServer`'s
+multi-process mode is used.
+Reloading loses any Python interpreter command-line arguments (e.g. ``-u``)
+because it re-executes Python using ``sys.executable`` and ``sys.argv``.
+Additionally, modifying these variables will cause reloading to behave
+incorrectly.
+"""
+from __future__ import absolute_import, division, print_function
+import os
+import sys
+if __name__ == "__main__":
+    if sys.path[0] == os.path.dirname(__file__):
+        del sys.path[0]
+import functools
+import logging
+import os
+import pkgutil  # type: ignore
+import sys
+import traceback
+import types
+import subprocess
+import weakref
+from salt.ext.tornado import ioloop
+from salt.ext.tornado.log import gen_log
+from salt.ext.tornado import process
+from salt.ext.tornado.util import exec_in
+try:
+    import signal
+except ImportError:
+    signal = None
+_has_execv = sys.platform != 'win32'
+_watched_files = set()
+_reload_hooks = []
+_reload_attempted = False
+_io_loops = weakref.WeakKeyDictionary()  # type: ignore
+def start(io_loop=None, check_time=500):
+    """Begins watching source files for changes.
+    .. versionchanged:: 4.1
+       The ``io_loop`` argument is deprecated.
+    """
+    io_loop = io_loop or ioloop.IOLoop.current()
+    if io_loop in _io_loops:
+        return
+    _io_loops[io_loop] = True
+    if len(_io_loops) > 1:
+        gen_log.warning("tornado.autoreload started more than once in the same process")
+    modify_times = {}
+    callback = functools.partial(_reload_on_update, modify_times)
+    scheduler = ioloop.PeriodicCallback(callback, check_time, io_loop=io_loop)
+    scheduler.start()
+def wait():
+    """Wait for a watched file to change, then restart the process.
+    Intended to be used at the end of scripts like unit test runners,
+    to run the tests again after any source file changes (but see also
+    the command-line interface in `main`)
+    """
+    io_loop = ioloop.IOLoop()
+    start(io_loop)
+    io_loop.start()
+def watch(filename):
+    """Add a file to the watch list.
+    All imported modules are watched by default.
+    """
+    _watched_files.add(filename)
+def add_reload_hook(fn):
+    """Add a function to be called before reloading the process.
+    Note that for open file and socket handles it is generally
+    preferable to set the ``FD_CLOEXEC`` flag (using `fcntl` or
+    ``tornado.platform.auto.set_close_exec``) instead
+    of using a reload hook to close them.
+    """
+    _reload_hooks.append(fn)
+def _reload_on_update(modify_times):
+    if _reload_attempted:
+        return
+    if process.task_id() is not None:
+        return
+    for module in list(sys.modules.values()):
+        if not isinstance(module, types.ModuleType):
+            continue
+        path = getattr(module, "__file__", None)
+        if not path:
+            continue
+        if path.endswith(".pyc") or path.endswith(".pyo"):
+            path = path[:-1]
+        _check_file(modify_times, path)
+    for path in _watched_files:
+        _check_file(modify_times, path)
+def _check_file(modify_times, path):
+    try:
+        modified = os.stat(path).st_mtime
+    except Exception:
+        return
+    if path not in modify_times:
+        modify_times[path] = modified
+        return
+    if modify_times[path] != modified:
+        gen_log.info("%s modified; restarting server", path)
+        _reload()
+def _reload():
+    global _reload_attempted
+    _reload_attempted = True
+    for fn in _reload_hooks:
+        fn()
+    if hasattr(signal, "setitimer"):
+        signal.setitimer(signal.ITIMER_REAL, 0, 0)
+    path_prefix = '.' + os.pathsep
+    if (sys.path[0] == '' and
+            not os.environ.get("PYTHONPATH", "").startswith(path_prefix)):
+        os.environ["PYTHONPATH"] = (path_prefix +
+                                    os.environ.get("PYTHONPATH", ""))
+    if not _has_execv:
+        subprocess.Popen([sys.executable] + sys.argv)
+        sys.exit(0)
+    else:
+        try:
+            os.execv(sys.executable, [sys.executable] + sys.argv)
+        except OSError:
+            os.spawnv(os.P_NOWAIT, sys.executable,
+                      [sys.executable] + sys.argv)
+            os._exit(0)
+_USAGE = """\
+Usage:
+  python -m tornado.autoreload -m module.to.run [args...]
+  python -m tornado.autoreload path/to/script.py [args...]
+"""
+def main():
+    """Command-line wrapper to re-run a script whenever its source changes.
+    Scripts may be specified by filename or module name::
+        python -m tornado.autoreload -m tornado.test.runtests
+        python -m tornado.autoreload tornado/test/runtests.py
+    Running a script with this wrapper is similar to calling
+    `tornado.autoreload.wait` at the end of the script, but this wrapper
+    can catch import-time problems like syntax errors that would otherwise
+    prevent the script from reaching its call to `wait`.
+    """
+    original_argv = sys.argv
+    sys.argv = sys.argv[:]
+    if len(sys.argv) >= 3 and sys.argv[1] == "-m":
+        mode = "module"
+        module = sys.argv[2]
+        del sys.argv[1:3]
+    elif len(sys.argv) >= 2:
+        mode = "script"
+        script = sys.argv[1]
+        sys.argv = sys.argv[1:]
+    else:
+        print(_USAGE, file=sys.stderr)
+        sys.exit(1)
+    try:
+        if mode == "module":
+            import runpy
+            runpy.run_module(module, run_name="__main__", alter_sys=True)
+        elif mode == "script":
+            with open(script) as f:
+                global __file__
+                __file__ = script
+                global __package__
+                del __package__
+                exec_in(f.read(), globals(), globals())
+    except SystemExit as e:
+        logging.basicConfig()
+        gen_log.info("Script exited with status %s", e.code)
+    except Exception as e:
+        logging.basicConfig()
+        gen_log.warning("Script exited with uncaught exception", exc_info=True)
+        for (filename, lineno, name, line) in traceback.extract_tb(sys.exc_info()[2]):
+            watch(filename)
+        if isinstance(e, SyntaxError):
+            watch(e.filename)
+    else:
+        logging.basicConfig()
+        gen_log.info("Script exited normally")
+    sys.argv = original_argv
+    if mode == 'module':
+        loader = pkgutil.get_loader(module)
+        if loader is not None:
+            watch(loader.get_filename())
+    wait()
+if __name__ == "__main__":
+    main()

--- a//dev/null
+++ b/salt/ext/tornado/concurrent.py
@@ -0,0 +1,397 @@
+"""Utilities for working with threads and ``Futures``.
+``Futures`` are a pattern for concurrent programming introduced in
+Python 3.2 in the `concurrent.futures` package. This package defines
+a mostly-compatible `Future` class designed for use from coroutines,
+as well as some utility functions for interacting with the
+`concurrent.futures` package.
+"""
+from __future__ import absolute_import, division, print_function
+import functools
+import platform
+import sys
+import textwrap
+import traceback
+from salt.ext.tornado.log import app_log
+from salt.ext.tornado.stack_context import ExceptionStackContext, wrap
+from salt.ext.tornado.util import ArgReplacer, is_finalizing, raise_exc_info
+try:
+    from concurrent import futures
+except ImportError:
+    futures = None
+try:
+    import typing
+except ImportError:
+    typing = None
+_GC_CYCLE_FINALIZERS = platform.python_implementation() == "CPython" and sys.version_info >= (
+    3,
+    4,
+)
+class ReturnValueIgnoredError(Exception):
+    pass
+class _TracebackLogger(object):
+    """Helper to log a traceback upon destruction if not cleared.
+    This solves a nasty problem with Futures and Tasks that have an
+    exception set: if nobody asks for the exception, the exception is
+    never logged.  This violates the Zen of Python: 'Errors should
+    never pass silently.  Unless explicitly silenced.'
+    However, we don't want to log the exception as soon as
+    set_exception() is called: if the calling code is written
+    properly, it will get the exception and handle it properly.  But
+    we *do* want to log it if result() or exception() was never called
+    -- otherwise developers waste a lot of time wondering why their
+    buggy code fails silently.
+    An earlier attempt added a __del__() method to the Future class
+    itself, but this backfired because the presence of __del__()
+    prevents garbage collection from breaking cycles.  A way out of
+    this catch-22 is to avoid having a __del__() method on the Future
+    class itself, but instead to have a reference to a helper object
+    with a __del__() method that logs the traceback, where we ensure
+    that the helper object doesn't participate in cycles, and only the
+    Future has a reference to it.
+    The helper object is added when set_exception() is called.  When
+    the Future is collected, and the helper is present, the helper
+    object is also collected, and its __del__() method will log the
+    traceback.  When the Future's result() or exception() method is
+    called (and a helper object is present), it removes the helper
+    object, after calling its clear() method to prevent it from
+    logging.
+    One downside is that we do a fair amount of work to extract the
+    traceback from the exception, even when it is never logged.  It
+    would seem cheaper to just store the exception object, but that
+    references the traceback, which references stack frames, which may
+    reference the Future, which references the _TracebackLogger, and
+    then the _TracebackLogger would be included in a cycle, which is
+    what we're trying to avoid!  As an optimization, we don't
+    immediately format the exception; we only do the work when
+    activate() is called, which call is delayed until after all the
+    Future's callbacks have run.  Since usually a Future has at least
+    one callback (typically set by 'yield From') and usually that
+    callback extracts the callback, thereby removing the need to
+    format the exception.
+    PS. I don't claim credit for this solution.  I first heard of it
+    in a discussion about closing files when they are collected.
+    """
+    __slots__ = ("exc_info", "formatted_tb")
+    def __init__(self, exc_info):
+        self.exc_info = exc_info
+        self.formatted_tb = None
+    def activate(self):
+        exc_info = self.exc_info
+        if exc_info is not None:
+            self.exc_info = None
+            self.formatted_tb = traceback.format_exception(*exc_info)
+    def clear(self):
+        self.exc_info = None
+        self.formatted_tb = None
+    def __del__(self, is_finalizing=is_finalizing):
+        if not is_finalizing() and self.formatted_tb:
+            app_log.error(
+                "Future exception was never retrieved: %s",
+                "".join(self.formatted_tb).rstrip(),
+            )
+class Future(object):
+    """Placeholder for an asynchronous result.
+    A ``Future`` encapsulates the result of an asynchronous
+    operation.  In synchronous applications ``Futures`` are used
+    to wait for the result from a thread or process pool; in
+    Tornado they are normally used with `.IOLoop.add_future` or by
+    yielding them in a `.gen.coroutine`.
+    `tornado.concurrent.Future` is similar to
+    `concurrent.futures.Future`, but not thread-safe (and therefore
+    faster for use with single-threaded event loops).
+    In addition to ``exception`` and ``set_exception``, methods ``exc_info``
+    and ``set_exc_info`` are supported to capture tracebacks in Python 2.
+    The traceback is automatically available in Python 3, but in the
+    Python 2 futures backport this information is discarded.
+    This functionality was previously available in a separate class
+    ``TracebackFuture``, which is now a deprecated alias for this class.
+    .. versionchanged:: 4.0
+       `tornado.concurrent.Future` is always a thread-unsafe ``Future``
+       with support for the ``exc_info`` methods.  Previously it would
+       be an alias for the thread-safe `concurrent.futures.Future`
+       if that package was available and fall back to the thread-unsafe
+       implementation if it was not.
+    .. versionchanged:: 4.1
+       If a `.Future` contains an error but that error is never observed
+       (by calling ``result()``, ``exception()``, or ``exc_info()``),
+       a stack trace will be logged when the `.Future` is garbage collected.
+       This normally indicates an error in the application, but in cases
+       where it results in undesired logging it may be necessary to
+       suppress the logging by ensuring that the exception is observed:
+       ``f.add_done_callback(lambda f: f.exception())``.
+    """
+    def __init__(self):
+        self._done = False
+        self._result = None
+        self._exc_info = None
+        self._log_traceback = False  # Used for Python >= 3.4
+        self._tb_logger = None  # Used for Python <= 3.3
+        self._callbacks = []
+    if sys.version_info >= (3, 3):
+        exec(
+            textwrap.dedent(
+                """
+        def __await__(self):
+            return (yield self)
+        """
+            )
+        )
+    else:
+        def __await__(self):
+            result = yield self
+            e = StopIteration()
+            e.args = (result,)
+            raise e
+    def cancel(self):
+        """Cancel the operation, if possible.
+        Tornado ``Futures`` do not support cancellation, so this method always
+        returns False.
+        """
+        return False
+    def cancelled(self):
+        """Returns True if the operation has been cancelled.
+        Tornado ``Futures`` do not support cancellation, so this method
+        always returns False.
+        """
+        return False
+    def running(self):
+        """Returns True if this operation is currently running."""
+        return not self._done
+    def done(self):
+        """Returns True if the future has finished running."""
+        return self._done
+    def _clear_tb_log(self):
+        self._log_traceback = False
+        if self._tb_logger is not None:
+            self._tb_logger.clear()
+            self._tb_logger = None
+    def result(self, timeout=None):
+        """If the operation succeeded, return its result.  If it failed,
+        re-raise its exception.
+        This method takes a ``timeout`` argument for compatibility with
+        `concurrent.futures.Future` but it is an error to call it
+        before the `Future` is done, so the ``timeout`` is never used.
+        """
+        self._clear_tb_log()
+        if self._result is not None:
+            return self._result
+        if self._exc_info is not None:
+            try:
+                raise_exc_info(self._exc_info)
+            finally:
+                self = None
+        self._check_done()
+        return self._result
+    def exception(self, timeout=None):
+        """If the operation raised an exception, return the `Exception`
+        object.  Otherwise returns None.
+        This method takes a ``timeout`` argument for compatibility with
+        `concurrent.futures.Future` but it is an error to call it
+        before the `Future` is done, so the ``timeout`` is never used.
+        """
+        self._clear_tb_log()
+        if self._exc_info is not None:
+            return self._exc_info[1]
+        else:
+            self._check_done()
+            return None
+    def add_done_callback(self, fn):
+        """Attaches the given callback to the `Future`.
+        It will be invoked with the `Future` as its argument when the Future
+        has finished running and its result is available.  In Tornado
+        consider using `.IOLoop.add_future` instead of calling
+        `add_done_callback` directly.
+        """
+        if self._done:
+            fn(self)
+        else:
+            self._callbacks.append(fn)
+    def set_result(self, result):
+        """Sets the result of a ``Future``.
+        It is undefined to call any of the ``set`` methods more than once
+        on the same object.
+        """
+        self._result = result
+        self._set_done()
+    def set_exception(self, exception):
+        """Sets the exception of a ``Future.``"""
+        self.set_exc_info(
+            (exception.__class__, exception, getattr(exception, "__traceback__", None))
+        )
+    def exc_info(self):
+        """Returns a tuple in the same format as `sys.exc_info` or None.
+        .. versionadded:: 4.0
+        """
+        self._clear_tb_log()
+        return self._exc_info
+    def set_exc_info(self, exc_info):
+        """Sets the exception information of a ``Future.``
+        Preserves tracebacks on Python 2.
+        .. versionadded:: 4.0
+        """
+        self._exc_info = exc_info
+        self._log_traceback = True
+        if not _GC_CYCLE_FINALIZERS:
+            self._tb_logger = _TracebackLogger(exc_info)
+        try:
+            self._set_done()
+        finally:
+            if self._log_traceback and self._tb_logger is not None:
+                self._tb_logger.activate()
+        self._exc_info = exc_info
+    def _check_done(self):
+        if not self._done:
+            raise Exception("DummyFuture does not support blocking for results")
+    def _set_done(self):
+        self._done = True
+        for cb in self._callbacks:
+            try:
+                cb(self)
+            except Exception:
+                app_log.exception("Exception in callback %r for %r", cb, self)
+        self._callbacks = None
+    if _GC_CYCLE_FINALIZERS:
+        def __del__(self, is_finalizing=is_finalizing):
+            if is_finalizing() or not self._log_traceback:
+                return
+            tb = traceback.format_exception(*self._exc_info)
+            app_log.error(
+                "Future %r exception was never retrieved: %s",
+                self,
+                "".join(tb).rstrip(),
+            )
+TracebackFuture = Future
+if futures is None:
+    FUTURES = Future  # type: typing.Union[type, typing.Tuple[type, ...]]
+else:
+    FUTURES = (futures.Future, Future)
+def is_future(x):
+    return isinstance(x, FUTURES)
+class DummyExecutor(object):
+    def submit(self, fn, *args, **kwargs):
+        future = TracebackFuture()
+        try:
+            future.set_result(fn(*args, **kwargs))
+        except Exception:
+            future.set_exc_info(sys.exc_info())
+        return future
+    def shutdown(self, wait=True):
+        pass
+dummy_executor = DummyExecutor()
+def run_on_executor(*args, **kwargs):
+    """Decorator to run a synchronous method asynchronously on an executor.
+    The decorated method may be called with a ``callback`` keyword
+    argument and returns a future.
+    The `.IOLoop` and executor to be used are determined by the ``io_loop``
+    and ``executor`` attributes of ``self``. To use different attributes,
+    pass keyword arguments to the decorator::
+        @run_on_executor(executor='_thread_pool')
+        def foo(self):
+            pass
+    .. versionchanged:: 4.2
+       Added keyword arguments to use alternative attributes.
+    """
+    def run_on_executor_decorator(fn):
+        executor = kwargs.get("executor", "executor")
+        io_loop = kwargs.get("io_loop", "io_loop")
+        @functools.wraps(fn)
+        def wrapper(self, *args, **kwargs):
+            callback = kwargs.pop("callback", None)
+            future = getattr(self, executor).submit(fn, self, *args, **kwargs)
+            if callback:
+                getattr(self, io_loop).add_future(
+                    future, lambda future: callback(future.result())
+                )
+            return future
+        return wrapper
+    if args and kwargs:
+        raise ValueError("cannot combine positional and keyword args")
+    if len(args) == 1:
+        return run_on_executor_decorator(args[0])
+    elif len(args) != 0:
+        raise ValueError("expected 1 argument, got %d", len(args))
+    return run_on_executor_decorator
+_NO_RESULT = object()
+def return_future(f):
+    """Decorator to make a function that returns via callback return a
+    `Future`.
+    The wrapped function should take a ``callback`` keyword argument
+    and invoke it with one argument when it has finished.  To signal failure,
+    the function can simply raise an exception (which will be
+    captured by the `.StackContext` and passed along to the ``Future``).
+    From the caller's perspective, the callback argument is optional.
+    If one is given, it will be invoked when the function is complete
+    with `Future.result()` as an argument.  If the function fails, the
+    callback will not be run and an exception will be raised into the
+    surrounding `.StackContext`.
+    If no callback is given, the caller should use the ``Future`` to
+    wait for the function to complete (perhaps by yielding it in a
+    `.gen.engine` function, or passing it to `.IOLoop.add_future`).
+    Usage:
+    .. testcode::
+        @return_future
+        def future_func(arg1, arg2, callback):
+            callback(result)
+        @gen.engine
+        def caller(callback):
+            yield future_func(arg1, arg2)
+            callback()
+    ..
+    Note that ``@return_future`` and ``@gen.engine`` can be applied to the
+    same function, provided ``@return_future`` appears first.  However,
+    consider using ``@gen.coroutine`` instead of this combination.
+    """
+    replacer = ArgReplacer(f, "callback")
+    @functools.wraps(f)
+    def wrapper(*args, **kwargs):
+        future = TracebackFuture()
+        callback, args, kwargs = replacer.replace(
+            lambda value=_NO_RESULT: future.set_result(value), args, kwargs
+        )
+        def handle_error(typ, value, tb):
+            future.set_exc_info((typ, value, tb))
+            return True
+        exc_info = None
+        with ExceptionStackContext(handle_error):
+            try:
+                result = f(*args, **kwargs)
+                if result is not None:
+                    raise ReturnValueIgnoredError(
+                        "@return_future should not be used with functions "
+                        "that return values"
+                    )
+            except:
+                exc_info = sys.exc_info()
+                raise
+        if exc_info is not None:
+            future.result()
+        if callback is not None:
+            def run_callback(future):
+                result = future.result()
+                if result is _NO_RESULT:
+                    callback()
+                else:
+                    callback(future.result())
+            future.add_done_callback(wrap(run_callback))
+        return future
+    return wrapper
+def chain_future(a, b):
+    """Chain two futures together so that when one completes, so does the other.
+    The result (success or failure) of ``a`` will be copied to ``b``, unless
+    ``b`` has already been completed or cancelled by the time ``a`` finishes.
+    """
+    def copy(future):
+        assert future is a
+        if b.done():
+            return
+        if (
+            isinstance(a, TracebackFuture)
+            and isinstance(b, TracebackFuture)
+            and a.exc_info() is not None
+        ):
+            b.set_exc_info(a.exc_info())
+        elif a.exception() is not None:
+            b.set_exception(a.exception())
+        else:
+            b.set_result(a.result())
+    a.add_done_callback(copy)

--- a//dev/null
+++ b/salt/ext/tornado/curl_httpclient.py
@@ -0,0 +1,367 @@
+"""Non-blocking HTTP client implementation using pycurl."""
+from __future__ import absolute_import, division, print_function
+import collections
+import functools
+import logging
+import pycurl  # type: ignore
+import threading
+import time
+from io import BytesIO
+from salt.ext.tornado import httputil
+from salt.ext.tornado import ioloop
+from salt.ext.tornado import stack_context
+from salt.ext.tornado.escape import utf8, native_str
+from salt.ext.tornado.httpclient import HTTPResponse, HTTPError, AsyncHTTPClient, main
+curl_log = logging.getLogger('tornado.curl_httpclient')
+class CurlAsyncHTTPClient(AsyncHTTPClient):
+    def initialize(self, io_loop, max_clients=10, defaults=None):
+        super(CurlAsyncHTTPClient, self).initialize(io_loop, defaults=defaults)
+        self._multi = pycurl.CurlMulti()
+        self._multi.setopt(pycurl.M_TIMERFUNCTION, self._set_timeout)
+        self._multi.setopt(pycurl.M_SOCKETFUNCTION, self._handle_socket)
+        self._curls = [self._curl_create() for i in range(max_clients)]
+        self._free_list = self._curls[:]
+        self._requests = collections.deque()
+        self._fds = {}
+        self._timeout = None
+        self._force_timeout_callback = ioloop.PeriodicCallback(
+            self._handle_force_timeout, 1000, io_loop=io_loop)
+        self._force_timeout_callback.start()
+        dummy_curl_handle = pycurl.Curl()
+        self._multi.add_handle(dummy_curl_handle)
+        self._multi.remove_handle(dummy_curl_handle)
+    def close(self):
+        self._force_timeout_callback.stop()
+        if self._timeout is not None:
+            self.io_loop.remove_timeout(self._timeout)
+        for curl in self._curls:
+            curl.close()
+        self._multi.close()
+        super(CurlAsyncHTTPClient, self).close()
+    def fetch_impl(self, request, callback):
+        self._requests.append((request, callback))
+        self._process_queue()
+        self._set_timeout(0)
+    def _handle_socket(self, event, fd, multi, data):
+        """Called by libcurl when it wants to change the file descriptors
+        it cares about.
+        """
+        event_map = {
+            pycurl.POLL_NONE: ioloop.IOLoop.NONE,
+            pycurl.POLL_IN: ioloop.IOLoop.READ,
+            pycurl.POLL_OUT: ioloop.IOLoop.WRITE,
+            pycurl.POLL_INOUT: ioloop.IOLoop.READ | ioloop.IOLoop.WRITE
+        }
+        if event == pycurl.POLL_REMOVE:
+            if fd in self._fds:
+                self.io_loop.remove_handler(fd)
+                del self._fds[fd]
+        else:
+            ioloop_event = event_map[event]
+            if fd in self._fds:
+                self.io_loop.remove_handler(fd)
+            self.io_loop.add_handler(fd, self._handle_events,
+                                     ioloop_event)
+            self._fds[fd] = ioloop_event
+    def _set_timeout(self, msecs):
+        """Called by libcurl to schedule a timeout."""
+        if self._timeout is not None:
+            self.io_loop.remove_timeout(self._timeout)
+        self._timeout = self.io_loop.add_timeout(
+            self.io_loop.time() + msecs / 1000.0, self._handle_timeout)
+    def _handle_events(self, fd, events):
+        """Called by IOLoop when there is activity on one of our
+        file descriptors.
+        """
+        action = 0
+        if events & ioloop.IOLoop.READ:
+            action |= pycurl.CSELECT_IN
+        if events & ioloop.IOLoop.WRITE:
+            action |= pycurl.CSELECT_OUT
+        while True:
+            try:
+                ret, num_handles = self._multi.socket_action(fd, action)
+            except pycurl.error as e:
+                ret = e.args[0]
+            if ret != pycurl.E_CALL_MULTI_PERFORM:
+                break
+        self._finish_pending_requests()
+    def _handle_timeout(self):
+        """Called by IOLoop when the requested timeout has passed."""
+        with stack_context.NullContext():
+            self._timeout = None
+            while True:
+                try:
+                    ret, num_handles = self._multi.socket_action(
+                        pycurl.SOCKET_TIMEOUT, 0)
+                except pycurl.error as e:
+                    ret = e.args[0]
+                if ret != pycurl.E_CALL_MULTI_PERFORM:
+                    break
+            self._finish_pending_requests()
+        new_timeout = self._multi.timeout()
+        if new_timeout >= 0:
+            self._set_timeout(new_timeout)
+    def _handle_force_timeout(self):
+        """Called by IOLoop periodically to ask libcurl to process any
+        events it may have forgotten about.
+        """
+        with stack_context.NullContext():
+            while True:
+                try:
+                    ret, num_handles = self._multi.socket_all()
+                except pycurl.error as e:
+                    ret = e.args[0]
+                if ret != pycurl.E_CALL_MULTI_PERFORM:
+                    break
+            self._finish_pending_requests()
+    def _finish_pending_requests(self):
+        """Process any requests that were completed by the last
+        call to multi.socket_action.
+        """
+        while True:
+            num_q, ok_list, err_list = self._multi.info_read()
+            for curl in ok_list:
+                self._finish(curl)
+            for curl, errnum, errmsg in err_list:
+                self._finish(curl, errnum, errmsg)
+            if num_q == 0:
+                break
+        self._process_queue()
+    def _process_queue(self):
+        with stack_context.NullContext():
+            while True:
+                started = 0
+                while self._free_list and self._requests:
+                    started += 1
+                    curl = self._free_list.pop()
+                    (request, callback) = self._requests.popleft()
+                    curl.info = {
+                        "headers": httputil.HTTPHeaders(),
+                        "buffer": BytesIO(),
+                        "request": request,
+                        "callback": callback,
+                        "curl_start_time": time.time(),
+                    }
+                    try:
+                        self._curl_setup_request(
+                            curl, request, curl.info["buffer"],
+                            curl.info["headers"])
+                    except Exception as e:
+                        self._free_list.append(curl)
+                        callback(HTTPResponse(
+                            request=request,
+                            code=599,
+                            error=e))
+                    else:
+                        self._multi.add_handle(curl)
+                if not started:
+                    break
+    def _finish(self, curl, curl_error=None, curl_message=None):
+        info = curl.info
+        curl.info = None
+        self._multi.remove_handle(curl)
+        self._free_list.append(curl)
+        buffer = info["buffer"]
+        if curl_error:
+            error = CurlError(curl_error, curl_message)
+            code = error.code
+            effective_url = None
+            buffer.close()
+            buffer = None
+        else:
+            error = None
+            code = curl.getinfo(pycurl.HTTP_CODE)
+            effective_url = curl.getinfo(pycurl.EFFECTIVE_URL)
+            buffer.seek(0)
+        time_info = dict(
+            queue=info["curl_start_time"] - info["request"].start_time,
+            namelookup=curl.getinfo(pycurl.NAMELOOKUP_TIME),
+            connect=curl.getinfo(pycurl.CONNECT_TIME),
+            pretransfer=curl.getinfo(pycurl.PRETRANSFER_TIME),
+            starttransfer=curl.getinfo(pycurl.STARTTRANSFER_TIME),
+            total=curl.getinfo(pycurl.TOTAL_TIME),
+            redirect=curl.getinfo(pycurl.REDIRECT_TIME),
+        )
+        try:
+            info["callback"](HTTPResponse(
+                request=info["request"], code=code, headers=info["headers"],
+                buffer=buffer, effective_url=effective_url, error=error,
+                reason=info['headers'].get("X-Http-Reason", None),
+                request_time=time.time() - info["curl_start_time"],
+                time_info=time_info))
+        except Exception:
+            self.handle_callback_exception(info["callback"])
+    def handle_callback_exception(self, callback):
+        self.io_loop.handle_callback_exception(callback)
+    def _curl_create(self):
+        curl = pycurl.Curl()
+        if curl_log.isEnabledFor(logging.DEBUG):
+            curl.setopt(pycurl.VERBOSE, 1)
+            curl.setopt(pycurl.DEBUGFUNCTION, self._curl_debug)
+        if hasattr(pycurl, 'PROTOCOLS'):  # PROTOCOLS first appeared in pycurl 7.19.5 (2014-07-12)
+            curl.setopt(pycurl.PROTOCOLS, pycurl.PROTO_HTTP | pycurl.PROTO_HTTPS)
+            curl.setopt(pycurl.REDIR_PROTOCOLS, pycurl.PROTO_HTTP | pycurl.PROTO_HTTPS)
+        return curl
+    def _curl_setup_request(self, curl, request, buffer, headers):
+        curl.setopt(pycurl.URL, native_str(request.url))
+        if "Expect" not in request.headers:
+            request.headers["Expect"] = ""
+        if "Pragma" not in request.headers:
+            request.headers["Pragma"] = ""
+        curl.setopt(pycurl.HTTPHEADER,
+                    ["%s: %s" % (native_str(k), native_str(v))
+                     for k, v in request.headers.get_all()])
+        curl.setopt(pycurl.HEADERFUNCTION,
+                    functools.partial(self._curl_header_callback,
+                                      headers, request.header_callback))
+        if request.streaming_callback:
+            def write_function(chunk):
+                self.io_loop.add_callback(request.streaming_callback, chunk)
+        else:
+            write_function = buffer.write
+        if bytes is str:  # py2
+            curl.setopt(pycurl.WRITEFUNCTION, write_function)
+        else:  # py3
+            curl.setopt(pycurl.WRITEFUNCTION, lambda s: write_function(utf8(s)))
+        curl.setopt(pycurl.FOLLOWLOCATION, request.follow_redirects)
+        curl.setopt(pycurl.MAXREDIRS, request.max_redirects)
+        curl.setopt(pycurl.CONNECTTIMEOUT_MS, int(1000 * request.connect_timeout))
+        curl.setopt(pycurl.TIMEOUT_MS, int(1000 * request.request_timeout))
+        if request.user_agent:
+            curl.setopt(pycurl.USERAGENT, native_str(request.user_agent))
+        else:
+            curl.setopt(pycurl.USERAGENT, "Mozilla/5.0 (compatible; pycurl)")
+        if request.network_interface:
+            curl.setopt(pycurl.INTERFACE, request.network_interface)
+        if request.decompress_response:
+            curl.setopt(pycurl.ENCODING, "gzip,deflate")
+        else:
+            curl.setopt(pycurl.ENCODING, "none")
+        if request.proxy_host and request.proxy_port:
+            curl.setopt(pycurl.PROXY, request.proxy_host)
+            curl.setopt(pycurl.PROXYPORT, request.proxy_port)
+            if request.proxy_username:
+                credentials = '%s:%s' % (request.proxy_username,
+                                         request.proxy_password)
+                curl.setopt(pycurl.PROXYUSERPWD, credentials)
+            if (request.proxy_auth_mode is None or
+                    request.proxy_auth_mode == "basic"):
+                curl.setopt(pycurl.PROXYAUTH, pycurl.HTTPAUTH_BASIC)
+            elif request.proxy_auth_mode == "digest":
+                curl.setopt(pycurl.PROXYAUTH, pycurl.HTTPAUTH_DIGEST)
+            else:
+                raise ValueError(
+                    "Unsupported proxy_auth_mode %s" % request.proxy_auth_mode)
+        else:
+            curl.setopt(pycurl.PROXY, '')
+            curl.unsetopt(pycurl.PROXYUSERPWD)
+        if request.validate_cert:
+            curl.setopt(pycurl.SSL_VERIFYPEER, 1)
+            curl.setopt(pycurl.SSL_VERIFYHOST, 2)
+        else:
+            curl.setopt(pycurl.SSL_VERIFYPEER, 0)
+            curl.setopt(pycurl.SSL_VERIFYHOST, 0)
+        if request.ca_certs is not None:
+            curl.setopt(pycurl.CAINFO, request.ca_certs)
+        else:
+            pass
+        if request.allow_ipv6 is False:
+            curl.setopt(pycurl.IPRESOLVE, pycurl.IPRESOLVE_V4)
+        else:
+            curl.setopt(pycurl.IPRESOLVE, pycurl.IPRESOLVE_WHATEVER)
+        curl_options = {
+            "GET": pycurl.HTTPGET,
+            "POST": pycurl.POST,
+            "PUT": pycurl.UPLOAD,
+            "HEAD": pycurl.NOBODY,
+        }
+        custom_methods = set(["DELETE", "OPTIONS", "PATCH"])
+        for o in curl_options.values():
+            curl.setopt(o, False)
+        if request.method in curl_options:
+            curl.unsetopt(pycurl.CUSTOMREQUEST)
+            curl.setopt(curl_options[request.method], True)
+        elif request.allow_nonstandard_methods or request.method in custom_methods:
+            curl.setopt(pycurl.CUSTOMREQUEST, request.method)
+        else:
+            raise KeyError('unknown method ' + request.method)
+        body_expected = request.method in ("POST", "PATCH", "PUT")
+        body_present = request.body is not None
+        if not request.allow_nonstandard_methods:
+            if ((body_expected and not body_present) or
+                    (body_present and not body_expected)):
+                raise ValueError(
+                    'Body must %sbe None for method %s (unless '
+                    'allow_nonstandard_methods is true)' %
+                    ('not ' if body_expected else '', request.method))
+        if body_expected or body_present:
+            if request.method == "GET":
+                raise ValueError('Body must be None for GET request')
+            request_buffer = BytesIO(utf8(request.body or ''))
+            def ioctl(cmd):
+                if cmd == curl.IOCMD_RESTARTREAD:
+                    request_buffer.seek(0)
+            curl.setopt(pycurl.READFUNCTION, request_buffer.read)
+            curl.setopt(pycurl.IOCTLFUNCTION, ioctl)
+            if request.method == "POST":
+                curl.setopt(pycurl.POSTFIELDSIZE, len(request.body or ''))
+            else:
+                curl.setopt(pycurl.UPLOAD, True)
+                curl.setopt(pycurl.INFILESIZE, len(request.body or ''))
+        if request.auth_username is not None:
+            userpwd = "%s:%s" % (request.auth_username, request.auth_password or '')
+            if request.auth_mode is None or request.auth_mode == "basic":
+                curl.setopt(pycurl.HTTPAUTH, pycurl.HTTPAUTH_BASIC)
+            elif request.auth_mode == "digest":
+                curl.setopt(pycurl.HTTPAUTH, pycurl.HTTPAUTH_DIGEST)
+            else:
+                raise ValueError("Unsupported auth_mode %s" % request.auth_mode)
+            curl.setopt(pycurl.USERPWD, native_str(userpwd))
+            curl_log.debug("%s %s (username: %r)", request.method, request.url,
+                           request.auth_username)
+        else:
+            curl.unsetopt(pycurl.USERPWD)
+            curl_log.debug("%s %s", request.method, request.url)
+        if request.client_cert is not None:
+            curl.setopt(pycurl.SSLCERT, request.client_cert)
+        if request.client_key is not None:
+            curl.setopt(pycurl.SSLKEY, request.client_key)
+        if request.ssl_options is not None:
+            raise ValueError("ssl_options not supported in curl_httpclient")
+        if threading.activeCount() > 1:
+            curl.setopt(pycurl.NOSIGNAL, 1)
+        if request.prepare_curl_callback is not None:
+            request.prepare_curl_callback(curl)
+    def _curl_header_callback(self, headers, header_callback, header_line):
+        header_line = native_str(header_line.decode('latin1'))
+        if header_callback is not None:
+            self.io_loop.add_callback(header_callback, header_line)
+        header_line = header_line.rstrip()
+        if header_line.startswith("HTTP/"):
+            headers.clear()
+            try:
+                (__, __, reason) = httputil.parse_response_start_line(header_line)
+                header_line = "X-Http-Reason: %s" % reason
+            except httputil.HTTPInputError:
+                return
+        if not header_line:
+            return
+        headers.parse_line(header_line)
+    def _curl_debug(self, debug_type, debug_msg):
+        debug_types = ('I', '<', '>', '<', '>')
+        debug_msg = native_str(debug_msg)
+        if debug_type == 0:
+            curl_log.debug('%s', debug_msg.strip())
+        elif debug_type in (1, 2):
+            for line in debug_msg.splitlines():
+                curl_log.debug('%s %s', debug_types[debug_type], line)
+        elif debug_type == 4:
+            curl_log.debug('%s %r', debug_types[debug_type], debug_msg)
+class CurlError(HTTPError):
+    def __init__(self, errno, message):
+        HTTPError.__init__(self, 599, message)
+        self.errno = errno
+if __name__ == "__main__":
+    AsyncHTTPClient.configure(CurlAsyncHTTPClient)
+    main()

--- a//dev/null
+++ b/salt/ext/tornado/escape.py
@@ -0,0 +1,258 @@
+"""Escaping/unescaping methods for HTML, JSON, URLs, and others.
+Also includes a few other miscellaneous string manipulation functions that
+have crept in over time.
+"""
+from __future__ import absolute_import, division, print_function
+import json
+import re
+from salt.ext.tornado.util import PY3, unicode_type, basestring_type
+if PY3:
+    from urllib.parse import parse_qs as _parse_qs
+    import html.entities as htmlentitydefs
+    import urllib.parse as urllib_parse
+    unichr = chr
+else:
+    from urlparse import parse_qs as _parse_qs
+    import htmlentitydefs
+    import urllib as urllib_parse
+try:
+    import typing  # noqa
+except ImportError:
+    pass
+_XHTML_ESCAPE_RE = re.compile('[&<>"\']')
+_XHTML_ESCAPE_DICT = {'&': '&amp;', '<': '&lt;', '>': '&gt;', '"': '&quot;',
+                      '\'': '&#39;'}
+def xhtml_escape(value):
+    """Escapes a string so it is valid within HTML or XML.
+    Escapes the characters ``<``, ``>``, ``"``, ``'``, and ``&``.
+    When used in attribute values the escaped strings must be enclosed
+    in quotes.
+    .. versionchanged:: 3.2
+       Added the single quote to the list of escaped characters.
+    """
+    return _XHTML_ESCAPE_RE.sub(lambda match: _XHTML_ESCAPE_DICT[match.group(0)],
+                                to_basestring(value))
+def xhtml_unescape(value):
+    """Un-escapes an XML-escaped string."""
+    return re.sub(r"&(#?)(\w+?);", _convert_entity, _unicode(value))
+def json_encode(value):
+    """JSON-encodes the given Python object."""
+    return json.dumps(value).replace("</", "<\\/")
+def json_decode(value):
+    """Returns Python objects for the given JSON string."""
+    return json.loads(to_basestring(value))
+def squeeze(value):
+    """Replace all sequences of whitespace chars with a single space."""
+    return re.sub(r"[\x00-\x20]+", " ", value).strip()
+def url_escape(value, plus=True):
+    """Returns a URL-encoded version of the given value.
+    If ``plus`` is true (the default), spaces will be represented
+    as "+" instead of "%20".  This is appropriate for query strings
+    but not for the path component of a URL.  Note that this default
+    is the reverse of Python's urllib module.
+    .. versionadded:: 3.1
+        The ``plus`` argument
+    """
+    quote = urllib_parse.quote_plus if plus else urllib_parse.quote
+    return quote(utf8(value))
+if not PY3:
+    def url_unescape(value, encoding='utf-8', plus=True):
+        """Decodes the given value from a URL.
+        The argument may be either a byte or unicode string.
+        If encoding is None, the result will be a byte string.  Otherwise,
+        the result is a unicode string in the specified encoding.
+        If ``plus`` is true (the default), plus signs will be interpreted
+        as spaces (literal plus signs must be represented as "%2B").  This
+        is appropriate for query strings and form-encoded values but not
+        for the path component of a URL.  Note that this default is the
+        reverse of Python's urllib module.
+        .. versionadded:: 3.1
+           The ``plus`` argument
+        """
+        unquote = (urllib_parse.unquote_plus if plus else urllib_parse.unquote)
+        if encoding is None:
+            return unquote(utf8(value))
+        else:
+            return unicode_type(unquote(utf8(value)), encoding)
+    parse_qs_bytes = _parse_qs
+else:
+    def url_unescape(value, encoding='utf-8', plus=True):
+        """Decodes the given value from a URL.
+        The argument may be either a byte or unicode string.
+        If encoding is None, the result will be a byte string.  Otherwise,
+        the result is a unicode string in the specified encoding.
+        If ``plus`` is true (the default), plus signs will be interpreted
+        as spaces (literal plus signs must be represented as "%2B").  This
+        is appropriate for query strings and form-encoded values but not
+        for the path component of a URL.  Note that this default is the
+        reverse of Python's urllib module.
+        .. versionadded:: 3.1
+           The ``plus`` argument
+        """
+        if encoding is None:
+            if plus:
+                value = to_basestring(value).replace('+', ' ')
+            return urllib_parse.unquote_to_bytes(value)
+        else:
+            unquote = (urllib_parse.unquote_plus if plus
+                       else urllib_parse.unquote)
+            return unquote(to_basestring(value), encoding=encoding)
+    def parse_qs_bytes(qs, keep_blank_values=False, strict_parsing=False):
+        """Parses a query string like urlparse.parse_qs, but returns the
+        values as byte strings.
+        Keys still become type str (interpreted as latin1 in python3!)
+        because it's too painful to keep them as byte strings in
+        python3 and in practice they're nearly always ascii anyway.
+        """
+        result = _parse_qs(qs, keep_blank_values, strict_parsing,
+                           encoding='latin1', errors='strict')
+        encoded = {}
+        for k, v in result.items():
+            encoded[k] = [i.encode('latin1') for i in v]
+        return encoded
+_UTF8_TYPES = (bytes, type(None))
+def utf8(value):
+    """Converts a string argument to a byte string.
+    If the argument is already a byte string or None, it is returned unchanged.
+    Otherwise it must be a unicode string and is encoded as utf8.
+    """
+    if isinstance(value, _UTF8_TYPES):
+        return value
+    if not isinstance(value, unicode_type):
+        raise TypeError(
+            "Expected bytes, unicode, or None; got %r" % type(value)
+        )
+    return value.encode("utf-8")
+_TO_UNICODE_TYPES = (unicode_type, type(None))
+def to_unicode(value):
+    """Converts a string argument to a unicode string.
+    If the argument is already a unicode string or None, it is returned
+    unchanged.  Otherwise it must be a byte string and is decoded as utf8.
+    """
+    if isinstance(value, _TO_UNICODE_TYPES):
+        return value
+    if not isinstance(value, bytes):
+        raise TypeError(
+            "Expected bytes, unicode, or None; got %r" % type(value)
+        )
+    return value.decode("utf-8")
+_unicode = to_unicode
+if str is unicode_type:
+    native_str = to_unicode
+else:
+    native_str = utf8
+_BASESTRING_TYPES = (basestring_type, type(None))
+def to_basestring(value):
+    """Converts a string argument to a subclass of basestring.
+    In python2, byte and unicode strings are mostly interchangeable,
+    so functions that deal with a user-supplied argument in combination
+    with ascii string constants can use either and should return the type
+    the user supplied.  In python3, the two types are not interchangeable,
+    so this method is needed to convert byte strings to unicode.
+    """
+    if isinstance(value, _BASESTRING_TYPES):
+        return value
+    if not isinstance(value, bytes):
+        raise TypeError(
+            "Expected bytes, unicode, or None; got %r" % type(value)
+        )
+    return value.decode("utf-8")
+def recursive_unicode(obj):
+    """Walks a simple data structure, converting byte strings to unicode.
+    Supports lists, tuples, and dictionaries.
+    """
+    if isinstance(obj, dict):
+        return dict((recursive_unicode(k), recursive_unicode(v)) for (k, v) in obj.items())
+    elif isinstance(obj, list):
+        return list(recursive_unicode(i) for i in obj)
+    elif isinstance(obj, tuple):
+        return tuple(recursive_unicode(i) for i in obj)
+    elif isinstance(obj, bytes):
+        return to_unicode(obj)
+    else:
+        return obj
+_URL_RE = re.compile(to_unicode(r"""\b((?:([\w-]+):(/{1,3})|www[.])(?:(?:(?:[^\s&()]|&amp;|&quot;)*(?:[^!"#$%&'()*+,.:;<=>?@\[\]^`{|}~\s]))|(?:\((?:[^\s&()]|&amp;|&quot;)*\)))+)"""))
+def linkify(text, shorten=False, extra_params="",
+            require_protocol=False, permitted_protocols=["http", "https"]):
+    """Converts plain text into HTML with links.
+    For example: ``linkify("Hello http://tornadoweb.org!")`` would return
+    ``Hello <a href="http://tornadoweb.org">http://tornadoweb.org</a>!``
+    Parameters:
+    * ``shorten``: Long urls will be shortened for display.
+    * ``extra_params``: Extra text to include in the link tag, or a callable
+        taking the link as an argument and returning the extra text
+        e.g. ``linkify(text, extra_params='rel="nofollow" class="external"')``,
+        or::
+            def extra_params_cb(url):
+                if url.startswith("http://example.com"):
+                    return 'class="internal"'
+                else:
+                    return 'class="external" rel="nofollow"'
+            linkify(text, extra_params=extra_params_cb)
+    * ``require_protocol``: Only linkify urls which include a protocol. If
+        this is False, urls such as www.facebook.com will also be linkified.
+    * ``permitted_protocols``: List (or set) of protocols which should be
+        linkified, e.g. ``linkify(text, permitted_protocols=["http", "ftp",
+        "mailto"])``. It is very unsafe to include protocols such as
+        ``javascript``.
+    """
+    if extra_params and not callable(extra_params):
+        extra_params = " " + extra_params.strip()
+    def make_link(m):
+        url = m.group(1)
+        proto = m.group(2)
+        if require_protocol and not proto:
+            return url  # not protocol, no linkify
+        if proto and proto not in permitted_protocols:
+            return url  # bad protocol, no linkify
+        href = m.group(1)
+        if not proto:
+            href = "http://" + href   # no proto specified, use http
+        if callable(extra_params):
+            params = " " + extra_params(href).strip()
+        else:
+            params = extra_params
+        max_len = 30
+        if shorten and len(url) > max_len:
+            before_clip = url
+            if proto:
+                proto_len = len(proto) + 1 + len(m.group(3) or "")  # +1 for :
+            else:
+                proto_len = 0
+            parts = url[proto_len:].split("/")
+            if len(parts) > 1:
+                url = url[:proto_len] + parts[0] + "/" + \
+                    parts[1][:8].split('?')[0].split('.')[0]
+            if len(url) > max_len * 1.5:  # still too long
+                url = url[:max_len]
+            if url != before_clip:
+                amp = url.rfind('&')
+                if amp > max_len - 5:
+                    url = url[:amp]
+                url += "..."
+                if len(url) >= len(before_clip):
+                    url = before_clip
+                else:
+                    params += ' title="%s"' % href
+        return u'<a href="%s"%s>%s</a>' % (href, params, url)
+    text = _unicode(xhtml_escape(text))
+    return _URL_RE.sub(make_link, text)
+def _convert_entity(m):
+    if m.group(1) == "#":
+        try:
+            if m.group(2)[:1].lower() == 'x':
+                return unichr(int(m.group(2)[1:], 16))
+            else:
+                return unichr(int(m.group(2)))
+        except ValueError:
+            return "&#%s;" % m.group(2)
+    try:
+        return _HTML_UNICODE_MAP[m.group(2)]
+    except KeyError:
+        return "&%s;" % m.group(2)
+def _build_unicode_map():
+    unicode_map = {}
+    for name, value in htmlentitydefs.name2codepoint.items():
+        unicode_map[name] = unichr(value)
+    return unicode_map
+_HTML_UNICODE_MAP = _build_unicode_map()

--- a//dev/null
+++ b/salt/ext/tornado/gen.py
@@ -0,0 +1,962 @@
+"""``tornado.gen`` is a generator-based interface to make it easier to
+work in an asynchronous environment.  Code using the ``gen`` module
+is technically asynchronous, but it is written as a single generator
+instead of a collection of separate functions.
+For example, the following asynchronous handler:
+.. testcode::
+    class AsyncHandler(RequestHandler):
+        @asynchronous
+        def get(self):
+            http_client = AsyncHTTPClient()
+            http_client.fetch("http://example.com",
+                              callback=self.on_fetch)
+        def on_fetch(self, response):
+            do_something_with_response(response)
+            self.render("template.html")
+.. testoutput::
+   :hide:
+could be written with ``gen`` as:
+.. testcode::
+    class GenAsyncHandler(RequestHandler):
+        @gen.coroutine
+        def get(self):
+            http_client = AsyncHTTPClient()
+            response = yield http_client.fetch("http://example.com")
+            do_something_with_response(response)
+            self.render("template.html")
+.. testoutput::
+   :hide:
+Most asynchronous functions in Tornado return a `.Future`;
+yielding this object returns its `~.Future.result`.
+You can also yield a list or dict of ``Futures``, which will be
+started at the same time and run in parallel; a list or dict of results will
+be returned when they are all finished:
+.. testcode::
+    @gen.coroutine
+    def get(self):
+        http_client = AsyncHTTPClient()
+        response1, response2 = yield [http_client.fetch(url1),
+                                      http_client.fetch(url2)]
+        response_dict = yield dict(response3=http_client.fetch(url3),
+                                   response4=http_client.fetch(url4))
+        response3 = response_dict['response3']
+        response4 = response_dict['response4']
+.. testoutput::
+   :hide:
+If the `~functools.singledispatch` library is available (standard in
+Python 3.4, available via the `singledispatch
+<https://pypi.python.org/pypi/singledispatch>`_ package on older
+versions), additional types of objects may be yielded. Tornado includes
+support for ``asyncio.Future`` and Twisted's ``Deferred`` class when
+``tornado.platform.asyncio`` and ``tornado.platform.twisted`` are imported.
+See the `convert_yielded` function to extend this mechanism.
+.. versionchanged:: 3.2
+   Dict support added.
+.. versionchanged:: 4.1
+   Support added for yielding ``asyncio`` Futures and Twisted Deferreds
+   via ``singledispatch``.
+"""
+from __future__ import absolute_import, division, print_function
+import collections
+import functools
+import itertools
+import os
+import sys
+import textwrap
+import types
+import weakref
+import salt.ext.tornado as tornado
+from salt.ext.tornado.concurrent import Future, TracebackFuture, is_future, chain_future
+from salt.ext.tornado.ioloop import IOLoop
+from salt.ext.tornado.log import app_log
+from salt.ext.tornado import stack_context
+from salt.ext.tornado.util import PY3, raise_exc_info
+try:
+    try:
+        from functools import singledispatch  # type: ignore
+    except ImportError:
+        from singledispatch import singledispatch  # backport
+except ImportError:
+    if 'APPENGINE_RUNTIME' not in os.environ:
+        raise
+    singledispatch = None
+try:
+    try:
+        from collections.abc import Generator as GeneratorType  # type: ignore
+    except ImportError:
+        from salt.ext.backports_abc import Generator as GeneratorType  # type: ignore
+    try:
+        from inspect import isawaitable  # type: ignore
+    except ImportError:
+        from salt.ext.backports_abc import isawaitable
+except ImportError:
+    if 'APPENGINE_RUNTIME' not in os.environ:
+        raise
+    from types import GeneratorType
+    def isawaitable(x):  # type: ignore
+        return False
+if PY3:
+    import builtins
+else:
+    import __builtin__ as builtins
+class KeyReuseError(Exception):
+    pass
+class UnknownKeyError(Exception):
+    pass
+class LeakedCallbackError(Exception):
+    pass
+class BadYieldError(Exception):
+    pass
+class ReturnValueIgnoredError(Exception):
+    pass
+class TimeoutError(Exception):
+    """Exception raised by ``with_timeout``."""
+def _value_from_stopiteration(e):
+    try:
+        return e.value
+    except AttributeError:
+        pass
+    try:
+        return e.args[0]
+    except (AttributeError, IndexError):
+        return None
+def engine(func):
+    """Callback-oriented decorator for asynchronous generators.
+    This is an older interface; for new code that does not need to be
+    compatible with versions of Tornado older than 3.0 the
+    `coroutine` decorator is recommended instead.
+    This decorator is similar to `coroutine`, except it does not
+    return a `.Future` and the ``callback`` argument is not treated
+    specially.
+    In most cases, functions decorated with `engine` should take
+    a ``callback`` argument and invoke it with their result when
+    they are finished.  One notable exception is the
+    `~tornado.web.RequestHandler` :ref:`HTTP verb methods <verbs>`,
+    which use ``self.finish()`` in place of a callback argument.
+    """
+    func = _make_coroutine_wrapper(func, replace_callback=False)
+    @functools.wraps(func)
+    def wrapper(*args, **kwargs):
+        future = func(*args, **kwargs)
+        def final_callback(future):
+            if future.result() is not None:
+                raise ReturnValueIgnoredError(
+                    "@gen.engine functions cannot return values: %r" %
+                    (future.result(),))
+        future.add_done_callback(stack_context.wrap(final_callback))
+    return wrapper
+def coroutine(func, replace_callback=True):
+    """Decorator for asynchronous generators.
+    Any generator that yields objects from this module must be wrapped
+    in either this decorator or `engine`.
+    Coroutines may "return" by raising the special exception
+    `Return(value) <Return>`.  In Python 3.3+, it is also possible for
+    the function to simply use the ``return value`` statement (prior to
+    Python 3.3 generators were not allowed to also return values).
+    In all versions of Python a coroutine that simply wishes to exit
+    early may use the ``return`` statement without a value.
+    Functions with this decorator return a `.Future`.  Additionally,
+    they may be called with a ``callback`` keyword argument, which
+    will be invoked with the future's result when it resolves.  If the
+    coroutine fails, the callback will not be run and an exception
+    will be raised into the surrounding `.StackContext`.  The
+    ``callback`` argument is not visible inside the decorated
+    function; it is handled by the decorator itself.
+    From the caller's perspective, ``@gen.coroutine`` is similar to
+    the combination of ``@return_future`` and ``@gen.engine``.
+    .. warning::
+       When exceptions occur inside a coroutine, the exception
+       information will be stored in the `.Future` object. You must
+       examine the result of the `.Future` object, or the exception
+       may go unnoticed by your code. This means yielding the function
+       if called from another coroutine, using something like
+       `.IOLoop.run_sync` for top-level calls, or passing the `.Future`
+       to `.IOLoop.add_future`.
+    """
+    return _make_coroutine_wrapper(func, replace_callback=True)
+_futures_to_runners = weakref.WeakKeyDictionary()
+def _make_coroutine_wrapper(func, replace_callback):
+    """The inner workings of ``@gen.coroutine`` and ``@gen.engine``.
+    The two decorators differ in their treatment of the ``callback``
+    argument, so we cannot simply implement ``@engine`` in terms of
+    ``@coroutine``.
+    """
+    wrapped = func
+    if hasattr(types, 'coroutine'):
+        func = types.coroutine(func)
+    @functools.wraps(wrapped)
+    def wrapper(*args, **kwargs):
+        future = TracebackFuture()
+        if replace_callback and 'callback' in kwargs:
+            callback = kwargs.pop('callback')
+            IOLoop.current().add_future(
+                future, lambda future: callback(future.result()))
+        try:
+            result = func(*args, **kwargs)
+        except (Return, StopIteration) as e:
+            result = _value_from_stopiteration(e)
+        except Exception:
+            future.set_exc_info(sys.exc_info())
+            return future
+        else:
+            if isinstance(result, GeneratorType):
+                try:
+                    orig_stack_contexts = stack_context._state.contexts
+                    yielded = next(result)
+                    if stack_context._state.contexts is not orig_stack_contexts:
+                        yielded = TracebackFuture()
+                        yielded.set_exception(
+                            stack_context.StackContextInconsistentError(
+                                'stack_context inconsistency (probably caused '
+                                'by yield within a "with StackContext" block)'))
+                except (StopIteration, Return) as e:
+                    future.set_result(_value_from_stopiteration(e))
+                except Exception:
+                    future.set_exc_info(sys.exc_info())
+                else:
+                    _futures_to_runners[future] = Runner(result, future, yielded)
+                yielded = None
+                try:
+                    return future
+                finally:
+                    future = None
+        future.set_result(result)
+        return future
+    wrapper.__wrapped__ = wrapped
+    wrapper.__tornado_coroutine__ = True
+    return wrapper
+def is_coroutine_function(func):
+    """Return whether *func* is a coroutine function, i.e. a function
+    wrapped with `~.gen.coroutine`.
+    .. versionadded:: 4.5
+    """
+    return getattr(func, '__tornado_coroutine__', False)
+class Return(Exception):
+    """Special exception to return a value from a `coroutine`.
+    If this exception is raised, its value argument is used as the
+    result of the coroutine::
+        @gen.coroutine
+        def fetch_json(url):
+            response = yield AsyncHTTPClient().fetch(url)
+            raise gen.Return(json_decode(response.body))
+    In Python 3.3, this exception is no longer necessary: the ``return``
+    statement can be used directly to return a value (previously
+    ``yield`` and ``return`` with a value could not be combined in the
+    same function).
+    By analogy with the return statement, the value argument is optional,
+    but it is never necessary to ``raise gen.Return()``.  The ``return``
+    statement can be used with no arguments instead.
+    """
+    def __init__(self, value=None):
+        super(Return, self).__init__()
+        self.value = value
+        self.args = (value,)
+class WaitIterator(object):
+    """Provides an iterator to yield the results of futures as they finish.
+    Yielding a set of futures like this:
+    ``results = yield [future1, future2]``
+    pauses the coroutine until both ``future1`` and ``future2``
+    return, and then restarts the coroutine with the results of both
+    futures. If either future is an exception, the expression will
+    raise that exception and all the results will be lost.
+    If you need to get the result of each future as soon as possible,
+    or if you need the result of some futures even if others produce
+    errors, you can use ``WaitIterator``::
+      wait_iterator = gen.WaitIterator(future1, future2)
+      while not wait_iterator.done():
+          try:
+              result = yield wait_iterator.next()
+          except Exception as e:
+              print("Error {} from {}".format(e, wait_iterator.current_future))
+          else:
+              print("Result {} received from {} at {}".format(
+                  result, wait_iterator.current_future,
+                  wait_iterator.current_index))
+    Because results are returned as soon as they are available the
+    output from the iterator *will not be in the same order as the
+    input arguments*. If you need to know which future produced the
+    current result, you can use the attributes
+    ``WaitIterator.current_future``, or ``WaitIterator.current_index``
+    to get the index of the future from the input list. (if keyword
+    arguments were used in the construction of the `WaitIterator`,
+    ``current_index`` will use the corresponding keyword).
+    On Python 3.5, `WaitIterator` implements the async iterator
+    protocol, so it can be used with the ``async for`` statement (note
+    that in this version the entire iteration is aborted if any value
+    raises an exception, while the previous example can continue past
+    individual errors)::
+      async for result in gen.WaitIterator(future1, future2):
+          print("Result {} received from {} at {}".format(
+              result, wait_iterator.current_future,
+              wait_iterator.current_index))
+    .. versionadded:: 4.1
+    .. versionchanged:: 4.3
+       Added ``async for`` support in Python 3.5.
+    """
+    def __init__(self, *args, **kwargs):
+        if args and kwargs:
+            raise ValueError(
+                "You must provide args or kwargs, not both")
+        if kwargs:
+            self._unfinished = dict((f, k) for (k, f) in kwargs.items())
+            futures = list(kwargs.values())
+        else:
+            self._unfinished = dict((f, i) for (i, f) in enumerate(args))
+            futures = args
+        self._finished = collections.deque()
+        self.current_index = self.current_future = None
+        self._running_future = None
+        for future in futures:
+            future.add_done_callback(self._done_callback)
+    def done(self):
+        """Returns True if this iterator has no more results."""
+        if self._finished or self._unfinished:
+            return False
+        self.current_index = self.current_future = None
+        return True
+    def next(self):
+        """Returns a `.Future` that will yield the next available result.
+        Note that this `.Future` will not be the same object as any of
+        the inputs.
+        """
+        self._running_future = TracebackFuture()
+        if self._finished:
+            self._return_result(self._finished.popleft())
+        return self._running_future
+    def _done_callback(self, done):
+        if self._running_future and not self._running_future.done():
+            self._return_result(done)
+        else:
+            self._finished.append(done)
+    def _return_result(self, done):
+        """Called set the returned future's state that of the future
+        we yielded, and set the current future for the iterator.
+        """
+        chain_future(done, self._running_future)
+        self.current_future = done
+        self.current_index = self._unfinished.pop(done)
+    def __aiter__(self):
+        raise self
+    def __anext__(self):
+        if self.done():
+            raise getattr(builtins, 'StopAsyncIteration')()
+        return self.next()
+class YieldPoint(object):
+    """Base class for objects that may be yielded from the generator.
+    .. deprecated:: 4.0
+       Use `Futures <.Future>` instead.
+    """
+    def start(self, runner):
+        """Called by the runner after the generator has yielded.
+        No other methods will be called on this object before ``start``.
+        """
+        raise NotImplementedError()
+    def is_ready(self):
+        """Called by the runner to determine whether to resume the generator.
+        Returns a boolean; may be called more than once.
+        """
+        raise NotImplementedError()
+    def get_result(self):
+        """Returns the value to use as the result of the yield expression.
+        This method will only be called once, and only after `is_ready`
+        has returned true.
+        """
+        raise NotImplementedError()
+class Callback(YieldPoint):
+    """Returns a callable object that will allow a matching `Wait` to proceed.
+    The key may be any value suitable for use as a dictionary key, and is
+    used to match ``Callbacks`` to their corresponding ``Waits``.  The key
+    must be unique among outstanding callbacks within a single run of the
+    generator function, but may be reused across different runs of the same
+    function (so constants generally work fine).
+    The callback may be called with zero or one arguments; if an argument
+    is given it will be returned by `Wait`.
+    .. deprecated:: 4.0
+       Use `Futures <.Future>` instead.
+    """
+    def __init__(self, key):
+        self.key = key
+    def start(self, runner):
+        self.runner = runner
+        runner.register_callback(self.key)
+    def is_ready(self):
+        return True
+    def get_result(self):
+        return self.runner.result_callback(self.key)
+class Wait(YieldPoint):
+    """Returns the argument passed to the result of a previous `Callback`.
+    .. deprecated:: 4.0
+       Use `Futures <.Future>` instead.
+    """
+    def __init__(self, key):
+        self.key = key
+    def start(self, runner):
+        self.runner = runner
+    def is_ready(self):
+        return self.runner.is_ready(self.key)
+    def get_result(self):
+        return self.runner.pop_result(self.key)
+class WaitAll(YieldPoint):
+    """Returns the results of multiple previous `Callbacks <Callback>`.
+    The argument is a sequence of `Callback` keys, and the result is
+    a list of results in the same order.
+    `WaitAll` is equivalent to yielding a list of `Wait` objects.
+    .. deprecated:: 4.0
+       Use `Futures <.Future>` instead.
+    """
+    def __init__(self, keys):
+        self.keys = keys
+    def start(self, runner):
+        self.runner = runner
+    def is_ready(self):
+        return all(self.runner.is_ready(key) for key in self.keys)
+    def get_result(self):
+        return [self.runner.pop_result(key) for key in self.keys]
+def Task(func, *args, **kwargs):
+    """Adapts a callback-based asynchronous function for use in coroutines.
+    Takes a function (and optional additional arguments) and runs it with
+    those arguments plus a ``callback`` keyword argument.  The argument passed
+    to the callback is returned as the result of the yield expression.
+    .. versionchanged:: 4.0
+       ``gen.Task`` is now a function that returns a `.Future`, instead of
+       a subclass of `YieldPoint`.  It still behaves the same way when
+       yielded.
+    """
+    future = Future()
+    def handle_exception(typ, value, tb):
+        if future.done():
+            return False
+        future.set_exc_info((typ, value, tb))
+        return True
+    def set_result(result):
+        if future.done():
+            return
+        future.set_result(result)
+    with stack_context.ExceptionStackContext(handle_exception):
+        func(*args, callback=_argument_adapter(set_result), **kwargs)
+    return future
+class YieldFuture(YieldPoint):
+    def __init__(self, future, io_loop=None):
+        """Adapts a `.Future` to the `YieldPoint` interface.
+        .. versionchanged:: 4.1
+           The ``io_loop`` argument is deprecated.
+        """
+        self.future = future
+        self.io_loop = io_loop or IOLoop.current()
+    def start(self, runner):
+        if not self.future.done():
+            self.runner = runner
+            self.key = object()
+            runner.register_callback(self.key)
+            self.io_loop.add_future(self.future, runner.result_callback(self.key))
+        else:
+            self.runner = None
+            self.result_fn = self.future.result
+    def is_ready(self):
+        if self.runner is not None:
+            return self.runner.is_ready(self.key)
+        else:
+            return True
+    def get_result(self):
+        if self.runner is not None:
+            return self.runner.pop_result(self.key).result()
+        else:
+            return self.result_fn()
+def _contains_yieldpoint(children):
+    """Returns True if ``children`` contains any YieldPoints.
+    ``children`` may be a dict or a list, as used by `MultiYieldPoint`
+    and `multi_future`.
+    """
+    if isinstance(children, dict):
+        return any(isinstance(i, YieldPoint) for i in children.values())
+    if isinstance(children, list):
+        return any(isinstance(i, YieldPoint) for i in children)
+    return False
+def multi(children, quiet_exceptions=()):
+    """Runs multiple asynchronous operations in parallel.
+    ``children`` may either be a list or a dict whose values are
+    yieldable objects. ``multi()`` returns a new yieldable
+    object that resolves to a parallel structure containing their
+    results. If ``children`` is a list, the result is a list of
+    results in the same order; if it is a dict, the result is a dict
+    with the same keys.
+    That is, ``results = yield multi(list_of_futures)`` is equivalent
+    to::
+        results = []
+        for future in list_of_futures:
+            results.append(yield future)
+    If any children raise exceptions, ``multi()`` will raise the first
+    one. All others will be logged, unless they are of types
+    contained in the ``quiet_exceptions`` argument.
+    If any of the inputs are `YieldPoints <YieldPoint>`, the returned
+    yieldable object is a `YieldPoint`. Otherwise, returns a `.Future`.
+    This means that the result of `multi` can be used in a native
+    coroutine if and only if all of its children can be.
+    In a ``yield``-based coroutine, it is not normally necessary to
+    call this function directly, since the coroutine runner will
+    do it automatically when a list or dict is yielded. However,
+    it is necessary in ``await``-based coroutines, or to pass
+    the ``quiet_exceptions`` argument.
+    This function is available under the names ``multi()`` and ``Multi()``
+    for historical reasons.
+    .. versionchanged:: 4.2
+       If multiple yieldables fail, any exceptions after the first
+       (which is raised) will be logged. Added the ``quiet_exceptions``
+       argument to suppress this logging for selected exception types.
+    .. versionchanged:: 4.3
+       Replaced the class ``Multi`` and the function ``multi_future``
+       with a unified function ``multi``. Added support for yieldables
+       other than `YieldPoint` and `.Future`.
+    """
+    if _contains_yieldpoint(children):
+        return MultiYieldPoint(children, quiet_exceptions=quiet_exceptions)
+    else:
+        return multi_future(children, quiet_exceptions=quiet_exceptions)
+Multi = multi
+class MultiYieldPoint(YieldPoint):
+    """Runs multiple asynchronous operations in parallel.
+    This class is similar to `multi`, but it always creates a stack
+    context even when no children require it. It is not compatible with
+    native coroutines.
+    .. versionchanged:: 4.2
+       If multiple ``YieldPoints`` fail, any exceptions after the first
+       (which is raised) will be logged. Added the ``quiet_exceptions``
+       argument to suppress this logging for selected exception types.
+    .. versionchanged:: 4.3
+       Renamed from ``Multi`` to ``MultiYieldPoint``. The name ``Multi``
+       remains as an alias for the equivalent `multi` function.
+    .. deprecated:: 4.3
+       Use `multi` instead.
+    """
+    def __init__(self, children, quiet_exceptions=()):
+        self.keys = None
+        if isinstance(children, dict):
+            self.keys = list(children.keys())
+            children = children.values()
+        self.children = []
+        for i in children:
+            if not isinstance(i, YieldPoint):
+                i = convert_yielded(i)
+            if is_future(i):
+                i = YieldFuture(i)
+            self.children.append(i)
+        assert all(isinstance(i, YieldPoint) for i in self.children)
+        self.unfinished_children = set(self.children)
+        self.quiet_exceptions = quiet_exceptions
+    def start(self, runner):
+        for i in self.children:
+            i.start(runner)
+    def is_ready(self):
+        finished = list(itertools.takewhile(
+            lambda i: i.is_ready(), self.unfinished_children))
+        self.unfinished_children.difference_update(finished)
+        return not self.unfinished_children
+    def get_result(self):
+        result_list = []
+        exc_info = None
+        for f in self.children:
+            try:
+                result_list.append(f.get_result())
+            except Exception as e:
+                if exc_info is None:
+                    exc_info = sys.exc_info()
+                else:
+                    if not isinstance(e, self.quiet_exceptions):
+                        app_log.error("Multiple exceptions in yield list",
+                                      exc_info=True)
+        if exc_info is not None:
+            raise_exc_info(exc_info)
+        if self.keys is not None:
+            return dict(zip(self.keys, result_list))
+        else:
+            return list(result_list)
+def multi_future(children, quiet_exceptions=()):
+    """Wait for multiple asynchronous futures in parallel.
+    This function is similar to `multi`, but does not support
+    `YieldPoints <YieldPoint>`.
+    .. versionadded:: 4.0
+    .. versionchanged:: 4.2
+       If multiple ``Futures`` fail, any exceptions after the first (which is
+       raised) will be logged. Added the ``quiet_exceptions``
+       argument to suppress this logging for selected exception types.
+    .. deprecated:: 4.3
+       Use `multi` instead.
+    """
+    if isinstance(children, dict):
+        keys = list(children.keys())
+        children = children.values()
+    else:
+        keys = None
+    children = list(map(convert_yielded, children))
+    assert all(is_future(i) for i in children)
+    unfinished_children = set(children)
+    future = Future()
+    if not children:
+        future.set_result({} if keys is not None else [])
+    def callback(f):
+        unfinished_children.remove(f)
+        if not unfinished_children:
+            result_list = []
+            for f in children:
+                try:
+                    result_list.append(f.result())
+                except Exception as e:
+                    if future.done():
+                        if not isinstance(e, quiet_exceptions):
+                            app_log.error("Multiple exceptions in yield list",
+                                          exc_info=True)
+                    else:
+                        future.set_exc_info(sys.exc_info())
+            if not future.done():
+                if keys is not None:
+                    future.set_result(dict(zip(keys, result_list)))
+                else:
+                    future.set_result(result_list)
+    listening = set()
+    for f in children:
+        if f not in listening:
+            listening.add(f)
+            f.add_done_callback(callback)
+    return future
+def maybe_future(x):
+    """Converts ``x`` into a `.Future`.
+    If ``x`` is already a `.Future`, it is simply returned; otherwise
+    it is wrapped in a new `.Future`.  This is suitable for use as
+    ``result = yield gen.maybe_future(f())`` when you don't know whether
+    ``f()`` returns a `.Future` or not.
+    .. deprecated:: 4.3
+       This function only handles ``Futures``, not other yieldable objects.
+       Instead of `maybe_future`, check for the non-future result types
+       you expect (often just ``None``), and ``yield`` anything unknown.
+    """
+    if is_future(x):
+        return x
+    else:
+        fut = Future()
+        fut.set_result(x)
+        return fut
+def with_timeout(timeout, future, io_loop=None, quiet_exceptions=()):
+    """Wraps a `.Future` (or other yieldable object) in a timeout.
+    Raises `TimeoutError` if the input future does not complete before
+    ``timeout``, which may be specified in any form allowed by
+    `.IOLoop.add_timeout` (i.e. a `datetime.timedelta` or an absolute time
+    relative to `.IOLoop.time`)
+    If the wrapped `.Future` fails after it has timed out, the exception
+    will be logged unless it is of a type contained in ``quiet_exceptions``
+    (which may be an exception type or a sequence of types).
+    Does not support `YieldPoint` subclasses.
+    .. versionadded:: 4.0
+    .. versionchanged:: 4.1
+       Added the ``quiet_exceptions`` argument and the logging of unhandled
+       exceptions.
+    .. versionchanged:: 4.4
+       Added support for yieldable objects other than `.Future`.
+    """
+    future = convert_yielded(future)
+    result = Future()
+    chain_future(future, result)
+    if io_loop is None:
+        io_loop = IOLoop.current()
+    def error_callback(future):
+        try:
+            future.result()
+        except Exception as e:
+            if not isinstance(e, quiet_exceptions):
+                app_log.error("Exception in Future %r after timeout",
+                              future, exc_info=True)
+    def timeout_callback():
+        result.set_exception(TimeoutError("Timeout"))
+        future.add_done_callback(error_callback)
+    timeout_handle = io_loop.add_timeout(
+        timeout, timeout_callback)
+    if isinstance(future, Future):
+        future.add_done_callback(
+            lambda future: io_loop.remove_timeout(timeout_handle))
+    else:
+        io_loop.add_future(
+            future, lambda future: io_loop.remove_timeout(timeout_handle))
+    return result
+def sleep(duration):
+    """Return a `.Future` that resolves after the given number of seconds.
+    When used with ``yield`` in a coroutine, this is a non-blocking
+    analogue to `time.sleep` (which should not be used in coroutines
+    because it is blocking)::
+        yield gen.sleep(0.5)
+    Note that calling this function on its own does nothing; you must
+    wait on the `.Future` it returns (usually by yielding it).
+    .. versionadded:: 4.1
+    """
+    f = Future()
+    IOLoop.current().call_later(duration, lambda: f.set_result(None))
+    return f
+_null_future = Future()
+_null_future.set_result(None)
+moment = Future()
+moment.__doc__ = \
+    """A special object which may be yielded to allow the IOLoop to run for
+one iteration.
+This is not needed in normal use but it can be helpful in long-running
+coroutines that are likely to yield Futures that are ready instantly.
+Usage: ``yield gen.moment``
+.. versionadded:: 4.0
+.. deprecated:: 4.5
+   ``yield None`` is now equivalent to ``yield gen.moment``.
+"""
+moment.set_result(None)
+class Runner(object):
+    """Internal implementation of `tornado.gen.engine`.
+    Maintains information about pending callbacks and their results.
+    The results of the generator are stored in ``result_future`` (a
+    `.TracebackFuture`)
+    """
+    def __init__(self, gen, result_future, first_yielded):
+        self.gen = gen
+        self.result_future = result_future
+        self.future = _null_future
+        self.yield_point = None
+        self.pending_callbacks = None
+        self.results = None
+        self.running = False
+        self.finished = False
+        self.had_exception = False
+        self.io_loop = IOLoop.current()
+        self.stack_context_deactivate = None
+        if self.handle_yield(first_yielded):
+            gen = result_future = first_yielded = None
+            self.run()
+    def register_callback(self, key):
+        """Adds ``key`` to the list of callbacks."""
+        if self.pending_callbacks is None:
+            self.pending_callbacks = set()
+            self.results = {}
+        if key in self.pending_callbacks:
+            raise KeyReuseError("key %r is already pending" % (key,))
+        self.pending_callbacks.add(key)
+    def is_ready(self, key):
+        """Returns true if a result is available for ``key``."""
+        if self.pending_callbacks is None or key not in self.pending_callbacks:
+            raise UnknownKeyError("key %r is not pending" % (key,))
+        return key in self.results
+    def set_result(self, key, result):
+        """Sets the result for ``key`` and attempts to resume the generator."""
+        self.results[key] = result
+        if self.yield_point is not None and self.yield_point.is_ready():
+            try:
+                self.future.set_result(self.yield_point.get_result())
+            except:
+                self.future.set_exc_info(sys.exc_info())
+            self.yield_point = None
+            self.run()
+    def pop_result(self, key):
+        """Returns the result for ``key`` and unregisters it."""
+        self.pending_callbacks.remove(key)
+        return self.results.pop(key)
+    def run(self):
+        """Starts or resumes the generator, running until it reaches a
+        yield point that is not ready.
+        """
+        if self.running or self.finished:
+            return
+        try:
+            self.running = True
+            while True:
+                future = self.future
+                if not future.done():
+                    return
+                self.future = None
+                try:
+                    orig_stack_contexts = stack_context._state.contexts
+                    exc_info = None
+                    try:
+                        value = future.result()
+                    except Exception:
+                        self.had_exception = True
+                        exc_info = sys.exc_info()
+                    future = None
+                    if exc_info is not None:
+                        try:
+                            yielded = self.gen.throw(*exc_info)
+                        finally:
+                            exc_info = None
+                    else:
+                        yielded = self.gen.send(value)
+                    if stack_context._state.contexts is not orig_stack_contexts:
+                        self.gen.throw(
+                            stack_context.StackContextInconsistentError(
+                                'stack_context inconsistency (probably caused '
+                                'by yield within a "with StackContext" block)'))
+                except (StopIteration, Return) as e:
+                    self.finished = True
+                    self.future = _null_future
+                    if self.pending_callbacks and not self.had_exception:
+                        raise LeakedCallbackError(
+                            "finished without waiting for callbacks %r" %
+                            self.pending_callbacks)
+                    self.result_future.set_result(_value_from_stopiteration(e))
+                    self.result_future = None
+                    self._deactivate_stack_context()
+                    return
+                except Exception:
+                    self.finished = True
+                    self.future = _null_future
+                    self.result_future.set_exc_info(sys.exc_info())
+                    self.result_future = None
+                    self._deactivate_stack_context()
+                    return
+                if not self.handle_yield(yielded):
+                    return
+                yielded = None
+        finally:
+            self.running = False
+    def handle_yield(self, yielded):
+        if _contains_yieldpoint(yielded):
+            yielded = multi(yielded)
+        if isinstance(yielded, YieldPoint):
+            self.future = TracebackFuture()
+            def start_yield_point():
+                try:
+                    yielded.start(self)
+                    if yielded.is_ready():
+                        self.future.set_result(
+                            yielded.get_result())
+                    else:
+                        self.yield_point = yielded
+                except Exception:
+                    self.future = TracebackFuture()
+                    self.future.set_exc_info(sys.exc_info())
+            if self.stack_context_deactivate is None:
+                with stack_context.ExceptionStackContext(
+                        self.handle_exception) as deactivate:
+                    self.stack_context_deactivate = deactivate
+                    def cb():
+                        start_yield_point()
+                        self.run()
+                    self.io_loop.add_callback(cb)
+                    return False
+            else:
+                start_yield_point()
+        else:
+            try:
+                self.future = convert_yielded(yielded)
+            except BadYieldError:
+                self.future = TracebackFuture()
+                self.future.set_exc_info(sys.exc_info())
+        if not self.future.done() or self.future is moment:
+            def inner(f):
+                f = None # noqa
+                self.run()
+            self.io_loop.add_future(
+                self.future, inner)
+            return False
+        return True
+    def result_callback(self, key):
+        return stack_context.wrap(_argument_adapter(
+            functools.partial(self.set_result, key)))
+    def handle_exception(self, typ, value, tb):
+        if not self.running and not self.finished:
+            self.future = TracebackFuture()
+            self.future.set_exc_info((typ, value, tb))
+            self.run()
+            return True
+        else:
+            return False
+    def _deactivate_stack_context(self):
+        if self.stack_context_deactivate is not None:
+            self.stack_context_deactivate()
+            self.stack_context_deactivate = None
+Arguments = collections.namedtuple('Arguments', ['args', 'kwargs'])
+def _argument_adapter(callback):
+    """Returns a function that when invoked runs ``callback`` with one arg.
+    If the function returned by this function is called with exactly
+    one argument, that argument is passed to ``callback``.  Otherwise
+    the args tuple and kwargs dict are wrapped in an `Arguments` object.
+    """
+    def wrapper(*args, **kwargs):
+        if kwargs or len(args) > 1:
+            callback(Arguments(args, kwargs))
+        elif args:
+            callback(args[0])
+        else:
+            callback(None)
+    return wrapper
+if sys.version_info >= (3, 3):
+    exec(textwrap.dedent("""
+    @coroutine
+    def _wrap_awaitable(x):
+        if hasattr(x, '__await__'):
+            x = x.__await__()
+        return (yield from x)
+    """))
+else:
+    @coroutine
+    def _wrap_awaitable(x):
+        if hasattr(x, '__await__'):
+            _i = x.__await__()
+        else:
+            _i = iter(x)
+        try:
+            _y = next(_i)
+        except StopIteration as _e:
+            _r = _value_from_stopiteration(_e)
+        else:
+            while 1:
+                try:
+                    _s = yield _y
+                except GeneratorExit as _e:
+                    try:
+                        _m = _i.close
+                    except AttributeError:
+                        pass
+                    else:
+                        _m()
+                    raise _e
+                except BaseException as _e:
+                    _x = sys.exc_info()
+                    try:
+                        _m = _i.throw
+                    except AttributeError:
+                        raise _e
+                    else:
+                        try:
+                            _y = _m(*_x)
+                        except StopIteration as _e:
+                            _r = _value_from_stopiteration(_e)
+                            break
+                else:
+                    try:
+                        if _s is None:
+                            _y = next(_i)
+                        else:
+                            _y = _i.send(_s)
+                    except StopIteration as _e:
+                        _r = _value_from_stopiteration(_e)
+                        break
+        raise Return(_r)
+def convert_yielded(yielded):
+    """Convert a yielded object into a `.Future`.
+    The default implementation accepts lists, dictionaries, and Futures.
+    If the `~functools.singledispatch` library is available, this function
+    may be extended to support additional types. For example::
+        @convert_yielded.register(asyncio.Future)
+        def _(asyncio_future):
+            return tornado.platform.asyncio.to_tornado_future(asyncio_future)
+    .. versionadded:: 4.1
+    """
+    if yielded is None:
+        return moment
+    elif isinstance(yielded, (list, dict)):
+        return multi(yielded)
+    elif is_future(yielded):
+        return yielded
+    elif isawaitable(yielded):
+        return _wrap_awaitable(yielded)
+    else:
+        raise BadYieldError("yielded unknown object %r" % (yielded,))
+if singledispatch is not None:
+    convert_yielded = singledispatch(convert_yielded)
+    try:
+        import salt.ext.tornado.platform.asyncio
+    except ImportError:
+        pass
+    else:
+        tornado

--- a//dev/null
+++ b/salt/ext/tornado/http1connection.py
@@ -0,0 +1,572 @@
+"""Client and server implementations of HTTP/1.x.
+.. versionadded:: 4.0
+"""
+from __future__ import absolute_import, division, print_function
+import re
+from salt.ext.tornado.concurrent import Future
+from salt.ext.tornado.escape import native_str, utf8
+from salt.ext.tornado import gen
+from salt.ext.tornado import httputil
+from salt.ext.tornado import iostream
+from salt.ext.tornado.log import gen_log, app_log
+from salt.ext.tornado import stack_context
+from salt.ext.tornado.util import GzipDecompressor, PY3
+class _QuietException(Exception):
+    def __init__(self):
+        pass
+class _ExceptionLoggingContext(object):
+    """Used with the ``with`` statement when calling delegate methods to
+    log any exceptions with the given logger.  Any exceptions caught are
+    converted to _QuietException
+    """
+    def __init__(self, logger):
+        self.logger = logger
+    def __enter__(self):
+        pass
+    def __exit__(self, typ, value, tb):
+        if value is not None:
+            self.logger.error("Uncaught exception", exc_info=(typ, value, tb))
+            raise _QuietException
+class HTTP1ConnectionParameters(object):
+    """Parameters for `.HTTP1Connection` and `.HTTP1ServerConnection`.
+    """
+    def __init__(self, no_keep_alive=False, chunk_size=None,
+                 max_header_size=None, header_timeout=None, max_body_size=None,
+                 body_timeout=None, decompress=False):
+        """
+        :arg bool no_keep_alive: If true, always close the connection after
+            one request.
+        :arg int chunk_size: how much data to read into memory at once
+        :arg int max_header_size:  maximum amount of data for HTTP headers
+        :arg float header_timeout: how long to wait for all headers (seconds)
+        :arg int max_body_size: maximum amount of data for body
+        :arg float body_timeout: how long to wait while reading body (seconds)
+        :arg bool decompress: if true, decode incoming
+            ``Content-Encoding: gzip``
+        """
+        self.no_keep_alive = no_keep_alive
+        self.chunk_size = chunk_size or 65536
+        self.max_header_size = max_header_size or 65536
+        self.header_timeout = header_timeout
+        self.max_body_size = max_body_size
+        self.body_timeout = body_timeout
+        self.decompress = decompress
+class HTTP1Connection(httputil.HTTPConnection):
+    """Implements the HTTP/1.x protocol.
+    This class can be on its own for clients, or via `HTTP1ServerConnection`
+    for servers.
+    """
+    def __init__(self, stream, is_client, params=None, context=None):
+        """
+        :arg stream: an `.IOStream`
+        :arg bool is_client: client or server
+        :arg params: a `.HTTP1ConnectionParameters` instance or ``None``
+        :arg context: an opaque application-defined object that can be accessed
+            as ``connection.context``.
+        """
+        self.is_client = is_client
+        self.stream = stream
+        if params is None:
+            params = HTTP1ConnectionParameters()
+        self.params = params
+        self.context = context
+        self.no_keep_alive = params.no_keep_alive
+        self._max_body_size = (self.params.max_body_size or
+                               self.stream.max_buffer_size)
+        self._body_timeout = self.params.body_timeout
+        self._write_finished = False
+        self._read_finished = False
+        self._finish_future = Future()
+        self._disconnect_on_finish = False
+        self._clear_callbacks()
+        self._request_start_line = None
+        self._response_start_line = None
+        self._request_headers = None
+        self._chunking_output = None
+        self._expected_content_remaining = None
+        self._pending_write = None
+    def read_response(self, delegate):
+        """Read a single HTTP response.
+        Typical client-mode usage is to write a request using `write_headers`,
+        `write`, and `finish`, and then call ``read_response``.
+        :arg delegate: a `.HTTPMessageDelegate`
+        Returns a `.Future` that resolves to None after the full response has
+        been read.
+        """
+        if self.params.decompress:
+            delegate = _GzipMessageDelegate(delegate, self.params.chunk_size)
+        return self._read_message(delegate)
+    @gen.coroutine
+    def _read_message(self, delegate):
+        need_delegate_close = False
+        try:
+            header_future = self.stream.read_until_regex(
+                b"\r?\n\r?\n",
+                max_bytes=self.params.max_header_size)
+            if self.params.header_timeout is None:
+                header_data = yield header_future
+            else:
+                try:
+                    header_data = yield gen.with_timeout(
+                        self.stream.io_loop.time() + self.params.header_timeout,
+                        header_future,
+                        io_loop=self.stream.io_loop,
+                        quiet_exceptions=iostream.StreamClosedError)
+                except gen.TimeoutError:
+                    self.close()
+                    raise gen.Return(False)
+            start_line, headers = self._parse_headers(header_data)
+            if self.is_client:
+                start_line = httputil.parse_response_start_line(start_line)
+                self._response_start_line = start_line
+            else:
+                start_line = httputil.parse_request_start_line(start_line)
+                self._request_start_line = start_line
+                self._request_headers = headers
+            self._disconnect_on_finish = not self._can_keep_alive(
+                start_line, headers)
+            need_delegate_close = True
+            with _ExceptionLoggingContext(app_log):
+                header_future = delegate.headers_received(start_line, headers)
+                if header_future is not None:
+                    yield header_future
+            if self.stream is None:
+                need_delegate_close = False
+                raise gen.Return(False)
+            skip_body = False
+            if self.is_client:
+                if (self._request_start_line is not None and
+                        self._request_start_line.method == 'HEAD'):
+                    skip_body = True
+                code = start_line.code
+                if code == 304:
+                    skip_body = True
+                if code >= 100 and code < 200:
+                    if ('Content-Length' in headers or
+                            'Transfer-Encoding' in headers):
+                        raise httputil.HTTPInputError(
+                            "Response code %d cannot have body" % code)
+                    yield self._read_message(delegate)
+            else:
+                if (headers.get("Expect") == "100-continue" and
+                        not self._write_finished):
+                    self.stream.write(b"HTTP/1.1 100 (Continue)\r\n\r\n")
+            if not skip_body:
+                body_future = self._read_body(
+                    start_line.code if self.is_client else 0, headers, delegate)
+                if body_future is not None:
+                    if self._body_timeout is None:
+                        yield body_future
+                    else:
+                        try:
+                            yield gen.with_timeout(
+                                self.stream.io_loop.time() + self._body_timeout,
+                                body_future, self.stream.io_loop,
+                                quiet_exceptions=iostream.StreamClosedError)
+                        except gen.TimeoutError:
+                            gen_log.info("Timeout reading body from %s",
+                                         self.context)
+                            self.stream.close()
+                            raise gen.Return(False)
+            self._read_finished = True
+            if not self._write_finished or self.is_client:
+                need_delegate_close = False
+                with _ExceptionLoggingContext(app_log):
+                    delegate.finish()
+            if (not self._finish_future.done() and
+                    self.stream is not None and
+                    not self.stream.closed()):
+                self.stream.set_close_callback(self._on_connection_close)
+                yield self._finish_future
+            if self.is_client and self._disconnect_on_finish:
+                self.close()
+            if self.stream is None:
+                raise gen.Return(False)
+        except httputil.HTTPInputError as e:
+            gen_log.info("Malformed HTTP message from %s: %s",
+                         self.context, e)
+            self.close()
+            raise gen.Return(False)
+        finally:
+            if need_delegate_close:
+                with _ExceptionLoggingContext(app_log):
+                    delegate.on_connection_close()
+            header_future = None
+            self._clear_callbacks()
+        raise gen.Return(True)
+    def _clear_callbacks(self):
+        """Clears the callback attributes.
+        This allows the request handler to be garbage collected more
+        quickly in CPython by breaking up reference cycles.
+        """
+        self._write_callback = None
+        self._write_future = None
+        self._close_callback = None
+        if self.stream is not None:
+            self.stream.set_close_callback(None)
+    def set_close_callback(self, callback):
+        """Sets a callback that will be run when the connection is closed.
+        .. deprecated:: 4.0
+            Use `.HTTPMessageDelegate.on_connection_close` instead.
+        """
+        self._close_callback = stack_context.wrap(callback)
+    def _on_connection_close(self):
+        if self._close_callback is not None:
+            callback = self._close_callback
+            self._close_callback = None
+            callback()
+        if not self._finish_future.done():
+            self._finish_future.set_result(None)
+        self._clear_callbacks()
+    def close(self):
+        if self.stream is not None:
+            self.stream.close()
+        self._clear_callbacks()
+        if not self._finish_future.done():
+            self._finish_future.set_result(None)
+    def detach(self):
+        """Take control of the underlying stream.
+        Returns the underlying `.IOStream` object and stops all further
+        HTTP processing.  May only be called during
+        `.HTTPMessageDelegate.headers_received`.  Intended for implementing
+        protocols like websockets that tunnel over an HTTP handshake.
+        """
+        self._clear_callbacks()
+        stream = self.stream
+        self.stream = None
+        if not self._finish_future.done():
+            self._finish_future.set_result(None)
+        return stream
+    def set_body_timeout(self, timeout):
+        """Sets the body timeout for a single request.
+        Overrides the value from `.HTTP1ConnectionParameters`.
+        """
+        self._body_timeout = timeout
+    def set_max_body_size(self, max_body_size):
+        """Sets the body size limit for a single request.
+        Overrides the value from `.HTTP1ConnectionParameters`.
+        """
+        self._max_body_size = max_body_size
+    def write_headers(self, start_line, headers, chunk=None, callback=None):
+        """Implements `.HTTPConnection.write_headers`."""
+        lines = []
+        if self.is_client:
+            self._request_start_line = start_line
+            lines.append(utf8('%s %s HTTP/1.1' % (start_line[0], start_line[1])))
+            self._chunking_output = (
+                start_line.method in ('POST', 'PUT', 'PATCH') and
+                'Content-Length' not in headers and
+                'Transfer-Encoding' not in headers)
+        else:
+            self._response_start_line = start_line
+            lines.append(utf8('HTTP/1.1 %d %s' % (start_line[1], start_line[2])))
+            self._chunking_output = (
+                self._request_start_line.version == 'HTTP/1.1' and
+                start_line.code not in (204, 304) and
+                (start_line.code < 100 or start_line.code >= 200) and
+                'Content-Length' not in headers and
+                'Transfer-Encoding' not in headers)
+            if (self._request_start_line.version == 'HTTP/1.0' and
+                (self._request_headers.get('Connection', '').lower() ==
+                 'keep-alive')):
+                headers['Connection'] = 'Keep-Alive'
+        if self._chunking_output:
+            headers['Transfer-Encoding'] = 'chunked'
+        if (not self.is_client and
+            (self._request_start_line.method == 'HEAD' or
+             start_line.code == 304)):
+            self._expected_content_remaining = 0
+        elif 'Content-Length' in headers:
+            self._expected_content_remaining = int(headers['Content-Length'])
+        else:
+            self._expected_content_remaining = None
+        header_lines = (native_str(n) + ": " + native_str(v) for n, v in headers.get_all())
+        if PY3:
+            lines.extend(l.encode('latin1') for l in header_lines)
+        else:
+            lines.extend(header_lines)
+        for line in lines:
+            if b'\n' in line:
+                raise ValueError('Newline in header: ' + repr(line))
+        future = None
+        if self.stream.closed():
+            future = self._write_future = Future()
+            future.set_exception(iostream.StreamClosedError())
+            future.exception()
+        else:
+            if callback is not None:
+                self._write_callback = stack_context.wrap(callback)
+            else:
+                future = self._write_future = Future()
+            data = b"\r\n".join(lines) + b"\r\n\r\n"
+            if chunk:
+                data += self._format_chunk(chunk)
+            self._pending_write = self.stream.write(data)
+            self._pending_write.add_done_callback(self._on_write_complete)
+        return future
+    def _format_chunk(self, chunk):
+        if self._expected_content_remaining is not None:
+            self._expected_content_remaining -= len(chunk)
+            if self._expected_content_remaining < 0:
+                self.stream.close()
+                raise httputil.HTTPOutputError(
+                    "Tried to write more data than Content-Length")
+        if self._chunking_output and chunk:
+            return utf8("%x" % len(chunk)) + b"\r\n" + chunk + b"\r\n"
+        else:
+            return chunk
+    def write(self, chunk, callback=None):
+        """Implements `.HTTPConnection.write`.
+        For backwards compatibility is is allowed but deprecated to
+        skip `write_headers` and instead call `write()` with a
+        pre-encoded header block.
+        """
+        future = None
+        if self.stream.closed():
+            future = self._write_future = Future()
+            self._write_future.set_exception(iostream.StreamClosedError())
+            self._write_future.exception()
+        else:
+            if callback is not None:
+                self._write_callback = stack_context.wrap(callback)
+            else:
+                future = self._write_future = Future()
+            self._pending_write = self.stream.write(self._format_chunk(chunk))
+            self._pending_write.add_done_callback(self._on_write_complete)
+        return future
+    def finish(self):
+        """Implements `.HTTPConnection.finish`."""
+        if (self._expected_content_remaining is not None and
+                self._expected_content_remaining != 0 and
+                not self.stream.closed()):
+            self.stream.close()
+            raise httputil.HTTPOutputError(
+                "Tried to write %d bytes less than Content-Length" %
+                self._expected_content_remaining)
+        if self._chunking_output:
+            if not self.stream.closed():
+                self._pending_write = self.stream.write(b"0\r\n\r\n")
+                self._pending_write.add_done_callback(self._on_write_complete)
+        self._write_finished = True
+        if not self._read_finished:
+            self._disconnect_on_finish = True
+        self.stream.set_nodelay(True)
+        if self._pending_write is None:
+            self._finish_request(None)
+        else:
+            self._pending_write.add_done_callback(self._finish_request)
+    def _on_write_complete(self, future):
+        exc = future.exception()
+        if exc is not None and not isinstance(exc, iostream.StreamClosedError):
+            future.result()
+        if self._write_callback is not None:
+            callback = self._write_callback
+            self._write_callback = None
+            self.stream.io_loop.add_callback(callback)
+        if self._write_future is not None:
+            future = self._write_future
+            self._write_future = None
+            future.set_result(None)
+    def _can_keep_alive(self, start_line, headers):
+        if self.params.no_keep_alive:
+            return False
+        connection_header = headers.get("Connection")
+        if connection_header is not None:
+            connection_header = connection_header.lower()
+        if start_line.version == "HTTP/1.1":
+            return connection_header != "close"
+        elif ("Content-Length" in headers or
+              headers.get("Transfer-Encoding", "").lower() == "chunked" or
+              getattr(start_line, 'method', None) in ("HEAD", "GET")):
+            return connection_header == "keep-alive"
+        return False
+    def _finish_request(self, future):
+        self._clear_callbacks()
+        if not self.is_client and self._disconnect_on_finish:
+            self.close()
+            return
+        self.stream.set_nodelay(False)
+        if not self._finish_future.done():
+            self._finish_future.set_result(None)
+    def _parse_headers(self, data):
+        data = native_str(data.decode('latin1')).lstrip("\r\n")
+        eol = data.find("\n")
+        start_line = data[:eol].rstrip("\r")
+        try:
+            headers = httputil.HTTPHeaders.parse(data[eol:])
+        except ValueError:
+            raise httputil.HTTPInputError("Malformed HTTP headers: %r" %
+                                          data[eol:100])
+        return start_line, headers
+    def _read_body(self, code, headers, delegate):
+        if "Content-Length" in headers:
+            if "Transfer-Encoding" in headers:
+                raise httputil.HTTPInputError(
+                    "Response with both Transfer-Encoding and Content-Length")
+            if "," in headers["Content-Length"]:
+                pieces = re.split(r',\s*', headers["Content-Length"])
+                if any(i != pieces[0] for i in pieces):
+                    raise httputil.HTTPInputError(
+                        "Multiple unequal Content-Lengths: %r" %
+                        headers["Content-Length"])
+                headers["Content-Length"] = pieces[0]
+            try:
+                content_length = int(headers["Content-Length"])
+            except ValueError:
+                raise httputil.HTTPInputError(
+                    "Only integer Content-Length is allowed: %s" % headers["Content-Length"])
+            if content_length > self._max_body_size:
+                raise httputil.HTTPInputError("Content-Length too long")
+        else:
+            content_length = None
+        if code == 204:
+            if ("Transfer-Encoding" in headers or
+                    content_length not in (None, 0)):
+                raise httputil.HTTPInputError(
+                    "Response with code %d should not have body" % code)
+            content_length = 0
+        if content_length is not None:
+            return self._read_fixed_body(content_length, delegate)
+        if headers.get("Transfer-Encoding", "").lower() == "chunked":
+            return self._read_chunked_body(delegate)
+        if self.is_client:
+            return self._read_body_until_close(delegate)
+        return None
+    @gen.coroutine
+    def _read_fixed_body(self, content_length, delegate):
+        while content_length > 0:
+            body = yield self.stream.read_bytes(
+                min(self.params.chunk_size, content_length), partial=True)
+            content_length -= len(body)
+            if not self._write_finished or self.is_client:
+                with _ExceptionLoggingContext(app_log):
+                    ret = delegate.data_received(body)
+                    if ret is not None:
+                        yield ret
+    @gen.coroutine
+    def _read_chunked_body(self, delegate):
+        total_size = 0
+        while True:
+            chunk_len = yield self.stream.read_until(b"\r\n", max_bytes=64)
+            chunk_len = int(chunk_len.strip(), 16)
+            if chunk_len == 0:
+                crlf = yield self.stream.read_bytes(2)
+                if crlf != b'\r\n':
+                    raise httputil.HTTPInputError("improperly terminated chunked request")
+                return
+            total_size += chunk_len
+            if total_size > self._max_body_size:
+                raise httputil.HTTPInputError("chunked body too large")
+            bytes_to_read = chunk_len
+            while bytes_to_read:
+                chunk = yield self.stream.read_bytes(
+                    min(bytes_to_read, self.params.chunk_size), partial=True)
+                bytes_to_read -= len(chunk)
+                if not self._write_finished or self.is_client:
+                    with _ExceptionLoggingContext(app_log):
+                        ret = delegate.data_received(chunk)
+                        if ret is not None:
+                            yield ret
+            crlf = yield self.stream.read_bytes(2)
+            assert crlf == b"\r\n"
+    @gen.coroutine
+    def _read_body_until_close(self, delegate):
+        body = yield self.stream.read_until_close()
+        if not self._write_finished or self.is_client:
+            with _ExceptionLoggingContext(app_log):
+                delegate.data_received(body)
+class _GzipMessageDelegate(httputil.HTTPMessageDelegate):
+    """Wraps an `HTTPMessageDelegate` to decode ``Content-Encoding: gzip``.
+    """
+    def __init__(self, delegate, chunk_size):
+        self._delegate = delegate
+        self._chunk_size = chunk_size
+        self._decompressor = None
+    def headers_received(self, start_line, headers):
+        if headers.get("Content-Encoding") == "gzip":
+            self._decompressor = GzipDecompressor()
+            headers.add("X-Consumed-Content-Encoding",
+                        headers["Content-Encoding"])
+            del headers["Content-Encoding"]
+        return self._delegate.headers_received(start_line, headers)
+    @gen.coroutine
+    def data_received(self, chunk):
+        if self._decompressor:
+            compressed_data = chunk
+            while compressed_data:
+                decompressed = self._decompressor.decompress(
+                    compressed_data, self._chunk_size)
+                if decompressed:
+                    ret = self._delegate.data_received(decompressed)
+                    if ret is not None:
+                        yield ret
+                compressed_data = self._decompressor.unconsumed_tail
+        else:
+            ret = self._delegate.data_received(chunk)
+            if ret is not None:
+                yield ret
+    def finish(self):
+        if self._decompressor is not None:
+            tail = self._decompressor.flush()
+            if tail:
+                self._delegate.data_received(tail)
+        return self._delegate.finish()
+    def on_connection_close(self):
+        return self._delegate.on_connection_close()
+class HTTP1ServerConnection(object):
+    """An HTTP/1.x server."""
+    def __init__(self, stream, params=None, context=None):
+        """
+        :arg stream: an `.IOStream`
+        :arg params: a `.HTTP1ConnectionParameters` or None
+        :arg context: an opaque application-defined object that is accessible
+            as ``connection.context``
+        """
+        self.stream = stream
+        if params is None:
+            params = HTTP1ConnectionParameters()
+        self.params = params
+        self.context = context
+        self._serving_future = None
+    @gen.coroutine
+    def close(self):
+        """Closes the connection.
+        Returns a `.Future` that resolves after the serving loop has exited.
+        """
+        self.stream.close()
+        try:
+            yield self._serving_future
+        except Exception:
+            pass
+    def start_serving(self, delegate):
+        """Starts serving requests on this connection.
+        :arg delegate: a `.HTTPServerConnectionDelegate`
+        """
+        assert isinstance(delegate, httputil.HTTPServerConnectionDelegate)
+        self._serving_future = self._server_request_loop(delegate)
+        self.stream.io_loop.add_future(self._serving_future,
+                                       lambda f: f.result())
+    @gen.coroutine
+    def _server_request_loop(self, delegate):
+        try:
+            while True:
+                conn = HTTP1Connection(self.stream, False,
+                                       self.params, self.context)
+                request_delegate = delegate.start_request(self, conn)
+                try:
+                    ret = yield conn.read_response(request_delegate)
+                except (iostream.StreamClosedError,
+                        iostream.UnsatisfiableReadError):
+                    return
+                except _QuietException:
+                    conn.close()
+                    return
+                except Exception:
+                    gen_log.error("Uncaught exception", exc_info=True)
+                    conn.close()
+                    return
+                if not ret:
+                    return
+                yield gen.moment
+        finally:
+            delegate.on_close(self)

--- a//dev/null
+++ b/salt/ext/tornado/httpclient.py
@@ -0,0 +1,552 @@
+"""Blocking and non-blocking HTTP client interfaces.
+This module defines a common interface shared by two implementations,
+``simple_httpclient`` and ``curl_httpclient``.  Applications may either
+instantiate their chosen implementation class directly or use the
+`AsyncHTTPClient` class from this module, which selects an implementation
+that can be overridden with the `AsyncHTTPClient.configure` method.
+The default implementation is ``simple_httpclient``, and this is expected
+to be suitable for most users' needs.  However, some applications may wish
+to switch to ``curl_httpclient`` for reasons such as the following:
+* ``curl_httpclient`` has some features not found in ``simple_httpclient``,
+  including support for HTTP proxies and the ability to use a specified
+  network interface.
+* ``curl_httpclient`` is more likely to be compatible with sites that are
+  not-quite-compliant with the HTTP spec, or sites that use little-exercised
+  features of HTTP.
+* ``curl_httpclient`` is faster.
+* ``curl_httpclient`` was the default prior to Tornado 2.0.
+Note that if you are using ``curl_httpclient``, it is highly
+recommended that you use a recent version of ``libcurl`` and
+``pycurl``.  Currently the minimum supported version of libcurl is
+7.22.0, and the minimum version of pycurl is 7.18.2.  It is highly
+recommended that your ``libcurl`` installation is built with
+asynchronous DNS resolver (threaded or c-ares), otherwise you may
+encounter various problems with request timeouts (for more
+information, see
+http://curl.haxx.se/libcurl/c/curl_easy_setopt.html#CURLOPTCONNECTTIMEOUTMS
+and comments in curl_httpclient.py).
+To select ``curl_httpclient``, call `AsyncHTTPClient.configure` at startup::
+    AsyncHTTPClient.configure("tornado.curl_httpclient.CurlAsyncHTTPClient")
+"""
+from __future__ import absolute_import, division, print_function
+import functools
+import time
+import weakref
+from salt.ext.tornado.concurrent import TracebackFuture
+from salt.ext.tornado.escape import utf8, native_str
+from salt.ext.tornado import httputil, stack_context
+from salt.ext.tornado.ioloop import IOLoop
+from salt.ext.tornado.util import Configurable
+class HTTPClient(object):
+    """A blocking HTTP client.
+    This interface is provided for convenience and testing; most applications
+    that are running an IOLoop will want to use `AsyncHTTPClient` instead.
+    Typical usage looks like this::
+        http_client = httpclient.HTTPClient()
+        try:
+            response = http_client.fetch("http://www.google.com/")
+            print(response.body)
+        except httpclient.HTTPError as e:
+            print("Error: " + str(e))
+        except Exception as e:
+            print("Error: " + str(e))
+        http_client.close()
+    """
+    def __init__(self, async_client_class=None, **kwargs):
+        self._io_loop = IOLoop(make_current=False)
+        if async_client_class is None:
+            async_client_class = AsyncHTTPClient
+        self._async_client = async_client_class(self._io_loop, **kwargs)
+        self._closed = False
+    def __del__(self):
+        self.close()
+    def close(self):
+        """Closes the HTTPClient, freeing any resources used."""
+        if not self._closed:
+            self._async_client.close()
+            self._io_loop.close()
+            self._closed = True
+    def fetch(self, request, **kwargs):
+        """Executes a request, returning an `HTTPResponse`.
+        The request may be either a string URL or an `HTTPRequest` object.
+        If it is a string, we construct an `HTTPRequest` using any additional
+        kwargs: ``HTTPRequest(request, **kwargs)``
+        If an error occurs during the fetch, we raise an `HTTPError` unless
+        the ``raise_error`` keyword argument is set to False.
+        """
+        response = self._io_loop.run_sync(functools.partial(
+            self._async_client.fetch, request, **kwargs))
+        return response
+class AsyncHTTPClient(Configurable):
+    """An non-blocking HTTP client.
+    Example usage::
+        def handle_response(response):
+            if response.error:
+                print("Error: %s" % response.error)
+            else:
+                print(response.body)
+        http_client = AsyncHTTPClient()
+        http_client.fetch("http://www.google.com/", handle_response)
+    The constructor for this class is magic in several respects: It
+    actually creates an instance of an implementation-specific
+    subclass, and instances are reused as a kind of pseudo-singleton
+    (one per `.IOLoop`).  The keyword argument ``force_instance=True``
+    can be used to suppress this singleton behavior.  Unless
+    ``force_instance=True`` is used, no arguments other than
+    ``io_loop`` should be passed to the `AsyncHTTPClient` constructor.
+    The implementation subclass as well as arguments to its
+    constructor can be set with the static method `configure()`
+    All `AsyncHTTPClient` implementations support a ``defaults``
+    keyword argument, which can be used to set default values for
+    `HTTPRequest` attributes.  For example::
+        AsyncHTTPClient.configure(
+            None, defaults=dict(user_agent="MyUserAgent"))
+        client = AsyncHTTPClient(force_instance=True,
+            defaults=dict(user_agent="MyUserAgent"))
+    .. versionchanged:: 4.1
+       The ``io_loop`` argument is deprecated.
+    """
+    @classmethod
+    def configurable_base(cls):
+        return AsyncHTTPClient
+    @classmethod
+    def configurable_default(cls):
+        from salt.ext.tornado.simple_httpclient import SimpleAsyncHTTPClient
+        return SimpleAsyncHTTPClient
+    @classmethod
+    def _async_clients(cls):
+        attr_name = '_async_client_dict_' + cls.__name__
+        if not hasattr(cls, attr_name):
+            setattr(cls, attr_name, weakref.WeakKeyDictionary())
+        return getattr(cls, attr_name)
+    def __new__(cls, io_loop=None, force_instance=False, **kwargs):
+        io_loop = io_loop or IOLoop.current()
+        if force_instance:
+            instance_cache = None
+        else:
+            instance_cache = cls._async_clients()
+        if instance_cache is not None and io_loop in instance_cache:
+            return instance_cache[io_loop]
+        instance = super(AsyncHTTPClient, cls).__new__(cls, io_loop=io_loop,
+                                                       **kwargs)
+        instance._instance_cache = instance_cache
+        if instance_cache is not None:
+            instance_cache[instance.io_loop] = instance
+        return instance
+    def initialize(self, io_loop, defaults=None):
+        self.io_loop = io_loop
+        self.defaults = dict(HTTPRequest._DEFAULTS)
+        if defaults is not None:
+            self.defaults.update(defaults)
+        self._closed = False
+    def close(self):
+        """Destroys this HTTP client, freeing any file descriptors used.
+        This method is **not needed in normal use** due to the way
+        that `AsyncHTTPClient` objects are transparently reused.
+        ``close()`` is generally only necessary when either the
+        `.IOLoop` is also being closed, or the ``force_instance=True``
+        argument was used when creating the `AsyncHTTPClient`.
+        No other methods may be called on the `AsyncHTTPClient` after
+        ``close()``.
+        """
+        if self._closed:
+            return
+        self._closed = True
+        if self._instance_cache is not None:
+            if self._instance_cache.get(self.io_loop) is not self:
+                raise RuntimeError("inconsistent AsyncHTTPClient cache")
+            del self._instance_cache[self.io_loop]
+    def fetch(self, request, callback=None, raise_error=True, **kwargs):
+        """Executes a request, asynchronously returning an `HTTPResponse`.
+        The request may be either a string URL or an `HTTPRequest` object.
+        If it is a string, we construct an `HTTPRequest` using any additional
+        kwargs: ``HTTPRequest(request, **kwargs)``
+        This method returns a `.Future` whose result is an
+        `HTTPResponse`. By default, the ``Future`` will raise an
+        `HTTPError` if the request returned a non-200 response code
+        (other errors may also be raised if the server could not be
+        contacted). Instead, if ``raise_error`` is set to False, the
+        response will always be returned regardless of the response
+        code.
+        If a ``callback`` is given, it will be invoked with the `HTTPResponse`.
+        In the callback interface, `HTTPError` is not automatically raised.
+        Instead, you must check the response's ``error`` attribute or
+        call its `~HTTPResponse.rethrow` method.
+        """
+        if self._closed:
+            raise RuntimeError("fetch() called on closed AsyncHTTPClient")
+        if not isinstance(request, HTTPRequest):
+            request = HTTPRequest(url=request, **kwargs)
+        else:
+            if kwargs:
+                raise ValueError("kwargs can't be used if request is an HTTPRequest object")
+        request.headers = httputil.HTTPHeaders(request.headers)
+        request = _RequestProxy(request, self.defaults)
+        future = TracebackFuture()
+        if callback is not None:
+            callback = stack_context.wrap(callback)
+            def handle_future(future):
+                exc = future.exception()
+                if isinstance(exc, HTTPError) and exc.response is not None:
+                    response = exc.response
+                elif exc is not None:
+                    response = HTTPResponse(
+                        request, 599, error=exc,
+                        request_time=time.time() - request.start_time)
+                else:
+                    response = future.result()
+                self.io_loop.add_callback(callback, response)
+            future.add_done_callback(handle_future)
+        def handle_response(response):
+            if raise_error and response.error:
+                future.set_exception(response.error)
+            else:
+                future.set_result(response)
+        self.fetch_impl(request, handle_response)
+        return future
+    def fetch_impl(self, request, callback):
+        raise NotImplementedError()
+    @classmethod
+    def configure(cls, impl, **kwargs):
+        """Configures the `AsyncHTTPClient` subclass to use.
+        ``AsyncHTTPClient()`` actually creates an instance of a subclass.
+        This method may be called with either a class object or the
+        fully-qualified name of such a class (or ``None`` to use the default,
+        ``SimpleAsyncHTTPClient``)
+        If additional keyword arguments are given, they will be passed
+        to the constructor of each subclass instance created.  The
+        keyword argument ``max_clients`` determines the maximum number
+        of simultaneous `~AsyncHTTPClient.fetch()` operations that can
+        execute in parallel on each `.IOLoop`.  Additional arguments
+        may be supported depending on the implementation class in use.
+        Example::
+           AsyncHTTPClient.configure("tornado.curl_httpclient.CurlAsyncHTTPClient")
+        """
+        super(AsyncHTTPClient, cls).configure(impl, **kwargs)
+class HTTPRequest(object):
+    """HTTP client request object."""
+    _DEFAULTS = dict(
+        connect_timeout=20.0,
+        request_timeout=20.0,
+        follow_redirects=True,
+        max_redirects=5,
+        decompress_response=True,
+        proxy_password='',
+        allow_nonstandard_methods=False,
+        validate_cert=True)
+    def __init__(self, url, method="GET", headers=None, body=None,
+                 auth_username=None, auth_password=None, auth_mode=None,
+                 connect_timeout=None, request_timeout=None,
+                 if_modified_since=None, follow_redirects=None,
+                 max_redirects=None, user_agent=None, use_gzip=None,
+                 network_interface=None, streaming_callback=None,
+                 header_callback=None, prepare_curl_callback=None,
+                 proxy_host=None, proxy_port=None, proxy_username=None,
+                 proxy_password=None, proxy_auth_mode=None,
+                 allow_nonstandard_methods=None, validate_cert=None,
+                 ca_certs=None, allow_ipv6=None, client_key=None,
+                 client_cert=None, body_producer=None,
+                 expect_100_continue=False, decompress_response=None,
+                 ssl_options=None):
+        r"""All parameters except ``url`` are optional.
+        :arg string url: URL to fetch
+        :arg string method: HTTP method, e.g. "GET" or "POST"
+        :arg headers: Additional HTTP headers to pass on the request
+        :type headers: `~tornado.httputil.HTTPHeaders` or `dict`
+        :arg body: HTTP request body as a string (byte or unicode; if unicode
+           the utf-8 encoding will be used)
+        :arg body_producer: Callable used for lazy/asynchronous request bodies.
+           It is called with one argument, a ``write`` function, and should
+           return a `.Future`.  It should call the write function with new
+           data as it becomes available.  The write function returns a
+           `.Future` which can be used for flow control.
+           Only one of ``body`` and ``body_producer`` may
+           be specified.  ``body_producer`` is not supported on
+           ``curl_httpclient``.  When using ``body_producer`` it is recommended
+           to pass a ``Content-Length`` in the headers as otherwise chunked
+           encoding will be used, and many servers do not support chunked
+           encoding on requests.  New in Tornado 4.0
+        :arg string auth_username: Username for HTTP authentication
+        :arg string auth_password: Password for HTTP authentication
+        :arg string auth_mode: Authentication mode; default is "basic".
+           Allowed values are implementation-defined; ``curl_httpclient``
+           supports "basic" and "digest"; ``simple_httpclient`` only supports
+           "basic"
+        :arg float connect_timeout: Timeout for initial connection in seconds,
+           default 20 seconds
+        :arg float request_timeout: Timeout for entire request in seconds,
+           default 20 seconds
+        :arg if_modified_since: Timestamp for ``If-Modified-Since`` header
+        :type if_modified_since: `datetime` or `float`
+        :arg bool follow_redirects: Should redirects be followed automatically
+           or return the 3xx response? Default True.
+        :arg int max_redirects: Limit for ``follow_redirects``, default 5.
+        :arg string user_agent: String to send as ``User-Agent`` header
+        :arg bool decompress_response: Request a compressed response from
+           the server and decompress it after downloading.  Default is True.
+           New in Tornado 4.0.
+        :arg bool use_gzip: Deprecated alias for ``decompress_response``
+           since Tornado 4.0.
+        :arg string network_interface: Network interface to use for request.
+           ``curl_httpclient`` only; see note below.
+        :arg callable streaming_callback: If set, ``streaming_callback`` will
+           be run with each chunk of data as it is received, and
+           ``HTTPResponse.body`` and ``HTTPResponse.buffer`` will be empty in
+           the final response.
+        :arg callable header_callback: If set, ``header_callback`` will
+           be run with each header line as it is received (including the
+           first line, e.g. ``HTTP/1.0 200 OK\r\n``, and a final line
+           containing only ``\r\n``.  All lines include the trailing newline
+           characters).  ``HTTPResponse.headers`` will be empty in the final
+           response.  This is most useful in conjunction with
+           ``streaming_callback``, because it's the only way to get access to
+           header data while the request is in progress.
+        :arg callable prepare_curl_callback: If set, will be called with
+           a ``pycurl.Curl`` object to allow the application to make additional
+           ``setopt`` calls.
+        :arg string proxy_host: HTTP proxy hostname.  To use proxies,
+           ``proxy_host`` and ``proxy_port`` must be set; ``proxy_username``,
+           ``proxy_pass`` and ``proxy_auth_mode`` are optional.  Proxies are
+           currently only supported with ``curl_httpclient``.
+        :arg int proxy_port: HTTP proxy port
+        :arg string proxy_username: HTTP proxy username
+        :arg string proxy_password: HTTP proxy password
+        :arg string proxy_auth_mode: HTTP proxy Authentication mode;
+           default is "basic". supports "basic" and "digest"
+        :arg bool allow_nonstandard_methods: Allow unknown values for ``method``
+           argument? Default is False.
+        :arg bool validate_cert: For HTTPS requests, validate the server's
+           certificate? Default is True.
+        :arg string ca_certs: filename of CA certificates in PEM format,
+           or None to use defaults.  See note below when used with
+           ``curl_httpclient``.
+        :arg string client_key: Filename for client SSL key, if any.  See
+           note below when used with ``curl_httpclient``.
+        :arg string client_cert: Filename for client SSL certificate, if any.
+           See note below when used with ``curl_httpclient``.
+        :arg ssl.SSLContext ssl_options: `ssl.SSLContext` object for use in
+           ``simple_httpclient`` (unsupported by ``curl_httpclient``).
+           Overrides ``validate_cert``, ``ca_certs``, ``client_key``,
+           and ``client_cert``.
+        :arg bool allow_ipv6: Use IPv6 when available?  Default is true.
+        :arg bool expect_100_continue: If true, send the
+           ``Expect: 100-continue`` header and wait for a continue response
+           before sending the request body.  Only supported with
+           simple_httpclient.
+        .. note::
+            When using ``curl_httpclient`` certain options may be
+            inherited by subsequent fetches because ``pycurl`` does
+            not allow them to be cleanly reset.  This applies to the
+            ``ca_certs``, ``client_key``, ``client_cert``, and
+            ``network_interface`` arguments.  If you use these
+            options, you should pass them on every request (you don't
+            have to always use the same values, but it's not possible
+            to mix requests that specify these options with ones that
+            use the defaults).
+        .. versionadded:: 3.1
+           The ``auth_mode`` argument.
+        .. versionadded:: 4.0
+           The ``body_producer`` and ``expect_100_continue`` arguments.
+        .. versionadded:: 4.2
+           The ``ssl_options`` argument.
+        .. versionadded:: 4.5
+           The ``proxy_auth_mode`` argument.
+        """
+        self.headers = headers
+        if if_modified_since:
+            self.headers["If-Modified-Since"] = httputil.format_timestamp(
+                if_modified_since)
+        self.proxy_host = proxy_host
+        self.proxy_port = proxy_port
+        self.proxy_username = proxy_username
+        self.proxy_password = proxy_password
+        self.proxy_auth_mode = proxy_auth_mode
+        self.url = url
+        self.method = method
+        self.body = body
+        self.body_producer = body_producer
+        self.auth_username = auth_username
+        self.auth_password = auth_password
+        self.auth_mode = auth_mode
+        self.connect_timeout = connect_timeout
+        self.request_timeout = request_timeout
+        self.follow_redirects = follow_redirects
+        self.max_redirects = max_redirects
+        self.user_agent = user_agent
+        if decompress_response is not None:
+            self.decompress_response = decompress_response
+        else:
+            self.decompress_response = use_gzip
+        self.network_interface = network_interface
+        self.streaming_callback = streaming_callback
+        self.header_callback = header_callback
+        self.prepare_curl_callback = prepare_curl_callback
+        self.allow_nonstandard_methods = allow_nonstandard_methods
+        self.validate_cert = validate_cert
+        self.ca_certs = ca_certs
+        self.allow_ipv6 = allow_ipv6
+        self.client_key = client_key
+        self.client_cert = client_cert
+        self.ssl_options = ssl_options
+        self.expect_100_continue = expect_100_continue
+        self.start_time = time.time()
+    @property
+    def headers(self):
+        return self._headers
+    @headers.setter
+    def headers(self, value):
+        if value is None:
+            self._headers = httputil.HTTPHeaders()
+        else:
+            self._headers = value
+    @property
+    def body(self):
+        return self._body
+    @body.setter
+    def body(self, value):
+        self._body = utf8(value)
+    @property
+    def body_producer(self):
+        return self._body_producer
+    @body_producer.setter
+    def body_producer(self, value):
+        self._body_producer = stack_context.wrap(value)
+    @property
+    def streaming_callback(self):
+        return self._streaming_callback
+    @streaming_callback.setter
+    def streaming_callback(self, value):
+        self._streaming_callback = stack_context.wrap(value)
+    @property
+    def header_callback(self):
+        return self._header_callback
+    @header_callback.setter
+    def header_callback(self, value):
+        self._header_callback = stack_context.wrap(value)
+    @property
+    def prepare_curl_callback(self):
+        return self._prepare_curl_callback
+    @prepare_curl_callback.setter
+    def prepare_curl_callback(self, value):
+        self._prepare_curl_callback = stack_context.wrap(value)
+class HTTPResponse(object):
+    """HTTP Response object.
+    Attributes:
+    * request: HTTPRequest object
+    * code: numeric HTTP status code, e.g. 200 or 404
+    * reason: human-readable reason phrase describing the status code
+    * headers: `tornado.httputil.HTTPHeaders` object
+    * effective_url: final location of the resource after following any
+      redirects
+    * buffer: ``cStringIO`` object for response body
+    * body: response body as bytes (created on demand from ``self.buffer``)
+    * error: Exception object, if any
+    * request_time: seconds from request start to finish
+    * time_info: dictionary of diagnostic timing information from the request.
+      Available data are subject to change, but currently uses timings
+      available from http://curl.haxx.se/libcurl/c/curl_easy_getinfo.html,
+      plus ``queue``, which is the delay (if any) introduced by waiting for
+      a slot under `AsyncHTTPClient`'s ``max_clients`` setting.
+    """
+    def __init__(self, request, code, headers=None, buffer=None,
+                 effective_url=None, error=None, request_time=None,
+                 time_info=None, reason=None):
+        if isinstance(request, _RequestProxy):
+            self.request = request.request
+        else:
+            self.request = request
+        self.code = code
+        self.reason = reason or httputil.responses.get(code, "Unknown")
+        if headers is not None:
+            self.headers = headers
+        else:
+            self.headers = httputil.HTTPHeaders()
+        self.buffer = buffer
+        self._body = None
+        if effective_url is None:
+            self.effective_url = request.url
+        else:
+            self.effective_url = effective_url
+        if error is None:
+            if self.code < 200 or self.code >= 300:
+                self.error = HTTPError(self.code, message=self.reason,
+                                       response=self)
+            else:
+                self.error = None
+        else:
+            self.error = error
+        self.request_time = request_time
+        self.time_info = time_info or {}
+    @property
+    def body(self):
+        if self.buffer is None:
+            return None
+        elif self._body is None:
+            self._body = self.buffer.getvalue()
+        return self._body
+    def rethrow(self):
+        """If there was an error on the request, raise an `HTTPError`."""
+        if self.error:
+            raise self.error
+    def __repr__(self):
+        args = ",".join("%s=%r" % i for i in sorted(self.__dict__.items()))
+        return "%s(%s)" % (self.__class__.__name__, args)
+class HTTPError(Exception):
+    """Exception thrown for an unsuccessful HTTP request.
+    Attributes:
+    * ``code`` - HTTP error integer error code, e.g. 404.  Error code 599 is
+      used when no HTTP response was received, e.g. for a timeout.
+    * ``response`` - `HTTPResponse` object, if any.
+    Note that if ``follow_redirects`` is False, redirects become HTTPErrors,
+    and you can look at ``error.response.headers['Location']`` to see the
+    destination of the redirect.
+    """
+    def __init__(self, code, message=None, response=None):
+        self.code = code
+        self.message = message or httputil.responses.get(code, "Unknown")
+        self.response = response
+        super(HTTPError, self).__init__(code, message, response)
+    def __str__(self):
+        return "HTTP %d: %s" % (self.code, self.message)
+    __repr__ = __str__
+class _RequestProxy(object):
+    """Combines an object with a dictionary of defaults.
+    Used internally by AsyncHTTPClient implementations.
+    """
+    def __init__(self, request, defaults):
+        self.request = request
+        self.defaults = defaults
+    def __getattr__(self, name):
+        request_attr = getattr(self.request, name)
+        if request_attr is not None:
+            return request_attr
+        elif self.defaults is not None:
+            return self.defaults.get(name, None)
+        else:
+            return None
+def main():
+    from salt.ext.tornado.options import define, options, parse_command_line
+    define("print_headers", type=bool, default=False)
+    define("print_body", type=bool, default=True)
+    define("follow_redirects", type=bool, default=True)
+    define("validate_cert", type=bool, default=True)
+    args = parse_command_line()
+    client = HTTPClient()
+    for arg in args:
+        try:
+            response = client.fetch(arg,
+                                    follow_redirects=options.follow_redirects,
+                                    validate_cert=options.validate_cert,
+                                    )
+        except HTTPError as e:
+            if e.response is not None:
+                response = e.response
+            else:
+                raise
+        if options.print_headers:
+            print(response.headers)
+        if options.print_body:
+            print(native_str(response.body))
+    client.close()
+if __name__ == "__main__":
+    main()

--- a//dev/null
+++ b/salt/ext/tornado/httpserver.py
@@ -0,0 +1,234 @@
+"""A non-blocking, single-threaded HTTP server.
+Typical applications have little direct interaction with the `HTTPServer`
+class except to start a server at the beginning of the process
+(and even that is often done indirectly via `tornado.web.Application.listen`).
+.. versionchanged:: 4.0
+   The ``HTTPRequest`` class that used to live in this module has been moved
+   to `tornado.httputil.HTTPServerRequest`.  The old name remains as an alias.
+"""
+from __future__ import absolute_import, division, print_function
+import socket
+from salt.ext.tornado.escape import native_str
+from salt.ext.tornado.http1connection import HTTP1ServerConnection, HTTP1ConnectionParameters
+from salt.ext.tornado import gen
+from salt.ext.tornado import httputil
+from salt.ext.tornado import iostream
+from salt.ext.tornado import netutil
+from salt.ext.tornado.tcpserver import TCPServer
+from salt.ext.tornado.util import Configurable
+class HTTPServer(TCPServer, Configurable,
+                 httputil.HTTPServerConnectionDelegate):
+    r"""A non-blocking, single-threaded HTTP server.
+    A server is defined by a subclass of `.HTTPServerConnectionDelegate`,
+    or, for backwards compatibility, a callback that takes an
+    `.HTTPServerRequest` as an argument. The delegate is usually a
+    `tornado.web.Application`.
+    `HTTPServer` supports keep-alive connections by default
+    (automatically for HTTP/1.1, or for HTTP/1.0 when the client
+    requests ``Connection: keep-alive``).
+    If ``xheaders`` is ``True``, we support the
+    ``X-Real-Ip``/``X-Forwarded-For`` and
+    ``X-Scheme``/``X-Forwarded-Proto`` headers, which override the
+    remote IP and URI scheme/protocol for all requests.  These headers
+    are useful when running Tornado behind a reverse proxy or load
+    balancer.  The ``protocol`` argument can also be set to ``https``
+    if Tornado is run behind an SSL-decoding proxy that does not set one of
+    the supported ``xheaders``.
+    By default, when parsing the ``X-Forwarded-For`` header, Tornado will
+    select the last (i.e., the closest) address on the list of hosts as the
+    remote host IP address.  To select the next server in the chain, a list of
+    trusted downstream hosts may be passed as the ``trusted_downstream``
+    argument.  These hosts will be skipped when parsing the ``X-Forwarded-For``
+    header.
+    To make this server serve SSL traffic, send the ``ssl_options`` keyword
+    argument with an `ssl.SSLContext` object. For compatibility with older
+    versions of Python ``ssl_options`` may also be a dictionary of keyword
+    arguments for the `ssl.wrap_socket` method.::
+       ssl_ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)
+       ssl_ctx.load_cert_chain(os.path.join(data_dir, "mydomain.crt"),
+                               os.path.join(data_dir, "mydomain.key"))
+       HTTPServer(applicaton, ssl_options=ssl_ctx)
+    `HTTPServer` initialization follows one of three patterns (the
+    initialization methods are defined on `tornado.tcpserver.TCPServer`):
+    1. `~tornado.tcpserver.TCPServer.listen`: simple single-process::
+            server = HTTPServer(app)
+            server.listen(8888)
+            IOLoop.current().start()
+       In many cases, `tornado.web.Application.listen` can be used to avoid
+       the need to explicitly create the `HTTPServer`.
+    2. `~tornado.tcpserver.TCPServer.bind`/`~tornado.tcpserver.TCPServer.start`:
+       simple multi-process::
+            server = HTTPServer(app)
+            server.bind(8888)
+            server.start(0)  # Forks multiple sub-processes
+            IOLoop.current().start()
+       When using this interface, an `.IOLoop` must *not* be passed
+       to the `HTTPServer` constructor.  `~.TCPServer.start` will always start
+       the server on the default singleton `.IOLoop`.
+    3. `~tornado.tcpserver.TCPServer.add_sockets`: advanced multi-process::
+            sockets = tornado.netutil.bind_sockets(8888)
+            tornado.process.fork_processes(0)
+            server = HTTPServer(app)
+            server.add_sockets(sockets)
+            IOLoop.current().start()
+       The `~.TCPServer.add_sockets` interface is more complicated,
+       but it can be used with `tornado.process.fork_processes` to
+       give you more flexibility in when the fork happens.
+       `~.TCPServer.add_sockets` can also be used in single-process
+       servers if you want to create your listening sockets in some
+       way other than `tornado.netutil.bind_sockets`.
+    .. versionchanged:: 4.0
+       Added ``decompress_request``, ``chunk_size``, ``max_header_size``,
+       ``idle_connection_timeout``, ``body_timeout``, ``max_body_size``
+       arguments.  Added support for `.HTTPServerConnectionDelegate`
+       instances as ``request_callback``.
+    .. versionchanged:: 4.1
+       `.HTTPServerConnectionDelegate.start_request` is now called with
+       two arguments ``(server_conn, request_conn)`` (in accordance with the
+       documentation) instead of one ``(request_conn)``.
+    .. versionchanged:: 4.2
+       `HTTPServer` is now a subclass of `tornado.util.Configurable`.
+    .. versionchanged:: 4.5
+       Added the ``trusted_downstream`` argument.
+    """
+    def __init__(self, *args, **kwargs):
+        pass
+    def initialize(self, request_callback, no_keep_alive=False, io_loop=None,
+                   xheaders=False, ssl_options=None, protocol=None,
+                   decompress_request=False,
+                   chunk_size=None, max_header_size=None,
+                   idle_connection_timeout=None, body_timeout=None,
+                   max_body_size=None, max_buffer_size=None,
+                   trusted_downstream=None):
+        self.request_callback = request_callback
+        self.no_keep_alive = no_keep_alive
+        self.xheaders = xheaders
+        self.protocol = protocol
+        self.conn_params = HTTP1ConnectionParameters(
+            decompress=decompress_request,
+            chunk_size=chunk_size,
+            max_header_size=max_header_size,
+            header_timeout=idle_connection_timeout or 3600,
+            max_body_size=max_body_size,
+            body_timeout=body_timeout,
+            no_keep_alive=no_keep_alive)
+        TCPServer.__init__(self, io_loop=io_loop, ssl_options=ssl_options,
+                           max_buffer_size=max_buffer_size,
+                           read_chunk_size=chunk_size)
+        self._connections = set()
+        self.trusted_downstream = trusted_downstream
+    @classmethod
+    def configurable_base(cls):
+        return HTTPServer
+    @classmethod
+    def configurable_default(cls):
+        return HTTPServer
+    @gen.coroutine
+    def close_all_connections(self):
+        while self._connections:
+            conn = next(iter(self._connections))
+            yield conn.close()
+    def handle_stream(self, stream, address):
+        context = _HTTPRequestContext(stream, address,
+                                      self.protocol,
+                                      self.trusted_downstream)
+        conn = HTTP1ServerConnection(
+            stream, self.conn_params, context)
+        self._connections.add(conn)
+        conn.start_serving(self)
+    def start_request(self, server_conn, request_conn):
+        if isinstance(self.request_callback, httputil.HTTPServerConnectionDelegate):
+            delegate = self.request_callback.start_request(server_conn, request_conn)
+        else:
+            delegate = _CallableAdapter(self.request_callback, request_conn)
+        if self.xheaders:
+            delegate = _ProxyAdapter(delegate, request_conn)
+        return delegate
+    def on_close(self, server_conn):
+        self._connections.remove(server_conn)
+class _CallableAdapter(httputil.HTTPMessageDelegate):
+    def __init__(self, request_callback, request_conn):
+        self.connection = request_conn
+        self.request_callback = request_callback
+        self.request = None
+        self.delegate = None
+        self._chunks = []
+    def headers_received(self, start_line, headers):
+        self.request = httputil.HTTPServerRequest(
+            connection=self.connection, start_line=start_line,
+            headers=headers)
+    def data_received(self, chunk):
+        self._chunks.append(chunk)
+    def finish(self):
+        self.request.body = b''.join(self._chunks)
+        self.request._parse_body()
+        self.request_callback(self.request)
+    def on_connection_close(self):
+        self._chunks = None
+class _HTTPRequestContext(object):
+    def __init__(self, stream, address, protocol, trusted_downstream=None):
+        self.address = address
+        if stream.socket is not None:
+            self.address_family = stream.socket.family
+        else:
+            self.address_family = None
+        if (self.address_family in (socket.AF_INET, socket.AF_INET6) and
+                address is not None):
+            self.remote_ip = address[0]
+        else:
+            self.remote_ip = '0.0.0.0'
+        if protocol:
+            self.protocol = protocol
+        elif isinstance(stream, iostream.SSLIOStream):
+            self.protocol = "https"
+        else:
+            self.protocol = "http"
+        self._orig_remote_ip = self.remote_ip
+        self._orig_protocol = self.protocol
+        self.trusted_downstream = set(trusted_downstream or [])
+    def __str__(self):
+        if self.address_family in (socket.AF_INET, socket.AF_INET6):
+            return self.remote_ip
+        elif isinstance(self.address, bytes):
+            return native_str(self.address)
+        else:
+            return str(self.address)
+    def _apply_xheaders(self, headers):
+        """Rewrite the ``remote_ip`` and ``protocol`` fields."""
+        ip = headers.get("X-Forwarded-For", self.remote_ip)
+        for ip in (cand.strip() for cand in reversed(ip.split(','))):
+            if ip not in self.trusted_downstream:
+                break
+        ip = headers.get("X-Real-Ip", ip)
+        if netutil.is_valid_ip(ip):
+            self.remote_ip = ip
+        proto_header = headers.get(
+            "X-Scheme", headers.get("X-Forwarded-Proto",
+                                    self.protocol))
+        if proto_header in ("http", "https"):
+            self.protocol = proto_header
+    def _unapply_xheaders(self):
+        """Undo changes from `_apply_xheaders`.
+        Xheaders are per-request so they should not leak to the next
+        request on the same connection.
+        """
+        self.remote_ip = self._orig_remote_ip
+        self.protocol = self._orig_protocol
+class _ProxyAdapter(httputil.HTTPMessageDelegate):
+    def __init__(self, delegate, request_conn):
+        self.connection = request_conn
+        self.delegate = delegate
+    def headers_received(self, start_line, headers):
+        self.connection.context._apply_xheaders(headers)
+        return self.delegate.headers_received(start_line, headers)
+    def data_received(self, chunk):
+        return self.delegate.data_received(chunk)
+    def finish(self):
+        self.delegate.finish()
+        self._cleanup()
+    def on_connection_close(self):
+        self.delegate.on_connection_close()
+        self._cleanup()
+    def _cleanup(self):
+        self.connection.context._unapply_xheaders()
+HTTPRequest = httputil.HTTPServerRequest

--- a//dev/null
+++ b/salt/ext/tornado/httputil.py
@@ -0,0 +1,775 @@
+"""HTTP utility code shared by clients and servers.
+This module also defines the `HTTPServerRequest` class which is exposed
+via `tornado.web.RequestHandler.request`.
+"""
+from __future__ import absolute_import, division, print_function
+import calendar
+import collections
+import copy
+import datetime
+import email.utils
+import numbers
+import re
+import time
+from collections.abc import MutableMapping
+from salt.ext.tornado.escape import native_str, parse_qs_bytes, utf8
+from salt.ext.tornado.log import gen_log
+from salt.ext.tornado.util import PY3, ObjectDict
+if PY3:
+    import http.cookies as Cookie
+    from http.client import responses
+    from urllib.parse import urlencode, urlparse, urlunparse, parse_qsl
+else:
+    import Cookie
+    from httplib import responses
+    from urllib import urlencode
+    from urlparse import urlparse, urlunparse, parse_qsl
+responses
+try:
+    from ssl import SSLError
+except ImportError:
+    class _SSLError(Exception):
+        pass
+    SSLError = _SSLError  # type: ignore
+try:
+    import typing
+except ImportError:
+    pass
+_CRLF_RE = re.compile(r"\r?\n")
+class _NormalizedHeaderCache(dict):
+    """Dynamic cached mapping of header names to Http-Header-Case.
+    Implemented as a dict subclass so that cache hits are as fast as a
+    normal dict lookup, without the overhead of a python function
+    call.
+    >>> normalized_headers = _NormalizedHeaderCache(10)
+    >>> normalized_headers["coNtent-TYPE"]
+    'Content-Type'
+    """
+    def __init__(self, size):
+        super(_NormalizedHeaderCache, self).__init__()
+        self.size = size
+        self.queue = collections.deque()
+    def __missing__(self, key):
+        normalized = "-".join([w.capitalize() for w in key.split("-")])
+        self[key] = normalized
+        self.queue.append(key)
+        if len(self.queue) > self.size:
+            old_key = self.queue.popleft()
+            del self[old_key]
+        return normalized
+_normalized_headers = _NormalizedHeaderCache(1000)
+class HTTPHeaders(MutableMapping):
+    """A dictionary that maintains ``Http-Header-Case`` for all keys.
+    Supports multiple values per key via a pair of new methods,
+    `add()` and `get_list()`.  The regular dictionary interface
+    returns a single value per key, with multiple values joined by a
+    comma.
+    >>> h = HTTPHeaders({"content-type": "text/html"})
+    >>> list(h.keys())
+    ['Content-Type']
+    >>> h["Content-Type"]
+    'text/html'
+    >>> h.add("Set-Cookie", "A=B")
+    >>> h.add("Set-Cookie", "C=D")
+    >>> h["set-cookie"]
+    'A=B,C=D'
+    >>> h.get_list("set-cookie")
+    ['A=B', 'C=D']
+    >>> for (k,v) in sorted(h.get_all()):
+    ...    print('%s: %s' % (k,v))
+    ...
+    Content-Type: text/html
+    Set-Cookie: A=B
+    Set-Cookie: C=D
+    """
+    def __init__(self, *args, **kwargs):
+        self._dict = {}  # type: typing.Dict[str, str]
+        self._as_list = {}  # type: typing.Dict[str, typing.List[str]]
+        self._last_key = None
+        if len(args) == 1 and len(kwargs) == 0 and isinstance(args[0], HTTPHeaders):
+            for k, v in args[0].get_all():
+                self.add(k, v)
+        else:
+            self.update(*args, **kwargs)
+    def add(self, name, value):
+        """Adds a new value for the given key."""
+        norm_name = _normalized_headers[name]
+        self._last_key = norm_name
+        if norm_name in self:
+            self._dict[norm_name] = (
+                native_str(self[norm_name]) + "," + native_str(value)
+            )
+            self._as_list[norm_name].append(value)
+        else:
+            self[norm_name] = value
+    def get_list(self, name):
+        """Returns all values for the given header as a list."""
+        norm_name = _normalized_headers[name]
+        return self._as_list.get(norm_name, [])
+    def get_all(self):
+        """Returns an iterable of all (name, value) pairs.
+        If a header has multiple values, multiple pairs will be
+        returned with the same name.
+        """
+        for name, values in self._as_list.items():
+            for value in values:
+                yield (name, value)
+    def parse_line(self, line):
+        """Updates the dictionary with a single header line.
+        >>> h = HTTPHeaders()
+        >>> h.parse_line("Content-Type: text/html")
+        >>> h.get('content-type')
+        'text/html'
+        """
+        if line[0].isspace():
+            new_part = " " + line.lstrip()
+            self._as_list[self._last_key][-1] += new_part
+            self._dict[self._last_key] += new_part
+        else:
+            name, value = line.split(":", 1)
+            self.add(name, value.strip())
+    @classmethod
+    def parse(cls, headers):
+        """Returns a dictionary from HTTP header text.
+        >>> h = HTTPHeaders.parse("Content-Type: text/html\\r\\nContent-Length: 42\\r\\n")
+        >>> sorted(h.items())
+        [('Content-Length', '42'), ('Content-Type', 'text/html')]
+        """
+        h = cls()
+        for line in _CRLF_RE.split(headers):
+            if line:
+                h.parse_line(line)
+        return h
+    def __setitem__(self, name, value):
+        norm_name = _normalized_headers[name]
+        self._dict[norm_name] = value
+        self._as_list[norm_name] = [value]
+    def __getitem__(self, name):
+        return self._dict[_normalized_headers[name]]
+    def __delitem__(self, name):
+        norm_name = _normalized_headers[name]
+        del self._dict[norm_name]
+        del self._as_list[norm_name]
+    def __len__(self):
+        return len(self._dict)
+    def __iter__(self):
+        return iter(self._dict)
+    def copy(self):
+        return HTTPHeaders(self)
+    __copy__ = copy
+    def __str__(self):
+        lines = []
+        for name, value in self.get_all():
+            lines.append("%s: %s\n" % (name, value))
+        return "".join(lines)
+    __unicode__ = __str__
+class HTTPServerRequest(object):
+    """A single HTTP request.
+    All attributes are type `str` unless otherwise noted.
+    .. attribute:: method
+       HTTP request method, e.g. "GET" or "POST"
+    .. attribute:: uri
+       The requested uri.
+    .. attribute:: path
+       The path portion of `uri`
+    .. attribute:: query
+       The query portion of `uri`
+    .. attribute:: version
+       HTTP version specified in request, e.g. "HTTP/1.1"
+    .. attribute:: headers
+       `.HTTPHeaders` dictionary-like object for request headers.  Acts like
+       a case-insensitive dictionary with additional methods for repeated
+       headers.
+    .. attribute:: body
+       Request body, if present, as a byte string.
+    .. attribute:: remote_ip
+       Client's IP address as a string.  If ``HTTPServer.xheaders`` is set,
+       will pass along the real IP address provided by a load balancer
+       in the ``X-Real-Ip`` or ``X-Forwarded-For`` header.
+    .. versionchanged:: 3.1
+       The list format of ``X-Forwarded-For`` is now supported.
+    .. attribute:: protocol
+       The protocol used, either "http" or "https".  If ``HTTPServer.xheaders``
+       is set, will pass along the protocol used by a load balancer if
+       reported via an ``X-Scheme`` header.
+    .. attribute:: host
+       The requested hostname, usually taken from the ``Host`` header.
+    .. attribute:: arguments
+       GET/POST arguments are available in the arguments property, which
+       maps arguments names to lists of values (to support multiple values
+       for individual names). Names are of type `str`, while arguments
+       are byte strings.  Note that this is different from
+       `.RequestHandler.get_argument`, which returns argument values as
+       unicode strings.
+    .. attribute:: query_arguments
+       Same format as ``arguments``, but contains only arguments extracted
+       from the query string.
+       .. versionadded:: 3.2
+    .. attribute:: body_arguments
+       Same format as ``arguments``, but contains only arguments extracted
+       from the request body.
+       .. versionadded:: 3.2
+    .. attribute:: files
+       File uploads are available in the files property, which maps file
+       names to lists of `.HTTPFile`.
+    .. attribute:: connection
+       An HTTP request is attached to a single HTTP connection, which can
+       be accessed through the "connection" attribute. Since connections
+       are typically kept open in HTTP/1.1, multiple requests can be handled
+       sequentially on a single connection.
+    .. versionchanged:: 4.0
+       Moved from ``tornado.httpserver.HTTPRequest``.
+    """
+    def __init__(
+        self,
+        method=None,
+        uri=None,
+        version="HTTP/1.0",
+        headers=None,
+        body=None,
+        host=None,
+        files=None,
+        connection=None,
+        start_line=None,
+        server_connection=None,
+    ):
+        if start_line is not None:
+            method, uri, version = start_line
+        self.method = method
+        self.uri = uri
+        self.version = version
+        self.headers = headers or HTTPHeaders()
+        self.body = body or b""
+        context = getattr(connection, "context", None)
+        self.remote_ip = getattr(context, "remote_ip", None)
+        self.protocol = getattr(context, "protocol", "http")
+        self.host = host or self.headers.get("Host") or "127.0.0.1"
+        self.host_name = split_host_and_port(self.host.lower())[0]
+        self.files = files or {}
+        self.connection = connection
+        self.server_connection = server_connection
+        self._start_time = time.time()
+        self._finish_time = None
+        self.path, sep, self.query = uri.partition("?")
+        self.arguments = parse_qs_bytes(self.query, keep_blank_values=True)
+        self.query_arguments = copy.deepcopy(self.arguments)
+        self.body_arguments = {}
+    def supports_http_1_1(self):
+        """Returns True if this request supports HTTP/1.1 semantics.
+        .. deprecated:: 4.0
+           Applications are less likely to need this information with the
+           introduction of `.HTTPConnection`.  If you still need it, access
+           the ``version`` attribute directly.
+        """
+        return self.version == "HTTP/1.1"
+    @property
+    def cookies(self):
+        """A dictionary of Cookie.Morsel objects."""
+        if not hasattr(self, "_cookies"):
+            self._cookies = Cookie.SimpleCookie()
+            if "Cookie" in self.headers:
+                try:
+                    parsed = parse_cookie(self.headers["Cookie"])
+                except Exception:
+                    pass
+                else:
+                    for k, v in parsed.items():
+                        try:
+                            self._cookies[k] = v
+                        except Exception:
+                            pass
+        return self._cookies
+    def write(self, chunk, callback=None):
+        """Writes the given chunk to the response stream.
+        .. deprecated:: 4.0
+           Use ``request.connection`` and the `.HTTPConnection` methods
+           to write the response.
+        """
+        assert isinstance(chunk, bytes)
+        assert self.version.startswith(
+            "HTTP/1."
+        ), "deprecated interface only supported in HTTP/1.x"
+        self.connection.write(chunk, callback=callback)
+    def finish(self):
+        """Finishes this HTTP request on the open connection.
+        .. deprecated:: 4.0
+           Use ``request.connection`` and the `.HTTPConnection` methods
+           to write the response.
+        """
+        self.connection.finish()
+        self._finish_time = time.time()
+    def full_url(self):
+        """Reconstructs the full URL for this request."""
+        return self.protocol + "://" + self.host + self.uri
+    def request_time(self):
+        """Returns the amount of time it took for this request to execute."""
+        if self._finish_time is None:
+            return time.time() - self._start_time
+        else:
+            return self._finish_time - self._start_time
+    def get_ssl_certificate(self, binary_form=False):
+        """Returns the client's SSL certificate, if any.
+        To use client certificates, the HTTPServer's
+        `ssl.SSLContext.verify_mode` field must be set, e.g.::
+            ssl_ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)
+            ssl_ctx.load_cert_chain("foo.crt", "foo.key")
+            ssl_ctx.load_verify_locations("cacerts.pem")
+            ssl_ctx.verify_mode = ssl.CERT_REQUIRED
+            server = HTTPServer(app, ssl_options=ssl_ctx)
+        By default, the return value is a dictionary (or None, if no
+        client certificate is present).  If ``binary_form`` is true, a
+        DER-encoded form of the certificate is returned instead.  See
+        SSLSocket.getpeercert() in the standard library for more
+        details.
+        http://docs.python.org/library/ssl.html#sslsocket-objects
+        """
+        try:
+            return self.connection.stream.socket.getpeercert(binary_form=binary_form)
+        except SSLError:
+            return None
+    def _parse_body(self):
+        parse_body_arguments(
+            self.headers.get("Content-Type", ""),
+            self.body,
+            self.body_arguments,
+            self.files,
+            self.headers,
+        )
+        for k, v in self.body_arguments.items():
+            self.arguments.setdefault(k, []).extend(v)
+    def __repr__(self):
+        attrs = ("protocol", "host", "method", "uri", "version", "remote_ip")
+        args = ", ".join(["%s=%r" % (n, getattr(self, n)) for n in attrs])
+        return "%s(%s, headers=%s)" % (
+            self.__class__.__name__,
+            args,
+            dict(self.headers),
+        )
+class HTTPInputError(Exception):
+    """Exception class for malformed HTTP requests or responses
+    from remote sources.
+    .. versionadded:: 4.0
+    """
+    pass
+class HTTPOutputError(Exception):
+    """Exception class for errors in HTTP output.
+    .. versionadded:: 4.0
+    """
+    pass
+class HTTPServerConnectionDelegate(object):
+    """Implement this interface to handle requests from `.HTTPServer`.
+    .. versionadded:: 4.0
+    """
+    def start_request(self, server_conn, request_conn):
+        """This method is called by the server when a new request has started.
+        :arg server_conn: is an opaque object representing the long-lived
+            (e.g. tcp-level) connection.
+        :arg request_conn: is a `.HTTPConnection` object for a single
+            request/response exchange.
+        This method should return a `.HTTPMessageDelegate`.
+        """
+        raise NotImplementedError()
+    def on_close(self, server_conn):
+        """This method is called when a connection has been closed.
+        :arg server_conn: is a server connection that has previously been
+            passed to ``start_request``.
+        """
+        pass
+class HTTPMessageDelegate(object):
+    """Implement this interface to handle an HTTP request or response.
+    .. versionadded:: 4.0
+    """
+    def headers_received(self, start_line, headers):
+        """Called when the HTTP headers have been received and parsed.
+        :arg start_line: a `.RequestStartLine` or `.ResponseStartLine`
+            depending on whether this is a client or server message.
+        :arg headers: a `.HTTPHeaders` instance.
+        Some `.HTTPConnection` methods can only be called during
+        ``headers_received``.
+        May return a `.Future`; if it does the body will not be read
+        until it is done.
+        """
+        pass
+    def data_received(self, chunk):
+        """Called when a chunk of data has been received.
+        May return a `.Future` for flow control.
+        """
+        pass
+    def finish(self):
+        """Called after the last chunk of data has been received."""
+        pass
+    def on_connection_close(self):
+        """Called if the connection is closed without finishing the request.
+        If ``headers_received`` is called, either ``finish`` or
+        ``on_connection_close`` will be called, but not both.
+        """
+        pass
+class HTTPConnection(object):
+    """Applications use this interface to write their responses.
+    .. versionadded:: 4.0
+    """
+    def write_headers(self, start_line, headers, chunk=None, callback=None):
+        """Write an HTTP header block.
+        :arg start_line: a `.RequestStartLine` or `.ResponseStartLine`.
+        :arg headers: a `.HTTPHeaders` instance.
+        :arg chunk: the first (optional) chunk of data.  This is an optimization
+            so that small responses can be written in the same call as their
+            headers.
+        :arg callback: a callback to be run when the write is complete.
+        The ``version`` field of ``start_line`` is ignored.
+        Returns a `.Future` if no callback is given.
+        """
+        raise NotImplementedError()
+    def write(self, chunk, callback=None):
+        """Writes a chunk of body data.
+        The callback will be run when the write is complete.  If no callback
+        is given, returns a Future.
+        """
+        raise NotImplementedError()
+    def finish(self):
+        """Indicates that the last body data has been written.
+        """
+        raise NotImplementedError()
+def url_concat(url, args):
+    """Concatenate url and arguments regardless of whether
+    url has existing query parameters.
+    ``args`` may be either a dictionary or a list of key-value pairs
+    (the latter allows for multiple values with the same key.
+    >>> url_concat("http://example.com/foo", dict(c="d"))
+    'http://example.com/foo?c=d'
+    >>> url_concat("http://example.com/foo?a=b", dict(c="d"))
+    'http://example.com/foo?a=b&c=d'
+    >>> url_concat("http://example.com/foo?a=b", [("c", "d"), ("c", "d2")])
+    'http://example.com/foo?a=b&c=d&c=d2'
+    """
+    if args is None:
+        return url
+    parsed_url = urlparse(url)
+    if isinstance(args, dict):
+        parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)
+        parsed_query.extend(args.items())
+    elif isinstance(args, list) or isinstance(args, tuple):
+        parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)
+        parsed_query.extend(args)
+    else:
+        err = "'args' parameter should be dict, list or tuple. Not {0}".format(
+            type(args)
+        )
+        raise TypeError(err)
+    final_query = urlencode(parsed_query)
+    url = urlunparse(
+        (
+            parsed_url[0],
+            parsed_url[1],
+            parsed_url[2],
+            parsed_url[3],
+            final_query,
+            parsed_url[5],
+        )
+    )
+    return url
+class HTTPFile(ObjectDict):
+    """Represents a file uploaded via a form.
+    For backwards compatibility, its instance attributes are also
+    accessible as dictionary keys.
+    * ``filename``
+    * ``body``
+    * ``content_type``
+    """
+    pass
+def _parse_request_range(range_header):
+    """Parses a Range header.
+    Returns either ``None`` or tuple ``(start, end)``.
+    Note that while the HTTP headers use inclusive byte positions,
+    this method returns indexes suitable for use in slices.
+    >>> start, end = _parse_request_range("bytes=1-2")
+    >>> start, end
+    (1, 3)
+    >>> [0, 1, 2, 3, 4][start:end]
+    [1, 2]
+    >>> _parse_request_range("bytes=6-")
+    (6, None)
+    >>> _parse_request_range("bytes=-6")
+    (-6, None)
+    >>> _parse_request_range("bytes=-0")
+    (None, 0)
+    >>> _parse_request_range("bytes=")
+    (None, None)
+    >>> _parse_request_range("foo=42")
+    >>> _parse_request_range("bytes=1-2,6-10")
+    Note: only supports one range (ex, ``bytes=1-2,6-10`` is not allowed).
+    See [0] for the details of the range header.
+    [0]: http://greenbytes.de/tech/webdav/draft-ietf-httpbis-p5-range-latest.html#byte.ranges
+    """
+    unit, _, value = range_header.partition("=")
+    unit, value = unit.strip(), value.strip()
+    if unit != "bytes":
+        return None
+    start_b, _, end_b = value.partition("-")
+    try:
+        start = _int_or_none(start_b)
+        end = _int_or_none(end_b)
+    except ValueError:
+        return None
+    if end is not None:
+        if start is None:
+            if end != 0:
+                start = -end
+                end = None
+        else:
+            end += 1
+    return (start, end)
+def _get_content_range(start, end, total):
+    """Returns a suitable Content-Range header:
+    >>> print(_get_content_range(None, 1, 4))
+    bytes 0-0/4
+    >>> print(_get_content_range(1, 3, 4))
+    bytes 1-2/4
+    >>> print(_get_content_range(None, None, 4))
+    bytes 0-3/4
+    """
+    start = start or 0
+    end = (end or total) - 1
+    return "bytes %s-%s/%s" % (start, end, total)
+def _int_or_none(val):
+    val = val.strip()
+    if val == "":
+        return None
+    return int(val)
+def parse_body_arguments(content_type, body, arguments, files, headers=None):
+    """Parses a form request body.
+    Supports ``application/x-www-form-urlencoded`` and
+    ``multipart/form-data``.  The ``content_type`` parameter should be
+    a string and ``body`` should be a byte string.  The ``arguments``
+    and ``files`` parameters are dictionaries that will be updated
+    with the parsed contents.
+    """
+    if headers and "Content-Encoding" in headers:
+        gen_log.warning("Unsupported Content-Encoding: %s", headers["Content-Encoding"])
+        return
+    if content_type.startswith("application/x-www-form-urlencoded"):
+        try:
+            uri_arguments = parse_qs_bytes(native_str(body), keep_blank_values=True)
+        except Exception as e:
+            gen_log.warning("Invalid x-www-form-urlencoded body: %s", e)
+            uri_arguments = {}
+        for name, values in uri_arguments.items():
+            if values:
+                arguments.setdefault(name, []).extend(values)
+    elif content_type.startswith("multipart/form-data"):
+        try:
+            fields = content_type.split(";")
+            for field in fields:
+                k, sep, v = field.strip().partition("=")
+                if k == "boundary" and v:
+                    parse_multipart_form_data(utf8(v), body, arguments, files)
+                    break
+            else:
+                raise ValueError("multipart boundary not found")
+        except Exception as e:
+            gen_log.warning("Invalid multipart/form-data: %s", e)
+def parse_multipart_form_data(boundary, data, arguments, files):
+    """Parses a ``multipart/form-data`` body.
+    The ``boundary`` and ``data`` parameters are both byte strings.
+    The dictionaries given in the arguments and files parameters
+    will be updated with the contents of the body.
+    """
+    if boundary.startswith(b'"') and boundary.endswith(b'"'):
+        boundary = boundary[1:-1]
+    final_boundary_index = data.rfind(b"--" + boundary + b"--")
+    if final_boundary_index == -1:
+        gen_log.warning("Invalid multipart/form-data: no final boundary")
+        return
+    parts = data[:final_boundary_index].split(b"--" + boundary + b"\r\n")
+    for part in parts:
+        if not part:
+            continue
+        eoh = part.find(b"\r\n\r\n")
+        if eoh == -1:
+            gen_log.warning("multipart/form-data missing headers")
+            continue
+        headers = HTTPHeaders.parse(part[:eoh].decode("utf-8"))
+        disp_header = headers.get("Content-Disposition", "")
+        disposition, disp_params = _parse_header(disp_header)
+        if disposition != "form-data" or not part.endswith(b"\r\n"):
+            gen_log.warning("Invalid multipart/form-data")
+            continue
+        value = part[eoh + 4 : -2]
+        if not disp_params.get("name"):
+            gen_log.warning("multipart/form-data value missing name")
+            continue
+        name = disp_params["name"]
+        if disp_params.get("filename"):
+            ctype = headers.get("Content-Type", "application/unknown")
+            files.setdefault(name, []).append(
+                HTTPFile(  # type: ignore
+                    filename=disp_params["filename"], body=value, content_type=ctype
+                )
+            )
+        else:
+            arguments.setdefault(name, []).append(value)
+def format_timestamp(ts):
+    """Formats a timestamp in the format used by HTTP.
+    The argument may be a numeric timestamp as returned by `time.time`,
+    a time tuple as returned by `time.gmtime`, or a `datetime.datetime`
+    object.
+    >>> format_timestamp(1359312200)
+    'Sun, 27 Jan 2013 18:43:20 GMT'
+    """
+    if isinstance(ts, numbers.Real):
+        pass
+    elif isinstance(ts, (tuple, time.struct_time)):
+        ts = calendar.timegm(ts)
+    elif isinstance(ts, datetime.datetime):
+        ts = calendar.timegm(ts.utctimetuple())
+    else:
+        raise TypeError("unknown timestamp type: %r" % ts)
+    return email.utils.formatdate(ts, usegmt=True)
+RequestStartLine = collections.namedtuple(
+    "RequestStartLine", ["method", "path", "version"]
+)
+def parse_request_start_line(line):
+    """Returns a (method, path, version) tuple for an HTTP 1.x request line.
+    The response is a `collections.namedtuple`.
+    >>> parse_request_start_line("GET /foo HTTP/1.1")
+    RequestStartLine(method='GET', path='/foo', version='HTTP/1.1')
+    """
+    try:
+        method, path, version = line.split(" ")
+    except ValueError:
+        raise HTTPInputError("Malformed HTTP request line")
+    if not re.match(r"^HTTP/1\.[0-9]$", version):
+        raise HTTPInputError(
+            "Malformed HTTP version in HTTP Request-Line: %r" % version
+        )
+    return RequestStartLine(method, path, version)
+ResponseStartLine = collections.namedtuple(
+    "ResponseStartLine", ["version", "code", "reason"]
+)
+def parse_response_start_line(line):
+    """Returns a (version, code, reason) tuple for an HTTP 1.x response line.
+    The response is a `collections.namedtuple`.
+    >>> parse_response_start_line("HTTP/1.1 200 OK")
+    ResponseStartLine(version='HTTP/1.1', code=200, reason='OK')
+    """
+    line = native_str(line)
+    match = re.match("(HTTP/1.[0-9]) ([0-9]+) ([^\r]*)", line)
+    if not match:
+        raise HTTPInputError("Error parsing response start line")
+    return ResponseStartLine(match.group(1), int(match.group(2)), match.group(3))
+def _parseparam(s):
+    while s[:1] == ";":
+        s = s[1:]
+        end = s.find(";")
+        while end > 0 and (s.count('"', 0, end) - s.count('\\"', 0, end)) % 2:
+            end = s.find(";", end + 1)
+        if end < 0:
+            end = len(s)
+        f = s[:end]
+        yield f.strip()
+        s = s[end:]
+def _parse_header(line):
+    """Parse a Content-type like header.
+    Return the main content-type and a dictionary of options.
+    """
+    parts = _parseparam(";" + line)
+    key = next(parts)
+    pdict = {}
+    for p in parts:
+        i = p.find("=")
+        if i >= 0:
+            name = p[:i].strip().lower()
+            value = p[i + 1 :].strip()
+            if len(value) >= 2 and value[0] == value[-1] == '"':
+                value = value[1:-1]
+                value = value.replace("\\\\", "\\").replace('\\"', '"')
+            pdict[name] = value
+        else:
+            pdict[p] = None
+    return key, pdict
+def _encode_header(key, pdict):
+    """Inverse of _parse_header.
+    >>> _encode_header('permessage-deflate',
+    ...     {'client_max_window_bits': 15, 'client_no_context_takeover': None})
+    'permessage-deflate; client_max_window_bits=15; client_no_context_takeover'
+    """
+    if not pdict:
+        return key
+    out = [key]
+    for k, v in sorted(pdict.items()):
+        if v is None:
+            out.append(k)
+        else:
+            out.append("%s=%s" % (k, v))
+    return "; ".join(out)
+def doctests():
+    import doctest
+    return doctest.DocTestSuite()
+def split_host_and_port(netloc):
+    """Returns ``(host, port)`` tuple from ``netloc``.
+    Returned ``port`` will be ``None`` if not present.
+    .. versionadded:: 4.1
+    """
+    match = re.match(r"^(.+):(\d+)$", netloc)
+    if match:
+        host = match.group(1)
+        port = int(match.group(2))
+    else:
+        host = netloc
+        port = None
+    return (host, port)
+_OctalPatt = re.compile(r"\\[0-3][0-7][0-7]")
+_QuotePatt = re.compile(r"[\\].")
+_nulljoin = "".join
+def _unquote_cookie(str):
+    """Handle double quotes and escaping in cookie values.
+    This method is copied verbatim from the Python 3.5 standard
+    library (http.cookies._unquote) so we don't have to depend on
+    non-public interfaces.
+    """
+    if str is None or len(str) < 2:
+        return str
+    if str[0] != '"' or str[-1] != '"':
+        return str
+    str = str[1:-1]
+    i = 0
+    n = len(str)
+    res = []
+    while 0 <= i < n:
+        o_match = _OctalPatt.search(str, i)
+        q_match = _QuotePatt.search(str, i)
+        if not o_match and not q_match:  # Neither matched
+            res.append(str[i:])
+            break
+        j = k = -1
+        if o_match:
+            j = o_match.start(0)
+        if q_match:
+            k = q_match.start(0)
+        if q_match and (not o_match or k < j):  # QuotePatt matched
+            res.append(str[i:k])
+            res.append(str[k + 1])
+            i = k + 2
+        else:  # OctalPatt matched
+            res.append(str[i:j])
+            res.append(chr(int(str[j + 1 : j + 4], 8)))
+            i = j + 4
+    return _nulljoin(res)
+def parse_cookie(cookie):
+    """Parse a ``Cookie`` HTTP header into a dict of name/value pairs.
+    This function attempts to mimic browser cookie parsing behavior;
+    it specifically does not follow any of the cookie-related RFCs
+    (because browsers don't either).
+    The algorithm used is identical to that used by Django version 1.9.10.
+    .. versionadded:: 4.4.2
+    """
+    cookiedict = {}
+    for chunk in cookie.split(str(";")):
+        if str("=") in chunk:
+            key, val = chunk.split(str("="), 1)
+        else:
+            key, val = str(""), chunk
+        key, val = key.strip(), val.strip()
+        if key or val:
+            cookiedict[key] = _unquote_cookie(val)
+    return cookiedict

--- a//dev/null
+++ b/salt/ext/tornado/ioloop.py
@@ -0,0 +1,762 @@
+"""An I/O event loop for non-blocking sockets.
+Typical applications will use a single `IOLoop` object, in the
+`IOLoop.instance` singleton.  The `IOLoop.start` method should usually
+be called at the end of the ``main()`` function.  Atypical applications may
+use more than one `IOLoop`, such as one `IOLoop` per thread, or per `unittest`
+case.
+In addition to I/O events, the `IOLoop` can also schedule time-based events.
+`IOLoop.add_timeout` is a non-blocking alternative to `time.sleep`.
+"""
+from __future__ import absolute_import, division, print_function
+import collections
+import datetime
+import errno
+import functools
+import heapq
+import itertools
+import logging
+import numbers
+import os
+import select
+import sys
+import threading
+import time
+import traceback
+import math
+from salt.ext.tornado.concurrent import TracebackFuture, is_future
+from salt.ext.tornado.log import app_log, gen_log
+from salt.ext.tornado.platform.auto import set_close_exec, Waker
+from salt.ext.tornado import stack_context
+from salt.ext.tornado.util import PY3, Configurable, errno_from_exception, timedelta_to_seconds
+try:
+    import signal
+except ImportError:
+    signal = None
+if PY3:
+    import _thread as thread
+else:
+    import thread
+_POLL_TIMEOUT = 3600.0
+class TimeoutError(Exception):
+    pass
+class IOLoop(Configurable):
+    """A level-triggered I/O loop.
+    We use ``epoll`` (Linux) or ``kqueue`` (BSD and Mac OS X) if they
+    are available, or else we fall back on select(). If you are
+    implementing a system that needs to handle thousands of
+    simultaneous connections, you should use a system that supports
+    either ``epoll`` or ``kqueue``.
+    Example usage for a simple TCP server:
+    .. testcode::
+        import errno
+        import functools
+        import tornado.ioloop
+        import socket
+        def connection_ready(sock, fd, events):
+            while True:
+                try:
+                    connection, address = sock.accept()
+                except socket.error as e:
+                    if e.args[0] not in (errno.EWOULDBLOCK, errno.EAGAIN):
+                        raise
+                    return
+                connection.setblocking(0)
+                handle_connection(connection, address)
+        if __name__ == '__main__':
+            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)
+            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+            sock.setblocking(0)
+            sock.bind(("", port))
+            sock.listen(128)
+            io_loop = tornado.ioloop.IOLoop.current()
+            callback = functools.partial(connection_ready, sock)
+            io_loop.add_handler(sock.fileno(), callback, io_loop.READ)
+            io_loop.start()
+    .. testoutput::
+       :hide:
+    By default, a newly-constructed `IOLoop` becomes the thread's current
+    `IOLoop`, unless there already is a current `IOLoop`. This behavior
+    can be controlled with the ``make_current`` argument to the `IOLoop`
+    constructor: if ``make_current=True``, the new `IOLoop` will always
+    try to become current and it raises an error if there is already a
+    current instance. If ``make_current=False``, the new `IOLoop` will
+    not try to become current.
+    .. versionchanged:: 4.2
+       Added the ``make_current`` keyword argument to the `IOLoop`
+       constructor.
+    """
+    _EPOLLIN = 0x001
+    _EPOLLPRI = 0x002
+    _EPOLLOUT = 0x004
+    _EPOLLERR = 0x008
+    _EPOLLHUP = 0x010
+    _EPOLLRDHUP = 0x2000
+    _EPOLLONESHOT = (1 << 30)
+    _EPOLLET = (1 << 31)
+    NONE = 0
+    READ = _EPOLLIN
+    WRITE = _EPOLLOUT
+    ERROR = _EPOLLERR | _EPOLLHUP
+    _instance_lock = threading.Lock()
+    _current = threading.local()
+    @staticmethod
+    def instance():
+        """Returns a global `IOLoop` instance.
+        Most applications have a single, global `IOLoop` running on the
+        main thread.  Use this method to get this instance from
+        another thread.  In most other cases, it is better to use `current()`
+        to get the current thread's `IOLoop`.
+        """
+        if not hasattr(IOLoop, "_instance"):
+            with IOLoop._instance_lock:
+                if not hasattr(IOLoop, "_instance"):
+                    IOLoop._instance = IOLoop()
+        return IOLoop._instance
+    @staticmethod
+    def initialized():
+        """Returns true if the singleton instance has been created."""
+        return hasattr(IOLoop, "_instance")
+    def install(self):
+        """Installs this `IOLoop` object as the singleton instance.
+        This is normally not necessary as `instance()` will create
+        an `IOLoop` on demand, but you may want to call `install` to use
+        a custom subclass of `IOLoop`.
+        When using an `IOLoop` subclass, `install` must be called prior
+        to creating any objects that implicitly create their own
+        `IOLoop` (e.g., :class:`tornado.httpclient.AsyncHTTPClient`).
+        """
+        assert not IOLoop.initialized()
+        IOLoop._instance = self
+    @staticmethod
+    def clear_instance():
+        """Clear the global `IOLoop` instance.
+        .. versionadded:: 4.0
+        """
+        if hasattr(IOLoop, "_instance"):
+            del IOLoop._instance
+    @staticmethod
+    def current(instance=True):
+        """Returns the current thread's `IOLoop`.
+        If an `IOLoop` is currently running or has been marked as
+        current by `make_current`, returns that instance.  If there is
+        no current `IOLoop`, returns `IOLoop.instance()` (i.e. the
+        main thread's `IOLoop`, creating one if necessary) if ``instance``
+        is true.
+        In general you should use `IOLoop.current` as the default when
+        constructing an asynchronous object, and use `IOLoop.instance`
+        when you mean to communicate to the main thread from a different
+        one.
+        .. versionchanged:: 4.1
+           Added ``instance`` argument to control the fallback to
+           `IOLoop.instance()`.
+        """
+        current = getattr(IOLoop._current, "instance", None)
+        if current is None and instance:
+            return IOLoop.instance()
+        return current
+    def make_current(self):
+        """Makes this the `IOLoop` for the current thread.
+        An `IOLoop` automatically becomes current for its thread
+        when it is started, but it is sometimes useful to call
+        `make_current` explicitly before starting the `IOLoop`,
+        so that code run at startup time can find the right
+        instance.
+        .. versionchanged:: 4.1
+           An `IOLoop` created while there is no current `IOLoop`
+           will automatically become current.
+        """
+        IOLoop._current.instance = self
+    @staticmethod
+    def clear_current():
+        IOLoop._current.instance = None
+    @classmethod
+    def configurable_base(cls):
+        return IOLoop
+    @classmethod
+    def configurable_default(cls):
+        if hasattr(select, "epoll"):
+            from salt.ext.tornado.platform.epoll import EPollIOLoop
+            return EPollIOLoop
+        if hasattr(select, "kqueue"):
+            from salt.ext.tornado.platform.kqueue import KQueueIOLoop
+            return KQueueIOLoop
+        from salt.ext.tornado.platform.select import SelectIOLoop
+        return SelectIOLoop
+    def initialize(self, make_current=None):
+        if make_current is None:
+            if IOLoop.current(instance=False) is None:
+                self.make_current()
+        elif make_current:
+            if IOLoop.current(instance=False) is not None:
+                raise RuntimeError("current IOLoop already exists")
+            self.make_current()
+    def close(self, all_fds=False):
+        """Closes the `IOLoop`, freeing any resources used.
+        If ``all_fds`` is true, all file descriptors registered on the
+        IOLoop will be closed (not just the ones created by the
+        `IOLoop` itself).
+        Many applications will only use a single `IOLoop` that runs for the
+        entire lifetime of the process.  In that case closing the `IOLoop`
+        is not necessary since everything will be cleaned up when the
+        process exits.  `IOLoop.close` is provided mainly for scenarios
+        such as unit tests, which create and destroy a large number of
+        ``IOLoops``.
+        An `IOLoop` must be completely stopped before it can be closed.  This
+        means that `IOLoop.stop()` must be called *and* `IOLoop.start()` must
+        be allowed to return before attempting to call `IOLoop.close()`.
+        Therefore the call to `close` will usually appear just after
+        the call to `start` rather than near the call to `stop`.
+        .. versionchanged:: 3.1
+           If the `IOLoop` implementation supports non-integer objects
+           for "file descriptors", those objects will have their
+           ``close`` method when ``all_fds`` is true.
+        """
+        raise NotImplementedError()
+    def add_handler(self, fd, handler, events):
+        """Registers the given handler to receive the given events for ``fd``.
+        The ``fd`` argument may either be an integer file descriptor or
+        a file-like object with a ``fileno()`` method (and optionally a
+        ``close()`` method, which may be called when the `IOLoop` is shut
+        down).
+        The ``events`` argument is a bitwise or of the constants
+        ``IOLoop.READ``, ``IOLoop.WRITE``, and ``IOLoop.ERROR``.
+        When an event occurs, ``handler(fd, events)`` will be run.
+        .. versionchanged:: 4.0
+           Added the ability to pass file-like objects in addition to
+           raw file descriptors.
+        """
+        raise NotImplementedError()
+    def update_handler(self, fd, events):
+        """Changes the events we listen for ``fd``.
+        .. versionchanged:: 4.0
+           Added the ability to pass file-like objects in addition to
+           raw file descriptors.
+        """
+        raise NotImplementedError()
+    def remove_handler(self, fd):
+        """Stop listening for events on ``fd``.
+        .. versionchanged:: 4.0
+           Added the ability to pass file-like objects in addition to
+           raw file descriptors.
+        """
+        raise NotImplementedError()
+    def set_blocking_signal_threshold(self, seconds, action):
+        """Sends a signal if the `IOLoop` is blocked for more than
+        ``s`` seconds.
+        Pass ``seconds=None`` to disable.  Requires Python 2.6 on a unixy
+        platform.
+        The action parameter is a Python signal handler.  Read the
+        documentation for the `signal` module for more information.
+        If ``action`` is None, the process will be killed if it is
+        blocked for too long.
+        """
+        raise NotImplementedError()
+    def set_blocking_log_threshold(self, seconds):
+        """Logs a stack trace if the `IOLoop` is blocked for more than
+        ``s`` seconds.
+        Equivalent to ``set_blocking_signal_threshold(seconds,
+        self.log_stack)``
+        """
+        self.set_blocking_signal_threshold(seconds, self.log_stack)
+    def log_stack(self, signal, frame):
+        """Signal handler to log the stack trace of the current thread.
+        For use with `set_blocking_signal_threshold`.
+        """
+        gen_log.warning('IOLoop blocked for %f seconds in\n%s',
+                        self._blocking_signal_threshold,
+                        ''.join(traceback.format_stack(frame)))
+    def start(self):
+        """Starts the I/O loop.
+        The loop will run until one of the callbacks calls `stop()`, which
+        will make the loop stop after the current event iteration completes.
+        """
+        raise NotImplementedError()
+    def _setup_logging(self):
+        """The IOLoop catches and logs exceptions, so it's
+        important that log output be visible.  However, python's
+        default behavior for non-root loggers (prior to python
+        3.2) is to print an unhelpful "no handlers could be
+        found" message rather than the actual log entry, so we
+        must explicitly configure logging if we've made it this
+        far without anything.
+        This method should be called from start() in subclasses.
+        """
+        if not any([logging.getLogger().handlers,
+                    logging.getLogger('tornado').handlers,
+                    logging.getLogger('tornado.application').handlers]):
+            logging.basicConfig()
+    def stop(self):
+        """Stop the I/O loop.
+        If the event loop is not currently running, the next call to `start()`
+        will return immediately.
+        To use asynchronous methods from otherwise-synchronous code (such as
+        unit tests), you can start and stop the event loop like this::
+          ioloop = IOLoop()
+          async_method(ioloop=ioloop, callback=ioloop.stop)
+          ioloop.start()
+        ``ioloop.start()`` will return after ``async_method`` has run
+        its callback, whether that callback was invoked before or
+        after ``ioloop.start``.
+        Note that even after `stop` has been called, the `IOLoop` is not
+        completely stopped until `IOLoop.start` has also returned.
+        Some work that was scheduled before the call to `stop` may still
+        be run before the `IOLoop` shuts down.
+        """
+        raise NotImplementedError()
+    def run_sync(self, func, timeout=None):
+        """Starts the `IOLoop`, runs the given function, and stops the loop.
+        The function must return either a yieldable object or
+        ``None``. If the function returns a yieldable object, the
+        `IOLoop` will run until the yieldable is resolved (and
+        `run_sync()` will return the yieldable's result). If it raises
+        an exception, the `IOLoop` will stop and the exception will be
+        re-raised to the caller.
+        The keyword-only argument ``timeout`` may be used to set
+        a maximum duration for the function.  If the timeout expires,
+        a `TimeoutError` is raised.
+        This method is useful in conjunction with `tornado.gen.coroutine`
+        to allow asynchronous calls in a ``main()`` function::
+            @gen.coroutine
+            def main():
+            if __name__ == '__main__':
+                IOLoop.current().run_sync(main)
+        .. versionchanged:: 4.3
+           Returning a non-``None``, non-yieldable value is now an error.
+        """
+        future_cell = [None]
+        def run():
+            try:
+                result = func()
+                if result is not None:
+                    from salt.ext.tornado.gen import convert_yielded
+                    result = convert_yielded(result)
+            except Exception:
+                future_cell[0] = TracebackFuture()
+                future_cell[0].set_exc_info(sys.exc_info())
+            else:
+                if is_future(result):
+                    future_cell[0] = result
+                else:
+                    future_cell[0] = TracebackFuture()
+                    future_cell[0].set_result(result)
+            self.add_future(future_cell[0], lambda future: self.stop())
+        self.add_callback(run)
+        if timeout is not None:
+            timeout_handle = self.add_timeout(self.time() + timeout, self.stop)
+        self.start()
+        if timeout is not None:
+            self.remove_timeout(timeout_handle)
+        if not future_cell[0].done():
+            raise TimeoutError('Operation timed out after %s seconds' % timeout)
+        return future_cell[0].result()
+    def time(self):
+        """Returns the current time according to the `IOLoop`'s clock.
+        The return value is a floating-point number relative to an
+        unspecified time in the past.
+        By default, the `IOLoop`'s time function is `time.time`.  However,
+        it may be configured to use e.g. `time.monotonic` instead.
+        Calls to `add_timeout` that pass a number instead of a
+        `datetime.timedelta` should use this function to compute the
+        appropriate time, so they can work no matter what time function
+        is chosen.
+        """
+        return time.time()
+    def add_timeout(self, deadline, callback, *args, **kwargs):
+        """Runs the ``callback`` at the time ``deadline`` from the I/O loop.
+        Returns an opaque handle that may be passed to
+        `remove_timeout` to cancel.
+        ``deadline`` may be a number denoting a time (on the same
+        scale as `IOLoop.time`, normally `time.time`), or a
+        `datetime.timedelta` object for a deadline relative to the
+        current time.  Since Tornado 4.0, `call_later` is a more
+        convenient alternative for the relative case since it does not
+        require a timedelta object.
+        Note that it is not safe to call `add_timeout` from other threads.
+        Instead, you must use `add_callback` to transfer control to the
+        `IOLoop`'s thread, and then call `add_timeout` from there.
+        Subclasses of IOLoop must implement either `add_timeout` or
+        `call_at`; the default implementations of each will call
+        the other.  `call_at` is usually easier to implement, but
+        subclasses that wish to maintain compatibility with Tornado
+        versions prior to 4.0 must use `add_timeout` instead.
+        .. versionchanged:: 4.0
+           Now passes through ``*args`` and ``**kwargs`` to the callback.
+        """
+        if isinstance(deadline, numbers.Real):
+            return self.call_at(deadline, callback, *args, **kwargs)
+        elif isinstance(deadline, datetime.timedelta):
+            return self.call_at(self.time() + timedelta_to_seconds(deadline),
+                                callback, *args, **kwargs)
+        else:
+            raise TypeError("Unsupported deadline %r" % deadline)
+    def call_later(self, delay, callback, *args, **kwargs):
+        """Runs the ``callback`` after ``delay`` seconds have passed.
+        Returns an opaque handle that may be passed to `remove_timeout`
+        to cancel.  Note that unlike the `asyncio` method of the same
+        name, the returned object does not have a ``cancel()`` method.
+        See `add_timeout` for comments on thread-safety and subclassing.
+        .. versionadded:: 4.0
+        """
+        return self.call_at(self.time() + delay, callback, *args, **kwargs)
+    def call_at(self, when, callback, *args, **kwargs):
+        """Runs the ``callback`` at the absolute time designated by ``when``.
+        ``when`` must be a number using the same reference point as
+        `IOLoop.time`.
+        Returns an opaque handle that may be passed to `remove_timeout`
+        to cancel.  Note that unlike the `asyncio` method of the same
+        name, the returned object does not have a ``cancel()`` method.
+        See `add_timeout` for comments on thread-safety and subclassing.
+        .. versionadded:: 4.0
+        """
+        return self.add_timeout(when, callback, *args, **kwargs)
+    def remove_timeout(self, timeout):
+        """Cancels a pending timeout.
+        The argument is a handle as returned by `add_timeout`.  It is
+        safe to call `remove_timeout` even if the callback has already
+        been run.
+        """
+        raise NotImplementedError()
+    def add_callback(self, callback, *args, **kwargs):
+        """Calls the given callback on the next I/O loop iteration.
+        It is safe to call this method from any thread at any time,
+        except from a signal handler.  Note that this is the **only**
+        method in `IOLoop` that makes this thread-safety guarantee; all
+        other interaction with the `IOLoop` must be done from that
+        `IOLoop`'s thread.  `add_callback()` may be used to transfer
+        control from other threads to the `IOLoop`'s thread.
+        To add a callback from a signal handler, see
+        `add_callback_from_signal`.
+        """
+        raise NotImplementedError()
+    def add_callback_from_signal(self, callback, *args, **kwargs):
+        """Calls the given callback on the next I/O loop iteration.
+        Safe for use from a Python signal handler; should not be used
+        otherwise.
+        Callbacks added with this method will be run without any
+        `.stack_context`, to avoid picking up the context of the function
+        that was interrupted by the signal.
+        """
+        raise NotImplementedError()
+    def spawn_callback(self, callback, *args, **kwargs):
+        """Calls the given callback on the next IOLoop iteration.
+        Unlike all other callback-related methods on IOLoop,
+        ``spawn_callback`` does not associate the callback with its caller's
+        ``stack_context``, so it is suitable for fire-and-forget callbacks
+        that should not interfere with the caller.
+        .. versionadded:: 4.0
+        """
+        with stack_context.NullContext():
+            self.add_callback(callback, *args, **kwargs)
+    def add_future(self, future, callback):
+        """Schedules a callback on the ``IOLoop`` when the given
+        `.Future` is finished.
+        The callback is invoked with one argument, the
+        `.Future`.
+        """
+        assert is_future(future)
+        callback = stack_context.wrap(callback)
+        future.add_done_callback(
+            lambda future: self.add_callback(callback, future))
+    def _run_callback(self, callback):
+        """Runs a callback with error handling.
+        For use in subclasses.
+        """
+        try:
+            ret = callback()
+            if ret is not None:
+                import salt.ext.tornado.gen
+                try:
+                    ret = salt.ext.tornado.gen.convert_yielded(ret)
+                except salt.ext.tornado.gen.BadYieldError:
+                    pass
+                else:
+                    self.add_future(ret, self._discard_future_result)
+        except Exception:
+            self.handle_callback_exception(callback)
+    def _discard_future_result(self, future):
+        """Avoid unhandled-exception warnings from spawned coroutines."""
+        future.result()
+    def handle_callback_exception(self, callback):
+        """This method is called whenever a callback run by the `IOLoop`
+        throws an exception.
+        By default simply logs the exception as an error.  Subclasses
+        may override this method to customize reporting of exceptions.
+        The exception itself is not passed explicitly, but is available
+        in `sys.exc_info`.
+        """
+        app_log.error("Exception in callback %r", callback, exc_info=True)
+    def split_fd(self, fd):
+        """Returns an (fd, obj) pair from an ``fd`` parameter.
+        We accept both raw file descriptors and file-like objects as
+        input to `add_handler` and related methods.  When a file-like
+        object is passed, we must retain the object itself so we can
+        close it correctly when the `IOLoop` shuts down, but the
+        poller interfaces favor file descriptors (they will accept
+        file-like objects and call ``fileno()`` for you, but they
+        always return the descriptor itself).
+        This method is provided for use by `IOLoop` subclasses and should
+        not generally be used by application code.
+        .. versionadded:: 4.0
+        """
+        try:
+            return fd.fileno(), fd
+        except AttributeError:
+            return fd, fd
+    def close_fd(self, fd):
+        """Utility method to close an ``fd``.
+        If ``fd`` is a file-like object, we close it directly; otherwise
+        we use `os.close`.
+        This method is provided for use by `IOLoop` subclasses (in
+        implementations of ``IOLoop.close(all_fds=True)`` and should
+        not generally be used by application code.
+        .. versionadded:: 4.0
+        """
+        try:
+            try:
+                fd.close()
+            except AttributeError:
+                os.close(fd)
+        except OSError:
+            pass
+class PollIOLoop(IOLoop):
+    """Base class for IOLoops built around a select-like function.
+    For concrete implementations, see `tornado.platform.epoll.EPollIOLoop`
+    (Linux), `tornado.platform.kqueue.KQueueIOLoop` (BSD and Mac), or
+    `tornado.platform.select.SelectIOLoop` (all platforms).
+    """
+    def initialize(self, impl, time_func=None, **kwargs):
+        super(PollIOLoop, self).initialize(**kwargs)
+        self._impl = impl
+        if hasattr(self._impl, 'fileno'):
+            set_close_exec(self._impl.fileno())
+        self.time_func = time_func or time.time
+        self._handlers = {}
+        self._events = {}
+        self._callbacks = collections.deque()
+        self._timeouts = []
+        self._cancellations = 0
+        self._running = False
+        self._stopped = False
+        self._closing = False
+        self._thread_ident = None
+        self._blocking_signal_threshold = None
+        self._timeout_counter = itertools.count()
+        self._waker = Waker()
+        self.add_handler(self._waker.fileno(),
+                         lambda fd, events: self._waker.consume(),
+                         self.READ)
+    def close(self, all_fds=False):
+        self._closing = True
+        self.remove_handler(self._waker.fileno())
+        if all_fds:
+            for fd, handler in list(self._handlers.values()):
+                self.close_fd(fd)
+        self._waker.close()
+        self._impl.close()
+        self._callbacks = None
+        self._timeouts = None
+    def add_handler(self, fd, handler, events):
+        fd, obj = self.split_fd(fd)
+        self._handlers[fd] = (obj, stack_context.wrap(handler))
+        self._impl.register(fd, events | self.ERROR)
+    def update_handler(self, fd, events):
+        fd, obj = self.split_fd(fd)
+        self._impl.modify(fd, events | self.ERROR)
+    def remove_handler(self, fd):
+        fd, obj = self.split_fd(fd)
+        self._handlers.pop(fd, None)
+        self._events.pop(fd, None)
+        try:
+            self._impl.unregister(fd)
+        except Exception:
+            gen_log.debug("Error deleting fd from IOLoop", exc_info=True)
+    def set_blocking_signal_threshold(self, seconds, action):
+        if not hasattr(signal, "setitimer"):
+            gen_log.error("set_blocking_signal_threshold requires a signal module "
+                          "with the setitimer method")
+            return
+        self._blocking_signal_threshold = seconds
+        if seconds is not None:
+            signal.signal(signal.SIGALRM,
+                          action if action is not None else signal.SIG_DFL)
+    def start(self):
+        if self._running:
+            raise RuntimeError("IOLoop is already running")
+        self._setup_logging()
+        if self._stopped:
+            self._stopped = False
+            return
+        old_current = getattr(IOLoop._current, "instance", None)
+        IOLoop._current.instance = self
+        self._thread_ident = thread.get_ident()
+        self._running = True
+        old_wakeup_fd = None
+        if hasattr(signal, 'set_wakeup_fd') and os.name == 'posix':
+            try:
+                old_wakeup_fd = signal.set_wakeup_fd(self._waker.write_fileno())
+                if old_wakeup_fd != -1:
+                    signal.set_wakeup_fd(old_wakeup_fd)
+                    old_wakeup_fd = None
+            except ValueError:
+                old_wakeup_fd = None
+        try:
+            while True:
+                ncallbacks = len(self._callbacks)
+                due_timeouts = []
+                if self._timeouts:
+                    now = self.time()
+                    while self._timeouts:
+                        if self._timeouts[0].callback is None:
+                            heapq.heappop(self._timeouts)
+                            self._cancellations -= 1
+                        elif self._timeouts[0].deadline <= now:
+                            due_timeouts.append(heapq.heappop(self._timeouts))
+                        else:
+                            break
+                    if (self._cancellations > 512 and
+                            self._cancellations > (len(self._timeouts) >> 1)):
+                        self._cancellations = 0
+                        self._timeouts = [x for x in self._timeouts
+                                          if x.callback is not None]
+                        heapq.heapify(self._timeouts)
+                for i in range(ncallbacks):
+                    self._run_callback(self._callbacks.popleft())
+                for timeout in due_timeouts:
+                    if timeout.callback is not None:
+                        self._run_callback(timeout.callback)
+                due_timeouts = timeout = None
+                if self._callbacks:
+                    poll_timeout = 0.0
+                elif self._timeouts:
+                    poll_timeout = self._timeouts[0].deadline - self.time()
+                    poll_timeout = max(0, min(poll_timeout, _POLL_TIMEOUT))
+                else:
+                    poll_timeout = _POLL_TIMEOUT
+                if not self._running:
+                    break
+                if self._blocking_signal_threshold is not None:
+                    signal.setitimer(signal.ITIMER_REAL, 0, 0)
+                try:
+                    event_pairs = self._impl.poll(poll_timeout)
+                except Exception as e:
+                    if errno_from_exception(e) == errno.EINTR:
+                        continue
+                    else:
+                        raise
+                if self._blocking_signal_threshold is not None:
+                    signal.setitimer(signal.ITIMER_REAL,
+                                     self._blocking_signal_threshold, 0)
+                self._events.update(event_pairs)
+                while self._events:
+                    fd, events = self._events.popitem()
+                    try:
+                        fd_obj, handler_func = self._handlers[fd]
+                        handler_func(fd_obj, events)
+                    except (OSError, IOError) as e:
+                        if errno_from_exception(e) == errno.EPIPE:
+                            pass
+                        else:
+                            self.handle_callback_exception(self._handlers.get(fd))
+                    except Exception:
+                        self.handle_callback_exception(self._handlers.get(fd))
+                fd_obj = handler_func = None
+        finally:
+            self._stopped = False
+            if self._blocking_signal_threshold is not None:
+                signal.setitimer(signal.ITIMER_REAL, 0, 0)
+            IOLoop._current.instance = old_current
+            if old_wakeup_fd is not None:
+                signal.set_wakeup_fd(old_wakeup_fd)
+    def stop(self):
+        self._running = False
+        self._stopped = True
+        self._waker.wake()
+    def time(self):
+        return self.time_func()
+    def call_at(self, deadline, callback, *args, **kwargs):
+        timeout = _Timeout(
+            deadline,
+            functools.partial(stack_context.wrap(callback), *args, **kwargs),
+            self)
+        heapq.heappush(self._timeouts, timeout)
+        return timeout
+    def remove_timeout(self, timeout):
+        timeout.callback = None
+        self._cancellations += 1
+    def add_callback(self, callback, *args, **kwargs):
+        if self._closing:
+            return
+        self._callbacks.append(functools.partial(
+            stack_context.wrap(callback), *args, **kwargs))
+        if thread.get_ident() != self._thread_ident:
+            self._waker.wake()
+        else:
+            pass
+    def add_callback_from_signal(self, callback, *args, **kwargs):
+        with stack_context.NullContext():
+            self.add_callback(callback, *args, **kwargs)
+class _Timeout(object):
+    """An IOLoop timeout, a UNIX timestamp and a callback"""
+    __slots__ = ['deadline', 'callback', 'tdeadline']
+    def __init__(self, deadline, callback, io_loop):
+        if not isinstance(deadline, numbers.Real):
+            raise TypeError("Unsupported deadline %r" % deadline)
+        self.deadline = deadline
+        self.callback = callback
+        self.tdeadline = (deadline, next(io_loop._timeout_counter))
+    def __lt__(self, other):
+        return self.tdeadline < other.tdeadline
+    def __le__(self, other):
+        return self.tdeadline <= other.tdeadline
+class PeriodicCallback(object):
+    """Schedules the given callback to be called periodically.
+    The callback is called every ``callback_time`` milliseconds.
+    Note that the timeout is given in milliseconds, while most other
+    time-related functions in Tornado use seconds.
+    If the callback runs for longer than ``callback_time`` milliseconds,
+    subsequent invocations will be skipped to get back on schedule.
+    `start` must be called after the `PeriodicCallback` is created.
+    .. versionchanged:: 4.1
+       The ``io_loop`` argument is deprecated.
+    """
+    def __init__(self, callback, callback_time, io_loop=None):
+        self.callback = callback
+        if callback_time <= 0:
+            raise ValueError("Periodic callback must have a positive callback_time")
+        self.callback_time = callback_time
+        self.io_loop = io_loop or IOLoop.current()
+        self._running = False
+        self._timeout = None
+    def start(self):
+        """Starts the timer."""
+        self._running = True
+        self._next_timeout = self.io_loop.time()
+        self._schedule_next()
+    def stop(self):
+        """Stops the timer."""
+        self._running = False
+        if self._timeout is not None:
+            self.io_loop.remove_timeout(self._timeout)
+            self._timeout = None
+    def is_running(self):
+        """Return True if this `.PeriodicCallback` has been started.
+        .. versionadded:: 4.1
+        """
+        return self._running
+    def _run(self):
+        if not self._running:
+            return
+        try:
+            return self.callback()
+        except Exception:
+            self.io_loop.handle_callback_exception(self.callback)
+        finally:
+            self._schedule_next()
+    def _schedule_next(self):
+        if self._running:
+            current_time = self.io_loop.time()
+            if self._next_timeout <= current_time:
+                callback_time_sec = self.callback_time / 1000.0
+                self._next_timeout += (math.floor((current_time - self._next_timeout) /
+                                                  callback_time_sec) + 1) * callback_time_sec
+            self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)

--- a//dev/null
+++ b/salt/ext/tornado/iostream.py
@@ -0,0 +1,1156 @@
+"""Utility classes to write to and read from non-blocking files and sockets.
+Contents:
+* `BaseIOStream`: Generic interface for reading and writing.
+* `IOStream`: Implementation of BaseIOStream using non-blocking sockets.
+* `SSLIOStream`: SSL-aware version of IOStream.
+* `PipeIOStream`: Pipe-based IOStream implementation.
+"""
+from __future__ import absolute_import, division, print_function
+import collections
+import errno
+import numbers
+import os
+import socket
+import sys
+import re
+from salt.ext.tornado.concurrent import TracebackFuture
+from salt.ext.tornado import ioloop
+from salt.ext.tornado.log import gen_log, app_log
+from salt.ext.tornado.netutil import ssl_wrap_socket, ssl_match_hostname, SSLCertificateError, _client_ssl_defaults, _server_ssl_defaults
+from salt.ext.tornado import stack_context
+from salt.ext.tornado.util import errno_from_exception
+try:
+    from salt.ext.tornado.platform.posix import _set_nonblocking
+except ImportError:
+    _set_nonblocking = None
+try:
+    import ssl
+except ImportError:
+    ssl = None
+_ERRNO_WOULDBLOCK = (errno.EWOULDBLOCK, errno.EAGAIN)
+if hasattr(errno, "WSAEWOULDBLOCK"):
+    _ERRNO_WOULDBLOCK += (errno.WSAEWOULDBLOCK,)  # type: ignore
+_ERRNO_CONNRESET = (errno.ECONNRESET, errno.ECONNABORTED, errno.EPIPE,
+                    errno.ETIMEDOUT)
+if hasattr(errno, "WSAECONNRESET"):
+    _ERRNO_CONNRESET += (errno.WSAECONNRESET, errno.WSAECONNABORTED, errno.WSAETIMEDOUT)  # type: ignore
+if sys.platform == 'darwin':
+    _ERRNO_CONNRESET += (errno.EPROTOTYPE,)  # type: ignore
+_ERRNO_INPROGRESS = (errno.EINPROGRESS,)
+if hasattr(errno, "WSAEINPROGRESS"):
+    _ERRNO_INPROGRESS += (errno.WSAEINPROGRESS,)  # type: ignore
+_WINDOWS = sys.platform.startswith('win')
+class StreamClosedError(IOError):
+    """Exception raised by `IOStream` methods when the stream is closed.
+    Note that the close callback is scheduled to run *after* other
+    callbacks on the stream (to allow for buffered data to be processed),
+    so you may see this error before you see the close callback.
+    The ``real_error`` attribute contains the underlying error that caused
+    the stream to close (if any).
+    .. versionchanged:: 4.3
+       Added the ``real_error`` attribute.
+    """
+    def __init__(self, real_error=None):
+        super(StreamClosedError, self).__init__('Stream is closed')
+        self.real_error = real_error
+class UnsatisfiableReadError(Exception):
+    """Exception raised when a read cannot be satisfied.
+    Raised by ``read_until`` and ``read_until_regex`` with a ``max_bytes``
+    argument.
+    """
+    pass
+class StreamBufferFullError(Exception):
+    """Exception raised by `IOStream` methods when the buffer is full.
+    """
+class BaseIOStream(object):
+    """A utility class to write to and read from a non-blocking file or socket.
+    We support a non-blocking ``write()`` and a family of ``read_*()`` methods.
+    All of the methods take an optional ``callback`` argument and return a
+    `.Future` only if no callback is given.  When the operation completes,
+    the callback will be run or the `.Future` will resolve with the data
+    read (or ``None`` for ``write()``).  All outstanding ``Futures`` will
+    resolve with a `StreamClosedError` when the stream is closed; users
+    of the callback interface will be notified via
+    `.BaseIOStream.set_close_callback` instead.
+    When a stream is closed due to an error, the IOStream's ``error``
+    attribute contains the exception object.
+    Subclasses must implement `fileno`, `close_fd`, `write_to_fd`,
+    `read_from_fd`, and optionally `get_fd_error`.
+    """
+    def __init__(self, io_loop=None, max_buffer_size=None,
+                 read_chunk_size=None, max_write_buffer_size=None):
+        """`BaseIOStream` constructor.
+        :arg io_loop: The `.IOLoop` to use; defaults to `.IOLoop.current`.
+                      Deprecated since Tornado 4.1.
+        :arg max_buffer_size: Maximum amount of incoming data to buffer;
+            defaults to 100MB.
+        :arg read_chunk_size: Amount of data to read at one time from the
+            underlying transport; defaults to 64KB.
+        :arg max_write_buffer_size: Amount of outgoing data to buffer;
+            defaults to unlimited.
+        .. versionchanged:: 4.0
+           Add the ``max_write_buffer_size`` parameter.  Changed default
+           ``read_chunk_size`` to 64KB.
+        """
+        self.io_loop = io_loop or ioloop.IOLoop.current()
+        self.max_buffer_size = max_buffer_size or 104857600
+        self.read_chunk_size = min(read_chunk_size or 65536,
+                                   self.max_buffer_size // 2)
+        self.max_write_buffer_size = max_write_buffer_size
+        self.error = None
+        self._read_buffer = bytearray()
+        self._read_buffer_pos = 0
+        self._read_buffer_size = 0
+        self._write_buffer = bytearray()
+        self._write_buffer_pos = 0
+        self._write_buffer_size = 0
+        self._write_buffer_frozen = False
+        self._total_write_index = 0
+        self._total_write_done_index = 0
+        self._pending_writes_while_frozen = []
+        self._read_delimiter = None
+        self._read_regex = None
+        self._read_max_bytes = None
+        self._read_bytes = None
+        self._read_partial = False
+        self._read_until_close = False
+        self._read_callback = None
+        self._read_future = None
+        self._streaming_callback = None
+        self._write_callback = None
+        self._write_futures = collections.deque()
+        self._close_callback = None
+        self._connect_callback = None
+        self._connect_future = None
+        self._ssl_connect_future = None
+        self._connecting = False
+        self._state = None
+        self._pending_callbacks = 0
+        self._closed = False
+    def fileno(self):
+        """Returns the file descriptor for this stream."""
+        raise NotImplementedError()
+    def close_fd(self):
+        """Closes the file underlying this stream.
+        ``close_fd`` is called by `BaseIOStream` and should not be called
+        elsewhere; other users should call `close` instead.
+        """
+        raise NotImplementedError()
+    def write_to_fd(self, data):
+        """Attempts to write ``data`` to the underlying file.
+        Returns the number of bytes written.
+        """
+        raise NotImplementedError()
+    def read_from_fd(self):
+        """Attempts to read from the underlying file.
+        Returns ``None`` if there was nothing to read (the socket
+        returned `~errno.EWOULDBLOCK` or equivalent), otherwise
+        returns the data.  When possible, should return no more than
+        ``self.read_chunk_size`` bytes at a time.
+        """
+        raise NotImplementedError()
+    def get_fd_error(self):
+        """Returns information about any error on the underlying file.
+        This method is called after the `.IOLoop` has signaled an error on the
+        file descriptor, and should return an Exception (such as `socket.error`
+        with additional information, or None if no such information is
+        available.
+        """
+        return None
+    def read_until_regex(self, regex, callback=None, max_bytes=None):
+        """Asynchronously read until we have matched the given regex.
+        The result includes the data that matches the regex and anything
+        that came before it.  If a callback is given, it will be run
+        with the data as an argument; if not, this method returns a
+        `.Future`.
+        If ``max_bytes`` is not None, the connection will be closed
+        if more than ``max_bytes`` bytes have been read and the regex is
+        not satisfied.
+        .. versionchanged:: 4.0
+            Added the ``max_bytes`` argument.  The ``callback`` argument is
+            now optional and a `.Future` will be returned if it is omitted.
+        """
+        future = self._set_read_callback(callback)
+        self._read_regex = re.compile(regex)
+        self._read_max_bytes = max_bytes
+        try:
+            self._try_inline_read()
+        except UnsatisfiableReadError as e:
+            gen_log.info("Unsatisfiable read, closing connection: %s" % e)
+            self.close(exc_info=True)
+            return future
+        except:
+            if future is not None:
+                future.add_done_callback(lambda f: f.exception())
+            raise
+        return future
+    def read_until(self, delimiter, callback=None, max_bytes=None):
+        """Asynchronously read until we have found the given delimiter.
+        The result includes all the data read including the delimiter.
+        If a callback is given, it will be run with the data as an argument;
+        if not, this method returns a `.Future`.
+        If ``max_bytes`` is not None, the connection will be closed
+        if more than ``max_bytes`` bytes have been read and the delimiter
+        is not found.
+        .. versionchanged:: 4.0
+            Added the ``max_bytes`` argument.  The ``callback`` argument is
+            now optional and a `.Future` will be returned if it is omitted.
+        """
+        future = self._set_read_callback(callback)
+        self._read_delimiter = delimiter
+        self._read_max_bytes = max_bytes
+        try:
+            self._try_inline_read()
+        except UnsatisfiableReadError as e:
+            gen_log.info("Unsatisfiable read, closing connection: %s" % e)
+            self.close(exc_info=True)
+            return future
+        except:
+            if future is not None:
+                future.add_done_callback(lambda f: f.exception())
+            raise
+        return future
+    def read_bytes(self, num_bytes, callback=None, streaming_callback=None,
+                   partial=False):
+        """Asynchronously read a number of bytes.
+        If a ``streaming_callback`` is given, it will be called with chunks
+        of data as they become available, and the final result will be empty.
+        Otherwise, the result is all the data that was read.
+        If a callback is given, it will be run with the data as an argument;
+        if not, this method returns a `.Future`.
+        If ``partial`` is true, the callback is run as soon as we have
+        any bytes to return (but never more than ``num_bytes``)
+        .. versionchanged:: 4.0
+            Added the ``partial`` argument.  The callback argument is now
+            optional and a `.Future` will be returned if it is omitted.
+        """
+        future = self._set_read_callback(callback)
+        assert isinstance(num_bytes, numbers.Integral)
+        self._read_bytes = num_bytes
+        self._read_partial = partial
+        self._streaming_callback = stack_context.wrap(streaming_callback)
+        try:
+            self._try_inline_read()
+        except:
+            if future is not None:
+                future.add_done_callback(lambda f: f.exception())
+            raise
+        return future
+    def read_until_close(self, callback=None, streaming_callback=None):
+        """Asynchronously reads all data from the socket until it is closed.
+        If a ``streaming_callback`` is given, it will be called with chunks
+        of data as they become available, and the final result will be empty.
+        Otherwise, the result is all the data that was read.
+        If a callback is given, it will be run with the data as an argument;
+        if not, this method returns a `.Future`.
+        Note that if a ``streaming_callback`` is used, data will be
+        read from the socket as quickly as it becomes available; there
+        is no way to apply backpressure or cancel the reads. If flow
+        control or cancellation are desired, use a loop with
+        `read_bytes(partial=True) <.read_bytes>` instead.
+        .. versionchanged:: 4.0
+            The callback argument is now optional and a `.Future` will
+            be returned if it is omitted.
+        """
+        future = self._set_read_callback(callback)
+        self._streaming_callback = stack_context.wrap(streaming_callback)
+        if self.closed():
+            if self._streaming_callback is not None:
+                self._run_read_callback(self._read_buffer_size, True)
+            self._run_read_callback(self._read_buffer_size, False)
+            return future
+        self._read_until_close = True
+        try:
+            self._try_inline_read()
+        except:
+            if future is not None:
+                future.add_done_callback(lambda f: f.exception())
+            raise
+        return future
+    def write(self, data, callback=None):
+        """Asynchronously write the given data to this stream.
+        If ``callback`` is given, we call it when all of the buffered write
+        data has been successfully written to the stream. If there was
+        previously buffered write data and an old write callback, that
+        callback is simply overwritten with this new callback.
+        If no ``callback`` is given, this method returns a `.Future` that
+        resolves (with a result of ``None``) when the write has been
+        completed.
+        The ``data`` argument may be of type `bytes` or `memoryview`.
+        .. versionchanged:: 4.0
+            Now returns a `.Future` if no callback is given.
+        .. versionchanged:: 4.5
+            Added support for `memoryview` arguments.
+        """
+        self._check_closed()
+        if data:
+            if (self.max_write_buffer_size is not None and
+                    self._write_buffer_size + len(data) > self.max_write_buffer_size):
+                raise StreamBufferFullError("Reached maximum write buffer size")
+            if self._write_buffer_frozen:
+                self._pending_writes_while_frozen.append(data)
+            else:
+                self._write_buffer += data
+                self._write_buffer_size += len(data)
+            self._total_write_index += len(data)
+        if callback is not None:
+            self._write_callback = stack_context.wrap(callback)
+            future = None
+        else:
+            future = TracebackFuture()
+            future.add_done_callback(lambda f: f.exception())
+            self._write_futures.append((self._total_write_index, future))
+        if not self._connecting:
+            self._handle_write()
+            if self._write_buffer_size:
+                self._add_io_state(self.io_loop.WRITE)
+            self._maybe_add_error_listener()
+        return future
+    def set_close_callback(self, callback):
+        """Call the given callback when the stream is closed.
+        This is not necessary for applications that use the `.Future`
+        interface; all outstanding ``Futures`` will resolve with a
+        `StreamClosedError` when the stream is closed.
+        """
+        self._close_callback = stack_context.wrap(callback)
+        self._maybe_add_error_listener()
+    def close(self, exc_info=False):
+        """Close this stream.
+        If ``exc_info`` is true, set the ``error`` attribute to the current
+        exception from `sys.exc_info` (or if ``exc_info`` is a tuple,
+        use that instead of `sys.exc_info`).
+        """
+        if not self.closed():
+            if exc_info:
+                if not isinstance(exc_info, tuple):
+                    exc_info = sys.exc_info()
+                if any(exc_info):
+                    self.error = exc_info[1]
+            if self._read_until_close:
+                if (self._streaming_callback is not None and
+                        self._read_buffer_size):
+                    self._run_read_callback(self._read_buffer_size, True)
+                self._read_until_close = False
+                self._run_read_callback(self._read_buffer_size, False)
+            if self._state is not None:
+                self.io_loop.remove_handler(self.fileno())
+                self._state = None
+            self.close_fd()
+            self._closed = True
+        self._maybe_run_close_callback()
+    def _maybe_run_close_callback(self):
+        if self.closed() and self._pending_callbacks == 0:
+            futures = []
+            if self._read_future is not None:
+                futures.append(self._read_future)
+                self._read_future = None
+            futures += [future for _, future in self._write_futures]
+            self._write_futures.clear()
+            if self._connect_future is not None:
+                futures.append(self._connect_future)
+                self._connect_future = None
+            if self._ssl_connect_future is not None:
+                futures.append(self._ssl_connect_future)
+                self._ssl_connect_future = None
+            for future in futures:
+                future.set_exception(StreamClosedError(real_error=self.error))
+            if self._close_callback is not None:
+                cb = self._close_callback
+                self._close_callback = None
+                self._run_callback(cb)
+            self._read_callback = self._write_callback = None
+            self._write_buffer = None
+            self._write_buffer_size = 0
+    def reading(self):
+        """Returns true if we are currently reading from the stream."""
+        return self._read_callback is not None or self._read_future is not None
+    def writing(self):
+        """Returns true if we are currently writing to the stream."""
+        return self._write_buffer_size > 0
+    def closed(self):
+        """Returns true if the stream has been closed."""
+        return self._closed
+    def set_nodelay(self, value):
+        """Sets the no-delay flag for this stream.
+        By default, data written to TCP streams may be held for a time
+        to make the most efficient use of bandwidth (according to
+        Nagle's algorithm).  The no-delay flag requests that data be
+        written as soon as possible, even if doing so would consume
+        additional bandwidth.
+        This flag is currently defined only for TCP-based ``IOStreams``.
+        .. versionadded:: 3.1
+        """
+        pass
+    def _handle_events(self, fd, events):
+        if self.closed():
+            gen_log.warning("Got events for closed stream %s", fd)
+            return
+        try:
+            if self._connecting:
+                self._handle_connect()
+            if self.closed():
+                return
+            if events & self.io_loop.READ:
+                self._handle_read()
+            if self.closed():
+                return
+            if events & self.io_loop.WRITE:
+                self._handle_write()
+            if self.closed():
+                return
+            if events & self.io_loop.ERROR:
+                self.error = self.get_fd_error()
+                self.io_loop.add_callback(self.close)
+                return
+            state = self.io_loop.ERROR
+            if self.reading():
+                state |= self.io_loop.READ
+            if self.writing():
+                state |= self.io_loop.WRITE
+            if state == self.io_loop.ERROR and self._read_buffer_size == 0:
+                state |= self.io_loop.READ
+            if state != self._state:
+                assert self._state is not None, \
+                    "shouldn't happen: _handle_events without self._state"
+                self._state = state
+                self.io_loop.update_handler(self.fileno(), self._state)
+        except UnsatisfiableReadError as e:
+            gen_log.info("Unsatisfiable read, closing connection: %s" % e)
+            self.close(exc_info=True)
+        except Exception:
+            gen_log.error("Uncaught exception, closing connection.",
+                          exc_info=True)
+            self.close(exc_info=True)
+            raise
+    def _run_callback(self, callback, *args):
+        def wrapper():
+            self._pending_callbacks -= 1
+            try:
+                return callback(*args)
+            except Exception:
+                app_log.error("Uncaught exception, closing connection.",
+                              exc_info=True)
+                self.close(exc_info=True)
+                raise
+            finally:
+                self._maybe_add_error_listener()
+        with stack_context.NullContext():
+            self._pending_callbacks += 1
+            self.io_loop.add_callback(wrapper)
+    def _read_to_buffer_loop(self):
+        try:
+            if self._read_bytes is not None:
+                target_bytes = self._read_bytes
+            elif self._read_max_bytes is not None:
+                target_bytes = self._read_max_bytes
+            elif self.reading():
+                target_bytes = None
+            else:
+                target_bytes = 0
+            next_find_pos = 0
+            self._pending_callbacks += 1
+            while not self.closed():
+                if self._read_to_buffer() == 0:
+                    break
+                self._run_streaming_callback()
+                if (target_bytes is not None and
+                        self._read_buffer_size >= target_bytes):
+                    break
+                if self._read_buffer_size >= next_find_pos:
+                    pos = self._find_read_pos()
+                    if pos is not None:
+                        return pos
+                    next_find_pos = self._read_buffer_size * 2
+            return self._find_read_pos()
+        finally:
+            self._pending_callbacks -= 1
+    def _handle_read(self):
+        try:
+            pos = self._read_to_buffer_loop()
+        except UnsatisfiableReadError:
+            raise
+        except Exception as e:
+            gen_log.warning("error on read: %s" % e)
+            self.close(exc_info=True)
+            return
+        if pos is not None:
+            self._read_from_buffer(pos)
+            return
+        else:
+            self._maybe_run_close_callback()
+    def _set_read_callback(self, callback):
+        assert self._read_callback is None, "Already reading"
+        assert self._read_future is None, "Already reading"
+        if callback is not None:
+            self._read_callback = stack_context.wrap(callback)
+        else:
+            self._read_future = TracebackFuture()
+        return self._read_future
+    def _run_read_callback(self, size, streaming):
+        if streaming:
+            callback = self._streaming_callback
+        else:
+            callback = self._read_callback
+            self._read_callback = self._streaming_callback = None
+            if self._read_future is not None:
+                assert callback is None
+                future = self._read_future
+                self._read_future = None
+                future.set_result(self._consume(size))
+        if callback is not None:
+            assert (self._read_future is None) or streaming
+            self._run_callback(callback, self._consume(size))
+        else:
+            self._maybe_add_error_listener()
+    def _try_inline_read(self):
+        """Attempt to complete the current read operation from buffered data.
+        If the read can be completed without blocking, schedules the
+        read callback on the next IOLoop iteration; otherwise starts
+        listening for reads on the socket.
+        """
+        self._run_streaming_callback()
+        pos = self._find_read_pos()
+        if pos is not None:
+            self._read_from_buffer(pos)
+            return
+        self._check_closed()
+        try:
+            pos = self._read_to_buffer_loop()
+        except Exception:
+            self._maybe_run_close_callback()
+            raise
+        if pos is not None:
+            self._read_from_buffer(pos)
+            return
+        if self.closed():
+            self._maybe_run_close_callback()
+        else:
+            self._add_io_state(ioloop.IOLoop.READ)
+    def _read_to_buffer(self):
+        """Reads from the socket and appends the result to the read buffer.
+        Returns the number of bytes read.  Returns 0 if there is nothing
+        to read (i.e. the read returns EWOULDBLOCK or equivalent).  On
+        error closes the socket and raises an exception.
+        """
+        while True:
+            try:
+                chunk = self.read_from_fd()
+            except (socket.error, IOError, OSError) as e:
+                if errno_from_exception(e) == errno.EINTR:
+                    continue
+                if self._is_connreset(e):
+                    self.close(exc_info=True)
+                    return
+                self.close(exc_info=True)
+                raise
+            break
+        if chunk is None:
+            return 0
+        self._read_buffer += chunk
+        self._read_buffer_size += len(chunk)
+        if self._read_buffer_size > self.max_buffer_size:
+            gen_log.error("Reached maximum read buffer size")
+            self.close()
+            raise StreamBufferFullError("Reached maximum read buffer size")
+        return len(chunk)
+    def _run_streaming_callback(self):
+        if self._streaming_callback is not None and self._read_buffer_size:
+            bytes_to_consume = self._read_buffer_size
+            if self._read_bytes is not None:
+                bytes_to_consume = min(self._read_bytes, bytes_to_consume)
+                self._read_bytes -= bytes_to_consume
+            self._run_read_callback(bytes_to_consume, True)
+    def _read_from_buffer(self, pos):
+        """Attempts to complete the currently-pending read from the buffer.
+        The argument is either a position in the read buffer or None,
+        as returned by _find_read_pos.
+        """
+        self._read_bytes = self._read_delimiter = self._read_regex = None
+        self._read_partial = False
+        self._run_read_callback(pos, False)
+    def _find_read_pos(self):
+        """Attempts to find a position in the read buffer that satisfies
+        the currently-pending read.
+        Returns a position in the buffer if the current read can be satisfied,
+        or None if it cannot.
+        """
+        if (self._read_bytes is not None and
+            (self._read_buffer_size >= self._read_bytes or
+             (self._read_partial and self._read_buffer_size > 0))):
+            num_bytes = min(self._read_bytes, self._read_buffer_size)
+            return num_bytes
+        elif self._read_delimiter is not None:
+            if self._read_buffer:
+                loc = self._read_buffer.find(self._read_delimiter,
+                                             self._read_buffer_pos)
+                if loc != -1:
+                    loc -= self._read_buffer_pos
+                    delimiter_len = len(self._read_delimiter)
+                    self._check_max_bytes(self._read_delimiter,
+                                          loc + delimiter_len)
+                    return loc + delimiter_len
+                self._check_max_bytes(self._read_delimiter,
+                                      self._read_buffer_size)
+        elif self._read_regex is not None:
+            if self._read_buffer:
+                m = self._read_regex.search(self._read_buffer,
+                                            self._read_buffer_pos)
+                if m is not None:
+                    loc = m.end() - self._read_buffer_pos
+                    self._check_max_bytes(self._read_regex, loc)
+                    return loc
+                self._check_max_bytes(self._read_regex, self._read_buffer_size)
+        return None
+    def _check_max_bytes(self, delimiter, size):
+        if (self._read_max_bytes is not None and
+                size > self._read_max_bytes):
+            raise UnsatisfiableReadError(
+                "delimiter %r not found within %d bytes" % (
+                    delimiter, self._read_max_bytes))
+    def _freeze_write_buffer(self, size):
+        self._write_buffer_frozen = size
+    def _unfreeze_write_buffer(self):
+        self._write_buffer_frozen = False
+        self._write_buffer += b''.join(self._pending_writes_while_frozen)
+        self._write_buffer_size += sum(map(len, self._pending_writes_while_frozen))
+        self._pending_writes_while_frozen[:] = []
+    def _got_empty_write(self, size):
+        """
+        Called when a non-blocking write() failed writing anything.
+        Can be overridden in subclasses.
+        """
+    def _handle_write(self):
+        while self._write_buffer_size:
+            assert self._write_buffer_size >= 0
+            try:
+                start = self._write_buffer_pos
+                if self._write_buffer_frozen:
+                    size = self._write_buffer_frozen
+                elif _WINDOWS:
+                    size = 128 * 1024
+                else:
+                    size = self._write_buffer_size
+                num_bytes = self.write_to_fd(
+                    memoryview(self._write_buffer)[start:start + size])
+                if num_bytes == 0:
+                    self._got_empty_write(size)
+                    break
+                self._write_buffer_pos += num_bytes
+                self._write_buffer_size -= num_bytes
+                if self._write_buffer_pos > self._write_buffer_size:
+                    del self._write_buffer[:self._write_buffer_pos]
+                    self._write_buffer_pos = 0
+                if self._write_buffer_frozen:
+                    self._unfreeze_write_buffer()
+                self._total_write_done_index += num_bytes
+            except (socket.error, IOError, OSError) as e:
+                if e.args[0] in _ERRNO_WOULDBLOCK:
+                    self._got_empty_write(size)
+                    break
+                else:
+                    if not self._is_connreset(e):
+                        gen_log.warning("Write error on %s: %s",
+                                        self.fileno(), e)
+                    self.close(exc_info=True)
+                    return
+        while self._write_futures:
+            index, future = self._write_futures[0]
+            if index > self._total_write_done_index:
+                break
+            self._write_futures.popleft()
+            future.set_result(None)
+        if not self._write_buffer_size:
+            if self._write_callback:
+                callback = self._write_callback
+                self._write_callback = None
+                self._run_callback(callback)
+    def _consume(self, loc):
+        if loc == 0:
+            return b""
+        assert loc <= self._read_buffer_size
+        b = (memoryview(self._read_buffer)
+             [self._read_buffer_pos:self._read_buffer_pos + loc]
+             ).tobytes()
+        self._read_buffer_pos += loc
+        self._read_buffer_size -= loc
+        if self._read_buffer_pos > self._read_buffer_size:
+            del self._read_buffer[:self._read_buffer_pos]
+            self._read_buffer_pos = 0
+        return b
+    def _check_closed(self):
+        if self.closed():
+            raise StreamClosedError(real_error=self.error)
+    def _maybe_add_error_listener(self):
+        if self._pending_callbacks != 0:
+            return
+        if self._state is None or self._state == ioloop.IOLoop.ERROR:
+            if self.closed():
+                self._maybe_run_close_callback()
+            elif (self._read_buffer_size == 0 and
+                  self._close_callback is not None):
+                self._add_io_state(ioloop.IOLoop.READ)
+    def _add_io_state(self, state):
+        """Adds `state` (IOLoop.{READ,WRITE} flags) to our event handler.
+        Implementation notes: Reads and writes have a fast path and a
+        slow path.  The fast path reads synchronously from socket
+        buffers, while the slow path uses `_add_io_state` to schedule
+        an IOLoop callback.  Note that in both cases, the callback is
+        run asynchronously with `_run_callback`.
+        To detect closed connections, we must have called
+        `_add_io_state` at some point, but we want to delay this as
+        much as possible so we don't have to set an `IOLoop.ERROR`
+        listener that will be overwritten by the next slow-path
+        operation.  As long as there are callbacks scheduled for
+        fast-path ops, those callbacks may do more reads.
+        If a sequence of fast-path ops do not end in a slow-path op,
+        (e.g. for an @asynchronous long-poll request), we must add
+        the error handler.  This is done in `_run_callback` and `write`
+        (since the write callback is optional so we can have a
+        fast-path write with no `_run_callback`)
+        """
+        if self.closed():
+            return
+        if self._state is None:
+            self._state = ioloop.IOLoop.ERROR | state
+            with stack_context.NullContext():
+                self.io_loop.add_handler(
+                    self.fileno(), self._handle_events, self._state)
+        elif not self._state & state:
+            self._state = self._state | state
+            self.io_loop.update_handler(self.fileno(), self._state)
+    def _is_connreset(self, exc):
+        """Return true if exc is ECONNRESET or equivalent.
+        May be overridden in subclasses.
+        """
+        return (isinstance(exc, (socket.error, IOError)) and
+                errno_from_exception(exc) in _ERRNO_CONNRESET)
+class IOStream(BaseIOStream):
+    r"""Socket-based `IOStream` implementation.
+    This class supports the read and write methods from `BaseIOStream`
+    plus a `connect` method.
+    The ``socket`` parameter may either be connected or unconnected.
+    For server operations the socket is the result of calling
+    `socket.accept <socket.socket.accept>`.  For client operations the
+    socket is created with `socket.socket`, and may either be
+    connected before passing it to the `IOStream` or connected with
+    `IOStream.connect`.
+    A very simple (and broken) HTTP client using this class:
+    .. testcode::
+        import tornado.ioloop
+        import tornado.iostream
+        import socket
+        def send_request():
+            stream.write(b"GET / HTTP/1.0\r\nHost: friendfeed.com\r\n\r\n")
+            stream.read_until(b"\r\n\r\n", on_headers)
+        def on_headers(data):
+            headers = {}
+            for line in data.split(b"\r\n"):
+               parts = line.split(b":")
+               if len(parts) == 2:
+                   headers[parts[0].strip()] = parts[1].strip()
+            stream.read_bytes(int(headers[b"Content-Length"]), on_body)
+        def on_body(data):
+            print(data)
+            stream.close()
+            tornado.ioloop.IOLoop.current().stop()
+        if __name__ == '__main__':
+            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)
+            stream = tornado.iostream.IOStream(s)
+            stream.connect(("friendfeed.com", 80), send_request)
+            tornado.ioloop.IOLoop.current().start()
+    .. testoutput::
+       :hide:
+    """
+    def __init__(self, socket, *args, **kwargs):
+        self.socket = socket
+        self.socket.setblocking(False)
+        super(IOStream, self).__init__(*args, **kwargs)
+    def fileno(self):
+        return self.socket
+    def close_fd(self):
+        self.socket.close()
+        self.socket = None
+    def get_fd_error(self):
+        errno = self.socket.getsockopt(socket.SOL_SOCKET,
+                                       socket.SO_ERROR)
+        return socket.error(errno, os.strerror(errno))
+    def read_from_fd(self):
+        try:
+            chunk = self.socket.recv(self.read_chunk_size)
+        except socket.error as e:
+            if e.args[0] in _ERRNO_WOULDBLOCK:
+                return None
+            else:
+                raise
+        if not chunk:
+            self.close()
+            return None
+        return chunk
+    def write_to_fd(self, data):
+        try:
+            return self.socket.send(data)
+        finally:
+            del data
+    def connect(self, address, callback=None, server_hostname=None):
+        """Connects the socket to a remote address without blocking.
+        May only be called if the socket passed to the constructor was
+        not previously connected.  The address parameter is in the
+        same format as for `socket.connect <socket.socket.connect>` for
+        the type of socket passed to the IOStream constructor,
+        e.g. an ``(ip, port)`` tuple.  Hostnames are accepted here,
+        but will be resolved synchronously and block the IOLoop.
+        If you have a hostname instead of an IP address, the `.TCPClient`
+        class is recommended instead of calling this method directly.
+        `.TCPClient` will do asynchronous DNS resolution and handle
+        both IPv4 and IPv6.
+        If ``callback`` is specified, it will be called with no
+        arguments when the connection is completed; if not this method
+        returns a `.Future` (whose result after a successful
+        connection will be the stream itself).
+        In SSL mode, the ``server_hostname`` parameter will be used
+        for certificate validation (unless disabled in the
+        ``ssl_options``) and SNI (if supported; requires Python
+        2.7.9+).
+        Note that it is safe to call `IOStream.write
+        <BaseIOStream.write>` while the connection is pending, in
+        which case the data will be written as soon as the connection
+        is ready.  Calling `IOStream` read methods before the socket is
+        connected works on some platforms but is non-portable.
+        .. versionchanged:: 4.0
+            If no callback is given, returns a `.Future`.
+        .. versionchanged:: 4.2
+           SSL certificates are validated by default; pass
+           ``ssl_options=dict(cert_reqs=ssl.CERT_NONE)`` or a
+           suitably-configured `ssl.SSLContext` to the
+           `SSLIOStream` constructor to disable.
+        """
+        self._connecting = True
+        if callback is not None:
+            self._connect_callback = stack_context.wrap(callback)
+            future = None
+        else:
+            future = self._connect_future = TracebackFuture()
+        try:
+            self.socket.connect(address)
+        except socket.error as e:
+            if (errno_from_exception(e) not in _ERRNO_INPROGRESS and
+                    errno_from_exception(e) not in _ERRNO_WOULDBLOCK):
+                if future is None:
+                    gen_log.warning("Connect error on fd %s: %s",
+                                    self.socket.fileno(), e)
+                self.close(exc_info=True)
+                return future
+        self._add_io_state(self.io_loop.WRITE)
+        return future
+    def start_tls(self, server_side, ssl_options=None, server_hostname=None):
+        """Convert this `IOStream` to an `SSLIOStream`.
+        This enables protocols that begin in clear-text mode and
+        switch to SSL after some initial negotiation (such as the
+        ``STARTTLS`` extension to SMTP and IMAP).
+        This method cannot be used if there are outstanding reads
+        or writes on the stream, or if there is any data in the
+        IOStream's buffer (data in the operating system's socket
+        buffer is allowed).  This means it must generally be used
+        immediately after reading or writing the last clear-text
+        data.  It can also be used immediately after connecting,
+        before any reads or writes.
+        The ``ssl_options`` argument may be either an `ssl.SSLContext`
+        object or a dictionary of keyword arguments for the
+        `ssl.wrap_socket` function.  The ``server_hostname`` argument
+        will be used for certificate validation unless disabled
+        in the ``ssl_options``.
+        This method returns a `.Future` whose result is the new
+        `SSLIOStream`.  After this method has been called,
+        any other operation on the original stream is undefined.
+        If a close callback is defined on this stream, it will be
+        transferred to the new stream.
+        .. versionadded:: 4.0
+        .. versionchanged:: 4.2
+           SSL certificates are validated by default; pass
+           ``ssl_options=dict(cert_reqs=ssl.CERT_NONE)`` or a
+           suitably-configured `ssl.SSLContext` to disable.
+        """
+        if (self._read_callback or self._read_future or
+                self._write_callback or self._write_futures or
+                self._connect_callback or self._connect_future or
+                self._pending_callbacks or self._closed or
+                self._read_buffer or self._write_buffer):
+            raise ValueError("IOStream is not idle; cannot convert to SSL")
+        if ssl_options is None:
+            if server_side:
+                ssl_options = _server_ssl_defaults
+            else:
+                ssl_options = _client_ssl_defaults
+        socket = self.socket
+        self.io_loop.remove_handler(socket)
+        self.socket = None
+        socket = ssl_wrap_socket(socket, ssl_options,
+                                 server_hostname=server_hostname,
+                                 server_side=server_side,
+                                 do_handshake_on_connect=False)
+        orig_close_callback = self._close_callback
+        self._close_callback = None
+        future = TracebackFuture()
+        ssl_stream = SSLIOStream(socket, ssl_options=ssl_options,
+                                 io_loop=self.io_loop)
+        def close_callback():
+            if not future.done():
+                future.set_exception(ssl_stream.error or StreamClosedError())
+            if orig_close_callback is not None:
+                orig_close_callback()
+        ssl_stream.set_close_callback(close_callback)
+        ssl_stream._ssl_connect_callback = lambda: future.set_result(ssl_stream)
+        ssl_stream.max_buffer_size = self.max_buffer_size
+        ssl_stream.read_chunk_size = self.read_chunk_size
+        return future
+    def _handle_connect(self):
+        err = self.socket.getsockopt(socket.SOL_SOCKET, socket.SO_ERROR)
+        if err != 0:
+            self.error = socket.error(err, os.strerror(err))
+            if self._connect_future is None:
+                gen_log.warning("Connect error on fd %s: %s",
+                                self.socket.fileno(), errno.errorcode[err])
+            self.close()
+            return
+        if self._connect_callback is not None:
+            callback = self._connect_callback
+            self._connect_callback = None
+            self._run_callback(callback)
+        if self._connect_future is not None:
+            future = self._connect_future
+            self._connect_future = None
+            future.set_result(self)
+        self._connecting = False
+    def set_nodelay(self, value):
+        if (self.socket is not None and
+                self.socket.family in (socket.AF_INET, socket.AF_INET6)):
+            try:
+                self.socket.setsockopt(socket.IPPROTO_TCP,
+                                       socket.TCP_NODELAY, 1 if value else 0)
+            except socket.error as e:
+                if e.errno != errno.EINVAL and not self._is_connreset(e):
+                    raise
+class SSLIOStream(IOStream):
+    """A utility class to write to and read from a non-blocking SSL socket.
+    If the socket passed to the constructor is already connected,
+    it should be wrapped with::
+        ssl.wrap_socket(sock, do_handshake_on_connect=False, **kwargs)
+    before constructing the `SSLIOStream`.  Unconnected sockets will be
+    wrapped when `IOStream.connect` is finished.
+    """
+    def __init__(self, *args, **kwargs):
+        """The ``ssl_options`` keyword argument may either be an
+        `ssl.SSLContext` object or a dictionary of keywords arguments
+        for `ssl.wrap_socket`
+        """
+        self._ssl_options = kwargs.pop('ssl_options', _client_ssl_defaults)
+        super(SSLIOStream, self).__init__(*args, **kwargs)
+        self._ssl_accepting = True
+        self._handshake_reading = False
+        self._handshake_writing = False
+        self._ssl_connect_callback = None
+        self._server_hostname = None
+        try:
+            self.socket.getpeername()
+        except socket.error:
+            pass
+        else:
+            self._add_io_state(self.io_loop.WRITE)
+    def reading(self):
+        return self._handshake_reading or super(SSLIOStream, self).reading()
+    def writing(self):
+        return self._handshake_writing or super(SSLIOStream, self).writing()
+    def _got_empty_write(self, size):
+        self._freeze_write_buffer(size)
+    def _do_ssl_handshake(self):
+        try:
+            self._handshake_reading = False
+            self._handshake_writing = False
+            self.socket.do_handshake()
+        except ssl.SSLError as err:
+            if err.args[0] == ssl.SSL_ERROR_WANT_READ:
+                self._handshake_reading = True
+                return
+            elif err.args[0] == ssl.SSL_ERROR_WANT_WRITE:
+                self._handshake_writing = True
+                return
+            elif err.args[0] in (ssl.SSL_ERROR_EOF,
+                                 ssl.SSL_ERROR_ZERO_RETURN):
+                return self.close(exc_info=True)
+            elif err.args[0] == ssl.SSL_ERROR_SSL:
+                try:
+                    peer = self.socket.getpeername()
+                except Exception:
+                    peer = '(not connected)'
+                gen_log.warning("SSL Error on %s %s: %s",
+                                self.socket.fileno(), peer, err)
+                return self.close(exc_info=True)
+            raise
+        except socket.error as err:
+            if (self._is_connreset(err) or
+                    err.args[0] in (errno.EBADF, errno.ENOTCONN)):
+                return self.close(exc_info=True)
+            raise
+        except AttributeError:
+            return self.close(exc_info=True)
+        else:
+            self._ssl_accepting = False
+            if not self._verify_cert(self.socket.getpeercert()):
+                self.close()
+                return
+            self._run_ssl_connect_callback()
+    def _run_ssl_connect_callback(self):
+        if self._ssl_connect_callback is not None:
+            callback = self._ssl_connect_callback
+            self._ssl_connect_callback = None
+            self._run_callback(callback)
+        if self._ssl_connect_future is not None:
+            future = self._ssl_connect_future
+            self._ssl_connect_future = None
+            future.set_result(self)
+    def _verify_cert(self, peercert):
+        """Returns True if peercert is valid according to the configured
+        validation mode and hostname.
+        The ssl handshake already tested the certificate for a valid
+        CA signature; the only thing that remains is to check
+        the hostname.
+        """
+        if isinstance(self._ssl_options, dict):
+            verify_mode = self._ssl_options.get('cert_reqs', ssl.CERT_NONE)
+        elif isinstance(self._ssl_options, ssl.SSLContext):
+            verify_mode = self._ssl_options.verify_mode
+        assert verify_mode in (ssl.CERT_NONE, ssl.CERT_REQUIRED, ssl.CERT_OPTIONAL)
+        if verify_mode == ssl.CERT_NONE or self._server_hostname is None:
+            return True
+        cert = self.socket.getpeercert()
+        if cert is None and verify_mode == ssl.CERT_REQUIRED:
+            gen_log.warning("No SSL certificate given")
+            return False
+        try:
+            ssl_match_hostname(peercert, self._server_hostname)
+        except SSLCertificateError as e:
+            gen_log.warning("Invalid SSL certificate: %s" % e)
+            return False
+        else:
+            return True
+    def _handle_read(self):
+        if self._ssl_accepting:
+            self._do_ssl_handshake()
+            return
+        super(SSLIOStream, self)._handle_read()
+    def _handle_write(self):
+        if self._ssl_accepting:
+            self._do_ssl_handshake()
+            return
+        super(SSLIOStream, self)._handle_write()
+    def connect(self, address, callback=None, server_hostname=None):
+        self._server_hostname = server_hostname
+        super(SSLIOStream, self).connect(address, callback=lambda: None)
+        return self.wait_for_handshake(callback)
+    def _handle_connect(self):
+        super(SSLIOStream, self)._handle_connect()
+        if self.closed():
+            return
+        self.io_loop.remove_handler(self.socket)
+        old_state = self._state
+        self._state = None
+        self.socket = ssl_wrap_socket(self.socket, self._ssl_options,
+                                      server_hostname=self._server_hostname,
+                                      do_handshake_on_connect=False)
+        self._add_io_state(old_state)
+    def wait_for_handshake(self, callback=None):
+        """Wait for the initial SSL handshake to complete.
+        If a ``callback`` is given, it will be called with no
+        arguments once the handshake is complete; otherwise this
+        method returns a `.Future` which will resolve to the
+        stream itself after the handshake is complete.
+        Once the handshake is complete, information such as
+        the peer's certificate and NPN/ALPN selections may be
+        accessed on ``self.socket``.
+        This method is intended for use on server-side streams
+        or after using `IOStream.start_tls`; it should not be used
+        with `IOStream.connect` (which already waits for the
+        handshake to complete). It may only be called once per stream.
+        .. versionadded:: 4.2
+        """
+        if (self._ssl_connect_callback is not None or
+                self._ssl_connect_future is not None):
+            raise RuntimeError("Already waiting")
+        if callback is not None:
+            self._ssl_connect_callback = stack_context.wrap(callback)
+            future = None
+        else:
+            future = self._ssl_connect_future = TracebackFuture()
+        if not self._ssl_accepting:
+            self._run_ssl_connect_callback()
+        return future
+    def write_to_fd(self, data):
+        try:
+            return self.socket.send(data)
+        except ssl.SSLError as e:
+            if e.args[0] == ssl.SSL_ERROR_WANT_WRITE:
+                return 0
+            raise
+        finally:
+            del data
+    def read_from_fd(self):
+        if self._ssl_accepting:
+            return None
+        try:
+            chunk = self.socket.read(self.read_chunk_size)
+        except ssl.SSLError as e:
+            if e.args[0] == ssl.SSL_ERROR_WANT_READ:
+                return None
+            else:
+                raise
+        except socket.error as e:
+            if e.args[0] in _ERRNO_WOULDBLOCK:
+                return None
+            else:
+                raise
+        if not chunk:
+            self.close()
+            return None
+        return chunk
+    def _is_connreset(self, e):
+        if isinstance(e, ssl.SSLError) and e.args[0] == ssl.SSL_ERROR_EOF:
+            return True
+        return super(SSLIOStream, self)._is_connreset(e)
+class PipeIOStream(BaseIOStream):
+    """Pipe-based `IOStream` implementation.
+    The constructor takes an integer file descriptor (such as one returned
+    by `os.pipe`) rather than an open file object.  Pipes are generally
+    one-way, so a `PipeIOStream` can be used for reading or writing but not
+    both.
+    """
+    def __init__(self, fd, *args, **kwargs):
+        self.fd = fd
+        _set_nonblocking(fd)
+        super(PipeIOStream, self).__init__(*args, **kwargs)
+    def fileno(self):
+        return self.fd
+    def close_fd(self):
+        os.close(self.fd)
+    def write_to_fd(self, data):
+        try:
+            return os.write(self.fd, data)
+        finally:
+            del data
+    def read_from_fd(self):
+        try:
+            chunk = os.read(self.fd, self.read_chunk_size)
+        except (IOError, OSError) as e:
+            if errno_from_exception(e) in _ERRNO_WOULDBLOCK:
+                return None
+            elif errno_from_exception(e) == errno.EBADF:
+                self.close(exc_info=True)
+                return None
+            else:
+                raise
+        if not chunk:
+            self.close()
+            return None
+        return chunk
+def doctests():
+    import doctest
+    return doctest.DocTestSuite()

--- a//dev/null
+++ b/salt/ext/tornado/locale.py
@@ -0,0 +1,405 @@
+"""Translation methods for generating localized strings.
+To load a locale and generate a translated string::
+    user_locale = tornado.locale.get("es_LA")
+    print(user_locale.translate("Sign out"))
+`tornado.locale.get()` returns the closest matching locale, not necessarily the
+specific locale you requested. You can support pluralization with
+additional arguments to `~Locale.translate()`, e.g.::
+    people = [...]
+    message = user_locale.translate(
+        "%(list)s is online", "%(list)s are online", len(people))
+    print(message % {"list": user_locale.list(people)})
+The first string is chosen if ``len(people) == 1``, otherwise the second
+string is chosen.
+Applications should call one of `load_translations` (which uses a simple
+CSV format) or `load_gettext_translations` (which uses the ``.mo`` format
+supported by `gettext` and related tools).  If neither method is called,
+the `Locale.translate` method will simply return the original string.
+"""
+from __future__ import absolute_import, division, print_function
+import codecs
+import csv
+import datetime
+from io import BytesIO
+import numbers
+import os
+import re
+from salt.ext.tornado import escape
+from salt.ext.tornado.log import gen_log
+from salt.ext.tornado.util import PY3
+from salt.ext.tornado._locale_data import LOCALE_NAMES
+_default_locale = "en_US"
+_translations = {}  # type: dict
+_supported_locales = frozenset([_default_locale])
+_use_gettext = False
+CONTEXT_SEPARATOR = "\x04"
+def get(*locale_codes):
+    """Returns the closest match for the given locale codes.
+    We iterate over all given locale codes in order. If we have a tight
+    or a loose match for the code (e.g., "en" for "en_US"), we return
+    the locale. Otherwise we move to the next code in the list.
+    By default we return ``en_US`` if no translations are found for any of
+    the specified locales. You can change the default locale with
+    `set_default_locale()`.
+    """
+    return Locale.get_closest(*locale_codes)
+def set_default_locale(code):
+    """Sets the default locale.
+    The default locale is assumed to be the language used for all strings
+    in the system. The translations loaded from disk are mappings from
+    the default locale to the destination locale. Consequently, you don't
+    need to create a translation file for the default locale.
+    """
+    global _default_locale
+    global _supported_locales
+    _default_locale = code
+    _supported_locales = frozenset(list(_translations.keys()) + [_default_locale])
+def load_translations(directory, encoding=None):
+    """Loads translations from CSV files in a directory.
+    Translations are strings with optional Python-style named placeholders
+    (e.g., ``My name is %(name)s``) and their associated translations.
+    The directory should have translation files of the form ``LOCALE.csv``,
+    e.g. ``es_GT.csv``. The CSV files should have two or three columns: string,
+    translation, and an optional plural indicator. Plural indicators should
+    be one of "plural" or "singular". A given string can have both singular
+    and plural forms. For example ``%(name)s liked this`` may have a
+    different verb conjugation depending on whether %(name)s is one
+    name or a list of names. There should be two rows in the CSV file for
+    that string, one with plural indicator "singular", and one "plural".
+    For strings with no verbs that would change on translation, simply
+    use "unknown" or the empty string (or don't include the column at all).
+    The file is read using the `csv` module in the default "excel" dialect.
+    In this format there should not be spaces after the commas.
+    If no ``encoding`` parameter is given, the encoding will be
+    detected automatically (among UTF-8 and UTF-16) if the file
+    contains a byte-order marker (BOM), defaulting to UTF-8 if no BOM
+    is present.
+    Example translation ``es_LA.csv``::
+        "I love you","Te amo"
+        "%(name)s liked this","A %(name)s les gust esto","plural"
+        "%(name)s liked this","A %(name)s le gust esto","singular"
+    .. versionchanged:: 4.3
+       Added ``encoding`` parameter. Added support for BOM-based encoding
+       detection, UTF-16, and UTF-8-with-BOM.
+    """
+    global _translations
+    global _supported_locales
+    _translations = {}
+    for path in os.listdir(directory):
+        if not path.endswith(".csv"):
+            continue
+        locale, extension = path.split(".")
+        if not re.match("[a-z]+(_[A-Z]+)?$", locale):
+            gen_log.error("Unrecognized locale %r (path: %s)", locale,
+                          os.path.join(directory, path))
+            continue
+        full_path = os.path.join(directory, path)
+        if encoding is None:
+            with open(full_path, 'rb') as f:
+                data = f.read(len(codecs.BOM_UTF16_LE))
+            if data in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):
+                encoding = 'utf-16'
+            else:
+                encoding = 'utf-8-sig'
+        if PY3:
+            f = open(full_path, "r", encoding=encoding)
+        else:
+            f = BytesIO()
+            with codecs.open(full_path, "r", encoding=encoding) as infile:
+                f.write(escape.utf8(infile.read()))
+            f.seek(0)
+        _translations[locale] = {}
+        for i, row in enumerate(csv.reader(f)):
+            if not row or len(row) < 2:
+                continue
+            row = [escape.to_unicode(c).strip() for c in row]
+            english, translation = row[:2]
+            if len(row) > 2:
+                plural = row[2] or "unknown"
+            else:
+                plural = "unknown"
+            if plural not in ("plural", "singular", "unknown"):
+                gen_log.error("Unrecognized plural indicator %r in %s line %d",
+                              plural, path, i + 1)
+                continue
+            _translations[locale].setdefault(plural, {})[english] = translation
+        f.close()
+    _supported_locales = frozenset(list(_translations.keys()) + [_default_locale])
+    gen_log.debug("Supported locales: %s", sorted(_supported_locales))
+def load_gettext_translations(directory, domain):
+    """Loads translations from `gettext`'s locale tree
+    Locale tree is similar to system's ``/usr/share/locale``, like::
+        {directory}/{lang}/LC_MESSAGES/{domain}.mo
+    Three steps are required to have your app translated:
+    1. Generate POT translation file::
+        xgettext --language=Python --keyword=_:1,2 -d mydomain file1.py file2.html etc
+    2. Merge against existing POT file::
+        msgmerge old.po mydomain.po > new.po
+    3. Compile::
+        msgfmt mydomain.po -o {directory}/pt_BR/LC_MESSAGES/mydomain.mo
+    """
+    import gettext
+    global _translations
+    global _supported_locales
+    global _use_gettext
+    _translations = {}
+    for lang in os.listdir(directory):
+        if lang.startswith('.'):
+            continue  # skip .svn, etc
+        if os.path.isfile(os.path.join(directory, lang)):
+            continue
+        try:
+            os.stat(os.path.join(directory, lang, "LC_MESSAGES", domain + ".mo"))
+            _translations[lang] = gettext.translation(domain, directory,
+                                                      languages=[lang])
+        except Exception as e:
+            gen_log.error("Cannot load translation for '%s': %s", lang, str(e))
+            continue
+    _supported_locales = frozenset(list(_translations.keys()) + [_default_locale])
+    _use_gettext = True
+    gen_log.debug("Supported locales: %s", sorted(_supported_locales))
+def get_supported_locales():
+    """Returns a list of all the supported locale codes."""
+    return _supported_locales
+class Locale(object):
+    """Object representing a locale.
+    After calling one of `load_translations` or `load_gettext_translations`,
+    call `get` or `get_closest` to get a Locale object.
+    """
+    @classmethod
+    def get_closest(cls, *locale_codes):
+        """Returns the closest match for the given locale code."""
+        for code in locale_codes:
+            if not code:
+                continue
+            code = code.replace("-", "_")
+            parts = code.split("_")
+            if len(parts) > 2:
+                continue
+            elif len(parts) == 2:
+                code = parts[0].lower() + "_" + parts[1].upper()
+            if code in _supported_locales:
+                return cls.get(code)
+            if parts[0].lower() in _supported_locales:
+                return cls.get(parts[0].lower())
+        return cls.get(_default_locale)
+    @classmethod
+    def get(cls, code):
+        """Returns the Locale for the given locale code.
+        If it is not supported, we raise an exception.
+        """
+        if not hasattr(cls, "_cache"):
+            cls._cache = {}
+        if code not in cls._cache:
+            assert code in _supported_locales
+            translations = _translations.get(code, None)
+            if translations is None:
+                locale = CSVLocale(code, {})
+            elif _use_gettext:
+                locale = GettextLocale(code, translations)
+            else:
+                locale = CSVLocale(code, translations)
+            cls._cache[code] = locale
+        return cls._cache[code]
+    def __init__(self, code, translations):
+        self.code = code
+        self.name = LOCALE_NAMES.get(code, {}).get("name", u"Unknown")
+        self.rtl = False
+        for prefix in ["fa", "ar", "he"]:
+            if self.code.startswith(prefix):
+                self.rtl = True
+                break
+        self.translations = translations
+        _ = self.translate
+        self._months = [
+            _("January"), _("February"), _("March"), _("April"),
+            _("May"), _("June"), _("July"), _("August"),
+            _("September"), _("October"), _("November"), _("December")]
+        self._weekdays = [
+            _("Monday"), _("Tuesday"), _("Wednesday"), _("Thursday"),
+            _("Friday"), _("Saturday"), _("Sunday")]
+    def translate(self, message, plural_message=None, count=None):
+        """Returns the translation for the given message for this locale.
+        If ``plural_message`` is given, you must also provide
+        ``count``. We return ``plural_message`` when ``count != 1``,
+        and we return the singular form for the given message when
+        ``count == 1``.
+        """
+        raise NotImplementedError()
+    def pgettext(self, context, message, plural_message=None, count=None):
+        raise NotImplementedError()
+    def format_date(self, date, gmt_offset=0, relative=True, shorter=False,
+                    full_format=False):
+        """Formats the given date (which should be GMT).
+        By default, we return a relative time (e.g., "2 minutes ago"). You
+        can return an absolute date string with ``relative=False``.
+        You can force a full format date ("July 10, 1980") with
+        ``full_format=True``.
+        This method is primarily intended for dates in the past.
+        For dates in the future, we fall back to full format.
+        """
+        if isinstance(date, numbers.Real):
+            date = datetime.datetime.utcfromtimestamp(date)
+        now = datetime.datetime.utcnow()
+        if date > now:
+            if relative and (date - now).seconds < 60:
+                date = now
+            else:
+                full_format = True
+        local_date = date - datetime.timedelta(minutes=gmt_offset)
+        local_now = now - datetime.timedelta(minutes=gmt_offset)
+        local_yesterday = local_now - datetime.timedelta(hours=24)
+        difference = now - date
+        seconds = difference.seconds
+        days = difference.days
+        _ = self.translate
+        format = None
+        if not full_format:
+            if relative and days == 0:
+                if seconds < 50:
+                    return _("1 second ago", "%(seconds)d seconds ago",
+                             seconds) % {"seconds": seconds}
+                if seconds < 50 * 60:
+                    minutes = round(seconds / 60.0)
+                    return _("1 minute ago", "%(minutes)d minutes ago",
+                             minutes) % {"minutes": minutes}
+                hours = round(seconds / (60.0 * 60))
+                return _("1 hour ago", "%(hours)d hours ago",
+                         hours) % {"hours": hours}
+            if days == 0:
+                format = _("%(time)s")
+            elif days == 1 and local_date.day == local_yesterday.day and \
+                    relative:
+                format = _("yesterday") if shorter else \
+                    _("yesterday at %(time)s")
+            elif days < 5:
+                format = _("%(weekday)s") if shorter else \
+                    _("%(weekday)s at %(time)s")
+            elif days < 334:  # 11mo, since confusing for same month last year
+                format = _("%(month_name)s %(day)s") if shorter else \
+                    _("%(month_name)s %(day)s at %(time)s")
+        if format is None:
+            format = _("%(month_name)s %(day)s, %(year)s") if shorter else \
+                _("%(month_name)s %(day)s, %(year)s at %(time)s")
+        tfhour_clock = self.code not in ("en", "en_US", "zh_CN")
+        if tfhour_clock:
+            str_time = "%d:%02d" % (local_date.hour, local_date.minute)
+        elif self.code == "zh_CN":
+            str_time = "%s%d:%02d" % (
+                (u'\u4e0a\u5348', u'\u4e0b\u5348')[local_date.hour >= 12],
+                local_date.hour % 12 or 12, local_date.minute)
+        else:
+            str_time = "%d:%02d %s" % (
+                local_date.hour % 12 or 12, local_date.minute,
+                ("am", "pm")[local_date.hour >= 12])
+        return format % {
+            "month_name": self._months[local_date.month - 1],
+            "weekday": self._weekdays[local_date.weekday()],
+            "day": str(local_date.day),
+            "year": str(local_date.year),
+            "time": str_time
+        }
+    def format_day(self, date, gmt_offset=0, dow=True):
+        """Formats the given date as a day of week.
+        Example: "Monday, January 22". You can remove the day of week with
+        ``dow=False``.
+        """
+        local_date = date - datetime.timedelta(minutes=gmt_offset)
+        _ = self.translate
+        if dow:
+            return _("%(weekday)s, %(month_name)s %(day)s") % {
+                "month_name": self._months[local_date.month - 1],
+                "weekday": self._weekdays[local_date.weekday()],
+                "day": str(local_date.day),
+            }
+        else:
+            return _("%(month_name)s %(day)s") % {
+                "month_name": self._months[local_date.month - 1],
+                "day": str(local_date.day),
+            }
+    def list(self, parts):
+        """Returns a comma-separated list for the given list of parts.
+        The format is, e.g., "A, B and C", "A and B" or just "A" for lists
+        of size 1.
+        """
+        _ = self.translate
+        if len(parts) == 0:
+            return ""
+        if len(parts) == 1:
+            return parts[0]
+        comma = u' \u0648 ' if self.code.startswith("fa") else u", "
+        return _("%(commas)s and %(last)s") % {
+            "commas": comma.join(parts[:-1]),
+            "last": parts[len(parts) - 1],
+        }
+    def friendly_number(self, value):
+        """Returns a comma-separated number for the given integer."""
+        if self.code not in ("en", "en_US"):
+            return str(value)
+        value = str(value)
+        parts = []
+        while value:
+            parts.append(value[-3:])
+            value = value[:-3]
+        return ",".join(reversed(parts))
+class CSVLocale(Locale):
+    """Locale implementation using tornado's CSV translation format."""
+    def translate(self, message, plural_message=None, count=None):
+        if plural_message is not None:
+            assert count is not None
+            if count != 1:
+                message = plural_message
+                message_dict = self.translations.get("plural", {})
+            else:
+                message_dict = self.translations.get("singular", {})
+        else:
+            message_dict = self.translations.get("unknown", {})
+        return message_dict.get(message, message)
+    def pgettext(self, context, message, plural_message=None, count=None):
+        if self.translations:
+            gen_log.warning('pgettext is not supported by CSVLocale')
+        return self.translate(message, plural_message, count)
+class GettextLocale(Locale):
+    """Locale implementation using the `gettext` module."""
+    def __init__(self, code, translations):
+        try:
+            self.ngettext = translations.ungettext
+            self.gettext = translations.ugettext
+        except AttributeError:
+            self.ngettext = translations.ngettext
+            self.gettext = translations.gettext
+        super(GettextLocale, self).__init__(code, translations)
+    def translate(self, message, plural_message=None, count=None):
+        if plural_message is not None:
+            assert count is not None
+            return self.ngettext(message, plural_message, count)
+        else:
+            return self.gettext(message)
+    def pgettext(self, context, message, plural_message=None, count=None):
+        """Allows to set context for translation, accepts plural forms.
+        Usage example::
+            pgettext("law", "right")
+            pgettext("good", "right")
+        Plural message example::
+            pgettext("organization", "club", "clubs", len(clubs))
+            pgettext("stick", "club", "clubs", len(clubs))
+        To generate POT file with context, add following options to step 1
+        of `load_gettext_translations` sequence::
+            xgettext [basic options] --keyword=pgettext:1c,2 --keyword=pgettext:1c,2,3
+        .. versionadded:: 4.2
+        """
+        if plural_message is not None:
+            assert count is not None
+            msgs_with_ctxt = ("%s%s%s" % (context, CONTEXT_SEPARATOR, message),
+                              "%s%s%s" % (context, CONTEXT_SEPARATOR, plural_message),
+                              count)
+            result = self.ngettext(*msgs_with_ctxt)
+            if CONTEXT_SEPARATOR in result:
+                result = self.ngettext(message, plural_message, count)
+            return result
+        else:
+            msg_with_ctxt = "%s%s%s" % (context, CONTEXT_SEPARATOR, message)
+            result = self.gettext(msg_with_ctxt)
+            if CONTEXT_SEPARATOR in result:
+                result = message
+            return result

--- a//dev/null
+++ b/salt/ext/tornado/locks.py
@@ -0,0 +1,361 @@
+from __future__ import absolute_import, division, print_function
+import collections
+from salt.ext.tornado import gen, ioloop
+from salt.ext.tornado.concurrent import Future
+__all__ = ['Condition', 'Event', 'Semaphore', 'BoundedSemaphore', 'Lock']
+class _TimeoutGarbageCollector(object):
+    """Base class for objects that periodically clean up timed-out waiters.
+    Avoids memory leak in a common pattern like:
+        while True:
+            yield condition.wait(short_timeout)
+            print('looping....')
+    """
+    def __init__(self):
+        self._waiters = collections.deque()  # Futures.
+        self._timeouts = 0
+    def _garbage_collect(self):
+        self._timeouts += 1
+        if self._timeouts > 100:
+            self._timeouts = 0
+            self._waiters = collections.deque(
+                w for w in self._waiters if not w.done())
+class Condition(_TimeoutGarbageCollector):
+    """A condition allows one or more coroutines to wait until notified.
+    Like a standard `threading.Condition`, but does not need an underlying lock
+    that is acquired and released.
+    With a `Condition`, coroutines can wait to be notified by other coroutines:
+    .. testcode::
+        from salt.ext.tornado import gen
+        from salt.ext.tornado.ioloop import IOLoop
+        from salt.ext.tornado.locks import Condition
+        condition = Condition()
+        @gen.coroutine
+        def waiter():
+            print("I'll wait right here")
+            yield condition.wait()  # Yield a Future.
+            print("I'm done waiting")
+        @gen.coroutine
+        def notifier():
+            print("About to notify")
+            condition.notify()
+            print("Done notifying")
+        @gen.coroutine
+        def runner():
+            yield [waiter(), notifier()]
+        IOLoop.current().run_sync(runner)
+    .. testoutput::
+        I'll wait right here
+        About to notify
+        Done notifying
+        I'm done waiting
+    `wait` takes an optional ``timeout`` argument, which is either an absolute
+    timestamp::
+        io_loop = IOLoop.current()
+        yield condition.wait(timeout=io_loop.time() + 1)
+    ...or a `datetime.timedelta` for a timeout relative to the current time::
+        yield condition.wait(timeout=datetime.timedelta(seconds=1))
+    The method raises `tornado.gen.TimeoutError` if there's no notification
+    before the deadline.
+    """
+    def __init__(self):
+        super(Condition, self).__init__()
+        self.io_loop = ioloop.IOLoop.current()
+    def __repr__(self):
+        result = '<%s' % (self.__class__.__name__, )
+        if self._waiters:
+            result += ' waiters[%s]' % len(self._waiters)
+        return result + '>'
+    def wait(self, timeout=None):
+        """Wait for `.notify`.
+        Returns a `.Future` that resolves ``True`` if the condition is notified,
+        or ``False`` after a timeout.
+        """
+        waiter = Future()
+        self._waiters.append(waiter)
+        if timeout:
+            def on_timeout():
+                waiter.set_result(False)
+                self._garbage_collect()
+            io_loop = ioloop.IOLoop.current()
+            timeout_handle = io_loop.add_timeout(timeout, on_timeout)
+            waiter.add_done_callback(
+                lambda _: io_loop.remove_timeout(timeout_handle))
+        return waiter
+    def notify(self, n=1):
+        """Wake ``n`` waiters."""
+        waiters = []  # Waiters we plan to run right now.
+        while n and self._waiters:
+            waiter = self._waiters.popleft()
+            if not waiter.done():  # Might have timed out.
+                n -= 1
+                waiters.append(waiter)
+        for waiter in waiters:
+            waiter.set_result(True)
+    def notify_all(self):
+        """Wake all waiters."""
+        self.notify(len(self._waiters))
+class Event(object):
+    """An event blocks coroutines until its internal flag is set to True.
+    Similar to `threading.Event`.
+    A coroutine can wait for an event to be set. Once it is set, calls to
+    ``yield event.wait()`` will not block unless the event has been cleared:
+    .. testcode::
+        from salt.ext.tornado import gen
+        from salt.ext.tornado.ioloop import IOLoop
+        from salt.ext.tornado.locks import Event
+        event = Event()
+        @gen.coroutine
+        def waiter():
+            print("Waiting for event")
+            yield event.wait()
+            print("Not waiting this time")
+            yield event.wait()
+            print("Done")
+        @gen.coroutine
+        def setter():
+            print("About to set the event")
+            event.set()
+        @gen.coroutine
+        def runner():
+            yield [waiter(), setter()]
+        IOLoop.current().run_sync(runner)
+    .. testoutput::
+        Waiting for event
+        About to set the event
+        Not waiting this time
+        Done
+    """
+    def __init__(self):
+        self._future = Future()
+    def __repr__(self):
+        return '<%s %s>' % (
+            self.__class__.__name__, 'set' if self.is_set() else 'clear')
+    def is_set(self):
+        """Return ``True`` if the internal flag is true."""
+        return self._future.done()
+    def set(self):
+        """Set the internal flag to ``True``. All waiters are awakened.
+        Calling `.wait` once the flag is set will not block.
+        """
+        if not self._future.done():
+            self._future.set_result(None)
+    def clear(self):
+        """Reset the internal flag to ``False``.
+        Calls to `.wait` will block until `.set` is called.
+        """
+        if self._future.done():
+            self._future = Future()
+    def wait(self, timeout=None):
+        """Block until the internal flag is true.
+        Returns a Future, which raises `tornado.gen.TimeoutError` after a
+        timeout.
+        """
+        if timeout is None:
+            return self._future
+        else:
+            return gen.with_timeout(timeout, self._future)
+class _ReleasingContextManager(object):
+    """Releases a Lock or Semaphore at the end of a "with" statement.
+        with (yield semaphore.acquire()):
+            pass
+    """
+    def __init__(self, obj):
+        self._obj = obj
+    def __enter__(self):
+        pass
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        self._obj.release()
+class Semaphore(_TimeoutGarbageCollector):
+    """A lock that can be acquired a fixed number of times before blocking.
+    A Semaphore manages a counter representing the number of `.release` calls
+    minus the number of `.acquire` calls, plus an initial value. The `.acquire`
+    method blocks if necessary until it can return without making the counter
+    negative.
+    Semaphores limit access to a shared resource. To allow access for two
+    workers at a time:
+    .. testsetup:: semaphore
+       from collections import deque
+       from salt.ext.tornado import gen
+       from salt.ext.tornado.ioloop import IOLoop
+       from salt.ext.tornado.concurrent import Future
+       futures_q = deque([Future() for _ in range(3)])
+       @gen.coroutine
+       def simulator(futures):
+           for f in futures:
+               yield gen.moment
+               f.set_result(None)
+       IOLoop.current().add_callback(simulator, list(futures_q))
+       def use_some_resource():
+           return futures_q.popleft()
+    .. testcode:: semaphore
+        from salt.ext.tornado import gen
+        from salt.ext.tornado.ioloop import IOLoop
+        from salt.ext.tornado.locks import Semaphore
+        sem = Semaphore(2)
+        @gen.coroutine
+        def worker(worker_id):
+            yield sem.acquire()
+            try:
+                print("Worker %d is working" % worker_id)
+                yield use_some_resource()
+            finally:
+                print("Worker %d is done" % worker_id)
+                sem.release()
+        @gen.coroutine
+        def runner():
+            yield [worker(i) for i in range(3)]
+        IOLoop.current().run_sync(runner)
+    .. testoutput:: semaphore
+        Worker 0 is working
+        Worker 1 is working
+        Worker 0 is done
+        Worker 2 is working
+        Worker 1 is done
+        Worker 2 is done
+    Workers 0 and 1 are allowed to run concurrently, but worker 2 waits until
+    the semaphore has been released once, by worker 0.
+    `.acquire` is a context manager, so ``worker`` could be written as::
+        @gen.coroutine
+        def worker(worker_id):
+            with (yield sem.acquire()):
+                print("Worker %d is working" % worker_id)
+                yield use_some_resource()
+            print("Worker %d is done" % worker_id)
+    In Python 3.5, the semaphore itself can be used as an async context
+    manager::
+        async def worker(worker_id):
+            async with sem:
+                print("Worker %d is working" % worker_id)
+                await use_some_resource()
+            print("Worker %d is done" % worker_id)
+    .. versionchanged:: 4.3
+       Added ``async with`` support in Python 3.5.
+    """
+    def __init__(self, value=1):
+        super(Semaphore, self).__init__()
+        if value < 0:
+            raise ValueError('semaphore initial value must be >= 0')
+        self._value = value
+    def __repr__(self):
+        res = super(Semaphore, self).__repr__()
+        extra = 'locked' if self._value == 0 else 'unlocked,value:{0}'.format(
+            self._value)
+        if self._waiters:
+            extra = '{0},waiters:{1}'.format(extra, len(self._waiters))
+        return '<{0} [{1}]>'.format(res[1:-1], extra)
+    def release(self):
+        """Increment the counter and wake one waiter."""
+        self._value += 1
+        while self._waiters:
+            waiter = self._waiters.popleft()
+            if not waiter.done():
+                self._value -= 1
+                waiter.set_result(_ReleasingContextManager(self))
+                break
+    def acquire(self, timeout=None):
+        """Decrement the counter. Returns a Future.
+        Block if the counter is zero and wait for a `.release`. The Future
+        raises `.TimeoutError` after the deadline.
+        """
+        waiter = Future()
+        if self._value > 0:
+            self._value -= 1
+            waiter.set_result(_ReleasingContextManager(self))
+        else:
+            self._waiters.append(waiter)
+            if timeout:
+                def on_timeout():
+                    waiter.set_exception(gen.TimeoutError())
+                    self._garbage_collect()
+                io_loop = ioloop.IOLoop.current()
+                timeout_handle = io_loop.add_timeout(timeout, on_timeout)
+                waiter.add_done_callback(
+                    lambda _: io_loop.remove_timeout(timeout_handle))
+        return waiter
+    def __enter__(self):
+        raise RuntimeError(
+            "Use Semaphore like 'with (yield semaphore.acquire())', not like"
+            " 'with semaphore'")
+    __exit__ = __enter__
+    @gen.coroutine
+    def __aenter__(self):
+        yield self.acquire()
+    @gen.coroutine
+    def __aexit__(self, typ, value, tb):
+        self.release()
+class BoundedSemaphore(Semaphore):
+    """A semaphore that prevents release() being called too many times.
+    If `.release` would increment the semaphore's value past the initial
+    value, it raises `ValueError`. Semaphores are mostly used to guard
+    resources with limited capacity, so a semaphore released too many times
+    is a sign of a bug.
+    """
+    def __init__(self, value=1):
+        super(BoundedSemaphore, self).__init__(value=value)
+        self._initial_value = value
+    def release(self):
+        """Increment the counter and wake one waiter."""
+        if self._value >= self._initial_value:
+            raise ValueError("Semaphore released too many times")
+        super(BoundedSemaphore, self).release()
+class Lock(object):
+    """A lock for coroutines.
+    A Lock begins unlocked, and `acquire` locks it immediately. While it is
+    locked, a coroutine that yields `acquire` waits until another coroutine
+    calls `release`.
+    Releasing an unlocked lock raises `RuntimeError`.
+    `acquire` supports the context manager protocol in all Python versions:
+    >>> from salt.ext.tornado import gen, locks
+    >>> lock = locks.Lock()
+    >>>
+    >>> @gen.coroutine
+    ... def f():
+    ...    with (yield lock.acquire()):
+    ...        # Do something holding the lock.
+    ...        pass
+    ...
+    ...    # Now the lock is released.
+    In Python 3.5, `Lock` also supports the async context manager
+    protocol. Note that in this case there is no `acquire`, because
+    ``async with`` includes both the ``yield`` and the ``acquire``
+    (just as it does with `threading.Lock`):
+    >>> async def f():  # doctest: +SKIP
+    ...    async with lock:
+    ...        # Do something holding the lock.
+    ...        pass
+    ...
+    ...    # Now the lock is released.
+    .. versionchanged:: 4.3
+       Added ``async with`` support in Python 3.5.
+    """
+    def __init__(self):
+        self._block = BoundedSemaphore(value=1)
+    def __repr__(self):
+        return "<%s _block=%s>" % (
+            self.__class__.__name__,
+            self._block)
+    def acquire(self, timeout=None):
+        """Attempt to lock. Returns a Future.
+        Returns a Future, which raises `tornado.gen.TimeoutError` after a
+        timeout.
+        """
+        return self._block.acquire(timeout)
+    def release(self):
+        """Unlock.
+        The first coroutine in line waiting for `acquire` gets the lock.
+        If not locked, raise a `RuntimeError`.
+        """
+        try:
+            self._block.release()
+        except ValueError:
+            raise RuntimeError('release unlocked lock')
+    def __enter__(self):
+        raise RuntimeError(
+            "Use Lock like 'with (yield lock)', not like 'with lock'")
+    __exit__ = __enter__
+    @gen.coroutine
+    def __aenter__(self):
+        yield self.acquire()
+    @gen.coroutine
+    def __aexit__(self, typ, value, tb):
+        self.release()

--- a//dev/null
+++ b/salt/ext/tornado/log.py
@@ -0,0 +1,201 @@
+"""Logging support for Tornado.
+Tornado uses three logger streams:
+* ``tornado.access``: Per-request logging for Tornado's HTTP servers (and
+  potentially other servers in the future)
+* ``tornado.application``: Logging of errors from application code (i.e.
+  uncaught exceptions from callbacks)
+* ``tornado.general``: General-purpose logging, including any errors
+  or warnings from Tornado itself.
+These streams may be configured independently using the standard library's
+`logging` module.  For example, you may wish to send ``tornado.access`` logs
+to a separate file for analysis.
+"""
+from __future__ import absolute_import, division, print_function
+import logging
+import logging.handlers
+import sys
+from salt.ext.tornado.escape import _unicode
+from salt.ext.tornado.util import unicode_type, basestring_type
+try:
+    import colorama
+except ImportError:
+    colorama = None
+try:
+    import curses  # type: ignore
+except ImportError:
+    curses = None
+access_log = logging.getLogger("tornado.access")
+app_log = logging.getLogger("tornado.application")
+gen_log = logging.getLogger("tornado.general")
+def _stderr_supports_color():
+    try:
+        if hasattr(sys.stderr, 'isatty') and sys.stderr.isatty():
+            if curses:
+                curses.setupterm()
+                if curses.tigetnum("colors") > 0:
+                    return True
+            elif colorama:
+                if sys.stderr is getattr(colorama.initialise, 'wrapped_stderr',
+                                         object()):
+                    return True
+    except Exception:
+        pass
+    return False
+def _safe_unicode(s):
+    try:
+        return _unicode(s)
+    except UnicodeDecodeError:
+        return repr(s)
+class LogFormatter(logging.Formatter):
+    """Log formatter used in Tornado.
+    Key features of this formatter are:
+    * Color support when logging to a terminal that supports it.
+    * Timestamps on every log line.
+    * Robust against str/bytes encoding problems.
+    This formatter is enabled automatically by
+    `tornado.options.parse_command_line` or `tornado.options.parse_config_file`
+    (unless ``--logging=none`` is used).
+    Color support on Windows versions that do not support ANSI color codes is
+    enabled by use of the colorama__ library. Applications that wish to use
+    this must first initialize colorama with a call to ``colorama.init``.
+    See the colorama documentation for details.
+    __ https://pypi.python.org/pypi/colorama
+    .. versionchanged:: 4.5
+       Added support for ``colorama``. Changed the constructor
+       signature to be compatible with `logging.config.dictConfig`.
+    """
+    DEFAULT_FORMAT = '%(color)s[%(levelname)1.1s %(asctime)s %(module)s:%(lineno)d]%(end_color)s %(message)s'
+    DEFAULT_DATE_FORMAT = '%y%m%d %H:%M:%S'
+    DEFAULT_COLORS = {
+        logging.DEBUG: 4,  # Blue
+        logging.INFO: 2,  # Green
+        logging.WARNING: 3,  # Yellow
+        logging.ERROR: 1,  # Red
+    }
+    def __init__(self, fmt=DEFAULT_FORMAT, datefmt=DEFAULT_DATE_FORMAT,
+                 style='%', color=True, colors=DEFAULT_COLORS):
+        r"""
+        :arg bool color: Enables color support.
+        :arg string fmt: Log message format.
+          It will be applied to the attributes dict of log records. The
+          text between ``%(color)s`` and ``%(end_color)s`` will be colored
+          depending on the level if color support is on.
+        :arg dict colors: color mappings from logging level to terminal color
+          code
+        :arg string datefmt: Datetime format.
+          Used for formatting ``(asctime)`` placeholder in ``prefix_fmt``.
+        .. versionchanged:: 3.2
+           Added ``fmt`` and ``datefmt`` arguments.
+        """
+        logging.Formatter.__init__(self, datefmt=datefmt)
+        self._fmt = fmt
+        self._colors = {}
+        if color and _stderr_supports_color():
+            if curses is not None:
+                fg_color = (curses.tigetstr("setaf") or
+                            curses.tigetstr("setf") or "")
+                if (3, 0) < sys.version_info < (3, 2, 3):
+                    fg_color = unicode_type(fg_color, "ascii")
+                for levelno, code in colors.items():
+                    self._colors[levelno] = unicode_type(curses.tparm(fg_color, code), "ascii")
+                self._normal = unicode_type(curses.tigetstr("sgr0"), "ascii")
+            else:
+                for levelno, code in colors.items():
+                    self._colors[levelno] = '\033[2;3%dm' % code
+                self._normal = '\033[0m'
+        else:
+            self._normal = ''
+    def format(self, record):
+        try:
+            message = record.getMessage()
+            assert isinstance(message, basestring_type)  # guaranteed by logging
+            record.message = _safe_unicode(message)
+        except Exception as e:
+            record.message = "Bad message (%r): %r" % (e, record.__dict__)
+        record.asctime = self.formatTime(record, self.datefmt)
+        if record.levelno in self._colors:
+            record.color = self._colors[record.levelno]
+            record.end_color = self._normal
+        else:
+            record.color = record.end_color = ''
+        formatted = self._fmt % record.__dict__
+        if record.exc_info:
+            if not record.exc_text:
+                record.exc_text = self.formatException(record.exc_info)
+        if record.exc_text:
+            lines = [formatted.rstrip()]
+            lines.extend(_safe_unicode(ln) for ln in record.exc_text.split('\n'))
+            formatted = '\n'.join(lines)
+        return formatted.replace("\n", "\n    ")
+def enable_pretty_logging(options=None, logger=None):
+    """Turns on formatted logging output as configured.
+    This is called automatically by `tornado.options.parse_command_line`
+    and `tornado.options.parse_config_file`.
+    """
+    if options is None:
+        import salt.ext.tornado.options
+        options = salt.ext.tornado.options.options
+    if options.logging is None or options.logging.lower() == 'none':
+        return
+    if logger is None:
+        logger = logging.getLogger()
+    logger.setLevel(getattr(logging, options.logging.upper()))
+    if options.log_file_prefix:
+        rotate_mode = options.log_rotate_mode
+        if rotate_mode == 'size':
+            channel = logging.handlers.RotatingFileHandler(
+                filename=options.log_file_prefix,
+                maxBytes=options.log_file_max_size,
+                backupCount=options.log_file_num_backups)
+        elif rotate_mode == 'time':
+            channel = logging.handlers.TimedRotatingFileHandler(
+                filename=options.log_file_prefix,
+                when=options.log_rotate_when,
+                interval=options.log_rotate_interval,
+                backupCount=options.log_file_num_backups)
+        else:
+            error_message = 'The value of log_rotate_mode option should be ' +\
+                            '"size" or "time", not "%s".' % rotate_mode
+            raise ValueError(error_message)
+        channel.setFormatter(LogFormatter(color=False))
+        logger.addHandler(channel)
+    if (options.log_to_stderr or
+            (options.log_to_stderr is None and not logger.handlers)):
+        channel = logging.StreamHandler()
+        channel.setFormatter(LogFormatter())
+        logger.addHandler(channel)
+def define_logging_options(options=None):
+    """Add logging-related flags to ``options``.
+    These options are present automatically on the default options instance;
+    this method is only necessary if you have created your own `.OptionParser`.
+    .. versionadded:: 4.2
+        This function existed in prior versions but was broken and undocumented until 4.2.
+    """
+    if options is None:
+        import salt.ext.tornado.options
+        options = salt.ext.tornado.options.options
+    options.define("logging", default="info",
+                   help=("Set the Python log level. If 'none', tornado won't touch the "
+                         "logging configuration."),
+                   metavar="debug|info|warning|error|none")
+    options.define("log_to_stderr", type=bool, default=None,
+                   help=("Send log output to stderr (colorized if possible). "
+                         "By default use stderr if --log_file_prefix is not set and "
+                         "no other logging is configured."))
+    options.define("log_file_prefix", type=str, default=None, metavar="PATH",
+                   help=("Path prefix for log files. "
+                         "Note that if you are running multiple tornado processes, "
+                         "log_file_prefix must be different for each of them (e.g. "
+                         "include the port number)"))
+    options.define("log_file_max_size", type=int, default=100 * 1000 * 1000,
+                   help="max size of log files before rollover")
+    options.define("log_file_num_backups", type=int, default=10,
+                   help="number of log files to keep")
+    options.define("log_rotate_when", type=str, default='midnight',
+                   help=("specify the type of TimedRotatingFileHandler interval "
+                         "other options:('S', 'M', 'H', 'D', 'W0'-'W6')"))
+    options.define("log_rotate_interval", type=int, default=1,
+                   help="The interval value of timed rotating")
+    options.define("log_rotate_mode", type=str, default='size',
+                   help="The mode of rotating files(time or size)")
+    options.add_parse_callback(lambda: enable_pretty_logging(options))

--- a//dev/null
+++ b/salt/ext/tornado/netutil.py
@@ -0,0 +1,356 @@
+"""Miscellaneous network utility code."""
+from __future__ import absolute_import, division, print_function
+import errno
+import os
+import sys
+import socket
+import stat
+from salt.ext.tornado.concurrent import dummy_executor, run_on_executor
+from salt.ext.tornado.ioloop import IOLoop
+from salt.ext.tornado.platform.auto import set_close_exec
+from salt.ext.tornado.util import PY3, Configurable, errno_from_exception
+try:
+    import ssl
+except ImportError:
+    ssl = None
+try:
+    import certifi
+except ImportError:
+    if ssl is None or hasattr(ssl, 'create_default_context'):
+        certifi = None
+    else:
+        raise
+if PY3:
+    xrange = range
+if hasattr(ssl, 'match_hostname') and hasattr(ssl, 'CertificateError'):  # python 3.2+
+    ssl_match_hostname = ssl.match_hostname
+    SSLCertificateError = ssl.CertificateError
+elif ssl is None:
+    ssl_match_hostname = SSLCertificateError = None  # type: ignore
+else:
+    import backports.ssl_match_hostname
+    ssl_match_hostname = backports.ssl_match_hostname.match_hostname
+    SSLCertificateError = backports.ssl_match_hostname.CertificateError  # type: ignore
+if hasattr(ssl, 'SSLContext'):
+    if hasattr(ssl, 'create_default_context'):
+        _client_ssl_defaults = ssl.create_default_context(
+            ssl.Purpose.SERVER_AUTH)
+        _server_ssl_defaults = ssl.create_default_context(
+            ssl.Purpose.CLIENT_AUTH)
+    else:
+        _client_ssl_defaults = ssl.SSLContext(ssl.PROTOCOL_SSLv23)
+        _client_ssl_defaults.verify_mode = ssl.CERT_REQUIRED
+        _client_ssl_defaults.load_verify_locations(certifi.where())
+        _server_ssl_defaults = ssl.SSLContext(ssl.PROTOCOL_SSLv23)
+        if hasattr(ssl, 'OP_NO_COMPRESSION'):
+            _client_ssl_defaults.options |= ssl.OP_NO_COMPRESSION
+            _server_ssl_defaults.options |= ssl.OP_NO_COMPRESSION
+elif ssl:
+    _client_ssl_defaults = dict(cert_reqs=ssl.CERT_REQUIRED,
+                                ca_certs=certifi.where())
+    _server_ssl_defaults = {}
+else:
+    _client_ssl_defaults = dict(cert_reqs=None,
+                                ca_certs=None)
+    _server_ssl_defaults = {}
+u'foo'.encode('idna')
+u'foo'.encode('latin1')
+_ERRNO_WOULDBLOCK = (errno.EWOULDBLOCK, errno.EAGAIN)
+if hasattr(errno, "WSAEWOULDBLOCK"):
+    _ERRNO_WOULDBLOCK += (errno.WSAEWOULDBLOCK,)  # type: ignore
+_DEFAULT_BACKLOG = 128
+def bind_sockets(port, address=None, family=socket.AF_UNSPEC,
+                 backlog=_DEFAULT_BACKLOG, flags=None, reuse_port=False):
+    """Creates listening sockets bound to the given port and address.
+    Returns a list of socket objects (multiple sockets are returned if
+    the given address maps to multiple IP addresses, which is most common
+    for mixed IPv4 and IPv6 use).
+    Address may be either an IP address or hostname.  If it's a hostname,
+    the server will listen on all IP addresses associated with the
+    name.  Address may be an empty string or None to listen on all
+    available interfaces.  Family may be set to either `socket.AF_INET`
+    or `socket.AF_INET6` to restrict to IPv4 or IPv6 addresses, otherwise
+    both will be used if available.
+    The ``backlog`` argument has the same meaning as for
+    `socket.listen() <socket.socket.listen>`.
+    ``flags`` is a bitmask of AI_* flags to `~socket.getaddrinfo`, like
+    ``socket.AI_PASSIVE | socket.AI_NUMERICHOST``.
+    ``reuse_port`` option sets ``SO_REUSEPORT`` option for every socket
+    in the list. If your platform doesn't support this option ValueError will
+    be raised.
+    """
+    if reuse_port and not hasattr(socket, "SO_REUSEPORT"):
+        raise ValueError("the platform doesn't support SO_REUSEPORT")
+    sockets = []
+    if address == "":
+        address = None
+    if not socket.has_ipv6 and family == socket.AF_UNSPEC:
+        family = socket.AF_INET
+    if flags is None:
+        flags = socket.AI_PASSIVE
+    bound_port = None
+    for res in set(socket.getaddrinfo(address, port, family, socket.SOCK_STREAM,
+                                      0, flags)):
+        af, socktype, proto, canonname, sockaddr = res
+        if (sys.platform == 'darwin' and address == 'localhost' and
+                af == socket.AF_INET6 and sockaddr[3] != 0):
+            continue
+        try:
+            sock = socket.socket(af, socktype, proto)
+        except socket.error as e:
+            if errno_from_exception(e) == errno.EAFNOSUPPORT:
+                continue
+            raise
+        set_close_exec(sock.fileno())
+        if os.name != 'nt':
+            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+        if reuse_port:
+            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)
+        if af == socket.AF_INET6:
+            if hasattr(socket, "IPPROTO_IPV6"):
+                sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 1)
+        host, requested_port = sockaddr[:2]
+        if requested_port == 0 and bound_port is not None:
+            sockaddr = tuple([host, bound_port] + list(sockaddr[2:]))
+        sock.setblocking(0)
+        sock.bind(sockaddr)
+        bound_port = sock.getsockname()[1]
+        sock.listen(backlog)
+        sockets.append(sock)
+    return sockets
+if hasattr(socket, 'AF_UNIX'):
+    def bind_unix_socket(file, mode=0o600, backlog=_DEFAULT_BACKLOG):
+        """Creates a listening unix socket.
+        If a socket with the given name already exists, it will be deleted.
+        If any other file with that name exists, an exception will be
+        raised.
+        Returns a socket object (not a list of socket objects like
+        `bind_sockets`)
+        """
+        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
+        set_close_exec(sock.fileno())
+        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+        sock.setblocking(0)
+        try:
+            st = os.stat(file)
+        except OSError as err:
+            if errno_from_exception(err) != errno.ENOENT:
+                raise
+        else:
+            if stat.S_ISSOCK(st.st_mode):
+                os.remove(file)
+            else:
+                raise ValueError("File %s exists and is not a socket", file)
+        sock.bind(file)
+        os.chmod(file, mode)
+        sock.listen(backlog)
+        return sock
+def add_accept_handler(sock, callback, io_loop=None):
+    """Adds an `.IOLoop` event handler to accept new connections on ``sock``.
+    When a connection is accepted, ``callback(connection, address)`` will
+    be run (``connection`` is a socket object, and ``address`` is the
+    address of the other end of the connection).  Note that this signature
+    is different from the ``callback(fd, events)`` signature used for
+    `.IOLoop` handlers.
+    .. versionchanged:: 4.1
+       The ``io_loop`` argument is deprecated.
+    """
+    if io_loop is None:
+        io_loop = IOLoop.current()
+    def accept_handler(fd, events):
+        for i in xrange(_DEFAULT_BACKLOG):
+            try:
+                connection, address = sock.accept()
+            except socket.error as e:
+                if errno_from_exception(e) in _ERRNO_WOULDBLOCK:
+                    return
+                if errno_from_exception(e) == errno.ECONNABORTED:
+                    continue
+                raise
+            set_close_exec(connection.fileno())
+            callback(connection, address)
+    io_loop.add_handler(sock, accept_handler, IOLoop.READ)
+def is_valid_ip(ip):
+    """Returns true if the given string is a well-formed IP address.
+    Supports IPv4 and IPv6.
+    """
+    if not ip or '\x00' in ip:
+        return False
+    try:
+        res = socket.getaddrinfo(ip, 0, socket.AF_UNSPEC,
+                                 socket.SOCK_STREAM,
+                                 0, socket.AI_NUMERICHOST)
+        return bool(res)
+    except socket.gaierror as e:
+        if e.args[0] == socket.EAI_NONAME:
+            return False
+        raise
+    return True
+class Resolver(Configurable):
+    """Configurable asynchronous DNS resolver interface.
+    By default, a blocking implementation is used (which simply calls
+    `socket.getaddrinfo`).  An alternative implementation can be
+    chosen with the `Resolver.configure <.Configurable.configure>`
+    class method::
+        Resolver.configure('tornado.netutil.ThreadedResolver')
+    The implementations of this interface included with Tornado are
+    * `tornado.netutil.BlockingResolver`
+    * `tornado.netutil.ThreadedResolver`
+    * `tornado.netutil.OverrideResolver`
+    * `tornado.platform.twisted.TwistedResolver`
+    * `tornado.platform.caresresolver.CaresResolver`
+    """
+    @classmethod
+    def configurable_base(cls):
+        return Resolver
+    @classmethod
+    def configurable_default(cls):
+        return BlockingResolver
+    def resolve(self, host, port, family=socket.AF_UNSPEC, callback=None):
+        """Resolves an address.
+        The ``host`` argument is a string which may be a hostname or a
+        literal IP address.
+        Returns a `.Future` whose result is a list of (family,
+        address) pairs, where address is a tuple suitable to pass to
+        `socket.connect <socket.socket.connect>` (i.e. a ``(host,
+        port)`` pair for IPv4; additional fields may be present for
+        IPv6). If a ``callback`` is passed, it will be run with the
+        result as an argument when it is complete.
+        :raises IOError: if the address cannot be resolved.
+        .. versionchanged:: 4.4
+           Standardized all implementations to raise `IOError`.
+        """
+        raise NotImplementedError()
+    def close(self):
+        """Closes the `Resolver`, freeing any resources used.
+        .. versionadded:: 3.1
+        """
+        pass
+class ExecutorResolver(Resolver):
+    """Resolver implementation using a `concurrent.futures.Executor`.
+    Use this instead of `ThreadedResolver` when you require additional
+    control over the executor being used.
+    The executor will be shut down when the resolver is closed unless
+    ``close_resolver=False``; use this if you want to reuse the same
+    executor elsewhere.
+    .. versionchanged:: 4.1
+       The ``io_loop`` argument is deprecated.
+    """
+    def initialize(self, io_loop=None, executor=None, close_executor=True):
+        self.io_loop = io_loop or IOLoop.current()
+        if executor is not None:
+            self.executor = executor
+            self.close_executor = close_executor
+        else:
+            self.executor = dummy_executor
+            self.close_executor = False
+    def close(self):
+        if self.close_executor:
+            self.executor.shutdown()
+        self.executor = None
+    @run_on_executor
+    def resolve(self, host, port, family=socket.AF_UNSPEC):
+        addrinfo = socket.getaddrinfo(host, port, family, socket.SOCK_STREAM)
+        results = []
+        for family, socktype, proto, canonname, address in addrinfo:
+            results.append((family, address))
+        return results
+class BlockingResolver(ExecutorResolver):
+    """Default `Resolver` implementation, using `socket.getaddrinfo`.
+    The `.IOLoop` will be blocked during the resolution, although the
+    callback will not be run until the next `.IOLoop` iteration.
+    """
+    def initialize(self, io_loop=None):
+        super(BlockingResolver, self).initialize(io_loop=io_loop)
+class ThreadedResolver(ExecutorResolver):
+    """Multithreaded non-blocking `Resolver` implementation.
+    Requires the `concurrent.futures` package to be installed
+    (available in the standard library since Python 3.2,
+    installable with ``pip install futures`` in older versions).
+    The thread pool size can be configured with::
+        Resolver.configure('tornado.netutil.ThreadedResolver',
+                           num_threads=10)
+    .. versionchanged:: 3.1
+       All ``ThreadedResolvers`` share a single thread pool, whose
+       size is set by the first one to be created.
+    """
+    _threadpool = None  # type: ignore
+    _threadpool_pid = None  # type: int
+    def initialize(self, io_loop=None, num_threads=10):
+        threadpool = ThreadedResolver._create_threadpool(num_threads)
+        super(ThreadedResolver, self).initialize(
+            io_loop=io_loop, executor=threadpool, close_executor=False)
+    @classmethod
+    def _create_threadpool(cls, num_threads):
+        pid = os.getpid()
+        if cls._threadpool_pid != pid:
+            cls._threadpool = None
+        if cls._threadpool is None:
+            from concurrent.futures import ThreadPoolExecutor
+            cls._threadpool = ThreadPoolExecutor(num_threads)
+            cls._threadpool_pid = pid
+        return cls._threadpool
+class OverrideResolver(Resolver):
+    """Wraps a resolver with a mapping of overrides.
+    This can be used to make local DNS changes (e.g. for testing)
+    without modifying system-wide settings.
+    The mapping can contain either host strings or host-port pairs.
+    """
+    def initialize(self, resolver, mapping):
+        self.resolver = resolver
+        self.mapping = mapping
+    def close(self):
+        self.resolver.close()
+    def resolve(self, host, port, *args, **kwargs):
+        if (host, port) in self.mapping:
+            host, port = self.mapping[(host, port)]
+        elif host in self.mapping:
+            host = self.mapping[host]
+        return self.resolver.resolve(host, port, *args, **kwargs)
+_SSL_CONTEXT_KEYWORDS = frozenset(['ssl_version', 'certfile', 'keyfile',
+                                   'cert_reqs', 'ca_certs', 'ciphers'])
+def ssl_options_to_context(ssl_options):
+    """Try to convert an ``ssl_options`` dictionary to an
+    `~ssl.SSLContext` object.
+    The ``ssl_options`` dictionary contains keywords to be passed to
+    `ssl.wrap_socket`.  In Python 2.7.9+, `ssl.SSLContext` objects can
+    be used instead.  This function converts the dict form to its
+    `~ssl.SSLContext` equivalent, and may be used when a component which
+    accepts both forms needs to upgrade to the `~ssl.SSLContext` version
+    to use features like SNI or NPN.
+    """
+    if isinstance(ssl_options, dict):
+        assert all(k in _SSL_CONTEXT_KEYWORDS for k in ssl_options), ssl_options
+    if (not hasattr(ssl, 'SSLContext') or
+            isinstance(ssl_options, ssl.SSLContext)):
+        return ssl_options
+    context = ssl.SSLContext(
+        ssl_options.get('ssl_version', ssl.PROTOCOL_SSLv23))
+    if 'certfile' in ssl_options:
+        context.load_cert_chain(ssl_options['certfile'], ssl_options.get('keyfile', None))
+    if 'cert_reqs' in ssl_options:
+        context.verify_mode = ssl_options['cert_reqs']
+    if 'ca_certs' in ssl_options:
+        context.load_verify_locations(ssl_options['ca_certs'])
+    if 'ciphers' in ssl_options:
+        context.set_ciphers(ssl_options['ciphers'])
+    if hasattr(ssl, 'OP_NO_COMPRESSION'):
+        context.options |= ssl.OP_NO_COMPRESSION
+    return context
+def ssl_wrap_socket(socket, ssl_options, server_hostname=None, **kwargs):
+    """Returns an ``ssl.SSLSocket`` wrapping the given socket.
+    ``ssl_options`` may be either an `ssl.SSLContext` object or a
+    dictionary (as accepted by `ssl_options_to_context`).  Additional
+    keyword arguments are passed to ``wrap_socket`` (either the
+    `~ssl.SSLContext` method or the `ssl` module function as
+    appropriate).
+    """
+    context = ssl_options_to_context(ssl_options)
+    if hasattr(ssl, 'SSLContext') and isinstance(context, ssl.SSLContext):
+        if server_hostname is not None and getattr(ssl, 'HAS_SNI'):
+            return context.wrap_socket(socket, server_hostname=server_hostname,
+                                       **kwargs)
+        else:
+            return context.wrap_socket(socket, **kwargs)
+    else:
+        return ssl.wrap_socket(socket, **dict(context, **kwargs))  # type: ignore

--- a//dev/null
+++ b/salt/ext/tornado/options.py
@@ -0,0 +1,451 @@
+"""A command line parsing module that lets modules define their own options.
+Each module defines its own options which are added to the global
+option namespace, e.g.::
+    from salt.ext.tornado.options import define, options
+    define("mysql_host", default="127.0.0.1:3306", help="Main user DB")
+    define("memcache_hosts", default="127.0.0.1:11011", multiple=True,
+           help="Main user memcache servers")
+    def connect():
+        db = database.Connection(options.mysql_host)
+        ...
+The ``main()`` method of your application does not need to be aware of all of
+the options used throughout your program; they are all automatically loaded
+when the modules are loaded.  However, all modules that define options
+must have been imported before the command line is parsed.
+Your ``main()`` method can parse the command line or parse a config file with
+either::
+    tornado.options.parse_command_line()
+    tornado.options.parse_config_file("/etc/server.conf")
+.. note:
+   When using tornado.options.parse_command_line or
+   tornado.options.parse_config_file, the only options that are set are
+   ones that were previously defined with tornado.options.define.
+Command line formats are what you would expect (``--myoption=myvalue``).
+Config files are just Python files. Global names become options, e.g.::
+    myoption = "myvalue"
+    myotheroption = "myothervalue"
+We support `datetimes <datetime.datetime>`, `timedeltas
+<datetime.timedelta>`, ints, and floats (just pass a ``type`` kwarg to
+`define`). We also accept multi-value options. See the documentation for
+`define()` below.
+`tornado.options.options` is a singleton instance of `OptionParser`, and
+the top-level functions in this module (`define`, `parse_command_line`, etc)
+simply call methods on it.  You may create additional `OptionParser`
+instances to define isolated sets of options, such as for subcommands.
+.. note::
+   By default, several options are defined that will configure the
+   standard `logging` module when `parse_command_line` or `parse_config_file`
+   are called.  If you want Tornado to leave the logging configuration
+   alone so you can manage it yourself, either pass ``--logging=none``
+   on the command line or do the following to disable it in code::
+       from salt.ext.tornado.options import options, parse_command_line
+       options.logging = None
+       parse_command_line()
+.. versionchanged:: 4.3
+   Dashes and underscores are fully interchangeable in option names;
+   options can be defined, set, and read with any mix of the two.
+   Dashes are typical for command-line usage while config files require
+   underscores.
+"""
+from __future__ import absolute_import, division, print_function
+import datetime
+import numbers
+import re
+import sys
+import os
+import textwrap
+from salt.ext.tornado.escape import _unicode, native_str
+from salt.ext.tornado.log import define_logging_options
+from salt.ext.tornado import stack_context
+from salt.ext.tornado.util import basestring_type, exec_in
+class Error(Exception):
+    """Exception raised by errors in the options module."""
+    pass
+class OptionParser(object):
+    """A collection of options, a dictionary with object-like access.
+    Normally accessed via static functions in the `tornado.options` module,
+    which reference a global instance.
+    """
+    def __init__(self):
+        self.__dict__['_options'] = {}
+        self.__dict__['_parse_callbacks'] = []
+        self.define("help", type=bool, help="show this help information",
+                    callback=self._help_callback)
+    def _normalize_name(self, name):
+        return name.replace('_', '-')
+    def __getattr__(self, name):
+        name = self._normalize_name(name)
+        if isinstance(self._options.get(name), _Option):
+            return self._options[name].value()
+        raise AttributeError("Unrecognized option %r" % name)
+    def __setattr__(self, name, value):
+        name = self._normalize_name(name)
+        if isinstance(self._options.get(name), _Option):
+            return self._options[name].set(value)
+        raise AttributeError("Unrecognized option %r" % name)
+    def __iter__(self):
+        return (opt.name for opt in self._options.values())
+    def __contains__(self, name):
+        name = self._normalize_name(name)
+        return name in self._options
+    def __getitem__(self, name):
+        return self.__getattr__(name)
+    def __setitem__(self, name, value):
+        return self.__setattr__(name, value)
+    def items(self):
+        """A sequence of (name, value) pairs.
+        .. versionadded:: 3.1
+        """
+        return [(opt.name, opt.value()) for name, opt in self._options.items()]
+    def groups(self):
+        """The set of option-groups created by ``define``.
+        .. versionadded:: 3.1
+        """
+        return set(opt.group_name for opt in self._options.values())
+    def group_dict(self, group):
+        """The names and values of options in a group.
+        Useful for copying options into Application settings::
+            from salt.ext.tornado.options import define, parse_command_line, options
+            define('template_path', group='application')
+            define('static_path', group='application')
+            parse_command_line()
+            application = Application(
+                handlers, **options.group_dict('application'))
+        .. versionadded:: 3.1
+        """
+        return dict(
+            (opt.name, opt.value()) for name, opt in self._options.items()
+            if not group or group == opt.group_name)
+    def as_dict(self):
+        """The names and values of all options.
+        .. versionadded:: 3.1
+        """
+        return dict(
+            (opt.name, opt.value()) for name, opt in self._options.items())
+    def define(self, name, default=None, type=None, help=None, metavar=None,
+               multiple=False, group=None, callback=None):
+        """Defines a new command line option.
+        If ``type`` is given (one of str, float, int, datetime, or timedelta)
+        or can be inferred from the ``default``, we parse the command line
+        arguments based on the given type. If ``multiple`` is True, we accept
+        comma-separated values, and the option value is always a list.
+        For multi-value integers, we also accept the syntax ``x:y``, which
+        turns into ``range(x, y)`` - very useful for long integer ranges.
+        ``help`` and ``metavar`` are used to construct the
+        automatically generated command line help string. The help
+        message is formatted like::
+           --name=METAVAR      help string
+        ``group`` is used to group the defined options in logical
+        groups. By default, command line options are grouped by the
+        file in which they are defined.
+        Command line option names must be unique globally. They can be parsed
+        from the command line with `parse_command_line` or parsed from a
+        config file with `parse_config_file`.
+        If a ``callback`` is given, it will be run with the new value whenever
+        the option is changed.  This can be used to combine command-line
+        and file-based options::
+            define("config", type=str, help="path to config file",
+                   callback=lambda path: parse_config_file(path, final=False))
+        With this definition, options in the file specified by ``--config`` will
+        override options set earlier on the command line, but can be overridden
+        by later flags.
+        """
+        normalized = self._normalize_name(name)
+        if normalized in self._options:
+            raise Error("Option %r already defined in %s" %
+                        (normalized, self._options[normalized].file_name))
+        frame = sys._getframe(0)
+        options_file = frame.f_code.co_filename
+        if (frame.f_back.f_code.co_filename == options_file and
+                frame.f_back.f_code.co_name == 'define'):
+            frame = frame.f_back
+        file_name = frame.f_back.f_code.co_filename
+        if file_name == options_file:
+            file_name = ""
+        if type is None:
+            if not multiple and default is not None:
+                type = default.__class__
+            else:
+                type = str
+        if group:
+            group_name = group
+        else:
+            group_name = file_name
+        option = _Option(name, file_name=file_name,
+                         default=default, type=type, help=help,
+                         metavar=metavar, multiple=multiple,
+                         group_name=group_name,
+                         callback=callback)
+        self._options[normalized] = option
+    def parse_command_line(self, args=None, final=True):
+        """Parses all options given on the command line (defaults to
+        `sys.argv`).
+        Note that ``args[0]`` is ignored since it is the program name
+        in `sys.argv`.
+        We return a list of all arguments that are not parsed as options.
+        If ``final`` is ``False``, parse callbacks will not be run.
+        This is useful for applications that wish to combine configurations
+        from multiple sources.
+        """
+        if args is None:
+            args = sys.argv
+        remaining = []
+        for i in range(1, len(args)):
+            if not args[i].startswith("-"):
+                remaining = args[i:]
+                break
+            if args[i] == "--":
+                remaining = args[i + 1:]
+                break
+            arg = args[i].lstrip("-")
+            name, equals, value = arg.partition("=")
+            name = self._normalize_name(name)
+            if name not in self._options:
+                self.print_help()
+                raise Error('Unrecognized command line option: %r' % name)
+            option = self._options[name]
+            if not equals:
+                if option.type == bool:
+                    value = "true"
+                else:
+                    raise Error('Option %r requires a value' % name)
+            option.parse(value)
+        if final:
+            self.run_parse_callbacks()
+        return remaining
+    def parse_config_file(self, path, final=True):
+        """Parses and loads the Python config file at the given path.
+        If ``final`` is ``False``, parse callbacks will not be run.
+        This is useful for applications that wish to combine configurations
+        from multiple sources.
+        .. versionchanged:: 4.1
+           Config files are now always interpreted as utf-8 instead of
+           the system default encoding.
+        .. versionchanged:: 4.4
+           The special variable ``__file__`` is available inside config
+           files, specifying the absolute path to the config file itself.
+        """
+        config = {'__file__': os.path.abspath(path)}
+        with open(path, 'rb') as f:
+            exec_in(native_str(f.read()), config, config)
+        for name in config:
+            normalized = self._normalize_name(name)
+            if normalized in self._options:
+                self._options[normalized].set(config[name])
+        if final:
+            self.run_parse_callbacks()
+    def print_help(self, file=None):
+        """Prints all the command line options to stderr (or another file)."""
+        if file is None:
+            file = sys.stderr
+        print("Usage: %s [OPTIONS]" % sys.argv[0], file=file)
+        print("\nOptions:\n", file=file)
+        by_group = {}
+        for option in self._options.values():
+            by_group.setdefault(option.group_name, []).append(option)
+        for filename, o in sorted(by_group.items()):
+            if filename:
+                print("\n%s options:\n" % os.path.normpath(filename), file=file)
+            o.sort(key=lambda option: option.name)
+            for option in o:
+                prefix = self._normalize_name(option.name)
+                if option.metavar:
+                    prefix += "=" + option.metavar
+                description = option.help or ""
+                if option.default is not None and option.default != '':
+                    description += " (default %s)" % option.default
+                lines = textwrap.wrap(description, 79 - 35)
+                if len(prefix) > 30 or len(lines) == 0:
+                    lines.insert(0, '')
+                print("  --%-30s %s" % (prefix, lines[0]), file=file)
+                for line in lines[1:]:
+                    print("%-34s %s" % (' ', line), file=file)
+        print(file=file)
+    def _help_callback(self, value):
+        if value:
+            self.print_help()
+            sys.exit(0)
+    def add_parse_callback(self, callback):
+        """Adds a parse callback, to be invoked when option parsing is done."""
+        self._parse_callbacks.append(stack_context.wrap(callback))
+    def run_parse_callbacks(self):
+        for callback in self._parse_callbacks:
+            callback()
+    def mockable(self):
+        """Returns a wrapper around self that is compatible with
+        `mock.patch <unittest.mock.patch>`.
+        The `mock.patch <unittest.mock.patch>` function (included in
+        the standard library `unittest.mock` package since Python 3.3,
+        or in the third-party ``mock`` package for older versions of
+        Python) is incompatible with objects like ``options`` that
+        override ``__getattr__`` and ``__setattr__``.  This function
+        returns an object that can be used with `mock.patch.object
+        <unittest.mock.patch.object>` to modify option values::
+            with mock.patch.object(options.mockable(), 'name', value):
+                assert options.name == value
+        """
+        return _Mockable(self)
+class _Mockable(object):
+    """`mock.patch` compatible wrapper for `OptionParser`.
+    As of ``mock`` version 1.0.1, when an object uses ``__getattr__``
+    hooks instead of ``__dict__``, ``patch.__exit__`` tries to delete
+    the attribute it set instead of setting a new one (assuming that
+    the object does not catpure ``__setattr__``, so the patch
+    created a new attribute in ``__dict__``).
+    _Mockable's getattr and setattr pass through to the underlying
+    OptionParser, and delattr undoes the effect of a previous setattr.
+    """
+    def __init__(self, options):
+        self.__dict__['_options'] = options
+        self.__dict__['_originals'] = {}
+    def __getattr__(self, name):
+        return getattr(self._options, name)
+    def __setattr__(self, name, value):
+        assert name not in self._originals, "don't reuse mockable objects"
+        self._originals[name] = getattr(self._options, name)
+        setattr(self._options, name, value)
+    def __delattr__(self, name):
+        setattr(self._options, name, self._originals.pop(name))
+class _Option(object):
+    UNSET = object()
+    def __init__(self, name, default=None, type=basestring_type, help=None,
+                 metavar=None, multiple=False, file_name=None, group_name=None,
+                 callback=None):
+        if default is None and multiple:
+            default = []
+        self.name = name
+        self.type = type
+        self.help = help
+        self.metavar = metavar
+        self.multiple = multiple
+        self.file_name = file_name
+        self.group_name = group_name
+        self.callback = callback
+        self.default = default
+        self._value = _Option.UNSET
+    def value(self):
+        return self.default if self._value is _Option.UNSET else self._value
+    def parse(self, value):
+        _parse = {
+            datetime.datetime: self._parse_datetime,
+            datetime.timedelta: self._parse_timedelta,
+            bool: self._parse_bool,
+            basestring_type: self._parse_string,
+        }.get(self.type, self.type)
+        if self.multiple:
+            self._value = []
+            for part in value.split(","):
+                if issubclass(self.type, numbers.Integral):
+                    lo, _, hi = part.partition(":")
+                    lo = _parse(lo)
+                    hi = _parse(hi) if hi else lo
+                    self._value.extend(range(lo, hi + 1))
+                else:
+                    self._value.append(_parse(part))
+        else:
+            self._value = _parse(value)
+        if self.callback is not None:
+            self.callback(self._value)
+        return self.value()
+    def set(self, value):
+        if self.multiple:
+            if not isinstance(value, list):
+                raise Error("Option %r is required to be a list of %s" %
+                            (self.name, self.type.__name__))
+            for item in value:
+                if item is not None and not isinstance(item, self.type):
+                    raise Error("Option %r is required to be a list of %s" %
+                                (self.name, self.type.__name__))
+        else:
+            if value is not None and not isinstance(value, self.type):
+                raise Error("Option %r is required to be a %s (%s given)" %
+                            (self.name, self.type.__name__, type(value)))
+        self._value = value
+        if self.callback is not None:
+            self.callback(self._value)
+    _DATETIME_FORMATS = [
+        "%a %b %d %H:%M:%S %Y",
+        "%Y-%m-%d %H:%M:%S",
+        "%Y-%m-%d %H:%M",
+        "%Y-%m-%dT%H:%M",
+        "%Y%m%d %H:%M:%S",
+        "%Y%m%d %H:%M",
+        "%Y-%m-%d",
+        "%Y%m%d",
+        "%H:%M:%S",
+        "%H:%M",
+    ]
+    def _parse_datetime(self, value):
+        for format in self._DATETIME_FORMATS:
+            try:
+                return datetime.datetime.strptime(value, format)
+            except ValueError:
+                pass
+        raise Error('Unrecognized date/time format: %r' % value)
+    _TIMEDELTA_ABBREV_DICT = {
+        'h': 'hours',
+        'm': 'minutes',
+        'min': 'minutes',
+        's': 'seconds',
+        'sec': 'seconds',
+        'ms': 'milliseconds',
+        'us': 'microseconds',
+        'd': 'days',
+        'w': 'weeks',
+    }
+    _FLOAT_PATTERN = r'[-+]?(?:\d+(?:\.\d*)?|\.\d+)(?:[eE][-+]?\d+)?'
+    _TIMEDELTA_PATTERN = re.compile(
+        r'\s*(%s)\s*(\w*)\s*' % _FLOAT_PATTERN, re.IGNORECASE)
+    def _parse_timedelta(self, value):
+        try:
+            sum = datetime.timedelta()
+            start = 0
+            while start < len(value):
+                m = self._TIMEDELTA_PATTERN.match(value, start)
+                if not m:
+                    raise Exception()
+                num = float(m.group(1))
+                units = m.group(2) or 'seconds'
+                units = self._TIMEDELTA_ABBREV_DICT.get(units, units)
+                sum += datetime.timedelta(**{units: num})
+                start = m.end()
+            return sum
+        except Exception:
+            raise
+    def _parse_bool(self, value):
+        return value.lower() not in ("false", "0", "f")
+    def _parse_string(self, value):
+        return _unicode(value)
+options = OptionParser()
+"""Global options object.
+All defined options are available as attributes on this object.
+"""
+def define(name, default=None, type=None, help=None, metavar=None,
+           multiple=False, group=None, callback=None):
+    """Defines an option in the global namespace.
+    See `OptionParser.define`.
+    """
+    return options.define(name, default=default, type=type, help=help,
+                          metavar=metavar, multiple=multiple, group=group,
+                          callback=callback)
+def parse_command_line(args=None, final=True):
+    """Parses global options from the command line.
+    See `OptionParser.parse_command_line`.
+    """
+    return options.parse_command_line(args, final=final)
+def parse_config_file(path, final=True):
+    """Parses global options from a config file.
+    See `OptionParser.parse_config_file`.
+    """
+    return options.parse_config_file(path, final=final)
+def print_help(file=None):
+    """Prints all the command line options to stderr (or another file).
+    See `OptionParser.print_help`.
+    """
+    return options.print_help(file)
+def add_parse_callback(callback):
+    """Adds a parse callback, to be invoked when option parsing is done.
+    See `OptionParser.add_parse_callback`
+    """
+    options.add_parse_callback(callback)
+define_logging_options(options)

--- a//dev/null
+++ b/salt/ext/tornado/platform/asyncio.py
@@ -0,0 +1,171 @@
+"""Bridges between the `asyncio` module and Tornado IOLoop.
+.. versionadded:: 3.2
+This module integrates Tornado with the ``asyncio`` module introduced
+in Python 3.4 (and available `as a separate download
+<https://pypi.python.org/pypi/asyncio>`_ for Python 3.3).  This makes
+it possible to combine the two libraries on the same event loop.
+Most applications should use `AsyncIOMainLoop` to run Tornado on the
+default ``asyncio`` event loop.  Applications that need to run event
+loops on multiple threads may use `AsyncIOLoop` to create multiple
+loops.
+.. note::
+   Tornado requires the `~asyncio.AbstractEventLoop.add_reader` family of
+   methods, so it is not compatible with the `~asyncio.ProactorEventLoop` on
+   Windows. Use the `~asyncio.SelectorEventLoop` instead.
+"""
+from __future__ import absolute_import, division, print_function
+import functools
+import salt.ext.tornado.concurrent
+from salt.ext.tornado.gen import convert_yielded
+from salt.ext.tornado.ioloop import IOLoop
+from salt.ext.tornado import stack_context
+try:
+    import asyncio  # type: ignore
+except ImportError as e:
+    try:
+        import trollius as asyncio  # type: ignore
+    except ImportError:
+        raise e
+class BaseAsyncIOLoop(IOLoop):
+    def initialize(self, asyncio_loop, close_loop=False, **kwargs):
+        super(BaseAsyncIOLoop, self).initialize(**kwargs)
+        self.asyncio_loop = asyncio_loop
+        self.close_loop = close_loop
+        self.handlers = {}
+        self.readers = set()
+        self.writers = set()
+        self.closing = False
+    def close(self, all_fds=False):
+        self.closing = True
+        for fd in list(self.handlers):
+            fileobj, handler_func = self.handlers[fd]
+            self.remove_handler(fd)
+            if all_fds:
+                self.close_fd(fileobj)
+        if self.close_loop:
+            self.asyncio_loop.close()
+    def add_handler(self, fd, handler, events):
+        fd, fileobj = self.split_fd(fd)
+        if fd in self.handlers:
+            raise ValueError("fd %s added twice" % fd)
+        self.handlers[fd] = (fileobj, stack_context.wrap(handler))
+        if events & IOLoop.READ:
+            self.asyncio_loop.add_reader(
+                fd, self._handle_events, fd, IOLoop.READ)
+            self.readers.add(fd)
+        if events & IOLoop.WRITE:
+            self.asyncio_loop.add_writer(
+                fd, self._handle_events, fd, IOLoop.WRITE)
+            self.writers.add(fd)
+    def update_handler(self, fd, events):
+        fd, fileobj = self.split_fd(fd)
+        if events & IOLoop.READ:
+            if fd not in self.readers:
+                self.asyncio_loop.add_reader(
+                    fd, self._handle_events, fd, IOLoop.READ)
+                self.readers.add(fd)
+        else:
+            if fd in self.readers:
+                self.asyncio_loop.remove_reader(fd)
+                self.readers.remove(fd)
+        if events & IOLoop.WRITE:
+            if fd not in self.writers:
+                self.asyncio_loop.add_writer(
+                    fd, self._handle_events, fd, IOLoop.WRITE)
+                self.writers.add(fd)
+        else:
+            if fd in self.writers:
+                self.asyncio_loop.remove_writer(fd)
+                self.writers.remove(fd)
+    def remove_handler(self, fd):
+        fd, fileobj = self.split_fd(fd)
+        if fd not in self.handlers:
+            return
+        if fd in self.readers:
+            self.asyncio_loop.remove_reader(fd)
+            self.readers.remove(fd)
+        if fd in self.writers:
+            self.asyncio_loop.remove_writer(fd)
+            self.writers.remove(fd)
+        del self.handlers[fd]
+    def _handle_events(self, fd, events):
+        fileobj, handler_func = self.handlers[fd]
+        handler_func(fileobj, events)
+    def start(self):
+        old_current = IOLoop.current(instance=False)
+        try:
+            self._setup_logging()
+            self.make_current()
+            self.asyncio_loop.run_forever()
+        finally:
+            if old_current is None:
+                IOLoop.clear_current()
+            else:
+                old_current.make_current()
+    def stop(self):
+        self.asyncio_loop.stop()
+    def call_at(self, when, callback, *args, **kwargs):
+        return self.asyncio_loop.call_later(
+            max(0, when - self.time()), self._run_callback,
+            functools.partial(stack_context.wrap(callback), *args, **kwargs))
+    def remove_timeout(self, timeout):
+        timeout.cancel()
+    def add_callback(self, callback, *args, **kwargs):
+        if self.closing:
+            raise RuntimeError("IOLoop is closing")
+        self.asyncio_loop.call_soon_threadsafe(
+            self._run_callback,
+            functools.partial(stack_context.wrap(callback), *args, **kwargs))
+    add_callback_from_signal = add_callback
+class AsyncIOMainLoop(BaseAsyncIOLoop):
+    """``AsyncIOMainLoop`` creates an `.IOLoop` that corresponds to the
+    current ``asyncio`` event loop (i.e. the one returned by
+    ``asyncio.get_event_loop()``).  Recommended usage::
+        from salt.ext.tornado.platform.asyncio import AsyncIOMainLoop
+        import asyncio
+        AsyncIOMainLoop().install()
+        asyncio.get_event_loop().run_forever()
+    See also :meth:`tornado.ioloop.IOLoop.install` for general notes on
+    installing alternative IOLoops.
+    """
+    def initialize(self, **kwargs):
+        super(AsyncIOMainLoop, self).initialize(asyncio.get_event_loop(),
+                                                close_loop=False, **kwargs)
+class AsyncIOLoop(BaseAsyncIOLoop):
+    """``AsyncIOLoop`` is an `.IOLoop` that runs on an ``asyncio`` event loop.
+    This class follows the usual Tornado semantics for creating new
+    ``IOLoops``; these loops are not necessarily related to the
+    ``asyncio`` default event loop.  Recommended usage::
+        from salt.ext.tornado.ioloop import IOLoop
+        IOLoop.configure('tornado.platform.asyncio.AsyncIOLoop')
+        IOLoop.current().start()
+    Each ``AsyncIOLoop`` creates a new ``asyncio.EventLoop``; this object
+    can be accessed with the ``asyncio_loop`` attribute.
+    """
+    def initialize(self, **kwargs):
+        loop = asyncio.new_event_loop()
+        try:
+            super(AsyncIOLoop, self).initialize(loop, close_loop=True, **kwargs)
+        except Exception:
+            loop.close()
+            raise
+def to_tornado_future(asyncio_future):
+    """Convert an `asyncio.Future` to a `tornado.concurrent.Future`.
+    .. versionadded:: 4.1
+    """
+    tf = salt.ext.tornado.concurrent.Future()
+    salt.ext.tornado.concurrent.chain_future(asyncio_future, tf)
+    return tf
+def to_asyncio_future(tornado_future):
+    """Convert a Tornado yieldable object to an `asyncio.Future`.
+    .. versionadded:: 4.1
+    .. versionchanged:: 4.3
+       Now accepts any yieldable object, not just
+       `tornado.concurrent.Future`.
+    """
+    tornado_future = convert_yielded(tornado_future)
+    af = asyncio.Future()
+    salt.ext.tornado.concurrent.chain_future(tornado_future, af)
+    return af
+if hasattr(convert_yielded, 'register'):
+    convert_yielded.register(asyncio.Future, to_tornado_future)  # type: ignore

--- a//dev/null
+++ b/salt/ext/tornado/platform/auto.py
@@ -0,0 +1,30 @@
+"""Implementation of platform-specific functionality.
+For each function or class described in `tornado.platform.interface`,
+the appropriate platform-specific implementation exists in this module.
+Most code that needs access to this functionality should do e.g.::
+    from salt.ext.tornado.platform.auto import set_close_exec
+"""
+from __future__ import absolute_import, division, print_function
+import os
+if 'APPENGINE_RUNTIME' in os.environ:
+    from salt.ext.tornado.platform.common import Waker
+    def set_close_exec(fd):
+        pass
+elif os.name == 'nt':
+    from salt.ext.tornado.platform.common import Waker
+    from salt.ext.tornado.platform.windows import set_close_exec
+else:
+    from salt.ext.tornado.platform.posix import set_close_exec, Waker
+try:
+    import monotime
+    monotime
+except ImportError:
+    pass
+try:
+    from monotonic import monotonic as monotonic_time
+except ImportError:
+    try:
+        from time import monotonic as monotonic_time
+    except ImportError:
+        monotonic_time = None
+__all__ = ['Waker', 'set_close_exec', 'monotonic_time']

--- a//dev/null
+++ b/salt/ext/tornado/platform/caresresolver.py
@@ -0,0 +1,69 @@
+from __future__ import absolute_import, division, print_function
+import pycares  # type: ignore
+import socket
+from salt.ext.tornado import gen
+from salt.ext.tornado.ioloop import IOLoop
+from salt.ext.tornado.netutil import Resolver, is_valid_ip
+class CaresResolver(Resolver):
+    """Name resolver based on the c-ares library.
+    This is a non-blocking and non-threaded resolver.  It may not produce
+    the same results as the system resolver, but can be used for non-blocking
+    resolution when threads cannot be used.
+    c-ares fails to resolve some names when ``family`` is ``AF_UNSPEC``,
+    so it is only recommended for use in ``AF_INET`` (i.e. IPv4).  This is
+    the default for ``tornado.simple_httpclient``, but other libraries
+    may default to ``AF_UNSPEC``.
+    .. versionchanged:: 4.1
+       The ``io_loop`` argument is deprecated.
+    """
+    def initialize(self, io_loop=None):
+        self.io_loop = io_loop or IOLoop.current()
+        self.channel = pycares.Channel(sock_state_cb=self._sock_state_cb)
+        self.fds = {}
+    def _sock_state_cb(self, fd, readable, writable):
+        state = ((IOLoop.READ if readable else 0) |
+                 (IOLoop.WRITE if writable else 0))
+        if not state:
+            self.io_loop.remove_handler(fd)
+            del self.fds[fd]
+        elif fd in self.fds:
+            self.io_loop.update_handler(fd, state)
+            self.fds[fd] = state
+        else:
+            self.io_loop.add_handler(fd, self._handle_events, state)
+            self.fds[fd] = state
+    def _handle_events(self, fd, events):
+        read_fd = pycares.ARES_SOCKET_BAD
+        write_fd = pycares.ARES_SOCKET_BAD
+        if events & IOLoop.READ:
+            read_fd = fd
+        if events & IOLoop.WRITE:
+            write_fd = fd
+        self.channel.process_fd(read_fd, write_fd)
+    @gen.coroutine
+    def resolve(self, host, port, family=0):
+        if is_valid_ip(host):
+            addresses = [host]
+        else:
+            self.channel.gethostbyname(host, family, (yield gen.Callback(1)))
+            callback_args = yield gen.Wait(1)
+            assert isinstance(callback_args, gen.Arguments)
+            assert not callback_args.kwargs
+            result, error = callback_args.args
+            if error:
+                raise IOError('C-Ares returned error %s: %s while resolving %s' %
+                              (error, pycares.errno.strerror(error), host))
+            addresses = result.addresses
+        addrinfo = []
+        for address in addresses:
+            if '.' in address:
+                address_family = socket.AF_INET
+            elif ':' in address:
+                address_family = socket.AF_INET6
+            else:
+                address_family = socket.AF_UNSPEC
+            if family != socket.AF_UNSPEC and family != address_family:
+                raise IOError('Requested socket family %d but got %d' %
+                              (family, address_family))
+            addrinfo.append((address_family, (address, port)))
+        raise gen.Return(addrinfo)

--- a//dev/null
+++ b/salt/ext/tornado/platform/common.py
@@ -0,0 +1,73 @@
+"""Lowest-common-denominator implementations of platform functionality."""
+from __future__ import absolute_import, division, print_function
+import errno
+import socket
+import time
+from salt.ext.tornado.platform import interface
+from salt.ext.tornado.util import errno_from_exception
+def try_close(f):
+    for i in range(10):
+        try:
+            f.close()
+        except IOError:
+            time.sleep(1e-3)
+        else:
+            break
+    f.close()
+class Waker(interface.Waker):
+    """Create an OS independent asynchronous pipe.
+    For use on platforms that don't have os.pipe() (or where pipes cannot
+    be passed to select()), but do have sockets.  This includes Windows
+    and Jython.
+    """
+    def __init__(self):
+        from .auto import set_close_exec
+        self.writer = socket.socket()
+        set_close_exec(self.writer.fileno())
+        self.writer.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
+        count = 0
+        while 1:
+            count += 1
+            a = socket.socket()
+            set_close_exec(a.fileno())
+            a.bind(("127.0.0.1", 0))
+            a.listen(1)
+            connect_address = a.getsockname()  # assigned (host, port) pair
+            try:
+                self.writer.connect(connect_address)
+                break    # success
+            except socket.error as detail:
+                if (not hasattr(errno, 'WSAEADDRINUSE') or
+                        errno_from_exception(detail) != errno.WSAEADDRINUSE):
+                    raise
+                if count >= 10:  # I've never seen it go above 2
+                    a.close()
+                    self.writer.close()
+                    raise socket.error("Cannot bind trigger!")
+                a.close()
+        self.reader, addr = a.accept()
+        set_close_exec(self.reader.fileno())
+        self.reader.setblocking(0)
+        self.writer.setblocking(0)
+        a.close()
+        self.reader_fd = self.reader.fileno()
+    def fileno(self):
+        return self.reader.fileno()
+    def write_fileno(self):
+        return self.writer.fileno()
+    def wake(self):
+        try:
+            self.writer.send(b"x")
+        except (IOError, socket.error, ValueError):
+            pass
+    def consume(self):
+        try:
+            while True:
+                result = self.reader.recv(1024)
+                if not result:
+                    break
+        except (IOError, socket.error):
+            pass
+    def close(self):
+        self.reader.close()
+        try_close(self.writer)

--- a//dev/null
+++ b/salt/ext/tornado/platform/epoll.py
@@ -0,0 +1,7 @@
+"""EPoll-based IOLoop implementation for Linux systems."""
+from __future__ import absolute_import, division, print_function
+import select
+from salt.ext.tornado.ioloop import PollIOLoop
+class EPollIOLoop(PollIOLoop):
+    def initialize(self, **kwargs):
+        super(EPollIOLoop, self).initialize(impl=select.epoll(), **kwargs)

--- a//dev/null
+++ b/salt/ext/tornado/platform/interface.py
@@ -0,0 +1,37 @@
+"""Interfaces for platform-specific functionality.
+This module exists primarily for documentation purposes and as base classes
+for other tornado.platform modules.  Most code should import the appropriate
+implementation from `tornado.platform.auto`.
+"""
+from __future__ import absolute_import, division, print_function
+def set_close_exec(fd):
+    """Sets the close-on-exec bit (``FD_CLOEXEC``)for a file descriptor."""
+    raise NotImplementedError()
+class Waker(object):
+    """A socket-like object that can wake another thread from ``select()``.
+    The `~tornado.ioloop.IOLoop` will add the Waker's `fileno()` to
+    its ``select`` (or ``epoll`` or ``kqueue``) calls.  When another
+    thread wants to wake up the loop, it calls `wake`.  Once it has woken
+    up, it will call `consume` to do any necessary per-wake cleanup.  When
+    the ``IOLoop`` is closed, it closes its waker too.
+    """
+    def fileno(self):
+        """Returns the read file descriptor for this waker.
+        Must be suitable for use with ``select()`` or equivalent on the
+        local platform.
+        """
+        raise NotImplementedError()
+    def write_fileno(self):
+        """Returns the write file descriptor for this waker."""
+        raise NotImplementedError()
+    def wake(self):
+        """Triggers activity on the waker's file descriptor."""
+        raise NotImplementedError()
+    def consume(self):
+        """Called after the listen has woken up to do any necessary cleanup."""
+        raise NotImplementedError()
+    def close(self):
+        """Closes the waker's file descriptor(s)."""
+        raise NotImplementedError()
+def monotonic_time():
+    raise NotImplementedError()

--- a//dev/null
+++ b/salt/ext/tornado/platform/kqueue.py
@@ -0,0 +1,53 @@
+"""KQueue-based IOLoop implementation for BSD/Mac systems."""
+from __future__ import absolute_import, division, print_function
+import select
+from salt.ext.tornado.ioloop import IOLoop, PollIOLoop
+assert hasattr(select, 'kqueue'), 'kqueue not supported'
+class _KQueue(object):
+    """A kqueue-based event loop for BSD/Mac systems."""
+    def __init__(self):
+        self._kqueue = select.kqueue()
+        self._active = {}
+    def fileno(self):
+        return self._kqueue.fileno()
+    def close(self):
+        self._kqueue.close()
+    def register(self, fd, events):
+        if fd in self._active:
+            raise IOError("fd %s already registered" % fd)
+        self._control(fd, events, select.KQ_EV_ADD)
+        self._active[fd] = events
+    def modify(self, fd, events):
+        self.unregister(fd)
+        self.register(fd, events)
+    def unregister(self, fd):
+        events = self._active.pop(fd)
+        self._control(fd, events, select.KQ_EV_DELETE)
+    def _control(self, fd, events, flags):
+        kevents = []
+        if events & IOLoop.WRITE:
+            kevents.append(select.kevent(
+                fd, filter=select.KQ_FILTER_WRITE, flags=flags))
+        if events & IOLoop.READ:
+            kevents.append(select.kevent(
+                fd, filter=select.KQ_FILTER_READ, flags=flags))
+        for kevent in kevents:
+            self._kqueue.control([kevent], 0)
+    def poll(self, timeout):
+        kevents = self._kqueue.control(None, 1000, timeout)
+        events = {}
+        for kevent in kevents:
+            fd = kevent.ident
+            if kevent.filter == select.KQ_FILTER_READ:
+                events[fd] = events.get(fd, 0) | IOLoop.READ
+            if kevent.filter == select.KQ_FILTER_WRITE:
+                if kevent.flags & select.KQ_EV_EOF:
+                    events[fd] = IOLoop.ERROR
+                else:
+                    events[fd] = events.get(fd, 0) | IOLoop.WRITE
+            if kevent.flags & select.KQ_EV_ERROR:
+                events[fd] = events.get(fd, 0) | IOLoop.ERROR
+        return events.items()
+class KQueueIOLoop(PollIOLoop):
+    def initialize(self, **kwargs):
+        super(KQueueIOLoop, self).initialize(impl=_KQueue(), **kwargs)

--- a//dev/null
+++ b/salt/ext/tornado/platform/posix.py
@@ -0,0 +1,40 @@
+"""Posix implementations of platform-specific functionality."""
+from __future__ import absolute_import, division, print_function
+import fcntl
+import os
+from salt.ext.tornado.platform import common, interface
+def set_close_exec(fd):
+    flags = fcntl.fcntl(fd, fcntl.F_GETFD)
+    fcntl.fcntl(fd, fcntl.F_SETFD, flags | fcntl.FD_CLOEXEC)
+def _set_nonblocking(fd):
+    flags = fcntl.fcntl(fd, fcntl.F_GETFL)
+    fcntl.fcntl(fd, fcntl.F_SETFL, flags | os.O_NONBLOCK)
+class Waker(interface.Waker):
+    def __init__(self):
+        r, w = os.pipe()
+        _set_nonblocking(r)
+        _set_nonblocking(w)
+        set_close_exec(r)
+        set_close_exec(w)
+        self.reader = os.fdopen(r, "rb", 0)
+        self.writer = os.fdopen(w, "wb", 0)
+    def fileno(self):
+        return self.reader.fileno()
+    def write_fileno(self):
+        return self.writer.fileno()
+    def wake(self):
+        try:
+            self.writer.write(b"x")
+        except (IOError, ValueError):
+            pass
+    def consume(self):
+        try:
+            while True:
+                result = self.reader.read()
+                if not result:
+                    break
+        except IOError:
+            pass
+    def close(self):
+        self.reader.close()
+        common.try_close(self.writer)

--- a//dev/null
+++ b/salt/ext/tornado/platform/select.py
@@ -0,0 +1,45 @@
+"""Select-based IOLoop implementation.
+Used as a fallback for systems that don't support epoll or kqueue.
+"""
+from __future__ import absolute_import, division, print_function
+import select
+from salt.ext.tornado.ioloop import IOLoop, PollIOLoop
+class _Select(object):
+    """A simple, select()-based IOLoop implementation for non-Linux systems"""
+    def __init__(self):
+        self.read_fds = set()
+        self.write_fds = set()
+        self.error_fds = set()
+        self.fd_sets = (self.read_fds, self.write_fds, self.error_fds)
+    def close(self):
+        pass
+    def register(self, fd, events):
+        if fd in self.read_fds or fd in self.write_fds or fd in self.error_fds:
+            raise IOError("fd %s already registered" % fd)
+        if events & IOLoop.READ:
+            self.read_fds.add(fd)
+        if events & IOLoop.WRITE:
+            self.write_fds.add(fd)
+        if events & IOLoop.ERROR:
+            self.error_fds.add(fd)
+    def modify(self, fd, events):
+        self.unregister(fd)
+        self.register(fd, events)
+    def unregister(self, fd):
+        self.read_fds.discard(fd)
+        self.write_fds.discard(fd)
+        self.error_fds.discard(fd)
+    def poll(self, timeout):
+        readable, writeable, errors = select.select(
+            self.read_fds, self.write_fds, self.error_fds, timeout)
+        events = {}
+        for fd in readable:
+            events[fd] = events.get(fd, 0) | IOLoop.READ
+        for fd in writeable:
+            events[fd] = events.get(fd, 0) | IOLoop.WRITE
+        for fd in errors:
+            events[fd] = events.get(fd, 0) | IOLoop.ERROR
+        return events.items()
+class SelectIOLoop(PollIOLoop):
+    def initialize(self, **kwargs):
+        super(SelectIOLoop, self).initialize(impl=_Select(), **kwargs)

--- a//dev/null
+++ b/salt/ext/tornado/platform/twisted.py
@@ -0,0 +1,440 @@
+"""Bridges between the Twisted reactor and Tornado IOLoop.
+This module lets you run applications and libraries written for
+Twisted in a Tornado application.  It can be used in two modes,
+depending on which library's underlying event loop you want to use.
+This module has been tested with Twisted versions 11.0.0 and newer.
+"""
+from __future__ import absolute_import, division, print_function
+import datetime
+import functools
+import numbers
+import socket
+import sys
+import twisted.internet.abstract  # type: ignore
+from twisted.internet.defer import Deferred  # type: ignore
+from twisted.internet.posixbase import PosixReactorBase  # type: ignore
+from twisted.internet.interfaces import IReactorFDSet, IDelayedCall, IReactorTime, IReadDescriptor, IWriteDescriptor  # type: ignore
+from twisted.python import failure, log  # type: ignore
+from twisted.internet import error  # type: ignore
+import twisted.names.cache  # type: ignore
+import twisted.names.client  # type: ignore
+import twisted.names.hosts  # type: ignore
+import twisted.names.resolve  # type: ignore
+from zope.interface import implementer  # type: ignore
+from salt.ext.tornado.concurrent import Future
+from salt.ext.tornado.escape import utf8
+from salt.ext.tornado import gen
+import salt.ext.tornado.ioloop
+from salt.ext.tornado.log import app_log
+from salt.ext.tornado.netutil import Resolver
+from salt.ext.tornado.stack_context import NullContext, wrap
+from salt.ext.tornado.ioloop import IOLoop
+from salt.ext.tornado.util import timedelta_to_seconds
+@implementer(IDelayedCall)
+class TornadoDelayedCall(object):
+    """DelayedCall object for Tornado."""
+    def __init__(self, reactor, seconds, f, *args, **kw):
+        self._reactor = reactor
+        self._func = functools.partial(f, *args, **kw)
+        self._time = self._reactor.seconds() + seconds
+        self._timeout = self._reactor._io_loop.add_timeout(self._time,
+                                                           self._called)
+        self._active = True
+    def _called(self):
+        self._active = False
+        self._reactor._removeDelayedCall(self)
+        try:
+            self._func()
+        except:
+            app_log.error("_called caught exception", exc_info=True)
+    def getTime(self):
+        return self._time
+    def cancel(self):
+        self._active = False
+        self._reactor._io_loop.remove_timeout(self._timeout)
+        self._reactor._removeDelayedCall(self)
+    def delay(self, seconds):
+        self._reactor._io_loop.remove_timeout(self._timeout)
+        self._time += seconds
+        self._timeout = self._reactor._io_loop.add_timeout(self._time,
+                                                           self._called)
+    def reset(self, seconds):
+        self._reactor._io_loop.remove_timeout(self._timeout)
+        self._time = self._reactor.seconds() + seconds
+        self._timeout = self._reactor._io_loop.add_timeout(self._time,
+                                                           self._called)
+    def active(self):
+        return self._active
+@implementer(IReactorTime, IReactorFDSet)
+class TornadoReactor(PosixReactorBase):
+    """Twisted reactor built on the Tornado IOLoop.
+    `TornadoReactor` implements the Twisted reactor interface on top of
+    the Tornado IOLoop.  To use it, simply call `install` at the beginning
+    of the application::
+        import tornado.platform.twisted
+        tornado.platform.twisted.install()
+        from twisted.internet import reactor
+    When the app is ready to start, call ``IOLoop.current().start()``
+    instead of ``reactor.run()``.
+    It is also possible to create a non-global reactor by calling
+    ``tornado.platform.twisted.TornadoReactor(io_loop)``.  However, if
+    the `.IOLoop` and reactor are to be short-lived (such as those used in
+    unit tests), additional cleanup may be required.  Specifically, it is
+    recommended to call::
+        reactor.fireSystemEvent('shutdown')
+        reactor.disconnectAll()
+    before closing the `.IOLoop`.
+    .. versionchanged:: 4.1
+       The ``io_loop`` argument is deprecated.
+    """
+    def __init__(self, io_loop=None):
+        if not io_loop:
+            io_loop = salt.ext.tornado.ioloop.IOLoop.current()
+        self._io_loop = io_loop
+        self._readers = {}  # map of reader objects to fd
+        self._writers = {}  # map of writer objects to fd
+        self._fds = {}  # a map of fd to a (reader, writer) tuple
+        self._delayedCalls = {}
+        PosixReactorBase.__init__(self)
+        self.addSystemEventTrigger('during', 'shutdown', self.crash)
+        def start_if_necessary():
+            if not self._started:
+                self.fireSystemEvent('startup')
+        self._io_loop.add_callback(start_if_necessary)
+    def seconds(self):
+        return self._io_loop.time()
+    def callLater(self, seconds, f, *args, **kw):
+        dc = TornadoDelayedCall(self, seconds, f, *args, **kw)
+        self._delayedCalls[dc] = True
+        return dc
+    def getDelayedCalls(self):
+        return [x for x in self._delayedCalls if x._active]
+    def _removeDelayedCall(self, dc):
+        if dc in self._delayedCalls:
+            del self._delayedCalls[dc]
+    def callFromThread(self, f, *args, **kw):
+        assert callable(f), "%s is not callable" % f
+        with NullContext():
+            self._io_loop.add_callback(f, *args, **kw)
+    def installWaker(self):
+        pass
+    def wakeUp(self):
+        pass
+    def _invoke_callback(self, fd, events):
+        if fd not in self._fds:
+            return
+        (reader, writer) = self._fds[fd]
+        if reader:
+            err = None
+            if reader.fileno() == -1:
+                err = error.ConnectionLost()
+            elif events & IOLoop.READ:
+                err = log.callWithLogger(reader, reader.doRead)
+            if err is None and events & IOLoop.ERROR:
+                err = error.ConnectionLost()
+            if err is not None:
+                self.removeReader(reader)
+                reader.readConnectionLost(failure.Failure(err))
+        if writer:
+            err = None
+            if writer.fileno() == -1:
+                err = error.ConnectionLost()
+            elif events & IOLoop.WRITE:
+                err = log.callWithLogger(writer, writer.doWrite)
+            if err is None and events & IOLoop.ERROR:
+                err = error.ConnectionLost()
+            if err is not None:
+                self.removeWriter(writer)
+                writer.writeConnectionLost(failure.Failure(err))
+    def addReader(self, reader):
+        if reader in self._readers:
+            return
+        fd = reader.fileno()
+        self._readers[reader] = fd
+        if fd in self._fds:
+            (_, writer) = self._fds[fd]
+            self._fds[fd] = (reader, writer)
+            if writer:
+                self._io_loop.update_handler(fd, IOLoop.READ | IOLoop.WRITE)
+        else:
+            with NullContext():
+                self._fds[fd] = (reader, None)
+                self._io_loop.add_handler(fd, self._invoke_callback,
+                                          IOLoop.READ)
+    def addWriter(self, writer):
+        if writer in self._writers:
+            return
+        fd = writer.fileno()
+        self._writers[writer] = fd
+        if fd in self._fds:
+            (reader, _) = self._fds[fd]
+            self._fds[fd] = (reader, writer)
+            if reader:
+                self._io_loop.update_handler(fd, IOLoop.READ | IOLoop.WRITE)
+        else:
+            with NullContext():
+                self._fds[fd] = (None, writer)
+                self._io_loop.add_handler(fd, self._invoke_callback,
+                                          IOLoop.WRITE)
+    def removeReader(self, reader):
+        if reader in self._readers:
+            fd = self._readers.pop(reader)
+            (_, writer) = self._fds[fd]
+            if writer:
+                self._fds[fd] = (None, writer)
+                self._io_loop.update_handler(fd, IOLoop.WRITE)
+            else:
+                del self._fds[fd]
+                self._io_loop.remove_handler(fd)
+    def removeWriter(self, writer):
+        if writer in self._writers:
+            fd = self._writers.pop(writer)
+            (reader, _) = self._fds[fd]
+            if reader:
+                self._fds[fd] = (reader, None)
+                self._io_loop.update_handler(fd, IOLoop.READ)
+            else:
+                del self._fds[fd]
+                self._io_loop.remove_handler(fd)
+    def removeAll(self):
+        return self._removeAll(self._readers, self._writers)
+    def getReaders(self):
+        return self._readers.keys()
+    def getWriters(self):
+        return self._writers.keys()
+    def stop(self):
+        PosixReactorBase.stop(self)
+        fire_shutdown = functools.partial(self.fireSystemEvent, "shutdown")
+        self._io_loop.add_callback(fire_shutdown)
+    def crash(self):
+        PosixReactorBase.crash(self)
+        self._io_loop.stop()
+    def doIteration(self, delay):
+        raise NotImplementedError("doIteration")
+    def mainLoop(self):
+        self._io_loop.start()
+class _TestReactor(TornadoReactor):
+    """Subclass of TornadoReactor for use in unittests.
+    This can't go in the test.py file because of import-order dependencies
+    with the Twisted reactor test builder.
+    """
+    def __init__(self):
+        super(_TestReactor, self).__init__(IOLoop())
+    def listenTCP(self, port, factory, backlog=50, interface=''):
+        if not interface:
+            interface = '127.0.0.1'
+        return super(_TestReactor, self).listenTCP(
+            port, factory, backlog=backlog, interface=interface)
+    def listenUDP(self, port, protocol, interface='', maxPacketSize=8192):
+        if not interface:
+            interface = '127.0.0.1'
+        return super(_TestReactor, self).listenUDP(
+            port, protocol, interface=interface, maxPacketSize=maxPacketSize)
+def install(io_loop=None):
+    """Install this package as the default Twisted reactor.
+    ``install()`` must be called very early in the startup process,
+    before most other twisted-related imports. Conversely, because it
+    initializes the `.IOLoop`, it cannot be called before
+    `.fork_processes` or multi-process `~.TCPServer.start`. These
+    conflicting requirements make it difficult to use `.TornadoReactor`
+    in multi-process mode, and an external process manager such as
+    ``supervisord`` is recommended instead.
+    .. versionchanged:: 4.1
+       The ``io_loop`` argument is deprecated.
+    """
+    if not io_loop:
+        io_loop = salt.ext.tornado.ioloop.IOLoop.current()
+    reactor = TornadoReactor(io_loop)
+    from twisted.internet.main import installReactor  # type: ignore
+    installReactor(reactor)
+    return reactor
+@implementer(IReadDescriptor, IWriteDescriptor)
+class _FD(object):
+    def __init__(self, fd, fileobj, handler):
+        self.fd = fd
+        self.fileobj = fileobj
+        self.handler = handler
+        self.reading = False
+        self.writing = False
+        self.lost = False
+    def fileno(self):
+        return self.fd
+    def doRead(self):
+        if not self.lost:
+            self.handler(self.fileobj, salt.ext.tornado.ioloop.IOLoop.READ)
+    def doWrite(self):
+        if not self.lost:
+            self.handler(self.fileobj, salt.ext.tornado.ioloop.IOLoop.WRITE)
+    def connectionLost(self, reason):
+        if not self.lost:
+            self.handler(self.fileobj, salt.ext.tornado.ioloop.IOLoop.ERROR)
+            self.lost = True
+    def logPrefix(self):
+        return ''
+class TwistedIOLoop(salt.ext.tornado.ioloop.IOLoop):
+    """IOLoop implementation that runs on Twisted.
+    `TwistedIOLoop` implements the Tornado IOLoop interface on top of
+    the Twisted reactor. Recommended usage::
+        from salt.ext.tornado.platform.twisted import TwistedIOLoop
+        from twisted.internet import reactor
+        TwistedIOLoop().install()
+        reactor.run()
+    Uses the global Twisted reactor by default.  To create multiple
+    ``TwistedIOLoops`` in the same process, you must pass a unique reactor
+    when constructing each one.
+    Not compatible with `tornado.process.Subprocess.set_exit_callback`
+    because the ``SIGCHLD`` handlers used by Tornado and Twisted conflict
+    with each other.
+    See also :meth:`tornado.ioloop.IOLoop.install` for general notes on
+    installing alternative IOLoops.
+    """
+    def initialize(self, reactor=None, **kwargs):
+        super(TwistedIOLoop, self).initialize(**kwargs)
+        if reactor is None:
+            import twisted.internet.reactor  # type: ignore
+            reactor = twisted.internet.reactor
+        self.reactor = reactor
+        self.fds = {}
+    def close(self, all_fds=False):
+        fds = self.fds
+        self.reactor.removeAll()
+        for c in self.reactor.getDelayedCalls():
+            c.cancel()
+        if all_fds:
+            for fd in fds.values():
+                self.close_fd(fd.fileobj)
+    def add_handler(self, fd, handler, events):
+        if fd in self.fds:
+            raise ValueError('fd %s added twice' % fd)
+        fd, fileobj = self.split_fd(fd)
+        self.fds[fd] = _FD(fd, fileobj, wrap(handler))
+        if events & salt.ext.tornado.ioloop.IOLoop.READ:
+            self.fds[fd].reading = True
+            self.reactor.addReader(self.fds[fd])
+        if events & salt.ext.tornado.ioloop.IOLoop.WRITE:
+            self.fds[fd].writing = True
+            self.reactor.addWriter(self.fds[fd])
+    def update_handler(self, fd, events):
+        fd, fileobj = self.split_fd(fd)
+        if events & salt.ext.tornado.ioloop.IOLoop.READ:
+            if not self.fds[fd].reading:
+                self.fds[fd].reading = True
+                self.reactor.addReader(self.fds[fd])
+        else:
+            if self.fds[fd].reading:
+                self.fds[fd].reading = False
+                self.reactor.removeReader(self.fds[fd])
+        if events & salt.ext.tornado.ioloop.IOLoop.WRITE:
+            if not self.fds[fd].writing:
+                self.fds[fd].writing = True
+                self.reactor.addWriter(self.fds[fd])
+        else:
+            if self.fds[fd].writing:
+                self.fds[fd].writing = False
+                self.reactor.removeWriter(self.fds[fd])
+    def remove_handler(self, fd):
+        fd, fileobj = self.split_fd(fd)
+        if fd not in self.fds:
+            return
+        self.fds[fd].lost = True
+        if self.fds[fd].reading:
+            self.reactor.removeReader(self.fds[fd])
+        if self.fds[fd].writing:
+            self.reactor.removeWriter(self.fds[fd])
+        del self.fds[fd]
+    def start(self):
+        old_current = IOLoop.current(instance=False)
+        try:
+            self._setup_logging()
+            self.make_current()
+            self.reactor.run()
+        finally:
+            if old_current is None:
+                IOLoop.clear_current()
+            else:
+                old_current.make_current()
+    def stop(self):
+        self.reactor.crash()
+    def add_timeout(self, deadline, callback, *args, **kwargs):
+        if isinstance(deadline, numbers.Real):
+            delay = max(deadline - self.time(), 0)
+        elif isinstance(deadline, datetime.timedelta):
+            delay = timedelta_to_seconds(deadline)
+        else:
+            raise TypeError("Unsupported deadline %r")
+        return self.reactor.callLater(
+            delay, self._run_callback,
+            functools.partial(wrap(callback), *args, **kwargs))
+    def remove_timeout(self, timeout):
+        if timeout.active():
+            timeout.cancel()
+    def add_callback(self, callback, *args, **kwargs):
+        self.reactor.callFromThread(
+            self._run_callback,
+            functools.partial(wrap(callback), *args, **kwargs))
+    def add_callback_from_signal(self, callback, *args, **kwargs):
+        self.add_callback(callback, *args, **kwargs)
+class TwistedResolver(Resolver):
+    """Twisted-based asynchronous resolver.
+    This is a non-blocking and non-threaded resolver.  It is
+    recommended only when threads cannot be used, since it has
+    limitations compared to the standard ``getaddrinfo``-based
+    `~tornado.netutil.Resolver` and
+    `~tornado.netutil.ThreadedResolver`.  Specifically, it returns at
+    most one result, and arguments other than ``host`` and ``family``
+    are ignored.  It may fail to resolve when ``family`` is not
+    ``socket.AF_UNSPEC``.
+    Requires Twisted 12.1 or newer.
+    .. versionchanged:: 4.1
+       The ``io_loop`` argument is deprecated.
+    """
+    def initialize(self, io_loop=None):
+        self.io_loop = io_loop or IOLoop.current()
+        self.reactor = salt.ext.tornado.platform.twisted.TornadoReactor(io_loop)
+        host_resolver = twisted.names.hosts.Resolver('/etc/hosts')
+        cache_resolver = twisted.names.cache.CacheResolver(reactor=self.reactor)
+        real_resolver = twisted.names.client.Resolver('/etc/resolv.conf',
+                                                      reactor=self.reactor)
+        self.resolver = twisted.names.resolve.ResolverChain(
+            [host_resolver, cache_resolver, real_resolver])
+    @gen.coroutine
+    def resolve(self, host, port, family=0):
+        if twisted.internet.abstract.isIPAddress(host):
+            resolved = host
+            resolved_family = socket.AF_INET
+        elif twisted.internet.abstract.isIPv6Address(host):
+            resolved = host
+            resolved_family = socket.AF_INET6
+        else:
+            deferred = self.resolver.getHostByName(utf8(host))
+            resolved = yield gen.Task(deferred.addBoth)
+            if isinstance(resolved, failure.Failure):
+                try:
+                    resolved.raiseException()
+                except twisted.names.error.DomainError as e:
+                    raise IOError(e)
+            elif twisted.internet.abstract.isIPAddress(resolved):
+                resolved_family = socket.AF_INET
+            elif twisted.internet.abstract.isIPv6Address(resolved):
+                resolved_family = socket.AF_INET6
+            else:
+                resolved_family = socket.AF_UNSPEC
+        if family != socket.AF_UNSPEC and family != resolved_family:
+            raise Exception('Requested socket family %d but got %d' %
+                            (family, resolved_family))
+        result = [
+            (resolved_family, (resolved, port)),
+        ]
+        raise gen.Return(result)
+if hasattr(gen.convert_yielded, 'register'):
+    @gen.convert_yielded.register(Deferred)  # type: ignore
+    def _(d):
+        f = Future()
+        def errback(failure):
+            try:
+                failure.raiseException()
+                raise Exception("errback called without error")
+            except:
+                f.set_exc_info(sys.exc_info())
+        d.addCallbacks(f.set_result, errback)
+        return f

--- a//dev/null
+++ b/salt/ext/tornado/platform/windows.py
@@ -0,0 +1,11 @@
+from __future__ import absolute_import, division, print_function
+import ctypes  # type: ignore
+import ctypes.wintypes  # type: ignore
+SetHandleInformation = ctypes.windll.kernel32.SetHandleInformation
+SetHandleInformation.argtypes = (ctypes.wintypes.HANDLE, ctypes.wintypes.DWORD, ctypes.wintypes.DWORD)
+SetHandleInformation.restype = ctypes.wintypes.BOOL
+HANDLE_FLAG_INHERIT = 0x00000001
+def set_close_exec(fd):
+    success = SetHandleInformation(fd, HANDLE_FLAG_INHERIT, 0)
+    if not success:
+        raise ctypes.WinError()

--- a//dev/null
+++ b/salt/ext/tornado/process.py
@@ -0,0 +1,277 @@
+"""Utilities for working with multiple processes, including both forking
+the server into multiple processes and managing subprocesses.
+"""
+from __future__ import absolute_import, division, print_function
+import errno
+import os
+import signal
+import subprocess
+import sys
+import time
+from binascii import hexlify
+from salt.ext.tornado.concurrent import Future
+from salt.ext.tornado import ioloop
+from salt.ext.tornado.iostream import PipeIOStream
+from salt.ext.tornado.log import gen_log
+from salt.ext.tornado.platform.auto import set_close_exec
+from salt.ext.tornado import stack_context
+from salt.ext.tornado.util import errno_from_exception, PY3
+try:
+    import multiprocessing
+except ImportError:
+    multiprocessing = None
+if PY3:
+    long = int
+try:
+    CalledProcessError = subprocess.CalledProcessError
+except AttributeError:
+    if 'APPENGINE_RUNTIME' not in os.environ:
+        raise
+def cpu_count():
+    """Returns the number of processors on this machine."""
+    if multiprocessing is None:
+        return 1
+    try:
+        return multiprocessing.cpu_count()
+    except NotImplementedError:
+        pass
+    try:
+        return os.sysconf("SC_NPROCESSORS_CONF")
+    except (AttributeError, ValueError):
+        pass
+    gen_log.error("Could not detect number of processors; assuming 1")
+    return 1
+def _reseed_random():
+    if 'random' not in sys.modules:
+        return
+    import random
+    try:
+        seed = long(hexlify(os.urandom(16)), 16)
+    except NotImplementedError:
+        seed = int(time.time() * 1000) ^ os.getpid()
+    random.seed(seed)
+def _pipe_cloexec():
+    r, w = os.pipe()
+    set_close_exec(r)
+    set_close_exec(w)
+    return r, w
+_task_id = None
+def fork_processes(num_processes, max_restarts=100):
+    """Starts multiple worker processes.
+    If ``num_processes`` is None or <= 0, we detect the number of cores
+    available on this machine and fork that number of child
+    processes. If ``num_processes`` is given and > 0, we fork that
+    specific number of sub-processes.
+    Since we use processes and not threads, there is no shared memory
+    between any server code.
+    Note that multiple processes are not compatible with the autoreload
+    module (or the ``autoreload=True`` option to `tornado.web.Application`
+    which defaults to True when ``debug=True``).
+    When using multiple processes, no IOLoops can be created or
+    referenced until after the call to ``fork_processes``.
+    In each child process, ``fork_processes`` returns its *task id*, a
+    number between 0 and ``num_processes``.  Processes that exit
+    abnormally (due to a signal or non-zero exit status) are restarted
+    with the same id (up to ``max_restarts`` times).  In the parent
+    process, ``fork_processes`` returns None if all child processes
+    have exited normally, but will otherwise only exit by throwing an
+    exception.
+    """
+    global _task_id
+    assert _task_id is None
+    if num_processes is None or num_processes <= 0:
+        num_processes = cpu_count()
+    if ioloop.IOLoop.initialized():
+        raise RuntimeError("Cannot run in multiple processes: IOLoop instance "
+                           "has already been initialized. You cannot call "
+                           "IOLoop.instance() before calling start_processes()")
+    gen_log.info("Starting %d processes", num_processes)
+    children = {}
+    def start_child(i):
+        pid = os.fork()
+        if pid == 0:
+            _reseed_random()
+            global _task_id
+            _task_id = i
+            return i
+        else:
+            children[pid] = i
+            return None
+    for i in range(num_processes):
+        id = start_child(i)
+        if id is not None:
+            return id
+    num_restarts = 0
+    while children:
+        try:
+            pid, status = os.wait()
+        except OSError as e:
+            if errno_from_exception(e) == errno.EINTR:
+                continue
+            raise
+        if pid not in children:
+            continue
+        id = children.pop(pid)
+        if os.WIFSIGNALED(status):
+            gen_log.warning("child %d (pid %d) killed by signal %d, restarting",
+                            id, pid, os.WTERMSIG(status))
+        elif os.WEXITSTATUS(status) != 0:
+            gen_log.warning("child %d (pid %d) exited with status %d, restarting",
+                            id, pid, os.WEXITSTATUS(status))
+        else:
+            gen_log.info("child %d (pid %d) exited normally", id, pid)
+            continue
+        num_restarts += 1
+        if num_restarts > max_restarts:
+            raise RuntimeError("Too many child restarts, giving up")
+        new_id = start_child(id)
+        if new_id is not None:
+            return new_id
+    sys.exit(0)
+def task_id():
+    """Returns the current task id, if any.
+    Returns None if this process was not created by `fork_processes`.
+    """
+    global _task_id
+    return _task_id
+class Subprocess(object):
+    """Wraps ``subprocess.Popen`` with IOStream support.
+    The constructor is the same as ``subprocess.Popen`` with the following
+    additions:
+    * ``stdin``, ``stdout``, and ``stderr`` may have the value
+      ``tornado.process.Subprocess.STREAM``, which will make the corresponding
+      attribute of the resulting Subprocess a `.PipeIOStream`.
+    * A new keyword argument ``io_loop`` may be used to pass in an IOLoop.
+    The ``Subprocess.STREAM`` option and the ``set_exit_callback`` and
+    ``wait_for_exit`` methods do not work on Windows. There is
+    therefore no reason to use this class instead of
+    ``subprocess.Popen`` on that platform.
+    .. versionchanged:: 4.1
+       The ``io_loop`` argument is deprecated.
+    """
+    STREAM = object()
+    _initialized = False
+    _waiting = {}  # type: ignore
+    def __init__(self, *args, **kwargs):
+        self.io_loop = kwargs.pop('io_loop', None) or ioloop.IOLoop.current()
+        pipe_fds = []
+        to_close = []
+        if kwargs.get('stdin') is Subprocess.STREAM:
+            in_r, in_w = _pipe_cloexec()
+            kwargs['stdin'] = in_r
+            pipe_fds.extend((in_r, in_w))
+            to_close.append(in_r)
+            self.stdin = PipeIOStream(in_w, io_loop=self.io_loop)
+        if kwargs.get('stdout') is Subprocess.STREAM:
+            out_r, out_w = _pipe_cloexec()
+            kwargs['stdout'] = out_w
+            pipe_fds.extend((out_r, out_w))
+            to_close.append(out_w)
+            self.stdout = PipeIOStream(out_r, io_loop=self.io_loop)
+        if kwargs.get('stderr') is Subprocess.STREAM:
+            err_r, err_w = _pipe_cloexec()
+            kwargs['stderr'] = err_w
+            pipe_fds.extend((err_r, err_w))
+            to_close.append(err_w)
+            self.stderr = PipeIOStream(err_r, io_loop=self.io_loop)
+        try:
+            self.proc = subprocess.Popen(*args, **kwargs)
+        except:
+            for fd in pipe_fds:
+                os.close(fd)
+            raise
+        for fd in to_close:
+            os.close(fd)
+        for attr in ['stdin', 'stdout', 'stderr', 'pid']:
+            if not hasattr(self, attr):  # don't clobber streams set above
+                setattr(self, attr, getattr(self.proc, attr))
+        self._exit_callback = None
+        self.returncode = None
+    def set_exit_callback(self, callback):
+        """Runs ``callback`` when this process exits.
+        The callback takes one argument, the return code of the process.
+        This method uses a ``SIGCHLD`` handler, which is a global setting
+        and may conflict if you have other libraries trying to handle the
+        same signal.  If you are using more than one ``IOLoop`` it may
+        be necessary to call `Subprocess.initialize` first to designate
+        one ``IOLoop`` to run the signal handlers.
+        In many cases a close callback on the stdout or stderr streams
+        can be used as an alternative to an exit callback if the
+        signal handler is causing a problem.
+        """
+        self._exit_callback = stack_context.wrap(callback)
+        Subprocess.initialize(self.io_loop)
+        Subprocess._waiting[self.pid] = self
+        Subprocess._try_cleanup_process(self.pid)
+    def wait_for_exit(self, raise_error=True):
+        """Returns a `.Future` which resolves when the process exits.
+        Usage::
+            ret = yield proc.wait_for_exit()
+        This is a coroutine-friendly alternative to `set_exit_callback`
+        (and a replacement for the blocking `subprocess.Popen.wait`).
+        By default, raises `subprocess.CalledProcessError` if the process
+        has a non-zero exit status. Use ``wait_for_exit(raise_error=False)``
+        to suppress this behavior and return the exit status without raising.
+        .. versionadded:: 4.2
+        """
+        future = Future()
+        def callback(ret):
+            if ret != 0 and raise_error:
+                future.set_exception(CalledProcessError(ret, None))
+            else:
+                future.set_result(ret)
+        self.set_exit_callback(callback)
+        return future
+    @classmethod
+    def initialize(cls, io_loop=None):
+        """Initializes the ``SIGCHLD`` handler.
+        The signal handler is run on an `.IOLoop` to avoid locking issues.
+        Note that the `.IOLoop` used for signal handling need not be the
+        same one used by individual Subprocess objects (as long as the
+        ``IOLoops`` are each running in separate threads).
+        .. versionchanged:: 4.1
+           The ``io_loop`` argument is deprecated.
+        """
+        if cls._initialized:
+            return
+        if io_loop is None:
+            io_loop = ioloop.IOLoop.current()
+        cls._old_sigchld = signal.signal(
+            signal.SIGCHLD,
+            lambda sig, frame: io_loop.add_callback_from_signal(cls._cleanup))
+        cls._initialized = True
+    @classmethod
+    def uninitialize(cls):
+        """Removes the ``SIGCHLD`` handler."""
+        if not cls._initialized:
+            return
+        signal.signal(signal.SIGCHLD, cls._old_sigchld)
+        cls._initialized = False
+    @classmethod
+    def _cleanup(cls):
+        for pid in list(cls._waiting.keys()):  # make a copy
+            cls._try_cleanup_process(pid)
+    @classmethod
+    def _try_cleanup_process(cls, pid):
+        try:
+            ret_pid, status = os.waitpid(pid, os.WNOHANG)
+        except OSError as e:
+            if errno_from_exception(e) == errno.ECHILD:
+                return
+        if ret_pid == 0:
+            return
+        assert ret_pid == pid
+        subproc = cls._waiting.pop(pid)
+        subproc.io_loop.add_callback_from_signal(
+            subproc._set_returncode, status)
+    def _set_returncode(self, status):
+        if os.WIFSIGNALED(status):
+            self.returncode = -os.WTERMSIG(status)
+        else:
+            assert os.WIFEXITED(status)
+            self.returncode = os.WEXITSTATUS(status)
+        self.proc.returncode = self.returncode
+        if self._exit_callback:
+            callback = self._exit_callback
+            self._exit_callback = None
+            callback(self.returncode)

--- a//dev/null
+++ b/salt/ext/tornado/queues.py
@@ -0,0 +1,266 @@
+"""Asynchronous queues for coroutines.
+.. warning::
+   Unlike the standard library's `queue` module, the classes defined here
+   are *not* thread-safe. To use these queues from another thread,
+   use `.IOLoop.add_callback` to transfer control to the `.IOLoop` thread
+   before calling any queue methods.
+"""
+from __future__ import absolute_import, division, print_function
+import collections
+import heapq
+from salt.ext.tornado import gen, ioloop
+from salt.ext.tornado.concurrent import Future
+from salt.ext.tornado.locks import Event
+__all__ = ['Queue', 'PriorityQueue', 'LifoQueue', 'QueueFull', 'QueueEmpty']
+class QueueEmpty(Exception):
+    """Raised by `.Queue.get_nowait` when the queue has no items."""
+    pass
+class QueueFull(Exception):
+    """Raised by `.Queue.put_nowait` when a queue is at its maximum size."""
+    pass
+def _set_timeout(future, timeout):
+    if timeout:
+        def on_timeout():
+            future.set_exception(gen.TimeoutError())
+        io_loop = ioloop.IOLoop.current()
+        timeout_handle = io_loop.add_timeout(timeout, on_timeout)
+        future.add_done_callback(
+            lambda _: io_loop.remove_timeout(timeout_handle))
+class _QueueIterator(object):
+    def __init__(self, q):
+        self.q = q
+    def __anext__(self):
+        return self.q.get()
+class Queue(object):
+    """Coordinate producer and consumer coroutines.
+    If maxsize is 0 (the default) the queue size is unbounded.
+    .. testcode::
+        from salt.ext.tornado import gen
+        from salt.ext.tornado.ioloop import IOLoop
+        from salt.ext.tornado.queues import Queue
+        q = Queue(maxsize=2)
+        @gen.coroutine
+        def consumer():
+            while True:
+                item = yield q.get()
+                try:
+                    print('Doing work on %s' % item)
+                    yield gen.sleep(0.01)
+                finally:
+                    q.task_done()
+        @gen.coroutine
+        def producer():
+            for item in range(5):
+                yield q.put(item)
+                print('Put %s' % item)
+        @gen.coroutine
+        def main():
+            IOLoop.current().spawn_callback(consumer)
+            yield producer()     # Wait for producer to put all tasks.
+            yield q.join()       # Wait for consumer to finish all tasks.
+            print('Done')
+        IOLoop.current().run_sync(main)
+    .. testoutput::
+        Put 0
+        Put 1
+        Doing work on 0
+        Put 2
+        Doing work on 1
+        Put 3
+        Doing work on 2
+        Put 4
+        Doing work on 3
+        Doing work on 4
+        Done
+    In Python 3.5, `Queue` implements the async iterator protocol, so
+    ``consumer()`` could be rewritten as::
+        async def consumer():
+            async for item in q:
+                try:
+                    print('Doing work on %s' % item)
+                    yield gen.sleep(0.01)
+                finally:
+                    q.task_done()
+    .. versionchanged:: 4.3
+       Added ``async for`` support in Python 3.5.
+    """
+    def __init__(self, maxsize=0):
+        if maxsize is None:
+            raise TypeError("maxsize can't be None")
+        if maxsize < 0:
+            raise ValueError("maxsize can't be negative")
+        self._maxsize = maxsize
+        self._init()
+        self._getters = collections.deque([])  # Futures.
+        self._putters = collections.deque([])  # Pairs of (item, Future).
+        self._unfinished_tasks = 0
+        self._finished = Event()
+        self._finished.set()
+    @property
+    def maxsize(self):
+        """Number of items allowed in the queue."""
+        return self._maxsize
+    def qsize(self):
+        """Number of items in the queue."""
+        return len(self._queue)
+    def empty(self):
+        return not self._queue
+    def full(self):
+        if self.maxsize == 0:
+            return False
+        else:
+            return self.qsize() >= self.maxsize
+    def put(self, item, timeout=None):
+        """Put an item into the queue, perhaps waiting until there is room.
+        Returns a Future, which raises `tornado.gen.TimeoutError` after a
+        timeout.
+        """
+        try:
+            self.put_nowait(item)
+        except QueueFull:
+            future = Future()
+            self._putters.append((item, future))
+            _set_timeout(future, timeout)
+            return future
+        else:
+            return gen._null_future
+    def put_nowait(self, item):
+        """Put an item into the queue without blocking.
+        If no free slot is immediately available, raise `QueueFull`.
+        """
+        self._consume_expired()
+        if self._getters:
+            assert self.empty(), "queue non-empty, why are getters waiting?"
+            getter = self._getters.popleft()
+            self.__put_internal(item)
+            getter.set_result(self._get())
+        elif self.full():
+            raise QueueFull
+        else:
+            self.__put_internal(item)
+    def get(self, timeout=None):
+        """Remove and return an item from the queue.
+        Returns a Future which resolves once an item is available, or raises
+        `tornado.gen.TimeoutError` after a timeout.
+        """
+        future = Future()
+        try:
+            future.set_result(self.get_nowait())
+        except QueueEmpty:
+            self._getters.append(future)
+            _set_timeout(future, timeout)
+        return future
+    def get_nowait(self):
+        """Remove and return an item from the queue without blocking.
+        Return an item if one is immediately available, else raise
+        `QueueEmpty`.
+        """
+        self._consume_expired()
+        if self._putters:
+            assert self.full(), "queue not full, why are putters waiting?"
+            item, putter = self._putters.popleft()
+            self.__put_internal(item)
+            putter.set_result(None)
+            return self._get()
+        elif self.qsize():
+            return self._get()
+        else:
+            raise QueueEmpty
+    def task_done(self):
+        """Indicate that a formerly enqueued task is complete.
+        Used by queue consumers. For each `.get` used to fetch a task, a
+        subsequent call to `.task_done` tells the queue that the processing
+        on the task is complete.
+        If a `.join` is blocking, it resumes when all items have been
+        processed; that is, when every `.put` is matched by a `.task_done`.
+        Raises `ValueError` if called more times than `.put`.
+        """
+        if self._unfinished_tasks <= 0:
+            raise ValueError('task_done() called too many times')
+        self._unfinished_tasks -= 1
+        if self._unfinished_tasks == 0:
+            self._finished.set()
+    def join(self, timeout=None):
+        """Block until all items in the queue are processed.
+        Returns a Future, which raises `tornado.gen.TimeoutError` after a
+        timeout.
+        """
+        return self._finished.wait(timeout)
+    def __aiter__(self):
+        return _QueueIterator(self)
+    def _init(self):
+        self._queue = collections.deque()
+    def _get(self):
+        return self._queue.popleft()
+    def _put(self, item):
+        self._queue.append(item)
+    def __put_internal(self, item):
+        self._unfinished_tasks += 1
+        self._finished.clear()
+        self._put(item)
+    def _consume_expired(self):
+        while self._putters and self._putters[0][1].done():
+            self._putters.popleft()
+        while self._getters and self._getters[0].done():
+            self._getters.popleft()
+    def __repr__(self):
+        return '<%s at %s %s>' % (
+            type(self).__name__, hex(id(self)), self._format())
+    def __str__(self):
+        return '<%s %s>' % (type(self).__name__, self._format())
+    def _format(self):
+        result = 'maxsize=%r' % (self.maxsize, )
+        if getattr(self, '_queue', None):
+            result += ' queue=%r' % self._queue
+        if self._getters:
+            result += ' getters[%s]' % len(self._getters)
+        if self._putters:
+            result += ' putters[%s]' % len(self._putters)
+        if self._unfinished_tasks:
+            result += ' tasks=%s' % self._unfinished_tasks
+        return result
+class PriorityQueue(Queue):
+    """A `.Queue` that retrieves entries in priority order, lowest first.
+    Entries are typically tuples like ``(priority number, data)``.
+    .. testcode::
+        from salt.ext.tornado.queues import PriorityQueue
+        q = PriorityQueue()
+        q.put((1, 'medium-priority item'))
+        q.put((0, 'high-priority item'))
+        q.put((10, 'low-priority item'))
+        print(q.get_nowait())
+        print(q.get_nowait())
+        print(q.get_nowait())
+    .. testoutput::
+        (0, 'high-priority item')
+        (1, 'medium-priority item')
+        (10, 'low-priority item')
+    """
+    def _init(self):
+        self._queue = []
+    def _put(self, item):
+        heapq.heappush(self._queue, item)
+    def _get(self):
+        return heapq.heappop(self._queue)
+class LifoQueue(Queue):
+    """A `.Queue` that retrieves the most recently put items first.
+    .. testcode::
+        from salt.ext.tornado.queues import LifoQueue
+        q = LifoQueue()
+        q.put(3)
+        q.put(2)
+        q.put(1)
+        print(q.get_nowait())
+        print(q.get_nowait())
+        print(q.get_nowait())
+    .. testoutput::
+        1
+        2
+        3
+    """
+    def _init(self):
+        self._queue = []
+    def _put(self, item):
+        self._queue.append(item)
+    def _get(self):
+        return self._queue.pop()
