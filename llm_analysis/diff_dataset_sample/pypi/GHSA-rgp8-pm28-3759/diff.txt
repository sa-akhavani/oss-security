--- a/docs/sidebars.js
+++ b/docs/sidebars.js
@@ -103,33 +103,26 @@
         { type: "category", label: "Tools", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/tools" }], link: {type: "generated-index", slug: "integrations/tools" }},
         { type: "category", label: "Agents and toolkits", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/toolkits" }], link: {type: "generated-index", slug: "integrations/toolkits" }},
         { type: "category", label: "Memory", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/memory" }], link: {type: "generated-index", slug: "integrations/memory" }},
         { type: "category", label: "Callbacks", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/callbacks" }], link: {type: "generated-index", slug: "integrations/callbacks" }},
         { type: "category", label: "Chat loaders", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/chat_loaders" }], link: {type: "generated-index", slug: "integrations/chat_loaders" }},
         { type: "category", label: "Adapters", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/adapters" }], link: {type: "generated-index", slug: "integrations/adapters" }},
         { type: "category", label: "Stores", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/stores" }], link: {type: "doc", id: "integrations/stores/index" }},
       ],
       link: {
         type: 'generated-index',
-        slug: "integrations/components",
+      slug: "integrations/components",
       },
     },
   ],
   use_cases: [
-    {
-      type: "category",
-      label: "Use cases",
-      items: [
-        { type: "autogenerated", dirName: "use_cases" },
-      ],
-      link: { type: 'generated-index', slug: "use_cases"}
-    },
+    {type: "autogenerated", dirName: "use_cases" }
   ],
   guides: [
     {type: "autogenerated", dirName: "guides" }
   ],
   templates: [
     {
       type: "category",
       label: "Templates",
       items: [
         { type: "autogenerated", dirName: "templates" },

--- a/libs/community/langchain_community/callbacks/openai_info.py
+++ b/libs/community/langchain_community/callbacks/openai_info.py
@@ -1,12 +1,11 @@
 """Callback Handler that prints to std out."""
-import threading
 from typing import Any, Dict, List
 from langchain_core.callbacks import BaseCallbackHandler
 from langchain_core.outputs import LLMResult
 MODEL_COST_PER_1K_TOKENS = {
     "gpt-4": 0.03,
     "gpt-4-0314": 0.03,
     "gpt-4-0613": 0.03,
     "gpt-4-32k": 0.06,
     "gpt-4-32k-0314": 0.06,
     "gpt-4-32k-0613": 0.06,
@@ -121,23 +120,20 @@
             "Known models are: " + ", ".join(MODEL_COST_PER_1K_TOKENS.keys())
         )
     return MODEL_COST_PER_1K_TOKENS[model_name] * (num_tokens / 1000)
 class OpenAICallbackHandler(BaseCallbackHandler):
     """Callback Handler that tracks OpenAI info."""
     total_tokens: int = 0
     prompt_tokens: int = 0
     completion_tokens: int = 0
     successful_requests: int = 0
     total_cost: float = 0.0
-    def __init__(self) -> None:
-        super().__init__()
-        self._lock = threading.Lock()
     def __repr__(self) -> str:
         return (
             f"Tokens Used: {self.total_tokens}\n"
             f"\tPrompt Tokens: {self.prompt_tokens}\n"
             f"\tCompletion Tokens: {self.completion_tokens}\n"
             f"Successful Requests: {self.successful_requests}\n"
             f"Total Cost (USD): ${self.total_cost}"
         )
     @property
     def always_verbose(self) -> bool:
@@ -148,38 +144,32 @@
     ) -> None:
         """Print out the prompts."""
         pass
     def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
         """Print out the token."""
         pass
     def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
         """Collect token usage."""
         if response.llm_output is None:
             return None
+        self.successful_requests += 1
         if "token_usage" not in response.llm_output:
-            with self._lock:
-                self.successful_requests += 1
             return None
         token_usage = response.llm_output["token_usage"]
         completion_tokens = token_usage.get("completion_tokens", 0)
         prompt_tokens = token_usage.get("prompt_tokens", 0)
         model_name = standardize_model_name(response.llm_output.get("model_name", ""))
         if model_name in MODEL_COST_PER_1K_TOKENS:
             completion_cost = get_openai_token_cost_for_model(
                 model_name, completion_tokens, is_completion=True
             )
             prompt_cost = get_openai_token_cost_for_model(model_name, prompt_tokens)
-        else:
-            completion_cost = 0
-            prompt_cost = 0
-        with self._lock:
             self.total_cost += prompt_cost + completion_cost
-            self.total_tokens += token_usage.get("total_tokens", 0)
-            self.prompt_tokens += prompt_tokens
-            self.completion_tokens += completion_tokens
-            self.successful_requests += 1
+        self.total_tokens += token_usage.get("total_tokens", 0)
+        self.prompt_tokens += prompt_tokens
+        self.completion_tokens += completion_tokens
     def __copy__(self) -> "OpenAICallbackHandler":
         """Return a copy of the callback handler."""
         return self
     def __deepcopy__(self, memo: Any) -> "OpenAICallbackHandler":
         """Return a deep copy of the callback handler."""
         return self

--- a/libs/community/langchain_community/chat_message_histories/elasticsearch.py
+++ b/libs/community/langchain_community/chat_message_histories/elasticsearch.py
@@ -13,41 +13,38 @@
 logger = logging.getLogger(__name__)
 class ElasticsearchChatMessageHistory(BaseChatMessageHistory):
     """Chat message history that stores history in Elasticsearch.
     Args:
         es_url: URL of the Elasticsearch instance to connect to.
         es_cloud_id: Cloud ID of the Elasticsearch instance to connect to.
         es_user: Username to use when connecting to Elasticsearch.
         es_password: Password to use when connecting to Elasticsearch.
         es_api_key: API key to use when connecting to Elasticsearch.
         es_connection: Optional pre-existing Elasticsearch connection.
-        esnsure_ascii: Used to escape ASCII symbols in json.dumps. Defaults to True.
         index: Name of the index to use.
         session_id: Arbitrary key that is used to store the messages
             of a single chat session.
     """
     def __init__(
         self,
         index: str,
         session_id: str,
         *,
         es_connection: Optional["Elasticsearch"] = None,
         es_url: Optional[str] = None,
         es_cloud_id: Optional[str] = None,
         es_user: Optional[str] = None,
         es_api_key: Optional[str] = None,
         es_password: Optional[str] = None,
-        esnsure_ascii: Optional[bool] = True,
     ):
         self.index: str = index
         self.session_id: str = session_id
-        self.ensure_ascii: bool = esnsure_ascii
         if es_connection is not None:
             self.client = es_connection.options(
                 headers={"user-agent": self.get_user_agent()}
             )
         elif es_url is not None or es_cloud_id is not None:
             self.client = ElasticsearchChatMessageHistory.connect_to_elasticsearch(
                 es_url=es_url,
                 username=es_user,
                 password=es_password,
                 cloud_id=es_cloud_id,
@@ -142,24 +139,21 @@
         return messages_from_dict(items)
     def add_message(self, message: BaseMessage) -> None:
         """Add a message to the chat session in Elasticsearch"""
         try:
             from elasticsearch import ApiError
             self.client.index(
                 index=self.index,
                 document={
                     "session_id": self.session_id,
                     "created_at": round(time() * 1000),
-                    "history": json.dumps(
-                        message_to_dict(message),
-                        ensure_ascii=self.ensure_ascii,
-                    ),
+                    "history": json.dumps(message_to_dict(message)),
                 },
                 refresh=True,
             )
         except ApiError as err:
             logger.error(f"Could not add message to Elasticsearch: {err}")
             raise err
     def clear(self) -> None:
         """Clear session memory in Elasticsearch"""
         try:
             from elasticsearch import ApiError

--- a/libs/community/langchain_community/chat_models/__init__.py
+++ b/libs/community/langchain_community/chat_models/__init__.py
@@ -17,21 +17,20 @@
 from langchain_community.chat_models.bedrock import BedrockChat
 from langchain_community.chat_models.cohere import ChatCohere
 from langchain_community.chat_models.databricks import ChatDatabricks
 from langchain_community.chat_models.ernie import ErnieBotChat
 from langchain_community.chat_models.everlyai import ChatEverlyAI
 from langchain_community.chat_models.fake import FakeListChatModel
 from langchain_community.chat_models.fireworks import ChatFireworks
 from langchain_community.chat_models.gigachat import GigaChat
 from langchain_community.chat_models.google_palm import ChatGooglePalm
 from langchain_community.chat_models.gpt_router import GPTRouter
-from langchain_community.chat_models.huggingface import ChatHuggingFace
 from langchain_community.chat_models.human import HumanInputChatModel
 from langchain_community.chat_models.hunyuan import ChatHunyuan
 from langchain_community.chat_models.javelin_ai_gateway import ChatJavelinAIGateway
 from langchain_community.chat_models.jinachat import JinaChat
 from langchain_community.chat_models.konko import ChatKonko
 from langchain_community.chat_models.litellm import ChatLiteLLM
 from langchain_community.chat_models.minimax import MiniMaxChat
 from langchain_community.chat_models.mlflow import ChatMlflow
 from langchain_community.chat_models.mlflow_ai_gateway import ChatMLflowAIGateway
 from langchain_community.chat_models.ollama import ChatOllama
@@ -50,21 +49,20 @@
     "ChatDatabricks",
     "ChatEverlyAI",
     "ChatAnthropic",
     "ChatCohere",
     "ChatGooglePalm",
     "ChatMlflow",
     "ChatMLflowAIGateway",
     "ChatOllama",
     "ChatVertexAI",
     "JinaChat",
-    "ChatHuggingFace",
     "HumanInputChatModel",
     "MiniMaxChat",
     "ChatAnyscale",
     "ChatLiteLLM",
     "ErnieBotChat",
     "ChatJavelinAIGateway",
     "ChatKonko",
     "PaiEasChatEndpoint",
     "QianfanChatEndpoint",
     "ChatFireworks",

--- a/libs/community/langchain_community/chat_models/azure_openai.py
+++ b/libs/community/langchain_community/chat_models/azure_openai.py
@@ -11,21 +11,21 @@
 from langchain_community.utils.openai import is_openai_v1
 logger = logging.getLogger(__name__)
 class AzureChatOpenAI(ChatOpenAI):
     """`Azure OpenAI` Chat Completion API.
     To use this class you
     must have a deployed model on Azure OpenAI. Use `deployment_name` in the
     constructor to refer to the "Model deployment name" in the Azure portal.
     In addition, you should have the ``openai`` python package installed, and the
     following environment variables set or passed in constructor in lower case:
     - ``AZURE_OPENAI_API_KEY``
-    - ``AZURE_OPENAI_ENDPOINT``
+    - ``AZURE_OPENAI_API_ENDPOINT``
     - ``AZURE_OPENAI_AD_TOKEN``
     - ``OPENAI_API_VERSION``
     - ``OPENAI_PROXY``
     For example, if you have `gpt-35-turbo` deployed, with the deployment name
     `35-turbo-dev`, the constructor should look like:
     .. code-block:: python
         AzureChatOpenAI(
             azure_deployment="35-turbo-dev",
             openai_api_version="2023-05-15",
         )

--- a/libs/community/langchain_community/chat_models/gpt_router.py
+++ b/libs/community/langchain_community/chat_models/gpt_router.py
@@ -183,22 +183,22 @@
             "max_tokens": self.max_tokens,
             "stream": self.streaming,
             "n": self.n,
             "temperature": self.temperature,
             **self.model_kwargs,
         }
     def _generate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        stream: Optional[bool] = None,
+        run_manager: CallbackManagerForLLMRun | None = None,
+        stream: bool | None = None,
         **kwargs: Any,
     ) -> ChatResult:
         should_stream = stream if stream is not None else self.streaming
         if should_stream:
             stream_iter = self._stream(
                 messages, stop=stop, run_manager=run_manager, **kwargs
             )
             return generate_from_stream(stream_iter)
         message_dicts, params = self._create_message_dicts(messages, stop)
         params = {**params, **kwargs, "stream": False}
@@ -207,22 +207,22 @@
             messages=message_dicts,
             models_priority_list=self.models_priority_list,
             run_manager=run_manager,
             **params,
         )
         return self._create_chat_result(response)
     async def _agenerate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        stream: Optional[bool] = None,
+        run_manager: AsyncCallbackManagerForLLMRun | None = None,
+        stream: bool | None = None,
         **kwargs: Any,
     ) -> ChatResult:
         should_stream = stream if stream is not None else self.streaming
         if should_stream:
             stream_iter = self._astream(
                 messages, stop=stop, run_manager=run_manager, **kwargs
             )
             return await agenerate_from_stream(stream_iter)
         message_dicts, params = self._create_message_dicts(messages, stop)
         params = {**params, **kwargs, "stream": False}

--- a/libs/community/langchain_community/chat_models/huggingface.py
+++ b//dev/null
@@ -1,131 +0,0 @@
-"""Hugging Face Chat Wrapper."""
-from typing import Any, List, Optional, Union
-from langchain_core.callbacks.manager import (
-    AsyncCallbackManagerForLLMRun,
-    CallbackManagerForLLMRun,
-)
-from langchain_core.language_models.chat_models import BaseChatModel
-from langchain_core.messages import (
-    AIMessage,
-    BaseMessage,
-    HumanMessage,
-    SystemMessage,
-)
-from langchain_core.outputs import (
-    ChatGeneration,
-    ChatResult,
-    LLMResult,
-)
-from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint
-from langchain_community.llms.huggingface_hub import HuggingFaceHub
-from langchain_community.llms.huggingface_text_gen_inference import (
-    HuggingFaceTextGenInference,
-)
-DEFAULT_SYSTEM_PROMPT = """You are a helpful, respectful, and honest assistant."""
-class ChatHuggingFace(BaseChatModel):
-    """
-    Wrapper for using Hugging Face LLM's as ChatModels.
-    Works with `HuggingFaceTextGenInference`, `HuggingFaceEndpoint`,
-    and `HuggingFaceHub` LLMs.
-    Upon instantiating this class, the model_id is resolved from the url
-    provided to the LLM, and the appropriate tokenizer is loaded from
-    the HuggingFace Hub.
-    Adapted from: https://python.langchain.com/docs/integrations/chat/llama2_chat
-    """
-    llm: Union[HuggingFaceTextGenInference, HuggingFaceEndpoint, HuggingFaceHub]
-    system_message: SystemMessage = SystemMessage(content=DEFAULT_SYSTEM_PROMPT)
-    tokenizer: Any = None
-    model_id: str = None  # type: ignore
-    def __init__(self, **kwargs: Any):
-        super().__init__(**kwargs)
-        from transformers import AutoTokenizer
-        self._resolve_model_id()
-        self.tokenizer = (
-            AutoTokenizer.from_pretrained(self.model_id)
-            if self.tokenizer is None
-            else self.tokenizer
-        )
-    def _generate(
-        self,
-        messages: List[BaseMessage],
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> ChatResult:
-        llm_input = self._to_chat_prompt(messages)
-        llm_result = self.llm._generate(
-            prompts=[llm_input], stop=stop, run_manager=run_manager, **kwargs
-        )
-        return self._to_chat_result(llm_result)
-    async def _agenerate(
-        self,
-        messages: List[BaseMessage],
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> ChatResult:
-        llm_input = self._to_chat_prompt(messages)
-        llm_result = await self.llm._agenerate(
-            prompts=[llm_input], stop=stop, run_manager=run_manager, **kwargs
-        )
-        return self._to_chat_result(llm_result)
-    def _to_chat_prompt(
-        self,
-        messages: List[BaseMessage],
-    ) -> str:
-        """Convert a list of messages into a prompt format expected by wrapped LLM."""
-        if not messages:
-            raise ValueError("at least one HumanMessage must be provided")
-        if not isinstance(messages[-1], HumanMessage):
-            raise ValueError("last message must be a HumanMessage")
-        messages_dicts = [self._to_chatml_format(m) for m in messages]
-        return self.tokenizer.apply_chat_template(
-            messages_dicts, tokenize=False, add_generation_prompt=True
-        )
-    def _to_chatml_format(self, message: BaseMessage) -> dict:
-        """Convert LangChain message to ChatML format."""
-        if isinstance(message, SystemMessage):
-            role = "system"
-        elif isinstance(message, AIMessage):
-            role = "assistant"
-        elif isinstance(message, HumanMessage):
-            role = "user"
-        else:
-            raise ValueError(f"Unknown message type: {type(message)}")
-        return {"role": role, "content": message.content}
-    @staticmethod
-    def _to_chat_result(llm_result: LLMResult) -> ChatResult:
-        chat_generations = []
-        for g in llm_result.generations[0]:
-            chat_generation = ChatGeneration(
-                message=AIMessage(content=g.text), generation_info=g.generation_info
-            )
-            chat_generations.append(chat_generation)
-        return ChatResult(
-            generations=chat_generations, llm_output=llm_result.llm_output
-        )
-    def _resolve_model_id(self) -> None:
-        """Resolve the model_id from the LLM's inference_server_url"""
-        from huggingface_hub import list_inference_endpoints
-        available_endpoints = list_inference_endpoints("*")
-        if isinstance(self.llm, HuggingFaceTextGenInference):
-            endpoint_url = self.llm.inference_server_url
-        elif isinstance(self.llm, HuggingFaceEndpoint):
-            endpoint_url = self.llm.endpoint_url
-        elif isinstance(self.llm, HuggingFaceHub):
-            self.model_id = self.llm.repo_id
-            return
-        else:
-            raise ValueError(f"Unknown LLM type: {type(self.llm)}")
-        for endpoint in available_endpoints:
-            if endpoint.url == endpoint_url:
-                self.model_id = endpoint.repository
-        if not self.model_id:
-            raise ValueError(
-                "Failed to resolve model_id"
-                f"Could not find model id for inference server provided: {endpoint_url}"
-                "Make sure that your Hugging Face token has access to the endpoint."
-            )
-    @property
-    def _llm_type(self) -> str:
-        return "huggingface-chat-wrapper"

--- a/libs/community/langchain_community/chat_models/ollama.py
+++ b/libs/community/langchain_community/chat_models/ollama.py
@@ -1,15 +1,14 @@
 import json
-from typing import Any, AsyncIterator, Dict, Iterator, List, Optional, Union
+from typing import Any, Dict, Iterator, List, Optional, Union
 from langchain_core._api import deprecated
 from langchain_core.callbacks import (
-    AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.chat_models import BaseChatModel
 from langchain_core.messages import (
     AIMessage,
     AIMessageChunk,
     BaseMessage,
     ChatMessage,
     HumanMessage,
     SystemMessage,
@@ -127,75 +126,38 @@
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> Iterator[str]:
         payload = {
             "messages": self._convert_messages_to_ollama_messages(messages),
         }
         yield from self._create_stream(
             payload=payload, stop=stop, api_url=f"{self.base_url}/api/chat/", **kwargs
         )
-    async def _acreate_chat_stream(
-        self,
-        messages: List[BaseMessage],
-        stop: Optional[List[str]] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[str]:
-        payload = {
-            "messages": self._convert_messages_to_ollama_messages(messages),
-        }
-        async for stream_resp in self._acreate_stream(
-            payload=payload, stop=stop, api_url=f"{self.base_url}/api/chat/", **kwargs
-        ):
-            yield stream_resp
     def _chat_stream_with_aggregation(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         verbose: bool = False,
         **kwargs: Any,
     ) -> ChatGenerationChunk:
         final_chunk: Optional[ChatGenerationChunk] = None
         for stream_resp in self._create_chat_stream(messages, stop, **kwargs):
             if stream_resp:
                 chunk = _chat_stream_response_to_chat_generation_chunk(stream_resp)
                 if final_chunk is None:
                     final_chunk = chunk
                 else:
                     final_chunk += chunk
                 if run_manager:
                     run_manager.on_llm_new_token(
-                        chunk.text,
-                        verbose=verbose,
-                    )
-        if final_chunk is None:
-            raise ValueError("No data received from Ollama stream.")
-        return final_chunk
-    async def _achat_stream_with_aggregation(
-        self,
-        messages: List[BaseMessage],
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        verbose: bool = False,
-        **kwargs: Any,
-    ) -> ChatGenerationChunk:
-        final_chunk: Optional[ChatGenerationChunk] = None
-        async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):
-            if stream_resp:
-                chunk = _chat_stream_response_to_chat_generation_chunk(stream_resp)
-                if final_chunk is None:
-                    final_chunk = chunk
-                else:
-                    final_chunk += chunk
-                if run_manager:
-                    await run_manager.on_llm_new_token(
                         chunk.text,
                         verbose=verbose,
                     )
         if final_chunk is None:
             raise ValueError("No data received from Ollama stream.")
         return final_chunk
     def _generate(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
@@ -219,92 +181,39 @@
             stop=stop,
             run_manager=run_manager,
             verbose=self.verbose,
             **kwargs,
         )
         chat_generation = ChatGeneration(
             message=AIMessage(content=final_chunk.text),
             generation_info=final_chunk.generation_info,
         )
         return ChatResult(generations=[chat_generation])
-    async def _agenerate(
-        self,
-        messages: List[BaseMessage],
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> ChatResult:
-        """Call out to Ollama's generate endpoint.
-        Args:
-            messages: The list of base messages to pass into the model.
-            stop: Optional list of stop words to use when generating.
-        Returns:
-            Chat generations from the model
-        Example:
-            .. code-block:: python
-                response = ollama([
-                    HumanMessage(content="Tell me about the history of AI")
-                ])
-        """
-        final_chunk = await self._achat_stream_with_aggregation(
-            messages,
-            stop=stop,
-            run_manager=run_manager,
-            verbose=self.verbose,
-            **kwargs,
-        )
-        chat_generation = ChatGeneration(
-            message=AIMessage(content=final_chunk.text),
-            generation_info=final_chunk.generation_info,
-        )
-        return ChatResult(generations=[chat_generation])
     def _stream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> Iterator[ChatGenerationChunk]:
         try:
             for stream_resp in self._create_chat_stream(messages, stop, **kwargs):
                 if stream_resp:
-                    chunk = _chat_stream_response_to_chat_generation_chunk(stream_resp)
+                    chunk = _stream_response_to_chat_generation_chunk(stream_resp)
                     yield chunk
                     if run_manager:
                         run_manager.on_llm_new_token(
                             chunk.text,
                             verbose=self.verbose,
                         )
         except OllamaEndpointNotFoundError:
             yield from self._legacy_stream(messages, stop, **kwargs)
-    async def _astream(
-        self,
-        messages: List[BaseMessage],
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[ChatGenerationChunk]:
-        try:
-            async for stream_resp in self._acreate_chat_stream(
-                messages, stop, **kwargs
-            ):
-                if stream_resp:
-                    chunk = _chat_stream_response_to_chat_generation_chunk(stream_resp)
-                    yield chunk
-                    if run_manager:
-                        await run_manager.on_llm_new_token(
-                            chunk.text,
-                            verbose=self.verbose,
-                        )
-        except OllamaEndpointNotFoundError:
-            async for chunk in self._legacy_astream(messages, stop, **kwargs):
-                yield chunk
     @deprecated("0.0.3", alternative="_stream")
     def _legacy_stream(
         self,
         messages: List[BaseMessage],
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> Iterator[ChatGenerationChunk]:
         prompt = self._format_messages_as_text(messages)
         for stream_resp in self._create_generate_stream(prompt, stop, **kwargs):

--- a/libs/community/langchain_community/chat_models/vertexai.py
+++ b/libs/community/langchain_community/chat_models/vertexai.py
@@ -1,19 +1,17 @@
 """Wrapper around Google VertexAI chat-based models."""
 from __future__ import annotations
 import base64
 import logging
 import re
 from dataclasses import dataclass, field
 from typing import TYPE_CHECKING, Any, Dict, Iterator, List, Optional, Union, cast
-from urllib.parse import urlparse
-import requests
 from langchain_core.callbacks import (
     AsyncCallbackManagerForLLMRun,
     CallbackManagerForLLMRun,
 )
 from langchain_core.language_models.chat_models import (
     BaseChatModel,
     generate_from_stream,
 )
 from langchain_core.messages import (
     AIMessage,
@@ -68,27 +66,20 @@
             vertex_messages.append(vertex_message)
         elif isinstance(message, HumanMessage):
             vertex_message = ChatMessage(content=message.content, author="user")
             vertex_messages.append(vertex_message)
         else:
             raise ValueError(
                 f"Unexpected message with type {type(message)} at the position {i}."
             )
     chat_history = _ChatHistory(context=context, history=vertex_messages)
     return chat_history
-def _is_url(s: str) -> bool:
-    try:
-        result = urlparse(s)
-        return all([result.scheme, result.netloc])
-    except Exception as e:
-        logger.debug(f"Unable to parse URL: {e}")
-        return False
 def _parse_chat_history_gemini(
     history: List[BaseMessage], project: Optional[str]
 ) -> List["Content"]:
     from vertexai.preview.generative_models import Content, Image, Part
     def _convert_to_prompt(part: Union[str, Dict]) -> Part:
         if isinstance(part, str):
             return Part.from_text(part)
         if not isinstance(part, Dict):
             raise ValueError(
                 f"Message's content is expected to be a dict, got {type(part)}!"
@@ -103,24 +94,20 @@
                 try:
                     encoded = re.search(r"data:image/\w{2,4};base64,(.*)", path).group(
                         1
                     )
                 except AttributeError:
                     raise ValueError(
                         "Invalid image uri. It should be in the format "
                         "data:image/<image_type>;base64,<base64_encoded_image>."
                     )
                 image = Image.from_bytes(base64.b64decode(encoded))
-            elif _is_url(path):
-                response = requests.get(path)
-                response.raise_for_status()
-                image = Image.from_bytes(response.content)
             else:
                 image = Image.load_from_file(path)
         else:
             raise ValueError("Only text and image_url types are supported!")
         return Part.from_image(image)
     vertex_messages = []
     for i, message in enumerate(history):
         if i == 0 and isinstance(message, SystemMessage):
             raise ValueError("SystemMessages are not yet supported!")
         elif isinstance(message, AIMessage):

--- a/libs/community/langchain_community/document_loaders/__init__.py
+++ b/libs/community/langchain_community/document_loaders/__init__.py
@@ -62,23 +62,20 @@
 from langchain_community.document_loaders.csv_loader import (
     CSVLoader,
     UnstructuredCSVLoader,
 )
 from langchain_community.document_loaders.cube_semantic import CubeSemanticLoader
 from langchain_community.document_loaders.datadog_logs import DatadogLogsLoader
 from langchain_community.document_loaders.dataframe import DataFrameLoader
 from langchain_community.document_loaders.diffbot import DiffbotLoader
 from langchain_community.document_loaders.directory import DirectoryLoader
 from langchain_community.document_loaders.discord import DiscordChatLoader
-from langchain_community.document_loaders.doc_intelligence import (
-    AzureAIDocumentIntelligenceLoader,
-)
 from langchain_community.document_loaders.docugami import DocugamiLoader
 from langchain_community.document_loaders.docusaurus import DocusaurusLoader
 from langchain_community.document_loaders.dropbox import DropboxLoader
 from langchain_community.document_loaders.duckdb_loader import DuckDBLoader
 from langchain_community.document_loaders.email import (
     OutlookMessageLoader,
     UnstructuredEmailLoader,
 )
 from langchain_community.document_loaders.epub import UnstructuredEPubLoader
 from langchain_community.document_loaders.etherscan import EtherscanLoader
@@ -230,21 +227,20 @@
     "AirbyteTypeformLoader",
     "AirbyteZendeskSupportLoader",
     "AirtableLoader",
     "AmazonTextractPDFLoader",
     "ApifyDatasetLoader",
     "ArcGISLoader",
     "ArxivLoader",
     "AssemblyAIAudioTranscriptLoader",
     "AsyncHtmlLoader",
     "AzureAIDataLoader",
-    "AzureAIDocumentIntelligenceLoader",
     "AzureBlobStorageContainerLoader",
     "AzureBlobStorageFileLoader",
     "BSHTMLLoader",
     "BibtexLoader",
     "BigQueryLoader",
     "BiliBiliLoader",
     "BlackboardLoader",
     "Blob",
     "BlobLoader",
     "BlockchainDocumentLoader",

--- a/libs/community/langchain_community/document_loaders/arxiv.py
+++ b/libs/community/langchain_community/document_loaders/arxiv.py
@@ -10,12 +10,10 @@
     """
     def __init__(
         self, query: str, doc_content_chars_max: Optional[int] = None, **kwargs: Any
     ):
         self.query = query
         self.client = ArxivAPIWrapper(
             doc_content_chars_max=doc_content_chars_max, **kwargs
         )
     def load(self) -> List[Document]:
         return self.client.load(self.query)
-    def get_summaries_as_docs(self) -> List[Document]:
-        return self.client.get_summaries_as_docs(self.query)

--- a/libs/community/langchain_community/document_loaders/directory.py
+++ b/libs/community/langchain_community/document_loaders/directory.py
@@ -46,22 +46,22 @@
             load_hidden: Whether to load hidden files. Defaults to False.
             loader_cls: Loader class to use for loading files.
               Defaults to UnstructuredFileLoader.
             loader_kwargs: Keyword arguments to pass to loader_cls. Defaults to None.
             recursive: Whether to recursively search for files. Defaults to False.
             show_progress: Whether to show a progress bar. Defaults to False.
             use_multithreading: Whether to use multithreading. Defaults to False.
             max_concurrency: The maximum number of threads to use. Defaults to 4.
             sample_size: The maximum number of files you would like to load from the
                 directory.
-            randomize_sample: Shuffle the files to get a random sample.
-            sample_seed: set the seed of the random shuffle for reproducibility.
+            randomize_sample: Suffle the files to get a random sample.
+            sample_seed: set the seed of the random shuffle for reporoducibility.
         """
         if loader_kwargs is None:
             loader_kwargs = {}
         self.path = path
         self.glob = glob
         self.load_hidden = load_hidden
         self.loader_cls = loader_cls
         self.loader_kwargs = loader_kwargs
         self.silent_errors = silent_errors
         self.recursive = recursive

--- a/libs/community/langchain_community/document_loaders/doc_intelligence.py
+++ b//dev/null
@@ -1,77 +0,0 @@
-from typing import Iterator, List, Optional
-from langchain_core.documents import Document
-from langchain_community.document_loaders.base import BaseLoader
-from langchain_community.document_loaders.blob_loaders import Blob
-from langchain_community.document_loaders.parsers import (
-    AzureAIDocumentIntelligenceParser,
-)
-class AzureAIDocumentIntelligenceLoader(BaseLoader):
-    """Loads a PDF with Azure Document Intelligence"""
-    def __init__(
-        self,
-        api_endpoint: str,
-        api_key: str,
-        file_path: Optional[str] = None,
-        url_path: Optional[str] = None,
-        api_version: Optional[str] = None,
-        api_model: str = "prebuilt-layout",
-        mode: str = "markdown",
-    ) -> None:
-        """
-        Initialize the object for file processing with Azure Document Intelligence
-        (formerly Form Recognizer).
-        This constructor initializes a AzureAIDocumentIntelligenceParser object to be
-        used for parsing files using the Azure Document Intelligence API. The load
-        method generates Documents whose content representations are determined by the
-        mode parameter.
-        Parameters:
-        -----------
-        api_endpoint: str
-            The API endpoint to use for DocumentIntelligenceClient construction.
-        api_key: str
-            The API key to use for DocumentIntelligenceClient construction.
-        file_path : Optional[str]
-            The path to the file that needs to be loaded.
-            Either file_path or url_path must be specified.
-        url_path : Optional[str]
-            The URL to the file that needs to be loaded.
-            Either file_path or url_path must be specified.
-        api_version: Optional[str]
-            The API version for DocumentIntelligenceClient. Setting None to use
-            the default value from SDK.
-        api_model: str
-            The model name or ID to be used for form recognition in Azure.
-        Examples:
-        ---------
-        >>> obj = AzureAIDocumentIntelligenceLoader(
-        ...     file_path="path/to/file",
-        ...     api_endpoint="https://endpoint.azure.com",
-        ...     api_key="APIKEY",
-        ...     api_version="2023-10-31-preview",
-        ...     model="prebuilt-document"
-        ... )
-        """
-        assert (
-            file_path is not None or url_path is not None
-        ), "file_path or url_path must be provided"
-        self.file_path = file_path
-        self.url_path = url_path
-        self.parser = AzureAIDocumentIntelligenceParser(
-            api_endpoint=api_endpoint,
-            api_key=api_key,
-            api_version=api_version,
-            api_model=api_model,
-            mode=mode,
-        )
-    def load(self) -> List[Document]:
-        """Load given path as pages."""
-        return list(self.lazy_load())
-    def lazy_load(
-        self,
-    ) -> Iterator[Document]:
-        """Lazy load given path as pages."""
-        if self.file_path is not None:
-            blob = Blob.from_path(self.file_path)
-            yield from self.parser.parse(blob)
-        else:
-            yield from self.parser.parse_url(self.url_path)

--- a/libs/community/langchain_community/document_loaders/html_bs.py
+++ b/libs/community/langchain_community/document_loaders/html_bs.py
@@ -5,21 +5,21 @@
 logger = logging.getLogger(__name__)
 class BSHTMLLoader(BaseLoader):
     """Load `HTML` files and parse them with `beautiful soup`."""
     def __init__(
         self,
         file_path: str,
         open_encoding: Union[str, None] = None,
         bs_kwargs: Union[dict, None] = None,
         get_text_separator: str = "",
     ) -> None:
-        """initialize with path, and optionally, file encoding to use, and any kwargs
+        """Initialise with path, and optionally, file encoding to use, and any kwargs
         to pass to the BeautifulSoup object.
         Args:
             file_path: The path to the file to load.
             open_encoding: The encoding to use when opening the file.
             bs_kwargs: Any kwargs to pass to the BeautifulSoup object.
             get_text_separator: The separator to use when calling get_text on the soup.
         """
         try:
             import bs4  # noqa:F401
         except ImportError:

--- a/libs/community/langchain_community/document_loaders/markdown.py
+++ b/libs/community/langchain_community/document_loaders/markdown.py
@@ -10,21 +10,21 @@
     different unstructured settings.
     Examples
     --------
     from langchain_community.document_loaders import UnstructuredMarkdownLoader
     loader = UnstructuredMarkdownLoader(
         "example.md", mode="elements", strategy="fast",
     )
     docs = loader.load()
     References
     ----------
-    https://unstructured-io.github.io/unstructured/core/partition.html#partition-md
+    https://unstructured-io.github.io/unstructured/bricks.html#partition-md
     """
     def _get_elements(self) -> List:
         from unstructured.__version__ import __version__ as __unstructured_version__
         from unstructured.partition.md import partition_md
         _unstructured_version = __unstructured_version__.split("-")[0]
         unstructured_version = tuple([int(x) for x in _unstructured_version.split(".")])
         if unstructured_version < (0, 4, 16):
             raise ValueError(
                 f"You are on unstructured version {__unstructured_version__}. "
                 "Partitioning markdown files is only supported in unstructured>=0.4.16."

--- a/libs/community/langchain_community/document_loaders/mhtml.py
+++ b/libs/community/langchain_community/document_loaders/mhtml.py
@@ -6,21 +6,21 @@
 logger = logging.getLogger(__name__)
 class MHTMLLoader(BaseLoader):
     """Parse `MHTML` files with `BeautifulSoup`."""
     def __init__(
         self,
         file_path: str,
         open_encoding: Union[str, None] = None,
         bs_kwargs: Union[dict, None] = None,
         get_text_separator: str = "",
     ) -> None:
-        """initialize with path, and optionally, file encoding to use, and any kwargs
+        """Initialise with path, and optionally, file encoding to use, and any kwargs
         to pass to the BeautifulSoup object.
         Args:
             file_path: Path to file to load.
             open_encoding: The encoding to use when opening the file.
             bs_kwargs: Any kwargs to pass to the BeautifulSoup object.
             get_text_separator: The separator to use when getting the text
                 from the soup.
         """
         try:
             import bs4  # noqa:F401

--- a/libs/community/langchain_community/document_loaders/parsers/__init__.py
+++ b/libs/community/langchain_community/document_loaders/parsers/__init__.py
@@ -1,27 +1,23 @@
 from langchain_community.document_loaders.parsers.audio import OpenAIWhisperParser
-from langchain_community.document_loaders.parsers.doc_intelligence import (
-    AzureAIDocumentIntelligenceParser,
-)
 from langchain_community.document_loaders.parsers.docai import DocAIParser
 from langchain_community.document_loaders.parsers.grobid import GrobidParser
 from langchain_community.document_loaders.parsers.html import BS4HTMLParser
 from langchain_community.document_loaders.parsers.language import LanguageParser
 from langchain_community.document_loaders.parsers.pdf import (
     PDFMinerParser,
     PDFPlumberParser,
     PyMuPDFParser,
     PyPDFium2Parser,
     PyPDFParser,
 )
 __all__ = [
-    "AzureAIDocumentIntelligenceParser",
     "BS4HTMLParser",
     "DocAIParser",
     "GrobidParser",
     "LanguageParser",
     "OpenAIWhisperParser",
     "PDFMinerParser",
     "PDFPlumberParser",
     "PyMuPDFParser",
     "PyPDFium2Parser",
     "PyPDFParser",

--- a/libs/community/langchain_community/document_loaders/parsers/doc_intelligence.py
+++ b//dev/null
@@ -1,98 +0,0 @@
-from typing import Any, Iterator, Optional
-from langchain_core.documents import Document
-from langchain_community.document_loaders.base import BaseBlobParser
-from langchain_community.document_loaders.blob_loaders import Blob
-class AzureAIDocumentIntelligenceParser(BaseBlobParser):
-    """Loads a PDF with Azure Document Intelligence
-    (formerly Forms Recognizer)."""
-    def __init__(
-        self,
-        api_endpoint: str,
-        api_key: str,
-        api_version: Optional[str] = None,
-        api_model: str = "prebuilt-layout",
-        mode: str = "markdown",
-    ):
-        from azure.ai.documentintelligence import DocumentIntelligenceClient
-        from azure.core.credentials import AzureKeyCredential
-        kwargs = {}
-        if api_version is not None:
-            kwargs["api_version"] = api_version
-        self.client = DocumentIntelligenceClient(
-            endpoint=api_endpoint,
-            credential=AzureKeyCredential(api_key),
-            headers={"x-ms-useragent": "langchain-parser/1.0.0"},
-            **kwargs,
-        )
-        self.api_model = api_model
-        self.mode = mode
-        assert self.mode in ["single", "page", "object", "markdown"]
-    def _generate_docs_page(self, result: Any) -> Iterator[Document]:
-        for p in result.pages:
-            content = " ".join([line.content for line in p.lines])
-            d = Document(
-                page_content=content,
-                metadata={
-                    "page": p.page_number,
-                },
-            )
-            yield d
-    def _generate_docs_single(self, result: Any) -> Iterator[Document]:
-        yield Document(page_content=result.content, metadata={})
-    def _generate_docs_object(self, result: Any) -> Iterator[Document]:
-        page_offset = []
-        for page in result.pages:
-            page_offset.append(page.spans[0]["offset"])
-        for para in result.paragraphs:
-            yield Document(
-                page_content=para.content,
-                metadata={
-                    "role": para.role,
-                    "page": para.bounding_regions[0].page_number,
-                    "bounding_box": para.bounding_regions[0].polygon,
-                    "type": "paragraph",
-                },
-            )
-        for table in result.tables:
-            yield Document(
-                page_content=table.cells,  # json object
-                metadata={
-                    "footnote": table.footnotes,
-                    "caption": table.caption,
-                    "page": para.bounding_regions[0].page_number,
-                    "bounding_box": para.bounding_regions[0].polygon,
-                    "row_count": table.row_count,
-                    "column_count": table.column_count,
-                    "type": "table",
-                },
-            )
-    def lazy_parse(self, blob: Blob) -> Iterator[Document]:
-        """Lazily parse the blob."""
-        with blob.as_bytes_io() as file_obj:
-            poller = self.client.begin_analyze_document(
-                self.api_model,
-                file_obj,
-                content_type="application/octet-stream",
-                output_content_format="markdown" if self.mode == "markdown" else "text",
-            )
-            result = poller.result()
-            if self.mode in ["single", "markdown"]:
-                yield from self._generate_docs_single(result)
-            elif self.mode == ["page"]:
-                yield from self._generate_docs_page(result)
-            else:
-                yield from self._generate_docs_object(result)
-    def parse_url(self, url: str) -> Iterator[Document]:
-        from azure.ai.documentintelligence.models import AnalyzeDocumentRequest
-        poller = self.client.begin_analyze_document(
-            self.api_model,
-            AnalyzeDocumentRequest(url_source=url),
-            output_content_format="markdown" if self.mode == "markdown" else "text",
-        )
-        result = poller.result()
-        if self.mode in ["single", "markdown"]:
-            yield from self._generate_docs_single(result)
-        elif self.mode == ["page"]:
-            yield from self._generate_docs_page(result)
-        else:
-            yield from self._generate_docs_object(result)

--- a/libs/community/langchain_community/document_loaders/parsers/pdf.py
+++ b/libs/community/langchain_community/document_loaders/parsers/pdf.py
@@ -445,30 +445,22 @@
             section_header_prefix="## ",
             list_element_prefix="*",
         )
         for idx, page in enumerate(document.pages):
             yield Document(
                 page_content=page.get_text(config=linearizer_config),
                 metadata={"source": blob.source, "page": idx + 1},
             )
 class DocumentIntelligenceParser(BaseBlobParser):
     """Loads a PDF with Azure Document Intelligence
-    (formerly Form Recognizer) and chunks at character level."""
+    (formerly Forms Recognizer) and chunks at character level."""
     def __init__(self, client: Any, model: str):
-        warnings.warn(
-            "langchain.document_loaders.parsers.pdf.DocumentIntelligenceParser"
-            "and langchain.document_loaders.pdf.DocumentIntelligenceLoader"
-            " are deprecated. Please upgrade to "
-            "langchain.document_loaders.DocumentIntelligenceLoader "
-            "for any file parsing purpose using Azure Document Intelligence "
-            "service."
-        )
         self.client = client
         self.model = model
     def _generate_docs(self, blob: Blob, result: Any) -> Iterator[Document]:
         for p in result.pages:
             content = " ".join([line.content for line in p.lines])
             d = Document(
                 page_content=content,
                 metadata={
                     "source": blob.source,
                     "page": p.page_number,

--- a/libs/community/langchain_community/document_loaders/rspace.py
+++ b/libs/community/langchain_community/document_loaders/rspace.py
@@ -46,21 +46,21 @@
         """Create a RSpace client."""
         try:
             from rspace_client.eln import eln, field_content
         except ImportError:
             raise ImportError("You must run " "`pip install rspace_client`")
         try:
             eln = eln.ELNClient(self.url, self.api_key)
             eln.get_status()
         except Exception:
             raise Exception(
-                f"Unable to initialize client - is url {self.url} or "
+                f"Unable to initialise client - is url {self.url} or "
                 f"api key  correct?"
             )
         return eln, field_content.FieldContent
     def _get_doc(self, cli: Any, field_content: Any, d_id: Union[str, int]) -> Document:
         content = ""
         doc = cli.get_document(d_id)
         content += f"<h2>{doc['name']}<h2/>"
         for f in doc["fields"]:
             content += f"{f['name']}\n"
             fc = field_content(f["content"])

--- a/libs/community/langchain_community/document_transformers/doctran_text_extract.py
+++ b/libs/community/langchain_community/document_transformers/doctran_text_extract.py
@@ -45,36 +45,36 @@
         openai_api_key: Optional[str] = None,
         openai_api_model: Optional[str] = None,
     ) -> None:
         self.properties = properties
         self.openai_api_key = openai_api_key or get_from_env(
             "openai_api_key", "OPENAI_API_KEY"
         )
         self.openai_api_model = openai_api_model or get_from_env(
             "openai_api_model", "OPENAI_API_MODEL"
         )
-    async def atransform_documents(
+    def transform_documents(
         self, documents: Sequence[Document], **kwargs: Any
     ) -> Sequence[Document]:
         raise NotImplementedError
-    def transform_documents(
+    async def atransform_documents(
         self, documents: Sequence[Document], **kwargs: Any
     ) -> Sequence[Document]:
         """Extracts properties from text documents using doctran."""
         try:
             from doctran import Doctran, ExtractProperty
             doctran = Doctran(
                 openai_api_key=self.openai_api_key, openai_model=self.openai_api_model
             )
         except ImportError:
             raise ImportError(
                 "Install doctran to use this parser. (pip install doctran)"
             )
         properties = [ExtractProperty(**property) for property in self.properties]
         for d in documents:
             doctran_doc = (
-                doctran.parse(content=d.page_content)
+                await doctran.parse(content=d.page_content)
                 .extract(properties=properties)
                 .execute()
             )
             d.metadata["extracted_properties"] = doctran_doc.extracted_properties
         return documents

--- a/libs/community/langchain_community/document_transformers/doctran_text_qa.py
+++ b/libs/community/langchain_community/document_transformers/doctran_text_qa.py
@@ -16,34 +16,36 @@
         self,
         openai_api_key: Optional[str] = None,
         openai_api_model: Optional[str] = None,
     ) -> None:
         self.openai_api_key = openai_api_key or get_from_env(
             "openai_api_key", "OPENAI_API_KEY"
         )
         self.openai_api_model = openai_api_model or get_from_env(
             "openai_api_model", "OPENAI_API_MODEL"
         )
-    async def atransform_documents(
+    def transform_documents(
         self, documents: Sequence[Document], **kwargs: Any
     ) -> Sequence[Document]:
         raise NotImplementedError
-    def transform_documents(
+    async def atransform_documents(
         self, documents: Sequence[Document], **kwargs: Any
     ) -> Sequence[Document]:
         """Extracts QA from text documents using doctran."""
         try:
             from doctran import Doctran
             doctran = Doctran(
                 openai_api_key=self.openai_api_key, openai_model=self.openai_api_model
             )
         except ImportError:
             raise ImportError(
                 "Install doctran to use this parser. (pip install doctran)"
             )
         for d in documents:
-            doctran_doc = doctran.parse(content=d.page_content).interrogate().execute()
+            doctran_doc = (
+                await doctran.parse(content=d.page_content).interrogate().execute()
+            )
             questions_and_answers = doctran_doc.extracted_properties.get(
                 "questions_and_answers"
             )
             d.metadata["questions_and_answers"] = questions_and_answers
         return documents

--- a/libs/community/langchain_community/document_transformers/doctran_text_translate.py
+++ b/libs/community/langchain_community/document_transformers/doctran_text_translate.py
@@ -19,37 +19,37 @@
         language: str = "english",
         openai_api_model: Optional[str] = None,
     ) -> None:
         self.openai_api_key = openai_api_key or get_from_env(
             "openai_api_key", "OPENAI_API_KEY"
         )
         self.openai_api_model = openai_api_model or get_from_env(
             "openai_api_model", "OPENAI_API_MODEL"
         )
         self.language = language
-    async def atransform_documents(
+    def transform_documents(
         self, documents: Sequence[Document], **kwargs: Any
     ) -> Sequence[Document]:
         raise NotImplementedError
-    def transform_documents(
+    async def atransform_documents(
         self, documents: Sequence[Document], **kwargs: Any
     ) -> Sequence[Document]:
         """Translates text documents using doctran."""
         try:
             from doctran import Doctran
             doctran = Doctran(
                 openai_api_key=self.openai_api_key, openai_model=self.openai_api_model
             )
         except ImportError:
             raise ImportError(
                 "Install doctran to use this parser. (pip install doctran)"
             )
         doctran_docs = [
             doctran.parse(content=doc.page_content, metadata=doc.metadata)
             for doc in documents
         ]
         for i, doc in enumerate(doctran_docs):
-            doctran_docs[i] = doc.translate(language=self.language).execute()
+            doctran_docs[i] = await doc.translate(language=self.language).execute()
         return [
             Document(page_content=doc.transformed_content, metadata=doc.metadata)
             for doc in doctran_docs
         ]

--- a/libs/community/langchain_community/embeddings/jina.py
+++ b/libs/community/langchain_community/embeddings/jina.py
@@ -1,39 +1,37 @@
 from typing import Any, Dict, List, Optional
 import requests
 from langchain_core.embeddings import Embeddings
-from langchain_core.pydantic_v1 import BaseModel, SecretStr, root_validator
-from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
+from langchain_core.pydantic_v1 import BaseModel, root_validator
+from langchain_core.utils import get_from_dict_or_env
 JINA_API_URL: str = "https://api.jina.ai/v1/embeddings"
 class JinaEmbeddings(BaseModel, Embeddings):
     """Jina embedding models."""
     session: Any  #: :meta private:
     model_name: str = "jina-embeddings-v2-base-en"
-    jina_api_key: Optional[SecretStr] = None
+    jina_api_key: Optional[str] = None
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that auth token exists in environment."""
         try:
-            jina_api_key = convert_to_secret_str(
-                get_from_dict_or_env(values, "jina_api_key", "JINA_API_KEY")
-            )
+            jina_api_key = get_from_dict_or_env(values, "jina_api_key", "JINA_API_KEY")
         except ValueError as original_exc:
             try:
-                jina_api_key = convert_to_secret_str(
-                    get_from_dict_or_env(values, "jina_auth_token", "JINA_AUTH_TOKEN")
+                jina_api_key = get_from_dict_or_env(
+                    values, "jina_auth_token", "JINA_AUTH_TOKEN"
                 )
             except ValueError:
                 raise original_exc
         session = requests.Session()
         session.headers.update(
             {
-                "Authorization": f"Bearer {jina_api_key.get_secret_value()}",
+                "Authorization": f"Bearer {jina_api_key}",
                 "Accept-Encoding": "identity",
                 "Content-type": "application/json",
             }
         )
         values["session"] = session
         return values
     def _embed(self, texts: List[str]) -> List[List[float]]:
         resp = self.session.post(  # type: ignore
             JINA_API_URL, json={"input": texts, "model": self.model_name}
         ).json()

--- a/libs/community/langchain_community/embeddings/minimax.py
+++ b/libs/community/langchain_community/embeddings/minimax.py
@@ -1,17 +1,17 @@
 from __future__ import annotations
 import logging
 from typing import Any, Callable, Dict, List, Optional
 import requests
 from langchain_core.embeddings import Embeddings
-from langchain_core.pydantic_v1 import BaseModel, Extra, SecretStr, root_validator
-from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
+from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
+from langchain_core.utils import get_from_dict_or_env
 from tenacity import (
     before_sleep_log,
     retry,
     stop_after_attempt,
     wait_exponential,
 )
 logger = logging.getLogger(__name__)
 def _create_retry_decorator() -> Callable[[Any], Any]:
     """Returns a tenacity retry decorator."""
     multiplier = 1
@@ -48,49 +48,49 @@
     endpoint_url: str = "https://api.minimax.chat/v1/embeddings"
     """Endpoint URL to use."""
     model: str = "embo-01"
     """Embeddings model name to use."""
     embed_type_db: str = "db"
     """For embed_documents"""
     embed_type_query: str = "query"
     """For embed_query"""
     minimax_group_id: Optional[str] = None
     """Group ID for MiniMax API."""
-    minimax_api_key: Optional[SecretStr] = None
+    minimax_api_key: Optional[str] = None
     """API Key for MiniMax API."""
     class Config:
         """Configuration for this pydantic object."""
         extra = Extra.forbid
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that group id and api key exists in environment."""
         minimax_group_id = get_from_dict_or_env(
             values, "minimax_group_id", "MINIMAX_GROUP_ID"
         )
-        minimax_api_key = convert_to_secret_str(
-            get_from_dict_or_env(values, "minimax_api_key", "MINIMAX_API_KEY")
+        minimax_api_key = get_from_dict_or_env(
+            values, "minimax_api_key", "MINIMAX_API_KEY"
         )
         values["minimax_group_id"] = minimax_group_id
         values["minimax_api_key"] = minimax_api_key
         return values
     def embed(
         self,
         texts: List[str],
         embed_type: str,
     ) -> List[List[float]]:
         payload = {
             "model": self.model,
             "type": embed_type,
             "texts": texts,
         }
         headers = {
-            "Authorization": f"Bearer {self.minimax_api_key.get_secret_value()}",
+            "Authorization": f"Bearer {self.minimax_api_key}",
             "Content-Type": "application/json",
         }
         params = {
             "GroupId": self.minimax_group_id,
         }
         response = requests.post(
             self.endpoint_url, params=params, headers=headers, json=payload
         )
         parsed_response = response.json()
         if parsed_response["base_resp"]["status_code"] != 0:

--- a/libs/community/langchain_community/embeddings/vertexai.py
+++ b/libs/community/langchain_community/embeddings/vertexai.py
@@ -13,40 +13,33 @@
 _MAX_TOKENS_PER_BATCH = 20000
 _MAX_BATCH_SIZE = 250
 _MIN_BATCH_SIZE = 5
 class VertexAIEmbeddings(_VertexAICommon, Embeddings):
     """Google Cloud VertexAI embedding models."""
     instance: Dict[str, Any] = {}  #: :meta private:
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validates that the python package exists in environment."""
         cls._try_init_vertexai(values)
-        if values["model_name"] == "textembedding-gecko-default":
-            logger.warning(
-                "Model_name will become a required arg for VertexAIEmbeddings "
-                "starting from Feb-01-2024. Currently the default is set to "
-                "textembedding-gecko@001"
-            )
-            values["model_name"] = "textembedding-gecko@001"
         try:
             from vertexai.language_models import TextEmbeddingModel
+            values["client"] = TextEmbeddingModel.from_pretrained(values["model_name"])
         except ImportError:
             raise_vertex_import_error()
-        values["client"] = TextEmbeddingModel.from_pretrained(values["model_name"])
         return values
     def __init__(
         self,
-        model_name: str = "textembedding-gecko-default",
         project: Optional[str] = None,
         location: str = "us-central1",
         request_parallelism: int = 5,
         max_retries: int = 6,
+        model_name: str = "textembedding-gecko",
         credentials: Optional[Any] = None,
         **kwargs: Any,
     ):
         """Initialize the sentence_transformer."""
         super().__init__(
             project=project,
             location=location,
             credentials=credentials,
             request_parallelism=request_parallelism,
             max_retries=max_retries,

--- a/libs/community/langchain_community/llms/__init__.py
+++ b/libs/community/langchain_community/llms/__init__.py
@@ -158,30 +158,20 @@
     return MlflowAIGateway
 def _import_modal() -> Any:
     from langchain_community.llms.modal import Modal
     return Modal
 def _import_mosaicml() -> Any:
     from langchain_community.llms.mosaicml import MosaicML
     return MosaicML
 def _import_nlpcloud() -> Any:
     from langchain_community.llms.nlpcloud import NLPCloud
     return NLPCloud
-def _import_oci_md_tgi() -> Any:
-    from langchain_community.llms.oci_data_science_model_deployment_endpoint import (
-        OCIModelDeploymentTGI,
-    )
-    return OCIModelDeploymentTGI
-def _import_oci_md_vllm() -> Any:
-    from langchain_community.llms.oci_data_science_model_deployment_endpoint import (
-        OCIModelDeploymentVLLM,
-    )
-    return OCIModelDeploymentVLLM
 def _import_octoai_endpoint() -> Any:
     from langchain_community.llms.octoai_endpoint import OctoAIEndpoint
     return OctoAIEndpoint
 def _import_ollama() -> Any:
     from langchain_community.llms.ollama import Ollama
     return Ollama
 def _import_opaqueprompts() -> Any:
     from langchain_community.llms.opaqueprompts import OpaquePrompts
     return OpaquePrompts
 def _import_azure_openai() -> Any:
@@ -375,24 +365,20 @@
     elif name == "Mlflow":
         return _import_mlflow()
     elif name == "MlflowAIGateway":
         return _import_mlflow_ai_gateway()
     elif name == "Modal":
         return _import_modal()
     elif name == "MosaicML":
         return _import_mosaicml()
     elif name == "NLPCloud":
         return _import_nlpcloud()
-    elif name == "OCIModelDeploymentTGI":
-        return _import_oci_md_tgi()
-    elif name == "OCIModelDeploymentVLLM":
-        return _import_oci_md_vllm()
     elif name == "OctoAIEndpoint":
         return _import_octoai_endpoint()
     elif name == "Ollama":
         return _import_ollama()
     elif name == "OpaquePrompts":
         return _import_opaqueprompts()
     elif name == "AzureOpenAI":
         return _import_azure_openai()
     elif name == "OpenAI":
         return _import_openai()
@@ -507,22 +493,20 @@
     "LlamaCpp",
     "TextGen",
     "ManifestWrapper",
     "Minimax",
     "MlflowAIGateway",
     "Modal",
     "MosaicML",
     "Nebula",
     "NIBittensorLLM",
     "NLPCloud",
-    "OCIModelDeploymentTGI",
-    "OCIModelDeploymentVLLM",
     "Ollama",
     "OpenAI",
     "OpenAIChat",
     "OpenLLM",
     "OpenLM",
     "PaiEasEndpoint",
     "Petals",
     "PipelineAI",
     "Predibase",
     "PredictionGuard",
@@ -594,22 +578,20 @@
         "textgen": _import_textgen,
         "minimax": _import_minimax,
         "mlflow": _import_mlflow,
         "mlflow-chat": _import_mlflow_chat,
         "mlflow-ai-gateway": _import_mlflow_ai_gateway,
         "modal": _import_modal,
         "mosaic": _import_mosaicml,
         "nebula": _import_symblai_nebula,
         "nibittensor": _import_bittensor,
         "nlpcloud": _import_nlpcloud,
-        "oci_model_deployment_tgi_endpoint": _import_oci_md_tgi,
-        "oci_model_deployment_vllm_endpoint": _import_oci_md_vllm,
         "ollama": _import_ollama,
         "openai": _import_openai,
         "openlm": _import_openlm,
         "pai_eas_endpoint": _import_pai_eas_endpoint,
         "petals": _import_petals,
         "pipelineai": _import_pipelineai,
         "predibase": _import_predibase,
         "opaqueprompts": _import_opaqueprompts,
         "replicate": _import_replicate,
         "rwkv": _import_rwkv,

--- a/libs/community/langchain_community/llms/baseten.py
+++ b/libs/community/langchain_community/llms/baseten.py
@@ -1,71 +1,59 @@
 import logging
-import os
 from typing import Any, Dict, List, Mapping, Optional
-import requests
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import LLM
 from langchain_core.pydantic_v1 import Field
 logger = logging.getLogger(__name__)
 class Baseten(LLM):
-    """Baseten model
-    This module allows using LLMs hosted on Baseten.
-    The LLM deployed on Baseten must have the following properties:
-    * Must accept input as a dictionary with the key "prompt"
-    * May accept other input in the dictionary passed through with kwargs
-    * Must return a string with the model output
-    To use this module, you must:
-    * Export your Baseten API key as the environment variable `BASETEN_API_KEY`
-    * Get the model ID for your model from your Baseten dashboard
-    * Identify the model deployment ("production" for all model library models)
-    These code samples use
-    [Mistral 7B Instruct](https://app.baseten.co/explore/mistral_7b_instruct)
-    from Baseten's model library.
-    Examples:
+    """Baseten models.
+    To use, you should have the ``baseten`` python package installed,
+    and run ``baseten.login()`` with your Baseten API key.
+    The required ``model`` param can be either a model id or model
+    version id. Using a model version ID will result in
+    slightly faster invocation.
+    Any other model parameters can also
+    be passed in with the format input={model_param: value, ...}
+    The Baseten model must accept a dictionary of input with the key
+    "prompt" and return a dictionary with a key "data" which maps
+    to a list of response strings.
+    Example:
         .. code-block:: python
             from langchain_community.llms import Baseten
-            mistral = Baseten(model="MODEL_ID", deployment="production")
-            mistral("What is the Mistral wind?")
-        .. code-block:: python
-            from langchain_community.llms import Baseten
-            mistral = Baseten(model="MODEL_ID", deployment="development")
-            mistral("What is the Mistral wind?")
-        .. code-block:: python
-            from langchain_community.llms import Baseten
-            mistral = Baseten(model="MODEL_ID", deployment="DEPLOYMENT_ID")
-            mistral("What is the Mistral wind?")
+            my_model = Baseten(model="MODEL_ID")
+            output = my_model("prompt")
     """
     model: str
-    deployment: str
     input: Dict[str, Any] = Field(default_factory=dict)
     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
     @property
     def _identifying_params(self) -> Mapping[str, Any]:
         """Get the identifying parameters."""
         return {
             **{"model_kwargs": self.model_kwargs},
         }
     @property
     def _llm_type(self) -> str:
         """Return type of model."""
         return "baseten"
     def _call(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> str:
-        baseten_api_key = os.environ["BASETEN_API_KEY"]
-        model_id = self.model
-        if self.deployment == "production":
-            model_url = f"https://model-{model_id}.api.baseten.co/production/predict"
-        elif self.deployment == "development":
-            model_url = f"https://model-{model_id}.api.baseten.co/development/predict"
-        else:  # try specific deployment ID
-            model_url = f"https://model-{model_id}.api.baseten.co/deployment/{self.deployment}/predict"
-        response = requests.post(
-            model_url,
-            headers={"Authorization": f"Api-Key {baseten_api_key}"},
-            json={"prompt": prompt, **kwargs},
-        )
-        return response.json()
+        """Call to Baseten deployed model endpoint."""
+        try:
+            import baseten
+        except ImportError as exc:
+            raise ImportError(
+                "Could not import Baseten Python package. "
+                "Please install it with `pip install baseten`."
+            ) from exc
+        try:
+            model = baseten.deployed_model_version_id(self.model)
+            response = model.predict({"prompt": prompt, **kwargs})
+        except baseten.common.core.ApiError:
+            model = baseten.deployed_model_id(self.model)
+            response = model.predict({"prompt": prompt, **kwargs})
+        return "".join(response)

--- a/libs/community/langchain_community/llms/bedrock.py
+++ b/libs/community/langchain_community/llms/bedrock.py
@@ -1,27 +1,24 @@
-from __future__ import annotations
 import json
 import warnings
 from abc import ABC
-from typing import TYPE_CHECKING, Any, Dict, Iterator, List, Mapping, Optional
+from typing import Any, Dict, Iterator, List, Mapping, Optional
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import LLM
 from langchain_core.outputs import GenerationChunk
 from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
 from langchain_core.utils import get_from_dict_or_env
 from langchain_community.llms.utils import enforce_stop_tokens
 from langchain_community.utilities.anthropic import (
     get_num_tokens_anthropic,
     get_token_ids_anthropic,
 )
-if TYPE_CHECKING:
-    from botocore.config import Config
 HUMAN_PROMPT = "\n\nHuman:"
 ASSISTANT_PROMPT = "\n\nAssistant:"
 ALTERNATION_ERROR = (
     "Error: Prompt must alternate between '\n\nHuman:' and '\n\nAssistant:'."
 )
 def _add_newlines_before_ha(input_text: str) -> str:
     new_text = input_text
     for word in ["Human:", "Assistant:"]:
         new_text = new_text.replace(word, "\n\n" + word)
         for i in range(2):
@@ -128,22 +125,20 @@
     """The aws region e.g., `us-west-2`. Fallsback to AWS_DEFAULT_REGION env variable
     or region specified in ~/.aws/config in case it is not provided here.
     """
     credentials_profile_name: Optional[str] = Field(default=None, exclude=True)
     """The name of the profile in the ~/.aws/credentials or ~/.aws/config files, which
     has either access keys or role information specified.
     If not specified, the default credential profile or, if on an EC2 instance,
     credentials from IMDS will be used.
     See: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html
     """
-    config: Optional[Config] = None
-    """An optional botocore.config.Config instance to pass to the client."""
     model_id: str
     """Id of the model to call, e.g., amazon.titan-text-express-v1, this is
     equivalent to the modelId property in the list-foundation-models api"""
     model_kwargs: Optional[Dict] = None
     """Keyword arguments to pass to the model."""
     endpoint_url: Optional[str] = None
     """Needed if you don't want to default to us-east-1 endpoint"""
     streaming: bool = False
     """Whether to stream the results."""
     provider_stop_sequence_key_name_map: Mapping[str, str] = {
@@ -167,22 +162,20 @@
                 values,
                 "region_name",
                 "AWS_DEFAULT_REGION",
                 default=session.region_name,
             )
             client_params = {}
             if values["region_name"]:
                 client_params["region_name"] = values["region_name"]
             if values["endpoint_url"]:
                 client_params["endpoint_url"] = values["endpoint_url"]
-            if values["config"]:
-                client_params["config"] = values["config"]
             values["client"] = session.client("bedrock-runtime", **client_params)
         except ImportError:
             raise ModuleNotFoundError(
                 "Could not import boto3 python package. "
                 "Please install it with `pip install boto3`."
             )
         except Exception as e:
             raise ValueError(
                 "Could not load credentials to authenticate with AWS client. "
                 "Please check that credentials in the specified "

--- a/libs/community/langchain_community/llms/databricks.py
+++ b/libs/community/langchain_community/llms/databricks.py
@@ -33,22 +33,20 @@
     @abstractmethod
     def post(
         self, request: Any, transform_output_fn: Optional[Callable[..., str]] = None
     ) -> Any:
         ...
     @property
     def llm(self) -> bool:
         return False
 def _transform_completions(response: Dict[str, Any]) -> str:
     return response["choices"][0]["text"]
-def _transform_llama2_chat(response: Dict[str, Any]) -> str:
-    return response["candidates"][0]["text"]
 def _transform_chat(response: Dict[str, Any]) -> str:
     return response["choices"][0]["message"]["content"]
 class _DatabricksServingEndpointClient(_DatabricksClientBase):
     """An API client that talks to a Databricks serving endpoint."""
     host: str
     endpoint_name: str
     databricks_uri: str
     client: Any = None
     external_or_foundation: bool = False
     task: Optional[str] = None
@@ -60,25 +58,24 @@
         except ImportError as e:
             raise ImportError(
                 "Failed to create the client. "
                 "Please install mlflow with `pip install mlflow`."
             ) from e
         endpoint = self.client.get_endpoint(self.endpoint_name)
         self.external_or_foundation = endpoint.get("endpoint_type", "").lower() in (
             "external_model",
             "foundation_model_api",
         )
-        if self.task is None:
-            self.task = endpoint.get("task")
+        self.task = endpoint.get("task")
     @property
     def llm(self) -> bool:
-        return self.task in ("llm/v1/chat", "llm/v1/completions", "llama2/chat")
+        return self.task in ("llm/v1/chat", "llm/v1/completions")
     @root_validator(pre=True)
     def set_api_url(cls, values: Dict[str, Any]) -> Dict[str, Any]:
         if "api_url" not in values:
             host = values["host"]
             endpoint_name = values["endpoint_name"]
             api_url = f"https://{host}/serving-endpoints/{endpoint_name}/invocations"
             values["api_url"] = api_url
         return values
     def post(
         self, request: Any, transform_output_fn: Optional[Callable[..., str]] = None
@@ -92,22 +89,20 @@
             elif self.task == "llm/v1/completions":
                 return _transform_completions(resp)
             return resp
         else:
             wrapped_request = {"dataframe_records": [request]}
             response = self.client.predict(
                 endpoint=self.endpoint_name, inputs=wrapped_request
             )
             preds = response["predictions"]
             pred = preds[0] if isinstance(preds, list) else preds
-            if self.task == "llama2/chat":
-                return _transform_llama2_chat(pred)
             return transform_output_fn(pred) if transform_output_fn else pred
 class _DatabricksClusterDriverProxyClient(_DatabricksClientBase):
     """An API client that talks to a Databricks cluster driver proxy app."""
     host: str
     cluster_id: str
     cluster_driver_port: str
     @root_validator(pre=True)
     def set_api_url(cls, values: Dict[str, Any]) -> Dict[str, Any]:
         if "api_url" not in values:
             host = values["host"]
@@ -253,24 +248,20 @@
     temperature: float = 0.0
     """The sampling temperature."""
     n: int = 1
     """The number of completion choices to generate."""
     stop: Optional[List[str]] = None
     """The stop sequence."""
     max_tokens: Optional[int] = None
     """The maximum number of tokens to generate."""
     extra_params: Dict[str, Any] = Field(default_factory=dict)
     """Any extra parameters to pass to the endpoint."""
-    task: Optional[str] = None
-    """The task of the endpoint. Only used when using a serving endpoint.
-    If not provided, the task is automatically inferred from the endpoint.
-    """
     _client: _DatabricksClientBase = PrivateAttr()
     class Config:
         extra = Extra.forbid
         underscore_attrs_are_private = True
     @property
     def _llm_params(self) -> Dict[str, Any]:
         params: Dict[str, Any] = {
             "temperature": self.temperature,
             "n": self.n,
         }
@@ -326,21 +317,20 @@
             warnings.warn(
                 "model_kwargs is deprecated. Please use extra_params instead.",
                 DeprecationWarning,
             )
         if self.endpoint_name:
             self._client = _DatabricksServingEndpointClient(
                 host=self.host,
                 api_token=self.api_token,
                 endpoint_name=self.endpoint_name,
                 databricks_uri=self.databricks_uri,
-                task=self.task,
             )
         elif self.cluster_id and self.cluster_driver_port:
             self._client = _DatabricksClusterDriverProxyClient(
                 host=self.host,
                 api_token=self.api_token,
                 cluster_id=self.cluster_id,
                 cluster_driver_port=self.cluster_driver_port,
             )
         else:
             raise ValueError(
@@ -354,21 +344,20 @@
             "endpoint_name": self.endpoint_name,
             "cluster_id": self.cluster_id,
             "cluster_driver_port": self.cluster_driver_port,
             "databricks_uri": self.databricks_uri,
             "model_kwargs": self.model_kwargs,
             "temperature": self.temperature,
             "n": self.n,
             "stop": self.stop,
             "max_tokens": self.max_tokens,
             "extra_params": self.extra_params,
-            "task": self.task,
         }
     @property
     def _identifying_params(self) -> Mapping[str, Any]:
         return self._default_params
     @property
     def _llm_type(self) -> str:
         """Return type of llm."""
         return "databricks"
     def _call(
         self,

--- a/libs/community/langchain_community/llms/deepsparse.py
+++ b/libs/community/langchain_community/llms/deepsparse.py
@@ -45,21 +45,21 @@
         """Return type of llm."""
         return "deepsparse"
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that ``deepsparse`` package is installed."""
         try:
             from deepsparse import Pipeline
         except ImportError:
             raise ImportError(
                 "Could not import `deepsparse` package. "
-                "Please install it with `pip install deepsparse[llm]`"
+                "Please install it with `pip install deepsparse`"
             )
         model_config = values["model_config"] or {}
         values["pipeline"] = Pipeline.create(
             task="text_generation",
             model_path=values["model"],
             **model_config,
         )
         return values
     def _call(
         self,
@@ -82,21 +82,23 @@
         """
         if self.streaming:
             combined_output = ""
             for chunk in self._stream(
                 prompt=prompt, stop=stop, run_manager=run_manager, **kwargs
             ):
                 combined_output += chunk.text
             text = combined_output
         else:
             text = (
-                self.pipeline(sequences=prompt, **self.generation_config)
+                self.pipeline(
+                    sequences=prompt, generation_config=self.generation_config
+                )
                 .generations[0]
                 .text
             )
         if stop is not None:
             text = enforce_stop_tokens(text, stop)
         return text
     async def _acall(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
@@ -117,21 +119,23 @@
         """
         if self.streaming:
             combined_output = ""
             async for chunk in self._astream(
                 prompt=prompt, stop=stop, run_manager=run_manager, **kwargs
             ):
                 combined_output += chunk.text
             text = combined_output
         else:
             text = (
-                self.pipeline(sequences=prompt, **self.generation_config)
+                self.pipeline(
+                    sequences=prompt, generation_config=self.generation_config
+                )
                 .generations[0]
                 .text
             )
         if stop is not None:
             text = enforce_stop_tokens(text, stop)
         return text
     def _stream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
@@ -153,21 +157,21 @@
                 from langchain_community.llms import DeepSparse
                 llm = DeepSparse(
                     model="zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base_quant-none",
                     streaming=True
                 )
                 for chunk in llm.stream("Tell me a joke",
                         stop=["'","\n"]):
                     print(chunk, end='', flush=True)
         """
         inference = self.pipeline(
-            sequences=prompt, streaming=True, **self.generation_config
+            sequences=prompt, generation_config=self.generation_config, streaming=True
         )
         for token in inference:
             chunk = GenerationChunk(text=token.generations[0].text)
             yield chunk
             if run_manager:
                 run_manager.on_llm_new_token(token=chunk.text)
     async def _astream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
@@ -189,17 +193,17 @@
                 from langchain_community.llms import DeepSparse
                 llm = DeepSparse(
                     model="zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base_quant-none",
                     streaming=True
                 )
                 for chunk in llm.stream("Tell me a joke",
                         stop=["'","\n"]):
                     print(chunk, end='', flush=True)
         """
         inference = self.pipeline(
-            sequences=prompt, streaming=True, **self.generation_config
+            sequences=prompt, generation_config=self.generation_config, streaming=True
         )
         for token in inference:
             chunk = GenerationChunk(text=token.generations[0].text)
             yield chunk
             if run_manager:
                 await run_manager.on_llm_new_token(token=chunk.text)

--- a/libs/community/langchain_community/llms/oci_data_science_model_deployment_endpoint.py
+++ b//dev/null
@@ -1,289 +0,0 @@
-import logging
-from typing import Any, Dict, List, Optional
-import requests
-from langchain_core.callbacks import CallbackManagerForLLMRun
-from langchain_core.language_models.llms import LLM
-from langchain_core.pydantic_v1 import Field, root_validator
-from langchain_core.utils import get_from_dict_or_env
-logger = logging.getLogger(__name__)
-DEFAULT_TIME_OUT = 300
-DEFAULT_CONTENT_TYPE_JSON = "application/json"
-class OCIModelDeploymentLLM(LLM):
-    """Base class for LLM deployed on OCI Data Science Model Deployment."""
-    auth: dict = Field(default_factory=dict, exclude=True)
-    """ADS auth dictionary for OCI authentication:
-    https://accelerated-data-science.readthedocs.io/en/latest/user_guide/cli/authentication.html.
-    This can be generated by calling `ads.common.auth.api_keys()`
-    or `ads.common.auth.resource_principal()`. If this is not
-    provided then the `ads.common.default_signer()` will be used."""
-    max_tokens: int = 256
-    """Denotes the number of tokens to predict per generation."""
-    temperature: float = 0.2
-    """A non-negative float that tunes the degree of randomness in generation."""
-    k: int = 0
-    """Number of most likely tokens to consider at each step."""
-    p: float = 0.75
-    """Total probability mass of tokens to consider at each step."""
-    endpoint: str = ""
-    """The uri of the endpoint from the deployed Model Deployment model."""
-    best_of: int = 1
-    """Generates best_of completions server-side and returns the "best"
-    (the one with the highest log probability per token).
-    """
-    stop: Optional[List[str]] = None
-    """Stop words to use when generating. Model output is cut off
-    at the first occurrence of any of these substrings."""
-    @root_validator()
-    def validate_environment(  # pylint: disable=no-self-argument
-        cls, values: Dict
-    ) -> Dict:
-        """Validate that python package exists in environment."""
-        try:
-            import ads
-        except ImportError as ex:
-            raise ImportError(
-                "Could not import ads python package. "
-                "Please install it with `pip install oracle_ads`."
-            ) from ex
-        if not values.get("auth", None):
-            values["auth"] = ads.common.auth.default_signer()
-        values["endpoint"] = get_from_dict_or_env(
-            values,
-            "endpoint",
-            "OCI_LLM_ENDPOINT",
-        )
-        return values
-    @property
-    def _default_params(self) -> Dict[str, Any]:
-        """Default parameters for the model."""
-        raise NotImplementedError
-    @property
-    def _identifying_params(self) -> Dict[str, Any]:
-        """Get the identifying parameters."""
-        return {
-            **{"endpoint": self.endpoint},
-            **self._default_params,
-        }
-    def _construct_json_body(self, prompt: str, params: dict) -> dict:
-        """Constructs the request body as a dictionary (JSON)."""
-        raise NotImplementedError
-    def _invocation_params(self, stop: Optional[List[str]], **kwargs: Any) -> dict:
-        """Combines the invocation parameters with default parameters."""
-        params = self._default_params
-        if self.stop is not None and stop is not None:
-            raise ValueError("`stop` found in both the input and default params.")
-        elif self.stop is not None:
-            params["stop"] = self.stop
-        elif stop is not None:
-            params["stop"] = stop
-        else:
-            params["stop"] = []
-        return {**params, **kwargs}
-    def _process_response(self, response_json: dict) -> str:
-        raise NotImplementedError
-    def _call(
-        self,
-        prompt: str,
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> str:
-        """Call out to OCI Data Science Model Deployment endpoint.
-        Args:
-            prompt (str):
-                The prompt to pass into the model.
-            stop (List[str], Optional):
-                List of stop words to use when generating.
-            kwargs:
-                requests_kwargs:
-                    Additional ``**kwargs`` to pass to requests.post
-        Returns:
-            The string generated by the model.
-        Example:
-            .. code-block:: python
-                response = oci_md("Tell me a joke.")
-        """
-        requests_kwargs = kwargs.pop("requests_kwargs", {})
-        params = self._invocation_params(stop, **kwargs)
-        body = self._construct_json_body(prompt, params)
-        logger.info(f"LLM API Request:\n{prompt}")
-        response = self._send_request(
-            data=body, endpoint=self.endpoint, **requests_kwargs
-        )
-        completion = self._process_response(response)
-        logger.info(f"LLM API Completion:\n{completion}")
-        return completion
-    def _send_request(
-        self,
-        data: Any,
-        endpoint: str,
-        header: Optional[dict] = {},
-        **kwargs: Any,
-    ) -> Dict:
-        """Sends request to the oci data science model deployment endpoint.
-        Args:
-            data (Json serializable):
-                data need to be sent to the endpoint.
-            endpoint (str):
-                The model HTTP endpoint.
-            header (dict, optional):
-                A dictionary of HTTP headers to send to the specified url.
-                Defaults to {}.
-            kwargs:
-                Additional ``**kwargs`` to pass to requests.post.
-        Raises:
-            Exception:
-                Raise when invoking fails.
-        Returns:
-            A JSON representation of a requests.Response object.
-        """
-        if not header:
-            header = {}
-        header["Content-Type"] = (
-            header.pop("content_type", DEFAULT_CONTENT_TYPE_JSON)
-            or DEFAULT_CONTENT_TYPE_JSON
-        )
-        request_kwargs = {"json": data}
-        request_kwargs["headers"] = header
-        timeout = kwargs.pop("timeout", DEFAULT_TIME_OUT)
-        attempts = 0
-        while attempts < 2:
-            request_kwargs["auth"] = self.auth.get("signer")
-            response = requests.post(
-                endpoint, timeout=timeout, **request_kwargs, **kwargs
-            )
-            if response.status_code == 401:
-                self._refresh_signer()
-                attempts += 1
-                continue
-            break
-        try:
-            response.raise_for_status()
-            response_json = response.json()
-        except Exception:
-            logger.error(
-                "DEBUG INFO: request_kwargs=%s, status_code=%s, content=%s",
-                request_kwargs,
-                response.status_code,
-                response.content,
-            )
-            raise
-        return response_json
-    def _refresh_signer(self) -> None:
-        if self.auth.get("signer", None) and hasattr(
-            self.auth["signer"], "refresh_security_token"
-        ):
-            self.auth["signer"].refresh_security_token()
-class OCIModelDeploymentTGI(OCIModelDeploymentLLM):
-    """OCI Data Science Model Deployment TGI Endpoint.
-    To use, you must provide the model HTTP endpoint from your deployed
-    model, e.g. https://<MD_OCID>/predict.
-    To authenticate, `oracle-ads` has been used to automatically load
-    credentials: https://accelerated-data-science.readthedocs.io/en/latest/user_guide/cli/authentication.html
-    Make sure to have the required policies to access the OCI Data
-    Science Model Deployment endpoint. See:
-    https://docs.oracle.com/en-us/iaas/data-science/using/model-dep-policies-auth.htm#model_dep_policies_auth__predict-endpoint
-    Example:
-        .. code-block:: python
-            from langchain.llms import ModelDeploymentTGI
-            oci_md = ModelDeploymentTGI(endpoint="https://<MD_OCID>/predict")
-    """
-    do_sample: bool = True
-    """If set to True, this parameter enables decoding strategies such as
-    multi-nominal sampling, beam-search multi-nominal sampling, Top-K
-    sampling and Top-p sampling.
-    """
-    watermark = True
-    """Watermarking with `A Watermark for Large Language Models <https://arxiv.org/abs/2301.10226>`_.
-    Defaults to True."""
-    return_full_text = False
-    """Whether to prepend the prompt to the generated text. Defaults to False."""
-    @property
-    def _llm_type(self) -> str:
-        """Return type of llm."""
-        return "oci_model_deployment_tgi_endpoint"
-    @property
-    def _default_params(self) -> Dict[str, Any]:
-        """Get the default parameters for invoking OCI model deployment TGI endpoint."""
-        return {
-            "best_of": self.best_of,
-            "max_new_tokens": self.max_tokens,
-            "temperature": self.temperature,
-            "top_k": self.k
-            if self.k > 0
-            else None,  # `top_k` must be strictly positive'
-            "top_p": self.p,
-            "do_sample": self.do_sample,
-            "return_full_text": self.return_full_text,
-            "watermark": self.watermark,
-        }
-    def _construct_json_body(self, prompt: str, params: dict) -> dict:
-        return {
-            "inputs": prompt,
-            "parameters": params,
-        }
-    def _process_response(self, response_json: dict) -> str:
-        return str(response_json.get("generated_text", response_json)) + "\n"
-class OCIModelDeploymentVLLM(OCIModelDeploymentLLM):
-    """VLLM deployed on OCI Data Science Model Deployment
-    To use, you must provide the model HTTP endpoint from your deployed
-    model, e.g. https://<MD_OCID>/predict.
-    To authenticate, `oracle-ads` has been used to automatically load
-    credentials: https://accelerated-data-science.readthedocs.io/en/latest/user_guide/cli/authentication.html
-    Make sure to have the required policies to access the OCI Data
-    Science Model Deployment endpoint. See:
-    https://docs.oracle.com/en-us/iaas/data-science/using/model-dep-policies-auth.htm#model_dep_policies_auth__predict-endpoint
-    Example:
-        .. code-block:: python
-            from langchain.llms import OCIModelDeploymentVLLM
-            oci_md = OCIModelDeploymentVLLM(
-                endpoint="https://<MD_OCID>/predict",
-                model="mymodel"
-            )
-    """
-    model: str
-    """The name of the model."""
-    n: int = 1
-    """Number of output sequences to return for the given prompt."""
-    k: int = -1
-    """Number of most likely tokens to consider at each step."""
-    frequency_penalty: float = 0.0
-    """Penalizes repeated tokens according to frequency. Between 0 and 1."""
-    presence_penalty: float = 0.0
-    """Penalizes repeated tokens. Between 0 and 1."""
-    use_beam_search: bool = False
-    """Whether to use beam search instead of sampling."""
-    ignore_eos: bool = False
-    """Whether to ignore the EOS token and continue generating tokens after
-    the EOS token is generated."""
-    logprobs: Optional[int] = None
-    """Number of log probabilities to return per output token."""
-    @property
-    def _llm_type(self) -> str:
-        """Return type of llm."""
-        return "oci_model_deployment_vllm_endpoint"
-    @property
-    def _default_params(self) -> Dict[str, Any]:
-        """Get the default parameters for calling vllm."""
-        return {
-            "best_of": self.best_of,
-            "frequency_penalty": self.frequency_penalty,
-            "ignore_eos": self.ignore_eos,
-            "logprobs": self.logprobs,
-            "max_tokens": self.max_tokens,
-            "model": self.model,
-            "n": self.n,
-            "presence_penalty": self.presence_penalty,
-            "stop": self.stop,
-            "temperature": self.temperature,
-            "top_k": self.k,
-            "top_p": self.p,
-            "use_beam_search": self.use_beam_search,
-        }
-    def _construct_json_body(self, prompt: str, params: dict) -> dict:
-        return {
-            "prompt": prompt,
-            **params,
-        }
-    def _process_response(self, response_json: dict) -> str:
-        return response_json["choices"][0]["text"]

--- a/libs/community/langchain_community/llms/ollama.py
+++ b/libs/community/langchain_community/llms/ollama.py
@@ -1,18 +1,14 @@
 import json
-from typing import Any, AsyncIterator, Dict, Iterator, List, Mapping, Optional
-import aiohttp
+from typing import Any, Dict, Iterator, List, Mapping, Optional
 import requests
-from langchain_core.callbacks import (
-    AsyncCallbackManagerForLLMRun,
-    CallbackManagerForLLMRun,
-)
+from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models import BaseLanguageModel
 from langchain_core.language_models.llms import BaseLLM
 from langchain_core.outputs import GenerationChunk, LLMResult
 from langchain_core.pydantic_v1 import Extra
 def _stream_response_to_generation_chunk(
     stream_response: str,
 ) -> GenerationChunk:
     """Convert a stream response to a generation chunk."""
     parsed_response = json.loads(stream_response)
     generation_info = parsed_response if parsed_response.get("done") is True else None
@@ -116,35 +112,20 @@
         images: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> Iterator[str]:
         payload = {"prompt": prompt, "images": images}
         yield from self._create_stream(
             payload=payload,
             stop=stop,
             api_url=f"{self.base_url}/api/generate/",
             **kwargs,
         )
-    async def _acreate_generate_stream(
-        self,
-        prompt: str,
-        stop: Optional[List[str]] = None,
-        images: Optional[List[str]] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[str]:
-        payload = {"prompt": prompt, "images": images}
-        async for item in self._acreate_stream(
-            payload=payload,
-            stop=stop,
-            api_url=f"{self.base_url}/api/generate/",
-            **kwargs,
-        ):
-            yield item
     def _create_stream(
         self,
         api_url: str,
         payload: Any,
         stop: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> Iterator[str]:
         if self.stop is not None and stop is not None:
             raise ValueError("`stop` found in both the input and default params.")
         elif self.stop is not None:
@@ -174,125 +155,47 @@
             url=api_url,
             headers={"Content-Type": "application/json"},
             json=request_payload,
             stream=True,
             timeout=self.timeout,
         )
         response.encoding = "utf-8"
         if response.status_code != 200:
             if response.status_code == 404:
                 raise OllamaEndpointNotFoundError(
-                    "Ollama call failed with status code 404. "
-                    "Maybe your model is not found "
-                    f"and you should pull the model with `ollama pull {self.model}`."
+                    "Ollama call failed with status code 404."
                 )
             else:
                 optional_detail = response.json().get("error")
                 raise ValueError(
                     f"Ollama call failed with status code {response.status_code}."
                     f" Details: {optional_detail}"
                 )
         return response.iter_lines(decode_unicode=True)
-    async def _acreate_stream(
-        self,
-        api_url: str,
-        payload: Any,
-        stop: Optional[List[str]] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[str]:
-        if self.stop is not None and stop is not None:
-            raise ValueError("`stop` found in both the input and default params.")
-        elif self.stop is not None:
-            stop = self.stop
-        elif stop is None:
-            stop = []
-        params = self._default_params
-        if "model" in kwargs:
-            params["model"] = kwargs["model"]
-        if "options" in kwargs:
-            params["options"] = kwargs["options"]
-        else:
-            params["options"] = {
-                **params["options"],
-                "stop": stop,
-                **kwargs,
-            }
-        if payload.get("messages"):
-            request_payload = {"messages": payload.get("messages", []), **params}
-        else:
-            request_payload = {
-                "prompt": payload.get("prompt"),
-                "images": payload.get("images", []),
-                **params,
-            }
-        async with aiohttp.ClientSession() as session:
-            async with session.post(
-                url=api_url,
-                headers={"Content-Type": "application/json"},
-                json=request_payload,
-                timeout=self.timeout,
-            ) as response:
-                if response.status != 200:
-                    if response.status == 404:
-                        raise OllamaEndpointNotFoundError(
-                            "Ollama call failed with status code 404."
-                        )
-                    else:
-                        optional_detail = await response.json().get("error")
-                        raise ValueError(
-                            f"Ollama call failed with status code {response.status}."
-                            f" Details: {optional_detail}"
-                        )
-                async for line in response.content:
-                    yield line.decode("utf-8")
     def _stream_with_aggregation(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         verbose: bool = False,
         **kwargs: Any,
     ) -> GenerationChunk:
         final_chunk: Optional[GenerationChunk] = None
         for stream_resp in self._create_generate_stream(prompt, stop, **kwargs):
             if stream_resp:
                 chunk = _stream_response_to_generation_chunk(stream_resp)
                 if final_chunk is None:
                     final_chunk = chunk
                 else:
                     final_chunk += chunk
                 if run_manager:
                     run_manager.on_llm_new_token(
-                        chunk.text,
-                        verbose=verbose,
-                    )
-        if final_chunk is None:
-            raise ValueError("No data received from Ollama stream.")
-        return final_chunk
-    async def _astream_with_aggregation(
-        self,
-        prompt: str,
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        verbose: bool = False,
-        **kwargs: Any,
-    ) -> GenerationChunk:
-        final_chunk: Optional[GenerationChunk] = None
-        async for stream_resp in self._acreate_generate_stream(prompt, stop, **kwargs):
-            if stream_resp:
-                chunk = _stream_response_to_generation_chunk(stream_resp)
-                if final_chunk is None:
-                    final_chunk = chunk
-                else:
-                    final_chunk += chunk
-                if run_manager:
-                    await run_manager.on_llm_new_token(
                         chunk.text,
                         verbose=verbose,
                     )
         if final_chunk is None:
             raise ValueError("No data received from Ollama stream.")
         return final_chunk
 class Ollama(BaseLLM, _OllamaCommon):
     """Ollama locally runs large language models.
     To use, follow the instructions at https://ollama.ai/.
     Example:
@@ -330,72 +233,26 @@
             final_chunk = super()._stream_with_aggregation(
                 prompt,
                 stop=stop,
                 images=images,
                 run_manager=run_manager,
                 verbose=self.verbose,
                 **kwargs,
             )
             generations.append([final_chunk])
         return LLMResult(generations=generations)
-    async def _agenerate(
-        self,
-        prompts: List[str],
-        stop: Optional[List[str]] = None,
-        images: Optional[List[str]] = None,
-        run_manager: Optional[CallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> LLMResult:
-        """Call out to Ollama's generate endpoint.
-        Args:
-            prompt: The prompt to pass into the model.
-            stop: Optional list of stop words to use when generating.
-        Returns:
-            The string generated by the model.
-        Example:
-            .. code-block:: python
-                response = ollama("Tell me a joke.")
-        """
-        generations = []
-        for prompt in prompts:
-            final_chunk = await super()._astream_with_aggregation(
-                prompt,
-                stop=stop,
-                images=images,
-                run_manager=run_manager,
-                verbose=self.verbose,
-                **kwargs,
-            )
-            generations.append([final_chunk])
-        return LLMResult(generations=generations)
     def _stream(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> Iterator[GenerationChunk]:
         for stream_resp in self._create_stream(prompt, stop, **kwargs):
             if stream_resp:
                 chunk = _stream_response_to_generation_chunk(stream_resp)
                 yield chunk
                 if run_manager:
                     run_manager.on_llm_new_token(
                         chunk.text,
                         verbose=self.verbose,
                     )
-    async def _astream(
-        self,
-        prompt: str,
-        stop: Optional[List[str]] = None,
-        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[GenerationChunk]:
-        async for stream_resp in self._acreate_stream(prompt, stop, **kwargs):
-            if stream_resp:
-                chunk = _stream_response_to_generation_chunk(stream_resp)
-                yield chunk
-                if run_manager:
-                    await run_manager.on_llm_new_token(
-                        chunk.text,
-                        verbose=self.verbose,
-                    )

--- a/libs/community/langchain_community/llms/petals.py
+++ b/libs/community/langchain_community/llms/petals.py
@@ -1,16 +1,16 @@
 import logging
 from typing import Any, Dict, List, Mapping, Optional
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import LLM
-from langchain_core.pydantic_v1 import Extra, Field, SecretStr, root_validator
-from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
+from langchain_core.pydantic_v1 import Extra, Field, root_validator
+from langchain_core.utils import get_from_dict_or_env
 from langchain_community.llms.utils import enforce_stop_tokens
 logger = logging.getLogger(__name__)
 class Petals(LLM):
     """Petals Bloom models.
     To use, you should have the ``petals`` python package installed, and the
     environment variable ``HUGGINGFACE_API_KEY`` set with your API key.
     Any parameters that are valid to be passed to the call can be passed
     in, even if not explicitly saved on this class.
     Example:
         .. code-block:: python
@@ -32,21 +32,21 @@
     top_k: Optional[int] = None
     """The number of highest probability vocabulary tokens
     to keep for top-k-filtering."""
     do_sample: bool = True
     """Whether or not to use sampling; use greedy decoding otherwise."""
     max_length: Optional[int] = None
     """The maximum length of the sequence to be generated."""
     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
     """Holds any model parameters valid for `create` call
     not explicitly specified."""
-    huggingface_api_key: Optional[SecretStr] = None
+    huggingface_api_key: Optional[str] = None
     class Config:
         """Configuration for this pydantic config."""
         extra = Extra.forbid
     @root_validator(pre=True)
     def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:
         """Build extra kwargs from additional params that were passed in."""
         all_required_field_names = {field.alias for field in cls.__fields__.values()}
         extra = values.get("model_kwargs", {})
         for field_name in list(values):
             if field_name not in all_required_field_names:
@@ -56,32 +56,32 @@
                     f"""WARNING! {field_name} is not default parameter.
                     {field_name} was transferred to model_kwargs.
                     Please confirm that {field_name} is what you intended."""
                 )
                 extra[field_name] = values.pop(field_name)
         values["model_kwargs"] = extra
         return values
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that api key and python package exists in environment."""
-        huggingface_api_key = convert_to_secret_str(
-            get_from_dict_or_env(values, "huggingface_api_key", "HUGGINGFACE_API_KEY")
+        huggingface_api_key = get_from_dict_or_env(
+            values, "huggingface_api_key", "HUGGINGFACE_API_KEY"
         )
         try:
             from petals import AutoDistributedModelForCausalLM
             from transformers import AutoTokenizer
             model_name = values["model_name"]
             values["tokenizer"] = AutoTokenizer.from_pretrained(model_name)
             values["client"] = AutoDistributedModelForCausalLM.from_pretrained(
                 model_name
             )
-            values["huggingface_api_key"] = huggingface_api_key.get_secret_value()
+            values["huggingface_api_key"] = huggingface_api_key
         except ImportError:
             raise ImportError(
                 "Could not import transformers or petals python package."
                 "Please install with `pip install -U transformers petals`."
             )
         return values
     @property
     def _default_params(self) -> Dict[str, Any]:
         """Get the default parameters for calling Petals API."""
         normal_params = {

--- a/libs/community/langchain_community/llms/pipelineai.py
+++ b/libs/community/langchain_community/llms/pipelineai.py
@@ -1,41 +1,35 @@
 import logging
 from typing import Any, Dict, List, Mapping, Optional
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import LLM
-from langchain_core.pydantic_v1 import (
-    BaseModel,
-    Extra,
-    Field,
-    SecretStr,
-    root_validator,
-)
-from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
+from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
+from langchain_core.utils import get_from_dict_or_env
 from langchain_community.llms.utils import enforce_stop_tokens
 logger = logging.getLogger(__name__)
 class PipelineAI(LLM, BaseModel):
     """PipelineAI large language models.
     To use, you should have the ``pipeline-ai`` python package installed,
     and the environment variable ``PIPELINE_API_KEY`` set with your API key.
     Any parameters that are valid to be passed to the call can be passed
     in, even if not explicitly saved on this class.
     Example:
         .. code-block:: python
             from langchain_community.llms import PipelineAI
             pipeline = PipelineAI(pipeline_key="")
     """
     pipeline_key: str = ""
     """The id or tag of the target pipeline"""
     pipeline_kwargs: Dict[str, Any] = Field(default_factory=dict)
     """Holds any pipeline parameters valid for `create` call not
     explicitly specified."""
-    pipeline_api_key: Optional[SecretStr] = None
+    pipeline_api_key: Optional[str] = None
     class Config:
         """Configuration for this pydantic config."""
         extra = Extra.forbid
     @root_validator(pre=True)
     def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:
         """Build extra kwargs from additional params that were passed in."""
         all_required_field_names = {field.alias for field in cls.__fields__.values()}
         extra = values.get("pipeline_kwargs", {})
         for field_name in list(values):
             if field_name not in all_required_field_names:
@@ -44,22 +38,22 @@
                 logger.warning(
                     f"""{field_name} was transferred to pipeline_kwargs.
                     Please confirm that {field_name} is what you intended."""
                 )
                 extra[field_name] = values.pop(field_name)
         values["pipeline_kwargs"] = extra
         return values
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that api key and python package exists in environment."""
-        pipeline_api_key = convert_to_secret_str(
-            get_from_dict_or_env(values, "pipeline_api_key", "PIPELINE_API_KEY")
+        pipeline_api_key = get_from_dict_or_env(
+            values, "pipeline_api_key", "PIPELINE_API_KEY"
         )
         values["pipeline_api_key"] = pipeline_api_key
         return values
     @property
     def _identifying_params(self) -> Mapping[str, Any]:
         """Get the identifying parameters."""
         return {
             **{"pipeline_key": self.pipeline_key},
             **{"pipeline_kwargs": self.pipeline_kwargs},
         }
@@ -75,21 +69,21 @@
         **kwargs: Any,
     ) -> str:
         """Call to Pipeline Cloud endpoint."""
         try:
             from pipeline import PipelineCloud
         except ImportError:
             raise ImportError(
                 "Could not import pipeline-ai python package. "
                 "Please install it with `pip install pipeline-ai`."
             )
-        client = PipelineCloud(token=self.pipeline_api_key.get_secret_value())
+        client = PipelineCloud(token=self.pipeline_api_key)
         params = self.pipeline_kwargs or {}
         params = {**params, **kwargs}
         run = client.run_pipeline(self.pipeline_key, [prompt, params])
         try:
             text = run.result_preview[0][0]
         except AttributeError:
             raise AttributeError(
                 f"A pipeline run should have a `result_preview` attribute."
                 f"Run was: {run}"
             )

--- a/libs/community/langchain_community/llms/predibase.py
+++ b/libs/community/langchain_community/llms/predibase.py
@@ -1,35 +1,35 @@
 from typing import Any, Dict, List, Mapping, Optional
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import LLM
-from langchain_core.pydantic_v1 import Field, SecretStr
+from langchain_core.pydantic_v1 import Field
 class Predibase(LLM):
     """Use your Predibase models with Langchain.
     To use, you should have the ``predibase`` python package installed,
     and have your Predibase API key.
     """
     model: str
-    predibase_api_key: SecretStr
+    predibase_api_key: str
     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
     @property
     def _llm_type(self) -> str:
         return "predibase"
     def _call(
         self,
         prompt: str,
         stop: Optional[List[str]] = None,
         run_manager: Optional[CallbackManagerForLLMRun] = None,
         **kwargs: Any,
     ) -> str:
         try:
             from predibase import PredibaseClient
-            pc = PredibaseClient(token=self.predibase_api_key.get_secret_value())
+            pc = PredibaseClient(token=self.predibase_api_key)
         except ImportError as e:
             raise ImportError(
                 "Could not import Predibase Python package. "
                 "Please install it with `pip install predibase`."
             ) from e
         except ValueError as e:
             raise ValueError("Your API key is not correct. Please try again") from e
         results = pc.prompt(prompt, model_name=self.model)
         return results[0].response
     @property

--- a/libs/community/langchain_community/llms/stochasticai.py
+++ b/libs/community/langchain_community/llms/stochasticai.py
@@ -1,35 +1,35 @@
 import logging
 import time
 from typing import Any, Dict, List, Mapping, Optional
 import requests
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import LLM
-from langchain_core.pydantic_v1 import Extra, Field, SecretStr, root_validator
-from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
+from langchain_core.pydantic_v1 import Extra, Field, root_validator
+from langchain_core.utils import get_from_dict_or_env
 from langchain_community.llms.utils import enforce_stop_tokens
 logger = logging.getLogger(__name__)
 class StochasticAI(LLM):
     """StochasticAI large language models.
     To use, you should have the environment variable ``STOCHASTICAI_API_KEY``
     set with your API key.
     Example:
         .. code-block:: python
             from langchain_community.llms import StochasticAI
             stochasticai = StochasticAI(api_url="")
     """
     api_url: str = ""
     """Model name to use."""
     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
     """Holds any model parameters valid for `create` call not
     explicitly specified."""
-    stochasticai_api_key: Optional[SecretStr] = None
+    stochasticai_api_key: Optional[str] = None
     class Config:
         """Configuration for this pydantic object."""
         extra = Extra.forbid
     @root_validator(pre=True)
     def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:
         """Build extra kwargs from additional params that were passed in."""
         all_required_field_names = {field.alias for field in cls.__fields__.values()}
         extra = values.get("model_kwargs", {})
         for field_name in list(values):
             if field_name not in all_required_field_names:
@@ -38,22 +38,22 @@
                 logger.warning(
                     f"""{field_name} was transferred to model_kwargs.
                     Please confirm that {field_name} is what you intended."""
                 )
                 extra[field_name] = values.pop(field_name)
         values["model_kwargs"] = extra
         return values
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
         """Validate that api key exists in environment."""
-        stochasticai_api_key = convert_to_secret_str(
-            get_from_dict_or_env(values, "stochasticai_api_key", "STOCHASTICAI_API_KEY")
+        stochasticai_api_key = get_from_dict_or_env(
+            values, "stochasticai_api_key", "STOCHASTICAI_API_KEY"
         )
         values["stochasticai_api_key"] = stochasticai_api_key
         return values
     @property
     def _identifying_params(self) -> Mapping[str, Any]:
         """Get the identifying parameters."""
         return {
             **{"endpoint_url": self.api_url},
             **{"model_kwargs": self.model_kwargs},
         }
@@ -77,33 +77,33 @@
         Example:
             .. code-block:: python
                 response = StochasticAI("Tell me a joke.")
         """
         params = self.model_kwargs or {}
         params = {**params, **kwargs}
         response_post = requests.post(
             url=self.api_url,
             json={"prompt": prompt, "params": params},
             headers={
-                "apiKey": f"{self.stochasticai_api_key.get_secret_value()}",
+                "apiKey": f"{self.stochasticai_api_key}",
                 "Accept": "application/json",
                 "Content-Type": "application/json",
             },
         )
         response_post.raise_for_status()
         response_post_json = response_post.json()
         completed = False
         while not completed:
             response_get = requests.get(
                 url=response_post_json["data"]["responseUrl"],
                 headers={
-                    "apiKey": f"{self.stochasticai_api_key.get_secret_value()}",
+                    "apiKey": f"{self.stochasticai_api_key}",
                     "Accept": "application/json",
                     "Content-Type": "application/json",
                 },
             )
             response_get.raise_for_status()
             response_get_json = response_get.json()["data"]
             text = response_get_json.get("completion")
             completed = text is not None
             time.sleep(0.5)
         text = text[0]

--- a/libs/community/langchain_community/llms/volcengine_maas.py
+++ b/libs/community/langchain_community/llms/volcengine_maas.py
@@ -1,23 +1,23 @@
 from __future__ import annotations
 from typing import Any, Dict, Iterator, List, Optional
 from langchain_core.callbacks import CallbackManagerForLLMRun
 from langchain_core.language_models.llms import LLM
 from langchain_core.outputs import GenerationChunk
-from langchain_core.pydantic_v1 import BaseModel, Field, SecretStr, root_validator
-from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
+from langchain_core.pydantic_v1 import BaseModel, Field, root_validator
+from langchain_core.utils import get_from_dict_or_env
 class VolcEngineMaasBase(BaseModel):
     """Base class for VolcEngineMaas models."""
     client: Any
-    volc_engine_maas_ak: Optional[SecretStr] = None
+    volc_engine_maas_ak: Optional[str] = None
     """access key for volc engine"""
-    volc_engine_maas_sk: Optional[SecretStr] = None
+    volc_engine_maas_sk: Optional[str] = None
     """secret key for volc engine"""
     endpoint: Optional[str] = "maas-api.ml-platform-cn-beijing.volces.com"
     """Endpoint of the VolcEngineMaas LLM."""
     region: Optional[str] = "Region"
     """Region of the VolcEngineMaas LLM."""
     model: str = "skylark-lite-public"
     """Model name. you could check this model details here 
     https://www.volcengine.com/docs/82379/1133187
     and you could choose other models by change this field"""
     model_version: Optional[str] = None
@@ -31,41 +31,37 @@
     """model special arguments, you could check detail on model page"""
     streaming: bool = False
     """Whether to stream the results."""
     connect_timeout: Optional[int] = 60
     """Timeout for connect to volc engine maas endpoint. Default is 60 seconds."""
     read_timeout: Optional[int] = 60
     """Timeout for read response from volc engine maas endpoint. 
     Default is 60 seconds."""
     @root_validator()
     def validate_environment(cls, values: Dict) -> Dict:
-        volc_engine_maas_ak = convert_to_secret_str(
-            get_from_dict_or_env(values, "volc_engine_maas_ak", "VOLC_ACCESSKEY")
-        )
-        volc_engine_maas_sk = convert_to_secret_str(
-            get_from_dict_or_env(values, "volc_engine_maas_sk", "VOLC_SECRETKEY")
-        )
+        ak = get_from_dict_or_env(values, "volc_engine_maas_ak", "VOLC_ACCESSKEY")
+        sk = get_from_dict_or_env(values, "volc_engine_maas_sk", "VOLC_SECRETKEY")
         endpoint = values["endpoint"]
         if values["endpoint"] is not None and values["endpoint"] != "":
             endpoint = values["endpoint"]
         try:
             from volcengine.maas import MaasService
             maas = MaasService(
                 endpoint,
                 values["region"],
                 connection_timeout=values["connect_timeout"],
                 socket_timeout=values["read_timeout"],
             )
-            maas.set_ak(volc_engine_maas_ak.get_secret_value())
-            maas.set_sk(volc_engine_maas_sk.get_secret_value())
-            values["volc_engine_maas_ak"] = volc_engine_maas_ak
-            values["volc_engine_maas_sk"] = volc_engine_maas_sk
+            maas.set_ak(ak)
+            values["volc_engine_maas_ak"] = ak
+            values["volc_engine_maas_sk"] = sk
+            maas.set_sk(sk)
             values["client"] = maas
         except ImportError:
             raise ImportError(
                 "volcengine package not found, please install it with "
                 "`pip install volcengine`"
             )
         return values
     @property
     def _default_params(self) -> Dict[str, Any]:
         """Get the default parameters for calling VolcEngineMaas API."""

--- a/libs/community/langchain_community/retrievers/google_vertex_ai_search.py
+++ b/libs/community/langchain_community/retrievers/google_vertex_ai_search.py
@@ -122,20 +122,21 @@
                         page_content=chunk.get("content", ""), metadata=doc_metadata
                     )
                 )
         return documents
     def _convert_website_search_response(
         self, results: Sequence[SearchResult], chunk_type: str
     ) -> List[Document]:
         """Converts a sequence of search results to a list of LangChain documents."""
         from google.protobuf.json_format import MessageToDict
         documents: List[Document] = []
+        chunk_type = "extractive_answers"
         for result in results:
             document_dict = MessageToDict(
                 result.document._pb, preserving_proto_field_name=True
             )
             derived_struct_data = document_dict.get("derived_struct_data")
             if not derived_struct_data:
                 continue
             doc_metadata = document_dict.get("struct_data", {})
             doc_metadata["id"] = document_dict["id"]
             doc_metadata["source"] = derived_struct_data.get("link", "")

--- a/libs/community/langchain_community/tools/e2b_data_analysis/tool.py
+++ b/libs/community/langchain_community/tools/e2b_data_analysis/tool.py
@@ -1,17 +1,17 @@
 from __future__ import annotations
 import ast
 import json
 import os
 from io import StringIO
 from sys import version_info
-from typing import IO, TYPE_CHECKING, Any, Callable, List, Optional, Type, Union
+from typing import IO, TYPE_CHECKING, Any, Callable, List, Optional, Type
 from langchain_core.callbacks import (
     AsyncCallbackManagerForToolRun,
     CallbackManager,
     CallbackManagerForToolRun,
 )
 from langchain_core.pydantic_v1 import BaseModel, Field, PrivateAttr
 from langchain_community.tools import BaseTool, Tool
 from langchain_community.tools.e2b_data_analysis.unparse import Unparser
 if TYPE_CHECKING:
     from e2b import EnvVars
@@ -154,24 +154,24 @@
         cmd: str,
     ) -> dict:
         """Run shell command in the sandbox."""
         proc = self.session.process.start(cmd)
         output = proc.wait()
         return {
             "stdout": output.stdout,
             "stderr": output.stderr,
             "exit_code": output.exit_code,
         }
-    def install_python_packages(self, package_names: Union[str, List[str]]) -> None:
+    def install_python_packages(self, package_names: str | List[str]) -> None:
         """Install python packages in the sandbox."""
         self.session.install_python_packages(package_names)
-    def install_system_packages(self, package_names: Union[str, List[str]]) -> None:
+    def install_system_packages(self, package_names: str | List[str]) -> None:
         """Install system packages (via apt) in the sandbox."""
         self.session.install_system_packages(package_names)
     def download_file(self, remote_path: str) -> bytes:
         """Download file from the sandbox."""
         return self.session.download_file(remote_path)
     def upload_file(self, file: IO, description: str) -> UploadedFile:
         """Upload file to the sandbox.
         The file is uploaded to the '/home/user/<filename>' path."""
         remote_path = self.session.upload_file(file)
         f = UploadedFile(

--- a/libs/community/langchain_community/tools/gmail/send_message.py
+++ b/libs/community/langchain_community/tools/gmail/send_message.py
@@ -1,15 +1,15 @@
 """Send Gmail messages."""
 import base64
 from email.mime.multipart import MIMEMultipart
 from email.mime.text import MIMEText
-from typing import Any, Dict, List, Optional, Type, Union
+from typing import Any, Dict, List, Optional, Union
 from langchain_core.callbacks import CallbackManagerForToolRun
 from langchain_core.pydantic_v1 import BaseModel, Field
 from langchain_community.tools.gmail.base import GmailBaseTool
 class SendMessageSchema(BaseModel):
     """Input for SendMessageTool."""
     message: str = Field(
         ...,
         description="The message to send.",
     )
     to: Union[str, List[str]] = Field(
@@ -27,21 +27,20 @@
     bcc: Optional[Union[str, List[str]]] = Field(
         None,
         description="The list of BCC recipients.",
     )
 class GmailSendMessage(GmailBaseTool):
     """Tool that sends a message to Gmail."""
     name: str = "send_gmail_message"
     description: str = (
         "Use this tool to send email messages." " The input is the message, recipients"
     )
-    args_schema: Type[SendMessageSchema] = SendMessageSchema
     def _prepare_message(
         self,
         message: str,
         to: Union[str, List[str]],
         subject: str,
         cc: Optional[Union[str, List[str]]] = None,
         bcc: Optional[Union[str, List[str]]] = None,
     ) -> Dict[str, Any]:
         """Create a message for an email."""
         mime_message = MIMEMultipart()

--- a/libs/community/langchain_community/tools/tavily_search/tool.py
+++ b/libs/community/langchain_community/tools/tavily_search/tool.py
@@ -11,21 +11,21 @@
     """Input for the Tavily tool."""
     query: str = Field(description="search query to look up")
 class TavilySearchResults(BaseTool):
     """Tool that queries the Tavily Search API and gets back json."""
     name: str = "tavily_search_results_json"
     description: str = (
         "A search engine optimized for comprehensive, accurate, and trusted results. "
         "Useful for when you need to answer questions about current events. "
         "Input should be a search query."
     )
-    api_wrapper: TavilySearchAPIWrapper = Field(default_factory=TavilySearchAPIWrapper)
+    api_wrapper: TavilySearchAPIWrapper
     max_results: int = 5
     args_schema: Type[BaseModel] = TavilyInput
     def _run(
         self,
         query: str,
         run_manager: Optional[CallbackManagerForToolRun] = None,
     ) -> Union[List[Dict], str]:
         """Use the tool."""
         try:
             return self.api_wrapper.results(
@@ -49,21 +49,21 @@
             return repr(e)
 class TavilyAnswer(BaseTool):
     """Tool that queries the Tavily Search API and gets back an answer."""
     name: str = "tavily_answer"
     description: str = (
         "A search engine optimized for comprehensive, accurate, and trusted results. "
         "Useful for when you need to answer questions about current events. "
         "Input should be a search query. "
         "This returns only the answer - not the original source data."
     )
-    api_wrapper: TavilySearchAPIWrapper = Field(default_factory=TavilySearchAPIWrapper)
+    api_wrapper: TavilySearchAPIWrapper
     args_schema: Type[BaseModel] = TavilyInput
     def _run(
         self,
         query: str,
         run_manager: Optional[CallbackManagerForToolRun] = None,
     ) -> Union[List[Dict], str]:
         """Use the tool."""
         try:
             return self.api_wrapper.raw_results(
                 query,

--- a/libs/community/langchain_community/utilities/github.py
+++ b/libs/community/langchain_community/utilities/github.py
@@ -59,27 +59,21 @@
         gi = GithubIntegration(auth=auth)
         installation = gi.get_installations()
         if not installation:
             raise ValueError(
                 f"Please make sure to install the created github app with id "
                 f"{github_app_id} on the repo: {github_repository}"
                 "More instructions can be found at "
                 "https://docs.github.com/en/apps/using-"
                 "github-apps/installing-your-own-github-app"
             )
-        try:
-            installation = installation[0]
-        except ValueError as e:
-            raise ValueError(
-                "Please make sure to give correct github parameters "
-                f"Error message: {e}"
-            )
+        installation = installation[0]
         g = installation.get_github_for_installation()
         repo = g.get_repo(github_repository)
         github_base_branch = get_from_dict_or_env(
             values,
             "github_base_branch",
             "GITHUB_BASE_BRANCH",
             default=repo.default_branch,
         )
         active_branch = get_from_dict_or_env(
             values,

--- a/libs/community/langchain_community/vectorstores/jaguar.py
+++ b/libs/community/langchain_community/vectorstores/jaguar.py
@@ -1,25 +1,27 @@
 from __future__ import annotations
 import json
 import logging
-from typing import Any, List, Optional, Tuple
+from typing import TYPE_CHECKING, Any, List, Optional, Tuple
+if TYPE_CHECKING:
+    from jaguardb_http_client.JaguarHttpClient import JaguarHttpClient
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
 from langchain_core.vectorstores import VectorStore
 logger = logging.getLogger(__name__)
 class Jaguar(VectorStore):
     """`Jaguar API` vector store.
     See http://www.jaguardb.com
     See http://github.com/fserv/jaguar-sdk
     Example:
        .. code-block:: python
-           from langchain_community.vectorstores.jaguar import Jaguar
+           from langchain.vectorstores import Jaguar
            vectorstore = Jaguar(
                pod = 'vdb',
                store = 'mystore',
                vector_index = 'v',
                vector_type = 'cosine_fraction_float',
                vector_dimension = 1536,
                url='http://192.168.8.88:8080/fwww/',
                embedding=openai_model
            )
     """
@@ -32,27 +34,20 @@
         vector_dimension: int,
         url: str,
         embedding: Embeddings,
     ):
         self._pod = pod
         self._store = store
         self._vector_index = vector_index
         self._vector_type = vector_type
         self._vector_dimension = vector_dimension
         self._embedding = embedding
-        try:
-            from jaguardb_http_client.JaguarHttpClient import JaguarHttpClient
-        except ImportError:
-            raise ValueError(
-                "Could not import jaguardb-http-client python package. "
-                "Please install it with `pip install -U jaguardb-http-client`"
-            )
         self._jag = JaguarHttpClient(url)
         self._token = ""
     def login(
         self,
         jaguar_api_key: Optional[str] = "",
     ) -> bool:
         """
         login to jaguardb server with a jaguar_api_key or let self._jag find a key
         Args:
             pod (str):  name of a Pod

--- a/libs/community/langchain_community/vectorstores/momento_vector_index.py
+++ b/libs/community/langchain_community/vectorstores/momento_vector_index.py
@@ -250,27 +250,22 @@
                 forwarded to the Momento Vector Index:
             - top_k (int, optional): The number of results to return.
         Returns:
             List[Tuple[Document, float]]: A list of tuples of the form
                 (Document, score).
         """
         from momento.requests.vector_index import ALL_METADATA
         from momento.responses.vector_index import Search
         if "top_k" in kwargs:
             k = kwargs["k"]
-        filter_expression = kwargs.get("filter_expression", None)
         response = self._client.search(
-            self.index_name,
-            embedding,
-            top_k=k,
-            metadata_fields=ALL_METADATA,
-            filter_expression=filter_expression,
+            self.index_name, embedding, top_k=k, metadata_fields=ALL_METADATA
         )
         if not isinstance(response, Search.Success):
             return []
         results = []
         for hit in response.hits:
             text = cast(str, hit.metadata.pop(self.text_field))
             doc = Document(page_content=text, metadata=hit.metadata)
             pair = (doc, hit.score)
             results.append(pair)
         return results
@@ -305,27 +300,22 @@
             fetch_k: Number of Documents to fetch to pass to MMR algorithm.
             lambda_mult: Number between 0 and 1 that determines the degree
                         of diversity among the results with 0 corresponding
                         to maximum diversity and 1 to minimum diversity.
                         Defaults to 0.5.
         Returns:
             List of Documents selected by maximal marginal relevance.
         """
         from momento.requests.vector_index import ALL_METADATA
         from momento.responses.vector_index import SearchAndFetchVectors
-        filter_expression = kwargs.get("filter_expression", None)
         response = self._client.search_and_fetch_vectors(
-            self.index_name,
-            embedding,
-            top_k=fetch_k,
-            metadata_fields=ALL_METADATA,
-            filter_expression=filter_expression,
+            self.index_name, embedding, top_k=fetch_k, metadata_fields=ALL_METADATA
         )
         if isinstance(response, SearchAndFetchVectors.Success):
             pass
         elif isinstance(response, SearchAndFetchVectors.Error):
             logger.error(f"Error searching and fetching vectors: {response}")
             return []
         else:
             logger.error(f"Unexpected response: {response}")
             raise Exception(f"Unexpected response: {response}")
         mmr_selected = maximal_marginal_relevance(

--- a/libs/community/langchain_community/vectorstores/pgvector.py
+++ b/libs/community/langchain_community/vectorstores/pgvector.py
@@ -148,89 +148,91 @@
     ) -> None:
         self.connection_string = connection_string
         self.embedding_function = embedding_function
         self.collection_name = collection_name
         self.collection_metadata = collection_metadata
         self._distance_strategy = distance_strategy
         self.pre_delete_collection = pre_delete_collection
         self.logger = logger or logging.getLogger(__name__)
         self.override_relevance_score_fn = relevance_score_fn
         self.engine_args = engine_args or {}
-        self._bind = connection if connection else self._create_engine()
+        self._conn = connection if connection else self.connect()
         self.__post_init__()
     def __post_init__(
         self,
     ) -> None:
         """Initialize the store."""
         self.create_vector_extension()
         EmbeddingStore, CollectionStore = _get_embedding_collection_store()
         self.CollectionStore = CollectionStore
         self.EmbeddingStore = EmbeddingStore
         self.create_tables_if_not_exists()
         self.create_collection()
     def __del__(self) -> None:
-        if isinstance(self._bind, sqlalchemy.engine.Connection):
-            self._bind.close()
+        if self._conn:
+            self._conn.close()
     @property
     def embeddings(self) -> Embeddings:
         return self.embedding_function
-    def _create_engine(self) -> sqlalchemy.engine.Engine:
-        return sqlalchemy.create_engine(url=self.connection_string, **self.engine_args)
+    def connect(self) -> sqlalchemy.engine.Connection:
+        engine = sqlalchemy.create_engine(self.connection_string, **self.engine_args)
+        conn = engine.connect()
+        return conn
     def create_vector_extension(self) -> None:
         try:
-            with Session(self._bind) as session:
+            with Session(self._conn) as session:
                 statement = sqlalchemy.text(
                     "BEGIN;"
                     "SELECT pg_advisory_xact_lock(1573678846307946496);"
                     "CREATE EXTENSION IF NOT EXISTS vector;"
                     "COMMIT;"
                 )
                 session.execute(statement)
                 session.commit()
         except Exception as e:
             raise Exception(f"Failed to create vector extension: {e}") from e
     def create_tables_if_not_exists(self) -> None:
-        with Session(self._bind) as session, session.begin():
-            Base.metadata.create_all(session.get_bind())
+        with self._conn.begin():
+            Base.metadata.create_all(self._conn)
     def drop_tables(self) -> None:
-        with Session(self._bind) as session, session.begin():
-            Base.metadata.drop_all(session.get_bind())
+        with self._conn.begin():
+            Base.metadata.drop_all(self._conn)
     def create_collection(self) -> None:
         if self.pre_delete_collection:
             self.delete_collection()
-        with Session(self._bind) as session:
+        with Session(self._conn) as session:
             self.CollectionStore.get_or_create(
                 session, self.collection_name, cmetadata=self.collection_metadata
             )
     def delete_collection(self) -> None:
         self.logger.debug("Trying to delete collection")
-        with Session(self._bind) as session:
+        with Session(self._conn) as session:
             collection = self.get_collection(session)
             if not collection:
                 self.logger.warning("Collection not found")
                 return
             session.delete(collection)
             session.commit()
     @contextlib.contextmanager
     def _make_session(self) -> Generator[Session, None, None]:
         """Create a context manager for the session, bind to _conn string."""
-        yield Session(self._bind)
+        yield Session(self._conn)
     def delete(
         self,
         ids: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> None:
         """Delete vectors by ids or uuids.
         Args:
             ids: List of ids to delete.
         """
-        with Session(self._bind) as session:
+        with Session(self._conn) as session:
             if ids is not None:
                 self.logger.debug(
                     "Trying to delete vectors by ids (represented by the model "
                     "using the custom ids field)"
                 )
                 stmt = delete(self.EmbeddingStore).where(
                     self.EmbeddingStore.custom_id.in_(ids)
                 )
                 session.execute(stmt)
             session.commit()
@@ -280,21 +282,21 @@
         Args:
             texts: Iterable of strings to add to the vectorstore.
             embeddings: List of list of embedding vectors.
             metadatas: List of metadatas associated with the texts.
             kwargs: vectorstore specific parameters
         """
         if ids is None:
             ids = [str(uuid.uuid1()) for _ in texts]
         if not metadatas:
             metadatas = [{} for _ in texts]
-        with Session(self._bind) as session:
+        with Session(self._conn) as session:
             collection = self.get_collection(session)
             if not collection:
                 raise ValueError("Collection not found")
             for text, metadata, embedding, id in zip(texts, metadatas, embeddings, ids):
                 embedding_store = self.EmbeddingStore(
                     embedding=embedding,
                     document=text,
                     cmetadata=metadata,
                     custom_id=id,
                     collection_id=collection.uuid,
@@ -395,21 +397,21 @@
             for result in results
         ]
         return docs
     def __query_collection(
         self,
         embedding: List[float],
         k: int = 4,
         filter: Optional[Dict[str, str]] = None,
     ) -> List[Any]:
         """Query the collection."""
-        with Session(self._bind) as session:
+        with Session(self._conn) as session:
             collection = self.get_collection(session)
             if not collection:
                 raise ValueError("Collection not found")
             filter_by = self.EmbeddingStore.collection_id == collection.uuid
             if filter is not None:
                 filter_clauses = []
                 IN, NIN = "in", "nin"
                 for key, value in filter.items():
                     if isinstance(value, dict):
                         value_case_insensitive = {

--- a/libs/community/langchain_community/vectorstores/semadb.py
+++ b/libs/community/langchain_community/vectorstores/semadb.py
@@ -18,21 +18,21 @@
     HOST = "semadb.p.rapidapi.com"
     BASE_URL = "https://" + HOST
     def __init__(
         self,
         collection_name: str,
         vector_size: int,
         embedding: Embeddings,
         distance_strategy: DistanceStrategy = DistanceStrategy.EUCLIDEAN_DISTANCE,
         api_key: str = "",
     ):
-        """initialize the SemaDB vector store."""
+        """Initialise the SemaDB vector store."""
         self.collection_name = collection_name
         self.vector_size = vector_size
         self.api_key = api_key or get_from_env("api_key", "SEMADB_API_KEY")
         self._embedding = embedding
         self.distance_strategy = distance_strategy
     @property
     def headers(self) -> dict:
         """Return the common headers."""
         return {
             "content-type": "application/json",

--- a/libs/community/langchain_community/vectorstores/surrealdb.py
+++ b/libs/community/langchain_community/vectorstores/surrealdb.py
@@ -43,21 +43,21 @@
         self,
         embedding_function: Embeddings,
         **kwargs: Any,
     ) -> None:
         from surrealdb import Surreal
         self.collection = kwargs.pop("collection", "documents")
         self.ns = kwargs.pop("ns", "langchain")
         self.db = kwargs.pop("db", "database")
         self.dburl = kwargs.pop("dburl", "ws://localhost:8000/rpc")
         self.embedding_function = embedding_function
-        self.sdb = Surreal(self.dburl)
+        self.sdb = Surreal()
         self.kwargs = kwargs
     async def initialize(self) -> None:
         """
         Initialize connection to surrealdb database
         and authenticate if credentials are provided
         """
         await self.sdb.connect(self.dburl)
         if "db_user" in self.kwargs and "db_pass" in self.kwargs:
             user = self.kwargs.get("db_user")
             password = self.kwargs.get("db_pass")
@@ -78,49 +78,38 @@
     ) -> List[str]:
         """Add list of text along with embeddings to the vector store asynchronously
         Args:
             texts (Iterable[str]): collection of text to add to the database
         Returns:
             List of ids for the newly inserted documents
         """
         embeddings = self.embedding_function.embed_documents(list(texts))
         ids = []
         for idx, text in enumerate(texts):
-            data = {"text": text, "embedding": embeddings[idx]}
-            if metadatas is not None and idx < len(metadatas):
-                data["metadata"] = metadatas[idx]
             record = await self.sdb.create(
-                self.collection,
-                data,
+                self.collection, {"text": text, "embedding": embeddings[idx]}
             )
             ids.append(record[0]["id"])
         return ids
     def add_texts(
         self,
         texts: Iterable[str],
         metadatas: Optional[List[dict]] = None,
         **kwargs: Any,
     ) -> List[str]:
         """Add list of text along with embeddings to the vector store
         Args:
             texts (Iterable[str]): collection of text to add to the database
         Returns:
             List of ids for the newly inserted documents
         """
-        async def _add_texts(
-            texts: Iterable[str],
-            metadatas: Optional[List[dict]] = None,
-            **kwargs: Any,
-        ) -> List[str]:
-            await self.initialize()
-            return await self.aadd_texts(texts, metadatas, **kwargs)
-        return asyncio.run(_add_texts(texts, metadatas, **kwargs))
+        return asyncio.run(self.aadd_texts(texts, metadatas, **kwargs))
     async def adelete(
         self,
         ids: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> Optional[bool]:
         """Delete by document ID asynchronously.
         Args:
             ids: List of ids to delete.
             **kwargs: Other keyword arguments that subclasses might use.
         Returns:
@@ -166,35 +155,32 @@
             k (int): Number of results to return. Defaults to 4.
         Returns:
             List of Documents most similar along with scores
         """
         args = {
             "collection": self.collection,
             "embedding": embedding,
             "k": k,
             "score_threshold": kwargs.get("score_threshold", 0),
         }
-        query = """select id, text, metadata,
+        query = """select id, text,
         vector::similarity::cosine(embedding,{embedding}) as similarity
         from {collection}
         where vector::similarity::cosine(embedding,{embedding}) >= {score_threshold}
         order by similarity desc LIMIT {k}
         """.format(**args)
         results = await self.sdb.query(query)
         if len(results) == 0:
             return []
         return [
             (
-                Document(
-                    page_content=result["text"],
-                    metadata={"id": result["id"], **result["metadata"]},
-                ),
+                Document(page_content=result["text"], metadata={"id": result["id"]}),
                 result["similarity"],
             )
             for result in results[0]["result"]
         ]
     async def asimilarity_search_with_relevance_scores(
         self, query: str, k: int = 4, **kwargs: Any
     ) -> List[Tuple[Document, float]]:
         """Run similarity search asynchronously and return relevance scores
         Args:
             query (str): Query
@@ -336,21 +322,21 @@
                 (default: "langchain")
             db (str): surrealdb database for the vector store.
                 (default: "database")
             collection (str): surrealdb collection for the vector store.
                 (default: "documents")
             (optional) db_user and db_pass: surrealdb credentials
         Returns:
             SurrealDBStore object initialized and ready for use."""
         sdb = cls(embedding, **kwargs)
         await sdb.initialize()
-        await sdb.aadd_texts(texts, metadatas, **kwargs)
+        await sdb.aadd_texts(texts)
         return sdb
     @classmethod
     def from_texts(
         cls,
         texts: List[str],
         embedding: Embeddings,
         metadatas: Optional[List[dict]] = None,
         **kwargs: Any,
     ) -> "SurrealDBStore":
         """Create SurrealDBStore from list of text

--- a/libs/community/langchain_community/vectorstores/vectara.py
+++ b/libs/community/langchain_community/vectorstores/vectara.py
@@ -1,79 +1,28 @@
 from __future__ import annotations
 import json
 import logging
 import os
-from dataclasses import dataclass, field
 from hashlib import md5
 from typing import Any, Iterable, List, Optional, Tuple, Type
 import requests
 from langchain_core.documents import Document
 from langchain_core.embeddings import Embeddings
 from langchain_core.pydantic_v1 import Field
 from langchain_core.vectorstores import VectorStore, VectorStoreRetriever
 logger = logging.getLogger(__name__)
-@dataclass
-class SummaryConfig:
-    """
-    is_enabled: True if summary is enabled, False otherwise
-    max_results: maximum number of results to summarize
-    response_lang: requested language for the summary
-    """
-    is_enabled: bool = False
-    max_results: int = 7
-    response_lang: str = "eng"
-@dataclass
-class MMRConfig:
-    """
-    is_enabled: True if MMR is enabled, False otherwise
-    mmr_k: number of results to fetch for MMR, defaults to 50
-    diversity_bias: number between 0 and 1 that determines the degree
-        of diversity among the results with 0 corresponding
-        to minimum diversity and 1 to maximum diversity.
-        Defaults to 0.3.
-        Note: diversity_bias is equivalent 1-lambda_mult
-        where lambda_mult is the value often used in max_marginal_relevance_search()
-        We chose to use that since we believe it's more intuitive to the user.
-    """
-    is_enabled: bool = False
-    mmr_k: int = 50
-    diversity_bias: float = 0.3
-@dataclass
-class VectaraQueryConfig:
-    """
-    k: Number of Documents to return. Defaults to 10.
-    lambda_val: lexical match parameter for hybrid search.
-    filter Dictionary of argument(s) to filter on metadata. For example a
-        filter can be "doc.rating > 3.0 and part.lang = 'deu'"} see
-        https://docs.vectara.com/docs/search-apis/sql/filter-overview
-        for more details.
-    score_threshold: minimal score threshold for the result.
-        If defined, results with score less than this value will be
-        filtered out.
-    n_sentence_context: number of sentences before/after the matching segment
-        to add, defaults to 2
-    mmr_config: MMRConfig configuration dataclass
-    summary_config: SummaryConfig configuration dataclass
-    """
-    k: int = 10
-    lambda_val: float = 0.0
-    filter: str = ""
-    score_threshold: Optional[float] = None
-    n_sentence_context: int = 2
-    mmr_config: MMRConfig = field(default_factory=MMRConfig)
-    summary_config: SummaryConfig = field(default_factory=SummaryConfig)
 class Vectara(VectorStore):
     """`Vectara API` vector store.
      See (https://vectara.com).
     Example:
         .. code-block:: python
-            from langchain.vectorstores import Vectara
+            from langchain_community.vectorstores import Vectara
             vectorstore = Vectara(
                 vectara_customer_id=vectara_customer_id,
                 vectara_corpus_id=vectara_corpus_id,
                 vectara_api_key=vectara_api_key
             )
     """
     def __init__(
         self,
         vectara_customer_id: Optional[str] = None,
         vectara_corpus_id: Optional[str] = None,
@@ -138,33 +87,28 @@
             timeout=self.vectara_api_timeout,
         )
         if response.status_code != 200:
             logger.error(
                 f"Delete request failed for doc_id = {doc_id} with status code "
                 f"{response.status_code}, reason {response.reason}, text "
                 f"{response.text}"
             )
             return False
         return True
-    def _index_doc(self, doc: dict, use_core_api: bool = False) -> str:
+    def _index_doc(self, doc: dict) -> str:
         request: dict[str, Any] = {}
         request["customer_id"] = self._vectara_customer_id
         request["corpus_id"] = self._vectara_corpus_id
         request["document"] = doc
-        api_endpoint = (
-            "https://api.vectara.io/v1/core/index"
-            if use_core_api
-            else "https://api.vectara.io/v1/index"
-        )
         response = self._session.post(
             headers=self._get_post_headers(),
-            url=api_endpoint,
+            url="https://api.vectara.io/v1/index",
             data=json.dumps(request),
             timeout=self.vectara_api_timeout,
             verify=True,
         )
         status_code = response.status_code
         result = response.json()
         status_str = result["status"]["code"] if "status" in result else None
         if status_code == 409 or status_str and (status_str == "ALREADY_EXISTS"):
             return "E_ALREADY_EXISTS"
         elif status_str and (status_str == "FORBIDDEN"):
@@ -241,223 +185,174 @@
         doc_hash = md5()
         for t in texts:
             doc_hash.update(t.encode())
         doc_id = doc_hash.hexdigest()
         if metadatas is None:
             metadatas = [{} for _ in texts]
         if doc_metadata:
             doc_metadata["source"] = "langchain"
         else:
             doc_metadata = {"source": "langchain"}
-        use_core_api = kwargs.get("use_core_api", False)
-        section_key = "parts" if use_core_api else "section"
         doc = {
             "document_id": doc_id,
             "metadataJson": json.dumps(doc_metadata),
-            section_key: [
+            "section": [
                 {"text": text, "metadataJson": json.dumps(md)}
                 for text, md in zip(texts, metadatas)
             ],
         }
-        success_str = self._index_doc(doc, use_core_api=use_core_api)
+        success_str = self._index_doc(doc)
         if success_str == "E_ALREADY_EXISTS":
             self._delete_doc(doc_id)
             self._index_doc(doc)
         elif success_str == "E_NO_PERMISSIONS":
             print(
                 """No permissions to add document to Vectara. 
                 Check your corpus ID, customer ID and API key"""
             )
         return [doc_id]
-    def vectara_query(
+    def similarity_search_with_score(
         self,
         query: str,
-        config: VectaraQueryConfig,
+        k: int = 5,
+        lambda_val: float = 0.025,
+        filter: Optional[str] = None,
+        score_threshold: Optional[float] = None,
+        n_sentence_context: int = 2,
         **kwargs: Any,
     ) -> List[Tuple[Document, float]]:
-        """Run a Vectara query
+        """Return Vectara documents most similar to query, along with scores.
         Args:
             query: Text to look up documents similar to.
-            config: VectaraQueryConfig object
+            k: Number of Documents to return. Defaults to 5.
+            lambda_val: lexical match parameter for hybrid search.
+            filter: Dictionary of argument(s) to filter on metadata. For example a
+                filter can be "doc.rating > 3.0 and part.lang = 'deu'"} see
+                https://docs.vectara.com/docs/search-apis/sql/filter-overview
+                for more details.
+            score_threshold: minimal score threshold for the result.
+                If defined, results with score less than this value will be
+                filtered out.
+            n_sentence_context: number of sentences before/after the matching segment
+                to add, defaults to 2
         Returns:
-            A list of k Documents matching the given query
-            If summary is enabled, last document is the summary text with 'summary'=True
-        """
-        if isinstance(config.mmr_config, dict):
-            config.mmr_config = MMRConfig(**config.mmr_config)
-        if isinstance(config.summary_config, dict):
-            config.summary_config = SummaryConfig(**config.summary_config)
-        data = {
-            "query": [
-                {
-                    "query": query,
-                    "start": 0,
-                    "numResults": config.mmr_config.mmr_k
-                    if config.mmr_config.is_enabled
-                    else config.k,
-                    "contextConfig": {
-                        "sentencesBefore": config.n_sentence_context,
-                        "sentencesAfter": config.n_sentence_context,
-                    },
-                    "corpusKey": [
-                        {
-                            "customerId": self._vectara_customer_id,
-                            "corpusId": self._vectara_corpus_id,
-                            "metadataFilter": config.filter,
-                            "lexicalInterpolationConfig": {"lambda": config.lambda_val},
-                        }
-                    ],
-                }
-            ]
-        }
-        if config.mmr_config.is_enabled:
-            data["query"][0]["rerankingConfig"] = {
-                "rerankerId": 272725718,
-                "mmrConfig": {"diversityBias": config.mmr_config.diversity_bias},
+            List of Documents most similar to the query and score for each.
+        """
+        data = json.dumps(
+            {
+                "query": [
+                    {
+                        "query": query,
+                        "start": 0,
+                        "num_results": k,
+                        "context_config": {
+                            "sentences_before": n_sentence_context,
+                            "sentences_after": n_sentence_context,
+                        },
+                        "corpus_key": [
+                            {
+                                "customer_id": self._vectara_customer_id,
+                                "corpus_id": self._vectara_corpus_id,
+                                "metadataFilter": filter,
+                                "lexical_interpolation_config": {"lambda": lambda_val},
+                            }
+                        ],
+                    }
+                ]
             }
-        if config.summary_config.is_enabled:
-            data["query"][0]["summary"] = [
-                {
-                    "maxSummarizedResults": config.summary_config.max_results,
-                    "responseLang": config.summary_config.response_lang,
-                }
-            ]
+        )
         response = self._session.post(
             headers=self._get_post_headers(),
             url="https://api.vectara.io/v1/query",
-            data=json.dumps(data),
+            data=data,
             timeout=self.vectara_api_timeout,
         )
         if response.status_code != 200:
             logger.error(
                 "Query failed %s",
                 f"(code {response.status_code}, reason {response.reason}, details "
                 f"{response.text})",
             )
-            return [], ""
+            return []
         result = response.json()
-        if config.score_threshold:
+        if score_threshold:
             responses = [
                 r
                 for r in result["responseSet"][0]["response"]
-                if r["score"] > config.score_threshold
+                if r["score"] > score_threshold
             ]
         else:
             responses = result["responseSet"][0]["response"]
         documents = result["responseSet"][0]["document"]
         metadatas = []
         for x in responses:
             md = {m["name"]: m["value"] for m in x["metadata"]}
             doc_num = x["documentIndex"]
             doc_md = {m["name"]: m["value"] for m in documents[doc_num]["metadata"]}
-            if "source" not in doc_md:
-                doc_md["source"] = "vectara"
             md.update(doc_md)
             metadatas.append(md)
-        res = [
+        docs_with_score = [
             (
                 Document(
                     page_content=x["text"],
                     metadata=md,
                 ),
                 x["score"],
             )
             for x, md in zip(responses, metadatas)
         ]
-        if config.mmr_config.is_enabled:
-            res = res[: config.k]
-        if config.summary_config.is_enabled:
-            summary = result["responseSet"][0]["summary"][0]["text"]
-            res.append(
-                (Document(page_content=summary, metadata={"summary": True}), 0.0)
-            )
-        return res
-    def similarity_search_with_score(
+        return docs_with_score
+    def similarity_search(
         self,
         query: str,
-        **kwargs: Any,
-    ) -> List[Tuple[Document, float]]:
-        """Return Vectara documents most similar to query, along with scores.
-        Args:
-            query: Text to look up documents similar to.
-            k: Number of Documents to return. Defaults to 10.
-            any other querying variable in VectaraQueryConfig like:
-            - lambda_val: lexical match parameter for hybrid search.
-            - filter: filter string
-            - score_threshold: minimal score threshold for the result.
-            - n_sentence_context: number of sentences before/after the matching segment
-            - mmr_config: optional configuration for MMR (see MMRConfig dataclass)
-            - summary_config: optional configuration for summary
-              (see SummaryConfig dataclass)
-        Returns:
-            List of Documents most similar to the query and score for each.
-        """
-        config = VectaraQueryConfig(**kwargs)
-        docs = self.vectara_query(query, config)
-        return docs
-    def similarity_search(
-        self,
-        query: str,
+        k: int = 5,
+        lambda_val: float = 0.025,
+        filter: Optional[str] = None,
+        n_sentence_context: int = 2,
         **kwargs: Any,
     ) -> List[Document]:
         """Return Vectara documents most similar to query, along with scores.
         Args:
             query: Text to look up documents similar to.
-            any other querying variable in VectaraQueryConfig
+            k: Number of Documents to return. Defaults to 5.
+            filter: Dictionary of argument(s) to filter on metadata. For example a
+                filter can be "doc.rating > 3.0 and part.lang = 'deu'"} see
+                https://docs.vectara.com/docs/search-apis/sql/filter-overview for more
+                details.
+            n_sentence_context: number of sentences before/after the matching segment
+                to add, defaults to 2
         Returns:
             List of Documents most similar to the query
         """
         docs_and_scores = self.similarity_search_with_score(
             query,
+            k=k,
+            lambda_val=lambda_val,
+            filter=filter,
+            score_threshold=None,
+            n_sentence_context=n_sentence_context,
             **kwargs,
         )
         return [doc for doc, _ in docs_and_scores]
-    def max_marginal_relevance_search(
-        self,
-        query: str,
-        fetch_k: int = 50,
-        lambda_mult: float = 0.5,
-        **kwargs: Any,
-    ) -> List[Document]:
-        """Return docs selected using the maximal marginal relevance.
-        Maximal marginal relevance optimizes for similarity to query AND diversity
-        among selected documents.
-        Args:
-            query: Text to look up documents similar to.
-            k: Number of Documents to return. Defaults to 5.
-            fetch_k: Number of Documents to fetch to pass to MMR algorithm.
-                     Defaults to 50
-            lambda_mult: Number between 0 and 1 that determines the degree
-                        of diversity among the results with 0 corresponding
-                        to maximum diversity and 1 to minimum diversity.
-                        Defaults to 0.5.
-            kwargs: any other querying variable in VectaraQueryConfig
-        Returns:
-            List of Documents selected by maximal marginal relevance.
-        """
-        kwargs["mmr_config"] = MMRConfig(
-            is_enabled=True, mmr_k=fetch_k, diversity_bias=1 - lambda_mult
-        )
-        return self.similarity_search(query, **kwargs)
     @classmethod
     def from_texts(
         cls: Type[Vectara],
         texts: List[str],
         embedding: Optional[Embeddings] = None,
         metadatas: Optional[List[dict]] = None,
         **kwargs: Any,
     ) -> Vectara:
         """Construct Vectara wrapper from raw documents.
         This is intended to be a quick way to get started.
         Example:
             .. code-block:: python
-                from langchain.vectorstores import Vectara
+                from langchain_community.vectorstores import Vectara
                 vectara = Vectara.from_texts(
                     texts,
                     vectara_customer_id=customer_id,
                     vectara_corpus_id=corpus_id,
                     vectara_api_key=api_key,
                 )
         """
         doc_metadata = kwargs.pop("doc_metadata", {})
         vectara = cls(**kwargs)
         vectara.add_texts(texts, metadatas, doc_metadata=doc_metadata, **kwargs)
@@ -467,31 +362,35 @@
         cls: Type[Vectara],
         files: List[str],
         embedding: Optional[Embeddings] = None,
         metadatas: Optional[List[dict]] = None,
         **kwargs: Any,
     ) -> Vectara:
         """Construct Vectara wrapper from raw documents.
         This is intended to be a quick way to get started.
         Example:
             .. code-block:: python
-                from langchain.vectorstores import Vectara
+                from langchain_community.vectorstores import Vectara
                 vectara = Vectara.from_files(
                     files_list,
                     vectara_customer_id=customer_id,
                     vectara_corpus_id=corpus_id,
                     vectara_api_key=api_key,
                 )
         """
         vectara = cls(**kwargs)
         vectara.add_files(files, metadatas)
         return vectara
+    def as_retriever(self, **kwargs: Any) -> VectaraRetriever:
+        tags = kwargs.pop("tags", None) or []
+        tags.extend(self._get_retriever_tags())
+        return VectaraRetriever(vectorstore=self, search_kwargs=kwargs, tags=tags)
 class VectaraRetriever(VectorStoreRetriever):
     """Retriever class for `Vectara`."""
     vectorstore: Vectara
     """Vectara vectorstore."""
     search_kwargs: dict = Field(
         default_factory=lambda: {
             "lambda_val": 0.0,
             "k": 5,
             "filter": "",
             "n_sentence_context": "2",

--- a/libs/core/langchain_core/beta/runnables/context.py
+++ b/libs/core/langchain_core/beta/runnables/context.py
@@ -5,21 +5,20 @@
 from itertools import groupby
 from typing import (
     Any,
     Awaitable,
     Callable,
     DefaultDict,
     Dict,
     List,
     Mapping,
     Optional,
-    Sequence,
     Type,
     TypeVar,
     Union,
 )
 from langchain_core.runnables.base import (
     Runnable,
     RunnableSerializable,
     coerce_to_runnable,
 )
 from langchain_core.runnables.config import RunnableConfig, patch_config
@@ -85,29 +84,30 @@
     )
     context_funcs: Dict[str, Callable[[], Any]] = {}
     for key, group in grouped_by_key.items():
         getters = [s for s in group if s[0].id.endswith(CONTEXT_CONFIG_SUFFIX_GET)]
         setters = [s for s in group if s[0].id.endswith(CONTEXT_CONFIG_SUFFIX_SET)]
         for dep in deps_by_key[key]:
             if key in deps_by_key[dep]:
                 raise ValueError(
                     f"Deadlock detected between context keys {key} and {dep}"
                 )
+        if len(getters) < 1:
+            raise ValueError(f"Expected at least one getter for context key {key}")
         if len(setters) != 1:
             raise ValueError(f"Expected exactly one setter for context key {key}")
         setter_idx = setters[0][1]
         if any(getter_idx < setter_idx for _, getter_idx in getters):
             raise ValueError(
                 f"Context setter for key {key} must be defined after all getters."
             )
-        if getters:
-            context_funcs[getters[0][0].id] = partial(getter, events[key], values)
+        context_funcs[getters[0][0].id] = partial(getter, events[key], values)
         context_funcs[setters[0][0].id] = partial(setter, events[key], values)
     return patch_config(config, configurable=context_funcs)
 def aconfig_with_context(
     config: RunnableConfig,
     steps: List[Runnable],
 ) -> RunnableConfig:
     """Asynchronously patch a runnable config with context getters and setters.
     Args:
         config: The runnable config.
         steps: The runnable steps.
@@ -124,22 +124,20 @@
         config: The runnable config.
         steps: The runnable steps.
     Returns:
         The patched runnable config.
     """
     return _config_with_context(config, steps, _setter, _getter, threading.Event)
 class ContextGet(RunnableSerializable):
     """Get a context value."""
     prefix: str = ""
     key: Union[str, List[str]]
-    def __str__(self) -> str:
-        return f"ContextGet({_print_keys(self.key)})"
     @property
     def ids(self) -> List[str]:
         prefix = self.prefix + "/" if self.prefix else ""
         keys = self.key if isinstance(self.key, list) else [self.key]
         return [
             f"{CONTEXT_CONFIG_PREFIX}{prefix}{k}{CONTEXT_CONFIG_SUFFIX_GET}"
             for k in keys
         ]
     @property
     def config_specs(self) -> List[ConfigurableFieldSpec]:
@@ -192,22 +190,20 @@
     ):
         if key is not None:
             kwargs[key] = value
         super().__init__(
             keys={
                 k: _coerce_set_value(v) if v is not None else None
                 for k, v in kwargs.items()
             },
             prefix=prefix,
         )
-    def __str__(self) -> str:
-        return f"ContextSet({_print_keys(list(self.keys.keys()))})"
     @property
     def ids(self) -> List[str]:
         prefix = self.prefix + "/" if self.prefix else ""
         return [
             f"{CONTEXT_CONFIG_PREFIX}{prefix}{key}{CONTEXT_CONFIG_SUFFIX_SET}"
             for key in self.keys
         ]
     @property
     def config_specs(self) -> List[ConfigurableFieldSpec]:
         mapper_config_specs = [
@@ -280,15 +276,10 @@
     def getter(self, key: Union[str, List[str]], /) -> ContextGet:
         return ContextGet(key=key, prefix=self.prefix)
     def setter(
         self,
         _key: Optional[str] = None,
         _value: Optional[SetValue] = None,
         /,
         **kwargs: SetValue,
     ) -> ContextSet:
         return ContextSet(_key, _value, prefix=self.prefix, **kwargs)
-def _print_keys(keys: Union[str, Sequence[str]]) -> str:
-    if isinstance(keys, str):
-        return f"'{keys}'"
-    else:
-        return ", ".join(f"'{k}'" for k in keys)

--- a/libs/core/langchain_core/callbacks/manager.py
+++ b/libs/core/langchain_core/callbacks/manager.py
@@ -1,18 +1,17 @@
 from __future__ import annotations
 import asyncio
 import functools
 import logging
 import uuid
 from concurrent.futures import ThreadPoolExecutor
 from contextlib import asynccontextmanager, contextmanager
-from contextvars import Context, copy_context
 from typing import (
     TYPE_CHECKING,
     Any,
     AsyncGenerator,
     Coroutine,
     Dict,
     Generator,
     List,
     Optional,
     Sequence,
@@ -227,68 +226,57 @@
                 if handler.raise_error:
                     raise e
     finally:
         if coros:
             try:
                 asyncio.get_running_loop()
                 loop_running = True
             except RuntimeError:
                 loop_running = False
             if loop_running:
-                with _executor_w_context(1) as executor:
+                with ThreadPoolExecutor(1) as executor:
                     executor.submit(_run_coros, coros).result()
             else:
                 _run_coros(coros)
-def _set_context(context: Context) -> None:
-    for var, value in context.items():
-        var.set(value)
-def _executor_w_context(max_workers: Optional[int] = None) -> ThreadPoolExecutor:
-    return ThreadPoolExecutor(
-        max_workers=max_workers,
-        initializer=_set_context,
-        initargs=(copy_context(),),
-    )
 def _run_coros(coros: List[Coroutine[Any, Any, Any]]) -> None:
     if hasattr(asyncio, "Runner"):
         with asyncio.Runner() as runner:
             for coro in coros:
                 runner.run(coro)
             while pending := asyncio.all_tasks(runner.get_loop()):
                 runner.run(asyncio.wait(pending))
     else:
         for coro in coros:
             asyncio.run(coro)
 async def _ahandle_event_for_handler(
-    executor: ThreadPoolExecutor,
     handler: BaseCallbackHandler,
     event_name: str,
     ignore_condition_name: Optional[str],
     *args: Any,
     **kwargs: Any,
 ) -> None:
     try:
         if ignore_condition_name is None or not getattr(handler, ignore_condition_name):
             event = getattr(handler, event_name)
             if asyncio.iscoroutinefunction(event):
                 await event(*args, **kwargs)
             else:
                 if handler.run_inline:
                     event(*args, **kwargs)
                 else:
                     await asyncio.get_event_loop().run_in_executor(
-                        executor, functools.partial(event, *args, **kwargs)
+                        None, functools.partial(event, *args, **kwargs)
                     )
     except NotImplementedError as e:
         if event_name == "on_chat_model_start":
             message_strings = [get_buffer_string(m) for m in args[1]]
             await _ahandle_event_for_handler(
-                executor,
                 handler,
                 "on_llm_start",
                 "ignore_llm",
                 args[0],
                 message_strings,
                 *args[2:],
                 **kwargs,
             )
         else:
             logger.warning(
@@ -312,39 +300,33 @@
     """Generic event handler for AsyncCallbackManager.
     Note: This function is used by langserve to handle events.
     Args:
         handlers: The list of handlers that will handle the event
         event_name: The name of the event (e.g., "on_llm_start")
         ignore_condition_name: Name of the attribute defined on handler
             that if True will cause the handler to be skipped for the given event
         *args: The arguments to pass to the event handler
         **kwargs: The keyword arguments to pass to the event handler
     """
-    with _executor_w_context() as executor:
-        for handler in [h for h in handlers if h.run_inline]:
-            await _ahandle_event_for_handler(
-                executor, handler, event_name, ignore_condition_name, *args, **kwargs
+    for handler in [h for h in handlers if h.run_inline]:
+        await _ahandle_event_for_handler(
+            handler, event_name, ignore_condition_name, *args, **kwargs
+        )
+    await asyncio.gather(
+        *(
+            _ahandle_event_for_handler(
+                handler, event_name, ignore_condition_name, *args, **kwargs
             )
-        await asyncio.gather(
-            *(
-                _ahandle_event_for_handler(
-                    executor,
-                    handler,
-                    event_name,
-                    ignore_condition_name,
-                    *args,
-                    **kwargs,
-                )
-                for handler in handlers
-                if not handler.run_inline
-            )
-        )
+            for handler in handlers
+            if not handler.run_inline
+        )
+    )
 BRM = TypeVar("BRM", bound="BaseRunManager")
 class BaseRunManager(RunManagerMixin):
     """Base class for run manager (a bound callback manager)."""
     def __init__(
         self,
         *,
         run_id: UUID,
         handlers: List[BaseCallbackHandler],
         inheritable_handlers: List[BaseCallbackHandler],
         parent_run_id: Optional[UUID] = None,

--- a/libs/core/langchain_core/chat_history.py
+++ b/libs/core/langchain_core/chat_history.py
@@ -1,13 +1,13 @@
 from __future__ import annotations
 from abc import ABC, abstractmethod
-from typing import List, Union
+from typing import List
 from langchain_core.messages import (
     AIMessage,
     BaseMessage,
     HumanMessage,
     get_buffer_string,
 )
 class BaseChatMessageHistory(ABC):
     """Abstract base class for storing chat message history.
     See `ChatMessageHistory` for default implementation.
     Example:
@@ -23,38 +23,32 @@
                def add_message(self, message: BaseMessage) -> None:
                    messages = self.messages.append(_message_to_dict(message))
                    with open(os.path.join(storage_path, session_id), 'w') as f:
                        json.dump(f, messages)
                def clear(self):
                    with open(os.path.join(storage_path, session_id), 'w') as f:
                        f.write("[]")
     """
     messages: List[BaseMessage]
     """A list of Messages stored in-memory."""
-    def add_user_message(self, message: Union[HumanMessage, str]) -> None:
+    def add_user_message(self, message: str) -> None:
         """Convenience method for adding a human message string to the store.
         Args:
-            message: The human message to add
+            message: The string contents of a human message.
         """
-        if isinstance(message, HumanMessage):
-            self.add_message(message)
-        else:
-            self.add_message(HumanMessage(content=message))
-    def add_ai_message(self, message: Union[AIMessage, str]) -> None:
+        self.add_message(HumanMessage(content=message))
+    def add_ai_message(self, message: str) -> None:
         """Convenience method for adding an AI message string to the store.
         Args:
-            message: The AI message to add.
+            message: The string contents of an AI message.
         """
-        if isinstance(message, AIMessage):
-            self.add_message(message)
-        else:
-            self.add_message(AIMessage(content=message))
+        self.add_message(AIMessage(content=message))
     @abstractmethod
     def add_message(self, message: BaseMessage) -> None:
         """Add a Message object to the store.
         Args:
             message: A BaseMessage object to store.
         """
         raise NotImplementedError()
     @abstractmethod
     def clear(self) -> None:
         """Remove all messages from the store"""

--- a/libs/core/langchain_core/env.py
+++ b/libs/core/langchain_core/env.py
@@ -1,13 +1,13 @@
 import platform
 from functools import lru_cache
 @lru_cache(maxsize=1)
 def get_runtime_environment() -> dict:
     """Get information about the LangChain runtime environment."""
     from langchain_core import __version__
     return {
         "library_version": __version__,
-        "library": "langchain-core",
+        "library": "langchain",
         "platform": platform.platform(),
         "runtime": "python",
         "runtime_version": platform.python_version(),
     }

--- a/libs/core/langchain_core/example_selectors/base.py
+++ b/libs/core/langchain_core/example_selectors/base.py
@@ -1,11 +1,11 @@
 """Interface for selecting examples to include in prompts."""
 from abc import ABC, abstractmethod
 from typing import Any, Dict, List
 class BaseExampleSelector(ABC):
     """Interface for selecting examples to include in prompts."""
     @abstractmethod
     def add_example(self, example: Dict[str, str]) -> Any:
-        """Add new example to store."""
+        """Add new example to store for a key."""
     @abstractmethod
     def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:
         """Select which examples to use based on the inputs."""

--- a/libs/core/langchain_core/language_models/__init__.py
+++ b/libs/core/langchain_core/language_models/__init__.py
@@ -1,20 +1,18 @@
 from langchain_core.language_models.base import (
     BaseLanguageModel,
     LanguageModelInput,
-    LanguageModelLike,
     LanguageModelOutput,
     get_tokenizer,
 )
 from langchain_core.language_models.chat_models import BaseChatModel, SimpleChatModel
 from langchain_core.language_models.llms import LLM, BaseLLM
 __all__ = [
     "BaseLanguageModel",
     "BaseChatModel",
     "SimpleChatModel",
     "BaseLLM",
     "LLM",
     "LanguageModelInput",
     "get_tokenizer",
     "LanguageModelOutput",
-    "LanguageModelLike",
 ]

--- a/libs/core/langchain_core/language_models/base.py
+++ b/libs/core/langchain_core/language_models/base.py
@@ -7,46 +7,44 @@
     List,
     Optional,
     Sequence,
     Set,
     TypeVar,
     Union,
 )
 from typing_extensions import TypeAlias
 from langchain_core.messages import AnyMessage, BaseMessage, get_buffer_string
 from langchain_core.prompt_values import PromptValue
-from langchain_core.runnables import Runnable, RunnableSerializable
+from langchain_core.runnables import RunnableSerializable
 from langchain_core.utils import get_pydantic_field_names
 if TYPE_CHECKING:
     from langchain_core.callbacks import Callbacks
     from langchain_core.outputs import LLMResult
 @lru_cache(maxsize=None)  # Cache the tokenizer
 def get_tokenizer() -> Any:
     try:
         from transformers import GPT2TokenizerFast  # type: ignore[import]
     except ImportError:
         raise ImportError(
             "Could not import transformers python package. "
             "This is needed in order to calculate get_token_ids. "
             "Please install it with `pip install transformers`."
         )
     return GPT2TokenizerFast.from_pretrained("gpt2")
 def _get_token_ids_default_method(text: str) -> List[int]:
     """Encode the text into token IDs."""
     tokenizer = get_tokenizer()
     return tokenizer.encode(text)
 LanguageModelInput = Union[PromptValue, str, List[BaseMessage]]
-LanguageModelOutput = Union[BaseMessage, str]
-LanguageModelLike = Runnable[LanguageModelInput, LanguageModelOutput]
-LanguageModelOutputVar = TypeVar("LanguageModelOutputVar", BaseMessage, str)
+LanguageModelOutput = TypeVar("LanguageModelOutput")
 class BaseLanguageModel(
-    RunnableSerializable[LanguageModelInput, LanguageModelOutputVar], ABC
+    RunnableSerializable[LanguageModelInput, LanguageModelOutput], ABC
 ):
     """Abstract base class for interfacing with language models.
     All language model wrappers inherit from BaseLanguageModel.
     Exposes three main methods:
     - generate_prompt: generate language model outputs for a sequence of prompt
         values. A prompt value is a model input that can be converted to any language
         model input format (string or messages).
     - predict: pass in a single string to a language model and return a string
         prediction.
     - predict_messages: pass in a sequence of BaseMessages (corresponding to a single

--- a/libs/core/langchain_core/language_models/chat_models.py
+++ b/libs/core/langchain_core/language_models/chat_models.py
@@ -225,22 +225,21 @@
     async def astream(
         self,
         input: LanguageModelInput,
         config: Optional[RunnableConfig] = None,
         *,
         stop: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> AsyncIterator[BaseMessageChunk]:
         if type(self)._astream == BaseChatModel._astream:
             yield cast(
-                BaseMessageChunk,
-                await self.ainvoke(input, config=config, stop=stop, **kwargs),
+                BaseMessageChunk, self.invoke(input, config=config, stop=stop, **kwargs)
             )
         else:
             config = config or {}
             messages = self._convert_input(input).to_messages()
             params = self._get_invocation_params(stop=stop, **kwargs)
             options = {"stop": stop, **kwargs}
             callback_manager = AsyncCallbackManager.configure(
                 config.get("callbacks"),
                 self.callbacks,
                 self.verbose,

--- a/libs/core/langchain_core/output_parsers/__init__.py
+++ b/libs/core/langchain_core/output_parsers/__init__.py
@@ -1,33 +1,28 @@
 from langchain_core.output_parsers.base import (
     BaseGenerationOutputParser,
     BaseLLMOutputParser,
     BaseOutputParser,
 )
-from langchain_core.output_parsers.json import JsonOutputParser, SimpleJsonOutputParser
 from langchain_core.output_parsers.list import (
     CommaSeparatedListOutputParser,
     ListOutputParser,
     MarkdownListOutputParser,
     NumberedListOutputParser,
 )
 from langchain_core.output_parsers.string import StrOutputParser
 from langchain_core.output_parsers.transform import (
     BaseCumulativeTransformOutputParser,
     BaseTransformOutputParser,
 )
-from langchain_core.output_parsers.xml import XMLOutputParser
 __all__ = [
     "BaseLLMOutputParser",
     "BaseGenerationOutputParser",
     "BaseOutputParser",
     "ListOutputParser",
     "CommaSeparatedListOutputParser",
     "NumberedListOutputParser",
     "MarkdownListOutputParser",
     "StrOutputParser",
     "BaseTransformOutputParser",
     "BaseCumulativeTransformOutputParser",
-    "SimpleJsonOutputParser",
-    "XMLOutputParser",
-    "JsonOutputParser",
 ]

--- a/libs/core/langchain_core/output_parsers/base.py
+++ b/libs/core/langchain_core/output_parsers/base.py
@@ -68,21 +68,21 @@
             )
         else:
             return self._call_with_config(
                 lambda inner_input: self.parse_result([Generation(text=inner_input)]),
                 input,
                 config,
                 run_type="parser",
             )
     async def ainvoke(
         self,
-        input: Union[str, BaseMessage],
+        input: str | BaseMessage,
         config: Optional[RunnableConfig] = None,
         **kwargs: Optional[Any],
     ) -> T:
         if isinstance(input, BaseMessage):
             return await self._acall_with_config(
                 lambda inner_input: self.aparse_result(
                     [ChatGeneration(message=inner_input)]
                 ),
                 input,
                 config,
@@ -145,21 +145,21 @@
             )
         else:
             return self._call_with_config(
                 lambda inner_input: self.parse_result([Generation(text=inner_input)]),
                 input,
                 config,
                 run_type="parser",
             )
     async def ainvoke(
         self,
-        input: Union[str, BaseMessage],
+        input: str | BaseMessage,
         config: Optional[RunnableConfig] = None,
         **kwargs: Optional[Any],
     ) -> T:
         if isinstance(input, BaseMessage):
             return await self._acall_with_config(
                 lambda inner_input: self.aparse_result(
                     [ChatGeneration(message=inner_input)]
                 ),
                 input,
                 config,

--- a/libs/core/langchain_core/output_parsers/format_instructions.py
+++ b//dev/null
@@ -1,7 +0,0 @@
-JSON_FORMAT_INSTRUCTIONS = """The output should be formatted as a JSON instance that conforms to the JSON schema below.
-As an example, for the schema {{"properties": {{"foo": {{"title": "Foo", "description": "a list of strings", "type": "array", "items": {{"type": "string"}}}}}}, "required": ["foo"]}}
-the object {{"foo": ["bar", "baz"]}} is a well-formatted instance of the schema. The object {{"properties": {{"foo": ["bar", "baz"]}}}} is not well-formatted.
-Here is the output schema:
-```
-{schema}
-```"""

--- a/libs/core/langchain_core/output_parsers/json.py
+++ b//dev/null
@@ -1,153 +0,0 @@
-from __future__ import annotations
-import json
-import re
-from json import JSONDecodeError
-from typing import Any, Callable, List, Optional, Type
-import jsonpatch  # type: ignore[import]
-from langchain_core.exceptions import OutputParserException
-from langchain_core.output_parsers.format_instructions import JSON_FORMAT_INSTRUCTIONS
-from langchain_core.output_parsers.transform import BaseCumulativeTransformOutputParser
-from langchain_core.pydantic_v1 import BaseModel
-def _replace_new_line(match: re.Match[str]) -> str:
-    value = match.group(2)
-    value = re.sub(r"\n", r"\\n", value)
-    value = re.sub(r"\r", r"\\r", value)
-    value = re.sub(r"\t", r"\\t", value)
-    value = re.sub(r'(?<!\\)"', r"\"", value)
-    return match.group(1) + value + match.group(3)
-def _custom_parser(multiline_string: str) -> str:
-    """
-    The LLM response for `action_input` may be a multiline
-    string containing unescaped newlines, tabs or quotes. This function
-    replaces those characters with their escaped counterparts.
-    (newlines in JSON must be double-escaped: `\\n`)
-    """
-    if isinstance(multiline_string, (bytes, bytearray)):
-        multiline_string = multiline_string.decode()
-    multiline_string = re.sub(
-        r'("action_input"\:\s*")(.*)(")',
-        _replace_new_line,
-        multiline_string,
-        flags=re.DOTALL,
-    )
-    return multiline_string
-def parse_partial_json(s: str, *, strict: bool = False) -> Any:
-    """Parse a JSON string that may be missing closing braces.
-    Args:
-        s: The JSON string to parse.
-        strict: Whether to use strict parsing. Defaults to False.
-    Returns:
-        The parsed JSON object as a Python dictionary.
-    """
-    try:
-        return json.loads(s, strict=strict)
-    except json.JSONDecodeError:
-        pass
-    new_s = ""
-    stack = []
-    is_inside_string = False
-    escaped = False
-    for char in s:
-        if is_inside_string:
-            if char == '"' and not escaped:
-                is_inside_string = False
-            elif char == "\n" and not escaped:
-                char = "\\n"  # Replace the newline character with the escape sequence.
-            elif char == "\\":
-                escaped = not escaped
-            else:
-                escaped = False
-        else:
-            if char == '"':
-                is_inside_string = True
-                escaped = False
-            elif char == "{":
-                stack.append("}")
-            elif char == "[":
-                stack.append("]")
-            elif char == "}" or char == "]":
-                if stack and stack[-1] == char:
-                    stack.pop()
-                else:
-                    return None
-        new_s += char
-    if is_inside_string:
-        new_s += '"'
-    for closing_char in reversed(stack):
-        new_s += closing_char
-    try:
-        return json.loads(new_s, strict=strict)
-    except json.JSONDecodeError:
-        return None
-def parse_json_markdown(
-    json_string: str, *, parser: Callable[[str], Any] = parse_partial_json
-) -> dict:
-    """
-    Parse a JSON string from a Markdown string.
-    Args:
-        json_string: The Markdown string.
-    Returns:
-        The parsed JSON object as a Python dictionary.
-    """
-    match = re.search(r"```(json)?(.*)```", json_string, re.DOTALL)
-    if match is None:
-        json_str = json_string
-    else:
-        json_str = match.group(2)
-    json_str = json_str.strip()
-    json_str = _custom_parser(json_str)
-    parsed = parser(json_str)
-    return parsed
-def parse_and_check_json_markdown(text: str, expected_keys: List[str]) -> dict:
-    """
-    Parse a JSON string from a Markdown string and check that it
-    contains the expected keys.
-    Args:
-        text: The Markdown string.
-        expected_keys: The expected keys in the JSON string.
-    Returns:
-        The parsed JSON object as a Python dictionary.
-    """
-    try:
-        json_obj = parse_json_markdown(text)
-    except json.JSONDecodeError as e:
-        raise OutputParserException(f"Got invalid JSON object. Error: {e}")
-    for key in expected_keys:
-        if key not in json_obj:
-            raise OutputParserException(
-                f"Got invalid return object. Expected key `{key}` "
-                f"to be present, but got {json_obj}"
-            )
-    return json_obj
-class JsonOutputParser(BaseCumulativeTransformOutputParser[Any]):
-    """Parse the output of an LLM call to a JSON object.
-    When used in streaming mode, it will yield partial JSON objects containing
-    all the keys that have been returned so far.
-    In streaming, if `diff` is set to `True`, yields JSONPatch operations
-    describing the difference between the previous and the current object.
-    """
-    pydantic_object: Optional[Type[BaseModel]] = None
-    def _diff(self, prev: Optional[Any], next: Any) -> Any:
-        return jsonpatch.make_patch(prev, next).patch
-    def parse(self, text: str) -> Any:
-        text = text.strip()
-        try:
-            return parse_json_markdown(text.strip())
-        except JSONDecodeError as e:
-            raise OutputParserException(f"Invalid json output: {text}") from e
-    def get_format_instructions(self) -> str:
-        if self.pydantic_object is None:
-            return "Return a JSON object."
-        else:
-            schema = self.pydantic_object.schema()
-            reduced_schema = schema
-            if "title" in reduced_schema:
-                del reduced_schema["title"]
-            if "type" in reduced_schema:
-                del reduced_schema["type"]
-            schema_str = json.dumps(reduced_schema)
-            return JSON_FORMAT_INSTRUCTIONS.format(schema=schema_str)
-    @property
-    def _type(self) -> str:
-        return "simple_json_output_parser"
-SimpleJsonOutputParser = JsonOutputParser

--- a/libs/core/langchain_core/output_parsers/list.py
+++ b/libs/core/langchain_core/output_parsers/list.py
@@ -1,86 +1,23 @@
 from __future__ import annotations
 import re
 from abc import abstractmethod
-from collections import deque
-from typing import AsyncIterator, Deque, Iterator, List, TypeVar, Union
-from langchain_core.messages import BaseMessage
-from langchain_core.output_parsers.transform import BaseTransformOutputParser
-T = TypeVar("T")
-def droplastn(iter: Iterator[T], n: int) -> Iterator[T]:
-    """Drop the last n elements of an iterator."""
-    buffer: Deque[T] = deque()
-    for item in iter:
-        buffer.append(item)
-        if len(buffer) > n:
-            yield buffer.popleft()
-class ListOutputParser(BaseTransformOutputParser[List[str]]):
+from typing import List
+from langchain_core.output_parsers.base import BaseOutputParser
+class ListOutputParser(BaseOutputParser[List[str]]):
     """Parse the output of an LLM call to a list."""
     @property
     def _type(self) -> str:
         return "list"
     @abstractmethod
     def parse(self, text: str) -> List[str]:
         """Parse the output of an LLM call."""
-    def parse_iter(self, text: str) -> Iterator[re.Match]:
-        """Parse the output of an LLM call."""
-        raise NotImplementedError
-    def _transform(
-        self, input: Iterator[Union[str, BaseMessage]]
-    ) -> Iterator[List[str]]:
-        buffer = ""
-        for chunk in input:
-            if isinstance(chunk, BaseMessage):
-                chunk_content = chunk.content
-                if not isinstance(chunk_content, str):
-                    continue
-                chunk = chunk_content
-            buffer += chunk
-            try:
-                done_idx = 0
-                for m in droplastn(self.parse_iter(buffer), 1):
-                    done_idx = m.end()
-                    yield [m.group(1)]
-                buffer = buffer[done_idx:]
-            except NotImplementedError:
-                parts = self.parse(buffer)
-                if len(parts) > 1:
-                    for part in parts[:-1]:
-                        yield [part]
-                    buffer = parts[-1]
-        for part in self.parse(buffer):
-            yield [part]
-    async def _atransform(
-        self, input: AsyncIterator[Union[str, BaseMessage]]
-    ) -> AsyncIterator[List[str]]:
-        buffer = ""
-        async for chunk in input:
-            if isinstance(chunk, BaseMessage):
-                chunk_content = chunk.content
-                if not isinstance(chunk_content, str):
-                    continue
-                chunk = chunk_content
-            buffer += chunk
-            try:
-                done_idx = 0
-                for m in droplastn(self.parse_iter(buffer), 1):
-                    done_idx = m.end()
-                    yield [m.group(1)]
-                buffer = buffer[done_idx:]
-            except NotImplementedError:
-                parts = self.parse(buffer)
-                if len(parts) > 1:
-                    for part in parts[:-1]:
-                        yield [part]
-                    buffer = parts[-1]
-        for part in self.parse(buffer):
-            yield [part]
 class CommaSeparatedListOutputParser(ListOutputParser):
     """Parse the output of an LLM call to a comma-separated list."""
     @classmethod
     def is_lc_serializable(cls) -> bool:
         return True
     @classmethod
     def get_lc_namespace(cls) -> List[str]:
         """Get the namespace of the langchain object."""
         return ["langchain", "output_parsers", "list"]
     def get_format_instructions(self) -> str:
@@ -89,39 +26,34 @@
             "eg: `foo, bar, baz`"
         )
     def parse(self, text: str) -> List[str]:
         """Parse the output of an LLM call."""
         return text.strip().split(", ")
     @property
     def _type(self) -> str:
         return "comma-separated-list"
 class NumberedListOutputParser(ListOutputParser):
     """Parse a numbered list."""
-    pattern = r"\d+\.\s([^\n]+)"
     def get_format_instructions(self) -> str:
         return (
             "Your response should be a numbered list with each item on a new line. "
             "For example: \n\n1. foo\n\n2. bar\n\n3. baz"
         )
     def parse(self, text: str) -> List[str]:
         """Parse the output of an LLM call."""
-        return re.findall(self.pattern, text)
-    def parse_iter(self, text: str) -> Iterator[re.Match]:
-        """Parse the output of an LLM call."""
-        return re.finditer(self.pattern, text)
+        pattern = r"\d+\.\s([^\n]+)"
+        matches = re.findall(pattern, text)
+        return matches
     @property
     def _type(self) -> str:
         return "numbered-list"
 class MarkdownListOutputParser(ListOutputParser):
     """Parse a markdown list."""
-    pattern = r"-\s([^\n]+)"
     def get_format_instructions(self) -> str:
         return "Your response should be a markdown list, " "eg: `- foo\n- bar\n- baz`"
     def parse(self, text: str) -> List[str]:
         """Parse the output of an LLM call."""
-        return re.findall(self.pattern, text)
-    def parse_iter(self, text: str) -> Iterator[re.Match]:
-        """Parse the output of an LLM call."""
-        return re.finditer(self.pattern, text)
+        pattern = r"-\s([^\n]+)"
+        return re.findall(pattern, text)
     @property
     def _type(self) -> str:
         return "markdown-list"

--- a/libs/core/langchain_core/output_parsers/xml.py
+++ b//dev/null
@@ -1,119 +0,0 @@
-import re
-import xml.etree.ElementTree as ET
-from typing import Any, AsyncIterator, Dict, Iterator, List, Optional, Union
-from langchain_core.messages import BaseMessage
-from langchain_core.output_parsers.transform import BaseTransformOutputParser
-from langchain_core.runnables.utils import AddableDict
-XML_FORMAT_INSTRUCTIONS = """The output should be formatted as a XML file.
-1. Output should conform to the tags below. 
-2. If tags are not given, make them on your own.
-3. Remember to always open and close all the tags.
-As an example, for the tags ["foo", "bar", "baz"]:
-1. String "<foo>\n   <bar>\n      <baz></baz>\n   </bar>\n</foo>" is a well-formatted instance of the schema. 
-2. String "<foo>\n   <bar>\n   </foo>" is a badly-formatted instance.
-3. String "<foo>\n   <tag>\n   </tag>\n</foo>" is a badly-formatted instance.
-Here are the output tags:
-```
-{tags}
-```"""  # noqa: E501
-class XMLOutputParser(BaseTransformOutputParser):
-    """Parse an output using xml format."""
-    tags: Optional[List[str]] = None
-    encoding_matcher: re.Pattern = re.compile(
-        r"<([^>]*encoding[^>]*)>\n(.*)", re.MULTILINE | re.DOTALL
-    )
-    def get_format_instructions(self) -> str:
-        return XML_FORMAT_INSTRUCTIONS.format(tags=self.tags)
-    def parse(self, text: str) -> Dict[str, List[Any]]:
-        match = re.search(r"```(xml)?(.*)```", text, re.DOTALL)
-        if match is not None:
-            text = match.group(2)
-        encoding_match = self.encoding_matcher.search(text)
-        if encoding_match:
-            text = encoding_match.group(2)
-        text = text.strip()
-        if (text.startswith("<") or text.startswith("\n<")) and (
-            text.endswith(">") or text.endswith(">\n")
-        ):
-            root = ET.fromstring(text)
-            return self._root_to_dict(root)
-        else:
-            raise ValueError(f"Could not parse output: {text}")
-    def _transform(
-        self, input: Iterator[Union[str, BaseMessage]]
-    ) -> Iterator[AddableDict]:
-        xml_start_re = re.compile(r"<[a-zA-Z:_]")
-        parser = ET.XMLPullParser(["start", "end"])
-        xml_started = False
-        current_path: List[str] = []
-        current_path_has_children = False
-        buffer = ""
-        for chunk in input:
-            if isinstance(chunk, BaseMessage):
-                chunk_content = chunk.content
-                if not isinstance(chunk_content, str):
-                    continue
-                chunk = chunk_content
-            buffer += chunk
-            if not xml_started:
-                if match := xml_start_re.search(buffer):
-                    buffer = buffer[match.start() :]
-                    xml_started = True
-                else:
-                    continue
-            parser.feed(buffer)
-            buffer = ""
-            for event, elem in parser.read_events():
-                if event == "start":
-                    current_path.append(elem.tag)
-                    current_path_has_children = False
-                elif event == "end":
-                    current_path.pop()
-                    if not current_path_has_children:
-                        yield nested_element(current_path, elem)
-                    if current_path:
-                        current_path_has_children = True
-                    else:
-                        xml_started = False
-        parser.close()
-    async def _atransform(
-        self, input: AsyncIterator[Union[str, BaseMessage]]
-    ) -> AsyncIterator[AddableDict]:
-        parser = ET.XMLPullParser(["start", "end"])
-        current_path: List[str] = []
-        current_path_has_children = False
-        async for chunk in input:
-            if isinstance(chunk, BaseMessage):
-                chunk_content = chunk.content
-                if not isinstance(chunk_content, str):
-                    continue
-                chunk = chunk_content
-            parser.feed(chunk)
-            for event, elem in parser.read_events():
-                if event == "start":
-                    current_path.append(elem.tag)
-                    current_path_has_children = False
-                elif event == "end":
-                    current_path.pop()
-                    if not current_path_has_children:
-                        yield nested_element(current_path, elem)
-                    current_path_has_children = True
-        parser.close()
-    def _root_to_dict(self, root: ET.Element) -> Dict[str, List[Any]]:
-        """Converts xml tree to python dictionary."""
-        result: Dict[str, List[Any]] = {root.tag: []}
-        for child in root:
-            if len(child) == 0:
-                result[root.tag].append({child.tag: child.text})
-            else:
-                result[root.tag].append(self._root_to_dict(child))
-        return result
-    @property
-    def _type(self) -> str:
-        return "xml"
-def nested_element(path: List[str], elem: ET.Element) -> Any:
-    """Get nested element from path."""
-    if len(path) == 0:
-        return AddableDict({elem.tag: elem.text})
-    else:
-        return AddableDict({path[0]: [nested_element(path[1:], elem)]})

--- a/libs/core/langchain_core/prompts/base.py
+++ b/libs/core/langchain_core/prompts/base.py
@@ -51,45 +51,46 @@
     def OutputType(self) -> Any:
         return Union[StringPromptValue, ChatPromptValueConcrete]
     def get_input_schema(
         self, config: Optional[RunnableConfig] = None
     ) -> Type[BaseModel]:
         return create_model(  # type: ignore[call-overload]
             "PromptInput",
             **{k: (self.input_types.get(k, str), None) for k in self.input_variables},
         )
     def _format_prompt_with_error_handling(self, inner_input: Dict) -> PromptValue:
-        if not isinstance(inner_input, dict):
+        try:
+            input_dict = {key: inner_input[key] for key in self.input_variables}
+        except TypeError as e:
             raise TypeError(
                 f"Expected mapping type as input to {self.__class__.__name__}. "
                 f"Received {type(inner_input)}."
-            )
-        missing = set(self.input_variables).difference(inner_input)
-        if missing:
+            ) from e
+        except KeyError as e:
             raise KeyError(
-                f"Input to {self.__class__.__name__} is missing variables {missing}. "
+                f"Input to {self.__class__.__name__} is missing variable {e}. "
                 f" Expected: {self.input_variables}"
                 f" Received: {list(inner_input.keys())}"
-            )
-        return self.format_prompt(**inner_input)
+            ) from e
+        return self.format_prompt(**input_dict)
     def invoke(
         self, input: Dict, config: Optional[RunnableConfig] = None
     ) -> PromptValue:
         return self._call_with_config(
             self._format_prompt_with_error_handling,
             input,
             config,
             run_type="prompt",
         )
     @abstractmethod
     def format_prompt(self, **kwargs: Any) -> PromptValue:
-        """Create Prompt Value."""
+        """Create Chat Messages."""
     @root_validator()
     def validate_variable_names(cls, values: Dict) -> Dict:
         """Validate variable names do not include restricted names."""
         if "stop" in values["input_variables"]:
             raise ValueError(
                 "Cannot have an input variable named 'stop', as it is used internally,"
                 " please rename."
             )
         if "stop" in values["partial_variables"]:
             raise ValueError(

--- a/libs/core/langchain_core/prompts/chat.py
+++ b/libs/core/langchain_core/prompts/chat.py
@@ -62,60 +62,53 @@
             other: Another prompt template.
         Returns:
             Combined prompt template.
         """
         prompt = ChatPromptTemplate(messages=[self])
         return prompt + other
 class MessagesPlaceholder(BaseMessagePromptTemplate):
     """Prompt template that assumes variable is already list of messages."""
     variable_name: str
     """Name of variable to use as messages."""
-    optional: bool = False
-    @classmethod
-    def get_lc_namespace(cls) -> List[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "prompts", "chat"]
-    def __init__(self, variable_name: str, *, optional: bool = False, **kwargs: Any):
-        return super().__init__(
-            variable_name=variable_name, optional=optional, **kwargs
-        )
+    @classmethod
+    def get_lc_namespace(cls) -> List[str]:
+        """Get the namespace of the langchain object."""
+        return ["langchain", "prompts", "chat"]
+    def __init__(self, variable_name: str, **kwargs: Any):
+        return super().__init__(variable_name=variable_name, **kwargs)
     def format_messages(self, **kwargs: Any) -> List[BaseMessage]:
         """Format messages from kwargs.
         Args:
             **kwargs: Keyword arguments to use for formatting.
         Returns:
             List of BaseMessage.
         """
-        value = (
-            kwargs.get(self.variable_name, [])
-            if self.optional
-            else kwargs[self.variable_name]
-        )
+        value = kwargs[self.variable_name]
         if not isinstance(value, list):
             raise ValueError(
                 f"variable {self.variable_name} should be a list of base messages, "
                 f"got {value}"
             )
         for v in value:
             if not isinstance(v, BaseMessage):
                 raise ValueError(
                     f"variable {self.variable_name} should be a list of base messages,"
                     f" got {value}"
                 )
         return value
     @property
     def input_variables(self) -> List[str]:
         """Input variables for this prompt template.
         Returns:
             List of input variable names.
         """
-        return [self.variable_name] if not self.optional else []
+        return [self.variable_name]
 MessagePromptTemplateT = TypeVar(
     "MessagePromptTemplateT", bound="BaseStringMessagePromptTemplate"
 )
 """Type variable for message prompt templates."""
 class BaseStringMessagePromptTemplate(BaseMessagePromptTemplate, ABC):
     """Base class for message prompt templates that use a string prompt template."""
     prompt: StringPromptTemplate
     """String prompt template."""
     additional_kwargs: dict = Field(default_factory=dict)
     """Additional keyword arguments to pass to the prompt template."""
@@ -475,21 +468,26 @@
             list of formatted messages
         """
         kwargs = self._merge_partial_and_user_variables(**kwargs)
         result = []
         for message_template in self.messages:
             if isinstance(message_template, BaseMessage):
                 result.extend([message_template])
             elif isinstance(
                 message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)
             ):
-                message = message_template.format_messages(**kwargs)
+                rel_params = {
+                    k: v
+                    for k, v in kwargs.items()
+                    if k in message_template.input_variables
+                }
+                message = message_template.format_messages(**rel_params)
                 result.extend(message)
             else:
                 raise ValueError(f"Unexpected input: {message_template}")
         return result
     def partial(self, **kwargs: Union[str, Callable[[], str]]) -> ChatPromptTemplate:
         """Get a new ChatPromptTemplate with some input variables already filled in.
         Args:
             **kwargs: keyword arguments to use for filling in template variables. Ought
                         to be a subset of the input variables.
         Returns:

--- a/libs/core/langchain_core/retrievers.py
+++ b/libs/core/langchain_core/retrievers.py
@@ -1,31 +1,27 @@
 from __future__ import annotations
 import asyncio
 import warnings
 from abc import ABC, abstractmethod
 from functools import partial
 from inspect import signature
 from typing import TYPE_CHECKING, Any, Dict, List, Optional
 from langchain_core.documents import Document
 from langchain_core.load.dump import dumpd
-from langchain_core.runnables import Runnable, RunnableConfig, RunnableSerializable
+from langchain_core.runnables import RunnableConfig, RunnableSerializable
 if TYPE_CHECKING:
     from langchain_core.callbacks.manager import (
         AsyncCallbackManagerForRetrieverRun,
         CallbackManagerForRetrieverRun,
         Callbacks,
     )
-RetrieverInput = str
-RetrieverOutput = List[Document]
-RetrieverLike = Runnable[RetrieverInput, RetrieverOutput]
-RetrieverOutputLike = Runnable[Any, RetrieverOutput]
-class BaseRetriever(RunnableSerializable[RetrieverInput, RetrieverOutput], ABC):
+class BaseRetriever(RunnableSerializable[str, List[Document]], ABC):
     """Abstract base class for a Document retrieval system.
     A retrieval system is defined as something that can take string queries and return
         the most 'relevant' Documents from some source.
     Example:
         .. code-block:: python
             class TFIDFRetriever(BaseRetriever, BaseModel):
                 vectorizer: Any
                 docs: List[Document]
                 tfidf_array: Any
                 k: int = 4

--- a/libs/core/langchain_core/runnables/__init__.py
+++ b/libs/core/langchain_core/runnables/__init__.py
@@ -19,25 +19,21 @@
     RunnableSequence,
     RunnableSerializable,
 )
 from langchain_core.runnables.branch import RunnableBranch
 from langchain_core.runnables.config import (
     RunnableConfig,
     get_config_list,
     patch_config,
 )
 from langchain_core.runnables.fallbacks import RunnableWithFallbacks
-from langchain_core.runnables.passthrough import (
-    RunnableAssign,
-    RunnablePassthrough,
-    RunnablePick,
-)
+from langchain_core.runnables.passthrough import RunnablePassthrough
 from langchain_core.runnables.router import RouterInput, RouterRunnable
 from langchain_core.runnables.utils import (
     AddableDict,
     ConfigurableField,
     ConfigurableFieldMultiOption,
     ConfigurableFieldSingleOption,
     aadd,
     add,
 )
 __all__ = [
@@ -51,18 +47,16 @@
     "Runnable",
     "RunnableSerializable",
     "RunnableBinding",
     "RunnableBranch",
     "RunnableConfig",
     "RunnableGenerator",
     "RunnableLambda",
     "RunnableMap",
     "RunnableParallel",
     "RunnablePassthrough",
-    "RunnableAssign",
-    "RunnablePick",
     "RunnableSequence",
     "RunnableWithFallbacks",
     "get_config_list",
     "aadd",
     "add",
 ]

--- a/libs/core/langchain_core/runnables/base.py
+++ b/libs/core/langchain_core/runnables/base.py
@@ -25,65 +25,61 @@
     Tuple,
     Type,
     TypeVar,
     Union,
     cast,
     overload,
 )
 from typing_extensions import Literal, get_args
 from langchain_core.load.dump import dumpd, dumps
 from langchain_core.load.serializable import Serializable
-from langchain_core.pydantic_v1 import BaseConfig, BaseModel, Field, create_model
+from langchain_core.pydantic_v1 import BaseModel, Field, create_model
 from langchain_core.runnables.config import (
     RunnableConfig,
     acall_func_with_variable_args,
     call_func_with_variable_args,
     ensure_config,
     get_async_callback_manager_for_config,
     get_callback_manager_for_config,
     get_config_list,
     get_executor_for_config,
     merge_configs,
     patch_config,
 )
-from langchain_core.runnables.graph import Graph
 from langchain_core.runnables.utils import (
     AddableDict,
     AnyConfigurableField,
     ConfigurableField,
     ConfigurableFieldSpec,
     Input,
     Output,
     accepts_config,
     accepts_run_manager,
     gather_with_concurrency,
     get_function_first_arg_dict_keys,
-    get_function_nonlocals,
     get_lambda_source,
     get_unique_config_specs,
     indent_lines_after_first,
 )
 from langchain_core.utils.aiter import atee, py_anext
 from langchain_core.utils.iter import safetee
 if TYPE_CHECKING:
     from langchain_core.callbacks.manager import (
         AsyncCallbackManagerForChainRun,
         CallbackManagerForChainRun,
     )
     from langchain_core.runnables.fallbacks import (
         RunnableWithFallbacks as RunnableWithFallbacksT,
     )
     from langchain_core.tracers.log_stream import RunLog, RunLogPatch
     from langchain_core.tracers.root_listeners import Listener
 Other = TypeVar("Other")
-class _SchemaConfig(BaseConfig):
-    arbitrary_types_allowed = True
 class Runnable(Generic[Input, Output], ABC):
     """A unit of work that can be invoked, batched, streamed, transformed and composed.
      Key Methods
      ===========
     * invoke/ainvoke: Transforms a single input into an output.
     * batch/abatch: Efficiently transforms multiple inputs into outputs.
     * stream/astream: Streams output from a single input as it's produced.
     * astream_log: Streams output and selected intermediate results from an input.
     Built-in optimizations:
     * Batch: By default, batch runs invoke() in parallel using a thread pool executor.
@@ -156,54 +152,40 @@
             set_debug(True)
     Alternatively, you can pass existing or custom callbacks to any given chain:
         .. code-block:: python
             from langchain_core.tracers import ConsoleCallbackHandler
             chain.invoke(
                 ...,
                 config={'callbacks': [ConsoleCallbackHandler()]}
             )
     For a UI (and much more) checkout LangSmith: https://docs.smith.langchain.com/
     """
-    name: Optional[str] = None
-    """The name of the runnable. Used for debugging and tracing."""
-    def get_name(
-        self, suffix: Optional[str] = None, *, name: Optional[str] = None
-    ) -> str:
-        """Get the name of the runnable."""
-        name = name or self.name or self.__class__.__name__
-        if suffix:
-            if name[0].isupper():
-                return name + suffix.title()
-            else:
-                return name + "_" + suffix.lower()
-        else:
-            return name
     @property
     def InputType(self) -> Type[Input]:
         """The type of input this runnable accepts specified as a type annotation."""
         for cls in self.__class__.__orig_bases__:  # type: ignore[attr-defined]
             type_args = get_args(cls)
             if type_args and len(type_args) == 2:
                 return type_args[0]
         raise TypeError(
-            f"Runnable {self.get_name()} doesn't have an inferable InputType. "
+            f"Runnable {self.__class__.__name__} doesn't have an inferable InputType. "
             "Override the InputType property to specify the input type."
         )
     @property
     def OutputType(self) -> Type[Output]:
         """The type of output this runnable produces specified as a type annotation."""
         for cls in self.__class__.__orig_bases__:  # type: ignore[attr-defined]
             type_args = get_args(cls)
             if type_args and len(type_args) == 2:
                 return type_args[1]
         raise TypeError(
-            f"Runnable {self.get_name()} doesn't have an inferable OutputType. "
+            f"Runnable {self.__class__.__name__} doesn't have an inferable OutputType. "
             "Override the OutputType property to specify the output type."
         )
     @property
     def input_schema(self) -> Type[BaseModel]:
         """The type of input this runnable accepts specified as a pydantic model."""
         return self.get_input_schema()
     def get_input_schema(
         self, config: Optional[RunnableConfig] = None
     ) -> Type[BaseModel]:
         """Get a pydantic model that can be used to validate input to the runnable.
@@ -213,23 +195,21 @@
         This method allows to get an input schema for a specific configuration.
         Args:
             config: A config to use when generating the schema.
         Returns:
             A pydantic model that can be used to validate input.
         """
         root_type = self.InputType
         if inspect.isclass(root_type) and issubclass(root_type, BaseModel):
             return root_type
         return create_model(
-            self.get_name("Input"),
-            __root__=(root_type, None),
-            __config__=_SchemaConfig,
+            self.__class__.__name__ + "Input", __root__=(root_type, None)
         )
     @property
     def output_schema(self) -> Type[BaseModel]:
         """The type of output this runnable produces specified as a pydantic model."""
         return self.get_output_schema()
     def get_output_schema(
         self, config: Optional[RunnableConfig] = None
     ) -> Type[BaseModel]:
         """Get a pydantic model that can be used to validate output to the runnable.
         Runnables that leverage the configurable_fields and configurable_alternatives
@@ -238,127 +218,89 @@
         This method allows to get an output schema for a specific configuration.
         Args:
             config: A config to use when generating the schema.
         Returns:
             A pydantic model that can be used to validate output.
         """
         root_type = self.OutputType
         if inspect.isclass(root_type) and issubclass(root_type, BaseModel):
             return root_type
         return create_model(
-            self.get_name("Output"),
-            __root__=(root_type, None),
-            __config__=_SchemaConfig,
+            self.__class__.__name__ + "Output", __root__=(root_type, None)
         )
     @property
     def config_specs(self) -> List[ConfigurableFieldSpec]:
         """List configurable fields for this runnable."""
         return []
     def config_schema(
         self, *, include: Optional[Sequence[str]] = None
     ) -> Type[BaseModel]:
         """The type of config this runnable accepts specified as a pydantic model.
         To mark a field as configurable, see the `configurable_fields`
         and `configurable_alternatives` methods.
         Args:
             include: A list of fields to include in the config schema.
         Returns:
             A pydantic model that can be used to validate config.
         """
+        class _Config:
+            arbitrary_types_allowed = True
         include = include or []
         config_specs = self.config_specs
         configurable = (
             create_model(  # type: ignore[call-overload]
                 "Configurable",
                 **{
                     spec.id: (
                         spec.annotation,
                         Field(
                             spec.default, title=spec.name, description=spec.description
                         ),
                     )
                     for spec in config_specs
                 },
-                __config__=_SchemaConfig,
             )
             if config_specs
             else None
         )
         return create_model(  # type: ignore[call-overload]
-            self.get_name("Config"),
-            __config__=_SchemaConfig,
+            self.__class__.__name__ + "Config",
+            __config__=_Config,
             **({"configurable": (configurable, None)} if configurable else {}),
             **{
                 field_name: (field_type, None)
                 for field_name, field_type in RunnableConfig.__annotations__.items()
                 if field_name in [i for i in include if i != "configurable"]
             },
         )
-    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
-        """Return a graph representation of this runnable."""
-        from langchain_core.runnables.graph import Graph
-        graph = Graph()
-        input_node = graph.add_node(self.get_input_schema(config))
-        runnable_node = graph.add_node(self)
-        output_node = graph.add_node(self.get_output_schema(config))
-        graph.add_edge(input_node, runnable_node)
-        graph.add_edge(runnable_node, output_node)
-        return graph
     def __or__(
         self,
         other: Union[
             Runnable[Any, Other],
             Callable[[Any], Other],
             Callable[[Iterator[Any]], Iterator[Other]],
             Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],
         ],
     ) -> RunnableSerializable[Input, Other]:
         """Compose this runnable with another object to create a RunnableSequence."""
-        return RunnableSequence(self, coerce_to_runnable(other))
+        return RunnableSequence(first=self, last=coerce_to_runnable(other))
     def __ror__(
         self,
         other: Union[
             Runnable[Other, Any],
             Callable[[Other], Any],
             Callable[[Iterator[Other]], Iterator[Any]],
             Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]],
         ],
     ) -> RunnableSerializable[Other, Output]:
         """Compose this runnable with another object to create a RunnableSequence."""
-        return RunnableSequence(coerce_to_runnable(other), self)
-    def pipe(
-        self,
-        *others: Union[Runnable[Any, Other], Callable[[Any], Other]],
-        name: Optional[str] = None,
-    ) -> RunnableSerializable[Input, Other]:
-        """Compose this runnable with another object to create a RunnableSequence."""
-        return RunnableSequence(self, *others, name=name)
-    def pick(self, keys: Union[str, List[str]]) -> RunnableSerializable[Any, Any]:
-        """Pick keys from the dict output of this runnable.
-        Returns a new runnable."""
-        from langchain_core.runnables.passthrough import RunnablePick
-        return self | RunnablePick(keys)
-    def assign(
-        self,
-        **kwargs: Union[
-            Runnable[Dict[str, Any], Any],
-            Callable[[Dict[str, Any]], Any],
-            Mapping[
-                str,
-                Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]],
-            ],
-        ],
-    ) -> RunnableSerializable[Any, Any]:
-        """Assigns new fields to the dict output of this runnable.
-        Returns a new runnable."""
-        from langchain_core.runnables.passthrough import RunnableAssign
-        return self | RunnableAssign(RunnableParallel(kwargs))
+        return RunnableSequence(first=coerce_to_runnable(other), last=self)
     """ --- Public API --- """
     @abstractmethod
     def invoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output:
         """Transform a single input into an output. Override to implement.
         Args:
             input: The input to the runnable.
             config: A config to use when invoking the runnable.
                The config supports standard keys like 'tags', 'metadata' for tracing
                purposes, 'max_concurrency' for controlling how much work to do
                in parallel, and other keys. Please refer to the RunnableConfig
@@ -367,24 +309,23 @@
             The output of the runnable.
         """
     async def ainvoke(
         self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
     ) -> Output:
         """Default implementation of ainvoke, calls invoke from a thread.
         The default implementation allows usage of async code even if
         the runnable did not implement a native async version of invoke.
         Subclasses should override this method if they can run asynchronously.
         """
-        with get_executor_for_config(config) as executor:
-            return await asyncio.get_running_loop().run_in_executor(
-                executor, partial(self.invoke, **kwargs), input, config
-            )
+        return await asyncio.get_running_loop().run_in_executor(
+            None, partial(self.invoke, **kwargs), input, config
+        )
     def batch(
         self,
         inputs: List[Input],
         config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,
         *,
         return_exceptions: bool = False,
         **kwargs: Optional[Any],
     ) -> List[Output]:
         """Default implementation runs invoke in parallel using a thread pool executor.
         The default implementation of batch works well for IO bound runnables.
@@ -775,21 +716,21 @@
         **kwargs: Optional[Any],
     ) -> Output:
         """Helper method to transform an Input value to an Output value,
         with callbacks. Use this method to implement invoke() in subclasses."""
         config = ensure_config(config)
         callback_manager = get_callback_manager_for_config(config)
         run_manager = callback_manager.on_chain_start(
             dumpd(self),
             input,
             run_type=run_type,
-            name=config.get("run_name") or self.get_name(),
+            name=config.get("run_name"),
         )
         try:
             output = call_func_with_variable_args(
                 func, input, config, run_manager, **kwargs
             )
         except BaseException as e:
             run_manager.on_chain_error(e)
             raise
         else:
             run_manager.on_chain_end(dumpd(output))
@@ -810,21 +751,21 @@
         **kwargs: Optional[Any],
     ) -> Output:
         """Helper method to transform an Input value to an Output value,
         with callbacks. Use this method to implement ainvoke() in subclasses."""
         config = ensure_config(config)
         callback_manager = get_async_callback_manager_for_config(config)
         run_manager = await callback_manager.on_chain_start(
             dumpd(self),
             input,
             run_type=run_type,
-            name=config.get("run_name") or self.get_name(),
+            name=config.get("run_name"),
         )
         try:
             output = await acall_func_with_variable_args(
                 func, input, config, run_manager, **kwargs
             )
         except BaseException as e:
             await run_manager.on_chain_error(e)
             raise
         else:
             await run_manager.on_chain_end(dumpd(output))
@@ -853,21 +794,21 @@
         with callbacks. Use this method to implement invoke() in subclasses."""
         if not input:
             return []
         configs = get_config_list(config, len(input))
         callback_managers = [get_callback_manager_for_config(c) for c in configs]
         run_managers = [
             callback_manager.on_chain_start(
                 dumpd(self),
                 input,
                 run_type=run_type,
-                name=config.get("run_name") or self.get_name(),
+                name=config.get("run_name"),
             )
             for callback_manager, input, config in zip(
                 callback_managers, input, configs
             )
         ]
         try:
             if accepts_config(func):
                 kwargs["config"] = [
                     patch_config(c, callbacks=rm.get_child())
                     for c, rm in zip(configs, run_managers)
@@ -923,21 +864,21 @@
         if not input:
             return []
         configs = get_config_list(config, len(input))
         callback_managers = [get_async_callback_manager_for_config(c) for c in configs]
         run_managers: List[AsyncCallbackManagerForChainRun] = await asyncio.gather(
             *(
                 callback_manager.on_chain_start(
                     dumpd(self),
                     input,
                     run_type=run_type,
-                    name=config.get("run_name") or self.get_name(),
+                    name=config.get("run_name"),
                 )
                 for callback_manager, input, config in zip(
                     callback_managers, input, configs
                 )
             )
         )
         try:
             if accepts_config(func):
                 kwargs["config"] = [
                     patch_config(c, callbacks=rm.get_child())
@@ -994,21 +935,21 @@
         final_input: Optional[Input] = next(input_for_tracing, None)
         final_input_supported = True
         final_output: Optional[Output] = None
         final_output_supported = True
         config = ensure_config(config)
         callback_manager = get_callback_manager_for_config(config)
         run_manager = callback_manager.on_chain_start(
             dumpd(self),
             {"input": ""},
             run_type=run_type,
-            name=config.get("run_name") or self.get_name(),
+            name=config.get("run_name"),
         )
         try:
             if accepts_config(transformer):
                 kwargs["config"] = patch_config(
                     config, callbacks=run_manager.get_child()
                 )
             if accepts_run_manager(transformer):
                 kwargs["run_manager"] = run_manager
             iterator = transformer(input_for_transform, **kwargs)  # type: ignore[call-arg]
             for chunk in iterator:
@@ -1066,21 +1007,21 @@
         final_input: Optional[Input] = await py_anext(input_for_tracing, None)
         final_input_supported = True
         final_output: Optional[Output] = None
         final_output_supported = True
         config = ensure_config(config)
         callback_manager = get_async_callback_manager_for_config(config)
         run_manager = await callback_manager.on_chain_start(
             dumpd(self),
             {"input": ""},
             run_type=run_type,
-            name=config.get("run_name") or self.get_name(),
+            name=config.get("run_name"),
         )
         try:
             if accepts_config(transformer):
                 kwargs["config"] = patch_config(
                     config, callbacks=run_manager.get_child()
                 )
             if accepts_run_manager(transformer):
                 kwargs["run_manager"] = run_manager
             iterator = transformer(input_for_transform, **kwargs)  # type: ignore[call-arg]
             async for chunk in iterator:
@@ -1104,22 +1045,20 @@
                         except TypeError:
                             final_input = None
                             final_input_supported = False
         except BaseException as e:
             await run_manager.on_chain_error(e, inputs=final_input)
             raise
         else:
             await run_manager.on_chain_end(final_output, inputs=final_input)
 class RunnableSerializable(Serializable, Runnable[Input, Output]):
     """A Runnable that can be serialized to JSON."""
-    name: Optional[str] = None
-    """The name of the runnable. Used for debugging and tracing."""
     def configurable_fields(
         self, **kwargs: AnyConfigurableField
     ) -> RunnableSerializable[Input, Output]:
         from langchain_core.runnables.configurable import RunnableConfigurableFields
         for key in kwargs:
             if key not in self.__fields__:
                 raise ValueError(
                     f"Configuration key {key} not found in {self}: "
                     "available keys are {self.__fields__.keys()}"
                 )
@@ -1135,88 +1074,20 @@
         from langchain_core.runnables.configurable import (
             RunnableConfigurableAlternatives,
         )
         return RunnableConfigurableAlternatives(
             which=which,
             default=self,
             alternatives=kwargs,
             default_key=default_key,
             prefix_keys=prefix_keys,
         )
-def _seq_input_schema(
-    steps: List[Runnable[Any, Any]], config: Optional[RunnableConfig]
-) -> Type[BaseModel]:
-    from langchain_core.runnables.passthrough import RunnableAssign, RunnablePick
-    first = steps[0]
-    if len(steps) == 1:
-        return first.get_input_schema(config)
-    elif isinstance(first, RunnableAssign):
-        next_input_schema = _seq_input_schema(steps[1:], config)
-        if not next_input_schema.__custom_root_type__:
-            return create_model(  # type: ignore[call-overload]
-                "RunnableSequenceInput",
-                **{
-                    k: (v.annotation, v.default)
-                    for k, v in next_input_schema.__fields__.items()
-                    if k not in first.mapper.steps
-                },
-                __config__=_SchemaConfig,
-            )
-    elif isinstance(first, RunnablePick):
-        return _seq_input_schema(steps[1:], config)
-    return first.get_input_schema(config)
-def _seq_output_schema(
-    steps: List[Runnable[Any, Any]], config: Optional[RunnableConfig]
-) -> Type[BaseModel]:
-    from langchain_core.runnables.passthrough import RunnableAssign, RunnablePick
-    last = steps[-1]
-    if len(steps) == 1:
-        return last.get_input_schema(config)
-    elif isinstance(last, RunnableAssign):
-        mapper_output_schema = last.mapper.get_output_schema(config)
-        prev_output_schema = _seq_output_schema(steps[:-1], config)
-        if not prev_output_schema.__custom_root_type__:
-            return create_model(  # type: ignore[call-overload]
-                "RunnableSequenceOutput",
-                **{
-                    **{
-                        k: (v.annotation, v.default)
-                        for k, v in prev_output_schema.__fields__.items()
-                    },
-                    **{
-                        k: (v.annotation, v.default)
-                        for k, v in mapper_output_schema.__fields__.items()
-                    },
-                },
-                __config__=_SchemaConfig,
-            )
-    elif isinstance(last, RunnablePick):
-        prev_output_schema = _seq_output_schema(steps[:-1], config)
-        if not prev_output_schema.__custom_root_type__:
-            if isinstance(last.keys, list):
-                return create_model(  # type: ignore[call-overload]
-                    "RunnableSequenceOutput",
-                    **{
-                        k: (v.annotation, v.default)
-                        for k, v in prev_output_schema.__fields__.items()
-                        if k in last.keys
-                    },
-                    __config__=_SchemaConfig,
-                )
-            else:
-                field = prev_output_schema.__fields__[last.keys]
-                return create_model(  # type: ignore[call-overload]
-                    "RunnableSequenceOutput",
-                    __root__=(field.annotation, field.default),
-                    __config__=_SchemaConfig,
-                )
-    return last.get_output_schema(config)
 class RunnableSequence(RunnableSerializable[Input, Output]):
     """A sequence of runnables, where the output of each is the input of the next.
     RunnableSequence is the most important composition operator in LangChain as it is
     used in virtually every chain.
     A RunnableSequence can be instantiated directly or more commonly by using the `|`
     operator where either the left or right operands (or both) must be a Runnable.
     Any RunnableSequence automatically supports sync, async, batch.
     The default implementations of `batch` and `abatch` utilize threadpools and
     asyncio gather and will be faster than naive invocation of invoke or ainvoke
     for IO bound runnables.
@@ -1263,51 +1134,20 @@
             async for chunk in chain.astream({'topic': 'colors'}):
                 print('-')
                 print(chunk, sep='', flush=True)
     """
     first: Runnable[Input, Any]
     """The first runnable in the sequence."""
     middle: List[Runnable[Any, Any]] = Field(default_factory=list)
     """The middle runnables in the sequence."""
     last: Runnable[Any, Output]
     """The last runnable in the sequence."""
-    def __init__(
-        self,
-        *steps: RunnableLike,
-        name: Optional[str] = None,
-        first: Optional[Runnable[Any, Any]] = None,
-        middle: Optional[List[Runnable[Any, Any]]] = None,
-        last: Optional[Runnable[Any, Any]] = None,
-    ) -> None:
-        """Create a new RunnableSequence.
-        Args:
-            steps: The steps to include in the sequence.
-        """
-        steps_flat: List[Runnable] = []
-        if not steps:
-            if first is not None and last is not None:
-                steps_flat = [first] + (middle or []) + [last]
-        for step in steps:
-            if isinstance(step, RunnableSequence):
-                steps_flat.extend(step.steps)
-            else:
-                steps_flat.append(coerce_to_runnable(step))
-        if len(steps_flat) < 2:
-            raise ValueError(
-                f"RunnableSequence must have at least 2 steps, got {len(steps_flat)}"
-            )
-        super().__init__(
-            first=steps_flat[0],
-            middle=list(steps_flat[1:-1]),
-            last=steps_flat[-1],
-            name=name,
-        )
     @classmethod
     def get_lc_namespace(cls) -> List[str]:
         """Get the namespace of the langchain object."""
         return ["langchain", "schema", "runnable"]
     @property
     def steps(self) -> List[Runnable[Any, Any]]:
         """All the runnables that make up the sequence in order."""
         return [self.first] + self.middle + [self.last]
     @classmethod
     def is_lc_serializable(cls) -> bool:
@@ -1316,25 +1156,39 @@
         arbitrary_types_allowed = True
     @property
     def InputType(self) -> Type[Input]:
         return self.first.InputType
     @property
     def OutputType(self) -> Type[Output]:
         return self.last.OutputType
     def get_input_schema(
         self, config: Optional[RunnableConfig] = None
     ) -> Type[BaseModel]:
-        return _seq_input_schema(self.steps, config)
+        from langchain_core.runnables.passthrough import RunnableAssign
+        if isinstance(self.first, RunnableAssign):
+            first = cast(RunnableAssign, self.first)
+            next_ = self.middle[0] if self.middle else self.last
+            next_input_schema = next_.get_input_schema(config)
+            if not next_input_schema.__custom_root_type__:
+                return create_model(  # type: ignore[call-overload]
+                    "RunnableSequenceInput",
+                    **{
+                        k: (v.annotation, v.default)
+                        for k, v in next_input_schema.__fields__.items()
+                        if k not in first.mapper.steps
+                    },
+                )
+        return self.first.get_input_schema(config)
     def get_output_schema(
         self, config: Optional[RunnableConfig] = None
     ) -> Type[BaseModel]:
-        return _seq_output_schema(self.steps, config)
+        return self.last.get_output_schema(config)
     @property
     def config_specs(self) -> List[ConfigurableFieldSpec]:
         from langchain_core.beta.runnables.context import (
             CONTEXT_CONFIG_PREFIX,
             _key_from_id,
         )
         all_specs = [
             (spec, idx)
             for idx, step in enumerate(self.steps)
             for spec in step.config_specs
@@ -1361,102 +1215,73 @@
                         dependencies=[
                             d
                             for d in deps_by_pos[idx]
                             if _key_from_id(d) != _key_from_id(spec.id)
                         ]
                         + (spec.dependencies or []),
                     ),
                     idx,
                 )
         return get_unique_config_specs(spec for spec, _ in all_specs)
-    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
-        from langchain_core.runnables.graph import Graph
-        graph = Graph()
-        for step in self.steps:
-            current_last_node = graph.last_node()
-            step_graph = step.get_graph(config)
-            if step is not self.first:
-                step_graph.trim_first_node()
-            if step is not self.last:
-                step_graph.trim_last_node()
-            graph.extend(step_graph)
-            step_first_node = step_graph.first_node()
-            if not step_first_node:
-                raise ValueError(f"Runnable {step} has no first node")
-            if current_last_node:
-                graph.add_edge(current_last_node, step_first_node)
-        return graph
     def __repr__(self) -> str:
         return "\n| ".join(
             repr(s) if i == 0 else indent_lines_after_first(repr(s), "| ")
             for i, s in enumerate(self.steps)
         )
     def __or__(
         self,
         other: Union[
             Runnable[Any, Other],
             Callable[[Any], Other],
             Callable[[Iterator[Any]], Iterator[Other]],
             Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],
         ],
     ) -> RunnableSerializable[Input, Other]:
         if isinstance(other, RunnableSequence):
             return RunnableSequence(
-                self.first,
-                *self.middle,
-                self.last,
-                other.first,
-                *other.middle,
-                other.last,
-                name=self.name or other.name,
+                first=self.first,
+                middle=self.middle + [self.last] + [other.first] + other.middle,
+                last=other.last,
             )
         else:
             return RunnableSequence(
-                self.first,
-                *self.middle,
-                self.last,
-                coerce_to_runnable(other),
-                name=self.name,
+                first=self.first,
+                middle=self.middle + [self.last],
+                last=coerce_to_runnable(other),
             )
     def __ror__(
         self,
         other: Union[
             Runnable[Other, Any],
             Callable[[Other], Any],
             Callable[[Iterator[Other]], Iterator[Any]],
             Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]],
         ],
     ) -> RunnableSerializable[Other, Output]:
         if isinstance(other, RunnableSequence):
             return RunnableSequence(
-                other.first,
-                *other.middle,
-                other.last,
-                self.first,
-                *self.middle,
-                self.last,
-                name=other.name or self.name,
+                first=other.first,
+                middle=other.middle + [other.last] + [self.first] + self.middle,
+                last=self.last,
             )
         else:
             return RunnableSequence(
-                coerce_to_runnable(other),
-                self.first,
-                *self.middle,
-                self.last,
-                name=self.name,
+                first=coerce_to_runnable(other),
+                middle=[self.first] + self.middle,
+                last=self.last,
             )
     def invoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output:
         from langchain_core.beta.runnables.context import config_with_context
         config = config_with_context(ensure_config(config), self.steps)
         callback_manager = get_callback_manager_for_config(config)
         run_manager = callback_manager.on_chain_start(
-            dumpd(self), input, name=config.get("run_name") or self.get_name()
+            dumpd(self), input, name=config.get("run_name")
         )
         try:
             for i, step in enumerate(self.steps):
                 input = step.invoke(
                     input,
                     patch_config(
                         config, callbacks=run_manager.get_child(f"seq:step:{i+1}")
                     ),
                 )
         except BaseException as e:
@@ -1468,21 +1293,21 @@
     async def ainvoke(
         self,
         input: Input,
         config: Optional[RunnableConfig] = None,
         **kwargs: Optional[Any],
     ) -> Output:
         from langchain_core.beta.runnables.context import aconfig_with_context
         config = aconfig_with_context(ensure_config(config), self.steps)
         callback_manager = get_async_callback_manager_for_config(config)
         run_manager = await callback_manager.on_chain_start(
-            dumpd(self), input, name=config.get("run_name") or self.get_name()
+            dumpd(self), input, name=config.get("run_name")
         )
         try:
             for i, step in enumerate(self.steps):
                 input = await step.ainvoke(
                     input,
                     patch_config(
                         config, callbacks=run_manager.get_child(f"seq:step:{i+1}")
                     ),
                 )
         except BaseException as e:
@@ -1516,21 +1341,21 @@
                 local_tags=None,
                 inheritable_metadata=config.get("metadata"),
                 local_metadata=None,
             )
             for config in configs
         ]
         run_managers = [
             cm.on_chain_start(
                 dumpd(self),
                 input,
-                name=config.get("run_name") or self.get_name(),
+                name=config.get("run_name"),
             )
             for cm, input, config in zip(callback_managers, inputs, configs)
         ]
         try:
             if return_exceptions:
                 failed_inputs_map: Dict[int, Exception] = {}
                 for stepidx, step in enumerate(self.steps):
                     remaining_idxs = [
                         i for i in range(len(configs)) if i not in failed_inputs_map
                     ]
@@ -1619,21 +1444,21 @@
                 inheritable_metadata=config.get("metadata"),
                 local_metadata=None,
             )
             for config in configs
         ]
         run_managers: List[AsyncCallbackManagerForChainRun] = await asyncio.gather(
             *(
                 cm.on_chain_start(
                     dumpd(self),
                     input,
-                    name=config.get("run_name") or self.get_name(),
+                    name=config.get("run_name"),
                 )
                 for cm, input, config in zip(callback_managers, inputs, configs)
             )
         )
         try:
             if return_exceptions:
                 failed_inputs_map: Dict[int, Exception] = {}
                 for stepidx, step in enumerate(self.steps):
                     remaining_idxs = [
                         i for i in range(len(configs)) if i not in failed_inputs_map
@@ -1738,43 +1563,37 @@
             )
         async for output in final_pipeline:
             yield output
     def transform(
         self,
         input: Iterator[Input],
         config: Optional[RunnableConfig] = None,
         **kwargs: Optional[Any],
     ) -> Iterator[Output]:
         yield from self._transform_stream_with_config(
-            input,
-            self._transform,
-            patch_config(config, run_name=(config or {}).get("run_name") or self.name),
-            **kwargs,
+            input, self._transform, config, **kwargs
         )
     def stream(
         self,
         input: Input,
         config: Optional[RunnableConfig] = None,
         **kwargs: Optional[Any],
     ) -> Iterator[Output]:
         yield from self.transform(iter([input]), config, **kwargs)
     async def atransform(
         self,
         input: AsyncIterator[Input],
         config: Optional[RunnableConfig] = None,
         **kwargs: Optional[Any],
     ) -> AsyncIterator[Output]:
         async for chunk in self._atransform_stream_with_config(
-            input,
-            self._atransform,
-            patch_config(config, run_name=(config or {}).get("run_name") or self.name),
-            **kwargs,
+            input, self._atransform, config, **kwargs
         ):
             yield chunk
     async def astream(
         self,
         input: Input,
         config: Optional[RunnableConfig] = None,
         **kwargs: Optional[Any],
     ) -> AsyncIterator[Output]:
         async def input_aiter() -> AsyncIterator[Input]:
             yield input
@@ -1811,84 +1630,55 @@
         )
     @classmethod
     def is_lc_serializable(cls) -> bool:
         return True
     @classmethod
     def get_lc_namespace(cls) -> List[str]:
         """Get the namespace of the langchain object."""
         return ["langchain", "schema", "runnable"]
     class Config:
         arbitrary_types_allowed = True
-    def get_name(
-        self, suffix: Optional[str] = None, *, name: Optional[str] = None
-    ) -> str:
-        name = name or self.name or f"RunnableParallel<{','.join(self.steps.keys())}>"
-        return super().get_name(suffix, name=name)
     @property
     def InputType(self) -> Any:
         for step in self.steps.values():
             if step.InputType:
                 return step.InputType
         return Any
     def get_input_schema(
         self, config: Optional[RunnableConfig] = None
     ) -> Type[BaseModel]:
         if all(
             s.get_input_schema(config).schema().get("type", "object") == "object"
             for s in self.steps.values()
         ):
             return create_model(  # type: ignore[call-overload]
-                self.get_name("Input"),
+                "RunnableParallelInput",
                 **{
                     k: (v.annotation, v.default)
                     for step in self.steps.values()
                     for k, v in step.get_input_schema(config).__fields__.items()
                     if k != "__root__"
                 },
-                __config__=_SchemaConfig,
             )
         return super().get_input_schema(config)
     def get_output_schema(
         self, config: Optional[RunnableConfig] = None
     ) -> Type[BaseModel]:
         return create_model(  # type: ignore[call-overload]
-            self.get_name("Output"),
+            "RunnableParallelOutput",
             **{k: (v.OutputType, None) for k, v in self.steps.items()},
-            __config__=_SchemaConfig,
         )
     @property
     def config_specs(self) -> List[ConfigurableFieldSpec]:
         return get_unique_config_specs(
             spec for step in self.steps.values() for spec in step.config_specs
         )
-    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
-        from langchain_core.runnables.graph import Graph
-        graph = Graph()
-        input_node = graph.add_node(self.get_input_schema(config))
-        output_node = graph.add_node(self.get_output_schema(config))
-        for step in self.steps.values():
-            step_graph = step.get_graph()
-            step_graph.trim_first_node()
-            step_graph.trim_last_node()
-            if not step_graph:
-                graph.add_edge(input_node, output_node)
-            else:
-                graph.extend(step_graph)
-                step_first_node = step_graph.first_node()
-                if not step_first_node:
-                    raise ValueError(f"Runnable {step} has no first node")
-                step_last_node = step_graph.last_node()
-                if not step_last_node:
-                    raise ValueError(f"Runnable {step} has no last node")
-                graph.add_edge(input_node, step_first_node)
-                graph.add_edge(step_last_node, output_node)
-        return graph
     def __repr__(self) -> str:
         map_for_repr = ",\n  ".join(
             f"{k}: {indent_lines_after_first(repr(v), '  ' + k + ': ')}"
             for k, v in self.steps.items()
         )
         return "{\n  " + map_for_repr + "\n}"
     def invoke(
         self, input: Input, config: Optional[RunnableConfig] = None
     ) -> Dict[str, Any]:
         from langchain_core.callbacks.manager import CallbackManager
@@ -1896,21 +1686,21 @@
         callback_manager = CallbackManager.configure(
             inheritable_callbacks=config.get("callbacks"),
             local_callbacks=None,
             verbose=False,
             inheritable_tags=config.get("tags"),
             local_tags=None,
             inheritable_metadata=config.get("metadata"),
             local_metadata=None,
         )
         run_manager = callback_manager.on_chain_start(
-            dumpd(self), input, name=config.get("run_name") or self.get_name()
+            dumpd(self), input, name=config.get("run_name")
         )
         try:
             steps = dict(self.steps)
             with get_executor_for_config(config) as executor:
                 futures = [
                     executor.submit(
                         step.invoke,
                         input,
                         patch_config(
                             config,
@@ -1928,21 +1718,21 @@
             return output
     async def ainvoke(
         self,
         input: Input,
         config: Optional[RunnableConfig] = None,
         **kwargs: Optional[Any],
     ) -> Dict[str, Any]:
         config = ensure_config(config)
         callback_manager = get_async_callback_manager_for_config(config)
         run_manager = await callback_manager.on_chain_start(
-            dumpd(self), input, name=config.get("run_name") or self.get_name()
+            dumpd(self), input, name=config.get("run_name")
         )
         try:
             steps = dict(self.steps)
             results = await asyncio.gather(
                 *(
                     step.ainvoke(
                         input,
                         patch_config(
                             config, callbacks=run_manager.get_child(f"map:key:{key}")
                         ),
@@ -2124,26 +1914,21 @@
         if isinstance(other, RunnableGenerator):
             if hasattr(self, "_transform") and hasattr(other, "_transform"):
                 return self._transform == other._transform
             elif hasattr(self, "_atransform") and hasattr(other, "_atransform"):
                 return self._atransform == other._atransform
             else:
                 return False
         else:
             return False
     def __repr__(self) -> str:
-        if hasattr(self, "_transform"):
-            return f"RunnableGenerator({self._transform.__name__})"
-        elif hasattr(self, "_atransform"):
-            return f"RunnableGenerator({self._atransform.__name__})"
-        else:
-            return "RunnableGenerator(...)"
+        return "RunnableGenerator(...)"
     def transform(
         self,
         input: Iterator[Input],
         config: Optional[RunnableConfig] = None,
         **kwargs: Any,
     ) -> Iterator[Output]:
         return self._transform_stream_with_config(
             input, self._transform, config, **kwargs
         )
     def stream(
@@ -2248,43 +2033,35 @@
     ) -> None:
         """Create a RunnableLambda from a callable, and async callable or both.
         Accepts both sync and async variants to allow providing efficient
         implementations for sync and async execution.
         Args:
             func: Either sync or async callable
             afunc: An async callable that takes an input and returns an output.
         """
         if afunc is not None:
             self.afunc = afunc
-            func_for_name: Callable = afunc
         if inspect.iscoroutinefunction(func):
             if afunc is not None:
                 raise TypeError(
                     "Func was provided as a coroutine function, but afunc was "
                     "also provided. If providing both, func should be a regular "
                     "function to avoid ambiguity."
                 )
             self.afunc = func
-            func_for_name = func
         elif callable(func):
             self.func = cast(Callable[[Input], Output], func)
-            func_for_name = func
         else:
             raise TypeError(
                 "Expected a callable type for `func`."
                 f"Instead got an unsupported type: {type(func)}"
             )
-        try:
-            if func_for_name.__name__ != "<lambda>":
-                self.name = func_for_name.__name__
-        except AttributeError:
-            pass
     @property
     def InputType(self) -> Any:
         """The type of the input to this runnable."""
         func = getattr(self, "func", None) or getattr(self, "afunc")
         try:
             params = inspect.signature(func).parameters
             first_param = next(iter(params.values()), None)
             if first_param and first_param.annotation != inspect.Parameter.empty:
                 return first_param.annotation
             else:
@@ -2295,106 +2072,59 @@
         self, config: Optional[RunnableConfig] = None
     ) -> Type[BaseModel]:
         """The pydantic schema for the input to this runnable."""
         func = getattr(self, "func", None) or getattr(self, "afunc")
         if isinstance(func, itemgetter):
             items = str(func).replace("operator.itemgetter(", "")[:-1].split(", ")
             if all(
                 item[0] == "'" and item[-1] == "'" and len(item) > 2 for item in items
             ):
                 return create_model(
-                    self.get_name("Input"),
+                    "RunnableLambdaInput",
                     **{item[1:-1]: (Any, None) for item in items},  # type: ignore
-                    __config__=_SchemaConfig,
                 )
             else:
-                return create_model(
-                    self.get_name("Input"),
-                    __root__=(List[Any], None),
-                    __config__=_SchemaConfig,
-                )
+                return create_model("RunnableLambdaInput", __root__=(List[Any], None))
         if self.InputType != Any:
             return super().get_input_schema(config)
         if dict_keys := get_function_first_arg_dict_keys(func):
             return create_model(
-                self.get_name("Input"),
+                "RunnableLambdaInput",
                 **{key: (Any, None) for key in dict_keys},  # type: ignore
-                __config__=_SchemaConfig,
             )
         return super().get_input_schema(config)
     @property
     def OutputType(self) -> Any:
         """The type of the output of this runnable as a type annotation."""
         func = getattr(self, "func", None) or getattr(self, "afunc")
         try:
             sig = inspect.signature(func)
             return (
                 sig.return_annotation
                 if sig.return_annotation != inspect.Signature.empty
                 else Any
             )
         except ValueError:
             return Any
-    @property
-    def deps(self) -> List[Runnable]:
-        """The dependencies of this runnable."""
-        if hasattr(self, "func"):
-            objects = get_function_nonlocals(self.func)
-        elif hasattr(self, "afunc"):
-            objects = get_function_nonlocals(self.afunc)
-        else:
-            objects = []
-        return [obj for obj in objects if isinstance(obj, Runnable)]
-    @property
-    def config_specs(self) -> List[ConfigurableFieldSpec]:
-        return get_unique_config_specs(
-            spec for dep in self.deps for spec in dep.config_specs
-        )
-    def get_graph(self, config: RunnableConfig | None = None) -> Graph:
-        if deps := self.deps:
-            graph = Graph()
-            input_node = graph.add_node(self.get_input_schema(config))
-            output_node = graph.add_node(self.get_output_schema(config))
-            for dep in deps:
-                dep_graph = dep.get_graph()
-                dep_graph.trim_first_node()
-                dep_graph.trim_last_node()
-                if not dep_graph:
-                    graph.add_edge(input_node, output_node)
-                else:
-                    graph.extend(dep_graph)
-                    dep_first_node = dep_graph.first_node()
-                    if not dep_first_node:
-                        raise ValueError(f"Runnable {dep} has no first node")
-                    dep_last_node = dep_graph.last_node()
-                    if not dep_last_node:
-                        raise ValueError(f"Runnable {dep} has no last node")
-                    graph.add_edge(input_node, dep_first_node)
-                    graph.add_edge(dep_last_node, output_node)
-        else:
-            graph = super().get_graph(config)
-        return graph
     def __eq__(self, other: Any) -> bool:
         if isinstance(other, RunnableLambda):
             if hasattr(self, "func") and hasattr(other, "func"):
                 return self.func == other.func
             elif hasattr(self, "afunc") and hasattr(other, "afunc"):
                 return self.afunc == other.afunc
             else:
                 return False
         else:
             return False
     def __repr__(self) -> str:
         """A string representation of this runnable."""
-        if hasattr(self, "func") and isinstance(self.func, itemgetter):
-            return f"RunnableLambda({str(self.func)[len('operator.'):]})"
-        elif hasattr(self, "func"):
+        if hasattr(self, "func"):
             return f"RunnableLambda({get_lambda_source(self.func) or '...'})"
         elif hasattr(self, "afunc"):
             return f"RunnableLambda(afunc={get_lambda_source(self.afunc) or '...'})"
         else:
             return "RunnableLambda(...)"
     def _invoke(
         self,
         input: Input,
         run_manager: CallbackManagerForChainRun,
         config: RunnableConfig,
@@ -2423,24 +2153,23 @@
         input: Input,
         run_manager: AsyncCallbackManagerForChainRun,
         config: RunnableConfig,
         **kwargs: Any,
     ) -> Output:
         if hasattr(self, "afunc"):
             afunc = self.afunc
         else:
             @wraps(self.func)
             async def f(*args, **kwargs):  # type: ignore[no-untyped-def]
-                with get_executor_for_config(config) as executor:
-                    return await asyncio.get_running_loop().run_in_executor(
-                        executor, partial(self.func, **kwargs), *args
-                    )
+                return await asyncio.get_running_loop().run_in_executor(
+                    None, partial(self.func, **kwargs), *args
+                )
             afunc = f
         output = await acall_func_with_variable_args(
             afunc, input, config, run_manager, **kwargs
         )
         if isinstance(output, Runnable):
             recursion_limit = config["recursion_limit"]
             if recursion_limit <= 0:
                 raise RecursionError(
                     f"Recursion limit reached when invoking {self} with input {input}."
                 )
@@ -2491,194 +2220,59 @@
         **kwargs: Optional[Any],
     ) -> Output:
         """Invoke this runnable asynchronously."""
         the_func = self.afunc if hasattr(self, "afunc") else self.func
         return await self._acall_with_config(
             self._ainvoke,
             input,
             self._config(config, the_func),
             **kwargs,
         )
-    def _transform(
-        self,
-        input: Iterator[Input],
-        run_manager: CallbackManagerForChainRun,
-        config: RunnableConfig,
-        **kwargs: Any,
-    ) -> Iterator[Output]:
-        final: Optional[Input] = None
-        for ichunk in input:
-            if final is None:
-                final = ichunk
-            else:
-                try:
-                    final = final + ichunk  # type: ignore[operator]
-                except TypeError:
-                    final = ichunk
-        output = call_func_with_variable_args(
-            self.func, cast(Input, final), config, run_manager, **kwargs
-        )
-        if isinstance(output, Runnable):
-            recursion_limit = config["recursion_limit"]
-            if recursion_limit <= 0:
-                raise RecursionError(
-                    f"Recursion limit reached when invoking "
-                    f"{self} with input {final}."
-                )
-            for chunk in output.stream(
-                final,
-                patch_config(
-                    config,
-                    callbacks=run_manager.get_child(),
-                    recursion_limit=recursion_limit - 1,
-                ),
-            ):
-                yield chunk
-        else:
-            yield output
-    def transform(
-        self,
-        input: Iterator[Input],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[Output]:
-        if hasattr(self, "func"):
-            for output in self._transform_stream_with_config(
-                input,
-                self._transform,
-                self._config(config, self.func),
-                **kwargs,
-            ):
-                yield output
-        else:
-            raise TypeError(
-                "Cannot stream a coroutine function synchronously."
-                "Use `astream` instead."
-            )
-    def stream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[Output]:
-        return self.transform(iter([input]), config, **kwargs)
-    async def _atransform(
-        self,
-        input: AsyncIterator[Input],
-        run_manager: AsyncCallbackManagerForChainRun,
-        config: RunnableConfig,
-    ) -> AsyncIterator[Output]:
-        final: Optional[Input] = None
-        async for ichunk in input:
-            if final is None:
-                final = ichunk
-            else:
-                try:
-                    final = final + ichunk  # type: ignore[operator]
-                except TypeError:
-                    final = ichunk
-        if hasattr(self, "afunc"):
-            afunc = self.afunc
-        else:
-            @wraps(self.func)
-            async def f(*args, **kwargs):  # type: ignore[no-untyped-def]
-                return await asyncio.get_running_loop().run_in_executor(
-                    None, partial(self.func, **kwargs), *args
-                )
-            afunc = f
-        output = await acall_func_with_variable_args(
-            afunc, cast(Input, final), config, run_manager
-        )
-        if isinstance(output, Runnable):
-            recursion_limit = config["recursion_limit"]
-            if recursion_limit <= 0:
-                raise RecursionError(
-                    f"Recursion limit reached when invoking "
-                    f"{self} with input {final}."
-                )
-            async for chunk in output.astream(
-                final,
-                patch_config(
-                    config,
-                    callbacks=run_manager.get_child(),
-                    recursion_limit=recursion_limit - 1,
-                ),
-            ):
-                yield chunk
-        else:
-            yield output
-    async def atransform(
-        self,
-        input: AsyncIterator[Input],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[Output]:
-        async for output in self._atransform_stream_with_config(
-            input,
-            self._atransform,
-            self._config(config, self.afunc if hasattr(self, "afunc") else self.func),
-            **kwargs,
-        ):
-            yield output
-    async def astream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[Output]:
-        async def input_aiter() -> AsyncIterator[Input]:
-            yield input
-        async for chunk in self.atransform(input_aiter(), config, **kwargs):
-            yield chunk
 class RunnableEachBase(RunnableSerializable[List[Input], List[Output]]):
     """
     A runnable that delegates calls to another runnable
     with each element of the input sequence.
     Use only if creating a new RunnableEach subclass with different __init__ args.
     """
     bound: Runnable[Input, Output]
     class Config:
         arbitrary_types_allowed = True
     @property
     def InputType(self) -> Any:
         return List[self.bound.InputType]  # type: ignore[name-defined]
     def get_input_schema(
         self, config: Optional[RunnableConfig] = None
     ) -> Type[BaseModel]:
         return create_model(
-            self.get_name("Input"),
+            "RunnableEachInput",
             __root__=(
                 List[self.bound.get_input_schema(config)],  # type: ignore
                 None,
             ),
-            __config__=_SchemaConfig,
         )
     @property
     def OutputType(self) -> Type[List[Output]]:
         return List[self.bound.OutputType]  # type: ignore[name-defined]
     def get_output_schema(
         self, config: Optional[RunnableConfig] = None
     ) -> Type[BaseModel]:
         schema = self.bound.get_output_schema(config)
         return create_model(
-            self.get_name("Output"),
+            "RunnableEachOutput",
             __root__=(
                 List[schema],  # type: ignore
                 None,
             ),
-            __config__=_SchemaConfig,
         )
     @property
     def config_specs(self) -> List[ConfigurableFieldSpec]:
         return self.bound.config_specs
-    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
-        return self.bound.get_graph(config)
     @classmethod
     def is_lc_serializable(cls) -> bool:
         return True
     @classmethod
     def get_lc_namespace(cls) -> List[str]:
         """Get the namespace of the langchain object."""
         return ["langchain", "schema", "runnable"]
     def _invoke(
         self,
         inputs: List[Input],
@@ -2709,25 +2303,20 @@
         return await self._acall_with_config(self._ainvoke, input, config, **kwargs)
 class RunnableEach(RunnableEachBase[Input, Output]):
     """
     A runnable that delegates calls to another runnable
     with each element of the input sequence.
     """
     @classmethod
     def get_lc_namespace(cls) -> List[str]:
         """Get the namespace of the langchain object."""
         return ["langchain", "schema", "runnable"]
-    def get_name(
-        self, suffix: Optional[str] = None, *, name: Optional[str] = None
-    ) -> str:
-        name = name or self.name or f"RunnableEach<{self.bound.get_name()}>"
-        return super().get_name(suffix, name=name)
     def bind(self, **kwargs: Any) -> RunnableEach[Input, Output]:
         return RunnableEach(bound=self.bound.bind(**kwargs))
     def with_config(
         self, config: Optional[RunnableConfig] = None, **kwargs: Any
     ) -> RunnableEach[Input, Output]:
         return RunnableEach(bound=self.bound.with_config(config, **kwargs))
     def with_listeners(
         self,
         *,
         on_start: Optional[Listener] = None,
@@ -2815,24 +2404,20 @@
                     )
         super().__init__(
             bound=bound,
             kwargs=kwargs or {},
             config=config or {},
             config_factories=config_factories or [],
             custom_input_type=custom_input_type,
             custom_output_type=custom_output_type,
             **other_kwargs,
         )
-    def get_name(
-        self, suffix: Optional[str] = None, *, name: Optional[str] = None
-    ) -> str:
-        return self.bound.get_name(suffix, name=name)
     @property
     def InputType(self) -> Type[Input]:
         return (
             cast(Type[Input], self.custom_input_type)
             if self.custom_input_type is not None
             else self.bound.InputType
         )
     @property
     def OutputType(self) -> Type[Output]:
         return (
@@ -2848,22 +2433,20 @@
         return self.bound.get_input_schema(merge_configs(self.config, config))
     def get_output_schema(
         self, config: Optional[RunnableConfig] = None
     ) -> Type[BaseModel]:
         if self.custom_output_type is not None:
             return super().get_output_schema(config)
         return self.bound.get_output_schema(merge_configs(self.config, config))
     @property
     def config_specs(self) -> List[ConfigurableFieldSpec]:
         return self.bound.config_specs
-    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
-        return self.bound.get_graph(config)
     @classmethod
     def is_lc_serializable(cls) -> bool:
         return True
     @classmethod
     def get_lc_namespace(cls) -> List[str]:
         """Get the namespace of the langchain object."""
         return ["langchain", "schema", "runnable"]
     def _merge_configs(self, *configs: Optional[RunnableConfig]) -> RunnableConfig:
         config = merge_configs(self.config, *configs)
         return merge_configs(config, *(f(config) for f in self.config_factories))

--- a/libs/core/langchain_core/runnables/branch.py
+++ b/libs/core/langchain_core/runnables/branch.py
@@ -1,37 +1,34 @@
 from typing import (
     Any,
-    AsyncIterator,
     Awaitable,
     Callable,
-    Iterator,
     List,
     Mapping,
     Optional,
     Sequence,
     Tuple,
     Type,
     Union,
     cast,
 )
 from langchain_core.load.dump import dumpd
 from langchain_core.pydantic_v1 import BaseModel
 from langchain_core.runnables.base import (
     Runnable,
     RunnableLike,
     RunnableSerializable,
     coerce_to_runnable,
 )
 from langchain_core.runnables.config import (
     RunnableConfig,
     ensure_config,
-    get_async_callback_manager_for_config,
     get_callback_manager_for_config,
     patch_config,
 )
 from langchain_core.runnables.utils import (
     ConfigurableFieldSpec,
     Input,
     Output,
     get_unique_config_specs,
 )
 class RunnableBranch(RunnableSerializable[Input, Output]):
@@ -176,32 +173,32 @@
                     )
                     break
             else:
                 output = self.default.invoke(
                     input,
                     config=patch_config(
                         config, callbacks=run_manager.get_child(tag="branch:default")
                     ),
                     **kwargs,
                 )
-        except BaseException as e:
+        except Exception as e:
             run_manager.on_chain_error(e)
             raise
         run_manager.on_chain_end(dumpd(output))
         return output
     async def ainvoke(
         self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
     ) -> Output:
         """Async version of invoke."""
         config = ensure_config(config)
-        callback_manager = get_async_callback_manager_for_config(config)
-        run_manager = await callback_manager.on_chain_start(
+        callback_manager = get_callback_manager_for_config(config)
+        run_manager = callback_manager.on_chain_start(
             dumpd(self),
             input,
             name=config.get("run_name"),
         )
         try:
             for idx, branch in enumerate(self.branches):
                 condition, runnable = branch
                 expression_value = await condition.ainvoke(
                     input,
                     config=patch_config(
@@ -220,155 +217,15 @@
                     )
                     break
             else:
                 output = await self.default.ainvoke(
                     input,
                     config=patch_config(
                         config, callbacks=run_manager.get_child(tag="branch:default")
                     ),
                     **kwargs,
                 )
-        except BaseException as e:
-            await run_manager.on_chain_error(e)
-            raise
-        await run_manager.on_chain_end(dumpd(output))
-        return output
-    def stream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> Iterator[Output]:
-        """First evaluates the condition,
-        then delegate to true or false branch."""
-        config = ensure_config(config)
-        callback_manager = get_callback_manager_for_config(config)
-        run_manager = callback_manager.on_chain_start(
-            dumpd(self),
-            input,
-            name=config.get("run_name"),
-        )
-        final_output: Optional[Output] = None
-        final_output_supported = True
-        try:
-            for idx, branch in enumerate(self.branches):
-                condition, runnable = branch
-                expression_value = condition.invoke(
-                    input,
-                    config=patch_config(
-                        config,
-                        callbacks=run_manager.get_child(tag=f"condition:{idx + 1}"),
-                    ),
-                )
-                if expression_value:
-                    for chunk in runnable.stream(
-                        input,
-                        config=patch_config(
-                            config,
-                            callbacks=run_manager.get_child(tag=f"branch:{idx + 1}"),
-                        ),
-                        **kwargs,
-                    ):
-                        yield chunk
-                        if final_output_supported:
-                            if final_output is None:
-                                final_output = chunk
-                            else:
-                                try:
-                                    final_output = final_output + chunk  # type: ignore
-                                except TypeError:
-                                    final_output = None
-                                    final_output_supported = False
-                    break
-            else:
-                for chunk in self.default.stream(
-                    input,
-                    config=patch_config(
-                        config,
-                        callbacks=run_manager.get_child(tag="branch:default"),
-                    ),
-                    **kwargs,
-                ):
-                    yield chunk
-                    if final_output_supported:
-                        if final_output is None:
-                            final_output = chunk
-                        else:
-                            try:
-                                final_output = final_output + chunk  # type: ignore
-                            except TypeError:
-                                final_output = None
-                                final_output_supported = False
-        except BaseException as e:
+        except Exception as e:
             run_manager.on_chain_error(e)
             raise
-        run_manager.on_chain_end(final_output)
-    async def astream(
-        self,
-        input: Input,
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Optional[Any],
-    ) -> AsyncIterator[Output]:
-        """First evaluates the condition,
-        then delegate to true or false branch."""
-        config = ensure_config(config)
-        callback_manager = get_async_callback_manager_for_config(config)
-        run_manager = await callback_manager.on_chain_start(
-            dumpd(self),
-            input,
-            name=config.get("run_name"),
-        )
-        final_output: Optional[Output] = None
-        final_output_supported = True
-        try:
-            for idx, branch in enumerate(self.branches):
-                condition, runnable = branch
-                expression_value = await condition.ainvoke(
-                    input,
-                    config=patch_config(
-                        config,
-                        callbacks=run_manager.get_child(tag=f"condition:{idx + 1}"),
-                    ),
-                )
-                if expression_value:
-                    async for chunk in runnable.astream(
-                        input,
-                        config=patch_config(
-                            config,
-                            callbacks=run_manager.get_child(tag=f"branch:{idx + 1}"),
-                        ),
-                        **kwargs,
-                    ):
-                        yield chunk
-                        if final_output_supported:
-                            if final_output is None:
-                                final_output = chunk
-                            else:
-                                try:
-                                    final_output = final_output + chunk  # type: ignore
-                                except TypeError:
-                                    final_output = None
-                                    final_output_supported = False
-                    break
-            else:
-                async for chunk in self.default.astream(
-                    input,
-                    config=patch_config(
-                        config,
-                        callbacks=run_manager.get_child(tag="branch:default"),
-                    ),
-                    **kwargs,
-                ):
-                    yield chunk
-                    if final_output_supported:
-                        if final_output is None:
-                            final_output = chunk
-                        else:
-                            try:
-                                final_output = final_output + chunk  # type: ignore
-                            except TypeError:
-                                final_output = None
-                                final_output_supported = False
-        except BaseException as e:
-            await run_manager.on_chain_error(e)
-            raise
-        await run_manager.on_chain_end(final_output)
+        run_manager.on_chain_end(dumpd(output))
+        return output

--- a/libs/core/langchain_core/runnables/config.py
+++ b/libs/core/langchain_core/runnables/config.py
@@ -1,14 +1,13 @@
 from __future__ import annotations
 from concurrent.futures import Executor, ThreadPoolExecutor
 from contextlib import contextmanager
-from contextvars import Context, copy_context
 from typing import (
     TYPE_CHECKING,
     Any,
     Awaitable,
     Callable,
     Dict,
     Generator,
     List,
     Optional,
     Union,
@@ -314,30 +313,20 @@
         config (RunnableConfig): The config.
     Returns:
         AsyncCallbackManager: The async callback manager.
     """
     from langchain_core.callbacks.manager import AsyncCallbackManager
     return AsyncCallbackManager.configure(
         inheritable_callbacks=config.get("callbacks"),
         inheritable_tags=config.get("tags"),
         inheritable_metadata=config.get("metadata"),
     )
-def _set_context(context: Context) -> None:
-    for var, value in context.items():
-        var.set(value)
 @contextmanager
-def get_executor_for_config(
-    config: Optional[RunnableConfig]
-) -> Generator[Executor, None, None]:
+def get_executor_for_config(config: RunnableConfig) -> Generator[Executor, None, None]:
     """Get an executor for a config.
     Args:
         config (RunnableConfig): The config.
     Yields:
         Generator[Executor, None, None]: The executor.
     """
-    config = config or {}
-    with ThreadPoolExecutor(
-        max_workers=config.get("max_concurrency"),
-        initializer=_set_context,
-        initargs=(copy_context(),),
-    ) as executor:
+    with ThreadPoolExecutor(max_workers=config.get("max_concurrency")) as executor:
         yield executor

--- a/libs/core/langchain_core/runnables/configurable.py
+++ b/libs/core/langchain_core/runnables/configurable.py
@@ -17,21 +17,20 @@
     cast,
 )
 from weakref import WeakValueDictionary
 from langchain_core.pydantic_v1 import BaseModel
 from langchain_core.runnables.base import Runnable, RunnableSerializable
 from langchain_core.runnables.config import (
     RunnableConfig,
     get_config_list,
     get_executor_for_config,
 )
-from langchain_core.runnables.graph import Graph
 from langchain_core.runnables.utils import (
     AnyConfigurableField,
     ConfigurableField,
     ConfigurableFieldMultiOption,
     ConfigurableFieldSingleOption,
     ConfigurableFieldSpec,
     Input,
     Output,
     gather_with_concurrency,
     get_unique_config_specs,
@@ -57,23 +56,20 @@
     def get_input_schema(
         self, config: Optional[RunnableConfig] = None
     ) -> Type[BaseModel]:
         runnable, config = self._prepare(config)
         return runnable.get_input_schema(config)
     def get_output_schema(
         self, config: Optional[RunnableConfig] = None
     ) -> Type[BaseModel]:
         runnable, config = self._prepare(config)
         return runnable.get_output_schema(config)
-    def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
-        runnable, config = self._prepare(config)
-        return runnable.get_graph(config)
     @abstractmethod
     def _prepare(
         self, config: Optional[RunnableConfig] = None
     ) -> Tuple[Runnable[Input, Output], RunnableConfig]:
         ...
     def invoke(
         self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
     ) -> Output:
         runnable, config = self._prepare(config)
         return runnable.invoke(input, config, **kwargs)

--- a/libs/core/langchain_core/runnables/graph.py
+++ b//dev/null
@@ -1,117 +0,0 @@
-from __future__ import annotations
-from dataclasses import dataclass, field
-from typing import TYPE_CHECKING, Dict, List, NamedTuple, Optional, Type, Union
-from uuid import uuid4
-from langchain_core.pydantic_v1 import BaseModel
-from langchain_core.runnables.graph_draw import draw
-if TYPE_CHECKING:
-    from langchain_core.runnables.base import Runnable as RunnableType
-class Edge(NamedTuple):
-    source: str
-    target: str
-class Node(NamedTuple):
-    id: str
-    data: Union[Type[BaseModel], RunnableType]
-@dataclass
-class Graph:
-    nodes: Dict[str, Node] = field(default_factory=dict)
-    edges: List[Edge] = field(default_factory=list)
-    def __bool__(self) -> bool:
-        return bool(self.nodes)
-    def next_id(self) -> str:
-        return uuid4().hex
-    def add_node(self, data: Union[Type[BaseModel], RunnableType]) -> Node:
-        """Add a node to the graph and return it."""
-        node = Node(id=self.next_id(), data=data)
-        self.nodes[node.id] = node
-        return node
-    def remove_node(self, node: Node) -> None:
-        """Remove a node from the graphm and all edges connected to it."""
-        self.nodes.pop(node.id)
-        self.edges = [
-            edge
-            for edge in self.edges
-            if edge.source != node.id and edge.target != node.id
-        ]
-    def add_edge(self, source: Node, target: Node) -> Edge:
-        """Add an edge to the graph and return it."""
-        if source.id not in self.nodes:
-            raise ValueError(f"Source node {source.id} not in graph")
-        if target.id not in self.nodes:
-            raise ValueError(f"Target node {target.id} not in graph")
-        edge = Edge(source=source.id, target=target.id)
-        self.edges.append(edge)
-        return edge
-    def extend(self, graph: Graph) -> None:
-        """Add all nodes and edges from another graph.
-        Note this doesn't check for duplicates, nor does it connect the graphs."""
-        self.nodes.update(graph.nodes)
-        self.edges.extend(graph.edges)
-    def first_node(self) -> Optional[Node]:
-        """Find the single node that is not a target of any edge.
-        If there is no such node, or there are multiple, return None.
-        When drawing the graph this node would be the origin."""
-        targets = {edge.target for edge in self.edges}
-        found: List[Node] = []
-        for node in self.nodes.values():
-            if node.id not in targets:
-                found.append(node)
-        return found[0] if len(found) == 1 else None
-    def last_node(self) -> Optional[Node]:
-        """Find the single node that is not a source of any edge.
-        If there is no such node, or there are multiple, return None.
-        When drawing the graph this node would be the destination.
-        """
-        sources = {edge.source for edge in self.edges}
-        found: List[Node] = []
-        for node in self.nodes.values():
-            if node.id not in sources:
-                found.append(node)
-        return found[0] if len(found) == 1 else None
-    def trim_first_node(self) -> None:
-        """Remove the first node if it exists and has a single outgoing edge,
-        ie. if removing it would not leave the graph without a "first" node."""
-        first_node = self.first_node()
-        if first_node:
-            if (
-                len(self.nodes) == 1
-                or len([edge for edge in self.edges if edge.source == first_node.id])
-                == 1
-            ):
-                self.remove_node(first_node)
-    def trim_last_node(self) -> None:
-        """Remove the last node if it exists and has a single incoming edge,
-        ie. if removing it would not leave the graph without a "last" node."""
-        last_node = self.last_node()
-        if last_node:
-            if (
-                len(self.nodes) == 1
-                or len([edge for edge in self.edges if edge.target == last_node.id])
-                == 1
-            ):
-                self.remove_node(last_node)
-    def draw_ascii(self) -> str:
-        from langchain_core.runnables.base import Runnable
-        def node_data(node: Node) -> str:
-            if isinstance(node.data, Runnable):
-                try:
-                    data = str(node.data)
-                    if (
-                        data.startswith("<")
-                        or data[0] != data[0].upper()
-                        or len(data.splitlines()) > 1
-                    ):
-                        data = node.data.__class__.__name__
-                    elif len(data) > 42:
-                        data = data[:42] + "..."
-                except Exception:
-                    data = node.data.__class__.__name__
-            else:
-                data = node.data.__name__
-            return data if not data.startswith("Runnable") else data[8:]
-        return draw(
-            {node.id: node_data(node) for node in self.nodes.values()},
-            [(edge.source, edge.target) for edge in self.edges],
-        )
-    def print_ascii(self) -> None:
-        print(self.draw_ascii())

--- a/libs/core/langchain_core/runnables/graph_draw.py
+++ b//dev/null
@@ -1,227 +0,0 @@
-"""Draws DAG in ASCII.
-Adapted from https://github.com/iterative/dvc/blob/main/dvc/dagascii.py"""
-import math
-import os
-from typing import Any, Mapping, Sequence, Tuple
-class VertexViewer:
-    """Class to define vertex box boundaries that will be accounted for during
-    graph building by grandalf.
-    Args:
-        name (str): name of the vertex.
-    """
-    HEIGHT = 3  # top and bottom box edges + text
-    def __init__(self, name: str) -> None:
-        self._h = self.HEIGHT  # top and bottom box edges + text
-        self._w = len(name) + 2  # right and left bottom edges + text
-    @property
-    def h(self) -> int:
-        """Height of the box."""
-        return self._h
-    @property
-    def w(self) -> int:
-        """Width of the box."""
-        return self._w
-class AsciiCanvas:
-    """Class for drawing in ASCII.
-    Args:
-        cols (int): number of columns in the canvas. Should be > 1.
-        lines (int): number of lines in the canvas. Should be > 1.
-    """
-    TIMEOUT = 10
-    def __init__(self, cols: int, lines: int) -> None:
-        assert cols > 1
-        assert lines > 1
-        self.cols = cols
-        self.lines = lines
-        self.canvas = [[" "] * cols for line in range(lines)]
-    def draw(self) -> str:
-        """Draws ASCII canvas on the screen."""
-        lines = map("".join, self.canvas)
-        return os.linesep.join(lines)
-    def point(self, x: int, y: int, char: str) -> None:
-        """Create a point on ASCII canvas.
-        Args:
-            x (int): x coordinate. Should be >= 0 and < number of columns in
-                the canvas.
-            y (int): y coordinate. Should be >= 0 an < number of lines in the
-                canvas.
-            char (str): character to place in the specified point on the
-                canvas.
-        """
-        assert len(char) == 1
-        assert x >= 0
-        assert x < self.cols
-        assert y >= 0
-        assert y < self.lines
-        self.canvas[y][x] = char
-    def line(self, x0: int, y0: int, x1: int, y1: int, char: str) -> None:
-        """Create a line on ASCII canvas.
-        Args:
-            x0 (int): x coordinate where the line should start.
-            y0 (int): y coordinate where the line should start.
-            x1 (int): x coordinate where the line should end.
-            y1 (int): y coordinate where the line should end.
-            char (str): character to draw the line with.
-        """
-        if x0 > x1:
-            x1, x0 = x0, x1
-            y1, y0 = y0, y1
-        dx = x1 - x0
-        dy = y1 - y0
-        if dx == 0 and dy == 0:
-            self.point(x0, y0, char)
-        elif abs(dx) >= abs(dy):
-            for x in range(x0, x1 + 1):
-                if dx == 0:
-                    y = y0
-                else:
-                    y = y0 + int(round((x - x0) * dy / float(dx)))
-                self.point(x, y, char)
-        elif y0 < y1:
-            for y in range(y0, y1 + 1):
-                if dy == 0:
-                    x = x0
-                else:
-                    x = x0 + int(round((y - y0) * dx / float(dy)))
-                self.point(x, y, char)
-        else:
-            for y in range(y1, y0 + 1):
-                if dy == 0:
-                    x = x0
-                else:
-                    x = x1 + int(round((y - y1) * dx / float(dy)))
-                self.point(x, y, char)
-    def text(self, x: int, y: int, text: str) -> None:
-        """Print a text on ASCII canvas.
-        Args:
-            x (int): x coordinate where the text should start.
-            y (int): y coordinate where the text should start.
-            text (str): string that should be printed.
-        """
-        for i, char in enumerate(text):
-            self.point(x + i, y, char)
-    def box(self, x0: int, y0: int, width: int, height: int) -> None:
-        """Create a box on ASCII canvas.
-        Args:
-            x0 (int): x coordinate of the box corner.
-            y0 (int): y coordinate of the box corner.
-            width (int): box width.
-            height (int): box height.
-        """
-        assert width > 1
-        assert height > 1
-        width -= 1
-        height -= 1
-        for x in range(x0, x0 + width):
-            self.point(x, y0, "-")
-            self.point(x, y0 + height, "-")
-        for y in range(y0, y0 + height):
-            self.point(x0, y, "|")
-            self.point(x0 + width, y, "|")
-        self.point(x0, y0, "+")
-        self.point(x0 + width, y0, "+")
-        self.point(x0, y0 + height, "+")
-        self.point(x0 + width, y0 + height, "+")
-def _build_sugiyama_layout(
-    vertices: Mapping[str, str], edges: Sequence[Tuple[str, str]]
-) -> Any:
-    try:
-        from grandalf.graphs import Edge, Graph, Vertex  # type: ignore[import]
-        from grandalf.layouts import SugiyamaLayout  # type: ignore[import]
-        from grandalf.routing import (  # type: ignore[import]
-            EdgeViewer,
-            route_with_lines,
-        )
-    except ImportError:
-        print("Install grandalf to draw graphs. `pip install grandalf`")
-        raise
-    vertices_ = {id: Vertex(f" {data} ") for id, data in vertices.items()}
-    edges_ = [Edge(vertices_[s], vertices_[e]) for s, e in edges]
-    vertices_list = vertices_.values()
-    graph = Graph(vertices_list, edges_)
-    for vertex in vertices_list:
-        vertex.view = VertexViewer(vertex.data)
-    minw = min(v.view.w for v in vertices_list)
-    for edge in edges_:
-        edge.view = EdgeViewer()
-    sug = SugiyamaLayout(graph.C[0])
-    graph = graph.C[0]
-    roots = list(filter(lambda x: len(x.e_in()) == 0, graph.sV))
-    sug.init_all(roots=roots, optimize=True)
-    sug.yspace = VertexViewer.HEIGHT
-    sug.xspace = minw
-    sug.route_edge = route_with_lines
-    sug.draw()
-    return sug
-def draw(vertices: Mapping[str, str], edges: Sequence[Tuple[str, str]]) -> str:
-    """Build a DAG and draw it in ASCII.
-    Args:
-        vertices (list): list of graph vertices.
-        edges (list): list of graph edges.
-    Returns:
-        str: ASCII representation
-    Example:
-        >>> from dvc.dagascii import draw
-        >>> vertices = [1, 2, 3, 4]
-        >>> edges = [(1, 2), (2, 3), (2, 4), (1, 4)]
-        >>> print(draw(vertices, edges))
-        +---+     +---+
-        | 3 |     | 4 |
-        +---+    *+---+
-          *    **   *
-          *  **     *
-          * *       *
-        +---+       *
-        | 2 |      *
-        +---+     *
-             *    *
-              *  *
-               **
-             +---+
-             | 1 |
-             +---+
-    """
-    Xs = []  # noqa: N806
-    Ys = []  # noqa: N806
-    sug = _build_sugiyama_layout(vertices, edges)
-    for vertex in sug.g.sV:
-        Xs.append(vertex.view.xy[0] - vertex.view.w / 2.0)
-        Xs.append(vertex.view.xy[0] + vertex.view.w / 2.0)
-        Ys.append(vertex.view.xy[1])
-        Ys.append(vertex.view.xy[1] + vertex.view.h)
-    for edge in sug.g.sE:
-        for x, y in edge.view._pts:
-            Xs.append(x)
-            Ys.append(y)
-    minx = min(Xs)
-    miny = min(Ys)
-    maxx = max(Xs)
-    maxy = max(Ys)
-    canvas_cols = int(math.ceil(math.ceil(maxx) - math.floor(minx))) + 1
-    canvas_lines = int(round(maxy - miny))
-    canvas = AsciiCanvas(canvas_cols, canvas_lines)
-    for edge in sug.g.sE:
-        assert len(edge.view._pts) > 1
-        for index in range(1, len(edge.view._pts)):
-            start = edge.view._pts[index - 1]
-            end = edge.view._pts[index]
-            start_x = int(round(start[0] - minx))
-            start_y = int(round(start[1] - miny))
-            end_x = int(round(end[0] - minx))
-            end_y = int(round(end[1] - miny))
-            assert start_x >= 0
-            assert start_y >= 0
-            assert end_x >= 0
-            assert end_y >= 0
-            canvas.line(start_x, start_y, end_x, end_y, "*")
-    for vertex in sug.g.sV:
-        x = vertex.view.xy[0] - vertex.view.w / 2.0
-        y = vertex.view.xy[1]
-        canvas.box(
-            int(round(x - minx)),
-            int(round(y - miny)),
-            vertex.view.w,
-            vertex.view.h,
-        )
-        canvas.text(int(round(x - minx)) + 1, int(round(y - miny)) + 1, vertex.data)
-    return canvas.draw()

--- a/libs/core/langchain_core/runnables/passthrough.py
+++ b/libs/core/langchain_core/runnables/passthrough.py
@@ -25,21 +25,20 @@
     RunnableParallel,
     RunnableSerializable,
 )
 from langchain_core.runnables.config import (
     RunnableConfig,
     acall_func_with_variable_args,
     call_func_with_variable_args,
     get_executor_for_config,
     patch_config,
 )
-from langchain_core.runnables.graph import Graph
 from langchain_core.runnables.utils import AddableDict, ConfigurableFieldSpec
 from langchain_core.utils.aiter import atee, py_anext
 from langchain_core.utils.iter import safetee
 if TYPE_CHECKING:
     from langchain_core.callbacks.manager import (
         AsyncCallbackManagerForChainRun,
         CallbackManagerForChainRun,
     )
 def identity(x: Other) -> Other:
     """An identity function"""
@@ -236,42 +235,34 @@
     async def astream(
         self,
         input: Other,
         config: Optional[RunnableConfig] = None,
         **kwargs: Any,
     ) -> AsyncIterator[Other]:
         async def input_aiter() -> AsyncIterator[Other]:
             yield input
         async for chunk in self.atransform(input_aiter(), config, **kwargs):
             yield chunk
-_graph_passthrough: RunnablePassthrough = RunnablePassthrough()
 class RunnableAssign(RunnableSerializable[Dict[str, Any], Dict[str, Any]]):
     """
     A runnable that assigns key-value pairs to Dict[str, Any] inputs.
     """
     mapper: RunnableParallel[Dict[str, Any]]
     def __init__(self, mapper: RunnableParallel[Dict[str, Any]], **kwargs: Any) -> None:
         super().__init__(mapper=mapper, **kwargs)
     @classmethod
     def is_lc_serializable(cls) -> bool:
         return True
     @classmethod
     def get_lc_namespace(cls) -> List[str]:
         """Get the namespace of the langchain object."""
         return ["langchain", "schema", "runnable"]
-    def get_name(
-        self, suffix: Optional[str] = None, *, name: Optional[str] = None
-    ) -> str:
-        name = (
-            name or self.name or f"RunnableAssign<{','.join(self.mapper.steps.keys())}>"
-        )
-        return super().get_name(suffix, name=name)
     def get_input_schema(
         self, config: Optional[RunnableConfig] = None
     ) -> Type[BaseModel]:
         map_input_schema = self.mapper.get_input_schema(config)
         if not map_input_schema.__custom_root_type__:
             return map_input_schema
         return super().get_input_schema(config)
     def get_output_schema(
         self, config: Optional[RunnableConfig] = None
     ) -> Type[BaseModel]:
@@ -288,29 +279,20 @@
                     for s in (map_input_schema, map_output_schema)
                     for k, v in s.__fields__.items()
                 },
             )
         elif not map_output_schema.__custom_root_type__:
             return map_output_schema
         return super().get_output_schema(config)
     @property
     def config_specs(self) -> List[ConfigurableFieldSpec]:
         return self.mapper.config_specs
-    def get_graph(self, config: RunnableConfig | None = None) -> Graph:
-        graph = self.mapper.get_graph(config)
-        input_node = graph.first_node()
-        output_node = graph.last_node()
-        if input_node is not None and output_node is not None:
-            passthrough_node = graph.add_node(_graph_passthrough)
-            graph.add_edge(input_node, passthrough_node)
-            graph.add_edge(passthrough_node, output_node)
-        return graph
     def _invoke(
         self,
         input: Dict[str, Any],
         run_manager: CallbackManagerForChainRun,
         config: RunnableConfig,
         **kwargs: Any,
     ) -> Dict[str, Any]:
         assert isinstance(
             input, dict
         ), "The input to RunnablePassthrough.assign() must be a dict."
@@ -450,121 +432,10 @@
     async def astream(
         self,
         input: Dict[str, Any],
         config: Optional[RunnableConfig] = None,
         **kwargs: Any,
     ) -> AsyncIterator[Dict[str, Any]]:
         async def input_aiter() -> AsyncIterator[Dict[str, Any]]:
             yield input
         async for chunk in self.atransform(input_aiter(), config, **kwargs):
             yield chunk
-class RunnablePick(RunnableSerializable[Dict[str, Any], Dict[str, Any]]):
-    """
-    A runnable that picks keys from Dict[str, Any] inputs.
-    """
-    keys: Union[str, List[str]]
-    def __init__(self, keys: Union[str, List[str]], **kwargs: Any) -> None:
-        super().__init__(keys=keys, **kwargs)
-    @classmethod
-    def is_lc_serializable(cls) -> bool:
-        return True
-    @classmethod
-    def get_lc_namespace(cls) -> List[str]:
-        """Get the namespace of the langchain object."""
-        return ["langchain", "schema", "runnable"]
-    def get_name(
-        self, suffix: Optional[str] = None, *, name: Optional[str] = None
-    ) -> str:
-        name = (
-            name
-            or self.name
-            or f"RunnablePick<{','.join([self.keys] if isinstance(self.keys, str) else self.keys)}>"  # noqa: E501
-        )
-        return super().get_name(suffix, name=name)
-    def _pick(self, input: Dict[str, Any]) -> Any:
-        assert isinstance(
-            input, dict
-        ), "The input to RunnablePassthrough.assign() must be a dict."
-        if isinstance(self.keys, str):
-            return input.get(self.keys)
-        else:
-            picked = {k: input.get(k) for k in self.keys if k in input}
-            if picked:
-                return AddableDict(picked)
-            else:
-                return None
-    def _invoke(
-        self,
-        input: Dict[str, Any],
-    ) -> Dict[str, Any]:
-        return self._pick(input)
-    def invoke(
-        self,
-        input: Dict[str, Any],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Dict[str, Any]:
-        return self._call_with_config(self._invoke, input, config, **kwargs)
-    async def _ainvoke(
-        self,
-        input: Dict[str, Any],
-    ) -> Dict[str, Any]:
-        return self._pick(input)
-    async def ainvoke(
-        self,
-        input: Dict[str, Any],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Dict[str, Any]:
-        return await self._acall_with_config(self._ainvoke, input, config, **kwargs)
-    def _transform(
-        self,
-        input: Iterator[Dict[str, Any]],
-    ) -> Iterator[Dict[str, Any]]:
-        for chunk in input:
-            picked = self._pick(chunk)
-            if picked is not None:
-                yield picked
-    def transform(
-        self,
-        input: Iterator[Dict[str, Any]],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Iterator[Dict[str, Any]]:
-        yield from self._transform_stream_with_config(
-            input, self._transform, config, **kwargs
-        )
-    async def _atransform(
-        self,
-        input: AsyncIterator[Dict[str, Any]],
-    ) -> AsyncIterator[Dict[str, Any]]:
-        async for chunk in input:
-            picked = self._pick(chunk)
-            if picked is not None:
-                yield picked
-    async def atransform(
-        self,
-        input: AsyncIterator[Dict[str, Any]],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[Dict[str, Any]]:
-        async for chunk in self._atransform_stream_with_config(
-            input, self._atransform, config, **kwargs
-        ):
-            yield chunk
-    def stream(
-        self,
-        input: Dict[str, Any],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> Iterator[Dict[str, Any]]:
-        return self.transform(iter([input]), config, **kwargs)
-    async def astream(
-        self,
-        input: Dict[str, Any],
-        config: Optional[RunnableConfig] = None,
-        **kwargs: Any,
-    ) -> AsyncIterator[Dict[str, Any]]:
-        async def input_aiter() -> AsyncIterator[Dict[str, Any]]:
-            yield input
-        async for chunk in self.atransform(input_aiter(), config, **kwargs):
-            yield chunk

--- a/libs/core/langchain_core/runnables/utils.py
+++ b/libs/core/langchain_core/runnables/utils.py
@@ -75,70 +75,28 @@
             and len(node.args) in (1, 2)
             and isinstance(node.args[0], ast.Constant)
             and isinstance(node.args[0].value, str)
         ):
             self.keys.add(node.args[0].value)
 class IsFunctionArgDict(ast.NodeVisitor):
     """Check if the first argument of a function is a dict."""
     def __init__(self) -> None:
         self.keys: Set[str] = set()
     def visit_Lambda(self, node: ast.Lambda) -> Any:
-        if not node.args.args:
-            return
         input_arg_name = node.args.args[0].arg
         IsLocalDict(input_arg_name, self.keys).visit(node.body)
     def visit_FunctionDef(self, node: ast.FunctionDef) -> Any:
-        if not node.args.args:
-            return
         input_arg_name = node.args.args[0].arg
         IsLocalDict(input_arg_name, self.keys).visit(node)
     def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> Any:
-        if not node.args.args:
-            return
         input_arg_name = node.args.args[0].arg
         IsLocalDict(input_arg_name, self.keys).visit(node)
-class NonLocals(ast.NodeVisitor):
-    """Get nonlocal variables accessed."""
-    def __init__(self) -> None:
-        self.loads: Set[str] = set()
-        self.stores: Set[str] = set()
-    def visit_Name(self, node: ast.Name) -> Any:
-        if isinstance(node.ctx, ast.Load):
-            self.loads.add(node.id)
-        elif isinstance(node.ctx, ast.Store):
-            self.stores.add(node.id)
-    def visit_Attribute(self, node: ast.Attribute) -> Any:
-        if isinstance(node.ctx, ast.Load):
-            parent = node.value
-            attr_expr = node.attr
-            while isinstance(parent, ast.Attribute):
-                attr_expr = parent.attr + "." + attr_expr
-                parent = parent.value
-            if isinstance(parent, ast.Name):
-                self.loads.add(parent.id + "." + attr_expr)
-                self.loads.discard(parent.id)
-class FunctionNonLocals(ast.NodeVisitor):
-    """Get the nonlocal variables accessed of a function."""
-    def __init__(self) -> None:
-        self.nonlocals: Set[str] = set()
-    def visit_FunctionDef(self, node: ast.FunctionDef) -> Any:
-        visitor = NonLocals()
-        visitor.visit(node)
-        self.nonlocals.update(visitor.loads - visitor.stores)
-    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> Any:
-        visitor = NonLocals()
-        visitor.visit(node)
-        self.nonlocals.update(visitor.loads - visitor.stores)
-    def visit_Lambda(self, node: ast.Lambda) -> Any:
-        visitor = NonLocals()
-        visitor.visit(node)
-        self.nonlocals.update(visitor.loads - visitor.stores)
 class GetLambdaSource(ast.NodeVisitor):
     """Get the source code of a lambda function."""
     def __init__(self) -> None:
         """Initialize the visitor."""
         self.source: Optional[str] = None
         self.count = 0
     def visit_Lambda(self, node: ast.Lambda) -> Any:
         """Visit a lambda function."""
         self.count += 1
         if hasattr(ast, "unparse"):
@@ -154,51 +112,27 @@
     except (SyntaxError, TypeError, OSError):
         return None
 def get_lambda_source(func: Callable) -> Optional[str]:
     """Get the source code of a lambda function.
     Args:
         func: a callable that can be a lambda function
     Returns:
         str: the source code of the lambda function
     """
     try:
-        name = func.__name__ if func.__name__ != "<lambda>" else None
-    except AttributeError:
-        name = None
-    try:
         code = inspect.getsource(func)
         tree = ast.parse(textwrap.dedent(code))
         visitor = GetLambdaSource()
         visitor.visit(tree)
-        return visitor.source if visitor.count == 1 else name
+        return visitor.source if visitor.count == 1 else None
     except (SyntaxError, TypeError, OSError):
-        return name
-def get_function_nonlocals(func: Callable) -> List[Any]:
-    """Get the nonlocal variables accessed by a function."""
-    try:
-        code = inspect.getsource(func)
-        tree = ast.parse(textwrap.dedent(code))
-        visitor = FunctionNonLocals()
-        visitor.visit(tree)
-        values: List[Any] = []
-        for k, v in inspect.getclosurevars(func).nonlocals.items():
-            if k in visitor.nonlocals:
-                values.append(v)
-            for kk in visitor.nonlocals:
-                if "." in kk and kk.startswith(k):
-                    vv = v
-                    for part in kk.split(".")[1:]:
-                        vv = getattr(vv, part)
-                    values.append(vv)
-        return values
-    except (SyntaxError, TypeError, OSError):
-        return []
+        return None
 def indent_lines_after_first(text: str, prefix: str) -> str:
     """Indent all lines of text after the first line.
     Args:
         text:  The text to indent
         prefix: Used to determine the number of spaces to indent
     Returns:
         str: The indented text
     """
     n_spaces = len(prefix)
     spaces = " " * n_spaces

--- a/libs/core/langchain_core/tracers/base.py
+++ b/libs/core/langchain_core/tracers/base.py
@@ -94,21 +94,21 @@
             return 1
         parent_run = self.run_map.get(parent_run_id)
         if parent_run is None:
             logger.debug(f"Parent run with UUID {parent_run_id} not found.")
             return 1
         if parent_run.child_execution_order is None:
             raise TracerException(
                 f"Parent run with UUID {parent_run_id} has no child execution order."
             )
         return parent_run.child_execution_order + 1
-    def _get_run(self, run_id: UUID, run_type: Optional[str] = None) -> Run:
+    def _get_run(self, run_id: UUID, run_type: str | None = None) -> Run:
         try:
             run = self.run_map[str(run_id)]
         except KeyError as exc:
             raise TracerException(f"No indexed run ID {run_id}.") from exc
         if run_type is not None and run.run_type != run_type:
             raise TracerException(
                 f"Found {run.run_type} run at ID {run_id}, but expected {run_type} run."
             )
         return run
     def on_llm_start(

--- a/libs/core/langchain_core/tracers/log_stream.py
+++ b/libs/core/langchain_core/tracers/log_stream.py
@@ -29,22 +29,20 @@
     type: str
     """Type of the object being run, eg. prompt, chain, llm, etc."""
     tags: List[str]
     """List of tags for the run."""
     metadata: Dict[str, Any]
     """Key-value pairs of metadata for the run."""
     start_time: str
     """ISO-8601 timestamp of when the run started."""
     streamed_output_str: List[str]
     """List of LLM tokens streamed by this run, if applicable."""
-    streamed_output: List[Any]
-    """List of output chunks streamed by this run, if available."""
     final_output: Optional[Any]
     """Final output of this run.
     Only available after the run has finished successfully."""
     end_time: Optional[str]
     """ISO-8601 timestamp of when the run ended.
     Only available after the run has finished."""
 class RunState(TypedDict):
     """State of the run."""
     id: str
     """ID of the run."""
@@ -187,21 +185,20 @@
                 {
                     "op": "add",
                     "path": f"/logs/{self._key_map_by_run_id[run.id]}",
                     "value": LogEntry(
                         id=str(run.id),
                         name=run.name,
                         type=run.run_type,
                         tags=run.tags or [],
                         metadata=(run.extra or {}).get("metadata", {}),
                         start_time=run.start_time.isoformat(timespec="milliseconds"),
-                        streamed_output=[],
                         streamed_output_str=[],
                         final_output=None,
                         end_time=None,
                     ),
                 }
             )
         )
     def _on_run_update(self, run: Run) -> None:
         """Finish a run."""
         try:
@@ -237,20 +234,13 @@
         """Process new LLM token."""
         index = self._key_map_by_run_id.get(run.id)
         if index is None:
             return
         self.send_stream.send_nowait(
             RunLogPatch(
                 {
                     "op": "add",
                     "path": f"/logs/{index}/streamed_output_str/-",
                     "value": token,
-                },
-                {
-                    "op": "add",
-                    "path": f"/logs/{index}/streamed_output/-",
-                    "value": chunk.message
-                    if isinstance(chunk, ChatGenerationChunk)
-                    else token,
-                },
-            )
-        )
+                }
+            )
+        )

--- a/libs/core/langchain_core/utils/env.py
+++ b/libs/core/langchain_core/utils/env.py
@@ -25,12 +25,12 @@
 def get_from_env(key: str, env_key: str, default: Optional[str] = None) -> str:
     """Get a value from a dictionary or an environment variable."""
     if env_key in os.environ and os.environ[env_key]:
         return os.environ[env_key]
     elif default is not None:
         return default
     else:
         raise ValueError(
             f"Did not find {key}, please add an environment variable"
             f" `{env_key}` which contains it, or pass"
-            f" `{key}` as a named parameter."
+            f"  `{key}` as a named parameter."
         )

--- a/libs/core/langchain_core/utils/formatting.py
+++ b/libs/core/langchain_core/utils/formatting.py
@@ -1,15 +1,25 @@
 """Utilities for formatting strings."""
 from string import Formatter
-from typing import Any, List, Mapping, Sequence
+from typing import Any, List, Mapping, Sequence, Union
 class StrictFormatter(Formatter):
     """A subclass of formatter that checks for extra keys."""
+    def check_unused_args(
+        self,
+        used_args: Sequence[Union[int, str]],
+        args: Sequence,
+        kwargs: Mapping[str, Any],
+    ) -> None:
+        """Check to see if extra parameters are passed."""
+        extra = set(kwargs).difference(used_args)
+        if extra:
+            raise KeyError(extra)
     def vformat(
         self, format_string: str, args: Sequence, kwargs: Mapping[str, Any]
     ) -> str:
         """Check that no arguments are provided."""
         if len(args) > 0:
             raise ValueError(
                 "No arguments should be provided, "
                 "everything should be passed as keyword arguments."
             )
         return super().vformat(format_string, args, kwargs)

--- a/libs/langchain/langchain/agents/__init__.py
+++ b/libs/langchain/langchain/agents/__init__.py
@@ -35,46 +35,34 @@
     create_pbi_chat_agent,
     create_spark_sql_agent,
     create_sql_agent,
     create_vectorstore_agent,
     create_vectorstore_router_agent,
 )
 from langchain.agents.agent_types import AgentType
 from langchain.agents.conversational.base import ConversationalAgent
 from langchain.agents.conversational_chat.base import ConversationalChatAgent
 from langchain.agents.initialize import initialize_agent
-from langchain.agents.json_chat.base import create_json_chat_agent
 from langchain.agents.load_tools import (
     get_all_tool_names,
     load_huggingface_tool,
     load_tools,
 )
 from langchain.agents.loading import load_agent
 from langchain.agents.mrkl.base import MRKLChain, ZeroShotAgent
-from langchain.agents.openai_functions_agent.base import (
-    OpenAIFunctionsAgent,
-    create_openai_functions_agent,
-)
+from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent
 from langchain.agents.openai_functions_multi_agent.base import OpenAIMultiFunctionsAgent
-from langchain.agents.openai_tools.base import create_openai_tools_agent
-from langchain.agents.react.agent import create_react_agent
 from langchain.agents.react.base import ReActChain, ReActTextWorldAgent
-from langchain.agents.self_ask_with_search.base import (
-    SelfAskWithSearchChain,
-    create_self_ask_with_search_agent,
-)
-from langchain.agents.structured_chat.base import (
-    StructuredChatAgent,
-    create_structured_chat_agent,
-)
+from langchain.agents.self_ask_with_search.base import SelfAskWithSearchChain
+from langchain.agents.structured_chat.base import StructuredChatAgent
 from langchain.agents.tools import Tool, tool
-from langchain.agents.xml.base import XMLAgent, create_xml_agent
+from langchain.agents.xml.base import XMLAgent
 DEPRECATED_CODE = [
     "create_csv_agent",
     "create_pandas_dataframe_agent",
     "create_spark_dataframe_agent",
     "create_xorbits_agent",
 ]
 def __getattr__(name: str) -> Any:
     """Get attr name."""
     if name in DEPRECATED_CODE:
         HERE = Path(__file__).parents[1]
@@ -118,18 +106,11 @@
     "create_sql_agent",
     "create_vectorstore_agent",
     "create_vectorstore_router_agent",
     "get_all_tool_names",
     "initialize_agent",
     "load_agent",
     "load_huggingface_tool",
     "load_tools",
     "tool",
     "XMLAgent",
-    "create_openai_functions_agent",
-    "create_xml_agent",
-    "create_react_agent",
-    "create_openai_tools_agent",
-    "create_self_ask_with_search_agent",
-    "create_json_chat_agent",
-    "create_structured_chat_agent",
 ]

--- a/libs/langchain/langchain/agents/agent_iterator.py
+++ b/libs/langchain/langchain/agents/agent_iterator.py
@@ -165,21 +165,21 @@
                 if not self.yield_actions or is_final:
                     yield output
                 if is_final:
                     return
         except BaseException as e:
             run_manager.on_chain_error(e)
             raise
         yield self._stop(run_manager)
     async def __aiter__(self) -> AsyncIterator[AddableDict]:
         """
-        N.B. __aiter__ must be a normal method, so need to initialize async run manager
+        N.B. __aiter__ must be a normal method, so need to initialise async run manager
         on first __anext__ call where we can await it
         """
         logger.debug("Initialising AgentExecutorIterator (async)")
         self.reset()
         callback_manager = AsyncCallbackManager.configure(
             self.callbacks,
             self.agent_executor.callbacks,
             self.agent_executor.verbose,
             self.tags,
             self.agent_executor.tags,

--- a/libs/langchain/langchain/agents/json_chat/base.py
+++ b//dev/null
@@ -1,65 +0,0 @@
-from typing import Sequence
-from langchain_core.language_models import BaseLanguageModel
-from langchain_core.prompts.chat import ChatPromptTemplate
-from langchain_core.runnables import Runnable, RunnablePassthrough
-from langchain_core.tools import BaseTool
-from langchain.agents.format_scratchpad import format_log_to_messages
-from langchain.agents.json_chat.prompt import TEMPLATE_TOOL_RESPONSE
-from langchain.agents.output_parsers import JSONAgentOutputParser
-from langchain.tools.render import render_text_description
-def create_json_chat_agent(
-    llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: ChatPromptTemplate
-) -> Runnable:
-    """Create an agent that uses JSON to format its logic, build for Chat Models.
-    Examples:
-        .. code-block:: python
-            from langchain import hub
-            from langchain.chat_models import ChatOpenAI
-            from langchain.agents import AgentExecutor, create_json_chat_agent
-            prompt = hub.pull("hwchase17/react-chat-json")
-            model = ChatOpenAI()
-            tools = ...
-            agent = create_json_chat_agent(model, tools, prompt)
-            agent_executor = AgentExecutor(agent=agent, tools=tools)
-            agent_executor.invoke({"input": "hi"})
-            from langchain_core.messages import AIMessage, HumanMessage
-            agent_executor.invoke(
-                {
-                    "input": "what's my name?",
-                    "chat_history": [
-                        HumanMessage(content="hi! my name is bob"),
-                        AIMessage(content="Hello Bob! How can I assist you today?"),
-                    ],
-                }
-            )
-    Args:
-        llm: LLM to use as the agent.
-        tools: Tools this agent has access to.
-        prompt: The prompt to use, must have input keys of
-            `tools`, `tool_names`, and `agent_scratchpad`.
-    Returns:
-        A runnable sequence representing an agent. It takes as input all the same input
-        variables as the prompt passed in does. It returns as output either an
-        AgentAction or AgentFinish.
-    """
-    missing_vars = {"tools", "tool_names", "agent_scratchpad"}.difference(
-        prompt.input_variables
-    )
-    if missing_vars:
-        raise ValueError(f"Prompt missing required variables: {missing_vars}")
-    prompt = prompt.partial(
-        tools=render_text_description(list(tools)),
-        tool_names=", ".join([t.name for t in tools]),
-    )
-    llm_with_stop = llm.bind(stop=["\nObservation"])
-    agent = (
-        RunnablePassthrough.assign(
-            agent_scratchpad=lambda x: format_log_to_messages(
-                x["intermediate_steps"], template_tool_response=TEMPLATE_TOOL_RESPONSE
-            )
-        )
-        | prompt
-        | llm_with_stop
-        | JSONAgentOutputParser()
-    )
-    return agent

--- a/libs/langchain/langchain/agents/json_chat/prompt.py
+++ b//dev/null
@@ -1,6 +0,0 @@
-TEMPLATE_TOOL_RESPONSE = """TOOL RESPONSE: 
----------------------
-{observation}
-USER'S INPUT
---------------------
-Okay, so what is the response to my last comment? If using information obtained from the tools you must mention it explicitly without mentioning the tool names - I have forgotten all TOOL RESPONSES! Remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else - even if you just want to respond to the user. Do NOT respond with anything except a JSON snippet no matter what!"""

--- a/libs/langchain/langchain/agents/openai_functions_agent/base.py
+++ b/libs/langchain/langchain/agents/openai_functions_agent/base.py
@@ -7,21 +7,20 @@
     SystemMessage,
 )
 from langchain_core.prompts import BasePromptTemplate
 from langchain_core.prompts.chat import (
     BaseMessagePromptTemplate,
     ChatPromptTemplate,
     HumanMessagePromptTemplate,
     MessagesPlaceholder,
 )
 from langchain_core.pydantic_v1 import root_validator
-from langchain_core.runnables import Runnable, RunnablePassthrough
 from langchain_core.tools import BaseTool
 from langchain.agents import BaseSingleActionAgent
 from langchain.agents.format_scratchpad.openai_functions import (
     format_to_openai_function_messages,
 )
 from langchain.agents.output_parsers.openai_functions import (
     OpenAIFunctionsAgentOutputParser,
 )
 from langchain.callbacks.base import BaseCallbackManager
 from langchain.callbacks.manager import Callbacks
@@ -196,66 +195,10 @@
             extra_prompt_messages=extra_prompt_messages,
             system_message=system_message,
         )
         return cls(
             llm=llm,
             prompt=prompt,
             tools=tools,
             callback_manager=callback_manager,
             **kwargs,
         )
-def create_openai_functions_agent(
-    llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: ChatPromptTemplate
-) -> Runnable:
-    """Create an agent that uses OpenAI function calling.
-    Examples:
-        Creating an agent with no memory
-        .. code-block:: python
-            from langchain.chat_models import ChatOpenAI
-            from langchain.agents import AgentExecutor, create_openai_functions_agent
-            from langchain import hub
-            prompt = hub.pull("hwchase17/openai-functions-agent")
-            model = ChatOpenAI()
-            tools = ...
-            agent = create_openai_functions_agent(model, tools, prompt)
-            agent_executor = AgentExecutor(agent=agent, tools=tools)
-            agent_executor.invoke({"input": "hi"})
-            from langchain_core.messages import AIMessage, HumanMessage
-            agent_executor.invoke(
-                {
-                    "input": "what's my name?",
-                    "chat_history": [
-                        HumanMessage(content="hi! my name is bob"),
-                        AIMessage(content="Hello Bob! How can I assist you today?"),
-                    ],
-                }
-            )
-    Args:
-        llm: LLM to use as the agent. Should work with OpenAI function calling,
-            so either be an OpenAI model that supports that or a wrapper of
-            a different model that adds in equivalent support.
-        tools: Tools this agent has access to.
-        prompt: The prompt to use, must have an input key of `agent_scratchpad`.
-    Returns:
-        A runnable sequence representing an agent. It takes as input all the same input
-        variables as the prompt passed in does. It returns as output either an
-        AgentAction or AgentFinish.
-    """
-    if "agent_scratchpad" not in prompt.input_variables:
-        raise ValueError(
-            "Prompt must have input variable `agent_scratchpad`, but wasn't found. "
-            f"Found {prompt.input_variables} instead."
-        )
-    llm_with_tools = llm.bind(
-        functions=[format_tool_to_openai_function(t) for t in tools]
-    )
-    agent = (
-        RunnablePassthrough.assign(
-            agent_scratchpad=lambda x: format_to_openai_function_messages(
-                x["intermediate_steps"]
-            )
-        )
-        | prompt
-        | llm_with_tools
-        | OpenAIFunctionsAgentOutputParser()
-    )
-    return agent

--- a/libs/langchain/langchain/agents/openai_functions_multi_agent/base.py
+++ b/libs/langchain/langchain/agents/openai_functions_multi_agent/base.py
@@ -26,21 +26,21 @@
 from langchain.callbacks.manager import Callbacks
 from langchain.tools import BaseTool
 _FunctionsAgentAction = AgentActionMessageLog
 def _parse_ai_message(message: BaseMessage) -> Union[List[AgentAction], AgentFinish]:
     """Parse an AI message."""
     if not isinstance(message, AIMessage):
         raise TypeError(f"Expected an AI message got {type(message)}")
     function_call = message.additional_kwargs.get("function_call", {})
     if function_call:
         try:
-            arguments = json.loads(function_call["arguments"], strict=False)
+            arguments = json.loads(function_call["arguments"])
         except JSONDecodeError:
             raise OutputParserException(
                 f"Could not parse tool input: {function_call} because "
                 f"the `arguments` is not valid JSON."
             )
         try:
             tools = arguments["actions"]
         except (TypeError, KeyError):
             raise OutputParserException(
                 f"Could not parse tool input: {function_call} because "

--- a/libs/langchain/langchain/agents/openai_tools/base.py
+++ b//dev/null
@@ -1,61 +0,0 @@
-from typing import Sequence
-from langchain_core.language_models import BaseLanguageModel
-from langchain_core.prompts.chat import ChatPromptTemplate
-from langchain_core.runnables import Runnable, RunnablePassthrough
-from langchain_core.tools import BaseTool
-from langchain.agents.format_scratchpad.openai_tools import (
-    format_to_openai_tool_messages,
-)
-from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser
-from langchain.tools.render import format_tool_to_openai_tool
-def create_openai_tools_agent(
-    llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: ChatPromptTemplate
-) -> Runnable:
-    """Create an agent that uses OpenAI tools.
-    Examples:
-        .. code-block:: python
-            from langchain import hub
-            from langchain.chat_models import ChatOpenAI
-            from langchain.agents import AgentExecutor, create_openai_tools_agent
-            prompt = hub.pull("hwchase17/openai-tools-agent")
-            model = ChatOpenAI()
-            tools = ...
-            agent = create_openai_tools_agent(model, tools, prompt)
-            agent_executor = AgentExecutor(agent=agent, tools=tools)
-            agent_executor.invoke({"input": "hi"})
-            from langchain_core.messages import AIMessage, HumanMessage
-            agent_executor.invoke(
-                {
-                    "input": "what's my name?",
-                    "chat_history": [
-                        HumanMessage(content="hi! my name is bob"),
-                        AIMessage(content="Hello Bob! How can I assist you today?"),
-                    ],
-                }
-            )
-    Args:
-        llm: LLM to use as the agent.
-        tools: Tools this agent has access to.
-        prompt: The prompt to use, must have input keys of `agent_scratchpad`.
-    Returns:
-        A runnable sequence representing an agent. It takes as input all the same input
-        variables as the prompt passed in does. It returns as output either an
-        AgentAction or AgentFinish.
-    """
-    missing_vars = {"agent_scratchpad"}.difference(prompt.input_variables)
-    if missing_vars:
-        raise ValueError(f"Prompt missing required variables: {missing_vars}")
-    llm_with_tools = llm.bind(
-        tools=[format_tool_to_openai_tool(tool) for tool in tools]
-    )
-    agent = (
-        RunnablePassthrough.assign(
-            agent_scratchpad=lambda x: format_to_openai_tool_messages(
-                x["intermediate_steps"]
-            )
-        )
-        | prompt
-        | llm_with_tools
-        | OpenAIToolsAgentOutputParser()
-    )
-    return agent

--- a/libs/langchain/langchain/agents/output_parsers/openai_functions.py
+++ b/libs/langchain/langchain/agents/output_parsers/openai_functions.py
@@ -26,21 +26,21 @@
         """Parse an AI message."""
         if not isinstance(message, AIMessage):
             raise TypeError(f"Expected an AI message got {type(message)}")
         function_call = message.additional_kwargs.get("function_call", {})
         if function_call:
             function_name = function_call["name"]
             try:
                 if len(function_call["arguments"].strip()) == 0:
                     _tool_input = {}
                 else:
-                    _tool_input = json.loads(function_call["arguments"], strict=False)
+                    _tool_input = json.loads(function_call["arguments"])
             except JSONDecodeError:
                 raise OutputParserException(
                     f"Could not parse tool input: {function_call} because "
                     f"the `arguments` is not valid JSON."
                 )
             if "__arg1" in _tool_input:
                 tool_input = _tool_input["__arg1"]
             else:
                 tool_input = _tool_input
             content_msg = f"responded: {message.content}\n" if message.content else "\n"

--- a/libs/langchain/langchain/agents/react/agent.py
+++ b//dev/null
@@ -1,60 +0,0 @@
-from __future__ import annotations
-from typing import Sequence
-from langchain_core.language_models import BaseLanguageModel
-from langchain_core.prompts import BasePromptTemplate
-from langchain_core.runnables import Runnable, RunnablePassthrough
-from langchain_core.tools import BaseTool
-from langchain.agents.format_scratchpad import format_log_to_str
-from langchain.agents.output_parsers import ReActSingleInputOutputParser
-from langchain.tools.render import render_text_description
-def create_react_agent(
-    llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: BasePromptTemplate
-) -> Runnable:
-    """Create an agent that uses ReAct prompting.
-    Examples:
-        .. code-block:: python
-            from langchain import hub
-            from langchain.llms import OpenAI
-            from langchain.agents import AgentExecutor, create_react_agent
-            prompt = hub.pull("hwchase17/react")
-            model = OpenAI()
-            tools = ...
-            agent = create_react_agent(model, tools, prompt)
-            agent_executor = AgentExecutor(agent=agent, tools=tools)
-            agent_executor.invoke({"input": "hi"})
-            from langchain_core.messages import AIMessage, HumanMessage
-            agent_executor.invoke(
-                {
-                    "input": "what's my name?",
-                    "chat_history": "Human: My name is Bob\nAI: Hello Bob!",
-                }
-            )
-    Args:
-        llm: LLM to use as the agent.
-        tools: Tools this agent has access to.
-        prompt: The prompt to use, must have input keys of
-            `tools`, `tool_names`, and `agent_scratchpad`.
-    Returns:
-        A runnable sequence representing an agent. It takes as input all the same input
-        variables as the prompt passed in does. It returns as output either an
-        AgentAction or AgentFinish.
-    """
-    missing_vars = {"tools", "tool_names", "agent_scratchpad"}.difference(
-        prompt.input_variables
-    )
-    if missing_vars:
-        raise ValueError(f"Prompt missing required variables: {missing_vars}")
-    prompt = prompt.partial(
-        tools=render_text_description(list(tools)),
-        tool_names=", ".join([t.name for t in tools]),
-    )
-    llm_with_stop = llm.bind(stop=["\nObservation"])
-    agent = (
-        RunnablePassthrough.assign(
-            agent_scratchpad=lambda x: format_log_to_str(x["intermediate_steps"]),
-        )
-        | prompt
-        | llm_with_stop
-        | ReActSingleInputOutputParser()
-    )
-    return agent

--- a/libs/langchain/langchain/agents/self_ask_with_search/base.py
+++ b/libs/langchain/langchain/agents/self_ask_with_search/base.py
@@ -1,20 +1,18 @@
 """Chain that does self-ask with search."""
 from typing import Any, Sequence, Union
 from langchain_core.language_models import BaseLanguageModel
 from langchain_core.prompts import BasePromptTemplate
 from langchain_core.pydantic_v1 import Field
-from langchain_core.runnables import Runnable, RunnablePassthrough
 from langchain_core.tools import BaseTool
 from langchain.agents.agent import Agent, AgentExecutor, AgentOutputParser
 from langchain.agents.agent_types import AgentType
-from langchain.agents.format_scratchpad import format_log_to_str
 from langchain.agents.self_ask_with_search.output_parser import SelfAskOutputParser
 from langchain.agents.self_ask_with_search.prompt import PROMPT
 from langchain.agents.tools import Tool
 from langchain.agents.utils import validate_tools_single_input
 from langchain.utilities.google_serper import GoogleSerperAPIWrapper
 from langchain.utilities.searchapi import SearchApiAPIWrapper
 from langchain.utilities.serpapi import SerpAPIWrapper
 class SelfAskWithSearchAgent(Agent):
     """Agent for the self-ask-with-search paper."""
     output_parser: AgentOutputParser = Field(default_factory=SelfAskOutputParser)
@@ -60,62 +58,10 @@
     ):
         """Initialize only with an LLM and a search chain."""
         search_tool = Tool(
             name="Intermediate Answer",
             func=search_chain.run,
             coroutine=search_chain.arun,
             description="Search",
         )
         agent = SelfAskWithSearchAgent.from_llm_and_tools(llm, [search_tool])
         super().__init__(agent=agent, tools=[search_tool], **kwargs)
-def create_self_ask_with_search_agent(
-    llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: BasePromptTemplate
-) -> Runnable:
-    """Create an agent that uses self-ask with search prompting.
-    Examples:
-        .. code-block:: python
-            from langchain import hub
-            from langchain.chat_models import ChatAnthropic
-            from langchain.agents import (
-                AgentExecutor, create_self_ask_with_search_agent
-            )
-            prompt = hub.pull("hwchase17/self-ask-with-search")
-            model = ChatAnthropic()
-            tools = [...]  # Should just be one tool with name `Intermediate Answer`
-            agent = create_self_ask_with_search_agent(model, tools, prompt)
-            agent_executor = AgentExecutor(agent=agent, tools=tools)
-            agent_executor.invoke({"input": "hi"})
-    Args:
-        llm: LLM to use as the agent.
-        tools: List of tools. Should just be of length 1, with that tool having
-            name `Intermediate Answer`
-        prompt: The prompt to use, must have input keys of `agent_scratchpad`.
-    Returns:
-        A runnable sequence representing an agent. It takes as input all the same input
-        variables as the prompt passed in does. It returns as output either an
-        AgentAction or AgentFinish.
-    """
-    missing_vars = {"agent_scratchpad"}.difference(prompt.input_variables)
-    if missing_vars:
-        raise ValueError(f"Prompt missing required variables: {missing_vars}")
-    if len(tools) != 1:
-        raise ValueError("This agent expects exactly one tool")
-    tool = list(tools)[0]
-    if tool.name != "Intermediate Answer":
-        raise ValueError(
-            "This agent expects the tool to be named `Intermediate Answer`"
-        )
-    llm_with_stop = llm.bind(stop=["\nIntermediate answer:"])
-    agent = (
-        RunnablePassthrough.assign(
-            agent_scratchpad=lambda x: format_log_to_str(
-                x["intermediate_steps"],
-                observation_prefix="\nIntermediate answer: ",
-                llm_prefix="",
-            ),
-            chat_history=lambda x: x.get("chat_history", ""),
-        )
-        | prompt
-        | llm_with_stop
-        | SelfAskOutputParser()
-    )
-    return agent

--- a/libs/langchain/langchain/agents/structured_chat/base.py
+++ b/libs/langchain/langchain/agents/structured_chat/base.py
@@ -2,32 +2,28 @@
 from typing import Any, List, Optional, Sequence, Tuple
 from langchain_core.agents import AgentAction
 from langchain_core.language_models import BaseLanguageModel
 from langchain_core.prompts import BasePromptTemplate
 from langchain_core.prompts.chat import (
     ChatPromptTemplate,
     HumanMessagePromptTemplate,
     SystemMessagePromptTemplate,
 )
 from langchain_core.pydantic_v1 import Field
-from langchain_core.runnables import Runnable, RunnablePassthrough
 from langchain.agents.agent import Agent, AgentOutputParser
-from langchain.agents.format_scratchpad import format_log_to_str
-from langchain.agents.output_parsers import JSONAgentOutputParser
 from langchain.agents.structured_chat.output_parser import (
     StructuredChatOutputParserWithRetries,
 )
 from langchain.agents.structured_chat.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX
 from langchain.callbacks.base import BaseCallbackManager
 from langchain.chains.llm import LLMChain
 from langchain.tools import BaseTool
-from langchain.tools.render import render_text_description_and_args
 HUMAN_MESSAGE_TEMPLATE = "{input}\n\n{agent_scratchpad}"
 class StructuredChatAgent(Agent):
     """Structured Chat Agent."""
     output_parser: AgentOutputParser = Field(
         default_factory=StructuredChatOutputParserWithRetries
     )
     """Output parser for the agent."""
     @property
     def observation_prefix(self) -> str:
         """Prefix to append the observation with."""
@@ -124,64 +120,10 @@
         _output_parser = output_parser or cls._get_default_output_parser(llm=llm)
         return cls(
             llm_chain=llm_chain,
             allowed_tools=tool_names,
             output_parser=_output_parser,
             **kwargs,
         )
     @property
     def _agent_type(self) -> str:
         raise ValueError
-def create_structured_chat_agent(
-    llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: ChatPromptTemplate
-) -> Runnable:
-    """Create an agent aimed at supporting tools with multiple inputs.
-    Examples:
-        .. code-block:: python
-            from langchain import hub
-            from langchain.chat_models import ChatOpenAI
-            from langchain.agents import AgentExecutor, create_structured_chat_agent
-            prompt = hub.pull("hwchase17/structured-chat-agent")
-            model = ChatOpenAI()
-            tools = ...
-            agent = create_structured_chat_agent(model, tools, prompt)
-            agent_executor = AgentExecutor(agent=agent, tools=tools)
-            agent_executor.invoke({"input": "hi"})
-            from langchain_core.messages import AIMessage, HumanMessage
-            agent_executor.invoke(
-                {
-                    "input": "what's my name?",
-                    "chat_history": [
-                        HumanMessage(content="hi! my name is bob"),
-                        AIMessage(content="Hello Bob! How can I assist you today?"),
-                    ],
-                }
-            )
-    Args:
-        llm: LLM to use as the agent.
-        tools: Tools this agent has access to.
-        prompt: The prompt to use, must have input keys of
-            `tools`, `tool_names`, and `agent_scratchpad`.
-    Returns:
-        A runnable sequence representing an agent. It takes as input all the same input
-        variables as the prompt passed in does. It returns as output either an
-        AgentAction or AgentFinish.
-    """
-    missing_vars = {"tools", "tool_names", "agent_scratchpad"}.difference(
-        prompt.input_variables
-    )
-    if missing_vars:
-        raise ValueError(f"Prompt missing required variables: {missing_vars}")
-    prompt = prompt.partial(
-        tools=render_text_description_and_args(list(tools)),
-        tool_names=", ".join([t.name for t in tools]),
-    )
-    llm_with_stop = llm.bind(stop=["Observation"])
-    agent = (
-        RunnablePassthrough.assign(
-            agent_scratchpad=lambda x: format_log_to_str(x["intermediate_steps"]),
-        )
-        | prompt
-        | llm_with_stop
-        | JSONAgentOutputParser()
-    )
-    return agent

--- a/libs/langchain/langchain/agents/xml/base.py
+++ b/libs/langchain/langchain/agents/xml/base.py
@@ -1,24 +1,19 @@
-from typing import Any, List, Sequence, Tuple, Union
+from typing import Any, List, Tuple, Union
 from langchain_core.agents import AgentAction, AgentFinish
-from langchain_core.language_models import BaseLanguageModel
-from langchain_core.prompts.base import BasePromptTemplate
 from langchain_core.prompts.chat import AIMessagePromptTemplate, ChatPromptTemplate
-from langchain_core.runnables import Runnable, RunnablePassthrough
 from langchain_core.tools import BaseTool
 from langchain.agents.agent import BaseSingleActionAgent
-from langchain.agents.format_scratchpad import format_xml
-from langchain.agents.output_parsers import XMLAgentOutputParser
+from langchain.agents.output_parsers.xml import XMLAgentOutputParser
 from langchain.agents.xml.prompt import agent_instructions
 from langchain.callbacks.base import Callbacks
 from langchain.chains.llm import LLMChain
-from langchain.tools.render import render_text_description
 class XMLAgent(BaseSingleActionAgent):
     """Agent that uses XML tags.
     Args:
         tools: list of tools the agent can choose from
         llm_chain: The LLMChain to call to predict the next action
     Examples:
         .. code-block:: python
             from langchain.agents import XMLAgent
             from langchain
             tools = ...
@@ -26,24 +21,23 @@
     """
     tools: List[BaseTool]
     """List of tools this agent has access to."""
     llm_chain: LLMChain
     """Chain to use to predict action."""
     @property
     def input_keys(self) -> List[str]:
         return ["input"]
     @staticmethod
     def get_default_prompt() -> ChatPromptTemplate:
-        base_prompt = ChatPromptTemplate.from_template(agent_instructions)
-        return base_prompt + AIMessagePromptTemplate.from_template(
-            "{intermediate_steps}"
-        )
+        return ChatPromptTemplate.from_template(
+            agent_instructions
+        ) + AIMessagePromptTemplate.from_template("{intermediate_steps}")
     @staticmethod
     def get_default_output_parser() -> XMLAgentOutputParser:
         return XMLAgentOutputParser()
     def plan(
         self,
         intermediate_steps: List[Tuple[AgentAction, str]],
         callbacks: Callbacks = None,
         **kwargs: Any,
     ) -> Union[AgentAction, AgentFinish]:
         log = ""
@@ -79,58 +73,10 @@
         for tool in self.tools:
             tools += f"{tool.name}: {tool.description}\n"
         inputs = {
             "intermediate_steps": log,
             "tools": tools,
             "question": kwargs["input"],
             "stop": ["</tool_input>", "</final_answer>"],
         }
         response = await self.llm_chain.acall(inputs, callbacks=callbacks)
         return response[self.llm_chain.output_key]
-def create_xml_agent(
-    llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: BasePromptTemplate
-) -> Runnable:
-    """Create an agent that uses XML to format its logic.
-    Examples:
-        .. code-block:: python
-            from langchain import hub
-            from langchain.chat_models import ChatAnthropic
-            from langchain.agents import AgentExecutor, create_xml_agent
-            prompt = hub.pull("hwchase17/xml-agent-convo")
-            model = ChatAnthropic()
-            tools = ...
-            agent = create_xml_agent(model, tools, prompt)
-            agent_executor = AgentExecutor(agent=agent, tools=tools)
-            agent_executor.invoke({"input": "hi"})
-            from langchain_core.messages import AIMessage, HumanMessage
-            agent_executor.invoke(
-                {
-                    "input": "what's my name?",
-                    "chat_history": "Human: My name is Bob\nAI: Hello Bob!",
-                }
-            )
-    Args:
-        llm: LLM to use as the agent.
-        tools: Tools this agent has access to.
-        prompt: The prompt to use, must have input keys of
-            `tools` and `agent_scratchpad`.
-    Returns:
-        A runnable sequence representing an agent. It takes as input all the same input
-        variables as the prompt passed in does. It returns as output either an
-        AgentAction or AgentFinish.
-    """
-    missing_vars = {"tools", "agent_scratchpad"}.difference(prompt.input_variables)
-    if missing_vars:
-        raise ValueError(f"Prompt missing required variables: {missing_vars}")
-    prompt = prompt.partial(
-        tools=render_text_description(list(tools)),
-    )
-    llm_with_stop = llm.bind(stop=["</tool_input>"])
-    agent = (
-        RunnablePassthrough.assign(
-            agent_scratchpad=lambda x: format_xml(x["intermediate_steps"]),
-        )
-        | prompt
-        | llm_with_stop
-        | XMLAgentOutputParser()
-    )
-    return agent

--- a/libs/langchain/langchain/chains/__init__.py
+++ b/libs/langchain/langchain/chains/__init__.py
@@ -28,21 +28,20 @@
 from langchain.chains.flare.base import FlareChain
 from langchain.chains.graph_qa.arangodb import ArangoGraphQAChain
 from langchain.chains.graph_qa.base import GraphQAChain
 from langchain.chains.graph_qa.cypher import GraphCypherQAChain
 from langchain.chains.graph_qa.falkordb import FalkorDBQAChain
 from langchain.chains.graph_qa.hugegraph import HugeGraphQAChain
 from langchain.chains.graph_qa.kuzu import KuzuQAChain
 from langchain.chains.graph_qa.nebulagraph import NebulaGraphQAChain
 from langchain.chains.graph_qa.neptune_cypher import NeptuneOpenCypherQAChain
 from langchain.chains.graph_qa.sparql import GraphSparqlQAChain
-from langchain.chains.history_aware_retriever import create_history_aware_retriever
 from langchain.chains.hyde.base import HypotheticalDocumentEmbedder
 from langchain.chains.llm import LLMChain
 from langchain.chains.llm_checker.base import LLMCheckerChain
 from langchain.chains.llm_math.base import LLMMathChain
 from langchain.chains.llm_requests import LLMRequestsChain
 from langchain.chains.llm_summarization_checker.base import LLMSummarizationCheckerChain
 from langchain.chains.loading import load_chain
 from langchain.chains.mapreduce import MapReduceChain
 from langchain.chains.moderation import OpenAIModerationChain
 from langchain.chains.natbot.base import NatBotChain
@@ -52,25 +51,21 @@
     create_extraction_chain_pydantic,
     create_qa_with_sources_chain,
     create_qa_with_structure_chain,
     create_tagging_chain,
     create_tagging_chain_pydantic,
 )
 from langchain.chains.qa_generation.base import QAGenerationChain
 from langchain.chains.qa_with_sources.base import QAWithSourcesChain
 from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain
 from langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain
-from langchain.chains.retrieval import create_retrieval_chain
-from langchain.chains.retrieval_qa.base import (
-    RetrievalQA,
-    VectorDBQA,
-)
+from langchain.chains.retrieval_qa.base import RetrievalQA, VectorDBQA
 from langchain.chains.router import (
     LLMRouterChain,
     MultiPromptChain,
     MultiRetrievalQAChain,
     MultiRouteChain,
     RouterChain,
 )
 from langchain.chains.sequential import SequentialChain, SimpleSequentialChain
 from langchain.chains.sql_database.query import create_sql_query_chain
 from langchain.chains.transform import TransformChain
@@ -123,13 +118,11 @@
     "create_citation_fuzzy_match_chain",
     "create_extraction_chain",
     "create_extraction_chain_pydantic",
     "create_qa_with_sources_chain",
     "create_qa_with_structure_chain",
     "create_tagging_chain",
     "create_tagging_chain_pydantic",
     "generate_example",
     "load_chain",
     "create_sql_query_chain",
-    "create_retrieval_chain",
-    "create_history_aware_retriever",
 ]

--- a/libs/langchain/langchain/chains/api/news_docs.py
+++ b/libs/langchain/langchain/chains/api/news_docs.py
@@ -1,19 +1,19 @@
 NEWS_DOCS = """API documentation:
 Endpoint: https://newsapi.org
 Top headlines /v2/top-headlines
 This endpoint provides live top and breaking headlines for a country, specific category in a country, single source, or multiple sources. You can also search with keywords. Articles are sorted by the earliest date published first.
 This endpoint is great for retrieving headlines for use with news tickers or similar.
 Request parameters
     country | The 2-letter ISO 3166-1 code of the country you want to get headlines for. Possible options: ae ar at au be bg br ca ch cn co cu cz de eg fr gb gr hk hu id ie il in it jp kr lt lv ma mx my ng nl no nz ph pl pt ro rs ru sa se sg si sk th tr tw ua us ve za. Note: you can't mix this param with the sources param.
     category | The category you want to get headlines for. Possible options: business entertainment general health science sports technology. Note: you can't mix this param with the sources param.
-    sources | A comma-separated string of identifiers for the news sources or blogs you want headlines from. Use the /top-headlines/sources endpoint to locate these programmatically or look at the sources index. Note: you can't mix this param with the country or category params.
+    sources | A comma-seperated string of identifiers for the news sources or blogs you want headlines from. Use the /top-headlines/sources endpoint to locate these programmatically or look at the sources index. Note: you can't mix this param with the country or category params.
     q | Keywords or a phrase to search for.
     pageSize | int | The number of results to return per page (request). 20 is the default, 100 is the maximum.
     page | int | Use this to page through the results if the total results found is greater than the page size.
 Response object
     status | string | If the request was successful or not. Options: ok, error. In the case of error a code and message property will be populated.
     totalResults | int | The total number of results available for your request.
     articles | array[article] | The results of the request.
     source | object | The identifier id and a display name name for the source this article came from.
     author | string | The author of the article
     title | string | The headline or title of the article.

--- a/libs/langchain/langchain/chains/combine_documents/__init__.py
+++ b/libs/langchain/langchain/chains/combine_documents/__init__.py
@@ -1,13 +1,7 @@
 """Different ways to combine documents."""
 from langchain.chains.combine_documents.reduce import (
     acollapse_docs,
     collapse_docs,
     split_list_of_docs,
 )
-from langchain.chains.combine_documents.stuff import create_stuff_documents_chain
-__all__ = [
-    "acollapse_docs",
-    "collapse_docs",
-    "split_list_of_docs",
-    "create_stuff_documents_chain",
-]
+__all__ = ["acollapse_docs", "collapse_docs", "split_list_of_docs"]

--- a/libs/langchain/langchain/chains/combine_documents/base.py
+++ b/libs/langchain/langchain/chains/combine_documents/base.py
@@ -1,32 +1,22 @@
 """Base interface for chains combining documents."""
 from abc import ABC, abstractmethod
 from typing import Any, Dict, List, Optional, Tuple, Type
 from langchain_core.documents import Document
-from langchain_core.prompts import BasePromptTemplate, PromptTemplate
 from langchain_core.pydantic_v1 import BaseModel, Field, create_model
 from langchain_core.runnables.config import RunnableConfig
 from langchain.callbacks.manager import (
     AsyncCallbackManagerForChainRun,
     CallbackManagerForChainRun,
 )
 from langchain.chains.base import Chain
 from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter
-DEFAULT_DOCUMENT_SEPARATOR = "\n\n"
-DOCUMENTS_KEY = "context"
-DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template("{page_content}")
-def _validate_prompt(prompt: BasePromptTemplate) -> None:
-    if DOCUMENTS_KEY not in prompt.input_variables:
-        raise ValueError(
-            f"Prompt must accept {DOCUMENTS_KEY} as an input variable. Received prompt "
-            f"with input variables: {prompt.input_variables}"
-        )
 class BaseCombineDocumentsChain(Chain, ABC):
     """Base interface for chains combining documents.
     Subclasses of this chain deal with combining documents in a variety of
     ways. This base class exists to add some uniformity in the interface these types
     of chains should expose. Namely, they expect an input key related to the documents
     to use (default `input_documents`), and then also expose a method to calculate
     the length of a prompt from documents (useful for outside callers to use to
     determine whether it's safe to pass a list of documents into this chain or whether
     that will longer than the context length).
     """

--- a/libs/langchain/langchain/chains/combine_documents/stuff.py
+++ b/libs/langchain/langchain/chains/combine_documents/stuff.py
@@ -1,84 +1,23 @@
 """Chain that combines documents by stuffing into context."""
 from typing import Any, Dict, List, Optional, Tuple
 from langchain_core.documents import Document
-from langchain_core.language_models import LanguageModelLike
-from langchain_core.output_parsers import BaseOutputParser, StrOutputParser
 from langchain_core.prompts import BasePromptTemplate, format_document
+from langchain_core.prompts.prompt import PromptTemplate
 from langchain_core.pydantic_v1 import Extra, Field, root_validator
-from langchain_core.runnables import Runnable, RunnablePassthrough
 from langchain.callbacks.manager import Callbacks
 from langchain.chains.combine_documents.base import (
-    DEFAULT_DOCUMENT_PROMPT,
-    DEFAULT_DOCUMENT_SEPARATOR,
-    DOCUMENTS_KEY,
     BaseCombineDocumentsChain,
-    _validate_prompt,
 )
 from langchain.chains.llm import LLMChain
-def create_stuff_documents_chain(
-    llm: LanguageModelLike,
-    prompt: BasePromptTemplate,
-    *,
-    output_parser: Optional[BaseOutputParser] = None,
-    document_prompt: Optional[BasePromptTemplate] = None,
-    document_separator: str = DEFAULT_DOCUMENT_SEPARATOR,
-) -> Runnable[Dict[str, Any], Any]:
-    """Create a chain for passing a list of Documents to a model.
-    Args:
-        llm: Language model.
-        prompt: Prompt template. Must contain input variable "context", which will be
-            used for passing in the formatted documents.
-        output_parser: Output parser. Defaults to StrOutputParser.
-        document_prompt: Prompt used for formatting each document into a string. Input
-            variables can be "page_content" or any metadata keys that are in all
-            documents. "page_content" will automatically retrieve the
-            `Document.page_content`, and all other inputs variables will be
-            automatically retrieved from the `Document.metadata` dictionary. Default to
-            a prompt that only contains `Document.page_content`.
-        document_separator: String separator to use between formatted document strings.
-    Returns:
-        An LCEL Runnable. The input is a dictionary that must have a "context" key that
-        maps to a List[Document], and any other input variables expected in the prompt.
-        The Runnable return type depends on output_parser used.
-    Example:
-        .. code-block:: python
-            from langchain_community.chat_models import ChatOpenAI
-            from langchain_core.documents import Document
-            from langchain_core.prompts import ChatPromptTemplate
-            from langchain.chains.combine_documents import create_stuff_documents_chain
-            prompt = ChatPromptTemplate.from_messages(
-                [("system", "What are everyone's favorite colors:\n\n{context}")]
-            )
-            llm = ChatOpenAI(model_name="gpt-3.5-turbo")
-            chain = create_stuff_documents_chain(llm, prompt)
-            docs = [
-                Document(page_content="Jesse loves red but not yellow"),
-                Document(page_content = "Jamal loves green but not as much as he loves orange")
-            ]
-            chain.invoke({"context": docs})
-    """  # noqa: E501
-    _validate_prompt(prompt)
-    _document_prompt = document_prompt or DEFAULT_DOCUMENT_PROMPT
-    _output_parser = output_parser or StrOutputParser()
-    def format_docs(inputs: dict) -> str:
-        return document_separator.join(
-            format_document(doc, _document_prompt) for doc in inputs[DOCUMENTS_KEY]
-        )
-    return (
-        RunnablePassthrough.assign(**{DOCUMENTS_KEY: format_docs}).with_config(
-            run_name="format_inputs"
-        )
-        | prompt
-        | llm
-        | _output_parser
-    ).with_config(run_name="stuff_documents_chain")
+def _get_default_document_prompt() -> PromptTemplate:
+    return PromptTemplate(input_variables=["page_content"], template="{page_content}")
 class StuffDocumentsChain(BaseCombineDocumentsChain):
     """Chain that combines documents by stuffing into context.
     This chain takes a list of documents and first combines them into a single string.
     It does this by formatting each document into a string with the `document_prompt`
     and then joining them together with `document_separator`. It then adds that new
     string to the inputs with the variable name set by `document_variable_name`.
     Those inputs are then passed to the `llm_chain`.
     Example:
         .. code-block:: python
             from langchain.chains import StuffDocumentsChain, LLMChain
@@ -97,21 +36,21 @@
             chain = StuffDocumentsChain(
                 llm_chain=llm_chain,
                 document_prompt=document_prompt,
                 document_variable_name=document_variable_name
             )
     """
     llm_chain: LLMChain
     """LLM chain which is called with the formatted document string,
     along with any other inputs."""
     document_prompt: BasePromptTemplate = Field(
-        default_factory=lambda: DEFAULT_DOCUMENT_PROMPT
+        default_factory=_get_default_document_prompt
     )
     """Prompt to use to format each document, gets passed to `format_document`."""
     document_variable_name: str
     """The variable name in the llm_chain to put the documents in.
     If only one variable in the llm_chain, this need not be provided."""
     document_separator: str = "\n\n"
     """The string with which to join the formatted documents"""
     class Config:
         """Configuration for this pydantic object."""
         extra = Extra.forbid

--- a/libs/langchain/langchain/chains/conversational_retrieval/base.py
+++ b/libs/langchain/langchain/chains/conversational_retrieval/base.py
@@ -4,21 +4,21 @@
 import warnings
 from abc import abstractmethod
 from pathlib import Path
 from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union
 from langchain_core.documents import Document
 from langchain_core.language_models import BaseLanguageModel
 from langchain_core.messages import BaseMessage
 from langchain_core.prompts import BasePromptTemplate
 from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
 from langchain_core.retrievers import BaseRetriever
-from langchain_core.runnables import RunnableConfig
+from langchain_core.runnables.config import RunnableConfig
 from langchain_core.vectorstores import VectorStore
 from langchain.callbacks.manager import (
     AsyncCallbackManagerForChainRun,
     CallbackManagerForChainRun,
     Callbacks,
 )
 from langchain.chains.base import Chain
 from langchain.chains.combine_documents.base import BaseCombineDocumentsChain
 from langchain.chains.combine_documents.stuff import StuffDocumentsChain
 from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT

--- a/libs/langchain/langchain/chains/history_aware_retriever.py
+++ b//dev/null
@@ -1,50 +0,0 @@
-from __future__ import annotations
-from langchain_core.language_models import LanguageModelLike
-from langchain_core.output_parsers import StrOutputParser
-from langchain_core.prompts import BasePromptTemplate
-from langchain_core.retrievers import RetrieverLike, RetrieverOutputLike
-from langchain_core.runnables import RunnableBranch
-def create_history_aware_retriever(
-    llm: LanguageModelLike,
-    retriever: RetrieverLike,
-    prompt: BasePromptTemplate,
-) -> RetrieverOutputLike:
-    """Create a chain that takes conversation history and returns documents.
-    If there is no `chat_history`, then the `input` is just passed directly to the
-    retriever. If there is `chat_history`, then the prompt and LLM will be used
-    to generate a search query. That search query is then passed to the retriever.
-    Args:
-        llm: Language model to use for generating a search term given chat history
-        retriever: RetrieverLike object that takes a string as input and outputs
-            a list of Documents.
-        prompt: The prompt used to generate the search query for the retriever.
-    Returns:
-        An LCEL Runnable. The runnable input must take in `input`, and if there
-        is chat history should take it in the form of `chat_history`.
-        The Runnable output is a list of Documents
-    Example:
-        .. code-block:: python
-            from langchain_community.chat_models import ChatOpenAI
-            from langchain.chains import create_history_aware_retriever
-            from langchain import hub
-            rephrase_prompt = hub.pull("langchain-ai/chat-langchain-rephrase")
-            llm = ChatOpenAI()
-            retriever = ...
-            chat_retriever_chain = create_history_aware_retriever(
-                llm, retriever, rephrase_prompt
-            )
-            chain.invoke({"input": "...", "chat_history": })
-    """
-    if "input" not in prompt.input_variables:
-        raise ValueError(
-            "Expected `input` to be a prompt variable, "
-            f"but got {prompt.input_variables}"
-        )
-    retrieve_documents: RetrieverOutputLike = RunnableBranch(
-        (
-            lambda x: not x.get("chat_history", False),
-            (lambda x: x["input"]) | retriever,
-        ),
-        prompt | llm | StrOutputParser() | retriever,
-    ).with_config(run_name="chat_retriever_chain")
-    return retrieve_documents

--- a/libs/langchain/langchain/chains/retrieval.py
+++ b//dev/null
@@ -1,54 +0,0 @@
-from __future__ import annotations
-from typing import Any, Dict, Union
-from langchain_core.retrievers import (
-    BaseRetriever,
-    RetrieverOutput,
-)
-from langchain_core.runnables import Runnable, RunnablePassthrough
-def create_retrieval_chain(
-    retriever: Union[BaseRetriever, Runnable[dict, RetrieverOutput]],
-    combine_docs_chain: Runnable[Dict[str, Any], str],
-) -> Runnable:
-    """Create retrieval chain that retrieves documents and then passes them on.
-    Args:
-        retriever: Retriever-like object that returns list of documents. Should
-            either be a subclass of BaseRetriever or a Runnable that returns
-            a list of documents. If a subclass of BaseRetriever, then it
-            is expected that an `input` key be passed in - this is what
-            is will be used to pass into the retriever. If this is NOT a
-            subclass of BaseRetriever, then all the inputs will be passed
-            into this runnable, meaning that runnable should take a dictionary
-            as input.
-        combine_docs_chain: Runnable that takes inputs and produces a string output.
-            The inputs to this will be any original inputs to this chain, a new
-            context key with the retrieved documents, and chat_history (if not present
-            in the inputs) with a value of `[]` (to easily enable conversational
-            retrieval.
-    Returns:
-        An LCEL Runnable. The Runnable return is a dictionary containing at the very
-        least a `context` and `answer` key.
-    Example:
-        .. code-block:: python
-            from langchain_community.chat_models import ChatOpenAI
-            from langchain.chains.combine_documents import create_stuff_documents_chain
-            from langchain.chains import create_retrieval_chain
-            from langchain import hub
-            retrieval_qa_chat_prompt = hub.pull("langchain-ai/retrieval-qa-chat")
-            llm = ChatOpenAI()
-            retriever = ...
-            combine_docs_chain = create_stuff_documents_chain(
-                llm, retrieval_qa_chat_prompt
-            )
-            retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)
-            chain.invoke({"input": "..."})
-    """
-    if not isinstance(retriever, BaseRetriever):
-        retrieval_docs: Runnable[dict, RetrieverOutput] = retriever
-    else:
-        retrieval_docs = (lambda x: x["input"]) | retriever
-    retrieval_chain = (
-        RunnablePassthrough.assign(
-            context=retrieval_docs.with_config(run_name="retrieve_documents"),
-        ).assign(answer=combine_docs_chain)
-    ).with_config(run_name="retrieval_chain")
-    return retrieval_chain

--- a/libs/langchain/langchain/evaluation/criteria/__init__.py
+++ b/libs/langchain/langchain/evaluation/criteria/__init__.py
@@ -1,21 +1,21 @@
 """Criteria or rubric based evaluators.
 These evaluators are useful for evaluating the
 output of a language model or chain against
 specified criteria or rubric.
 Classes
 -------
 CriteriaEvalChain : Evaluates the output of a language model or
 chain against specified criteria.
 Examples
 --------
-Using a predefined criterion:
+Using a pre-defined criterion:
 >>> from langchain.llms import OpenAI
 >>> from langchain.evaluation.criteria import CriteriaEvalChain
 >>> llm = OpenAI()
 >>> criteria = "conciseness"
 >>> chain = CriteriaEvalChain.from_llm(llm=llm, criteria=criteria)
 >>> chain.evaluate_strings(
         prediction="The answer is 42.",
         reference="42",
         input="What is the answer to life, the universe, and everything?",
     )

--- a/libs/langchain/langchain/evaluation/parsing/base.py
+++ b/libs/langchain/langchain/evaluation/parsing/base.py
@@ -1,12 +1,11 @@
 """Evaluators for parsing strings."""
-import json
 from operator import eq
 from typing import Any, Callable, Optional, Union, cast
 from langchain.evaluation.schema import StringEvaluator
 from langchain.output_parsers.json import parse_json_markdown
 class JsonValidityEvaluator(StringEvaluator):
     """Evaluates whether the prediction is valid JSON.
     This evaluator checks if the prediction is a valid JSON string. It does not
         require any input or reference.
     Attributes:
         requires_input (bool): Whether this evaluator requires an input
@@ -47,21 +46,21 @@
             prediction (str): The prediction string to evaluate.
             input (str, optional): Not used in this evaluator. Defaults to None.
             reference (str, optional): Not used in this evaluator. Defaults to None.
         Returns:
             dict: A dictionary containing the evaluation score. The score is 1 if
             the prediction is valid JSON, and 0 otherwise.
                 If the prediction is not valid JSON, the dictionary also contains
                 a "reasoning" field with the error message.
         """
         try:
-            parse_json_markdown(prediction, parser=json.loads)
+            parse_json_markdown(prediction)
             return {"score": 1}
         except Exception as e:
             return {"score": 0, "reasoning": str(e)}
 class JsonEqualityEvaluator(StringEvaluator):
     """Evaluates whether the prediction is equal to the reference after
         parsing both as JSON.
     This evaluator checks if the prediction, after parsing as JSON, is equal
         to the reference,
     which is also parsed as JSON. It does not require an input string.
     Attributes:

--- a/libs/langchain/langchain/output_parsers/datetime.py
+++ b/libs/langchain/langchain/output_parsers/datetime.py
@@ -23,26 +23,22 @@
         dt = start_date + timedelta(seconds=random_delta)
         date_string = dt.strftime(pattern)
         examples.append(date_string)
     return examples
 class DatetimeOutputParser(BaseOutputParser[datetime]):
     """Parse the output of an LLM call to a datetime."""
     format: str = "%Y-%m-%dT%H:%M:%S.%fZ"
     """The string value that used as the datetime format."""
     def get_format_instructions(self) -> str:
         examples = comma_list(_generate_random_datetime_strings(self.format))
-        return (
-            f"Write a datetime string that matches the "
-            f"following pattern: '{self.format}'.\n\n"
-            f"Examples: {examples}\n\n"
-            f"Return ONLY this string, no other words!"
-        )
+        return f"""Write a datetime string that matches the 
+            following pattern: "{self.format}". Examples: {examples}"""
     def parse(self, response: str) -> datetime:
         try:
             return datetime.strptime(response.strip(), self.format)
         except ValueError as e:
             raise OutputParserException(
                 f"Could not parse datetime string: {response}"
             ) from e
     @property
     def _type(self) -> str:
         return "datetime"

--- a/libs/langchain/langchain/output_parsers/format_instructions.py
+++ b/libs/langchain/langchain/output_parsers/format_instructions.py
@@ -27,23 +27,35 @@
 - name: John Doe
   avg: 0.3
 - name: Jane Maxfield
   avg: 1.4
 ```
 Please follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: 
 ```
 {schema}
 ```
 Make sure to always enclose the YAML output in triple backticks (```)"""
+XML_FORMAT_INSTRUCTIONS = """The output should be formatted as a XML file.
+1. Output should conform to the tags below. 
+2. If tags are not given, make them on your own.
+3. Remember to always open and close all the tags.
+As an example, for the tags ["foo", "bar", "baz"]:
+1. String "<foo>\n   <bar>\n      <baz></baz>\n   </bar>\n</foo>" is a well-formatted instance of the schema. 
+2. String "<foo>\n   <bar>\n   </foo>" is a badly-formatted instance.
+3. String "<foo>\n   <tag>\n   </tag>\n</foo>" is a badly-formatted instance.
+Here are the output tags:
+```
+{tags}
+```"""
 PANDAS_DATAFRAME_FORMAT_INSTRUCTIONS = """The output should be formatted as a string as the operation, followed by a colon, followed by the column or row to be queried on, followed by optional array parameters.
 1. The column names are limited to the possible columns below.
-2. Arrays must either be a comma-separated list of numbers formatted as [1,3,5], or it must be in range of numbers formatted as [0..4].
+2. Arrays must either be a comma-seperated list of numbers formatted as [1,3,5], or it must be in range of numbers formatted as [0..4].
 3. Remember that arrays are optional and not necessarily required.
 4. If the column is not in the possible columns or the operation is not a valid Pandas DataFrame operation, return why it is invalid as a sentence starting with either "Invalid column" or "Invalid operation".
 As an example, for the formats:
 1. String "column:num_legs" is a well-formatted instance which gets the column num_legs, where num_legs is a possible column.
 2. String "row:1" is a well-formatted instance which gets row 1.
 3. String "column:num_legs[1,2]" is a well-formatted instance which gets the column num_legs for rows 1 and 2, where num_legs is a possible column.
 4. String "row:1[num_legs]" is a well-formatted instance which gets row 1, but for just column num_legs, where num_legs is a possible column.
 5. String "mean:num_legs[1..3]" is a well-formatted instance which takes the mean of num_legs from rows 1 to 3, where num_legs is a possible column and mean is a valid Pandas DataFrame operation.
 6. String "do_something:num_legs" is a badly-formatted instance, where do_something is not a valid Pandas DataFrame operation.
 7. String "mean:invalid_col" is a badly-formatted instance, where invalid_col is not a possible column.

--- a/libs/langchain/langchain/output_parsers/json.py
+++ b/libs/langchain/langchain/output_parsers/json.py
@@ -1,12 +1,137 @@
-from langchain_core.output_parsers.json import (
-    SimpleJsonOutputParser,
-    parse_and_check_json_markdown,
-    parse_json_markdown,
-    parse_partial_json,
-)
-__all__ = [
-    "SimpleJsonOutputParser",
-    "parse_partial_json",
-    "parse_json_markdown",
-    "parse_and_check_json_markdown",
-]
+from __future__ import annotations
+import json
+import re
+from json import JSONDecodeError
+from typing import Any, Callable, List, Optional
+import jsonpatch
+from langchain_core.exceptions import OutputParserException
+from langchain_core.output_parsers import BaseCumulativeTransformOutputParser
+def _replace_new_line(match: re.Match[str]) -> str:
+    value = match.group(2)
+    value = re.sub(r"\n", r"\\n", value)
+    value = re.sub(r"\r", r"\\r", value)
+    value = re.sub(r"\t", r"\\t", value)
+    value = re.sub(r'(?<!\\)"', r"\"", value)
+    return match.group(1) + value + match.group(3)
+def _custom_parser(multiline_string: str) -> str:
+    """
+    The LLM response for `action_input` may be a multiline
+    string containing unescaped newlines, tabs or quotes. This function
+    replaces those characters with their escaped counterparts.
+    (newlines in JSON must be double-escaped: `\\n`)
+    """
+    if isinstance(multiline_string, (bytes, bytearray)):
+        multiline_string = multiline_string.decode()
+    multiline_string = re.sub(
+        r'("action_input"\:\s*")(.*)(")',
+        _replace_new_line,
+        multiline_string,
+        flags=re.DOTALL,
+    )
+    return multiline_string
+def parse_partial_json(s: str, *, strict: bool = False) -> Any:
+    """Parse a JSON string that may be missing closing braces.
+    Args:
+        s: The JSON string to parse.
+        strict: Whether to use strict parsing. Defaults to False.
+    Returns:
+        The parsed JSON object as a Python dictionary.
+    """
+    try:
+        return json.loads(s, strict=strict)
+    except json.JSONDecodeError:
+        pass
+    new_s = ""
+    stack = []
+    is_inside_string = False
+    escaped = False
+    for char in s:
+        if is_inside_string:
+            if char == '"' and not escaped:
+                is_inside_string = False
+            elif char == "\n" and not escaped:
+                char = "\\n"  # Replace the newline character with the escape sequence.
+            elif char == "\\":
+                escaped = not escaped
+            else:
+                escaped = False
+        else:
+            if char == '"':
+                is_inside_string = True
+                escaped = False
+            elif char == "{":
+                stack.append("}")
+            elif char == "[":
+                stack.append("]")
+            elif char == "}" or char == "]":
+                if stack and stack[-1] == char:
+                    stack.pop()
+                else:
+                    return None
+        new_s += char
+    if is_inside_string:
+        new_s += '"'
+    for closing_char in reversed(stack):
+        new_s += closing_char
+    try:
+        return json.loads(new_s, strict=strict)
+    except json.JSONDecodeError:
+        return None
+def parse_json_markdown(
+    json_string: str, *, parser: Callable[[str], Any] = json.loads
+) -> dict:
+    """
+    Parse a JSON string from a Markdown string.
+    Args:
+        json_string: The Markdown string.
+    Returns:
+        The parsed JSON object as a Python dictionary.
+    """
+    match = re.search(r"```(json)?(.*)```", json_string, re.DOTALL)
+    if match is None:
+        json_str = json_string
+    else:
+        json_str = match.group(2)
+    json_str = json_str.strip()
+    json_str = _custom_parser(json_str)
+    parsed = parser(json_str)
+    return parsed
+def parse_and_check_json_markdown(text: str, expected_keys: List[str]) -> dict:
+    """
+    Parse a JSON string from a Markdown string and check that it
+    contains the expected keys.
+    Args:
+        text: The Markdown string.
+        expected_keys: The expected keys in the JSON string.
+    Returns:
+        The parsed JSON object as a Python dictionary.
+    """
+    try:
+        json_obj = parse_json_markdown(text)
+    except json.JSONDecodeError as e:
+        raise OutputParserException(f"Got invalid JSON object. Error: {e}")
+    for key in expected_keys:
+        if key not in json_obj:
+            raise OutputParserException(
+                f"Got invalid return object. Expected key `{key}` "
+                f"to be present, but got {json_obj}"
+            )
+    return json_obj
+class SimpleJsonOutputParser(BaseCumulativeTransformOutputParser[Any]):
+    """Parse the output of an LLM call to a JSON object.
+    When used in streaming mode, it will yield partial JSON objects containing
+    all the keys that have been returned so far.
+    In streaming, if `diff` is set to `True`, yields JSONPatch operations
+    describing the difference between the previous and the current object.
+    """
+    def _diff(self, prev: Optional[Any], next: Any) -> Any:
+        return jsonpatch.make_patch(prev, next).patch
+    def parse(self, text: str) -> Any:
+        text = text.strip()
+        try:
+            return parse_json_markdown(text.strip(), parser=parse_partial_json)
+        except JSONDecodeError as e:
+            raise OutputParserException(f"Invalid json output: {text}") from e
+    @property
+    def _type(self) -> str:
+        return "simple_json_output_parser"

--- a/libs/langchain/langchain/output_parsers/retry.py
+++ b/libs/langchain/langchain/output_parsers/retry.py
@@ -35,21 +35,21 @@
     max_retries: int = 1
     """The maximum number of times to retry the parse."""
     @classmethod
     def from_llm(
         cls,
         llm: BaseLanguageModel,
         parser: BaseOutputParser[T],
         prompt: BasePromptTemplate = NAIVE_RETRY_PROMPT,
         max_retries: int = 1,
     ) -> RetryOutputParser[T]:
-        """Create an RetryOutputParser from a language model and a parser.
+        """Create an OutputFixingParser from a language model and a parser.
         Args:
             llm: llm to use for fixing
             parser: parser to use for parsing
             prompt: prompt to use for fixing
             max_retries: Maximum number of retries to parse.
         Returns:
             RetryOutputParser
         """
         from langchain.chains.llm import LLMChain
         chain = LLMChain(llm=llm, prompt=prompt)

--- a/libs/langchain/langchain/output_parsers/xml.py
+++ b/libs/langchain/langchain/output_parsers/xml.py
@@ -1,2 +1,38 @@
-from langchain_core.output_parsers.xml import XMLOutputParser
-__all__ = ["XMLOutputParser"]
+import re
+import xml.etree.ElementTree as ET
+from typing import Any, Dict, List, Optional
+from langchain_core.output_parsers import BaseOutputParser
+from langchain.output_parsers.format_instructions import XML_FORMAT_INSTRUCTIONS
+class XMLOutputParser(BaseOutputParser):
+    """Parse an output using xml format."""
+    tags: Optional[List[str]] = None
+    encoding_matcher: re.Pattern = re.compile(
+        r"<([^>]*encoding[^>]*)>\n(.*)", re.MULTILINE | re.DOTALL
+    )
+    def get_format_instructions(self) -> str:
+        return XML_FORMAT_INSTRUCTIONS.format(tags=self.tags)
+    def parse(self, text: str) -> Dict[str, List[Any]]:
+        text = text.strip("`").strip("xml")
+        encoding_match = self.encoding_matcher.search(text)
+        if encoding_match:
+            text = encoding_match.group(2)
+        text = text.strip()
+        if (text.startswith("<") or text.startswith("\n<")) and (
+            text.endswith(">") or text.endswith(">\n")
+        ):
+            root = ET.fromstring(text)
+            return self._root_to_dict(root)
+        else:
+            raise ValueError(f"Could not parse output: {text}")
+    def _root_to_dict(self, root: ET.Element) -> Dict[str, List[Any]]:
+        """Converts xml tree to python dictionary."""
+        result: Dict[str, List[Any]] = {root.tag: []}
+        for child in root:
+            if len(child) == 0:
+                result[root.tag].append({child.tag: child.text})
+            else:
+                result[root.tag].append(self._root_to_dict(child))
+        return result
+    @property
+    def _type(self) -> str:
+        return "xml"

--- a/libs/langchain/langchain/storage/file_system.py
+++ b/libs/langchain/langchain/storage/file_system.py
@@ -1,11 +1,10 @@
-import os
 import re
 from pathlib import Path
 from typing import Iterator, List, Optional, Sequence, Tuple, Union
 from langchain_core.stores import ByteStore
 from langchain.storage.exceptions import InvalidKeyException
 class LocalFileStore(ByteStore):
     """BaseStore interface that works on the local file system.
     Examples:
         Create a LocalFileStore instance and perform operations on it:
         .. code-block:: python
@@ -16,38 +15,31 @@
             file_store.mdelete(["key1"])
             for key in file_store.yield_keys():
                 print(key)
     """
     def __init__(self, root_path: Union[str, Path]) -> None:
         """Implement the BaseStore interface for the local file system.
         Args:
             root_path (Union[str, Path]): The root path of the file store. All keys are
                 interpreted as paths relative to this root.
         """
-        self.root_path = Path(root_path).absolute()
+        self.root_path = Path(root_path)
     def _get_full_path(self, key: str) -> Path:
         """Get the full path for a given key relative to the root path.
         Args:
             key (str): The key relative to the root path.
         Returns:
             Path: The full path for the given key.
         """
         if not re.match(r"^[a-zA-Z0-9_.\-/]+$", key):
             raise InvalidKeyException(f"Invalid characters in key: {key}")
-        full_path = os.path.abspath(self.root_path / key)
-        common_path = os.path.commonpath([str(self.root_path), full_path])
-        if common_path != str(self.root_path):
-            raise InvalidKeyException(
-                f"Invalid key: {key}. Key should be relative to the full path."
-                f"{self.root_path} vs. {common_path} and full path of {full_path}"
-            )
-        return Path(full_path)
+        return self.root_path / key
     def mget(self, keys: Sequence[str]) -> List[Optional[bytes]]:
         """Get the values associated with the given keys.
         Args:
             keys: A sequence of keys.
         Returns:
             A sequence of optional values associated with the keys.
             If a key is not found, the corresponding value will be None.
         """
         values: List[Optional[bytes]] = []
         for key in keys:

--- a/templates/rag-chroma-multi-modal-multi-vector/ingest.py
+++ b/templates/rag-chroma-multi-modal-multi-vector/ingest.py
@@ -3,21 +3,21 @@
 import os
 import uuid
 from io import BytesIO
 from pathlib import Path
 import pypdfium2 as pdfium
 from langchain.chat_models import ChatOpenAI
 from langchain.embeddings import OpenAIEmbeddings
 from langchain.retrievers.multi_vector import MultiVectorRetriever
 from langchain.schema.document import Document
 from langchain.schema.messages import HumanMessage
-from langchain.storage import LocalFileStore, UpstashRedisByteStore
+from langchain.storage import UpstashRedisByteStore
 from langchain.vectorstores import Chroma
 from PIL import Image
 def image_summarize(img_base64, prompt):
     """
     Make image summary
     :param img_base64: Base64 encoded string for image
     :param prompt: Text prompt for summarizatiomn
     :return: Image summarization prompt
     """
     chat = ChatOpenAI(model="gpt-4-vision-preview", max_tokens=1024)
@@ -84,39 +84,31 @@
     """
     Convert PIL images to Base64 encoded strings
     :param pil_image: PIL image
     :return: Re-sized Base64 string
     """
     buffered = BytesIO()
     pil_image.save(buffered, format="JPEG")  # You can change the format if needed
     img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
     img_str = resize_base64_image(img_str, size=(960, 540))
     return img_str
-def create_multi_vector_retriever(
-    vectorstore, image_summaries, images, local_file_store
-):
+def create_multi_vector_retriever(vectorstore, image_summaries, images):
     """
     Create retriever that indexes summaries, but returns raw images or texts
     :param vectorstore: Vectorstore to store embedded image sumamries
     :param image_summaries: Image summaries
     :param images: Base64 encoded images
-    :param local_file_store: Use local file storage
     :return: Retriever
     """
-    if local_file_store:
-        store = LocalFileStore(
-            str(Path(__file__).parent / "multi_vector_retriever_metadata")
-        )
-    else:
-        UPSTASH_URL = os.getenv("UPSTASH_URL")
-        UPSTASH_TOKEN = os.getenv("UPSTASH_TOKEN")
-        store = UpstashRedisByteStore(url=UPSTASH_URL, token=UPSTASH_TOKEN)
+    UPSTASH_URL = os.getenv("UPSTASH_URL")
+    UPSTASH_TOKEN = os.getenv("UPSTASH_TOKEN")
+    store = UpstashRedisByteStore(url=UPSTASH_URL, token=UPSTASH_TOKEN)
     id_key = "doc_id"
     retriever = MultiVectorRetriever(
         vectorstore=vectorstore,
         byte_store=store,
         id_key=id_key,
     )
     def add_documents(retriever, doc_summaries, doc_contents):
         doc_ids = [str(uuid.uuid4()) for _ in doc_contents]
         summary_docs = [
             Document(page_content=s, metadata={id_key: doc_ids[i]})
@@ -138,12 +130,11 @@
     persist_directory=str(Path(__file__).parent / "chroma_db_multi_modal"),
     embedding_function=OpenAIEmbeddings(),
 )
 images_base_64_processed_documents = [
     Document(page_content=i) for i in images_base_64_processed
 ]
 retriever_multi_vector_img = create_multi_vector_retriever(
     vectorstore_mvr,
     image_summaries,
     images_base_64_processed_documents,
-    local_file_store=True,
 )

--- a/templates/rag-chroma-multi-modal-multi-vector/rag_chroma_multi_modal_multi_vector/chain.py
+++ b/templates/rag-chroma-multi-modal-multi-vector/rag_chroma_multi_modal_multi_vector/chain.py
@@ -3,21 +3,21 @@
 import os
 from pathlib import Path
 from langchain.chat_models import ChatOpenAI
 from langchain.embeddings import OpenAIEmbeddings
 from langchain.pydantic_v1 import BaseModel
 from langchain.retrievers.multi_vector import MultiVectorRetriever
 from langchain.schema.document import Document
 from langchain.schema.messages import HumanMessage
 from langchain.schema.output_parser import StrOutputParser
 from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
-from langchain.storage import LocalFileStore, UpstashRedisByteStore
+from langchain.storage import UpstashRedisByteStore
 from langchain.vectorstores import Chroma
 from PIL import Image
 def resize_base64_image(base64_string, size=(128, 128)):
     """
     Resize an image encoded as a Base64 string.
     :param base64_string: A Base64 encoded string of the image to be resized.
     :param size: A tuple representing the new size (width, height) for the image.
     :return: A Base64 encoded string of the resized image.
     """
     img_data = base64.b64decode(base64_string)
@@ -75,34 +75,28 @@
     chain = (
         {
             "context": retriever | RunnableLambda(get_resized_images),
             "question": RunnablePassthrough(),
         }
         | RunnableLambda(img_prompt_func)
         | model
         | StrOutputParser()
     )
     return chain
-local_file_store = True
 vectorstore_mvr = Chroma(
     collection_name="image_summaries",
     persist_directory=str(Path(__file__).parent.parent / "chroma_db_multi_modal"),
     embedding_function=OpenAIEmbeddings(),
 )
-if local_file_store:
-    store = LocalFileStore(
-        str(Path(__file__).parent.parent / "multi_vector_retriever_metadata")
-    )
-else:
-    UPSTASH_URL = os.getenv("UPSTASH_URL")
-    UPSTASH_TOKEN = os.getenv("UPSTASH_TOKEN")
-    store = UpstashRedisByteStore(url=UPSTASH_URL, token=UPSTASH_TOKEN)
+UPSTASH_URL = os.getenv("UPSTASH_URL")
+UPSTASH_TOKEN = os.getenv("UPSTASH_TOKEN")
+store = UpstashRedisByteStore(url=UPSTASH_URL, token=UPSTASH_TOKEN)
 id_key = "doc_id"
 retriever = MultiVectorRetriever(
     vectorstore=vectorstore_mvr,
     byte_store=store,
     id_key=id_key,
 )
 chain = multi_modal_rag_chain(retriever)
 class Question(BaseModel):
     __root__: str
 chain = chain.with_types(input_type=Question)

--- a/templates/rag-multi-modal-local/ingest.py
+++ b//dev/null
@@ -1,24 +0,0 @@
-import os
-from pathlib import Path
-from langchain.vectorstores import Chroma
-from langchain_experimental.open_clip import OpenCLIPEmbeddings
-img_dump_path = Path(__file__).parent / "docs/"
-rel_img_dump_path = img_dump_path.relative_to(Path.cwd())
-image_uris = sorted(
-    [
-        os.path.join(rel_img_dump_path, image_name)
-        for image_name in os.listdir(rel_img_dump_path)
-        if image_name.endswith(".jpg")
-    ]
-)
-vectorstore = Path(__file__).parent / "chroma_db_multi_modal"
-re_vectorstore_path = vectorstore.relative_to(Path.cwd())
-print("Loading embedding function")
-embedding = OpenCLIPEmbeddings(model_name="ViT-H-14", checkpoint="laion2b_s32b_b79k")
-vectorstore_mmembd = Chroma(
-    collection_name="multi-modal-rag",
-    persist_directory=str(Path(__file__).parent / "chroma_db_multi_modal"),
-    embedding_function=embedding,
-)
-print("Embedding images")
-vectorstore_mmembd.add_images(uris=image_uris)

--- a/templates/rag-multi-modal-local/rag_multi_modal_local/__init__.py
+++ b//dev/null
@@ -1,2 +0,0 @@
-from rag_multi_modal_local.chain import chain
-__all__ = ["chain"]

--- a/templates/rag-multi-modal-local/rag_multi_modal_local/chain.py
+++ b//dev/null
@@ -1,91 +0,0 @@
-import base64
-import io
-from pathlib import Path
-from langchain.chat_models import ChatOllama
-from langchain.vectorstores import Chroma
-from langchain_core.documents import Document
-from langchain_core.messages import HumanMessage
-from langchain_core.output_parsers import StrOutputParser
-from langchain_core.pydantic_v1 import BaseModel
-from langchain_core.runnables import RunnableLambda, RunnablePassthrough
-from langchain_experimental.open_clip import OpenCLIPEmbeddings
-from PIL import Image
-def resize_base64_image(base64_string, size=(128, 128)):
-    """
-    Resize an image encoded as a Base64 string.
-    :param base64_string: A Base64 encoded string of the image to be resized.
-    :param size: A tuple representing the new size (width, height) for the image.
-    :return: A Base64 encoded string of the resized image.
-    """
-    img_data = base64.b64decode(base64_string)
-    img = Image.open(io.BytesIO(img_data))
-    resized_img = img.resize(size, Image.LANCZOS)
-    buffered = io.BytesIO()
-    resized_img.save(buffered, format=img.format)
-    return base64.b64encode(buffered.getvalue()).decode("utf-8")
-def get_resized_images(docs):
-    """
-    Resize images from base64-encoded strings.
-    :param docs: A list of base64-encoded image to be resized.
-    :return: Dict containing a list of resized base64-encoded strings.
-    """
-    b64_images = []
-    for doc in docs:
-        if isinstance(doc, Document):
-            doc = doc.page_content
-        b64_images.append(doc)
-    return {"images": b64_images}
-def img_prompt_func(data_dict, num_images=1):
-    """
-    GPT-4V prompt for image analysis.
-    :param data_dict: A dict with images and a user-provided question.
-    :param num_images: Number of images to include in the prompt.
-    :return: A list containing message objects for each image and the text prompt.
-    """
-    messages = []
-    if data_dict["context"]["images"]:
-        for image in data_dict["context"]["images"][:num_images]:
-            image_message = {
-                "type": "image_url",
-                "image_url": f"data:image/jpeg;base64,{image}",
-            }
-            messages.append(image_message)
-    text_message = {
-        "type": "text",
-        "text": (
-            "You are a helpful assistant that gives a description of food pictures.\n"
-            "Give a detailed summary of the image.\n"
-            "Give reccomendations for similar foods to try.\n"
-        ),
-    }
-    messages.append(text_message)
-    return [HumanMessage(content=messages)]
-def multi_modal_rag_chain(retriever):
-    """
-    Multi-modal RAG chain,
-    :param retriever: A function that retrieves the necessary context for the model.
-    :return: A chain of functions representing the multi-modal RAG process.
-    """
-    model = ChatOllama(model="bakllava", temperature=0)
-    chain = (
-        {
-            "context": retriever | RunnableLambda(get_resized_images),
-            "question": RunnablePassthrough(),
-        }
-        | RunnableLambda(img_prompt_func)
-        | model
-        | StrOutputParser()
-    )
-    return chain
-vectorstore_mmembd = Chroma(
-    collection_name="multi-modal-rag",
-    persist_directory=str(Path(__file__).parent.parent / "chroma_db_multi_modal"),
-    embedding_function=OpenCLIPEmbeddings(
-        model_name="ViT-H-14", checkpoint="laion2b_s32b_b79k"
-    ),
-)
-retriever_mmembd = vectorstore_mmembd.as_retriever()
-chain = multi_modal_rag_chain(retriever_mmembd)
-class Question(BaseModel):
-    __root__: str
-chain = chain.with_types(input_type=Question)

--- a/templates/rag-multi-modal-mv-local/ingest.py
+++ b//dev/null
@@ -1,133 +0,0 @@
-import base64
-import io
-import os
-import uuid
-from io import BytesIO
-from pathlib import Path
-from langchain.chat_models import ChatOllama
-from langchain.embeddings import OllamaEmbeddings
-from langchain.retrievers.multi_vector import MultiVectorRetriever
-from langchain.schema.document import Document
-from langchain.schema.messages import HumanMessage
-from langchain.storage import LocalFileStore
-from langchain.vectorstores import Chroma
-from PIL import Image
-def image_summarize(img_base64, prompt):
-    """
-    Make image summary
-    :param img_base64: Base64 encoded string for image
-    :param prompt: Text prompt for summarizatiomn
-    :return: Image summarization prompt
-    """
-    chat = ChatOllama(model="bakllava", temperature=0)
-    msg = chat.invoke(
-        [
-            HumanMessage(
-                content=[
-                    {"type": "text", "text": prompt},
-                    {
-                        "type": "image_url",
-                        "image_url": f"data:image/jpeg;base64,{img_base64}",
-                    },
-                ]
-            )
-        ]
-    )
-    return msg.content
-def generate_img_summaries(img_base64_list):
-    """
-    Generate summaries for images
-    :param img_base64_list: Base64 encoded images
-    :return: List of image summaries and processed images
-    """
-    image_summaries = []
-    processed_images = []
-    prompt = """Give a detailed summary of the image."""
-    for i, base64_image in enumerate(img_base64_list):
-        try:
-            image_summaries.append(image_summarize(base64_image, prompt))
-            processed_images.append(base64_image)
-        except Exception as e:
-            print(f"Error with image {i+1}: {e}")
-    return image_summaries, processed_images
-def get_images(img_path):
-    """
-    Extract images.
-    :param img_path: A string representing the path to the images.
-    """
-    pil_images = [
-        Image.open(os.path.join(img_path, image_name))
-        for image_name in os.listdir(img_path)
-        if image_name.endswith(".jpg")
-    ]
-    return pil_images
-def resize_base64_image(base64_string, size=(128, 128)):
-    """
-    Resize an image encoded as a Base64 string
-    :param base64_string: Base64 string
-    :param size: Image size
-    :return: Re-sized Base64 string
-    """
-    img_data = base64.b64decode(base64_string)
-    img = Image.open(io.BytesIO(img_data))
-    resized_img = img.resize(size, Image.LANCZOS)
-    buffered = io.BytesIO()
-    resized_img.save(buffered, format=img.format)
-    return base64.b64encode(buffered.getvalue()).decode("utf-8")
-def convert_to_base64(pil_image):
-    """
-    Convert PIL images to Base64 encoded strings
-    :param pil_image: PIL image
-    :return: Re-sized Base64 string
-    """
-    buffered = BytesIO()
-    pil_image.save(buffered, format="JPEG")  # You can change the format if needed
-    img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
-    return img_str
-def create_multi_vector_retriever(vectorstore, image_summaries, images):
-    """
-    Create retriever that indexes summaries, but returns raw images or texts
-    :param vectorstore: Vectorstore to store embedded image sumamries
-    :param image_summaries: Image summaries
-    :param images: Base64 encoded images
-    :return: Retriever
-    """
-    store = LocalFileStore(
-        str(Path(__file__).parent / "multi_vector_retriever_metadata")
-    )
-    id_key = "doc_id"
-    retriever = MultiVectorRetriever(
-        vectorstore=vectorstore,
-        byte_store=store,
-        id_key=id_key,
-    )
-    def add_documents(retriever, doc_summaries, doc_contents):
-        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]
-        summary_docs = [
-            Document(page_content=s, metadata={id_key: doc_ids[i]})
-            for i, s in enumerate(doc_summaries)
-        ]
-        retriever.vectorstore.add_documents(summary_docs)
-        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))
-    add_documents(retriever, image_summaries, images)
-    return retriever
-doc_path = Path(__file__).parent / "docs/"
-rel_doc_path = doc_path.relative_to(Path.cwd())
-print("Read images")
-pil_images = get_images(rel_doc_path)
-images_base_64 = [convert_to_base64(i) for i in pil_images]
-print("Generate image summaries")
-image_summaries, images_base_64_processed = generate_img_summaries(images_base_64)
-vectorstore_mvr = Chroma(
-    collection_name="image_summaries",
-    persist_directory=str(Path(__file__).parent / "chroma_db_multi_modal"),
-    embedding_function=OllamaEmbeddings(model="llama2:7b"),
-)
-images_base_64_processed_documents = [
-    Document(page_content=i) for i in images_base_64_processed
-]
-retriever_multi_vector_img = create_multi_vector_retriever(
-    vectorstore_mvr,
-    image_summaries,
-    images_base_64_processed_documents,
-)

--- a/templates/rag-multi-modal-mv-local/rag_multi_modal_mv_local/__init__.py
+++ b//dev/null
@@ -1,2 +0,0 @@
-from rag_multi_modal_mv_local.chain import chain
-__all__ = ["chain"]

--- a/templates/rag-multi-modal-mv-local/rag_multi_modal_mv_local/chain.py
+++ b//dev/null
@@ -1,98 +0,0 @@
-import base64
-import io
-from pathlib import Path
-from langchain.chat_models import ChatOllama
-from langchain.embeddings import OllamaEmbeddings
-from langchain.pydantic_v1 import BaseModel
-from langchain.retrievers.multi_vector import MultiVectorRetriever
-from langchain.schema.document import Document
-from langchain.schema.messages import HumanMessage
-from langchain.schema.output_parser import StrOutputParser
-from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
-from langchain.storage import LocalFileStore
-from langchain.vectorstores import Chroma
-from PIL import Image
-def resize_base64_image(base64_string, size=(128, 128)):
-    """
-    Resize an image encoded as a Base64 string.
-    :param base64_string: A Base64 encoded string of the image to be resized.
-    :param size: A tuple representing the new size (width, height) for the image.
-    :return: A Base64 encoded string of the resized image.
-    """
-    img_data = base64.b64decode(base64_string)
-    img = Image.open(io.BytesIO(img_data))
-    resized_img = img.resize(size, Image.LANCZOS)
-    buffered = io.BytesIO()
-    resized_img.save(buffered, format=img.format)
-    return base64.b64encode(buffered.getvalue()).decode("utf-8")
-def get_resized_images(docs):
-    """
-    Resize images from base64-encoded strings.
-    :param docs: A list of base64-encoded image to be resized.
-    :return: Dict containing a list of resized base64-encoded strings.
-    """
-    b64_images = []
-    for doc in docs:
-        if isinstance(doc, Document):
-            doc = doc.page_content
-        b64_images.append(doc)
-    return {"images": b64_images}
-def img_prompt_func(data_dict, num_images=1):
-    """
-    Ollama prompt for image analysis.
-    :param data_dict: A dict with images and a user-provided question.
-    :param num_images: Number of images to include in the prompt.
-    :return: A list containing message objects for each image and the text prompt.
-    """
-    messages = []
-    if data_dict["context"]["images"]:
-        for image in data_dict["context"]["images"][:num_images]:
-            image_message = {
-                "type": "image_url",
-                "image_url": f"data:image/jpeg;base64,{image}",
-            }
-            messages.append(image_message)
-    text_message = {
-        "type": "text",
-        "text": (
-            "You are a helpful assistant that gives a description of food pictures.\n"
-            "Give a detailed summary of the image.\n"
-        ),
-    }
-    messages.append(text_message)
-    return [HumanMessage(content=messages)]
-def multi_modal_rag_chain(retriever):
-    """
-    Multi-modal RAG chain,
-    :param retriever: A function that retrieves the necessary context for the model.
-    :return: A chain of functions representing the multi-modal RAG process.
-    """
-    model = ChatOllama(model="bakllava", temperature=0)
-    chain = (
-        {
-            "context": retriever | RunnableLambda(get_resized_images),
-            "question": RunnablePassthrough(),
-        }
-        | RunnableLambda(img_prompt_func)
-        | model
-        | StrOutputParser()
-    )
-    return chain
-vectorstore_mvr = Chroma(
-    collection_name="image_summaries",
-    persist_directory=str(Path(__file__).parent.parent / "chroma_db_multi_modal"),
-    embedding_function=OllamaEmbeddings(model="llama2:7b"),
-)
-store = LocalFileStore(
-    str(Path(__file__).parent.parent / "multi_vector_retriever_metadata")
-)
-id_key = "doc_id"
-retriever = MultiVectorRetriever(
-    vectorstore=vectorstore_mvr,
-    byte_store=store,
-    id_key=id_key,
-)
-chain = multi_modal_rag_chain(retriever)
-class Question(BaseModel):
-    __root__: str
-chain = chain.with_types(input_type=Question)

--- a/templates/rag-vectara-multiquery/rag_vectara_multiquery/chain.py
+++ b/templates/rag-vectara-multiquery/rag_vectara_multiquery/chain.py
@@ -1,25 +1,31 @@
 import os
 from langchain.chat_models import ChatOpenAI
+from langchain.prompts import ChatPromptTemplate
 from langchain.retrievers.multi_query import MultiQueryRetriever
 from langchain.vectorstores import Vectara
 from langchain_core.output_parsers import StrOutputParser
 from langchain_core.pydantic_v1 import BaseModel
 from langchain_core.runnables import RunnableParallel, RunnablePassthrough
 if os.environ.get("VECTARA_CUSTOMER_ID", None) is None:
     raise Exception("Missing `VECTARA_CUSTOMER_ID` environment variable.")
 if os.environ.get("VECTARA_CORPUS_ID", None) is None:
     raise Exception("Missing `VECTARA_CORPUS_ID` environment variable.")
 if os.environ.get("VECTARA_API_KEY", None) is None:
     raise Exception("Missing `VECTARA_API_KEY` environment variable.")
-vectara_retriever = Vectara().as_retriever()
 llm = ChatOpenAI(temperature=0)
-retriever = MultiQueryRetriever.from_llm(retriever=vectara_retriever, llm=llm)
+retriever = MultiQueryRetriever.from_llm(retriever=Vectara().as_retriever(), llm=llm)
+template = """Answer the question based only on the following context:
+{context}
+Question: {question}
+"""
+prompt = ChatPromptTemplate.from_template(template)
 model = ChatOpenAI()
 chain = (
     RunnableParallel({"context": retriever, "question": RunnablePassthrough()})
-    | (lambda res: res[-1])
+    | prompt
+    | model
     | StrOutputParser()
 )
 class Question(BaseModel):
     __root__: str
 chain = chain.with_types(input_type=Question)

--- a/templates/rag-vectara/rag_vectara/chain.py
+++ b/templates/rag-vectara/rag_vectara/chain.py
@@ -1,20 +1,29 @@
 import os
+from langchain.chat_models import ChatOpenAI
+from langchain.prompts import ChatPromptTemplate
 from langchain.vectorstores import Vectara
 from langchain_core.output_parsers import StrOutputParser
 from langchain_core.pydantic_v1 import BaseModel
 from langchain_core.runnables import RunnableParallel, RunnablePassthrough
 if os.environ.get("VECTARA_CUSTOMER_ID", None) is None:
     raise Exception("Missing `VECTARA_CUSTOMER_ID` environment variable.")
 if os.environ.get("VECTARA_CORPUS_ID", None) is None:
     raise Exception("Missing `VECTARA_CORPUS_ID` environment variable.")
 if os.environ.get("VECTARA_API_KEY", None) is None:
     raise Exception("Missing `VECTARA_API_KEY` environment variable.")
 retriever = Vectara().as_retriever()
+template = """Answer the question based only on the following context:
+{context}
+Question: {question}
+"""
+prompt = ChatPromptTemplate.from_template(template)
+model = ChatOpenAI()
 chain = (
     RunnableParallel({"context": retriever, "question": RunnablePassthrough()})
-    | (lambda res: res[-1])
+    | prompt
+    | model
     | StrOutputParser()
 )
 class Question(BaseModel):
     __root__: str
 chain = chain.with_types(input_type=Question)
