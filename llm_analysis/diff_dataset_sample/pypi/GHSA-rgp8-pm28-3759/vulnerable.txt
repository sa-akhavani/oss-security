# ====================================================================
# FILE: docs/sidebars.js
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 93-136 ---
    93|       label: "Components",
    94|       collapsible: false,
    95|       items: [
    96|         { type: "category", label: "LLMs", collapsed: true, items: [{type:"autogenerated", dirName: "integrations/llms" }], link: { type: 'doc', id: "integrations/llms/index"}},
    97|         { type: "category", label: "Chat models", collapsed: true, items: [{type:"autogenerated", dirName: "integrations/chat" }], link: { type: 'doc', id: "integrations/chat/index"}},
    98|         { type: "category", label: "Document loaders", collapsed: true, items: [{type:"autogenerated", dirName: "integrations/document_loaders" }], link: {type: "generated-index", slug: "integrations/document_loaders" }},
    99|         { type: "category", label: "Document transformers", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/document_transformers" }], link: {type: "generated-index", slug: "integrations/document_transformers" }},
   100|         { type: "category", label: "Text embedding models", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/text_embedding" }], link: {type: "generated-index", slug: "integrations/text_embedding" }},
   101|         { type: "category", label: "Vector stores", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/vectorstores" }], link: {type: "generated-index", slug: "integrations/vectorstores" }},
   102|         { type: "category", label: "Retrievers", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/retrievers" }], link: {type: "generated-index", slug: "integrations/retrievers" }},
   103|         { type: "category", label: "Tools", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/tools" }], link: {type: "generated-index", slug: "integrations/tools" }},
   104|         { type: "category", label: "Agents and toolkits", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/toolkits" }], link: {type: "generated-index", slug: "integrations/toolkits" }},
   105|         { type: "category", label: "Memory", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/memory" }], link: {type: "generated-index", slug: "integrations/memory" }},
   106|         { type: "category", label: "Callbacks", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/callbacks" }], link: {type: "generated-index", slug: "integrations/callbacks" }},
   107|         { type: "category", label: "Chat loaders", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/chat_loaders" }], link: {type: "generated-index", slug: "integrations/chat_loaders" }},
   108|         { type: "category", label: "Adapters", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/adapters" }], link: {type: "generated-index", slug: "integrations/adapters" }},
   109|         { type: "category", label: "Stores", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/stores" }], link: {type: "doc", id: "integrations/stores/index" }},
   110|       ],
   111|       link: {
   112|         type: 'generated-index',
   113|       slug: "integrations/components",
   114|       },
   115|     },
   116|   ],
   117|   use_cases: [
   118|     {type: "autogenerated", dirName: "use_cases" }
   119|   ],
   120|   guides: [
   121|     {type: "autogenerated", dirName: "guides" }
   122|   ],
   123|   templates: [
   124|     {
   125|       type: "category",
   126|       label: "Templates",
   127|       items: [
   128|         { type: "autogenerated", dirName: "templates" },
   129|       ],
   130|       link: { type: 'doc', id: "templates/index" }
   131|     },
   132|   ],
   133|   contributing: [
   134|     {type: "autogenerated", dirName: "contributing" }
   135|   ],
   136| };


# ====================================================================
# FILE: libs/community/langchain_community/callbacks/openai_info.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-21 ---
     1| """Callback Handler that prints to std out."""
     2| from typing import Any, Dict, List
     3| from langchain_core.callbacks import BaseCallbackHandler
     4| from langchain_core.outputs import LLMResult
     5| MODEL_COST_PER_1K_TOKENS = {
     6|     "gpt-4": 0.03,
     7|     "gpt-4-0314": 0.03,
     8|     "gpt-4-0613": 0.03,
     9|     "gpt-4-32k": 0.06,
    10|     "gpt-4-32k-0314": 0.06,
    11|     "gpt-4-32k-0613": 0.06,
    12|     "gpt-4-vision-preview": 0.01,
    13|     "gpt-4-1106-preview": 0.01,
    14|     "gpt-4-completion": 0.06,
    15|     "gpt-4-0314-completion": 0.06,
    16|     "gpt-4-0613-completion": 0.06,
    17|     "gpt-4-32k-completion": 0.12,
    18|     "gpt-4-32k-0314-completion": 0.12,
    19|     "gpt-4-32k-0613-completion": 0.12,
    20|     "gpt-4-vision-preview-completion": 0.03,
    21|     "gpt-4-1106-preview-completion": 0.03,

# --- HUNK 2: Lines 110-175 ---
   110|         num_tokens: Number of tokens.
   111|         is_completion: Whether the model is used for completion or not.
   112|             Defaults to False.
   113|     Returns:
   114|         Cost in USD.
   115|     """
   116|     model_name = standardize_model_name(model_name, is_completion=is_completion)
   117|     if model_name not in MODEL_COST_PER_1K_TOKENS:
   118|         raise ValueError(
   119|             f"Unknown model: {model_name}. Please provide a valid OpenAI model name."
   120|             "Known models are: " + ", ".join(MODEL_COST_PER_1K_TOKENS.keys())
   121|         )
   122|     return MODEL_COST_PER_1K_TOKENS[model_name] * (num_tokens / 1000)
   123| class OpenAICallbackHandler(BaseCallbackHandler):
   124|     """Callback Handler that tracks OpenAI info."""
   125|     total_tokens: int = 0
   126|     prompt_tokens: int = 0
   127|     completion_tokens: int = 0
   128|     successful_requests: int = 0
   129|     total_cost: float = 0.0
   130|     def __repr__(self) -> str:
   131|         return (
   132|             f"Tokens Used: {self.total_tokens}\n"
   133|             f"\tPrompt Tokens: {self.prompt_tokens}\n"
   134|             f"\tCompletion Tokens: {self.completion_tokens}\n"
   135|             f"Successful Requests: {self.successful_requests}\n"
   136|             f"Total Cost (USD): ${self.total_cost}"
   137|         )
   138|     @property
   139|     def always_verbose(self) -> bool:
   140|         """Whether to call verbose callbacks even if verbose is False."""
   141|         return True
   142|     def on_llm_start(
   143|         self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
   144|     ) -> None:
   145|         """Print out the prompts."""
   146|         pass
   147|     def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
   148|         """Print out the token."""
   149|         pass
   150|     def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
   151|         """Collect token usage."""
   152|         if response.llm_output is None:
   153|             return None
   154|         self.successful_requests += 1
   155|         if "token_usage" not in response.llm_output:
   156|             return None
   157|         token_usage = response.llm_output["token_usage"]
   158|         completion_tokens = token_usage.get("completion_tokens", 0)
   159|         prompt_tokens = token_usage.get("prompt_tokens", 0)
   160|         model_name = standardize_model_name(response.llm_output.get("model_name", ""))
   161|         if model_name in MODEL_COST_PER_1K_TOKENS:
   162|             completion_cost = get_openai_token_cost_for_model(
   163|                 model_name, completion_tokens, is_completion=True
   164|             )
   165|             prompt_cost = get_openai_token_cost_for_model(model_name, prompt_tokens)
   166|             self.total_cost += prompt_cost + completion_cost
   167|         self.total_tokens += token_usage.get("total_tokens", 0)
   168|         self.prompt_tokens += prompt_tokens
   169|         self.completion_tokens += completion_tokens
   170|     def __copy__(self) -> "OpenAICallbackHandler":
   171|         """Return a copy of the callback handler."""
   172|         return self
   173|     def __deepcopy__(self, memo: Any) -> "OpenAICallbackHandler":
   174|         """Return a deep copy of the callback handler."""
   175|         return self


# ====================================================================
# FILE: libs/community/langchain_community/chat_message_histories/elasticsearch.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 3-60 ---
     3| from time import time
     4| from typing import TYPE_CHECKING, Any, Dict, List, Optional
     5| from langchain_core.chat_history import BaseChatMessageHistory
     6| from langchain_core.messages import (
     7|     BaseMessage,
     8|     message_to_dict,
     9|     messages_from_dict,
    10| )
    11| if TYPE_CHECKING:
    12|     from elasticsearch import Elasticsearch
    13| logger = logging.getLogger(__name__)
    14| class ElasticsearchChatMessageHistory(BaseChatMessageHistory):
    15|     """Chat message history that stores history in Elasticsearch.
    16|     Args:
    17|         es_url: URL of the Elasticsearch instance to connect to.
    18|         es_cloud_id: Cloud ID of the Elasticsearch instance to connect to.
    19|         es_user: Username to use when connecting to Elasticsearch.
    20|         es_password: Password to use when connecting to Elasticsearch.
    21|         es_api_key: API key to use when connecting to Elasticsearch.
    22|         es_connection: Optional pre-existing Elasticsearch connection.
    23|         index: Name of the index to use.
    24|         session_id: Arbitrary key that is used to store the messages
    25|             of a single chat session.
    26|     """
    27|     def __init__(
    28|         self,
    29|         index: str,
    30|         session_id: str,
    31|         *,
    32|         es_connection: Optional["Elasticsearch"] = None,
    33|         es_url: Optional[str] = None,
    34|         es_cloud_id: Optional[str] = None,
    35|         es_user: Optional[str] = None,
    36|         es_api_key: Optional[str] = None,
    37|         es_password: Optional[str] = None,
    38|     ):
    39|         self.index: str = index
    40|         self.session_id: str = session_id
    41|         if es_connection is not None:
    42|             self.client = es_connection.options(
    43|                 headers={"user-agent": self.get_user_agent()}
    44|             )
    45|         elif es_url is not None or es_cloud_id is not None:
    46|             self.client = ElasticsearchChatMessageHistory.connect_to_elasticsearch(
    47|                 es_url=es_url,
    48|                 username=es_user,
    49|                 password=es_password,
    50|                 cloud_id=es_cloud_id,
    51|                 api_key=es_api_key,
    52|             )
    53|         else:
    54|             raise ValueError(
    55|                 """Either provide a pre-existing Elasticsearch connection, \
    56|                 or valid credentials for creating a new connection."""
    57|             )
    58|         if self.client.indices.exists(index=index):
    59|             logger.debug(
    60|                 f"Chat history index {index} already exists, skipping creation."

# --- HUNK 2: Lines 129-167 ---
   129|         except ApiError as err:
   130|             logger.error(f"Could not retrieve messages from Elasticsearch: {err}")
   131|             raise err
   132|         if result and len(result["hits"]["hits"]) > 0:
   133|             items = [
   134|                 json.loads(document["_source"]["history"])
   135|                 for document in result["hits"]["hits"]
   136|             ]
   137|         else:
   138|             items = []
   139|         return messages_from_dict(items)
   140|     def add_message(self, message: BaseMessage) -> None:
   141|         """Add a message to the chat session in Elasticsearch"""
   142|         try:
   143|             from elasticsearch import ApiError
   144|             self.client.index(
   145|                 index=self.index,
   146|                 document={
   147|                     "session_id": self.session_id,
   148|                     "created_at": round(time() * 1000),
   149|                     "history": json.dumps(message_to_dict(message)),
   150|                 },
   151|                 refresh=True,
   152|             )
   153|         except ApiError as err:
   154|             logger.error(f"Could not add message to Elasticsearch: {err}")
   155|             raise err
   156|     def clear(self) -> None:
   157|         """Clear session memory in Elasticsearch"""
   158|         try:
   159|             from elasticsearch import ApiError
   160|             self.client.delete_by_query(
   161|                 index=self.index,
   162|                 query={"term": {"session_id": self.session_id}},
   163|                 refresh=True,
   164|             )
   165|         except ApiError as err:
   166|             logger.error(f"Could not clear session memory in Elasticsearch: {err}")
   167|             raise err


# ====================================================================
# FILE: libs/community/langchain_community/chat_models/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 7-75 ---
     7|     BaseLanguageModel --> BaseChatModel --> <name>  # Examples: ChatOpenAI, ChatGooglePalm
     8| **Main helpers:**
     9| .. code-block::
    10|     AIMessage, BaseMessage, HumanMessage
    11| """  # noqa: E501
    12| from langchain_community.chat_models.anthropic import ChatAnthropic
    13| from langchain_community.chat_models.anyscale import ChatAnyscale
    14| from langchain_community.chat_models.azure_openai import AzureChatOpenAI
    15| from langchain_community.chat_models.baichuan import ChatBaichuan
    16| from langchain_community.chat_models.baidu_qianfan_endpoint import QianfanChatEndpoint
    17| from langchain_community.chat_models.bedrock import BedrockChat
    18| from langchain_community.chat_models.cohere import ChatCohere
    19| from langchain_community.chat_models.databricks import ChatDatabricks
    20| from langchain_community.chat_models.ernie import ErnieBotChat
    21| from langchain_community.chat_models.everlyai import ChatEverlyAI
    22| from langchain_community.chat_models.fake import FakeListChatModel
    23| from langchain_community.chat_models.fireworks import ChatFireworks
    24| from langchain_community.chat_models.gigachat import GigaChat
    25| from langchain_community.chat_models.google_palm import ChatGooglePalm
    26| from langchain_community.chat_models.gpt_router import GPTRouter
    27| from langchain_community.chat_models.human import HumanInputChatModel
    28| from langchain_community.chat_models.hunyuan import ChatHunyuan
    29| from langchain_community.chat_models.javelin_ai_gateway import ChatJavelinAIGateway
    30| from langchain_community.chat_models.jinachat import JinaChat
    31| from langchain_community.chat_models.konko import ChatKonko
    32| from langchain_community.chat_models.litellm import ChatLiteLLM
    33| from langchain_community.chat_models.minimax import MiniMaxChat
    34| from langchain_community.chat_models.mlflow import ChatMlflow
    35| from langchain_community.chat_models.mlflow_ai_gateway import ChatMLflowAIGateway
    36| from langchain_community.chat_models.ollama import ChatOllama
    37| from langchain_community.chat_models.openai import ChatOpenAI
    38| from langchain_community.chat_models.pai_eas_endpoint import PaiEasChatEndpoint
    39| from langchain_community.chat_models.promptlayer_openai import PromptLayerChatOpenAI
    40| from langchain_community.chat_models.vertexai import ChatVertexAI
    41| from langchain_community.chat_models.volcengine_maas import VolcEngineMaasChat
    42| from langchain_community.chat_models.yandex import ChatYandexGPT
    43| __all__ = [
    44|     "ChatOpenAI",
    45|     "BedrockChat",
    46|     "AzureChatOpenAI",
    47|     "FakeListChatModel",
    48|     "PromptLayerChatOpenAI",
    49|     "ChatDatabricks",
    50|     "ChatEverlyAI",
    51|     "ChatAnthropic",
    52|     "ChatCohere",
    53|     "ChatGooglePalm",
    54|     "ChatMlflow",
    55|     "ChatMLflowAIGateway",
    56|     "ChatOllama",
    57|     "ChatVertexAI",
    58|     "JinaChat",
    59|     "HumanInputChatModel",
    60|     "MiniMaxChat",
    61|     "ChatAnyscale",
    62|     "ChatLiteLLM",
    63|     "ErnieBotChat",
    64|     "ChatJavelinAIGateway",
    65|     "ChatKonko",
    66|     "PaiEasChatEndpoint",
    67|     "QianfanChatEndpoint",
    68|     "ChatFireworks",
    69|     "ChatYandexGPT",
    70|     "ChatBaichuan",
    71|     "ChatHunyuan",
    72|     "GigaChat",
    73|     "VolcEngineMaasChat",
    74|     "GPTRouter",
    75| ]


# ====================================================================
# FILE: libs/community/langchain_community/chat_models/azure_openai.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-41 ---
     1| """Azure OpenAI chat wrapper."""
     2| from __future__ import annotations
     3| import logging
     4| import os
     5| import warnings
     6| from typing import Any, Callable, Dict, List, Union
     7| from langchain_core.outputs import ChatResult
     8| from langchain_core.pydantic_v1 import BaseModel, Field, root_validator
     9| from langchain_core.utils import get_from_dict_or_env
    10| from langchain_community.chat_models.openai import ChatOpenAI
    11| from langchain_community.utils.openai import is_openai_v1
    12| logger = logging.getLogger(__name__)
    13| class AzureChatOpenAI(ChatOpenAI):
    14|     """`Azure OpenAI` Chat Completion API.
    15|     To use this class you
    16|     must have a deployed model on Azure OpenAI. Use `deployment_name` in the
    17|     constructor to refer to the "Model deployment name" in the Azure portal.
    18|     In addition, you should have the ``openai`` python package installed, and the
    19|     following environment variables set or passed in constructor in lower case:
    20|     - ``AZURE_OPENAI_API_KEY``
    21|     - ``AZURE_OPENAI_API_ENDPOINT``
    22|     - ``AZURE_OPENAI_AD_TOKEN``
    23|     - ``OPENAI_API_VERSION``
    24|     - ``OPENAI_PROXY``
    25|     For example, if you have `gpt-35-turbo` deployed, with the deployment name
    26|     `35-turbo-dev`, the constructor should look like:
    27|     .. code-block:: python
    28|         AzureChatOpenAI(
    29|             azure_deployment="35-turbo-dev",
    30|             openai_api_version="2023-05-15",
    31|         )
    32|     Be aware the API version may change.
    33|     You can also specify the version of the model using ``model_version`` constructor
    34|     parameter, as Azure OpenAI doesn't return model version with the response.
    35|     Default is empty. When you specify the version, it will be appended to the
    36|     model name in the response. Setting correct version will help you to calculate the
    37|     cost properly. Model version is not validated, so make sure you set it correctly
    38|     to get the correct cost.
    39|     Any parameters that are valid to be passed to the openai.create call can be passed
    40|     in, even if not explicitly saved on this class.
    41|     """


# ====================================================================
# FILE: libs/community/langchain_community/chat_models/gpt_router.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 173-238 ---
   173|     def _identifying_params(self) -> Dict[str, Any]:
   174|         """Get the identifying parameters."""
   175|         return {
   176|             **{"models_priority_list": self.models_priority_list},
   177|             **self._default_params,
   178|         }
   179|     @property
   180|     def _default_params(self) -> Dict[str, Any]:
   181|         """Get the default parameters for calling GPTRouter API."""
   182|         return {
   183|             "max_tokens": self.max_tokens,
   184|             "stream": self.streaming,
   185|             "n": self.n,
   186|             "temperature": self.temperature,
   187|             **self.model_kwargs,
   188|         }
   189|     def _generate(
   190|         self,
   191|         messages: List[BaseMessage],
   192|         stop: Optional[List[str]] = None,
   193|         run_manager: CallbackManagerForLLMRun | None = None,
   194|         stream: bool | None = None,
   195|         **kwargs: Any,
   196|     ) -> ChatResult:
   197|         should_stream = stream if stream is not None else self.streaming
   198|         if should_stream:
   199|             stream_iter = self._stream(
   200|                 messages, stop=stop, run_manager=run_manager, **kwargs
   201|             )
   202|             return generate_from_stream(stream_iter)
   203|         message_dicts, params = self._create_message_dicts(messages, stop)
   204|         params = {**params, **kwargs, "stream": False}
   205|         response = completion_with_retry(
   206|             self,
   207|             messages=message_dicts,
   208|             models_priority_list=self.models_priority_list,
   209|             run_manager=run_manager,
   210|             **params,
   211|         )
   212|         return self._create_chat_result(response)
   213|     async def _agenerate(
   214|         self,
   215|         messages: List[BaseMessage],
   216|         stop: Optional[List[str]] = None,
   217|         run_manager: AsyncCallbackManagerForLLMRun | None = None,
   218|         stream: bool | None = None,
   219|         **kwargs: Any,
   220|     ) -> ChatResult:
   221|         should_stream = stream if stream is not None else self.streaming
   222|         if should_stream:
   223|             stream_iter = self._astream(
   224|                 messages, stop=stop, run_manager=run_manager, **kwargs
   225|             )
   226|             return await agenerate_from_stream(stream_iter)
   227|         message_dicts, params = self._create_message_dicts(messages, stop)
   228|         params = {**params, **kwargs, "stream": False}
   229|         response = await acompletion_with_retry(
   230|             self,
   231|             messages=message_dicts,
   232|             models_priority_list=self.models_priority_list,
   233|             run_manager=run_manager,
   234|             **params,
   235|         )
   236|         return self._create_chat_result(response)
   237|     def _create_chat_generation_chunk(
   238|         self, data: Mapping[str, Any], default_chunk_class


# ====================================================================
# FILE: libs/community/langchain_community/chat_models/ollama.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-24 ---
     1| import json
     2| from typing import Any, Dict, Iterator, List, Optional, Union
     3| from langchain_core._api import deprecated
     4| from langchain_core.callbacks import (
     5|     CallbackManagerForLLMRun,
     6| )
     7| from langchain_core.language_models.chat_models import BaseChatModel
     8| from langchain_core.messages import (
     9|     AIMessage,
    10|     AIMessageChunk,
    11|     BaseMessage,
    12|     ChatMessage,
    13|     HumanMessage,
    14|     SystemMessage,
    15| )
    16| from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
    17| from langchain_community.llms.ollama import OllamaEndpointNotFoundError, _OllamaCommon
    18| @deprecated("0.0.3", alternative="_chat_stream_response_to_chat_generation_chunk")
    19| def _stream_response_to_chat_generation_chunk(
    20|     stream_response: str,
    21| ) -> ChatGenerationChunk:
    22|     """Convert a stream response to a generation chunk."""
    23|     parsed_response = json.loads(stream_response)
    24|     generation_info = parsed_response if parsed_response.get("done") is True else None

# --- HUNK 2: Lines 116-227 ---
   116|             ollama_messages.append(
   117|                 {
   118|                     "role": role,
   119|                     "content": content,
   120|                     "images": images,
   121|                 }
   122|             )
   123|         return ollama_messages
   124|     def _create_chat_stream(
   125|         self,
   126|         messages: List[BaseMessage],
   127|         stop: Optional[List[str]] = None,
   128|         **kwargs: Any,
   129|     ) -> Iterator[str]:
   130|         payload = {
   131|             "messages": self._convert_messages_to_ollama_messages(messages),
   132|         }
   133|         yield from self._create_stream(
   134|             payload=payload, stop=stop, api_url=f"{self.base_url}/api/chat/", **kwargs
   135|         )
   136|     def _chat_stream_with_aggregation(
   137|         self,
   138|         messages: List[BaseMessage],
   139|         stop: Optional[List[str]] = None,
   140|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   141|         verbose: bool = False,
   142|         **kwargs: Any,
   143|     ) -> ChatGenerationChunk:
   144|         final_chunk: Optional[ChatGenerationChunk] = None
   145|         for stream_resp in self._create_chat_stream(messages, stop, **kwargs):
   146|             if stream_resp:
   147|                 chunk = _chat_stream_response_to_chat_generation_chunk(stream_resp)
   148|                 if final_chunk is None:
   149|                     final_chunk = chunk
   150|                 else:
   151|                     final_chunk += chunk
   152|                 if run_manager:
   153|                     run_manager.on_llm_new_token(
   154|                         chunk.text,
   155|                         verbose=verbose,
   156|                     )
   157|         if final_chunk is None:
   158|             raise ValueError("No data received from Ollama stream.")
   159|         return final_chunk
   160|     def _generate(
   161|         self,
   162|         messages: List[BaseMessage],
   163|         stop: Optional[List[str]] = None,
   164|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   165|         **kwargs: Any,
   166|     ) -> ChatResult:
   167|         """Call out to Ollama's generate endpoint.
   168|         Args:
   169|             messages: The list of base messages to pass into the model.
   170|             stop: Optional list of stop words to use when generating.
   171|         Returns:
   172|             Chat generations from the model
   173|         Example:
   174|             .. code-block:: python
   175|                 response = ollama([
   176|                     HumanMessage(content="Tell me about the history of AI")
   177|                 ])
   178|         """
   179|         final_chunk = self._chat_stream_with_aggregation(
   180|             messages,
   181|             stop=stop,
   182|             run_manager=run_manager,
   183|             verbose=self.verbose,
   184|             **kwargs,
   185|         )
   186|         chat_generation = ChatGeneration(
   187|             message=AIMessage(content=final_chunk.text),
   188|             generation_info=final_chunk.generation_info,
   189|         )
   190|         return ChatResult(generations=[chat_generation])
   191|     def _stream(
   192|         self,
   193|         messages: List[BaseMessage],
   194|         stop: Optional[List[str]] = None,
   195|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   196|         **kwargs: Any,
   197|     ) -> Iterator[ChatGenerationChunk]:
   198|         try:
   199|             for stream_resp in self._create_chat_stream(messages, stop, **kwargs):
   200|                 if stream_resp:
   201|                     chunk = _stream_response_to_chat_generation_chunk(stream_resp)
   202|                     yield chunk
   203|                     if run_manager:
   204|                         run_manager.on_llm_new_token(
   205|                             chunk.text,
   206|                             verbose=self.verbose,
   207|                         )
   208|         except OllamaEndpointNotFoundError:
   209|             yield from self._legacy_stream(messages, stop, **kwargs)
   210|     @deprecated("0.0.3", alternative="_stream")
   211|     def _legacy_stream(
   212|         self,
   213|         messages: List[BaseMessage],
   214|         stop: Optional[List[str]] = None,
   215|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   216|         **kwargs: Any,
   217|     ) -> Iterator[ChatGenerationChunk]:
   218|         prompt = self._format_messages_as_text(messages)
   219|         for stream_resp in self._create_generate_stream(prompt, stop, **kwargs):
   220|             if stream_resp:
   221|                 chunk = _stream_response_to_chat_generation_chunk(stream_resp)
   222|                 yield chunk
   223|                 if run_manager:
   224|                     run_manager.on_llm_new_token(
   225|                         chunk.text,
   226|                         verbose=self.verbose,
   227|                     )


# ====================================================================
# FILE: libs/community/langchain_community/chat_models/vertexai.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-27 ---
     1| """Wrapper around Google VertexAI chat-based models."""
     2| from __future__ import annotations
     3| import base64
     4| import logging
     5| import re
     6| from dataclasses import dataclass, field
     7| from typing import TYPE_CHECKING, Any, Dict, Iterator, List, Optional, Union, cast
     8| from langchain_core.callbacks import (
     9|     AsyncCallbackManagerForLLMRun,
    10|     CallbackManagerForLLMRun,
    11| )
    12| from langchain_core.language_models.chat_models import (
    13|     BaseChatModel,
    14|     generate_from_stream,
    15| )
    16| from langchain_core.messages import (
    17|     AIMessage,
    18|     AIMessageChunk,
    19|     BaseMessage,
    20|     HumanMessage,
    21|     SystemMessage,
    22| )
    23| from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
    24| from langchain_core.pydantic_v1 import root_validator
    25| from langchain_community.llms.vertexai import (
    26|     _VertexAICommon,
    27|     is_codey_model,

# --- HUNK 2: Lines 56-123 ---
    56|         first place.
    57|     """
    58|     from vertexai.language_models import ChatMessage
    59|     vertex_messages, context = [], None
    60|     for i, message in enumerate(history):
    61|         content = cast(str, message.content)
    62|         if i == 0 and isinstance(message, SystemMessage):
    63|             context = content
    64|         elif isinstance(message, AIMessage):
    65|             vertex_message = ChatMessage(content=message.content, author="bot")
    66|             vertex_messages.append(vertex_message)
    67|         elif isinstance(message, HumanMessage):
    68|             vertex_message = ChatMessage(content=message.content, author="user")
    69|             vertex_messages.append(vertex_message)
    70|         else:
    71|             raise ValueError(
    72|                 f"Unexpected message with type {type(message)} at the position {i}."
    73|             )
    74|     chat_history = _ChatHistory(context=context, history=vertex_messages)
    75|     return chat_history
    76| def _parse_chat_history_gemini(
    77|     history: List[BaseMessage], project: Optional[str]
    78| ) -> List["Content"]:
    79|     from vertexai.preview.generative_models import Content, Image, Part
    80|     def _convert_to_prompt(part: Union[str, Dict]) -> Part:
    81|         if isinstance(part, str):
    82|             return Part.from_text(part)
    83|         if not isinstance(part, Dict):
    84|             raise ValueError(
    85|                 f"Message's content is expected to be a dict, got {type(part)}!"
    86|             )
    87|         if part["type"] == "text":
    88|             return Part.from_text(part["text"])
    89|         elif part["type"] == "image_url":
    90|             path = part["image_url"]["url"]
    91|             if path.startswith("gs://"):
    92|                 image = load_image_from_gcs(path=path, project=project)
    93|             elif path.startswith("data:image/"):
    94|                 try:
    95|                     encoded = re.search(r"data:image/\w{2,4};base64,(.*)", path).group(
    96|                         1
    97|                     )
    98|                 except AttributeError:
    99|                     raise ValueError(
   100|                         "Invalid image uri. It should be in the format "
   101|                         "data:image/<image_type>;base64,<base64_encoded_image>."
   102|                     )
   103|                 image = Image.from_bytes(base64.b64decode(encoded))
   104|             else:
   105|                 image = Image.load_from_file(path)
   106|         else:
   107|             raise ValueError("Only text and image_url types are supported!")
   108|         return Part.from_image(image)
   109|     vertex_messages = []
   110|     for i, message in enumerate(history):
   111|         if i == 0 and isinstance(message, SystemMessage):
   112|             raise ValueError("SystemMessages are not yet supported!")
   113|         elif isinstance(message, AIMessage):
   114|             role = "model"
   115|         elif isinstance(message, HumanMessage):
   116|             role = "user"
   117|         else:
   118|             raise ValueError(
   119|                 f"Unexpected message with type {type(message)} at the position {i}."
   120|             )
   121|         raw_content = message.content
   122|         if isinstance(raw_content, str):
   123|             raw_content = [raw_content]


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 52-91 ---
    52| from langchain_community.document_loaders.browserless import BrowserlessLoader
    53| from langchain_community.document_loaders.chatgpt import ChatGPTLoader
    54| from langchain_community.document_loaders.chromium import AsyncChromiumLoader
    55| from langchain_community.document_loaders.college_confidential import (
    56|     CollegeConfidentialLoader,
    57| )
    58| from langchain_community.document_loaders.concurrent import ConcurrentLoader
    59| from langchain_community.document_loaders.confluence import ConfluenceLoader
    60| from langchain_community.document_loaders.conllu import CoNLLULoader
    61| from langchain_community.document_loaders.couchbase import CouchbaseLoader
    62| from langchain_community.document_loaders.csv_loader import (
    63|     CSVLoader,
    64|     UnstructuredCSVLoader,
    65| )
    66| from langchain_community.document_loaders.cube_semantic import CubeSemanticLoader
    67| from langchain_community.document_loaders.datadog_logs import DatadogLogsLoader
    68| from langchain_community.document_loaders.dataframe import DataFrameLoader
    69| from langchain_community.document_loaders.diffbot import DiffbotLoader
    70| from langchain_community.document_loaders.directory import DirectoryLoader
    71| from langchain_community.document_loaders.discord import DiscordChatLoader
    72| from langchain_community.document_loaders.docugami import DocugamiLoader
    73| from langchain_community.document_loaders.docusaurus import DocusaurusLoader
    74| from langchain_community.document_loaders.dropbox import DropboxLoader
    75| from langchain_community.document_loaders.duckdb_loader import DuckDBLoader
    76| from langchain_community.document_loaders.email import (
    77|     OutlookMessageLoader,
    78|     UnstructuredEmailLoader,
    79| )
    80| from langchain_community.document_loaders.epub import UnstructuredEPubLoader
    81| from langchain_community.document_loaders.etherscan import EtherscanLoader
    82| from langchain_community.document_loaders.evernote import EverNoteLoader
    83| from langchain_community.document_loaders.excel import UnstructuredExcelLoader
    84| from langchain_community.document_loaders.facebook_chat import FacebookChatLoader
    85| from langchain_community.document_loaders.fauna import FaunaLoader
    86| from langchain_community.document_loaders.figma import FigmaFileLoader
    87| from langchain_community.document_loaders.gcs_directory import GCSDirectoryLoader
    88| from langchain_community.document_loaders.gcs_file import GCSFileLoader
    89| from langchain_community.document_loaders.geodataframe import GeoDataFrameLoader
    90| from langchain_community.document_loaders.git import GitLoader
    91| from langchain_community.document_loaders.gitbook import GitbookLoader

# --- HUNK 2: Lines 217-256 ---
   217|     "AsyncChromiumLoader",
   218|     "AZLyricsLoader",
   219|     "AcreomLoader",
   220|     "AirbyteCDKLoader",
   221|     "AirbyteGongLoader",
   222|     "AirbyteJSONLoader",
   223|     "AirbyteHubspotLoader",
   224|     "AirbyteSalesforceLoader",
   225|     "AirbyteShopifyLoader",
   226|     "AirbyteStripeLoader",
   227|     "AirbyteTypeformLoader",
   228|     "AirbyteZendeskSupportLoader",
   229|     "AirtableLoader",
   230|     "AmazonTextractPDFLoader",
   231|     "ApifyDatasetLoader",
   232|     "ArcGISLoader",
   233|     "ArxivLoader",
   234|     "AssemblyAIAudioTranscriptLoader",
   235|     "AsyncHtmlLoader",
   236|     "AzureAIDataLoader",
   237|     "AzureBlobStorageContainerLoader",
   238|     "AzureBlobStorageFileLoader",
   239|     "BSHTMLLoader",
   240|     "BibtexLoader",
   241|     "BigQueryLoader",
   242|     "BiliBiliLoader",
   243|     "BlackboardLoader",
   244|     "Blob",
   245|     "BlobLoader",
   246|     "BlockchainDocumentLoader",
   247|     "BraveSearchLoader",
   248|     "BrowserlessLoader",
   249|     "CSVLoader",
   250|     "ChatGPTLoader",
   251|     "CoNLLULoader",
   252|     "CollegeConfidentialLoader",
   253|     "ConcurrentLoader",
   254|     "ConfluenceLoader",
   255|     "CouchbaseLoader",
   256|     "CubeSemanticLoader",


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/arxiv.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-19 ---
     1| from typing import Any, List, Optional
     2| from langchain_core.documents import Document
     3| from langchain_community.document_loaders.base import BaseLoader
     4| from langchain_community.utilities.arxiv import ArxivAPIWrapper
     5| class ArxivLoader(BaseLoader):
     6|     """Load a query result from `Arxiv`.
     7|     The loader converts the original PDF format into the text.
     8|     Args:
     9|         Supports all arguments of `ArxivAPIWrapper`.
    10|     """
    11|     def __init__(
    12|         self, query: str, doc_content_chars_max: Optional[int] = None, **kwargs: Any
    13|     ):
    14|         self.query = query
    15|         self.client = ArxivAPIWrapper(
    16|             doc_content_chars_max=doc_content_chars_max, **kwargs
    17|         )
    18|     def load(self) -> List[Document]:
    19|         return self.client.load(self.query)


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/directory.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 36-77 ---
    36|         sample_size: int = 0,
    37|         randomize_sample: bool = False,
    38|         sample_seed: Union[int, None] = None,
    39|     ):
    40|         """Initialize with a path to directory and how to glob over it.
    41|         Args:
    42|             path: Path to directory.
    43|             glob: Glob pattern to use to find files. Defaults to "**/[!.]*"
    44|                (all files except hidden).
    45|             silent_errors: Whether to silently ignore errors. Defaults to False.
    46|             load_hidden: Whether to load hidden files. Defaults to False.
    47|             loader_cls: Loader class to use for loading files.
    48|               Defaults to UnstructuredFileLoader.
    49|             loader_kwargs: Keyword arguments to pass to loader_cls. Defaults to None.
    50|             recursive: Whether to recursively search for files. Defaults to False.
    51|             show_progress: Whether to show a progress bar. Defaults to False.
    52|             use_multithreading: Whether to use multithreading. Defaults to False.
    53|             max_concurrency: The maximum number of threads to use. Defaults to 4.
    54|             sample_size: The maximum number of files you would like to load from the
    55|                 directory.
    56|             randomize_sample: Suffle the files to get a random sample.
    57|             sample_seed: set the seed of the random shuffle for reporoducibility.
    58|         """
    59|         if loader_kwargs is None:
    60|             loader_kwargs = {}
    61|         self.path = path
    62|         self.glob = glob
    63|         self.load_hidden = load_hidden
    64|         self.loader_cls = loader_cls
    65|         self.loader_kwargs = loader_kwargs
    66|         self.silent_errors = silent_errors
    67|         self.recursive = recursive
    68|         self.show_progress = show_progress
    69|         self.use_multithreading = use_multithreading
    70|         self.max_concurrency = max_concurrency
    71|         self.sample_size = sample_size
    72|         self.randomize_sample = randomize_sample
    73|         self.sample_seed = sample_seed
    74|     def load_file(
    75|         self, item: Path, path: Path, docs: List[Document], pbar: Optional[Any]
    76|     ) -> None:
    77|         """Load a file.


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/html_bs.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-35 ---
     1| import logging
     2| from typing import Dict, List, Union
     3| from langchain_core.documents import Document
     4| from langchain_community.document_loaders.base import BaseLoader
     5| logger = logging.getLogger(__name__)
     6| class BSHTMLLoader(BaseLoader):
     7|     """Load `HTML` files and parse them with `beautiful soup`."""
     8|     def __init__(
     9|         self,
    10|         file_path: str,
    11|         open_encoding: Union[str, None] = None,
    12|         bs_kwargs: Union[dict, None] = None,
    13|         get_text_separator: str = "",
    14|     ) -> None:
    15|         """Initialise with path, and optionally, file encoding to use, and any kwargs
    16|         to pass to the BeautifulSoup object.
    17|         Args:
    18|             file_path: The path to the file to load.
    19|             open_encoding: The encoding to use when opening the file.
    20|             bs_kwargs: Any kwargs to pass to the BeautifulSoup object.
    21|             get_text_separator: The separator to use when calling get_text on the soup.
    22|         """
    23|         try:
    24|             import bs4  # noqa:F401
    25|         except ImportError:
    26|             raise ImportError(
    27|                 "beautifulsoup4 package not found, please install it with "
    28|                 "`pip install beautifulsoup4`"
    29|             )
    30|         self.file_path = file_path
    31|         self.open_encoding = open_encoding
    32|         if bs_kwargs is None:
    33|             bs_kwargs = {"features": "lxml"}
    34|         self.bs_kwargs = bs_kwargs
    35|         self.get_text_separator = get_text_separator


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/markdown.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-32 ---
     1| from typing import List
     2| from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
     3| class UnstructuredMarkdownLoader(UnstructuredFileLoader):
     4|     """Load `Markdown` files using `Unstructured`.
     5|     You can run the loader in one of two modes: "single" and "elements".
     6|     If you use "single" mode, the document will be returned as a single
     7|     langchain Document object. If you use "elements" mode, the unstructured
     8|     library will split the document into elements such as Title and NarrativeText.
     9|     You can pass in additional unstructured kwargs after mode to apply
    10|     different unstructured settings.
    11|     Examples
    12|     --------
    13|     from langchain_community.document_loaders import UnstructuredMarkdownLoader
    14|     loader = UnstructuredMarkdownLoader(
    15|         "example.md", mode="elements", strategy="fast",
    16|     )
    17|     docs = loader.load()
    18|     References
    19|     ----------
    20|     https://unstructured-io.github.io/unstructured/bricks.html#partition-md
    21|     """
    22|     def _get_elements(self) -> List:
    23|         from unstructured.__version__ import __version__ as __unstructured_version__
    24|         from unstructured.partition.md import partition_md
    25|         _unstructured_version = __unstructured_version__.split("-")[0]
    26|         unstructured_version = tuple([int(x) for x in _unstructured_version.split(".")])
    27|         if unstructured_version < (0, 4, 16):
    28|             raise ValueError(
    29|                 f"You are on unstructured version {__unstructured_version__}. "
    30|                 "Partitioning markdown files is only supported in unstructured>=0.4.16."
    31|             )
    32|         return partition_md(filename=self.file_path, **self.unstructured_kwargs)


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/mhtml.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-36 ---
     1| import email
     2| import logging
     3| from typing import Dict, List, Union
     4| from langchain_core.documents import Document
     5| from langchain_community.document_loaders.base import BaseLoader
     6| logger = logging.getLogger(__name__)
     7| class MHTMLLoader(BaseLoader):
     8|     """Parse `MHTML` files with `BeautifulSoup`."""
     9|     def __init__(
    10|         self,
    11|         file_path: str,
    12|         open_encoding: Union[str, None] = None,
    13|         bs_kwargs: Union[dict, None] = None,
    14|         get_text_separator: str = "",
    15|     ) -> None:
    16|         """Initialise with path, and optionally, file encoding to use, and any kwargs
    17|         to pass to the BeautifulSoup object.
    18|         Args:
    19|             file_path: Path to file to load.
    20|             open_encoding: The encoding to use when opening the file.
    21|             bs_kwargs: Any kwargs to pass to the BeautifulSoup object.
    22|             get_text_separator: The separator to use when getting the text
    23|                 from the soup.
    24|         """
    25|         try:
    26|             import bs4  # noqa:F401
    27|         except ImportError:
    28|             raise ImportError(
    29|                 "beautifulsoup4 package not found, please install it with "
    30|                 "`pip install beautifulsoup4`"
    31|             )
    32|         self.file_path = file_path
    33|         self.open_encoding = open_encoding
    34|         if bs_kwargs is None:
    35|             bs_kwargs = {"features": "lxml"}
    36|         self.bs_kwargs = bs_kwargs


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/parsers/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-24 ---
     1| from langchain_community.document_loaders.parsers.audio import OpenAIWhisperParser
     2| from langchain_community.document_loaders.parsers.docai import DocAIParser
     3| from langchain_community.document_loaders.parsers.grobid import GrobidParser
     4| from langchain_community.document_loaders.parsers.html import BS4HTMLParser
     5| from langchain_community.document_loaders.parsers.language import LanguageParser
     6| from langchain_community.document_loaders.parsers.pdf import (
     7|     PDFMinerParser,
     8|     PDFPlumberParser,
     9|     PyMuPDFParser,
    10|     PyPDFium2Parser,
    11|     PyPDFParser,
    12| )
    13| __all__ = [
    14|     "BS4HTMLParser",
    15|     "DocAIParser",
    16|     "GrobidParser",
    17|     "LanguageParser",
    18|     "OpenAIWhisperParser",
    19|     "PDFMinerParser",
    20|     "PDFPlumberParser",
    21|     "PyMuPDFParser",
    22|     "PyPDFium2Parser",
    23|     "PyPDFParser",
    24| ]


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/parsers/pdf.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 435-476 ---
   435|             textract_response_json = self.tc.call_textract(
   436|                 input_document=blob.as_bytes(),
   437|                 features=self.textract_features,
   438|                 call_mode=self.tc.Textract_Call_Mode.FORCE_SYNC,
   439|                 boto3_textract_client=self.boto3_textract_client,
   440|             )
   441|         document = self.textractor.Document.open(textract_response_json)
   442|         linearizer_config = self.textractor.TextLinearizationConfig(
   443|             hide_figure_layout=True,
   444|             title_prefix="# ",
   445|             section_header_prefix="## ",
   446|             list_element_prefix="*",
   447|         )
   448|         for idx, page in enumerate(document.pages):
   449|             yield Document(
   450|                 page_content=page.get_text(config=linearizer_config),
   451|                 metadata={"source": blob.source, "page": idx + 1},
   452|             )
   453| class DocumentIntelligenceParser(BaseBlobParser):
   454|     """Loads a PDF with Azure Document Intelligence
   455|     (formerly Forms Recognizer) and chunks at character level."""
   456|     def __init__(self, client: Any, model: str):
   457|         self.client = client
   458|         self.model = model
   459|     def _generate_docs(self, blob: Blob, result: Any) -> Iterator[Document]:
   460|         for p in result.pages:
   461|             content = " ".join([line.content for line in p.lines])
   462|             d = Document(
   463|                 page_content=content,
   464|                 metadata={
   465|                     "source": blob.source,
   466|                     "page": p.page_number,
   467|                 },
   468|             )
   469|             yield d
   470|     def lazy_parse(self, blob: Blob) -> Iterator[Document]:
   471|         """Lazily parse the blob."""
   472|         with blob.as_bytes_io() as file_obj:
   473|             poller = self.client.begin_analyze_document(self.model, file_obj)
   474|             result = poller.result()
   475|             docs = self._generate_docs(blob, result)
   476|             yield from docs


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/rspace.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 36-76 ---
    36|     def validate_environment(cls, values: Dict) -> Dict:
    37|         """Validate that API key and URL exist in environment."""
    38|         values["api_key"] = get_from_dict_or_env(values, "api_key", "RSPACE_API_KEY")
    39|         values["url"] = get_from_dict_or_env(values, "url", "RSPACE_URL")
    40|         if "global_id" not in values or values["global_id"] is None:
    41|             raise ValueError(
    42|                 "No value supplied for global_id. Please supply an RSpace global ID"
    43|             )
    44|         return values
    45|     def _create_rspace_client(self) -> Any:
    46|         """Create a RSpace client."""
    47|         try:
    48|             from rspace_client.eln import eln, field_content
    49|         except ImportError:
    50|             raise ImportError("You must run " "`pip install rspace_client`")
    51|         try:
    52|             eln = eln.ELNClient(self.url, self.api_key)
    53|             eln.get_status()
    54|         except Exception:
    55|             raise Exception(
    56|                 f"Unable to initialise client - is url {self.url} or "
    57|                 f"api key  correct?"
    58|             )
    59|         return eln, field_content.FieldContent
    60|     def _get_doc(self, cli: Any, field_content: Any, d_id: Union[str, int]) -> Document:
    61|         content = ""
    62|         doc = cli.get_document(d_id)
    63|         content += f"<h2>{doc['name']}<h2/>"
    64|         for f in doc["fields"]:
    65|             content += f"{f['name']}\n"
    66|             fc = field_content(f["content"])
    67|             content += fc.get_text()
    68|             content += "\n"
    69|         return Document(
    70|             metadata={"source": f"rspace: {doc['name']}-{doc['globalId']}"},
    71|             page_content=content,
    72|         )
    73|     def _load_structured_doc(self) -> Iterator[Document]:
    74|         cli, field_content = self._create_rspace_client()
    75|         yield self._get_doc(cli, field_content, self.global_id)
    76|     def _load_folder_tree(self) -> Iterator[Document]:


# ====================================================================
# FILE: libs/community/langchain_community/document_transformers/doctran_text_extract.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 35-80 ---
    35|                     "type": "string",
    36|                     "required": True,
    37|                 },
    38|             ]
    39|             property_extractor = DoctranPropertyExtractor(properties)
    40|             transformed_document = await qa_transformer.atransform_documents(documents)
    41|     """  # noqa: E501
    42|     def __init__(
    43|         self,
    44|         properties: List[dict],
    45|         openai_api_key: Optional[str] = None,
    46|         openai_api_model: Optional[str] = None,
    47|     ) -> None:
    48|         self.properties = properties
    49|         self.openai_api_key = openai_api_key or get_from_env(
    50|             "openai_api_key", "OPENAI_API_KEY"
    51|         )
    52|         self.openai_api_model = openai_api_model or get_from_env(
    53|             "openai_api_model", "OPENAI_API_MODEL"
    54|         )
    55|     def transform_documents(
    56|         self, documents: Sequence[Document], **kwargs: Any
    57|     ) -> Sequence[Document]:
    58|         raise NotImplementedError
    59|     async def atransform_documents(
    60|         self, documents: Sequence[Document], **kwargs: Any
    61|     ) -> Sequence[Document]:
    62|         """Extracts properties from text documents using doctran."""
    63|         try:
    64|             from doctran import Doctran, ExtractProperty
    65|             doctran = Doctran(
    66|                 openai_api_key=self.openai_api_key, openai_model=self.openai_api_model
    67|             )
    68|         except ImportError:
    69|             raise ImportError(
    70|                 "Install doctran to use this parser. (pip install doctran)"
    71|             )
    72|         properties = [ExtractProperty(**property) for property in self.properties]
    73|         for d in documents:
    74|             doctran_doc = (
    75|                 await doctran.parse(content=d.page_content)
    76|                 .extract(properties=properties)
    77|                 .execute()
    78|             )
    79|             d.metadata["extracted_properties"] = doctran_doc.extracted_properties
    80|         return documents


# ====================================================================
# FILE: libs/community/langchain_community/document_transformers/doctran_text_qa.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 6-51 ---
     6|     Arguments:
     7|         openai_api_key: OpenAI API key. Can also be specified via environment variable
     8|             ``OPENAI_API_KEY``.
     9|     Example:
    10|         .. code-block:: python
    11|             from langchain_community.document_transformers import DoctranQATransformer
    12|             qa_transformer = DoctranQATransformer()
    13|             transformed_document = await qa_transformer.atransform_documents(documents)
    14|     """
    15|     def __init__(
    16|         self,
    17|         openai_api_key: Optional[str] = None,
    18|         openai_api_model: Optional[str] = None,
    19|     ) -> None:
    20|         self.openai_api_key = openai_api_key or get_from_env(
    21|             "openai_api_key", "OPENAI_API_KEY"
    22|         )
    23|         self.openai_api_model = openai_api_model or get_from_env(
    24|             "openai_api_model", "OPENAI_API_MODEL"
    25|         )
    26|     def transform_documents(
    27|         self, documents: Sequence[Document], **kwargs: Any
    28|     ) -> Sequence[Document]:
    29|         raise NotImplementedError
    30|     async def atransform_documents(
    31|         self, documents: Sequence[Document], **kwargs: Any
    32|     ) -> Sequence[Document]:
    33|         """Extracts QA from text documents using doctran."""
    34|         try:
    35|             from doctran import Doctran
    36|             doctran = Doctran(
    37|                 openai_api_key=self.openai_api_key, openai_model=self.openai_api_model
    38|             )
    39|         except ImportError:
    40|             raise ImportError(
    41|                 "Install doctran to use this parser. (pip install doctran)"
    42|             )
    43|         for d in documents:
    44|             doctran_doc = (
    45|                 await doctran.parse(content=d.page_content).interrogate().execute()
    46|             )
    47|             questions_and_answers = doctran_doc.extracted_properties.get(
    48|                 "questions_and_answers"
    49|             )
    50|             d.metadata["questions_and_answers"] = questions_and_answers
    51|         return documents


# ====================================================================
# FILE: libs/community/langchain_community/document_transformers/doctran_text_translate.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 9-55 ---
     9|         language: The language to translate *to*.
    10|     Example:
    11|         .. code-block:: python
    12|         from langchain_community.document_transformers import DoctranTextTranslator
    13|         qa_translator = DoctranTextTranslator(language="spanish")
    14|         translated_document = await qa_translator.atransform_documents(documents)
    15|     """
    16|     def __init__(
    17|         self,
    18|         openai_api_key: Optional[str] = None,
    19|         language: str = "english",
    20|         openai_api_model: Optional[str] = None,
    21|     ) -> None:
    22|         self.openai_api_key = openai_api_key or get_from_env(
    23|             "openai_api_key", "OPENAI_API_KEY"
    24|         )
    25|         self.openai_api_model = openai_api_model or get_from_env(
    26|             "openai_api_model", "OPENAI_API_MODEL"
    27|         )
    28|         self.language = language
    29|     def transform_documents(
    30|         self, documents: Sequence[Document], **kwargs: Any
    31|     ) -> Sequence[Document]:
    32|         raise NotImplementedError
    33|     async def atransform_documents(
    34|         self, documents: Sequence[Document], **kwargs: Any
    35|     ) -> Sequence[Document]:
    36|         """Translates text documents using doctran."""
    37|         try:
    38|             from doctran import Doctran
    39|             doctran = Doctran(
    40|                 openai_api_key=self.openai_api_key, openai_model=self.openai_api_model
    41|             )
    42|         except ImportError:
    43|             raise ImportError(
    44|                 "Install doctran to use this parser. (pip install doctran)"
    45|             )
    46|         doctran_docs = [
    47|             doctran.parse(content=doc.page_content, metadata=doc.metadata)
    48|             for doc in documents
    49|         ]
    50|         for i, doc in enumerate(doctran_docs):
    51|             doctran_docs[i] = await doc.translate(language=self.language).execute()
    52|         return [
    53|             Document(page_content=doc.transformed_content, metadata=doc.metadata)
    54|             for doc in doctran_docs
    55|         ]


# ====================================================================
# FILE: libs/community/langchain_community/embeddings/jina.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-47 ---
     1| from typing import Any, Dict, List, Optional
     2| import requests
     3| from langchain_core.embeddings import Embeddings
     4| from langchain_core.pydantic_v1 import BaseModel, root_validator
     5| from langchain_core.utils import get_from_dict_or_env
     6| JINA_API_URL: str = "https://api.jina.ai/v1/embeddings"
     7| class JinaEmbeddings(BaseModel, Embeddings):
     8|     """Jina embedding models."""
     9|     session: Any  #: :meta private:
    10|     model_name: str = "jina-embeddings-v2-base-en"
    11|     jina_api_key: Optional[str] = None
    12|     @root_validator()
    13|     def validate_environment(cls, values: Dict) -> Dict:
    14|         """Validate that auth token exists in environment."""
    15|         try:
    16|             jina_api_key = get_from_dict_or_env(values, "jina_api_key", "JINA_API_KEY")
    17|         except ValueError as original_exc:
    18|             try:
    19|                 jina_api_key = get_from_dict_or_env(
    20|                     values, "jina_auth_token", "JINA_AUTH_TOKEN"
    21|                 )
    22|             except ValueError:
    23|                 raise original_exc
    24|         session = requests.Session()
    25|         session.headers.update(
    26|             {
    27|                 "Authorization": f"Bearer {jina_api_key}",
    28|                 "Accept-Encoding": "identity",
    29|                 "Content-type": "application/json",
    30|             }
    31|         )
    32|         values["session"] = session
    33|         return values
    34|     def _embed(self, texts: List[str]) -> List[List[float]]:
    35|         resp = self.session.post(  # type: ignore
    36|             JINA_API_URL, json={"input": texts, "model": self.model_name}
    37|         ).json()
    38|         if "data" not in resp:
    39|             raise RuntimeError(resp["detail"])
    40|         embeddings = resp["data"]
    41|         sorted_embeddings = sorted(embeddings, key=lambda e: e["index"])  # type: ignore
    42|         return [result["embedding"] for result in sorted_embeddings]
    43|     def embed_documents(self, texts: List[str]) -> List[List[float]]:
    44|         """Call out to Jina's embedding endpoint.
    45|         Args:
    46|             texts: The list of texts to embed.
    47|         Returns:


# ====================================================================
# FILE: libs/community/langchain_community/embeddings/minimax.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-27 ---
     1| from __future__ import annotations
     2| import logging
     3| from typing import Any, Callable, Dict, List, Optional
     4| import requests
     5| from langchain_core.embeddings import Embeddings
     6| from langchain_core.pydantic_v1 import BaseModel, Extra, root_validator
     7| from langchain_core.utils import get_from_dict_or_env
     8| from tenacity import (
     9|     before_sleep_log,
    10|     retry,
    11|     stop_after_attempt,
    12|     wait_exponential,
    13| )
    14| logger = logging.getLogger(__name__)
    15| def _create_retry_decorator() -> Callable[[Any], Any]:
    16|     """Returns a tenacity retry decorator."""
    17|     multiplier = 1
    18|     min_seconds = 1
    19|     max_seconds = 4
    20|     max_retries = 6
    21|     return retry(
    22|         reraise=True,
    23|         stop=stop_after_attempt(max_retries),
    24|         wait=wait_exponential(multiplier=multiplier, min=min_seconds, max=max_seconds),
    25|         before_sleep=before_sleep_log(logger, logging.WARNING),
    26|     )
    27| def embed_with_retry(embeddings: MiniMaxEmbeddings, *args: Any, **kwargs: Any) -> Any:

# --- HUNK 2: Lines 38-106 ---
    38|     the constructor.
    39|     Example:
    40|         .. code-block:: python
    41|             from langchain_community.embeddings import MiniMaxEmbeddings
    42|             embeddings = MiniMaxEmbeddings()
    43|             query_text = "This is a test query."
    44|             query_result = embeddings.embed_query(query_text)
    45|             document_text = "This is a test document."
    46|             document_result = embeddings.embed_documents([document_text])
    47|     """
    48|     endpoint_url: str = "https://api.minimax.chat/v1/embeddings"
    49|     """Endpoint URL to use."""
    50|     model: str = "embo-01"
    51|     """Embeddings model name to use."""
    52|     embed_type_db: str = "db"
    53|     """For embed_documents"""
    54|     embed_type_query: str = "query"
    55|     """For embed_query"""
    56|     minimax_group_id: Optional[str] = None
    57|     """Group ID for MiniMax API."""
    58|     minimax_api_key: Optional[str] = None
    59|     """API Key for MiniMax API."""
    60|     class Config:
    61|         """Configuration for this pydantic object."""
    62|         extra = Extra.forbid
    63|     @root_validator()
    64|     def validate_environment(cls, values: Dict) -> Dict:
    65|         """Validate that group id and api key exists in environment."""
    66|         minimax_group_id = get_from_dict_or_env(
    67|             values, "minimax_group_id", "MINIMAX_GROUP_ID"
    68|         )
    69|         minimax_api_key = get_from_dict_or_env(
    70|             values, "minimax_api_key", "MINIMAX_API_KEY"
    71|         )
    72|         values["minimax_group_id"] = minimax_group_id
    73|         values["minimax_api_key"] = minimax_api_key
    74|         return values
    75|     def embed(
    76|         self,
    77|         texts: List[str],
    78|         embed_type: str,
    79|     ) -> List[List[float]]:
    80|         payload = {
    81|             "model": self.model,
    82|             "type": embed_type,
    83|             "texts": texts,
    84|         }
    85|         headers = {
    86|             "Authorization": f"Bearer {self.minimax_api_key}",
    87|             "Content-Type": "application/json",
    88|         }
    89|         params = {
    90|             "GroupId": self.minimax_group_id,
    91|         }
    92|         response = requests.post(
    93|             self.endpoint_url, params=params, headers=headers, json=payload
    94|         )
    95|         parsed_response = response.json()
    96|         if parsed_response["base_resp"]["status_code"] != 0:
    97|             raise ValueError(
    98|                 f"MiniMax API returned an error: {parsed_response['base_resp']}"
    99|             )
   100|         embeddings = parsed_response["vectors"]
   101|         return embeddings
   102|     def embed_documents(self, texts: List[str]) -> List[List[float]]:
   103|         """Embed documents using a MiniMax embedding endpoint.
   104|         Args:
   105|             texts: The list of texts to embed.
   106|         Returns:


# ====================================================================
# FILE: libs/community/langchain_community/embeddings/vertexai.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 3-55 ---
     3| import string
     4| import threading
     5| from concurrent.futures import ThreadPoolExecutor, wait
     6| from typing import Any, Dict, List, Literal, Optional, Tuple
     7| from langchain_core.embeddings import Embeddings
     8| from langchain_core.language_models.llms import create_base_retry_decorator
     9| from langchain_core.pydantic_v1 import root_validator
    10| from langchain_community.llms.vertexai import _VertexAICommon
    11| from langchain_community.utilities.vertexai import raise_vertex_import_error
    12| logger = logging.getLogger(__name__)
    13| _MAX_TOKENS_PER_BATCH = 20000
    14| _MAX_BATCH_SIZE = 250
    15| _MIN_BATCH_SIZE = 5
    16| class VertexAIEmbeddings(_VertexAICommon, Embeddings):
    17|     """Google Cloud VertexAI embedding models."""
    18|     instance: Dict[str, Any] = {}  #: :meta private:
    19|     @root_validator()
    20|     def validate_environment(cls, values: Dict) -> Dict:
    21|         """Validates that the python package exists in environment."""
    22|         cls._try_init_vertexai(values)
    23|         try:
    24|             from vertexai.language_models import TextEmbeddingModel
    25|             values["client"] = TextEmbeddingModel.from_pretrained(values["model_name"])
    26|         except ImportError:
    27|             raise_vertex_import_error()
    28|         return values
    29|     def __init__(
    30|         self,
    31|         project: Optional[str] = None,
    32|         location: str = "us-central1",
    33|         request_parallelism: int = 5,
    34|         max_retries: int = 6,
    35|         model_name: str = "textembedding-gecko",
    36|         credentials: Optional[Any] = None,
    37|         **kwargs: Any,
    38|     ):
    39|         """Initialize the sentence_transformer."""
    40|         super().__init__(
    41|             project=project,
    42|             location=location,
    43|             credentials=credentials,
    44|             request_parallelism=request_parallelism,
    45|             max_retries=max_retries,
    46|             model_name=model_name,
    47|             **kwargs,
    48|         )
    49|         self.instance["max_batch_size"] = kwargs.get("max_batch_size", _MAX_BATCH_SIZE)
    50|         self.instance["batch_size"] = self.instance["max_batch_size"]
    51|         self.instance["min_batch_size"] = kwargs.get("min_batch_size", _MIN_BATCH_SIZE)
    52|         self.instance["min_good_batch_size"] = self.instance["min_batch_size"]
    53|         self.instance["lock"] = threading.Lock()
    54|         self.instance["batch_size_validated"] = False
    55|         self.instance["task_executor"] = ThreadPoolExecutor(


# ====================================================================
# FILE: libs/community/langchain_community/llms/__init__.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 148-187 ---
   148|     from langchain_community.llms.minimax import Minimax
   149|     return Minimax
   150| def _import_mlflow() -> Any:
   151|     from langchain_community.llms.mlflow import Mlflow
   152|     return Mlflow
   153| def _import_mlflow_chat() -> Any:
   154|     from langchain_community.chat_models.mlflow import ChatMlflow
   155|     return ChatMlflow
   156| def _import_mlflow_ai_gateway() -> Any:
   157|     from langchain_community.llms.mlflow_ai_gateway import MlflowAIGateway
   158|     return MlflowAIGateway
   159| def _import_modal() -> Any:
   160|     from langchain_community.llms.modal import Modal
   161|     return Modal
   162| def _import_mosaicml() -> Any:
   163|     from langchain_community.llms.mosaicml import MosaicML
   164|     return MosaicML
   165| def _import_nlpcloud() -> Any:
   166|     from langchain_community.llms.nlpcloud import NLPCloud
   167|     return NLPCloud
   168| def _import_octoai_endpoint() -> Any:
   169|     from langchain_community.llms.octoai_endpoint import OctoAIEndpoint
   170|     return OctoAIEndpoint
   171| def _import_ollama() -> Any:
   172|     from langchain_community.llms.ollama import Ollama
   173|     return Ollama
   174| def _import_opaqueprompts() -> Any:
   175|     from langchain_community.llms.opaqueprompts import OpaquePrompts
   176|     return OpaquePrompts
   177| def _import_azure_openai() -> Any:
   178|     from langchain_community.llms.openai import AzureOpenAI
   179|     return AzureOpenAI
   180| def _import_openai() -> Any:
   181|     from langchain_community.llms.openai import OpenAI
   182|     return OpenAI
   183| def _import_openai_chat() -> Any:
   184|     from langchain_community.llms.openai import OpenAIChat
   185|     return OpenAIChat
   186| def _import_openllm() -> Any:
   187|     from langchain_community.llms.openllm import OpenLLM

# --- HUNK 2: Lines 355-394 ---
   355|     elif name == "JavelinAIGateway":
   356|         return _import_javelin_ai_gateway()
   357|     elif name == "KoboldApiLLM":
   358|         return _import_koboldai()
   359|     elif name == "LlamaCpp":
   360|         return _import_llamacpp()
   361|     elif name == "ManifestWrapper":
   362|         return _import_manifest()
   363|     elif name == "Minimax":
   364|         return _import_minimax()
   365|     elif name == "Mlflow":
   366|         return _import_mlflow()
   367|     elif name == "MlflowAIGateway":
   368|         return _import_mlflow_ai_gateway()
   369|     elif name == "Modal":
   370|         return _import_modal()
   371|     elif name == "MosaicML":
   372|         return _import_mosaicml()
   373|     elif name == "NLPCloud":
   374|         return _import_nlpcloud()
   375|     elif name == "OctoAIEndpoint":
   376|         return _import_octoai_endpoint()
   377|     elif name == "Ollama":
   378|         return _import_ollama()
   379|     elif name == "OpaquePrompts":
   380|         return _import_opaqueprompts()
   381|     elif name == "AzureOpenAI":
   382|         return _import_azure_openai()
   383|     elif name == "OpenAI":
   384|         return _import_openai()
   385|     elif name == "OpenAIChat":
   386|         return _import_openai_chat()
   387|     elif name == "OpenLLM":
   388|         return _import_openllm()
   389|     elif name == "OpenLM":
   390|         return _import_openlm()
   391|     elif name == "PaiEasEndpoint":
   392|         return _import_pai_eas_endpoint()
   393|     elif name == "Petals":
   394|         return _import_petals()

# --- HUNK 3: Lines 483-522 ---
   483|     "GPT4All",
   484|     "GooglePalm",
   485|     "GooseAI",
   486|     "GradientLLM",
   487|     "HuggingFaceEndpoint",
   488|     "HuggingFaceHub",
   489|     "HuggingFacePipeline",
   490|     "HuggingFaceTextGenInference",
   491|     "HumanInputLLM",
   492|     "KoboldApiLLM",
   493|     "LlamaCpp",
   494|     "TextGen",
   495|     "ManifestWrapper",
   496|     "Minimax",
   497|     "MlflowAIGateway",
   498|     "Modal",
   499|     "MosaicML",
   500|     "Nebula",
   501|     "NIBittensorLLM",
   502|     "NLPCloud",
   503|     "Ollama",
   504|     "OpenAI",
   505|     "OpenAIChat",
   506|     "OpenLLM",
   507|     "OpenLM",
   508|     "PaiEasEndpoint",
   509|     "Petals",
   510|     "PipelineAI",
   511|     "Predibase",
   512|     "PredictionGuard",
   513|     "PromptLayerOpenAI",
   514|     "PromptLayerOpenAIChat",
   515|     "OpaquePrompts",
   516|     "RWKV",
   517|     "Replicate",
   518|     "SagemakerEndpoint",
   519|     "SelfHostedHuggingFaceLLM",
   520|     "SelfHostedPipeline",
   521|     "StochasticAI",
   522|     "TitanTakeoff",

# --- HUNK 4: Lines 568-607 ---
   568|         "gooseai": _import_gooseai,
   569|         "gradient": _import_gradient_ai,
   570|         "gpt4all": _import_gpt4all,
   571|         "huggingface_endpoint": _import_huggingface_endpoint,
   572|         "huggingface_hub": _import_huggingface_hub,
   573|         "huggingface_pipeline": _import_huggingface_pipeline,
   574|         "huggingface_textgen_inference": _import_huggingface_text_gen_inference,
   575|         "human-input": _import_human,
   576|         "koboldai": _import_koboldai,
   577|         "llamacpp": _import_llamacpp,
   578|         "textgen": _import_textgen,
   579|         "minimax": _import_minimax,
   580|         "mlflow": _import_mlflow,
   581|         "mlflow-chat": _import_mlflow_chat,
   582|         "mlflow-ai-gateway": _import_mlflow_ai_gateway,
   583|         "modal": _import_modal,
   584|         "mosaic": _import_mosaicml,
   585|         "nebula": _import_symblai_nebula,
   586|         "nibittensor": _import_bittensor,
   587|         "nlpcloud": _import_nlpcloud,
   588|         "ollama": _import_ollama,
   589|         "openai": _import_openai,
   590|         "openlm": _import_openlm,
   591|         "pai_eas_endpoint": _import_pai_eas_endpoint,
   592|         "petals": _import_petals,
   593|         "pipelineai": _import_pipelineai,
   594|         "predibase": _import_predibase,
   595|         "opaqueprompts": _import_opaqueprompts,
   596|         "replicate": _import_replicate,
   597|         "rwkv": _import_rwkv,
   598|         "sagemaker_endpoint": _import_sagemaker_endpoint,
   599|         "self_hosted": _import_self_hosted,
   600|         "self_hosted_hugging_face": _import_self_hosted_hugging_face,
   601|         "stochasticai": _import_stochasticai,
   602|         "together": _import_together,
   603|         "tongyi": _import_tongyi,
   604|         "titan_takeoff": _import_titan_takeoff,
   605|         "titan_takeoff_pro": _import_titan_takeoff_pro,
   606|         "vertexai": _import_vertex,
   607|         "vertexai_model_garden": _import_vertex_model_garden,


# ====================================================================
# FILE: libs/community/langchain_community/llms/baseten.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-59 ---
     1| import logging
     2| from typing import Any, Dict, List, Mapping, Optional
     3| from langchain_core.callbacks import CallbackManagerForLLMRun
     4| from langchain_core.language_models.llms import LLM
     5| from langchain_core.pydantic_v1 import Field
     6| logger = logging.getLogger(__name__)
     7| class Baseten(LLM):
     8|     """Baseten models.
     9|     To use, you should have the ``baseten`` python package installed,
    10|     and run ``baseten.login()`` with your Baseten API key.
    11|     The required ``model`` param can be either a model id or model
    12|     version id. Using a model version ID will result in
    13|     slightly faster invocation.
    14|     Any other model parameters can also
    15|     be passed in with the format input={model_param: value, ...}
    16|     The Baseten model must accept a dictionary of input with the key
    17|     "prompt" and return a dictionary with a key "data" which maps
    18|     to a list of response strings.
    19|     Example:
    20|         .. code-block:: python
    21|             from langchain_community.llms import Baseten
    22|             my_model = Baseten(model="MODEL_ID")
    23|             output = my_model("prompt")
    24|     """
    25|     model: str
    26|     input: Dict[str, Any] = Field(default_factory=dict)
    27|     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    28|     @property
    29|     def _identifying_params(self) -> Mapping[str, Any]:
    30|         """Get the identifying parameters."""
    31|         return {
    32|             **{"model_kwargs": self.model_kwargs},
    33|         }
    34|     @property
    35|     def _llm_type(self) -> str:
    36|         """Return type of model."""
    37|         return "baseten"
    38|     def _call(
    39|         self,
    40|         prompt: str,
    41|         stop: Optional[List[str]] = None,
    42|         run_manager: Optional[CallbackManagerForLLMRun] = None,
    43|         **kwargs: Any,
    44|     ) -> str:
    45|         """Call to Baseten deployed model endpoint."""
    46|         try:
    47|             import baseten
    48|         except ImportError as exc:
    49|             raise ImportError(
    50|                 "Could not import Baseten Python package. "
    51|                 "Please install it with `pip install baseten`."
    52|             ) from exc
    53|         try:
    54|             model = baseten.deployed_model_version_id(self.model)
    55|             response = model.predict({"prompt": prompt, **kwargs})
    56|         except baseten.common.core.ApiError:
    57|             model = baseten.deployed_model_id(self.model)
    58|             response = model.predict({"prompt": prompt, **kwargs})
    59|         return "".join(response)


# ====================================================================
# FILE: libs/community/langchain_community/llms/bedrock.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-34 ---
     1| import json
     2| import warnings
     3| from abc import ABC
     4| from typing import Any, Dict, Iterator, List, Mapping, Optional
     5| from langchain_core.callbacks import CallbackManagerForLLMRun
     6| from langchain_core.language_models.llms import LLM
     7| from langchain_core.outputs import GenerationChunk
     8| from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
     9| from langchain_core.utils import get_from_dict_or_env
    10| from langchain_community.llms.utils import enforce_stop_tokens
    11| from langchain_community.utilities.anthropic import (
    12|     get_num_tokens_anthropic,
    13|     get_token_ids_anthropic,
    14| )
    15| HUMAN_PROMPT = "\n\nHuman:"
    16| ASSISTANT_PROMPT = "\n\nAssistant:"
    17| ALTERNATION_ERROR = (
    18|     "Error: Prompt must alternate between '\n\nHuman:' and '\n\nAssistant:'."
    19| )
    20| def _add_newlines_before_ha(input_text: str) -> str:
    21|     new_text = input_text
    22|     for word in ["Human:", "Assistant:"]:
    23|         new_text = new_text.replace(word, "\n\n" + word)
    24|         for i in range(2):
    25|             new_text = new_text.replace("\n\n\n" + word, "\n\n" + word)
    26|     return new_text
    27| def _human_assistant_format(input_text: str) -> str:
    28|     if input_text.count("Human:") == 0 or (
    29|         input_text.find("Human:") > input_text.find("Assistant:")
    30|         and "Assistant:" in input_text
    31|     ):
    32|         input_text = HUMAN_PROMPT + " " + input_text  # SILENT CORRECTION
    33|     if input_text.count("Assistant:") == 0:
    34|         input_text = input_text + ASSISTANT_PROMPT  # SILENT CORRECTION

# --- HUNK 2: Lines 115-191 ---
   115|                     == "<EOS_TOKEN>"
   116|                 ):
   117|                     return
   118|                 yield GenerationChunk(
   119|                     text=chunk_obj[cls.provider_to_output_key_map[provider]]
   120|                 )
   121| class BedrockBase(BaseModel, ABC):
   122|     """Base class for Bedrock models."""
   123|     client: Any = Field(exclude=True)  #: :meta private:
   124|     region_name: Optional[str] = None
   125|     """The aws region e.g., `us-west-2`. Fallsback to AWS_DEFAULT_REGION env variable
   126|     or region specified in ~/.aws/config in case it is not provided here.
   127|     """
   128|     credentials_profile_name: Optional[str] = Field(default=None, exclude=True)
   129|     """The name of the profile in the ~/.aws/credentials or ~/.aws/config files, which
   130|     has either access keys or role information specified.
   131|     If not specified, the default credential profile or, if on an EC2 instance,
   132|     credentials from IMDS will be used.
   133|     See: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html
   134|     """
   135|     model_id: str
   136|     """Id of the model to call, e.g., amazon.titan-text-express-v1, this is
   137|     equivalent to the modelId property in the list-foundation-models api"""
   138|     model_kwargs: Optional[Dict] = None
   139|     """Keyword arguments to pass to the model."""
   140|     endpoint_url: Optional[str] = None
   141|     """Needed if you don't want to default to us-east-1 endpoint"""
   142|     streaming: bool = False
   143|     """Whether to stream the results."""
   144|     provider_stop_sequence_key_name_map: Mapping[str, str] = {
   145|         "anthropic": "stop_sequences",
   146|         "amazon": "stopSequences",
   147|         "ai21": "stop_sequences",
   148|         "cohere": "stop_sequences",
   149|     }
   150|     @root_validator()
   151|     def validate_environment(cls, values: Dict) -> Dict:
   152|         """Validate that AWS credentials to and python package exists in environment."""
   153|         if values["client"] is not None:
   154|             return values
   155|         try:
   156|             import boto3
   157|             if values["credentials_profile_name"] is not None:
   158|                 session = boto3.Session(profile_name=values["credentials_profile_name"])
   159|             else:
   160|                 session = boto3.Session()
   161|             values["region_name"] = get_from_dict_or_env(
   162|                 values,
   163|                 "region_name",
   164|                 "AWS_DEFAULT_REGION",
   165|                 default=session.region_name,
   166|             )
   167|             client_params = {}
   168|             if values["region_name"]:
   169|                 client_params["region_name"] = values["region_name"]
   170|             if values["endpoint_url"]:
   171|                 client_params["endpoint_url"] = values["endpoint_url"]
   172|             values["client"] = session.client("bedrock-runtime", **client_params)
   173|         except ImportError:
   174|             raise ModuleNotFoundError(
   175|                 "Could not import boto3 python package. "
   176|                 "Please install it with `pip install boto3`."
   177|             )
   178|         except Exception as e:
   179|             raise ValueError(
   180|                 "Could not load credentials to authenticate with AWS client. "
   181|                 "Please check that credentials in the specified "
   182|                 "profile name are valid."
   183|             ) from e
   184|         return values
   185|     @property
   186|     def _identifying_params(self) -> Mapping[str, Any]:
   187|         """Get the identifying parameters."""
   188|         _model_kwargs = self.model_kwargs or {}
   189|         return {
   190|             **{"model_kwargs": _model_kwargs},
   191|         }


# ====================================================================
# FILE: libs/community/langchain_community/llms/databricks.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 23-118 ---
    23|         response = requests.request(
    24|             method=method, url=url, headers=headers, json=request
    25|         )
    26|         if not response.ok:
    27|             raise ValueError(f"HTTP {response.status_code} error: {response.text}")
    28|         return response.json()
    29|     def _get(self, url: str) -> Any:
    30|         return self.request("GET", url, None)
    31|     def _post(self, url: str, request: Any) -> Any:
    32|         return self.request("POST", url, request)
    33|     @abstractmethod
    34|     def post(
    35|         self, request: Any, transform_output_fn: Optional[Callable[..., str]] = None
    36|     ) -> Any:
    37|         ...
    38|     @property
    39|     def llm(self) -> bool:
    40|         return False
    41| def _transform_completions(response: Dict[str, Any]) -> str:
    42|     return response["choices"][0]["text"]
    43| def _transform_chat(response: Dict[str, Any]) -> str:
    44|     return response["choices"][0]["message"]["content"]
    45| class _DatabricksServingEndpointClient(_DatabricksClientBase):
    46|     """An API client that talks to a Databricks serving endpoint."""
    47|     host: str
    48|     endpoint_name: str
    49|     databricks_uri: str
    50|     client: Any = None
    51|     external_or_foundation: bool = False
    52|     task: Optional[str] = None
    53|     def __init__(self, **data: Any):
    54|         super().__init__(**data)
    55|         try:
    56|             from mlflow.deployments import get_deploy_client
    57|             self.client = get_deploy_client(self.databricks_uri)
    58|         except ImportError as e:
    59|             raise ImportError(
    60|                 "Failed to create the client. "
    61|                 "Please install mlflow with `pip install mlflow`."
    62|             ) from e
    63|         endpoint = self.client.get_endpoint(self.endpoint_name)
    64|         self.external_or_foundation = endpoint.get("endpoint_type", "").lower() in (
    65|             "external_model",
    66|             "foundation_model_api",
    67|         )
    68|         self.task = endpoint.get("task")
    69|     @property
    70|     def llm(self) -> bool:
    71|         return self.task in ("llm/v1/chat", "llm/v1/completions")
    72|     @root_validator(pre=True)
    73|     def set_api_url(cls, values: Dict[str, Any]) -> Dict[str, Any]:
    74|         if "api_url" not in values:
    75|             host = values["host"]
    76|             endpoint_name = values["endpoint_name"]
    77|             api_url = f"https://{host}/serving-endpoints/{endpoint_name}/invocations"
    78|             values["api_url"] = api_url
    79|         return values
    80|     def post(
    81|         self, request: Any, transform_output_fn: Optional[Callable[..., str]] = None
    82|     ) -> Any:
    83|         if self.external_or_foundation:
    84|             resp = self.client.predict(endpoint=self.endpoint_name, inputs=request)
    85|             if transform_output_fn:
    86|                 return transform_output_fn(resp)
    87|             if self.task == "llm/v1/chat":
    88|                 return _transform_chat(resp)
    89|             elif self.task == "llm/v1/completions":
    90|                 return _transform_completions(resp)
    91|             return resp
    92|         else:
    93|             wrapped_request = {"dataframe_records": [request]}
    94|             response = self.client.predict(
    95|                 endpoint=self.endpoint_name, inputs=wrapped_request
    96|             )
    97|             preds = response["predictions"]
    98|             pred = preds[0] if isinstance(preds, list) else preds
    99|             return transform_output_fn(pred) if transform_output_fn else pred
   100| class _DatabricksClusterDriverProxyClient(_DatabricksClientBase):
   101|     """An API client that talks to a Databricks cluster driver proxy app."""
   102|     host: str
   103|     cluster_id: str
   104|     cluster_driver_port: str
   105|     @root_validator(pre=True)
   106|     def set_api_url(cls, values: Dict[str, Any]) -> Dict[str, Any]:
   107|         if "api_url" not in values:
   108|             host = values["host"]
   109|             cluster_id = values["cluster_id"]
   110|             port = values["cluster_driver_port"]
   111|             api_url = f"https://{host}/driver-proxy-api/o/0/{cluster_id}/{port}"
   112|             values["api_url"] = api_url
   113|         return values
   114|     def post(
   115|         self, request: Any, transform_output_fn: Optional[Callable[..., str]] = None
   116|     ) -> Any:
   117|         resp = self._post(self.api_url, request)
   118|         return transform_output_fn(resp) if transform_output_fn else resp

# --- HUNK 2: Lines 238-277 ---
   238|     transform_input_fn: Optional[Callable] = None
   239|     """A function that transforms ``{prompt, stop, **kwargs}`` into a JSON-compatible
   240|     request object that the endpoint accepts.
   241|     For example, you can apply a prompt template to the input prompt.
   242|     """
   243|     transform_output_fn: Optional[Callable[..., str]] = None
   244|     """A function that transforms the output from the endpoint to the generated text.
   245|     """
   246|     databricks_uri: str = "databricks"
   247|     """The databricks URI. Only used when using a serving endpoint."""
   248|     temperature: float = 0.0
   249|     """The sampling temperature."""
   250|     n: int = 1
   251|     """The number of completion choices to generate."""
   252|     stop: Optional[List[str]] = None
   253|     """The stop sequence."""
   254|     max_tokens: Optional[int] = None
   255|     """The maximum number of tokens to generate."""
   256|     extra_params: Dict[str, Any] = Field(default_factory=dict)
   257|     """Any extra parameters to pass to the endpoint."""
   258|     _client: _DatabricksClientBase = PrivateAttr()
   259|     class Config:
   260|         extra = Extra.forbid
   261|         underscore_attrs_are_private = True
   262|     @property
   263|     def _llm_params(self) -> Dict[str, Any]:
   264|         params: Dict[str, Any] = {
   265|             "temperature": self.temperature,
   266|             "n": self.n,
   267|         }
   268|         if self.stop:
   269|             params["stop"] = self.stop
   270|         if self.max_tokens is not None:
   271|             params["max_tokens"] = self.max_tokens
   272|         return params
   273|     @validator("cluster_id", always=True)
   274|     def set_cluster_id(cls, v: Any, values: Dict[str, Any]) -> Optional[str]:
   275|         if v and values["endpoint_name"]:
   276|             raise ValueError("Cannot set both endpoint_name and cluster_id.")
   277|         elif values["endpoint_name"]:

# --- HUNK 3: Lines 307-373 ---
   307|     def set_model_kwargs(cls, v: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
   308|         if v:
   309|             assert "prompt" not in v, "model_kwargs must not contain key 'prompt'"
   310|             assert "stop" not in v, "model_kwargs must not contain key 'stop'"
   311|         return v
   312|     def __init__(self, **data: Any):
   313|         super().__init__(**data)
   314|         if self.model_kwargs is not None and self.extra_params is not None:
   315|             raise ValueError("Cannot set both extra_params and extra_params.")
   316|         elif self.model_kwargs is not None:
   317|             warnings.warn(
   318|                 "model_kwargs is deprecated. Please use extra_params instead.",
   319|                 DeprecationWarning,
   320|             )
   321|         if self.endpoint_name:
   322|             self._client = _DatabricksServingEndpointClient(
   323|                 host=self.host,
   324|                 api_token=self.api_token,
   325|                 endpoint_name=self.endpoint_name,
   326|                 databricks_uri=self.databricks_uri,
   327|             )
   328|         elif self.cluster_id and self.cluster_driver_port:
   329|             self._client = _DatabricksClusterDriverProxyClient(
   330|                 host=self.host,
   331|                 api_token=self.api_token,
   332|                 cluster_id=self.cluster_id,
   333|                 cluster_driver_port=self.cluster_driver_port,
   334|             )
   335|         else:
   336|             raise ValueError(
   337|                 "Must specify either endpoint_name or cluster_id/cluster_driver_port."
   338|             )
   339|     @property
   340|     def _default_params(self) -> Dict[str, Any]:
   341|         """Return default params."""
   342|         return {
   343|             "host": self.host,
   344|             "endpoint_name": self.endpoint_name,
   345|             "cluster_id": self.cluster_id,
   346|             "cluster_driver_port": self.cluster_driver_port,
   347|             "databricks_uri": self.databricks_uri,
   348|             "model_kwargs": self.model_kwargs,
   349|             "temperature": self.temperature,
   350|             "n": self.n,
   351|             "stop": self.stop,
   352|             "max_tokens": self.max_tokens,
   353|             "extra_params": self.extra_params,
   354|         }
   355|     @property
   356|     def _identifying_params(self) -> Mapping[str, Any]:
   357|         return self._default_params
   358|     @property
   359|     def _llm_type(self) -> str:
   360|         """Return type of llm."""
   361|         return "databricks"
   362|     def _call(
   363|         self,
   364|         prompt: str,
   365|         stop: Optional[List[str]] = None,
   366|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   367|         **kwargs: Any,
   368|     ) -> str:
   369|         """Queries the LLM endpoint with the given prompt and stop sequence."""
   370|         request: Dict[str, Any] = {"prompt": prompt}
   371|         if self._client.llm:
   372|             request.update(self._llm_params)
   373|         request.update(self.model_kwargs or self.extra_params)


# ====================================================================
# FILE: libs/community/langchain_community/llms/deepsparse.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 35-209 ---
    35|     def _identifying_params(self) -> Dict[str, Any]:
    36|         """Get the identifying parameters."""
    37|         return {
    38|             "model": self.model,
    39|             "model_config": self.model_config,
    40|             "generation_config": self.generation_config,
    41|             "streaming": self.streaming,
    42|         }
    43|     @property
    44|     def _llm_type(self) -> str:
    45|         """Return type of llm."""
    46|         return "deepsparse"
    47|     @root_validator()
    48|     def validate_environment(cls, values: Dict) -> Dict:
    49|         """Validate that ``deepsparse`` package is installed."""
    50|         try:
    51|             from deepsparse import Pipeline
    52|         except ImportError:
    53|             raise ImportError(
    54|                 "Could not import `deepsparse` package. "
    55|                 "Please install it with `pip install deepsparse`"
    56|             )
    57|         model_config = values["model_config"] or {}
    58|         values["pipeline"] = Pipeline.create(
    59|             task="text_generation",
    60|             model_path=values["model"],
    61|             **model_config,
    62|         )
    63|         return values
    64|     def _call(
    65|         self,
    66|         prompt: str,
    67|         stop: Optional[List[str]] = None,
    68|         run_manager: Optional[CallbackManagerForLLMRun] = None,
    69|         **kwargs: Any,
    70|     ) -> str:
    71|         """Generate text from a prompt.
    72|         Args:
    73|             prompt: The prompt to generate text from.
    74|             stop: A list of strings to stop generation when encountered.
    75|         Returns:
    76|             The generated text.
    77|         Example:
    78|             .. code-block:: python
    79|                 from langchain_community.llms import DeepSparse
    80|                 llm = DeepSparse(model="zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base_quant-none")
    81|                 llm("Tell me a joke.")
    82|         """
    83|         if self.streaming:
    84|             combined_output = ""
    85|             for chunk in self._stream(
    86|                 prompt=prompt, stop=stop, run_manager=run_manager, **kwargs
    87|             ):
    88|                 combined_output += chunk.text
    89|             text = combined_output
    90|         else:
    91|             text = (
    92|                 self.pipeline(
    93|                     sequences=prompt, generation_config=self.generation_config
    94|                 )
    95|                 .generations[0]
    96|                 .text
    97|             )
    98|         if stop is not None:
    99|             text = enforce_stop_tokens(text, stop)
   100|         return text
   101|     async def _acall(
   102|         self,
   103|         prompt: str,
   104|         stop: Optional[List[str]] = None,
   105|         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
   106|         **kwargs: Any,
   107|     ) -> str:
   108|         """Generate text from a prompt.
   109|         Args:
   110|             prompt: The prompt to generate text from.
   111|             stop: A list of strings to stop generation when encountered.
   112|         Returns:
   113|             The generated text.
   114|         Example:
   115|             .. code-block:: python
   116|                 from langchain_community.llms import DeepSparse
   117|                 llm = DeepSparse(model="zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base_quant-none")
   118|                 llm("Tell me a joke.")
   119|         """
   120|         if self.streaming:
   121|             combined_output = ""
   122|             async for chunk in self._astream(
   123|                 prompt=prompt, stop=stop, run_manager=run_manager, **kwargs
   124|             ):
   125|                 combined_output += chunk.text
   126|             text = combined_output
   127|         else:
   128|             text = (
   129|                 self.pipeline(
   130|                     sequences=prompt, generation_config=self.generation_config
   131|                 )
   132|                 .generations[0]
   133|                 .text
   134|             )
   135|         if stop is not None:
   136|             text = enforce_stop_tokens(text, stop)
   137|         return text
   138|     def _stream(
   139|         self,
   140|         prompt: str,
   141|         stop: Optional[List[str]] = None,
   142|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   143|         **kwargs: Any,
   144|     ) -> Iterator[GenerationChunk]:
   145|         """Yields results objects as they are generated in real time.
   146|         It also calls the callback manager's on_llm_new_token event with
   147|         similar parameters to the OpenAI LLM class method of the same name.
   148|         Args:
   149|             prompt: The prompt to pass into the model.
   150|             stop: Optional list of stop words to use when generating.
   151|         Returns:
   152|             A generator representing the stream of tokens being generated.
   153|         Yields:
   154|             A dictionary like object containing a string token.
   155|         Example:
   156|             .. code-block:: python
   157|                 from langchain_community.llms import DeepSparse
   158|                 llm = DeepSparse(
   159|                     model="zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base_quant-none",
   160|                     streaming=True
   161|                 )
   162|                 for chunk in llm.stream("Tell me a joke",
   163|                         stop=["'","\n"]):
   164|                     print(chunk, end='', flush=True)
   165|         """
   166|         inference = self.pipeline(
   167|             sequences=prompt, generation_config=self.generation_config, streaming=True
   168|         )
   169|         for token in inference:
   170|             chunk = GenerationChunk(text=token.generations[0].text)
   171|             yield chunk
   172|             if run_manager:
   173|                 run_manager.on_llm_new_token(token=chunk.text)
   174|     async def _astream(
   175|         self,
   176|         prompt: str,
   177|         stop: Optional[List[str]] = None,
   178|         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
   179|         **kwargs: Any,
   180|     ) -> AsyncIterator[GenerationChunk]:
   181|         """Yields results objects as they are generated in real time.
   182|         It also calls the callback manager's on_llm_new_token event with
   183|         similar parameters to the OpenAI LLM class method of the same name.
   184|         Args:
   185|             prompt: The prompt to pass into the model.
   186|             stop: Optional list of stop words to use when generating.
   187|         Returns:
   188|             A generator representing the stream of tokens being generated.
   189|         Yields:
   190|             A dictionary like object containing a string token.
   191|         Example:
   192|             .. code-block:: python
   193|                 from langchain_community.llms import DeepSparse
   194|                 llm = DeepSparse(
   195|                     model="zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base_quant-none",
   196|                     streaming=True
   197|                 )
   198|                 for chunk in llm.stream("Tell me a joke",
   199|                         stop=["'","\n"]):
   200|                     print(chunk, end='', flush=True)
   201|         """
   202|         inference = self.pipeline(
   203|             sequences=prompt, generation_config=self.generation_config, streaming=True
   204|         )
   205|         for token in inference:
   206|             chunk = GenerationChunk(text=token.generations[0].text)
   207|             yield chunk
   208|             if run_manager:
   209|                 await run_manager.on_llm_new_token(token=chunk.text)


# ====================================================================
# FILE: libs/community/langchain_community/llms/ollama.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-24 ---
     1| import json
     2| from typing import Any, Dict, Iterator, List, Mapping, Optional
     3| import requests
     4| from langchain_core.callbacks import CallbackManagerForLLMRun
     5| from langchain_core.language_models import BaseLanguageModel
     6| from langchain_core.language_models.llms import BaseLLM
     7| from langchain_core.outputs import GenerationChunk, LLMResult
     8| from langchain_core.pydantic_v1 import Extra
     9| def _stream_response_to_generation_chunk(
    10|     stream_response: str,
    11| ) -> GenerationChunk:
    12|     """Convert a stream response to a generation chunk."""
    13|     parsed_response = json.loads(stream_response)
    14|     generation_info = parsed_response if parsed_response.get("done") is True else None
    15|     return GenerationChunk(
    16|         text=parsed_response.get("response", ""), generation_info=generation_info
    17|     )
    18| class OllamaEndpointNotFoundError(Exception):
    19|     """Raised when the Ollama endpoint is not found."""
    20| class _OllamaCommon(BaseLanguageModel):
    21|     base_url: str = "http://localhost:11434"
    22|     """Base url the model is hosted under."""
    23|     model: str = "llama2"
    24|     """Model name to use."""

# --- HUNK 2: Lines 102-141 ---
   102|             "template": self.template,
   103|         }
   104|     @property
   105|     def _identifying_params(self) -> Mapping[str, Any]:
   106|         """Get the identifying parameters."""
   107|         return {**{"model": self.model, "format": self.format}, **self._default_params}
   108|     def _create_generate_stream(
   109|         self,
   110|         prompt: str,
   111|         stop: Optional[List[str]] = None,
   112|         images: Optional[List[str]] = None,
   113|         **kwargs: Any,
   114|     ) -> Iterator[str]:
   115|         payload = {"prompt": prompt, "images": images}
   116|         yield from self._create_stream(
   117|             payload=payload,
   118|             stop=stop,
   119|             api_url=f"{self.base_url}/api/generate/",
   120|             **kwargs,
   121|         )
   122|     def _create_stream(
   123|         self,
   124|         api_url: str,
   125|         payload: Any,
   126|         stop: Optional[List[str]] = None,
   127|         **kwargs: Any,
   128|     ) -> Iterator[str]:
   129|         if self.stop is not None and stop is not None:
   130|             raise ValueError("`stop` found in both the input and default params.")
   131|         elif self.stop is not None:
   132|             stop = self.stop
   133|         elif stop is None:
   134|             stop = []
   135|         params = self._default_params
   136|         if "model" in kwargs:
   137|             params["model"] = kwargs["model"]
   138|         if "options" in kwargs:
   139|             params["options"] = kwargs["options"]
   140|         else:
   141|             params["options"] = {

# --- HUNK 3: Lines 145-211 ---
   145|             }
   146|         if payload.get("messages"):
   147|             request_payload = {"messages": payload.get("messages", []), **params}
   148|         else:
   149|             request_payload = {
   150|                 "prompt": payload.get("prompt"),
   151|                 "images": payload.get("images", []),
   152|                 **params,
   153|             }
   154|         response = requests.post(
   155|             url=api_url,
   156|             headers={"Content-Type": "application/json"},
   157|             json=request_payload,
   158|             stream=True,
   159|             timeout=self.timeout,
   160|         )
   161|         response.encoding = "utf-8"
   162|         if response.status_code != 200:
   163|             if response.status_code == 404:
   164|                 raise OllamaEndpointNotFoundError(
   165|                     "Ollama call failed with status code 404."
   166|                 )
   167|             else:
   168|                 optional_detail = response.json().get("error")
   169|                 raise ValueError(
   170|                     f"Ollama call failed with status code {response.status_code}."
   171|                     f" Details: {optional_detail}"
   172|                 )
   173|         return response.iter_lines(decode_unicode=True)
   174|     def _stream_with_aggregation(
   175|         self,
   176|         prompt: str,
   177|         stop: Optional[List[str]] = None,
   178|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   179|         verbose: bool = False,
   180|         **kwargs: Any,
   181|     ) -> GenerationChunk:
   182|         final_chunk: Optional[GenerationChunk] = None
   183|         for stream_resp in self._create_generate_stream(prompt, stop, **kwargs):
   184|             if stream_resp:
   185|                 chunk = _stream_response_to_generation_chunk(stream_resp)
   186|                 if final_chunk is None:
   187|                     final_chunk = chunk
   188|                 else:
   189|                     final_chunk += chunk
   190|                 if run_manager:
   191|                     run_manager.on_llm_new_token(
   192|                         chunk.text,
   193|                         verbose=verbose,
   194|                     )
   195|         if final_chunk is None:
   196|             raise ValueError("No data received from Ollama stream.")
   197|         return final_chunk
   198| class Ollama(BaseLLM, _OllamaCommon):
   199|     """Ollama locally runs large language models.
   200|     To use, follow the instructions at https://ollama.ai/.
   201|     Example:
   202|         .. code-block:: python
   203|             from langchain_community.llms import Ollama
   204|             ollama = Ollama(model="llama2")
   205|     """
   206|     class Config:
   207|         """Configuration for this pydantic object."""
   208|         extra = Extra.forbid
   209|     @property
   210|     def _llm_type(self) -> str:
   211|         """Return type of llm."""

# --- HUNK 4: Lines 223-258 ---
   223|             prompt: The prompt to pass into the model.
   224|             stop: Optional list of stop words to use when generating.
   225|         Returns:
   226|             The string generated by the model.
   227|         Example:
   228|             .. code-block:: python
   229|                 response = ollama("Tell me a joke.")
   230|         """
   231|         generations = []
   232|         for prompt in prompts:
   233|             final_chunk = super()._stream_with_aggregation(
   234|                 prompt,
   235|                 stop=stop,
   236|                 images=images,
   237|                 run_manager=run_manager,
   238|                 verbose=self.verbose,
   239|                 **kwargs,
   240|             )
   241|             generations.append([final_chunk])
   242|         return LLMResult(generations=generations)
   243|     def _stream(
   244|         self,
   245|         prompt: str,
   246|         stop: Optional[List[str]] = None,
   247|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   248|         **kwargs: Any,
   249|     ) -> Iterator[GenerationChunk]:
   250|         for stream_resp in self._create_stream(prompt, stop, **kwargs):
   251|             if stream_resp:
   252|                 chunk = _stream_response_to_generation_chunk(stream_resp)
   253|                 yield chunk
   254|                 if run_manager:
   255|                     run_manager.on_llm_new_token(
   256|                         chunk.text,
   257|                         verbose=self.verbose,
   258|                     )


# ====================================================================
# FILE: libs/community/langchain_community/llms/petals.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-97 ---
     1| import logging
     2| from typing import Any, Dict, List, Mapping, Optional
     3| from langchain_core.callbacks import CallbackManagerForLLMRun
     4| from langchain_core.language_models.llms import LLM
     5| from langchain_core.pydantic_v1 import Extra, Field, root_validator
     6| from langchain_core.utils import get_from_dict_or_env
     7| from langchain_community.llms.utils import enforce_stop_tokens
     8| logger = logging.getLogger(__name__)
     9| class Petals(LLM):
    10|     """Petals Bloom models.
    11|     To use, you should have the ``petals`` python package installed, and the
    12|     environment variable ``HUGGINGFACE_API_KEY`` set with your API key.
    13|     Any parameters that are valid to be passed to the call can be passed
    14|     in, even if not explicitly saved on this class.
    15|     Example:
    16|         .. code-block:: python
    17|             from langchain_community.llms import petals
    18|             petals = Petals()
    19|     """
    20|     client: Any
    21|     """The client to use for the API calls."""
    22|     tokenizer: Any
    23|     """The tokenizer to use for the API calls."""
    24|     model_name: str = "bigscience/bloom-petals"
    25|     """The model to use."""
    26|     temperature: float = 0.7
    27|     """What sampling temperature to use"""
    28|     max_new_tokens: int = 256
    29|     """The maximum number of new tokens to generate in the completion."""
    30|     top_p: float = 0.9
    31|     """The cumulative probability for top-p sampling."""
    32|     top_k: Optional[int] = None
    33|     """The number of highest probability vocabulary tokens
    34|     to keep for top-k-filtering."""
    35|     do_sample: bool = True
    36|     """Whether or not to use sampling; use greedy decoding otherwise."""
    37|     max_length: Optional[int] = None
    38|     """The maximum length of the sequence to be generated."""
    39|     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    40|     """Holds any model parameters valid for `create` call
    41|     not explicitly specified."""
    42|     huggingface_api_key: Optional[str] = None
    43|     class Config:
    44|         """Configuration for this pydantic config."""
    45|         extra = Extra.forbid
    46|     @root_validator(pre=True)
    47|     def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:
    48|         """Build extra kwargs from additional params that were passed in."""
    49|         all_required_field_names = {field.alias for field in cls.__fields__.values()}
    50|         extra = values.get("model_kwargs", {})
    51|         for field_name in list(values):
    52|             if field_name not in all_required_field_names:
    53|                 if field_name in extra:
    54|                     raise ValueError(f"Found {field_name} supplied twice.")
    55|                 logger.warning(
    56|                     f"""WARNING! {field_name} is not default parameter.
    57|                     {field_name} was transferred to model_kwargs.
    58|                     Please confirm that {field_name} is what you intended."""
    59|                 )
    60|                 extra[field_name] = values.pop(field_name)
    61|         values["model_kwargs"] = extra
    62|         return values
    63|     @root_validator()
    64|     def validate_environment(cls, values: Dict) -> Dict:
    65|         """Validate that api key and python package exists in environment."""
    66|         huggingface_api_key = get_from_dict_or_env(
    67|             values, "huggingface_api_key", "HUGGINGFACE_API_KEY"
    68|         )
    69|         try:
    70|             from petals import AutoDistributedModelForCausalLM
    71|             from transformers import AutoTokenizer
    72|             model_name = values["model_name"]
    73|             values["tokenizer"] = AutoTokenizer.from_pretrained(model_name)
    74|             values["client"] = AutoDistributedModelForCausalLM.from_pretrained(
    75|                 model_name
    76|             )
    77|             values["huggingface_api_key"] = huggingface_api_key
    78|         except ImportError:
    79|             raise ImportError(
    80|                 "Could not import transformers or petals python package."
    81|                 "Please install with `pip install -U transformers petals`."
    82|             )
    83|         return values
    84|     @property
    85|     def _default_params(self) -> Dict[str, Any]:
    86|         """Get the default parameters for calling Petals API."""
    87|         normal_params = {
    88|             "temperature": self.temperature,
    89|             "max_new_tokens": self.max_new_tokens,
    90|             "top_p": self.top_p,
    91|             "top_k": self.top_k,
    92|             "do_sample": self.do_sample,
    93|             "max_length": self.max_length,
    94|         }
    95|         return {**normal_params, **self.model_kwargs}
    96|     @property
    97|     def _identifying_params(self) -> Mapping[str, Any]:


# ====================================================================
# FILE: libs/community/langchain_community/llms/pipelineai.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-92 ---
     1| import logging
     2| from typing import Any, Dict, List, Mapping, Optional
     3| from langchain_core.callbacks import CallbackManagerForLLMRun
     4| from langchain_core.language_models.llms import LLM
     5| from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
     6| from langchain_core.utils import get_from_dict_or_env
     7| from langchain_community.llms.utils import enforce_stop_tokens
     8| logger = logging.getLogger(__name__)
     9| class PipelineAI(LLM, BaseModel):
    10|     """PipelineAI large language models.
    11|     To use, you should have the ``pipeline-ai`` python package installed,
    12|     and the environment variable ``PIPELINE_API_KEY`` set with your API key.
    13|     Any parameters that are valid to be passed to the call can be passed
    14|     in, even if not explicitly saved on this class.
    15|     Example:
    16|         .. code-block:: python
    17|             from langchain_community.llms import PipelineAI
    18|             pipeline = PipelineAI(pipeline_key="")
    19|     """
    20|     pipeline_key: str = ""
    21|     """The id or tag of the target pipeline"""
    22|     pipeline_kwargs: Dict[str, Any] = Field(default_factory=dict)
    23|     """Holds any pipeline parameters valid for `create` call not
    24|     explicitly specified."""
    25|     pipeline_api_key: Optional[str] = None
    26|     class Config:
    27|         """Configuration for this pydantic config."""
    28|         extra = Extra.forbid
    29|     @root_validator(pre=True)
    30|     def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:
    31|         """Build extra kwargs from additional params that were passed in."""
    32|         all_required_field_names = {field.alias for field in cls.__fields__.values()}
    33|         extra = values.get("pipeline_kwargs", {})
    34|         for field_name in list(values):
    35|             if field_name not in all_required_field_names:
    36|                 if field_name in extra:
    37|                     raise ValueError(f"Found {field_name} supplied twice.")
    38|                 logger.warning(
    39|                     f"""{field_name} was transferred to pipeline_kwargs.
    40|                     Please confirm that {field_name} is what you intended."""
    41|                 )
    42|                 extra[field_name] = values.pop(field_name)
    43|         values["pipeline_kwargs"] = extra
    44|         return values
    45|     @root_validator()
    46|     def validate_environment(cls, values: Dict) -> Dict:
    47|         """Validate that api key and python package exists in environment."""
    48|         pipeline_api_key = get_from_dict_or_env(
    49|             values, "pipeline_api_key", "PIPELINE_API_KEY"
    50|         )
    51|         values["pipeline_api_key"] = pipeline_api_key
    52|         return values
    53|     @property
    54|     def _identifying_params(self) -> Mapping[str, Any]:
    55|         """Get the identifying parameters."""
    56|         return {
    57|             **{"pipeline_key": self.pipeline_key},
    58|             **{"pipeline_kwargs": self.pipeline_kwargs},
    59|         }
    60|     @property
    61|     def _llm_type(self) -> str:
    62|         """Return type of llm."""
    63|         return "pipeline_ai"
    64|     def _call(
    65|         self,
    66|         prompt: str,
    67|         stop: Optional[List[str]] = None,
    68|         run_manager: Optional[CallbackManagerForLLMRun] = None,
    69|         **kwargs: Any,
    70|     ) -> str:
    71|         """Call to Pipeline Cloud endpoint."""
    72|         try:
    73|             from pipeline import PipelineCloud
    74|         except ImportError:
    75|             raise ImportError(
    76|                 "Could not import pipeline-ai python package. "
    77|                 "Please install it with `pip install pipeline-ai`."
    78|             )
    79|         client = PipelineCloud(token=self.pipeline_api_key)
    80|         params = self.pipeline_kwargs or {}
    81|         params = {**params, **kwargs}
    82|         run = client.run_pipeline(self.pipeline_key, [prompt, params])
    83|         try:
    84|             text = run.result_preview[0][0]
    85|         except AttributeError:
    86|             raise AttributeError(
    87|                 f"A pipeline run should have a `result_preview` attribute."
    88|                 f"Run was: {run}"
    89|             )
    90|         if stop is not None:
    91|             text = enforce_stop_tokens(text, stop)
    92|         return text


# ====================================================================
# FILE: libs/community/langchain_community/llms/predibase.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-40 ---
     1| from typing import Any, Dict, List, Mapping, Optional
     2| from langchain_core.callbacks import CallbackManagerForLLMRun
     3| from langchain_core.language_models.llms import LLM
     4| from langchain_core.pydantic_v1 import Field
     5| class Predibase(LLM):
     6|     """Use your Predibase models with Langchain.
     7|     To use, you should have the ``predibase`` python package installed,
     8|     and have your Predibase API key.
     9|     """
    10|     model: str
    11|     predibase_api_key: str
    12|     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    13|     @property
    14|     def _llm_type(self) -> str:
    15|         return "predibase"
    16|     def _call(
    17|         self,
    18|         prompt: str,
    19|         stop: Optional[List[str]] = None,
    20|         run_manager: Optional[CallbackManagerForLLMRun] = None,
    21|         **kwargs: Any,
    22|     ) -> str:
    23|         try:
    24|             from predibase import PredibaseClient
    25|             pc = PredibaseClient(token=self.predibase_api_key)
    26|         except ImportError as e:
    27|             raise ImportError(
    28|                 "Could not import Predibase Python package. "
    29|                 "Please install it with `pip install predibase`."
    30|             ) from e
    31|         except ValueError as e:
    32|             raise ValueError("Your API key is not correct. Please try again") from e
    33|         results = pc.prompt(prompt, model_name=self.model)
    34|         return results[0].response
    35|     @property
    36|     def _identifying_params(self) -> Mapping[str, Any]:
    37|         """Get the identifying parameters."""
    38|         return {
    39|             **{"model_kwargs": self.model_kwargs},
    40|         }


# ====================================================================
# FILE: libs/community/langchain_community/llms/stochasticai.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-112 ---
     1| import logging
     2| import time
     3| from typing import Any, Dict, List, Mapping, Optional
     4| import requests
     5| from langchain_core.callbacks import CallbackManagerForLLMRun
     6| from langchain_core.language_models.llms import LLM
     7| from langchain_core.pydantic_v1 import Extra, Field, root_validator
     8| from langchain_core.utils import get_from_dict_or_env
     9| from langchain_community.llms.utils import enforce_stop_tokens
    10| logger = logging.getLogger(__name__)
    11| class StochasticAI(LLM):
    12|     """StochasticAI large language models.
    13|     To use, you should have the environment variable ``STOCHASTICAI_API_KEY``
    14|     set with your API key.
    15|     Example:
    16|         .. code-block:: python
    17|             from langchain_community.llms import StochasticAI
    18|             stochasticai = StochasticAI(api_url="")
    19|     """
    20|     api_url: str = ""
    21|     """Model name to use."""
    22|     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    23|     """Holds any model parameters valid for `create` call not
    24|     explicitly specified."""
    25|     stochasticai_api_key: Optional[str] = None
    26|     class Config:
    27|         """Configuration for this pydantic object."""
    28|         extra = Extra.forbid
    29|     @root_validator(pre=True)
    30|     def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:
    31|         """Build extra kwargs from additional params that were passed in."""
    32|         all_required_field_names = {field.alias for field in cls.__fields__.values()}
    33|         extra = values.get("model_kwargs", {})
    34|         for field_name in list(values):
    35|             if field_name not in all_required_field_names:
    36|                 if field_name in extra:
    37|                     raise ValueError(f"Found {field_name} supplied twice.")
    38|                 logger.warning(
    39|                     f"""{field_name} was transferred to model_kwargs.
    40|                     Please confirm that {field_name} is what you intended."""
    41|                 )
    42|                 extra[field_name] = values.pop(field_name)
    43|         values["model_kwargs"] = extra
    44|         return values
    45|     @root_validator()
    46|     def validate_environment(cls, values: Dict) -> Dict:
    47|         """Validate that api key exists in environment."""
    48|         stochasticai_api_key = get_from_dict_or_env(
    49|             values, "stochasticai_api_key", "STOCHASTICAI_API_KEY"
    50|         )
    51|         values["stochasticai_api_key"] = stochasticai_api_key
    52|         return values
    53|     @property
    54|     def _identifying_params(self) -> Mapping[str, Any]:
    55|         """Get the identifying parameters."""
    56|         return {
    57|             **{"endpoint_url": self.api_url},
    58|             **{"model_kwargs": self.model_kwargs},
    59|         }
    60|     @property
    61|     def _llm_type(self) -> str:
    62|         """Return type of llm."""
    63|         return "stochasticai"
    64|     def _call(
    65|         self,
    66|         prompt: str,
    67|         stop: Optional[List[str]] = None,
    68|         run_manager: Optional[CallbackManagerForLLMRun] = None,
    69|         **kwargs: Any,
    70|     ) -> str:
    71|         """Call out to StochasticAI's complete endpoint.
    72|         Args:
    73|             prompt: The prompt to pass into the model.
    74|             stop: Optional list of stop words to use when generating.
    75|         Returns:
    76|             The string generated by the model.
    77|         Example:
    78|             .. code-block:: python
    79|                 response = StochasticAI("Tell me a joke.")
    80|         """
    81|         params = self.model_kwargs or {}
    82|         params = {**params, **kwargs}
    83|         response_post = requests.post(
    84|             url=self.api_url,
    85|             json={"prompt": prompt, "params": params},
    86|             headers={
    87|                 "apiKey": f"{self.stochasticai_api_key}",
    88|                 "Accept": "application/json",
    89|                 "Content-Type": "application/json",
    90|             },
    91|         )
    92|         response_post.raise_for_status()
    93|         response_post_json = response_post.json()
    94|         completed = False
    95|         while not completed:
    96|             response_get = requests.get(
    97|                 url=response_post_json["data"]["responseUrl"],
    98|                 headers={
    99|                     "apiKey": f"{self.stochasticai_api_key}",
   100|                     "Accept": "application/json",
   101|                     "Content-Type": "application/json",
   102|                 },
   103|             )
   104|             response_get.raise_for_status()
   105|             response_get_json = response_get.json()["data"]
   106|             text = response_get_json.get("completion")
   107|             completed = text is not None
   108|             time.sleep(0.5)
   109|         text = text[0]
   110|         if stop is not None:
   111|             text = enforce_stop_tokens(text, stop)
   112|         return text


# ====================================================================
# FILE: libs/community/langchain_community/llms/volcengine_maas.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-77 ---
     1| from __future__ import annotations
     2| from typing import Any, Dict, Iterator, List, Optional
     3| from langchain_core.callbacks import CallbackManagerForLLMRun
     4| from langchain_core.language_models.llms import LLM
     5| from langchain_core.outputs import GenerationChunk
     6| from langchain_core.pydantic_v1 import BaseModel, Field, root_validator
     7| from langchain_core.utils import get_from_dict_or_env
     8| class VolcEngineMaasBase(BaseModel):
     9|     """Base class for VolcEngineMaas models."""
    10|     client: Any
    11|     volc_engine_maas_ak: Optional[str] = None
    12|     """access key for volc engine"""
    13|     volc_engine_maas_sk: Optional[str] = None
    14|     """secret key for volc engine"""
    15|     endpoint: Optional[str] = "maas-api.ml-platform-cn-beijing.volces.com"
    16|     """Endpoint of the VolcEngineMaas LLM."""
    17|     region: Optional[str] = "Region"
    18|     """Region of the VolcEngineMaas LLM."""
    19|     model: str = "skylark-lite-public"
    20|     """Model name. you could check this model details here 
    21|     https://www.volcengine.com/docs/82379/1133187
    22|     and you could choose other models by change this field"""
    23|     model_version: Optional[str] = None
    24|     """Model version. Only used in moonshot large language model. 
    25|     you could check details here https://www.volcengine.com/docs/82379/1158281"""
    26|     top_p: Optional[float] = 0.8
    27|     """Total probability mass of tokens to consider at each step."""
    28|     temperature: Optional[float] = 0.95
    29|     """A non-negative float that tunes the degree of randomness in generation."""
    30|     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    31|     """model special arguments, you could check detail on model page"""
    32|     streaming: bool = False
    33|     """Whether to stream the results."""
    34|     connect_timeout: Optional[int] = 60
    35|     """Timeout for connect to volc engine maas endpoint. Default is 60 seconds."""
    36|     read_timeout: Optional[int] = 60
    37|     """Timeout for read response from volc engine maas endpoint. 
    38|     Default is 60 seconds."""
    39|     @root_validator()
    40|     def validate_environment(cls, values: Dict) -> Dict:
    41|         ak = get_from_dict_or_env(values, "volc_engine_maas_ak", "VOLC_ACCESSKEY")
    42|         sk = get_from_dict_or_env(values, "volc_engine_maas_sk", "VOLC_SECRETKEY")
    43|         endpoint = values["endpoint"]
    44|         if values["endpoint"] is not None and values["endpoint"] != "":
    45|             endpoint = values["endpoint"]
    46|         try:
    47|             from volcengine.maas import MaasService
    48|             maas = MaasService(
    49|                 endpoint,
    50|                 values["region"],
    51|                 connection_timeout=values["connect_timeout"],
    52|                 socket_timeout=values["read_timeout"],
    53|             )
    54|             maas.set_ak(ak)
    55|             values["volc_engine_maas_ak"] = ak
    56|             values["volc_engine_maas_sk"] = sk
    57|             maas.set_sk(sk)
    58|             values["client"] = maas
    59|         except ImportError:
    60|             raise ImportError(
    61|                 "volcengine package not found, please install it with "
    62|                 "`pip install volcengine`"
    63|             )
    64|         return values
    65|     @property
    66|     def _default_params(self) -> Dict[str, Any]:
    67|         """Get the default parameters for calling VolcEngineMaas API."""
    68|         normal_params = {
    69|             "top_p": self.top_p,
    70|             "temperature": self.temperature,
    71|         }
    72|         return {**normal_params, **self.model_kwargs}
    73| class VolcEngineMaasLLM(LLM, VolcEngineMaasBase):
    74|     """volc engine maas hosts a plethora of models.
    75|     You can utilize these models through this class.
    76|     To use, you should have the ``volcengine`` python package installed.
    77|     and set access key and secret key by environment variable or direct pass those to


# ====================================================================
# FILE: libs/community/langchain_community/retrievers/google_vertex_ai_search.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 112-152 ---
   112|             doc_metadata = document_dict.get("struct_data", {})
   113|             doc_metadata["id"] = document_dict["id"]
   114|             if chunk_type not in derived_struct_data:
   115|                 continue
   116|             for chunk in derived_struct_data[chunk_type]:
   117|                 doc_metadata["source"] = derived_struct_data.get("link", "")
   118|                 if chunk_type == "extractive_answers":
   119|                     doc_metadata["source"] += f":{chunk.get('pageNumber', '')}"
   120|                 documents.append(
   121|                     Document(
   122|                         page_content=chunk.get("content", ""), metadata=doc_metadata
   123|                     )
   124|                 )
   125|         return documents
   126|     def _convert_website_search_response(
   127|         self, results: Sequence[SearchResult], chunk_type: str
   128|     ) -> List[Document]:
   129|         """Converts a sequence of search results to a list of LangChain documents."""
   130|         from google.protobuf.json_format import MessageToDict
   131|         documents: List[Document] = []
   132|         chunk_type = "extractive_answers"
   133|         for result in results:
   134|             document_dict = MessageToDict(
   135|                 result.document._pb, preserving_proto_field_name=True
   136|             )
   137|             derived_struct_data = document_dict.get("derived_struct_data")
   138|             if not derived_struct_data:
   139|                 continue
   140|             doc_metadata = document_dict.get("struct_data", {})
   141|             doc_metadata["id"] = document_dict["id"]
   142|             doc_metadata["source"] = derived_struct_data.get("link", "")
   143|             if chunk_type not in derived_struct_data:
   144|                 continue
   145|             text_field = "snippet" if chunk_type == "snippets" else "content"
   146|             for chunk in derived_struct_data[chunk_type]:
   147|                 documents.append(
   148|                     Document(
   149|                         page_content=chunk.get(text_field, ""), metadata=doc_metadata
   150|                     )
   151|                 )
   152|         if not documents:


# ====================================================================
# FILE: libs/community/langchain_community/tools/e2b_data_analysis/tool.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-27 ---
     1| from __future__ import annotations
     2| import ast
     3| import json
     4| import os
     5| from io import StringIO
     6| from sys import version_info
     7| from typing import IO, TYPE_CHECKING, Any, Callable, List, Optional, Type
     8| from langchain_core.callbacks import (
     9|     AsyncCallbackManagerForToolRun,
    10|     CallbackManager,
    11|     CallbackManagerForToolRun,
    12| )
    13| from langchain_core.pydantic_v1 import BaseModel, Field, PrivateAttr
    14| from langchain_community.tools import BaseTool, Tool
    15| from langchain_community.tools.e2b_data_analysis.unparse import Unparser
    16| if TYPE_CHECKING:
    17|     from e2b import EnvVars
    18|     from e2b.templates.data_analysis import Artifact
    19| base_description = """Evaluates python code in a sandbox environment. \
    20| The environment is long running and exists across multiple executions. \
    21| You must send the whole script every time and print your outputs. \
    22| Script should be pure python code that can be evaluated. \
    23| It should be in python format NOT markdown. \
    24| The code should NOT be wrapped in backticks. \
    25| All python packages including requests, matplotlib, scipy, numpy, pandas, \
    26| etc are available. Create and display chart using `plt.show()`."""
    27| def _unparse(tree: ast.AST) -> str:

# --- HUNK 2: Lines 144-187 ---
   144|         }
   145|         return json.dumps(out)
   146|     async def _arun(
   147|         self,
   148|         python_code: str,
   149|         run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
   150|     ) -> str:
   151|         raise NotImplementedError("e2b_data_analysis does not support async")
   152|     def run_command(
   153|         self,
   154|         cmd: str,
   155|     ) -> dict:
   156|         """Run shell command in the sandbox."""
   157|         proc = self.session.process.start(cmd)
   158|         output = proc.wait()
   159|         return {
   160|             "stdout": output.stdout,
   161|             "stderr": output.stderr,
   162|             "exit_code": output.exit_code,
   163|         }
   164|     def install_python_packages(self, package_names: str | List[str]) -> None:
   165|         """Install python packages in the sandbox."""
   166|         self.session.install_python_packages(package_names)
   167|     def install_system_packages(self, package_names: str | List[str]) -> None:
   168|         """Install system packages (via apt) in the sandbox."""
   169|         self.session.install_system_packages(package_names)
   170|     def download_file(self, remote_path: str) -> bytes:
   171|         """Download file from the sandbox."""
   172|         return self.session.download_file(remote_path)
   173|     def upload_file(self, file: IO, description: str) -> UploadedFile:
   174|         """Upload file to the sandbox.
   175|         The file is uploaded to the '/home/user/<filename>' path."""
   176|         remote_path = self.session.upload_file(file)
   177|         f = UploadedFile(
   178|             name=os.path.basename(file.name),
   179|             remote_path=remote_path,
   180|             description=description,
   181|         )
   182|         self._uploaded_files.append(f)
   183|         self.description = self.description + "\n" + self.uploaded_files_description
   184|         return f
   185|     def remove_uploaded_file(self, uploaded_file: UploadedFile) -> None:
   186|         """Remove uploaded file from the sandbox."""
   187|         self.session.filesystem.remove(uploaded_file.remote_path)


# ====================================================================
# FILE: libs/community/langchain_community/tools/gmail/send_message.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-56 ---
     1| """Send Gmail messages."""
     2| import base64
     3| from email.mime.multipart import MIMEMultipart
     4| from email.mime.text import MIMEText
     5| from typing import Any, Dict, List, Optional, Union
     6| from langchain_core.callbacks import CallbackManagerForToolRun
     7| from langchain_core.pydantic_v1 import BaseModel, Field
     8| from langchain_community.tools.gmail.base import GmailBaseTool
     9| class SendMessageSchema(BaseModel):
    10|     """Input for SendMessageTool."""
    11|     message: str = Field(
    12|         ...,
    13|         description="The message to send.",
    14|     )
    15|     to: Union[str, List[str]] = Field(
    16|         ...,
    17|         description="The list of recipients.",
    18|     )
    19|     subject: str = Field(
    20|         ...,
    21|         description="The subject of the message.",
    22|     )
    23|     cc: Optional[Union[str, List[str]]] = Field(
    24|         None,
    25|         description="The list of CC recipients.",
    26|     )
    27|     bcc: Optional[Union[str, List[str]]] = Field(
    28|         None,
    29|         description="The list of BCC recipients.",
    30|     )
    31| class GmailSendMessage(GmailBaseTool):
    32|     """Tool that sends a message to Gmail."""
    33|     name: str = "send_gmail_message"
    34|     description: str = (
    35|         "Use this tool to send email messages." " The input is the message, recipients"
    36|     )
    37|     def _prepare_message(
    38|         self,
    39|         message: str,
    40|         to: Union[str, List[str]],
    41|         subject: str,
    42|         cc: Optional[Union[str, List[str]]] = None,
    43|         bcc: Optional[Union[str, List[str]]] = None,
    44|     ) -> Dict[str, Any]:
    45|         """Create a message for an email."""
    46|         mime_message = MIMEMultipart()
    47|         mime_message.attach(MIMEText(message, "html"))
    48|         mime_message["To"] = ", ".join(to if isinstance(to, list) else [to])
    49|         mime_message["Subject"] = subject
    50|         if cc is not None:
    51|             mime_message["Cc"] = ", ".join(cc if isinstance(cc, list) else [cc])
    52|         if bcc is not None:
    53|             mime_message["Bcc"] = ", ".join(bcc if isinstance(bcc, list) else [bcc])
    54|         encoded_message = base64.urlsafe_b64encode(mime_message.as_bytes()).decode()
    55|         return {"raw": encoded_message}
    56|     def _run(


# ====================================================================
# FILE: libs/community/langchain_community/tools/tavily_search/tool.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-79 ---
     1| """Tool for the Tavily search API."""
     2| from typing import Dict, List, Optional, Type, Union
     3| from langchain_core.callbacks import (
     4|     AsyncCallbackManagerForToolRun,
     5|     CallbackManagerForToolRun,
     6| )
     7| from langchain_core.pydantic_v1 import BaseModel, Field
     8| from langchain_core.tools import BaseTool
     9| from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper
    10| class TavilyInput(BaseModel):
    11|     """Input for the Tavily tool."""
    12|     query: str = Field(description="search query to look up")
    13| class TavilySearchResults(BaseTool):
    14|     """Tool that queries the Tavily Search API and gets back json."""
    15|     name: str = "tavily_search_results_json"
    16|     description: str = (
    17|         "A search engine optimized for comprehensive, accurate, and trusted results. "
    18|         "Useful for when you need to answer questions about current events. "
    19|         "Input should be a search query."
    20|     )
    21|     api_wrapper: TavilySearchAPIWrapper
    22|     max_results: int = 5
    23|     args_schema: Type[BaseModel] = TavilyInput
    24|     def _run(
    25|         self,
    26|         query: str,
    27|         run_manager: Optional[CallbackManagerForToolRun] = None,
    28|     ) -> Union[List[Dict], str]:
    29|         """Use the tool."""
    30|         try:
    31|             return self.api_wrapper.results(
    32|                 query,
    33|                 self.max_results,
    34|             )
    35|         except Exception as e:
    36|             return repr(e)
    37|     async def _arun(
    38|         self,
    39|         query: str,
    40|         run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
    41|     ) -> Union[List[Dict], str]:
    42|         """Use the tool asynchronously."""
    43|         try:
    44|             return await self.api_wrapper.results_async(
    45|                 query,
    46|                 self.max_results,
    47|             )
    48|         except Exception as e:
    49|             return repr(e)
    50| class TavilyAnswer(BaseTool):
    51|     """Tool that queries the Tavily Search API and gets back an answer."""
    52|     name: str = "tavily_answer"
    53|     description: str = (
    54|         "A search engine optimized for comprehensive, accurate, and trusted results. "
    55|         "Useful for when you need to answer questions about current events. "
    56|         "Input should be a search query. "
    57|         "This returns only the answer - not the original source data."
    58|     )
    59|     api_wrapper: TavilySearchAPIWrapper
    60|     args_schema: Type[BaseModel] = TavilyInput
    61|     def _run(
    62|         self,
    63|         query: str,
    64|         run_manager: Optional[CallbackManagerForToolRun] = None,
    65|     ) -> Union[List[Dict], str]:
    66|         """Use the tool."""
    67|         try:
    68|             return self.api_wrapper.raw_results(
    69|                 query,
    70|                 max_results=5,
    71|                 include_answer=True,
    72|                 search_depth="basic",
    73|             )["answer"]
    74|         except Exception as e:
    75|             return repr(e)
    76|     async def _arun(
    77|         self,
    78|         query: str,
    79|         run_manager: Optional[AsyncCallbackManagerForToolRun] = None,


# ====================================================================
# FILE: libs/community/langchain_community/utilities/github.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 49-89 ---
    49|             )
    50|         try:
    51|             with open(github_app_private_key, "r") as f:
    52|                 private_key = f.read()
    53|         except Exception:
    54|             private_key = github_app_private_key
    55|         auth = Auth.AppAuth(
    56|             github_app_id,
    57|             private_key,
    58|         )
    59|         gi = GithubIntegration(auth=auth)
    60|         installation = gi.get_installations()
    61|         if not installation:
    62|             raise ValueError(
    63|                 f"Please make sure to install the created github app with id "
    64|                 f"{github_app_id} on the repo: {github_repository}"
    65|                 "More instructions can be found at "
    66|                 "https://docs.github.com/en/apps/using-"
    67|                 "github-apps/installing-your-own-github-app"
    68|             )
    69|         installation = installation[0]
    70|         g = installation.get_github_for_installation()
    71|         repo = g.get_repo(github_repository)
    72|         github_base_branch = get_from_dict_or_env(
    73|             values,
    74|             "github_base_branch",
    75|             "GITHUB_BASE_BRANCH",
    76|             default=repo.default_branch,
    77|         )
    78|         active_branch = get_from_dict_or_env(
    79|             values,
    80|             "active_branch",
    81|             "ACTIVE_BRANCH",
    82|             default=repo.default_branch,
    83|         )
    84|         values["github"] = g
    85|         values["github_repo_instance"] = repo
    86|         values["github_repository"] = github_repository
    87|         values["github_app_id"] = github_app_id
    88|         values["github_app_private_key"] = github_app_private_key
    89|         values["active_branch"] = active_branch


# ====================================================================
# FILE: libs/community/langchain_community/vectorstores/jaguar.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-63 ---
     1| from __future__ import annotations
     2| import json
     3| import logging
     4| from typing import TYPE_CHECKING, Any, List, Optional, Tuple
     5| if TYPE_CHECKING:
     6|     from jaguardb_http_client.JaguarHttpClient import JaguarHttpClient
     7| from langchain_core.documents import Document
     8| from langchain_core.embeddings import Embeddings
     9| from langchain_core.vectorstores import VectorStore
    10| logger = logging.getLogger(__name__)
    11| class Jaguar(VectorStore):
    12|     """`Jaguar API` vector store.
    13|     See http://www.jaguardb.com
    14|     See http://github.com/fserv/jaguar-sdk
    15|     Example:
    16|        .. code-block:: python
    17|            from langchain.vectorstores import Jaguar
    18|            vectorstore = Jaguar(
    19|                pod = 'vdb',
    20|                store = 'mystore',
    21|                vector_index = 'v',
    22|                vector_type = 'cosine_fraction_float',
    23|                vector_dimension = 1536,
    24|                url='http://192.168.8.88:8080/fwww/',
    25|                embedding=openai_model
    26|            )
    27|     """
    28|     def __init__(
    29|         self,
    30|         pod: str,
    31|         store: str,
    32|         vector_index: str,
    33|         vector_type: str,
    34|         vector_dimension: int,
    35|         url: str,
    36|         embedding: Embeddings,
    37|     ):
    38|         self._pod = pod
    39|         self._store = store
    40|         self._vector_index = vector_index
    41|         self._vector_type = vector_type
    42|         self._vector_dimension = vector_dimension
    43|         self._embedding = embedding
    44|         self._jag = JaguarHttpClient(url)
    45|         self._token = ""
    46|     def login(
    47|         self,
    48|         jaguar_api_key: Optional[str] = "",
    49|     ) -> bool:
    50|         """
    51|         login to jaguardb server with a jaguar_api_key or let self._jag find a key
    52|         Args:
    53|             pod (str):  name of a Pod
    54|             store (str):  name of a vector store
    55|             optional jaguar_api_key (str): API key of user to jaguardb server
    56|         Returns:
    57|             True if successful; False if not successful
    58|         """
    59|         if jaguar_api_key == "":
    60|             jaguar_api_key = self._jag.getApiKey()
    61|         self._jaguar_api_key = jaguar_api_key
    62|         self._token = self._jag.login(jaguar_api_key)
    63|         if self._token == "":


# ====================================================================
# FILE: libs/community/langchain_community/vectorstores/momento_vector_index.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 240-281 ---
   240|         self,
   241|         embedding: List[float],
   242|         k: int = 4,
   243|         **kwargs: Any,
   244|     ) -> List[Tuple[Document, float]]:
   245|         """Search for similar documents to the query vector.
   246|         Args:
   247|             embedding (List[float]): The query vector to search for.
   248|             k (int, optional): The number of results to return. Defaults to 4.
   249|             kwargs (Any): Vector Store specific search parameters. The following are
   250|                 forwarded to the Momento Vector Index:
   251|             - top_k (int, optional): The number of results to return.
   252|         Returns:
   253|             List[Tuple[Document, float]]: A list of tuples of the form
   254|                 (Document, score).
   255|         """
   256|         from momento.requests.vector_index import ALL_METADATA
   257|         from momento.responses.vector_index import Search
   258|         if "top_k" in kwargs:
   259|             k = kwargs["k"]
   260|         response = self._client.search(
   261|             self.index_name, embedding, top_k=k, metadata_fields=ALL_METADATA
   262|         )
   263|         if not isinstance(response, Search.Success):
   264|             return []
   265|         results = []
   266|         for hit in response.hits:
   267|             text = cast(str, hit.metadata.pop(self.text_field))
   268|             doc = Document(page_content=text, metadata=hit.metadata)
   269|             pair = (doc, hit.score)
   270|             results.append(pair)
   271|         return results
   272|     def similarity_search_by_vector(
   273|         self, embedding: List[float], k: int = 4, **kwargs: Any
   274|     ) -> List[Document]:
   275|         """Search for similar documents to the query vector.
   276|         Args:
   277|             embedding (List[float]): The query vector to search for.
   278|             k (int, optional): The number of results to return. Defaults to 4.
   279|         Returns:
   280|             List[Document]: A list of documents that are similar to the query.
   281|         """

# --- HUNK 2: Lines 290-331 ---
   290|         fetch_k: int = 20,
   291|         lambda_mult: float = 0.5,
   292|         **kwargs: Any,
   293|     ) -> List[Document]:
   294|         """Return docs selected using the maximal marginal relevance.
   295|         Maximal marginal relevance optimizes for similarity to query AND diversity
   296|         among selected documents.
   297|         Args:
   298|             embedding: Embedding to look up documents similar to.
   299|             k: Number of Documents to return. Defaults to 4.
   300|             fetch_k: Number of Documents to fetch to pass to MMR algorithm.
   301|             lambda_mult: Number between 0 and 1 that determines the degree
   302|                         of diversity among the results with 0 corresponding
   303|                         to maximum diversity and 1 to minimum diversity.
   304|                         Defaults to 0.5.
   305|         Returns:
   306|             List of Documents selected by maximal marginal relevance.
   307|         """
   308|         from momento.requests.vector_index import ALL_METADATA
   309|         from momento.responses.vector_index import SearchAndFetchVectors
   310|         response = self._client.search_and_fetch_vectors(
   311|             self.index_name, embedding, top_k=fetch_k, metadata_fields=ALL_METADATA
   312|         )
   313|         if isinstance(response, SearchAndFetchVectors.Success):
   314|             pass
   315|         elif isinstance(response, SearchAndFetchVectors.Error):
   316|             logger.error(f"Error searching and fetching vectors: {response}")
   317|             return []
   318|         else:
   319|             logger.error(f"Unexpected response: {response}")
   320|             raise Exception(f"Unexpected response: {response}")
   321|         mmr_selected = maximal_marginal_relevance(
   322|             query_embedding=np.array([embedding], dtype=np.float32),
   323|             embedding_list=[hit.vector for hit in response.hits],
   324|             lambda_mult=lambda_mult,
   325|             k=k,
   326|         )
   327|         selected = [response.hits[i].metadata for i in mmr_selected]
   328|         return [
   329|             Document(page_content=metadata.pop(self.text_field, ""), metadata=metadata)  # type: ignore  # noqa: E501
   330|             for metadata in selected
   331|         ]


# ====================================================================
# FILE: libs/community/langchain_community/vectorstores/pgvector.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 138-248 ---
   138|         embedding_function: Embeddings,
   139|         collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,
   140|         collection_metadata: Optional[dict] = None,
   141|         distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,
   142|         pre_delete_collection: bool = False,
   143|         logger: Optional[logging.Logger] = None,
   144|         relevance_score_fn: Optional[Callable[[float], float]] = None,
   145|         *,
   146|         connection: Optional[sqlalchemy.engine.Connection] = None,
   147|         engine_args: Optional[dict[str, Any]] = None,
   148|     ) -> None:
   149|         self.connection_string = connection_string
   150|         self.embedding_function = embedding_function
   151|         self.collection_name = collection_name
   152|         self.collection_metadata = collection_metadata
   153|         self._distance_strategy = distance_strategy
   154|         self.pre_delete_collection = pre_delete_collection
   155|         self.logger = logger or logging.getLogger(__name__)
   156|         self.override_relevance_score_fn = relevance_score_fn
   157|         self.engine_args = engine_args or {}
   158|         self._conn = connection if connection else self.connect()
   159|         self.__post_init__()
   160|     def __post_init__(
   161|         self,
   162|     ) -> None:
   163|         """Initialize the store."""
   164|         self.create_vector_extension()
   165|         EmbeddingStore, CollectionStore = _get_embedding_collection_store()
   166|         self.CollectionStore = CollectionStore
   167|         self.EmbeddingStore = EmbeddingStore
   168|         self.create_tables_if_not_exists()
   169|         self.create_collection()
   170|     def __del__(self) -> None:
   171|         if self._conn:
   172|             self._conn.close()
   173|     @property
   174|     def embeddings(self) -> Embeddings:
   175|         return self.embedding_function
   176|     def connect(self) -> sqlalchemy.engine.Connection:
   177|         engine = sqlalchemy.create_engine(self.connection_string, **self.engine_args)
   178|         conn = engine.connect()
   179|         return conn
   180|     def create_vector_extension(self) -> None:
   181|         try:
   182|             with Session(self._conn) as session:
   183|                 statement = sqlalchemy.text(
   184|                     "BEGIN;"
   185|                     "SELECT pg_advisory_xact_lock(1573678846307946496);"
   186|                     "CREATE EXTENSION IF NOT EXISTS vector;"
   187|                     "COMMIT;"
   188|                 )
   189|                 session.execute(statement)
   190|                 session.commit()
   191|         except Exception as e:
   192|             raise Exception(f"Failed to create vector extension: {e}") from e
   193|     def create_tables_if_not_exists(self) -> None:
   194|         with self._conn.begin():
   195|             Base.metadata.create_all(self._conn)
   196|     def drop_tables(self) -> None:
   197|         with self._conn.begin():
   198|             Base.metadata.drop_all(self._conn)
   199|     def create_collection(self) -> None:
   200|         if self.pre_delete_collection:
   201|             self.delete_collection()
   202|         with Session(self._conn) as session:
   203|             self.CollectionStore.get_or_create(
   204|                 session, self.collection_name, cmetadata=self.collection_metadata
   205|             )
   206|     def delete_collection(self) -> None:
   207|         self.logger.debug("Trying to delete collection")
   208|         with Session(self._conn) as session:
   209|             collection = self.get_collection(session)
   210|             if not collection:
   211|                 self.logger.warning("Collection not found")
   212|                 return
   213|             session.delete(collection)
   214|             session.commit()
   215|     @contextlib.contextmanager
   216|     def _make_session(self) -> Generator[Session, None, None]:
   217|         """Create a context manager for the session, bind to _conn string."""
   218|         yield Session(self._conn)
   219|     def delete(
   220|         self,
   221|         ids: Optional[List[str]] = None,
   222|         **kwargs: Any,
   223|     ) -> None:
   224|         """Delete vectors by ids or uuids.
   225|         Args:
   226|             ids: List of ids to delete.
   227|         """
   228|         with Session(self._conn) as session:
   229|             if ids is not None:
   230|                 self.logger.debug(
   231|                     "Trying to delete vectors by ids (represented by the model "
   232|                     "using the custom ids field)"
   233|                 )
   234|                 stmt = delete(self.EmbeddingStore).where(
   235|                     self.EmbeddingStore.custom_id.in_(ids)
   236|                 )
   237|                 session.execute(stmt)
   238|             session.commit()
   239|     def get_collection(self, session: Session) -> Any:
   240|         return self.CollectionStore.get_by_name(session, self.collection_name)
   241|     @classmethod
   242|     def __from(
   243|         cls,
   244|         texts: List[str],
   245|         embeddings: List[List[float]],
   246|         embedding: Embeddings,
   247|         metadatas: Optional[List[dict]] = None,
   248|         ids: Optional[List[str]] = None,

# --- HUNK 2: Lines 272-312 ---
   272|         return store
   273|     def add_embeddings(
   274|         self,
   275|         texts: Iterable[str],
   276|         embeddings: List[List[float]],
   277|         metadatas: Optional[List[dict]] = None,
   278|         ids: Optional[List[str]] = None,
   279|         **kwargs: Any,
   280|     ) -> List[str]:
   281|         """Add embeddings to the vectorstore.
   282|         Args:
   283|             texts: Iterable of strings to add to the vectorstore.
   284|             embeddings: List of list of embedding vectors.
   285|             metadatas: List of metadatas associated with the texts.
   286|             kwargs: vectorstore specific parameters
   287|         """
   288|         if ids is None:
   289|             ids = [str(uuid.uuid1()) for _ in texts]
   290|         if not metadatas:
   291|             metadatas = [{} for _ in texts]
   292|         with Session(self._conn) as session:
   293|             collection = self.get_collection(session)
   294|             if not collection:
   295|                 raise ValueError("Collection not found")
   296|             for text, metadata, embedding, id in zip(texts, metadatas, embeddings, ids):
   297|                 embedding_store = self.EmbeddingStore(
   298|                     embedding=embedding,
   299|                     document=text,
   300|                     cmetadata=metadata,
   301|                     custom_id=id,
   302|                     collection_id=collection.uuid,
   303|                 )
   304|                 session.add(embedding_store)
   305|             session.commit()
   306|         return ids
   307|     def add_texts(
   308|         self,
   309|         texts: Iterable[str],
   310|         metadatas: Optional[List[dict]] = None,
   311|         ids: Optional[List[str]] = None,
   312|         **kwargs: Any,

# --- HUNK 3: Lines 387-427 ---
   387|     def _results_to_docs_and_scores(self, results: Any) -> List[Tuple[Document, float]]:
   388|         """Return docs and scores from results."""
   389|         docs = [
   390|             (
   391|                 Document(
   392|                     page_content=result.EmbeddingStore.document,
   393|                     metadata=result.EmbeddingStore.cmetadata,
   394|                 ),
   395|                 result.distance if self.embedding_function is not None else None,
   396|             )
   397|             for result in results
   398|         ]
   399|         return docs
   400|     def __query_collection(
   401|         self,
   402|         embedding: List[float],
   403|         k: int = 4,
   404|         filter: Optional[Dict[str, str]] = None,
   405|     ) -> List[Any]:
   406|         """Query the collection."""
   407|         with Session(self._conn) as session:
   408|             collection = self.get_collection(session)
   409|             if not collection:
   410|                 raise ValueError("Collection not found")
   411|             filter_by = self.EmbeddingStore.collection_id == collection.uuid
   412|             if filter is not None:
   413|                 filter_clauses = []
   414|                 IN, NIN = "in", "nin"
   415|                 for key, value in filter.items():
   416|                     if isinstance(value, dict):
   417|                         value_case_insensitive = {
   418|                             k.lower(): v for k, v in value.items()
   419|                         }
   420|                         if IN in map(str.lower, value):
   421|                             filter_by_metadata = self.EmbeddingStore.cmetadata[
   422|                                 key
   423|                             ].astext.in_(value_case_insensitive[IN])
   424|                         elif NIN in map(str.lower, value):
   425|                             filter_by_metadata = self.EmbeddingStore.cmetadata[
   426|                                 key
   427|                             ].astext.not_in(value_case_insensitive[NIN])


# ====================================================================
# FILE: libs/community/langchain_community/vectorstores/semadb.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 8-48 ---
     8| from langchain_core.vectorstores import VectorStore
     9| from langchain_community.vectorstores.utils import DistanceStrategy
    10| class SemaDB(VectorStore):
    11|     """`SemaDB` vector store.
    12|     This vector store is a wrapper around the SemaDB database.
    13|     Example:
    14|         .. code-block:: python
    15|             from langchain_community.vectorstores import SemaDB
    16|             db = SemaDB('mycollection', 768, embeddings, DistanceStrategy.COSINE)
    17|     """
    18|     HOST = "semadb.p.rapidapi.com"
    19|     BASE_URL = "https://" + HOST
    20|     def __init__(
    21|         self,
    22|         collection_name: str,
    23|         vector_size: int,
    24|         embedding: Embeddings,
    25|         distance_strategy: DistanceStrategy = DistanceStrategy.EUCLIDEAN_DISTANCE,
    26|         api_key: str = "",
    27|     ):
    28|         """Initialise the SemaDB vector store."""
    29|         self.collection_name = collection_name
    30|         self.vector_size = vector_size
    31|         self.api_key = api_key or get_from_env("api_key", "SEMADB_API_KEY")
    32|         self._embedding = embedding
    33|         self.distance_strategy = distance_strategy
    34|     @property
    35|     def headers(self) -> dict:
    36|         """Return the common headers."""
    37|         return {
    38|             "content-type": "application/json",
    39|             "X-RapidAPI-Key": self.api_key,
    40|             "X-RapidAPI-Host": SemaDB.HOST,
    41|         }
    42|     def _get_internal_distance_strategy(self) -> str:
    43|         """Return the internal distance strategy."""
    44|         if self.distance_strategy == DistanceStrategy.EUCLIDEAN_DISTANCE:
    45|             return "euclidean"
    46|         elif self.distance_strategy == DistanceStrategy.MAX_INNER_PRODUCT:
    47|             raise ValueError("Max inner product is not supported by SemaDB")
    48|         elif self.distance_strategy == DistanceStrategy.DOT_PRODUCT:


# ====================================================================
# FILE: libs/community/langchain_community/vectorstores/surrealdb.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 33-125 ---
    33|             db_user = "root"
    34|             db_pass = "root"
    35|             sdb = SurrealDBStore.from_texts(
    36|                     texts=texts,
    37|                     embedding=embedding_function,
    38|                     dburl,
    39|                     ns, db, collection,
    40|                     db_user=db_user, db_pass=db_pass)
    41|     """
    42|     def __init__(
    43|         self,
    44|         embedding_function: Embeddings,
    45|         **kwargs: Any,
    46|     ) -> None:
    47|         from surrealdb import Surreal
    48|         self.collection = kwargs.pop("collection", "documents")
    49|         self.ns = kwargs.pop("ns", "langchain")
    50|         self.db = kwargs.pop("db", "database")
    51|         self.dburl = kwargs.pop("dburl", "ws://localhost:8000/rpc")
    52|         self.embedding_function = embedding_function
    53|         self.sdb = Surreal()
    54|         self.kwargs = kwargs
    55|     async def initialize(self) -> None:
    56|         """
    57|         Initialize connection to surrealdb database
    58|         and authenticate if credentials are provided
    59|         """
    60|         await self.sdb.connect(self.dburl)
    61|         if "db_user" in self.kwargs and "db_pass" in self.kwargs:
    62|             user = self.kwargs.get("db_user")
    63|             password = self.kwargs.get("db_pass")
    64|             await self.sdb.signin({"user": user, "pass": password})
    65|         await self.sdb.use(self.ns, self.db)
    66|     @property
    67|     def embeddings(self) -> Optional[Embeddings]:
    68|         return (
    69|             self.embedding_function
    70|             if isinstance(self.embedding_function, Embeddings)
    71|             else None
    72|         )
    73|     async def aadd_texts(
    74|         self,
    75|         texts: Iterable[str],
    76|         metadatas: Optional[List[dict]] = None,
    77|         **kwargs: Any,
    78|     ) -> List[str]:
    79|         """Add list of text along with embeddings to the vector store asynchronously
    80|         Args:
    81|             texts (Iterable[str]): collection of text to add to the database
    82|         Returns:
    83|             List of ids for the newly inserted documents
    84|         """
    85|         embeddings = self.embedding_function.embed_documents(list(texts))
    86|         ids = []
    87|         for idx, text in enumerate(texts):
    88|             record = await self.sdb.create(
    89|                 self.collection, {"text": text, "embedding": embeddings[idx]}
    90|             )
    91|             ids.append(record[0]["id"])
    92|         return ids
    93|     def add_texts(
    94|         self,
    95|         texts: Iterable[str],
    96|         metadatas: Optional[List[dict]] = None,
    97|         **kwargs: Any,
    98|     ) -> List[str]:
    99|         """Add list of text along with embeddings to the vector store
   100|         Args:
   101|             texts (Iterable[str]): collection of text to add to the database
   102|         Returns:
   103|             List of ids for the newly inserted documents
   104|         """
   105|         return asyncio.run(self.aadd_texts(texts, metadatas, **kwargs))
   106|     async def adelete(
   107|         self,
   108|         ids: Optional[List[str]] = None,
   109|         **kwargs: Any,
   110|     ) -> Optional[bool]:
   111|         """Delete by document ID asynchronously.
   112|         Args:
   113|             ids: List of ids to delete.
   114|             **kwargs: Other keyword arguments that subclasses might use.
   115|         Returns:
   116|             Optional[bool]: True if deletion is successful,
   117|             False otherwise.
   118|         """
   119|         if ids is None:
   120|             await self.sdb.delete(self.collection)
   121|             return True
   122|         else:
   123|             if isinstance(ids, str):
   124|                 await self.sdb.delete(ids)
   125|                 return True

# --- HUNK 2: Lines 145-196 ---
   145|             await self.initialize()
   146|             return await self.adelete(ids=ids, **kwargs)
   147|         return asyncio.run(_delete(ids, **kwargs))
   148|     async def _asimilarity_search_by_vector_with_score(
   149|         self, embedding: List[float], k: int = 4, **kwargs: Any
   150|     ) -> List[Tuple[Document, float]]:
   151|         """Run similarity search for query embedding asynchronously
   152|         and return documents and scores
   153|         Args:
   154|             embedding (List[float]): Query embedding.
   155|             k (int): Number of results to return. Defaults to 4.
   156|         Returns:
   157|             List of Documents most similar along with scores
   158|         """
   159|         args = {
   160|             "collection": self.collection,
   161|             "embedding": embedding,
   162|             "k": k,
   163|             "score_threshold": kwargs.get("score_threshold", 0),
   164|         }
   165|         query = """select id, text,
   166|         vector::similarity::cosine(embedding,{embedding}) as similarity
   167|         from {collection}
   168|         where vector::similarity::cosine(embedding,{embedding}) >= {score_threshold}
   169|         order by similarity desc LIMIT {k}
   170|         """.format(**args)
   171|         results = await self.sdb.query(query)
   172|         if len(results) == 0:
   173|             return []
   174|         return [
   175|             (
   176|                 Document(page_content=result["text"], metadata={"id": result["id"]}),
   177|                 result["similarity"],
   178|             )
   179|             for result in results[0]["result"]
   180|         ]
   181|     async def asimilarity_search_with_relevance_scores(
   182|         self, query: str, k: int = 4, **kwargs: Any
   183|     ) -> List[Tuple[Document, float]]:
   184|         """Run similarity search asynchronously and return relevance scores
   185|         Args:
   186|             query (str): Query
   187|             k (int): Number of results to return. Defaults to 4.
   188|         Returns:
   189|             List of Documents most similar along with relevance scores
   190|         """
   191|         query_embedding = self.embedding_function.embed_query(query)
   192|         return [
   193|             (document, similarity)
   194|             for document, similarity in (
   195|                 await self._asimilarity_search_by_vector_with_score(
   196|                     query_embedding, k, **kwargs

# --- HUNK 3: Lines 312-352 ---
   312|         metadatas: Optional[List[dict]] = None,
   313|         **kwargs: Any,
   314|     ) -> "SurrealDBStore":
   315|         """Create SurrealDBStore from list of text asynchronously
   316|         Args:
   317|             texts (List[str]): list of text to vectorize and store
   318|             embedding (Optional[Embeddings]): Embedding function.
   319|             dburl (str): SurrealDB connection url
   320|                 (default: "ws://localhost:8000/rpc")
   321|             ns (str): surrealdb namespace for the vector store.
   322|                 (default: "langchain")
   323|             db (str): surrealdb database for the vector store.
   324|                 (default: "database")
   325|             collection (str): surrealdb collection for the vector store.
   326|                 (default: "documents")
   327|             (optional) db_user and db_pass: surrealdb credentials
   328|         Returns:
   329|             SurrealDBStore object initialized and ready for use."""
   330|         sdb = cls(embedding, **kwargs)
   331|         await sdb.initialize()
   332|         await sdb.aadd_texts(texts)
   333|         return sdb
   334|     @classmethod
   335|     def from_texts(
   336|         cls,
   337|         texts: List[str],
   338|         embedding: Embeddings,
   339|         metadatas: Optional[List[dict]] = None,
   340|         **kwargs: Any,
   341|     ) -> "SurrealDBStore":
   342|         """Create SurrealDBStore from list of text
   343|         Args:
   344|             texts (List[str]): list of text to vectorize and store
   345|             embedding (Optional[Embeddings]): Embedding function.
   346|             dburl (str): SurrealDB connection url
   347|             ns (str): surrealdb namespace for the vector store.
   348|                 (default: "langchain")
   349|             db (str): surrealdb database for the vector store.
   350|                 (default: "database")
   351|             collection (str): surrealdb collection for the vector store.
   352|                 (default: "documents")


# ====================================================================
# FILE: libs/community/langchain_community/vectorstores/vectara.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-38 ---
     1| from __future__ import annotations
     2| import json
     3| import logging
     4| import os
     5| from hashlib import md5
     6| from typing import Any, Iterable, List, Optional, Tuple, Type
     7| import requests
     8| from langchain_core.documents import Document
     9| from langchain_core.embeddings import Embeddings
    10| from langchain_core.pydantic_v1 import Field
    11| from langchain_core.vectorstores import VectorStore, VectorStoreRetriever
    12| logger = logging.getLogger(__name__)
    13| class Vectara(VectorStore):
    14|     """`Vectara API` vector store.
    15|      See (https://vectara.com).
    16|     Example:
    17|         .. code-block:: python
    18|             from langchain_community.vectorstores import Vectara
    19|             vectorstore = Vectara(
    20|                 vectara_customer_id=vectara_customer_id,
    21|                 vectara_corpus_id=vectara_corpus_id,
    22|                 vectara_api_key=vectara_api_key
    23|             )
    24|     """
    25|     def __init__(
    26|         self,
    27|         vectara_customer_id: Optional[str] = None,
    28|         vectara_corpus_id: Optional[str] = None,
    29|         vectara_api_key: Optional[str] = None,
    30|         vectara_api_timeout: int = 120,
    31|         source: str = "langchain",
    32|     ):
    33|         """Initialize with Vectara API."""
    34|         self._vectara_customer_id = vectara_customer_id or os.environ.get(
    35|             "VECTARA_CUSTOMER_ID"
    36|         )
    37|         self._vectara_corpus_id = vectara_corpus_id or os.environ.get(
    38|             "VECTARA_CORPUS_ID"

# --- HUNK 2: Lines 77-124 ---
    77|         body = {
    78|             "customer_id": self._vectara_customer_id,
    79|             "corpus_id": self._vectara_corpus_id,
    80|             "document_id": doc_id,
    81|         }
    82|         response = self._session.post(
    83|             "https://api.vectara.io/v1/delete-doc",
    84|             data=json.dumps(body),
    85|             verify=True,
    86|             headers=self._get_post_headers(),
    87|             timeout=self.vectara_api_timeout,
    88|         )
    89|         if response.status_code != 200:
    90|             logger.error(
    91|                 f"Delete request failed for doc_id = {doc_id} with status code "
    92|                 f"{response.status_code}, reason {response.reason}, text "
    93|                 f"{response.text}"
    94|             )
    95|             return False
    96|         return True
    97|     def _index_doc(self, doc: dict) -> str:
    98|         request: dict[str, Any] = {}
    99|         request["customer_id"] = self._vectara_customer_id
   100|         request["corpus_id"] = self._vectara_corpus_id
   101|         request["document"] = doc
   102|         response = self._session.post(
   103|             headers=self._get_post_headers(),
   104|             url="https://api.vectara.io/v1/index",
   105|             data=json.dumps(request),
   106|             timeout=self.vectara_api_timeout,
   107|             verify=True,
   108|         )
   109|         status_code = response.status_code
   110|         result = response.json()
   111|         status_str = result["status"]["code"] if "status" in result else None
   112|         if status_code == 409 or status_str and (status_str == "ALREADY_EXISTS"):
   113|             return "E_ALREADY_EXISTS"
   114|         elif status_str and (status_str == "FORBIDDEN"):
   115|             return "E_NO_PERMISSIONS"
   116|         else:
   117|             return "E_SUCCEEDED"
   118|     def add_files(
   119|         self,
   120|         files_list: Iterable[str],
   121|         metadatas: Optional[List[dict]] = None,
   122|         **kwargs: Any,
   123|     ) -> List[str]:
   124|         """

# --- HUNK 3: Lines 175-406 ---
   175|             texts: Iterable of strings to add to the vectorstore.
   176|             metadatas: Optional list of metadatas associated with the texts.
   177|             doc_metadata: optional metadata for the document
   178|         This function indexes all the input text strings in the Vectara corpus as a
   179|         single Vectara document, where each input text is considered a "section" and the
   180|         metadata are associated with each section.
   181|         if 'doc_metadata' is provided, it is associated with the Vectara document.
   182|         Returns:
   183|             document ID of the document added
   184|         """
   185|         doc_hash = md5()
   186|         for t in texts:
   187|             doc_hash.update(t.encode())
   188|         doc_id = doc_hash.hexdigest()
   189|         if metadatas is None:
   190|             metadatas = [{} for _ in texts]
   191|         if doc_metadata:
   192|             doc_metadata["source"] = "langchain"
   193|         else:
   194|             doc_metadata = {"source": "langchain"}
   195|         doc = {
   196|             "document_id": doc_id,
   197|             "metadataJson": json.dumps(doc_metadata),
   198|             "section": [
   199|                 {"text": text, "metadataJson": json.dumps(md)}
   200|                 for text, md in zip(texts, metadatas)
   201|             ],
   202|         }
   203|         success_str = self._index_doc(doc)
   204|         if success_str == "E_ALREADY_EXISTS":
   205|             self._delete_doc(doc_id)
   206|             self._index_doc(doc)
   207|         elif success_str == "E_NO_PERMISSIONS":
   208|             print(
   209|                 """No permissions to add document to Vectara. 
   210|                 Check your corpus ID, customer ID and API key"""
   211|             )
   212|         return [doc_id]
   213|     def similarity_search_with_score(
   214|         self,
   215|         query: str,
   216|         k: int = 5,
   217|         lambda_val: float = 0.025,
   218|         filter: Optional[str] = None,
   219|         score_threshold: Optional[float] = None,
   220|         n_sentence_context: int = 2,
   221|         **kwargs: Any,
   222|     ) -> List[Tuple[Document, float]]:
   223|         """Return Vectara documents most similar to query, along with scores.
   224|         Args:
   225|             query: Text to look up documents similar to.
   226|             k: Number of Documents to return. Defaults to 5.
   227|             lambda_val: lexical match parameter for hybrid search.
   228|             filter: Dictionary of argument(s) to filter on metadata. For example a
   229|                 filter can be "doc.rating > 3.0 and part.lang = 'deu'"} see
   230|                 https://docs.vectara.com/docs/search-apis/sql/filter-overview
   231|                 for more details.
   232|             score_threshold: minimal score threshold for the result.
   233|                 If defined, results with score less than this value will be
   234|                 filtered out.
   235|             n_sentence_context: number of sentences before/after the matching segment
   236|                 to add, defaults to 2
   237|         Returns:
   238|             List of Documents most similar to the query and score for each.
   239|         """
   240|         data = json.dumps(
   241|             {
   242|                 "query": [
   243|                     {
   244|                         "query": query,
   245|                         "start": 0,
   246|                         "num_results": k,
   247|                         "context_config": {
   248|                             "sentences_before": n_sentence_context,
   249|                             "sentences_after": n_sentence_context,
   250|                         },
   251|                         "corpus_key": [
   252|                             {
   253|                                 "customer_id": self._vectara_customer_id,
   254|                                 "corpus_id": self._vectara_corpus_id,
   255|                                 "metadataFilter": filter,
   256|                                 "lexical_interpolation_config": {"lambda": lambda_val},
   257|                             }
   258|                         ],
   259|                     }
   260|                 ]
   261|             }
   262|         )
   263|         response = self._session.post(
   264|             headers=self._get_post_headers(),
   265|             url="https://api.vectara.io/v1/query",
   266|             data=data,
   267|             timeout=self.vectara_api_timeout,
   268|         )
   269|         if response.status_code != 200:
   270|             logger.error(
   271|                 "Query failed %s",
   272|                 f"(code {response.status_code}, reason {response.reason}, details "
   273|                 f"{response.text})",
   274|             )
   275|             return []
   276|         result = response.json()
   277|         if score_threshold:
   278|             responses = [
   279|                 r
   280|                 for r in result["responseSet"][0]["response"]
   281|                 if r["score"] > score_threshold
   282|             ]
   283|         else:
   284|             responses = result["responseSet"][0]["response"]
   285|         documents = result["responseSet"][0]["document"]
   286|         metadatas = []
   287|         for x in responses:
   288|             md = {m["name"]: m["value"] for m in x["metadata"]}
   289|             doc_num = x["documentIndex"]
   290|             doc_md = {m["name"]: m["value"] for m in documents[doc_num]["metadata"]}
   291|             md.update(doc_md)
   292|             metadatas.append(md)
   293|         docs_with_score = [
   294|             (
   295|                 Document(
   296|                     page_content=x["text"],
   297|                     metadata=md,
   298|                 ),
   299|                 x["score"],
   300|             )
   301|             for x, md in zip(responses, metadatas)
   302|         ]
   303|         return docs_with_score
   304|     def similarity_search(
   305|         self,
   306|         query: str,
   307|         k: int = 5,
   308|         lambda_val: float = 0.025,
   309|         filter: Optional[str] = None,
   310|         n_sentence_context: int = 2,
   311|         **kwargs: Any,
   312|     ) -> List[Document]:
   313|         """Return Vectara documents most similar to query, along with scores.
   314|         Args:
   315|             query: Text to look up documents similar to.
   316|             k: Number of Documents to return. Defaults to 5.
   317|             filter: Dictionary of argument(s) to filter on metadata. For example a
   318|                 filter can be "doc.rating > 3.0 and part.lang = 'deu'"} see
   319|                 https://docs.vectara.com/docs/search-apis/sql/filter-overview for more
   320|                 details.
   321|             n_sentence_context: number of sentences before/after the matching segment
   322|                 to add, defaults to 2
   323|         Returns:
   324|             List of Documents most similar to the query
   325|         """
   326|         docs_and_scores = self.similarity_search_with_score(
   327|             query,
   328|             k=k,
   329|             lambda_val=lambda_val,
   330|             filter=filter,
   331|             score_threshold=None,
   332|             n_sentence_context=n_sentence_context,
   333|             **kwargs,
   334|         )
   335|         return [doc for doc, _ in docs_and_scores]
   336|     @classmethod
   337|     def from_texts(
   338|         cls: Type[Vectara],
   339|         texts: List[str],
   340|         embedding: Optional[Embeddings] = None,
   341|         metadatas: Optional[List[dict]] = None,
   342|         **kwargs: Any,
   343|     ) -> Vectara:
   344|         """Construct Vectara wrapper from raw documents.
   345|         This is intended to be a quick way to get started.
   346|         Example:
   347|             .. code-block:: python
   348|                 from langchain_community.vectorstores import Vectara
   349|                 vectara = Vectara.from_texts(
   350|                     texts,
   351|                     vectara_customer_id=customer_id,
   352|                     vectara_corpus_id=corpus_id,
   353|                     vectara_api_key=api_key,
   354|                 )
   355|         """
   356|         doc_metadata = kwargs.pop("doc_metadata", {})
   357|         vectara = cls(**kwargs)
   358|         vectara.add_texts(texts, metadatas, doc_metadata=doc_metadata, **kwargs)
   359|         return vectara
   360|     @classmethod
   361|     def from_files(
   362|         cls: Type[Vectara],
   363|         files: List[str],
   364|         embedding: Optional[Embeddings] = None,
   365|         metadatas: Optional[List[dict]] = None,
   366|         **kwargs: Any,
   367|     ) -> Vectara:
   368|         """Construct Vectara wrapper from raw documents.
   369|         This is intended to be a quick way to get started.
   370|         Example:
   371|             .. code-block:: python
   372|                 from langchain_community.vectorstores import Vectara
   373|                 vectara = Vectara.from_files(
   374|                     files_list,
   375|                     vectara_customer_id=customer_id,
   376|                     vectara_corpus_id=corpus_id,
   377|                     vectara_api_key=api_key,
   378|                 )
   379|         """
   380|         vectara = cls(**kwargs)
   381|         vectara.add_files(files, metadatas)
   382|         return vectara
   383|     def as_retriever(self, **kwargs: Any) -> VectaraRetriever:
   384|         tags = kwargs.pop("tags", None) or []
   385|         tags.extend(self._get_retriever_tags())
   386|         return VectaraRetriever(vectorstore=self, search_kwargs=kwargs, tags=tags)
   387| class VectaraRetriever(VectorStoreRetriever):
   388|     """Retriever class for `Vectara`."""
   389|     vectorstore: Vectara
   390|     """Vectara vectorstore."""
   391|     search_kwargs: dict = Field(
   392|         default_factory=lambda: {
   393|             "lambda_val": 0.0,
   394|             "k": 5,
   395|             "filter": "",
   396|             "n_sentence_context": "2",
   397|         }
   398|     )
   399|     """Search params.
   400|         k: Number of Documents to return. Defaults to 5.
   401|         lambda_val: lexical match parameter for hybrid search.
   402|         filter: Dictionary of argument(s) to filter on metadata. For example a
   403|             filter can be "doc.rating > 3.0 and part.lang = 'deu'"} see
   404|             https://docs.vectara.com/docs/search-apis/sql/filter-overview
   405|             for more details.
   406|         n_sentence_context: number of sentences before/after the matching segment to add


# ====================================================================
# FILE: libs/core/langchain_core/beta/runnables/context.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 1-34 ---
     1| import asyncio
     2| import threading
     3| from collections import defaultdict
     4| from functools import partial
     5| from itertools import groupby
     6| from typing import (
     7|     Any,
     8|     Awaitable,
     9|     Callable,
    10|     DefaultDict,
    11|     Dict,
    12|     List,
    13|     Mapping,
    14|     Optional,
    15|     Type,
    16|     TypeVar,
    17|     Union,
    18| )
    19| from langchain_core.runnables.base import (
    20|     Runnable,
    21|     RunnableSerializable,
    22|     coerce_to_runnable,
    23| )
    24| from langchain_core.runnables.config import RunnableConfig, patch_config
    25| from langchain_core.runnables.utils import ConfigurableFieldSpec, Input, Output
    26| T = TypeVar("T")
    27| Values = Dict[Union[asyncio.Event, threading.Event], Any]
    28| CONTEXT_CONFIG_PREFIX = "__context__/"
    29| CONTEXT_CONFIG_SUFFIX_GET = "/get"
    30| CONTEXT_CONFIG_SUFFIX_SET = "/set"
    31| async def _asetter(done: asyncio.Event, values: Values, value: T) -> T:
    32|     values[done] = value
    33|     done.set()
    34|     return value

# --- HUNK 2: Lines 74-153 ---
    74|     }
    75|     deps_by_key = {
    76|         key: set(
    77|             _key_from_id(dep) for spec in group for dep in (spec[0].dependencies or [])
    78|         )
    79|         for key, group in grouped_by_key.items()
    80|     }
    81|     values: Values = {}
    82|     events: DefaultDict[str, Union[asyncio.Event, threading.Event]] = defaultdict(
    83|         event_cls
    84|     )
    85|     context_funcs: Dict[str, Callable[[], Any]] = {}
    86|     for key, group in grouped_by_key.items():
    87|         getters = [s for s in group if s[0].id.endswith(CONTEXT_CONFIG_SUFFIX_GET)]
    88|         setters = [s for s in group if s[0].id.endswith(CONTEXT_CONFIG_SUFFIX_SET)]
    89|         for dep in deps_by_key[key]:
    90|             if key in deps_by_key[dep]:
    91|                 raise ValueError(
    92|                     f"Deadlock detected between context keys {key} and {dep}"
    93|                 )
    94|         if len(getters) < 1:
    95|             raise ValueError(f"Expected at least one getter for context key {key}")
    96|         if len(setters) != 1:
    97|             raise ValueError(f"Expected exactly one setter for context key {key}")
    98|         setter_idx = setters[0][1]
    99|         if any(getter_idx < setter_idx for _, getter_idx in getters):
   100|             raise ValueError(
   101|                 f"Context setter for key {key} must be defined after all getters."
   102|             )
   103|         context_funcs[getters[0][0].id] = partial(getter, events[key], values)
   104|         context_funcs[setters[0][0].id] = partial(setter, events[key], values)
   105|     return patch_config(config, configurable=context_funcs)
   106| def aconfig_with_context(
   107|     config: RunnableConfig,
   108|     steps: List[Runnable],
   109| ) -> RunnableConfig:
   110|     """Asynchronously patch a runnable config with context getters and setters.
   111|     Args:
   112|         config: The runnable config.
   113|         steps: The runnable steps.
   114|     Returns:
   115|         The patched runnable config.
   116|     """
   117|     return _config_with_context(config, steps, _asetter, _agetter, asyncio.Event)
   118| def config_with_context(
   119|     config: RunnableConfig,
   120|     steps: List[Runnable],
   121| ) -> RunnableConfig:
   122|     """Patch a runnable config with context getters and setters.
   123|     Args:
   124|         config: The runnable config.
   125|         steps: The runnable steps.
   126|     Returns:
   127|         The patched runnable config.
   128|     """
   129|     return _config_with_context(config, steps, _setter, _getter, threading.Event)
   130| class ContextGet(RunnableSerializable):
   131|     """Get a context value."""
   132|     prefix: str = ""
   133|     key: Union[str, List[str]]
   134|     @property
   135|     def ids(self) -> List[str]:
   136|         prefix = self.prefix + "/" if self.prefix else ""
   137|         keys = self.key if isinstance(self.key, list) else [self.key]
   138|         return [
   139|             f"{CONTEXT_CONFIG_PREFIX}{prefix}{k}{CONTEXT_CONFIG_SUFFIX_GET}"
   140|             for k in keys
   141|         ]
   142|     @property
   143|     def config_specs(self) -> List[ConfigurableFieldSpec]:
   144|         return super().config_specs + [
   145|             ConfigurableFieldSpec(
   146|                 id=id_,
   147|                 annotation=Callable[[], Any],
   148|             )
   149|             for id_ in self.ids
   150|         ]
   151|     def invoke(self, input: Any, config: Optional[RunnableConfig] = None) -> Any:
   152|         config = config or {}
   153|         configurable = config.get("configurable", {})

# --- HUNK 3: Lines 180-219 ---
   180|     prefix: str = ""
   181|     keys: Mapping[str, Optional[Runnable]]
   182|     class Config:
   183|         arbitrary_types_allowed = True
   184|     def __init__(
   185|         self,
   186|         key: Optional[str] = None,
   187|         value: Optional[SetValue] = None,
   188|         prefix: str = "",
   189|         **kwargs: SetValue,
   190|     ):
   191|         if key is not None:
   192|             kwargs[key] = value
   193|         super().__init__(
   194|             keys={
   195|                 k: _coerce_set_value(v) if v is not None else None
   196|                 for k, v in kwargs.items()
   197|             },
   198|             prefix=prefix,
   199|         )
   200|     @property
   201|     def ids(self) -> List[str]:
   202|         prefix = self.prefix + "/" if self.prefix else ""
   203|         return [
   204|             f"{CONTEXT_CONFIG_PREFIX}{prefix}{key}{CONTEXT_CONFIG_SUFFIX_SET}"
   205|             for key in self.keys
   206|         ]
   207|     @property
   208|     def config_specs(self) -> List[ConfigurableFieldSpec]:
   209|         mapper_config_specs = [
   210|             s
   211|             for mapper in self.keys.values()
   212|             if mapper is not None
   213|             for s in mapper.config_specs
   214|         ]
   215|         for spec in mapper_config_specs:
   216|             if spec.id.endswith(CONTEXT_CONFIG_SUFFIX_GET):
   217|                 getter_key = spec.id.split("/")[1]
   218|                 if getter_key in self.keys:
   219|                     raise ValueError(

# --- HUNK 4: Lines 266-285 ---
   266|         _value: Optional[SetValue] = None,
   267|         /,
   268|         **kwargs: SetValue,
   269|     ) -> ContextSet:
   270|         return ContextSet(_key, _value, prefix="", **kwargs)
   271| class PrefixContext:
   272|     """Context for a runnable with a prefix."""
   273|     prefix: str = ""
   274|     def __init__(self, prefix: str = ""):
   275|         self.prefix = prefix
   276|     def getter(self, key: Union[str, List[str]], /) -> ContextGet:
   277|         return ContextGet(key=key, prefix=self.prefix)
   278|     def setter(
   279|         self,
   280|         _key: Optional[str] = None,
   281|         _value: Optional[SetValue] = None,
   282|         /,
   283|         **kwargs: SetValue,
   284|     ) -> ContextSet:
   285|         return ContextSet(_key, _value, prefix=self.prefix, **kwargs)


# ====================================================================
# FILE: libs/core/langchain_core/callbacks/manager.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-27 ---
     1| from __future__ import annotations
     2| import asyncio
     3| import functools
     4| import logging
     5| import uuid
     6| from concurrent.futures import ThreadPoolExecutor
     7| from contextlib import asynccontextmanager, contextmanager
     8| from typing import (
     9|     TYPE_CHECKING,
    10|     Any,
    11|     AsyncGenerator,
    12|     Coroutine,
    13|     Dict,
    14|     Generator,
    15|     List,
    16|     Optional,
    17|     Sequence,
    18|     Type,
    19|     TypeVar,
    20|     Union,
    21|     cast,
    22| )
    23| from uuid import UUID
    24| from langsmith.run_helpers import get_run_tree_context
    25| from tenacity import RetryCallState
    26| from langchain_core.callbacks.base import (
    27|     BaseCallbackHandler,

# --- HUNK 2: Lines 216-342 ---
   216|                     handler_name = handler.__class__.__name__
   217|                     logger.warning(
   218|                         f"NotImplementedError in {handler_name}.{event_name}"
   219|                         f" callback: {repr(e)}"
   220|                     )
   221|             except Exception as e:
   222|                 logger.warning(
   223|                     f"Error in {handler.__class__.__name__}.{event_name} callback:"
   224|                     f" {repr(e)}"
   225|                 )
   226|                 if handler.raise_error:
   227|                     raise e
   228|     finally:
   229|         if coros:
   230|             try:
   231|                 asyncio.get_running_loop()
   232|                 loop_running = True
   233|             except RuntimeError:
   234|                 loop_running = False
   235|             if loop_running:
   236|                 with ThreadPoolExecutor(1) as executor:
   237|                     executor.submit(_run_coros, coros).result()
   238|             else:
   239|                 _run_coros(coros)
   240| def _run_coros(coros: List[Coroutine[Any, Any, Any]]) -> None:
   241|     if hasattr(asyncio, "Runner"):
   242|         with asyncio.Runner() as runner:
   243|             for coro in coros:
   244|                 runner.run(coro)
   245|             while pending := asyncio.all_tasks(runner.get_loop()):
   246|                 runner.run(asyncio.wait(pending))
   247|     else:
   248|         for coro in coros:
   249|             asyncio.run(coro)
   250| async def _ahandle_event_for_handler(
   251|     handler: BaseCallbackHandler,
   252|     event_name: str,
   253|     ignore_condition_name: Optional[str],
   254|     *args: Any,
   255|     **kwargs: Any,
   256| ) -> None:
   257|     try:
   258|         if ignore_condition_name is None or not getattr(handler, ignore_condition_name):
   259|             event = getattr(handler, event_name)
   260|             if asyncio.iscoroutinefunction(event):
   261|                 await event(*args, **kwargs)
   262|             else:
   263|                 if handler.run_inline:
   264|                     event(*args, **kwargs)
   265|                 else:
   266|                     await asyncio.get_event_loop().run_in_executor(
   267|                         None, functools.partial(event, *args, **kwargs)
   268|                     )
   269|     except NotImplementedError as e:
   270|         if event_name == "on_chat_model_start":
   271|             message_strings = [get_buffer_string(m) for m in args[1]]
   272|             await _ahandle_event_for_handler(
   273|                 handler,
   274|                 "on_llm_start",
   275|                 "ignore_llm",
   276|                 args[0],
   277|                 message_strings,
   278|                 *args[2:],
   279|                 **kwargs,
   280|             )
   281|         else:
   282|             logger.warning(
   283|                 f"NotImplementedError in {handler.__class__.__name__}.{event_name}"
   284|                 f" callback: {repr(e)}"
   285|             )
   286|     except Exception as e:
   287|         logger.warning(
   288|             f"Error in {handler.__class__.__name__}.{event_name} callback:"
   289|             f" {repr(e)}"
   290|         )
   291|         if handler.raise_error:
   292|             raise e
   293| async def ahandle_event(
   294|     handlers: List[BaseCallbackHandler],
   295|     event_name: str,
   296|     ignore_condition_name: Optional[str],
   297|     *args: Any,
   298|     **kwargs: Any,
   299| ) -> None:
   300|     """Generic event handler for AsyncCallbackManager.
   301|     Note: This function is used by langserve to handle events.
   302|     Args:
   303|         handlers: The list of handlers that will handle the event
   304|         event_name: The name of the event (e.g., "on_llm_start")
   305|         ignore_condition_name: Name of the attribute defined on handler
   306|             that if True will cause the handler to be skipped for the given event
   307|         *args: The arguments to pass to the event handler
   308|         **kwargs: The keyword arguments to pass to the event handler
   309|     """
   310|     for handler in [h for h in handlers if h.run_inline]:
   311|         await _ahandle_event_for_handler(
   312|             handler, event_name, ignore_condition_name, *args, **kwargs
   313|         )
   314|     await asyncio.gather(
   315|         *(
   316|             _ahandle_event_for_handler(
   317|                 handler, event_name, ignore_condition_name, *args, **kwargs
   318|             )
   319|             for handler in handlers
   320|             if not handler.run_inline
   321|         )
   322|     )
   323| BRM = TypeVar("BRM", bound="BaseRunManager")
   324| class BaseRunManager(RunManagerMixin):
   325|     """Base class for run manager (a bound callback manager)."""
   326|     def __init__(
   327|         self,
   328|         *,
   329|         run_id: UUID,
   330|         handlers: List[BaseCallbackHandler],
   331|         inheritable_handlers: List[BaseCallbackHandler],
   332|         parent_run_id: Optional[UUID] = None,
   333|         tags: Optional[List[str]] = None,
   334|         inheritable_tags: Optional[List[str]] = None,
   335|         metadata: Optional[Dict[str, Any]] = None,
   336|         inheritable_metadata: Optional[Dict[str, Any]] = None,
   337|     ) -> None:
   338|         """Initialize the run manager.
   339|         Args:
   340|             run_id (UUID): The ID of the run.
   341|             handlers (List[BaseCallbackHandler]): The list of handlers.
   342|             inheritable_handlers (List[BaseCallbackHandler]):


# ====================================================================
# FILE: libs/core/langchain_core/chat_history.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-56 ---
     1| from __future__ import annotations
     2| from abc import ABC, abstractmethod
     3| from typing import List
     4| from langchain_core.messages import (
     5|     AIMessage,
     6|     BaseMessage,
     7|     HumanMessage,
     8|     get_buffer_string,
     9| )
    10| class BaseChatMessageHistory(ABC):
    11|     """Abstract base class for storing chat message history.
    12|     See `ChatMessageHistory` for default implementation.
    13|     Example:
    14|         .. code-block:: python
    15|             class FileChatMessageHistory(BaseChatMessageHistory):
    16|                 storage_path:  str
    17|                 session_id: str
    18|                @property
    19|                def messages(self):
    20|                    with open(os.path.join(storage_path, session_id), 'r:utf-8') as f:
    21|                        messages = json.loads(f.read())
    22|                     return messages_from_dict(messages)
    23|                def add_message(self, message: BaseMessage) -> None:
    24|                    messages = self.messages.append(_message_to_dict(message))
    25|                    with open(os.path.join(storage_path, session_id), 'w') as f:
    26|                        json.dump(f, messages)
    27|                def clear(self):
    28|                    with open(os.path.join(storage_path, session_id), 'w') as f:
    29|                        f.write("[]")
    30|     """
    31|     messages: List[BaseMessage]
    32|     """A list of Messages stored in-memory."""
    33|     def add_user_message(self, message: str) -> None:
    34|         """Convenience method for adding a human message string to the store.
    35|         Args:
    36|             message: The string contents of a human message.
    37|         """
    38|         self.add_message(HumanMessage(content=message))
    39|     def add_ai_message(self, message: str) -> None:
    40|         """Convenience method for adding an AI message string to the store.
    41|         Args:
    42|             message: The string contents of an AI message.
    43|         """
    44|         self.add_message(AIMessage(content=message))
    45|     @abstractmethod
    46|     def add_message(self, message: BaseMessage) -> None:
    47|         """Add a Message object to the store.
    48|         Args:
    49|             message: A BaseMessage object to store.
    50|         """
    51|         raise NotImplementedError()
    52|     @abstractmethod
    53|     def clear(self) -> None:
    54|         """Remove all messages from the store"""
    55|     def __str__(self) -> str:
    56|         return get_buffer_string(self.messages)


# ====================================================================
# FILE: libs/core/langchain_core/env.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-13 ---
     1| import platform
     2| from functools import lru_cache
     3| @lru_cache(maxsize=1)
     4| def get_runtime_environment() -> dict:
     5|     """Get information about the LangChain runtime environment."""
     6|     from langchain_core import __version__
     7|     return {
     8|         "library_version": __version__,
     9|         "library": "langchain",
    10|         "platform": platform.platform(),
    11|         "runtime": "python",
    12|         "runtime_version": platform.python_version(),
    13|     }


# ====================================================================
# FILE: libs/core/langchain_core/example_selectors/base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-11 ---
     1| """Interface for selecting examples to include in prompts."""
     2| from abc import ABC, abstractmethod
     3| from typing import Any, Dict, List
     4| class BaseExampleSelector(ABC):
     5|     """Interface for selecting examples to include in prompts."""
     6|     @abstractmethod
     7|     def add_example(self, example: Dict[str, str]) -> Any:
     8|         """Add new example to store for a key."""
     9|     @abstractmethod
    10|     def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:
    11|         """Select which examples to use based on the inputs."""


# ====================================================================
# FILE: libs/core/langchain_core/language_models/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-18 ---
     1| from langchain_core.language_models.base import (
     2|     BaseLanguageModel,
     3|     LanguageModelInput,
     4|     LanguageModelOutput,
     5|     get_tokenizer,
     6| )
     7| from langchain_core.language_models.chat_models import BaseChatModel, SimpleChatModel
     8| from langchain_core.language_models.llms import LLM, BaseLLM
     9| __all__ = [
    10|     "BaseLanguageModel",
    11|     "BaseChatModel",
    12|     "SimpleChatModel",
    13|     "BaseLLM",
    14|     "LLM",
    15|     "LanguageModelInput",
    16|     "get_tokenizer",
    17|     "LanguageModelOutput",
    18| ]


# ====================================================================
# FILE: libs/core/langchain_core/language_models/base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-60 ---
     1| from __future__ import annotations
     2| from abc import ABC, abstractmethod
     3| from functools import lru_cache
     4| from typing import (
     5|     TYPE_CHECKING,
     6|     Any,
     7|     List,
     8|     Optional,
     9|     Sequence,
    10|     Set,
    11|     TypeVar,
    12|     Union,
    13| )
    14| from typing_extensions import TypeAlias
    15| from langchain_core.messages import AnyMessage, BaseMessage, get_buffer_string
    16| from langchain_core.prompt_values import PromptValue
    17| from langchain_core.runnables import RunnableSerializable
    18| from langchain_core.utils import get_pydantic_field_names
    19| if TYPE_CHECKING:
    20|     from langchain_core.callbacks import Callbacks
    21|     from langchain_core.outputs import LLMResult
    22| @lru_cache(maxsize=None)  # Cache the tokenizer
    23| def get_tokenizer() -> Any:
    24|     try:
    25|         from transformers import GPT2TokenizerFast  # type: ignore[import]
    26|     except ImportError:
    27|         raise ImportError(
    28|             "Could not import transformers python package. "
    29|             "This is needed in order to calculate get_token_ids. "
    30|             "Please install it with `pip install transformers`."
    31|         )
    32|     return GPT2TokenizerFast.from_pretrained("gpt2")
    33| def _get_token_ids_default_method(text: str) -> List[int]:
    34|     """Encode the text into token IDs."""
    35|     tokenizer = get_tokenizer()
    36|     return tokenizer.encode(text)
    37| LanguageModelInput = Union[PromptValue, str, List[BaseMessage]]
    38| LanguageModelOutput = TypeVar("LanguageModelOutput")
    39| class BaseLanguageModel(
    40|     RunnableSerializable[LanguageModelInput, LanguageModelOutput], ABC
    41| ):
    42|     """Abstract base class for interfacing with language models.
    43|     All language model wrappers inherit from BaseLanguageModel.
    44|     Exposes three main methods:
    45|     - generate_prompt: generate language model outputs for a sequence of prompt
    46|         values. A prompt value is a model input that can be converted to any language
    47|         model input format (string or messages).
    48|     - predict: pass in a single string to a language model and return a string
    49|         prediction.
    50|     - predict_messages: pass in a sequence of BaseMessages (corresponding to a single
    51|         model call) to a language model and return a BaseMessage prediction.
    52|     Each of these has an equivalent asynchronous method.
    53|     """
    54|     @property
    55|     def InputType(self) -> TypeAlias:
    56|         """Get the input type for this runnable."""
    57|         from langchain_core.prompt_values import (
    58|             ChatPromptValueConcrete,
    59|             StringPromptValue,
    60|         )


# ====================================================================
# FILE: libs/core/langchain_core/language_models/chat_models.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 215-255 ---
   215|             except BaseException as e:
   216|                 run_manager.on_llm_error(
   217|                     e,
   218|                     response=LLMResult(
   219|                         generations=[[generation]] if generation else []
   220|                     ),
   221|                 )
   222|                 raise e
   223|             else:
   224|                 run_manager.on_llm_end(LLMResult(generations=[[generation]]))
   225|     async def astream(
   226|         self,
   227|         input: LanguageModelInput,
   228|         config: Optional[RunnableConfig] = None,
   229|         *,
   230|         stop: Optional[List[str]] = None,
   231|         **kwargs: Any,
   232|     ) -> AsyncIterator[BaseMessageChunk]:
   233|         if type(self)._astream == BaseChatModel._astream:
   234|             yield cast(
   235|                 BaseMessageChunk, self.invoke(input, config=config, stop=stop, **kwargs)
   236|             )
   237|         else:
   238|             config = config or {}
   239|             messages = self._convert_input(input).to_messages()
   240|             params = self._get_invocation_params(stop=stop, **kwargs)
   241|             options = {"stop": stop, **kwargs}
   242|             callback_manager = AsyncCallbackManager.configure(
   243|                 config.get("callbacks"),
   244|                 self.callbacks,
   245|                 self.verbose,
   246|                 config.get("tags"),
   247|                 self.tags,
   248|                 config.get("metadata"),
   249|                 self.metadata,
   250|             )
   251|             (run_manager,) = await callback_manager.on_chat_model_start(
   252|                 dumpd(self),
   253|                 [messages],
   254|                 invocation_params=params,
   255|                 options=options,


# ====================================================================
# FILE: libs/core/langchain_core/output_parsers/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| from langchain_core.output_parsers.base import (
     2|     BaseGenerationOutputParser,
     3|     BaseLLMOutputParser,
     4|     BaseOutputParser,
     5| )
     6| from langchain_core.output_parsers.list import (
     7|     CommaSeparatedListOutputParser,
     8|     ListOutputParser,
     9|     MarkdownListOutputParser,
    10|     NumberedListOutputParser,
    11| )
    12| from langchain_core.output_parsers.string import StrOutputParser
    13| from langchain_core.output_parsers.transform import (
    14|     BaseCumulativeTransformOutputParser,
    15|     BaseTransformOutputParser,
    16| )
    17| __all__ = [
    18|     "BaseLLMOutputParser",
    19|     "BaseGenerationOutputParser",
    20|     "BaseOutputParser",
    21|     "ListOutputParser",
    22|     "CommaSeparatedListOutputParser",
    23|     "NumberedListOutputParser",
    24|     "MarkdownListOutputParser",
    25|     "StrOutputParser",
    26|     "BaseTransformOutputParser",
    27|     "BaseCumulativeTransformOutputParser",
    28| ]


# ====================================================================
# FILE: libs/core/langchain_core/output_parsers/base.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 58-98 ---
    58|         self, input: Union[str, BaseMessage], config: Optional[RunnableConfig] = None
    59|     ) -> T:
    60|         if isinstance(input, BaseMessage):
    61|             return self._call_with_config(
    62|                 lambda inner_input: self.parse_result(
    63|                     [ChatGeneration(message=inner_input)]
    64|                 ),
    65|                 input,
    66|                 config,
    67|                 run_type="parser",
    68|             )
    69|         else:
    70|             return self._call_with_config(
    71|                 lambda inner_input: self.parse_result([Generation(text=inner_input)]),
    72|                 input,
    73|                 config,
    74|                 run_type="parser",
    75|             )
    76|     async def ainvoke(
    77|         self,
    78|         input: str | BaseMessage,
    79|         config: Optional[RunnableConfig] = None,
    80|         **kwargs: Optional[Any],
    81|     ) -> T:
    82|         if isinstance(input, BaseMessage):
    83|             return await self._acall_with_config(
    84|                 lambda inner_input: self.aparse_result(
    85|                     [ChatGeneration(message=inner_input)]
    86|                 ),
    87|                 input,
    88|                 config,
    89|                 run_type="parser",
    90|             )
    91|         else:
    92|             return await self._acall_with_config(
    93|                 lambda inner_input: self.aparse_result([Generation(text=inner_input)]),
    94|                 input,
    95|                 config,
    96|                 run_type="parser",
    97|             )
    98| class BaseOutputParser(

# --- HUNK 2: Lines 135-175 ---
   135|         self, input: Union[str, BaseMessage], config: Optional[RunnableConfig] = None
   136|     ) -> T:
   137|         if isinstance(input, BaseMessage):
   138|             return self._call_with_config(
   139|                 lambda inner_input: self.parse_result(
   140|                     [ChatGeneration(message=inner_input)]
   141|                 ),
   142|                 input,
   143|                 config,
   144|                 run_type="parser",
   145|             )
   146|         else:
   147|             return self._call_with_config(
   148|                 lambda inner_input: self.parse_result([Generation(text=inner_input)]),
   149|                 input,
   150|                 config,
   151|                 run_type="parser",
   152|             )
   153|     async def ainvoke(
   154|         self,
   155|         input: str | BaseMessage,
   156|         config: Optional[RunnableConfig] = None,
   157|         **kwargs: Optional[Any],
   158|     ) -> T:
   159|         if isinstance(input, BaseMessage):
   160|             return await self._acall_with_config(
   161|                 lambda inner_input: self.aparse_result(
   162|                     [ChatGeneration(message=inner_input)]
   163|                 ),
   164|                 input,
   165|                 config,
   166|                 run_type="parser",
   167|             )
   168|         else:
   169|             return await self._acall_with_config(
   170|                 lambda inner_input: self.aparse_result([Generation(text=inner_input)]),
   171|                 input,
   172|                 config,
   173|                 run_type="parser",
   174|             )
   175|     def parse_result(self, result: List[Generation], *, partial: bool = False) -> T:


# ====================================================================
# FILE: libs/core/langchain_core/output_parsers/list.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-59 ---
     1| from __future__ import annotations
     2| import re
     3| from abc import abstractmethod
     4| from typing import List
     5| from langchain_core.output_parsers.base import BaseOutputParser
     6| class ListOutputParser(BaseOutputParser[List[str]]):
     7|     """Parse the output of an LLM call to a list."""
     8|     @property
     9|     def _type(self) -> str:
    10|         return "list"
    11|     @abstractmethod
    12|     def parse(self, text: str) -> List[str]:
    13|         """Parse the output of an LLM call."""
    14| class CommaSeparatedListOutputParser(ListOutputParser):
    15|     """Parse the output of an LLM call to a comma-separated list."""
    16|     @classmethod
    17|     def is_lc_serializable(cls) -> bool:
    18|         return True
    19|     @classmethod
    20|     def get_lc_namespace(cls) -> List[str]:
    21|         """Get the namespace of the langchain object."""
    22|         return ["langchain", "output_parsers", "list"]
    23|     def get_format_instructions(self) -> str:
    24|         return (
    25|             "Your response should be a list of comma separated values, "
    26|             "eg: `foo, bar, baz`"
    27|         )
    28|     def parse(self, text: str) -> List[str]:
    29|         """Parse the output of an LLM call."""
    30|         return text.strip().split(", ")
    31|     @property
    32|     def _type(self) -> str:
    33|         return "comma-separated-list"
    34| class NumberedListOutputParser(ListOutputParser):
    35|     """Parse a numbered list."""
    36|     def get_format_instructions(self) -> str:
    37|         return (
    38|             "Your response should be a numbered list with each item on a new line. "
    39|             "For example: \n\n1. foo\n\n2. bar\n\n3. baz"
    40|         )
    41|     def parse(self, text: str) -> List[str]:
    42|         """Parse the output of an LLM call."""
    43|         pattern = r"\d+\.\s([^\n]+)"
    44|         matches = re.findall(pattern, text)
    45|         return matches
    46|     @property
    47|     def _type(self) -> str:
    48|         return "numbered-list"
    49| class MarkdownListOutputParser(ListOutputParser):
    50|     """Parse a markdown list."""
    51|     def get_format_instructions(self) -> str:
    52|         return "Your response should be a markdown list, " "eg: `- foo\n- bar\n- baz`"
    53|     def parse(self, text: str) -> List[str]:
    54|         """Parse the output of an LLM call."""
    55|         pattern = r"-\s([^\n]+)"
    56|         return re.findall(pattern, text)
    57|     @property
    58|     def _type(self) -> str:
    59|         return "markdown-list"


# ====================================================================
# FILE: libs/core/langchain_core/prompts/base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 41-106 ---
    41|         """Get the namespace of the langchain object."""
    42|         return ["langchain", "schema", "prompt_template"]
    43|     @classmethod
    44|     def is_lc_serializable(cls) -> bool:
    45|         """Return whether this class is serializable."""
    46|         return True
    47|     class Config:
    48|         """Configuration for this pydantic object."""
    49|         arbitrary_types_allowed = True
    50|     @property
    51|     def OutputType(self) -> Any:
    52|         return Union[StringPromptValue, ChatPromptValueConcrete]
    53|     def get_input_schema(
    54|         self, config: Optional[RunnableConfig] = None
    55|     ) -> Type[BaseModel]:
    56|         return create_model(  # type: ignore[call-overload]
    57|             "PromptInput",
    58|             **{k: (self.input_types.get(k, str), None) for k in self.input_variables},
    59|         )
    60|     def _format_prompt_with_error_handling(self, inner_input: Dict) -> PromptValue:
    61|         try:
    62|             input_dict = {key: inner_input[key] for key in self.input_variables}
    63|         except TypeError as e:
    64|             raise TypeError(
    65|                 f"Expected mapping type as input to {self.__class__.__name__}. "
    66|                 f"Received {type(inner_input)}."
    67|             ) from e
    68|         except KeyError as e:
    69|             raise KeyError(
    70|                 f"Input to {self.__class__.__name__} is missing variable {e}. "
    71|                 f" Expected: {self.input_variables}"
    72|                 f" Received: {list(inner_input.keys())}"
    73|             ) from e
    74|         return self.format_prompt(**input_dict)
    75|     def invoke(
    76|         self, input: Dict, config: Optional[RunnableConfig] = None
    77|     ) -> PromptValue:
    78|         return self._call_with_config(
    79|             self._format_prompt_with_error_handling,
    80|             input,
    81|             config,
    82|             run_type="prompt",
    83|         )
    84|     @abstractmethod
    85|     def format_prompt(self, **kwargs: Any) -> PromptValue:
    86|         """Create Chat Messages."""
    87|     @root_validator()
    88|     def validate_variable_names(cls, values: Dict) -> Dict:
    89|         """Validate variable names do not include restricted names."""
    90|         if "stop" in values["input_variables"]:
    91|             raise ValueError(
    92|                 "Cannot have an input variable named 'stop', as it is used internally,"
    93|                 " please rename."
    94|             )
    95|         if "stop" in values["partial_variables"]:
    96|             raise ValueError(
    97|                 "Cannot have an partial variable named 'stop', as it is used "
    98|                 "internally, please rename."
    99|             )
   100|         overall = set(values["input_variables"]).intersection(
   101|             values["partial_variables"]
   102|         )
   103|         if overall:
   104|             raise ValueError(
   105|                 f"Found overlapping input and partial variables: {overall}"
   106|             )


# ====================================================================
# FILE: libs/core/langchain_core/prompts/chat.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 52-124 ---
    52|     @property
    53|     @abstractmethod
    54|     def input_variables(self) -> List[str]:
    55|         """Input variables for this prompt template.
    56|         Returns:
    57|             List of input variables.
    58|         """
    59|     def __add__(self, other: Any) -> ChatPromptTemplate:
    60|         """Combine two prompt templates.
    61|         Args:
    62|             other: Another prompt template.
    63|         Returns:
    64|             Combined prompt template.
    65|         """
    66|         prompt = ChatPromptTemplate(messages=[self])
    67|         return prompt + other
    68| class MessagesPlaceholder(BaseMessagePromptTemplate):
    69|     """Prompt template that assumes variable is already list of messages."""
    70|     variable_name: str
    71|     """Name of variable to use as messages."""
    72|     @classmethod
    73|     def get_lc_namespace(cls) -> List[str]:
    74|         """Get the namespace of the langchain object."""
    75|         return ["langchain", "prompts", "chat"]
    76|     def __init__(self, variable_name: str, **kwargs: Any):
    77|         return super().__init__(variable_name=variable_name, **kwargs)
    78|     def format_messages(self, **kwargs: Any) -> List[BaseMessage]:
    79|         """Format messages from kwargs.
    80|         Args:
    81|             **kwargs: Keyword arguments to use for formatting.
    82|         Returns:
    83|             List of BaseMessage.
    84|         """
    85|         value = kwargs[self.variable_name]
    86|         if not isinstance(value, list):
    87|             raise ValueError(
    88|                 f"variable {self.variable_name} should be a list of base messages, "
    89|                 f"got {value}"
    90|             )
    91|         for v in value:
    92|             if not isinstance(v, BaseMessage):
    93|                 raise ValueError(
    94|                     f"variable {self.variable_name} should be a list of base messages,"
    95|                     f" got {value}"
    96|                 )
    97|         return value
    98|     @property
    99|     def input_variables(self) -> List[str]:
   100|         """Input variables for this prompt template.
   101|         Returns:
   102|             List of input variable names.
   103|         """
   104|         return [self.variable_name]
   105| MessagePromptTemplateT = TypeVar(
   106|     "MessagePromptTemplateT", bound="BaseStringMessagePromptTemplate"
   107| )
   108| """Type variable for message prompt templates."""
   109| class BaseStringMessagePromptTemplate(BaseMessagePromptTemplate, ABC):
   110|     """Base class for message prompt templates that use a string prompt template."""
   111|     prompt: StringPromptTemplate
   112|     """String prompt template."""
   113|     additional_kwargs: dict = Field(default_factory=dict)
   114|     """Additional keyword arguments to pass to the prompt template."""
   115|     @classmethod
   116|     def get_lc_namespace(cls) -> List[str]:
   117|         """Get the namespace of the langchain object."""
   118|         return ["langchain", "prompts", "chat"]
   119|     @classmethod
   120|     def from_template(
   121|         cls: Type[MessagePromptTemplateT],
   122|         template: str,
   123|         template_format: str = "f-string",
   124|         partial_variables: Optional[Dict[str, Any]] = None,

# --- HUNK 2: Lines 458-503 ---
   458|         Returns:
   459|             formatted string
   460|         """
   461|         return self.format_prompt(**kwargs).to_string()
   462|     def format_messages(self, **kwargs: Any) -> List[BaseMessage]:
   463|         """Format the chat template into a list of finalized messages.
   464|         Args:
   465|             **kwargs: keyword arguments to use for filling in template variables
   466|                       in all the template messages in this chat template.
   467|         Returns:
   468|             list of formatted messages
   469|         """
   470|         kwargs = self._merge_partial_and_user_variables(**kwargs)
   471|         result = []
   472|         for message_template in self.messages:
   473|             if isinstance(message_template, BaseMessage):
   474|                 result.extend([message_template])
   475|             elif isinstance(
   476|                 message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)
   477|             ):
   478|                 rel_params = {
   479|                     k: v
   480|                     for k, v in kwargs.items()
   481|                     if k in message_template.input_variables
   482|                 }
   483|                 message = message_template.format_messages(**rel_params)
   484|                 result.extend(message)
   485|             else:
   486|                 raise ValueError(f"Unexpected input: {message_template}")
   487|         return result
   488|     def partial(self, **kwargs: Union[str, Callable[[], str]]) -> ChatPromptTemplate:
   489|         """Get a new ChatPromptTemplate with some input variables already filled in.
   490|         Args:
   491|             **kwargs: keyword arguments to use for filling in template variables. Ought
   492|                         to be a subset of the input variables.
   493|         Returns:
   494|             A new ChatPromptTemplate.
   495|         Example:
   496|             .. code-block:: python
   497|                 from langchain_core.prompts import ChatPromptTemplate
   498|                 template = ChatPromptTemplate.from_messages(
   499|                     [
   500|                         ("system", "You are an AI assistant named {name}."),
   501|                         ("human", "Hi I'm {user}"),
   502|                         ("ai", "Hi there, {user}, I'm {name}."),
   503|                         ("human", "{input}"),


# ====================================================================
# FILE: libs/core/langchain_core/retrievers.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| from __future__ import annotations
     2| import asyncio
     3| import warnings
     4| from abc import ABC, abstractmethod
     5| from functools import partial
     6| from inspect import signature
     7| from typing import TYPE_CHECKING, Any, Dict, List, Optional
     8| from langchain_core.documents import Document
     9| from langchain_core.load.dump import dumpd
    10| from langchain_core.runnables import RunnableConfig, RunnableSerializable
    11| if TYPE_CHECKING:
    12|     from langchain_core.callbacks.manager import (
    13|         AsyncCallbackManagerForRetrieverRun,
    14|         CallbackManagerForRetrieverRun,
    15|         Callbacks,
    16|     )
    17| class BaseRetriever(RunnableSerializable[str, List[Document]], ABC):
    18|     """Abstract base class for a Document retrieval system.
    19|     A retrieval system is defined as something that can take string queries and return
    20|         the most 'relevant' Documents from some source.
    21|     Example:
    22|         .. code-block:: python
    23|             class TFIDFRetriever(BaseRetriever, BaseModel):
    24|                 vectorizer: Any
    25|                 docs: List[Document]
    26|                 tfidf_array: Any
    27|                 k: int = 4
    28|                 class Config:
    29|                     arbitrary_types_allowed = True
    30|                 def get_relevant_documents(self, query: str) -> List[Document]:
    31|                     from sklearn.metrics.pairwise import cosine_similarity
    32|                     query_vec = self.vectorizer.transform([query])
    33|                     results = cosine_similarity(self.tfidf_array, query_vec).reshape((-1,))
    34|                     return [self.docs[i] for i in results.argsort()[-self.k :][::-1]]
    35|     """  # noqa: E501
    36|     class Config:
    37|         """Configuration for this pydantic object."""


# ====================================================================
# FILE: libs/core/langchain_core/runnables/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 9-62 ---
     9| creating more responsive UX.
    10| This module contains schema and implementation of LangChain Runnables primitives.
    11| """
    12| from langchain_core.runnables.base import (
    13|     Runnable,
    14|     RunnableBinding,
    15|     RunnableGenerator,
    16|     RunnableLambda,
    17|     RunnableMap,
    18|     RunnableParallel,
    19|     RunnableSequence,
    20|     RunnableSerializable,
    21| )
    22| from langchain_core.runnables.branch import RunnableBranch
    23| from langchain_core.runnables.config import (
    24|     RunnableConfig,
    25|     get_config_list,
    26|     patch_config,
    27| )
    28| from langchain_core.runnables.fallbacks import RunnableWithFallbacks
    29| from langchain_core.runnables.passthrough import RunnablePassthrough
    30| from langchain_core.runnables.router import RouterInput, RouterRunnable
    31| from langchain_core.runnables.utils import (
    32|     AddableDict,
    33|     ConfigurableField,
    34|     ConfigurableFieldMultiOption,
    35|     ConfigurableFieldSingleOption,
    36|     aadd,
    37|     add,
    38| )
    39| __all__ = [
    40|     "AddableDict",
    41|     "ConfigurableField",
    42|     "ConfigurableFieldSingleOption",
    43|     "ConfigurableFieldMultiOption",
    44|     "patch_config",
    45|     "RouterInput",
    46|     "RouterRunnable",
    47|     "Runnable",
    48|     "RunnableSerializable",
    49|     "RunnableBinding",
    50|     "RunnableBranch",
    51|     "RunnableConfig",
    52|     "RunnableGenerator",
    53|     "RunnableLambda",
    54|     "RunnableMap",
    55|     "RunnableParallel",
    56|     "RunnablePassthrough",
    57|     "RunnableSequence",
    58|     "RunnableWithFallbacks",
    59|     "get_config_list",
    60|     "aadd",
    61|     "add",
    62| ]


# ====================================================================
# FILE: libs/core/langchain_core/runnables/base.py
# Total hunks: 31
# ====================================================================
# --- HUNK 1: Lines 15-95 ---
    15|     Awaitable,
    16|     Callable,
    17|     Dict,
    18|     Generic,
    19|     Iterator,
    20|     List,
    21|     Mapping,
    22|     Optional,
    23|     Sequence,
    24|     Set,
    25|     Tuple,
    26|     Type,
    27|     TypeVar,
    28|     Union,
    29|     cast,
    30|     overload,
    31| )
    32| from typing_extensions import Literal, get_args
    33| from langchain_core.load.dump import dumpd, dumps
    34| from langchain_core.load.serializable import Serializable
    35| from langchain_core.pydantic_v1 import BaseModel, Field, create_model
    36| from langchain_core.runnables.config import (
    37|     RunnableConfig,
    38|     acall_func_with_variable_args,
    39|     call_func_with_variable_args,
    40|     ensure_config,
    41|     get_async_callback_manager_for_config,
    42|     get_callback_manager_for_config,
    43|     get_config_list,
    44|     get_executor_for_config,
    45|     merge_configs,
    46|     patch_config,
    47| )
    48| from langchain_core.runnables.utils import (
    49|     AddableDict,
    50|     AnyConfigurableField,
    51|     ConfigurableField,
    52|     ConfigurableFieldSpec,
    53|     Input,
    54|     Output,
    55|     accepts_config,
    56|     accepts_run_manager,
    57|     gather_with_concurrency,
    58|     get_function_first_arg_dict_keys,
    59|     get_lambda_source,
    60|     get_unique_config_specs,
    61|     indent_lines_after_first,
    62| )
    63| from langchain_core.utils.aiter import atee, py_anext
    64| from langchain_core.utils.iter import safetee
    65| if TYPE_CHECKING:
    66|     from langchain_core.callbacks.manager import (
    67|         AsyncCallbackManagerForChainRun,
    68|         CallbackManagerForChainRun,
    69|     )
    70|     from langchain_core.runnables.fallbacks import (
    71|         RunnableWithFallbacks as RunnableWithFallbacksT,
    72|     )
    73|     from langchain_core.tracers.log_stream import RunLog, RunLogPatch
    74|     from langchain_core.tracers.root_listeners import Listener
    75| Other = TypeVar("Other")
    76| class Runnable(Generic[Input, Output], ABC):
    77|     """A unit of work that can be invoked, batched, streamed, transformed and composed.
    78|      Key Methods
    79|      ===========
    80|     * invoke/ainvoke: Transforms a single input into an output.
    81|     * batch/abatch: Efficiently transforms multiple inputs into outputs.
    82|     * stream/astream: Streams output from a single input as it's produced.
    83|     * astream_log: Streams output and selected intermediate results from an input.
    84|     Built-in optimizations:
    85|     * Batch: By default, batch runs invoke() in parallel using a thread pool executor.
    86|              Override to optimize batching.
    87|     * Async: Methods with "a" suffix are asynchronous. By default, they execute
    88|              the sync counterpart using asyncio's thread pool.
    89|              Override for native async.
    90|     All methods accept an optional config argument, which can be used to configure
    91|     execution, add tags and metadata for tracing and debugging etc.
    92|     Runnables expose schematic information about their input, output and config via
    93|     the input_schema property, the output_schema property and config_schema method.
    94|     LCEL and Composition
    95|     ====================

# --- HUNK 2: Lines 142-341 ---
   142|         print(sequence.input_schema.schema()) # Show inferred input schema
   143|         print(sequence.output_schema.schema()) # Show inferred output schema
   144|         print(sequence.invoke(2)) # invoke the sequence (note the retry above!!)
   145|     Debugging and tracing
   146|     =====================
   147|     As the chains get longer, it can be useful to be able to see intermediate results
   148|     to debug and trace the chain.
   149|     You can set the global debug flag to True to enable debug output for all chains:
   150|         .. code-block:: python
   151|             from langchain_core.globals import set_debug
   152|             set_debug(True)
   153|     Alternatively, you can pass existing or custom callbacks to any given chain:
   154|         .. code-block:: python
   155|             from langchain_core.tracers import ConsoleCallbackHandler
   156|             chain.invoke(
   157|                 ...,
   158|                 config={'callbacks': [ConsoleCallbackHandler()]}
   159|             )
   160|     For a UI (and much more) checkout LangSmith: https://docs.smith.langchain.com/
   161|     """
   162|     @property
   163|     def InputType(self) -> Type[Input]:
   164|         """The type of input this runnable accepts specified as a type annotation."""
   165|         for cls in self.__class__.__orig_bases__:  # type: ignore[attr-defined]
   166|             type_args = get_args(cls)
   167|             if type_args and len(type_args) == 2:
   168|                 return type_args[0]
   169|         raise TypeError(
   170|             f"Runnable {self.__class__.__name__} doesn't have an inferable InputType. "
   171|             "Override the InputType property to specify the input type."
   172|         )
   173|     @property
   174|     def OutputType(self) -> Type[Output]:
   175|         """The type of output this runnable produces specified as a type annotation."""
   176|         for cls in self.__class__.__orig_bases__:  # type: ignore[attr-defined]
   177|             type_args = get_args(cls)
   178|             if type_args and len(type_args) == 2:
   179|                 return type_args[1]
   180|         raise TypeError(
   181|             f"Runnable {self.__class__.__name__} doesn't have an inferable OutputType. "
   182|             "Override the OutputType property to specify the output type."
   183|         )
   184|     @property
   185|     def input_schema(self) -> Type[BaseModel]:
   186|         """The type of input this runnable accepts specified as a pydantic model."""
   187|         return self.get_input_schema()
   188|     def get_input_schema(
   189|         self, config: Optional[RunnableConfig] = None
   190|     ) -> Type[BaseModel]:
   191|         """Get a pydantic model that can be used to validate input to the runnable.
   192|         Runnables that leverage the configurable_fields and configurable_alternatives
   193|         methods will have a dynamic input schema that depends on which
   194|         configuration the runnable is invoked with.
   195|         This method allows to get an input schema for a specific configuration.
   196|         Args:
   197|             config: A config to use when generating the schema.
   198|         Returns:
   199|             A pydantic model that can be used to validate input.
   200|         """
   201|         root_type = self.InputType
   202|         if inspect.isclass(root_type) and issubclass(root_type, BaseModel):
   203|             return root_type
   204|         return create_model(
   205|             self.__class__.__name__ + "Input", __root__=(root_type, None)
   206|         )
   207|     @property
   208|     def output_schema(self) -> Type[BaseModel]:
   209|         """The type of output this runnable produces specified as a pydantic model."""
   210|         return self.get_output_schema()
   211|     def get_output_schema(
   212|         self, config: Optional[RunnableConfig] = None
   213|     ) -> Type[BaseModel]:
   214|         """Get a pydantic model that can be used to validate output to the runnable.
   215|         Runnables that leverage the configurable_fields and configurable_alternatives
   216|         methods will have a dynamic output schema that depends on which
   217|         configuration the runnable is invoked with.
   218|         This method allows to get an output schema for a specific configuration.
   219|         Args:
   220|             config: A config to use when generating the schema.
   221|         Returns:
   222|             A pydantic model that can be used to validate output.
   223|         """
   224|         root_type = self.OutputType
   225|         if inspect.isclass(root_type) and issubclass(root_type, BaseModel):
   226|             return root_type
   227|         return create_model(
   228|             self.__class__.__name__ + "Output", __root__=(root_type, None)
   229|         )
   230|     @property
   231|     def config_specs(self) -> List[ConfigurableFieldSpec]:
   232|         """List configurable fields for this runnable."""
   233|         return []
   234|     def config_schema(
   235|         self, *, include: Optional[Sequence[str]] = None
   236|     ) -> Type[BaseModel]:
   237|         """The type of config this runnable accepts specified as a pydantic model.
   238|         To mark a field as configurable, see the `configurable_fields`
   239|         and `configurable_alternatives` methods.
   240|         Args:
   241|             include: A list of fields to include in the config schema.
   242|         Returns:
   243|             A pydantic model that can be used to validate config.
   244|         """
   245|         class _Config:
   246|             arbitrary_types_allowed = True
   247|         include = include or []
   248|         config_specs = self.config_specs
   249|         configurable = (
   250|             create_model(  # type: ignore[call-overload]
   251|                 "Configurable",
   252|                 **{
   253|                     spec.id: (
   254|                         spec.annotation,
   255|                         Field(
   256|                             spec.default, title=spec.name, description=spec.description
   257|                         ),
   258|                     )
   259|                     for spec in config_specs
   260|                 },
   261|             )
   262|             if config_specs
   263|             else None
   264|         )
   265|         return create_model(  # type: ignore[call-overload]
   266|             self.__class__.__name__ + "Config",
   267|             __config__=_Config,
   268|             **({"configurable": (configurable, None)} if configurable else {}),
   269|             **{
   270|                 field_name: (field_type, None)
   271|                 for field_name, field_type in RunnableConfig.__annotations__.items()
   272|                 if field_name in [i for i in include if i != "configurable"]
   273|             },
   274|         )
   275|     def __or__(
   276|         self,
   277|         other: Union[
   278|             Runnable[Any, Other],
   279|             Callable[[Any], Other],
   280|             Callable[[Iterator[Any]], Iterator[Other]],
   281|             Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],
   282|         ],
   283|     ) -> RunnableSerializable[Input, Other]:
   284|         """Compose this runnable with another object to create a RunnableSequence."""
   285|         return RunnableSequence(first=self, last=coerce_to_runnable(other))
   286|     def __ror__(
   287|         self,
   288|         other: Union[
   289|             Runnable[Other, Any],
   290|             Callable[[Other], Any],
   291|             Callable[[Iterator[Other]], Iterator[Any]],
   292|             Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]],
   293|         ],
   294|     ) -> RunnableSerializable[Other, Output]:
   295|         """Compose this runnable with another object to create a RunnableSequence."""
   296|         return RunnableSequence(first=coerce_to_runnable(other), last=self)
   297|     """ --- Public API --- """
   298|     @abstractmethod
   299|     def invoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output:
   300|         """Transform a single input into an output. Override to implement.
   301|         Args:
   302|             input: The input to the runnable.
   303|             config: A config to use when invoking the runnable.
   304|                The config supports standard keys like 'tags', 'metadata' for tracing
   305|                purposes, 'max_concurrency' for controlling how much work to do
   306|                in parallel, and other keys. Please refer to the RunnableConfig
   307|                for more details.
   308|         Returns:
   309|             The output of the runnable.
   310|         """
   311|     async def ainvoke(
   312|         self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
   313|     ) -> Output:
   314|         """Default implementation of ainvoke, calls invoke from a thread.
   315|         The default implementation allows usage of async code even if
   316|         the runnable did not implement a native async version of invoke.
   317|         Subclasses should override this method if they can run asynchronously.
   318|         """
   319|         return await asyncio.get_running_loop().run_in_executor(
   320|             None, partial(self.invoke, **kwargs), input, config
   321|         )
   322|     def batch(
   323|         self,
   324|         inputs: List[Input],
   325|         config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,
   326|         *,
   327|         return_exceptions: bool = False,
   328|         **kwargs: Optional[Any],
   329|     ) -> List[Output]:
   330|         """Default implementation runs invoke in parallel using a thread pool executor.
   331|         The default implementation of batch works well for IO bound runnables.
   332|         Subclasses should override this method if they can batch more efficiently;
   333|         e.g., if the underlying runnable uses an API which supports a batch mode.
   334|         """
   335|         if not inputs:
   336|             return []
   337|         configs = get_config_list(config, len(inputs))
   338|         def invoke(input: Input, config: RunnableConfig) -> Union[Output, Exception]:
   339|             if return_exceptions:
   340|                 try:
   341|                     return self.invoke(input, config, **kwargs)

# --- HUNK 3: Lines 706-781 ---
   706|     def _call_with_config(
   707|         self,
   708|         func: Union[
   709|             Callable[[Input], Output],
   710|             Callable[[Input, CallbackManagerForChainRun], Output],
   711|             Callable[[Input, CallbackManagerForChainRun, RunnableConfig], Output],
   712|         ],
   713|         input: Input,
   714|         config: Optional[RunnableConfig],
   715|         run_type: Optional[str] = None,
   716|         **kwargs: Optional[Any],
   717|     ) -> Output:
   718|         """Helper method to transform an Input value to an Output value,
   719|         with callbacks. Use this method to implement invoke() in subclasses."""
   720|         config = ensure_config(config)
   721|         callback_manager = get_callback_manager_for_config(config)
   722|         run_manager = callback_manager.on_chain_start(
   723|             dumpd(self),
   724|             input,
   725|             run_type=run_type,
   726|             name=config.get("run_name"),
   727|         )
   728|         try:
   729|             output = call_func_with_variable_args(
   730|                 func, input, config, run_manager, **kwargs
   731|             )
   732|         except BaseException as e:
   733|             run_manager.on_chain_error(e)
   734|             raise
   735|         else:
   736|             run_manager.on_chain_end(dumpd(output))
   737|             return output
   738|     async def _acall_with_config(
   739|         self,
   740|         func: Union[
   741|             Callable[[Input], Awaitable[Output]],
   742|             Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],
   743|             Callable[
   744|                 [Input, AsyncCallbackManagerForChainRun, RunnableConfig],
   745|                 Awaitable[Output],
   746|             ],
   747|         ],
   748|         input: Input,
   749|         config: Optional[RunnableConfig],
   750|         run_type: Optional[str] = None,
   751|         **kwargs: Optional[Any],
   752|     ) -> Output:
   753|         """Helper method to transform an Input value to an Output value,
   754|         with callbacks. Use this method to implement ainvoke() in subclasses."""
   755|         config = ensure_config(config)
   756|         callback_manager = get_async_callback_manager_for_config(config)
   757|         run_manager = await callback_manager.on_chain_start(
   758|             dumpd(self),
   759|             input,
   760|             run_type=run_type,
   761|             name=config.get("run_name"),
   762|         )
   763|         try:
   764|             output = await acall_func_with_variable_args(
   765|                 func, input, config, run_manager, **kwargs
   766|             )
   767|         except BaseException as e:
   768|             await run_manager.on_chain_error(e)
   769|             raise
   770|         else:
   771|             await run_manager.on_chain_end(dumpd(output))
   772|             return output
   773|     def _batch_with_config(
   774|         self,
   775|         func: Union[
   776|             Callable[[List[Input]], List[Union[Exception, Output]]],
   777|             Callable[
   778|                 [List[Input], List[CallbackManagerForChainRun]],
   779|                 List[Union[Exception, Output]],
   780|             ],
   781|             Callable[

# --- HUNK 4: Lines 784-824 ---
   784|             ],
   785|         ],
   786|         input: List[Input],
   787|         config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,
   788|         *,
   789|         return_exceptions: bool = False,
   790|         run_type: Optional[str] = None,
   791|         **kwargs: Optional[Any],
   792|     ) -> List[Output]:
   793|         """Helper method to transform an Input value to an Output value,
   794|         with callbacks. Use this method to implement invoke() in subclasses."""
   795|         if not input:
   796|             return []
   797|         configs = get_config_list(config, len(input))
   798|         callback_managers = [get_callback_manager_for_config(c) for c in configs]
   799|         run_managers = [
   800|             callback_manager.on_chain_start(
   801|                 dumpd(self),
   802|                 input,
   803|                 run_type=run_type,
   804|                 name=config.get("run_name"),
   805|             )
   806|             for callback_manager, input, config in zip(
   807|                 callback_managers, input, configs
   808|             )
   809|         ]
   810|         try:
   811|             if accepts_config(func):
   812|                 kwargs["config"] = [
   813|                     patch_config(c, callbacks=rm.get_child())
   814|                     for c, rm in zip(configs, run_managers)
   815|                 ]
   816|             if accepts_run_manager(func):
   817|                 kwargs["run_manager"] = run_managers
   818|             output = func(input, **kwargs)  # type: ignore[call-arg]
   819|         except BaseException as e:
   820|             for run_manager in run_managers:
   821|                 run_manager.on_chain_error(e)
   822|             if return_exceptions:
   823|                 return cast(List[Output], [e for _ in input])
   824|             else:

# --- HUNK 5: Lines 854-894 ---
   854|         ],
   855|         input: List[Input],
   856|         config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,
   857|         *,
   858|         return_exceptions: bool = False,
   859|         run_type: Optional[str] = None,
   860|         **kwargs: Optional[Any],
   861|     ) -> List[Output]:
   862|         """Helper method to transform an Input value to an Output value,
   863|         with callbacks. Use this method to implement invoke() in subclasses."""
   864|         if not input:
   865|             return []
   866|         configs = get_config_list(config, len(input))
   867|         callback_managers = [get_async_callback_manager_for_config(c) for c in configs]
   868|         run_managers: List[AsyncCallbackManagerForChainRun] = await asyncio.gather(
   869|             *(
   870|                 callback_manager.on_chain_start(
   871|                     dumpd(self),
   872|                     input,
   873|                     run_type=run_type,
   874|                     name=config.get("run_name"),
   875|                 )
   876|                 for callback_manager, input, config in zip(
   877|                     callback_managers, input, configs
   878|                 )
   879|             )
   880|         )
   881|         try:
   882|             if accepts_config(func):
   883|                 kwargs["config"] = [
   884|                     patch_config(c, callbacks=rm.get_child())
   885|                     for c, rm in zip(configs, run_managers)
   886|                 ]
   887|             if accepts_run_manager(func):
   888|                 kwargs["run_manager"] = run_managers
   889|             output = await func(input, **kwargs)  # type: ignore[call-arg]
   890|         except BaseException as e:
   891|             await asyncio.gather(
   892|                 *(run_manager.on_chain_error(e) for run_manager in run_managers)
   893|             )
   894|             if return_exceptions:

# --- HUNK 6: Lines 925-965 ---
   925|             ],
   926|         ],
   927|         config: Optional[RunnableConfig],
   928|         run_type: Optional[str] = None,
   929|         **kwargs: Optional[Any],
   930|     ) -> Iterator[Output]:
   931|         """Helper method to transform an Iterator of Input values into an Iterator of
   932|         Output values, with callbacks.
   933|         Use this to implement `stream()` or `transform()` in Runnable subclasses."""
   934|         input_for_tracing, input_for_transform = tee(input, 2)
   935|         final_input: Optional[Input] = next(input_for_tracing, None)
   936|         final_input_supported = True
   937|         final_output: Optional[Output] = None
   938|         final_output_supported = True
   939|         config = ensure_config(config)
   940|         callback_manager = get_callback_manager_for_config(config)
   941|         run_manager = callback_manager.on_chain_start(
   942|             dumpd(self),
   943|             {"input": ""},
   944|             run_type=run_type,
   945|             name=config.get("run_name"),
   946|         )
   947|         try:
   948|             if accepts_config(transformer):
   949|                 kwargs["config"] = patch_config(
   950|                     config, callbacks=run_manager.get_child()
   951|                 )
   952|             if accepts_run_manager(transformer):
   953|                 kwargs["run_manager"] = run_manager
   954|             iterator = transformer(input_for_transform, **kwargs)  # type: ignore[call-arg]
   955|             for chunk in iterator:
   956|                 yield chunk
   957|                 if final_output_supported:
   958|                     if final_output is None:
   959|                         final_output = chunk
   960|                     else:
   961|                         try:
   962|                             final_output = final_output + chunk  # type: ignore
   963|                         except TypeError:
   964|                             final_output = None
   965|                             final_output_supported = False

# --- HUNK 7: Lines 997-1103 ---
   997|             ],
   998|         ],
   999|         config: Optional[RunnableConfig],
  1000|         run_type: Optional[str] = None,
  1001|         **kwargs: Optional[Any],
  1002|     ) -> AsyncIterator[Output]:
  1003|         """Helper method to transform an Async Iterator of Input values into an Async
  1004|         Iterator of Output values, with callbacks.
  1005|         Use this to implement `astream()` or `atransform()` in Runnable subclasses."""
  1006|         input_for_tracing, input_for_transform = atee(input, 2)
  1007|         final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  1008|         final_input_supported = True
  1009|         final_output: Optional[Output] = None
  1010|         final_output_supported = True
  1011|         config = ensure_config(config)
  1012|         callback_manager = get_async_callback_manager_for_config(config)
  1013|         run_manager = await callback_manager.on_chain_start(
  1014|             dumpd(self),
  1015|             {"input": ""},
  1016|             run_type=run_type,
  1017|             name=config.get("run_name"),
  1018|         )
  1019|         try:
  1020|             if accepts_config(transformer):
  1021|                 kwargs["config"] = patch_config(
  1022|                     config, callbacks=run_manager.get_child()
  1023|                 )
  1024|             if accepts_run_manager(transformer):
  1025|                 kwargs["run_manager"] = run_manager
  1026|             iterator = transformer(input_for_transform, **kwargs)  # type: ignore[call-arg]
  1027|             async for chunk in iterator:
  1028|                 yield chunk
  1029|                 if final_output_supported:
  1030|                     if final_output is None:
  1031|                         final_output = chunk
  1032|                     else:
  1033|                         try:
  1034|                             final_output = final_output + chunk  # type: ignore
  1035|                         except TypeError:
  1036|                             final_output = None
  1037|                             final_output_supported = False
  1038|             async for ichunk in input_for_tracing:
  1039|                 if final_input_supported:
  1040|                     if final_input is None:
  1041|                         final_input = ichunk
  1042|                     else:
  1043|                         try:
  1044|                             final_input = final_input + ichunk  # type: ignore[operator]
  1045|                         except TypeError:
  1046|                             final_input = None
  1047|                             final_input_supported = False
  1048|         except BaseException as e:
  1049|             await run_manager.on_chain_error(e, inputs=final_input)
  1050|             raise
  1051|         else:
  1052|             await run_manager.on_chain_end(final_output, inputs=final_input)
  1053| class RunnableSerializable(Serializable, Runnable[Input, Output]):
  1054|     """A Runnable that can be serialized to JSON."""
  1055|     def configurable_fields(
  1056|         self, **kwargs: AnyConfigurableField
  1057|     ) -> RunnableSerializable[Input, Output]:
  1058|         from langchain_core.runnables.configurable import RunnableConfigurableFields
  1059|         for key in kwargs:
  1060|             if key not in self.__fields__:
  1061|                 raise ValueError(
  1062|                     f"Configuration key {key} not found in {self}: "
  1063|                     "available keys are {self.__fields__.keys()}"
  1064|                 )
  1065|         return RunnableConfigurableFields(default=self, fields=kwargs)
  1066|     def configurable_alternatives(
  1067|         self,
  1068|         which: ConfigurableField,
  1069|         *,
  1070|         default_key: str = "default",
  1071|         prefix_keys: bool = False,
  1072|         **kwargs: Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]],
  1073|     ) -> RunnableSerializable[Input, Output]:
  1074|         from langchain_core.runnables.configurable import (
  1075|             RunnableConfigurableAlternatives,
  1076|         )
  1077|         return RunnableConfigurableAlternatives(
  1078|             which=which,
  1079|             default=self,
  1080|             alternatives=kwargs,
  1081|             default_key=default_key,
  1082|             prefix_keys=prefix_keys,
  1083|         )
  1084| class RunnableSequence(RunnableSerializable[Input, Output]):
  1085|     """A sequence of runnables, where the output of each is the input of the next.
  1086|     RunnableSequence is the most important composition operator in LangChain as it is
  1087|     used in virtually every chain.
  1088|     A RunnableSequence can be instantiated directly or more commonly by using the `|`
  1089|     operator where either the left or right operands (or both) must be a Runnable.
  1090|     Any RunnableSequence automatically supports sync, async, batch.
  1091|     The default implementations of `batch` and `abatch` utilize threadpools and
  1092|     asyncio gather and will be faster than naive invocation of invoke or ainvoke
  1093|     for IO bound runnables.
  1094|     Batching is implemented by invoking the batch method on each component of the
  1095|     RunnableSequence in order.
  1096|     A RunnableSequence preserves the streaming properties of its components, so if all
  1097|     components of the sequence implement a `transform` method -- which
  1098|     is the method that implements the logic to map a streaming input to a streaming
  1099|     output -- then the sequence will be able to stream input to output!
  1100|     If any component of the sequence does not implement transform then the
  1101|     streaming will only begin after this component is run. If there are
  1102|     multiple blocking components, streaming begins after the last one.
  1103|     Please note: RunnableLambdas do not support `transform` by default! So if

# --- HUNK 8: Lines 1124-1323 ---
  1124|         .. code-block:: python
  1125|             from langchain_core.output_parsers.json import SimpleJsonOutputParser
  1126|             from langchain_core.chat_models.openai import ChatOpenAI
  1127|             prompt = PromptTemplate.from_template(
  1128|                 'In JSON format, give me a list of {topic} and their '
  1129|                 'corresponding names in French, Spanish and in a '
  1130|                 'Cat Language.'
  1131|             )
  1132|             model = ChatOpenAI()
  1133|             chain = prompt | model | SimpleJsonOutputParser()
  1134|             async for chunk in chain.astream({'topic': 'colors'}):
  1135|                 print('-')
  1136|                 print(chunk, sep='', flush=True)
  1137|     """
  1138|     first: Runnable[Input, Any]
  1139|     """The first runnable in the sequence."""
  1140|     middle: List[Runnable[Any, Any]] = Field(default_factory=list)
  1141|     """The middle runnables in the sequence."""
  1142|     last: Runnable[Any, Output]
  1143|     """The last runnable in the sequence."""
  1144|     @classmethod
  1145|     def get_lc_namespace(cls) -> List[str]:
  1146|         """Get the namespace of the langchain object."""
  1147|         return ["langchain", "schema", "runnable"]
  1148|     @property
  1149|     def steps(self) -> List[Runnable[Any, Any]]:
  1150|         """All the runnables that make up the sequence in order."""
  1151|         return [self.first] + self.middle + [self.last]
  1152|     @classmethod
  1153|     def is_lc_serializable(cls) -> bool:
  1154|         return True
  1155|     class Config:
  1156|         arbitrary_types_allowed = True
  1157|     @property
  1158|     def InputType(self) -> Type[Input]:
  1159|         return self.first.InputType
  1160|     @property
  1161|     def OutputType(self) -> Type[Output]:
  1162|         return self.last.OutputType
  1163|     def get_input_schema(
  1164|         self, config: Optional[RunnableConfig] = None
  1165|     ) -> Type[BaseModel]:
  1166|         from langchain_core.runnables.passthrough import RunnableAssign
  1167|         if isinstance(self.first, RunnableAssign):
  1168|             first = cast(RunnableAssign, self.first)
  1169|             next_ = self.middle[0] if self.middle else self.last
  1170|             next_input_schema = next_.get_input_schema(config)
  1171|             if not next_input_schema.__custom_root_type__:
  1172|                 return create_model(  # type: ignore[call-overload]
  1173|                     "RunnableSequenceInput",
  1174|                     **{
  1175|                         k: (v.annotation, v.default)
  1176|                         for k, v in next_input_schema.__fields__.items()
  1177|                         if k not in first.mapper.steps
  1178|                     },
  1179|                 )
  1180|         return self.first.get_input_schema(config)
  1181|     def get_output_schema(
  1182|         self, config: Optional[RunnableConfig] = None
  1183|     ) -> Type[BaseModel]:
  1184|         return self.last.get_output_schema(config)
  1185|     @property
  1186|     def config_specs(self) -> List[ConfigurableFieldSpec]:
  1187|         from langchain_core.beta.runnables.context import (
  1188|             CONTEXT_CONFIG_PREFIX,
  1189|             _key_from_id,
  1190|         )
  1191|         all_specs = [
  1192|             (spec, idx)
  1193|             for idx, step in enumerate(self.steps)
  1194|             for spec in step.config_specs
  1195|         ]
  1196|         specs_by_pos = groupby(
  1197|             [tup for tup in all_specs if tup[0].id.startswith(CONTEXT_CONFIG_PREFIX)],
  1198|             lambda x: x[1],
  1199|         )
  1200|         next_deps: Set[str] = set()
  1201|         deps_by_pos: Dict[int, Set[str]] = {}
  1202|         for pos, specs in specs_by_pos:
  1203|             deps_by_pos[pos] = next_deps
  1204|             next_deps = next_deps | {spec[0].id for spec in specs}
  1205|         for pos, (spec, idx) in enumerate(all_specs):
  1206|             if spec.id.startswith(CONTEXT_CONFIG_PREFIX):
  1207|                 all_specs[pos] = (
  1208|                     ConfigurableFieldSpec(
  1209|                         id=spec.id,
  1210|                         annotation=spec.annotation,
  1211|                         name=spec.name,
  1212|                         default=spec.default,
  1213|                         description=spec.description,
  1214|                         is_shared=spec.is_shared,
  1215|                         dependencies=[
  1216|                             d
  1217|                             for d in deps_by_pos[idx]
  1218|                             if _key_from_id(d) != _key_from_id(spec.id)
  1219|                         ]
  1220|                         + (spec.dependencies or []),
  1221|                     ),
  1222|                     idx,
  1223|                 )
  1224|         return get_unique_config_specs(spec for spec, _ in all_specs)
  1225|     def __repr__(self) -> str:
  1226|         return "\n| ".join(
  1227|             repr(s) if i == 0 else indent_lines_after_first(repr(s), "| ")
  1228|             for i, s in enumerate(self.steps)
  1229|         )
  1230|     def __or__(
  1231|         self,
  1232|         other: Union[
  1233|             Runnable[Any, Other],
  1234|             Callable[[Any], Other],
  1235|             Callable[[Iterator[Any]], Iterator[Other]],
  1236|             Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],
  1237|         ],
  1238|     ) -> RunnableSerializable[Input, Other]:
  1239|         if isinstance(other, RunnableSequence):
  1240|             return RunnableSequence(
  1241|                 first=self.first,
  1242|                 middle=self.middle + [self.last] + [other.first] + other.middle,
  1243|                 last=other.last,
  1244|             )
  1245|         else:
  1246|             return RunnableSequence(
  1247|                 first=self.first,
  1248|                 middle=self.middle + [self.last],
  1249|                 last=coerce_to_runnable(other),
  1250|             )
  1251|     def __ror__(
  1252|         self,
  1253|         other: Union[
  1254|             Runnable[Other, Any],
  1255|             Callable[[Other], Any],
  1256|             Callable[[Iterator[Other]], Iterator[Any]],
  1257|             Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]],
  1258|         ],
  1259|     ) -> RunnableSerializable[Other, Output]:
  1260|         if isinstance(other, RunnableSequence):
  1261|             return RunnableSequence(
  1262|                 first=other.first,
  1263|                 middle=other.middle + [other.last] + [self.first] + self.middle,
  1264|                 last=self.last,
  1265|             )
  1266|         else:
  1267|             return RunnableSequence(
  1268|                 first=coerce_to_runnable(other),
  1269|                 middle=[self.first] + self.middle,
  1270|                 last=self.last,
  1271|             )
  1272|     def invoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output:
  1273|         from langchain_core.beta.runnables.context import config_with_context
  1274|         config = config_with_context(ensure_config(config), self.steps)
  1275|         callback_manager = get_callback_manager_for_config(config)
  1276|         run_manager = callback_manager.on_chain_start(
  1277|             dumpd(self), input, name=config.get("run_name")
  1278|         )
  1279|         try:
  1280|             for i, step in enumerate(self.steps):
  1281|                 input = step.invoke(
  1282|                     input,
  1283|                     patch_config(
  1284|                         config, callbacks=run_manager.get_child(f"seq:step:{i+1}")
  1285|                     ),
  1286|                 )
  1287|         except BaseException as e:
  1288|             run_manager.on_chain_error(e)
  1289|             raise
  1290|         else:
  1291|             run_manager.on_chain_end(input)
  1292|             return cast(Output, input)
  1293|     async def ainvoke(
  1294|         self,
  1295|         input: Input,
  1296|         config: Optional[RunnableConfig] = None,
  1297|         **kwargs: Optional[Any],
  1298|     ) -> Output:
  1299|         from langchain_core.beta.runnables.context import aconfig_with_context
  1300|         config = aconfig_with_context(ensure_config(config), self.steps)
  1301|         callback_manager = get_async_callback_manager_for_config(config)
  1302|         run_manager = await callback_manager.on_chain_start(
  1303|             dumpd(self), input, name=config.get("run_name")
  1304|         )
  1305|         try:
  1306|             for i, step in enumerate(self.steps):
  1307|                 input = await step.ainvoke(
  1308|                     input,
  1309|                     patch_config(
  1310|                         config, callbacks=run_manager.get_child(f"seq:step:{i+1}")
  1311|                     ),
  1312|                 )
  1313|         except BaseException as e:
  1314|             await run_manager.on_chain_error(e)
  1315|             raise
  1316|         else:
  1317|             await run_manager.on_chain_end(input)
  1318|             return cast(Output, input)
  1319|     def batch(
  1320|         self,
  1321|         inputs: List[Input],
  1322|         config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,
  1323|         *,

# --- HUNK 9: Lines 1331-1371 ---
  1331|         configs = [
  1332|             config_with_context(c, self.steps)
  1333|             for c in get_config_list(config, len(inputs))
  1334|         ]
  1335|         callback_managers = [
  1336|             CallbackManager.configure(
  1337|                 inheritable_callbacks=config.get("callbacks"),
  1338|                 local_callbacks=None,
  1339|                 verbose=False,
  1340|                 inheritable_tags=config.get("tags"),
  1341|                 local_tags=None,
  1342|                 inheritable_metadata=config.get("metadata"),
  1343|                 local_metadata=None,
  1344|             )
  1345|             for config in configs
  1346|         ]
  1347|         run_managers = [
  1348|             cm.on_chain_start(
  1349|                 dumpd(self),
  1350|                 input,
  1351|                 name=config.get("run_name"),
  1352|             )
  1353|             for cm, input, config in zip(callback_managers, inputs, configs)
  1354|         ]
  1355|         try:
  1356|             if return_exceptions:
  1357|                 failed_inputs_map: Dict[int, Exception] = {}
  1358|                 for stepidx, step in enumerate(self.steps):
  1359|                     remaining_idxs = [
  1360|                         i for i in range(len(configs)) if i not in failed_inputs_map
  1361|                     ]
  1362|                     inputs = step.batch(
  1363|                         [
  1364|                             inp
  1365|                             for i, inp in zip(remaining_idxs, inputs)
  1366|                             if i not in failed_inputs_map
  1367|                         ],
  1368|                         [
  1369|                             patch_config(
  1370|                                 config, callbacks=rm.get_child(f"seq:step:{stepidx+1}")
  1371|                             )

# --- HUNK 10: Lines 1434-1474 ---
  1434|             aconfig_with_context(c, self.steps)
  1435|             for c in get_config_list(config, len(inputs))
  1436|         ]
  1437|         callback_managers = [
  1438|             AsyncCallbackManager.configure(
  1439|                 inheritable_callbacks=config.get("callbacks"),
  1440|                 local_callbacks=None,
  1441|                 verbose=False,
  1442|                 inheritable_tags=config.get("tags"),
  1443|                 local_tags=None,
  1444|                 inheritable_metadata=config.get("metadata"),
  1445|                 local_metadata=None,
  1446|             )
  1447|             for config in configs
  1448|         ]
  1449|         run_managers: List[AsyncCallbackManagerForChainRun] = await asyncio.gather(
  1450|             *(
  1451|                 cm.on_chain_start(
  1452|                     dumpd(self),
  1453|                     input,
  1454|                     name=config.get("run_name"),
  1455|                 )
  1456|                 for cm, input, config in zip(callback_managers, inputs, configs)
  1457|             )
  1458|         )
  1459|         try:
  1460|             if return_exceptions:
  1461|                 failed_inputs_map: Dict[int, Exception] = {}
  1462|                 for stepidx, step in enumerate(self.steps):
  1463|                     remaining_idxs = [
  1464|                         i for i in range(len(configs)) if i not in failed_inputs_map
  1465|                     ]
  1466|                     inputs = await step.abatch(
  1467|                         [
  1468|                             inp
  1469|                             for i, inp in zip(remaining_idxs, inputs)
  1470|                             if i not in failed_inputs_map
  1471|                         ],
  1472|                         [
  1473|                             patch_config(
  1474|                                 config, callbacks=rm.get_child(f"seq:step:{stepidx+1}")

# --- HUNK 11: Lines 1553-1609 ---
  1553|         steps = [self.first] + self.middle + [self.last]
  1554|         config = aconfig_with_context(config, self.steps)
  1555|         final_pipeline = cast(AsyncIterator[Output], input)
  1556|         for step in steps:
  1557|             final_pipeline = step.atransform(
  1558|                 final_pipeline,
  1559|                 patch_config(
  1560|                     config,
  1561|                     callbacks=run_manager.get_child(f"seq:step:{steps.index(step)+1}"),
  1562|                 ),
  1563|             )
  1564|         async for output in final_pipeline:
  1565|             yield output
  1566|     def transform(
  1567|         self,
  1568|         input: Iterator[Input],
  1569|         config: Optional[RunnableConfig] = None,
  1570|         **kwargs: Optional[Any],
  1571|     ) -> Iterator[Output]:
  1572|         yield from self._transform_stream_with_config(
  1573|             input, self._transform, config, **kwargs
  1574|         )
  1575|     def stream(
  1576|         self,
  1577|         input: Input,
  1578|         config: Optional[RunnableConfig] = None,
  1579|         **kwargs: Optional[Any],
  1580|     ) -> Iterator[Output]:
  1581|         yield from self.transform(iter([input]), config, **kwargs)
  1582|     async def atransform(
  1583|         self,
  1584|         input: AsyncIterator[Input],
  1585|         config: Optional[RunnableConfig] = None,
  1586|         **kwargs: Optional[Any],
  1587|     ) -> AsyncIterator[Output]:
  1588|         async for chunk in self._atransform_stream_with_config(
  1589|             input, self._atransform, config, **kwargs
  1590|         ):
  1591|             yield chunk
  1592|     async def astream(
  1593|         self,
  1594|         input: Input,
  1595|         config: Optional[RunnableConfig] = None,
  1596|         **kwargs: Optional[Any],
  1597|     ) -> AsyncIterator[Output]:
  1598|         async def input_aiter() -> AsyncIterator[Input]:
  1599|             yield input
  1600|         async for chunk in self.atransform(input_aiter(), config, **kwargs):
  1601|             yield chunk
  1602| class RunnableParallel(RunnableSerializable[Input, Dict[str, Any]]):
  1603|     """
  1604|     A runnable that runs a mapping of runnables in parallel,
  1605|     and returns a mapping of their outputs.
  1606|     """
  1607|     steps: Mapping[str, Runnable[Input, Any]]
  1608|     def __init__(
  1609|         self,

# --- HUNK 12: Lines 1620-1748 ---
  1620|         **kwargs: Union[
  1621|             Runnable[Input, Any],
  1622|             Callable[[Input], Any],
  1623|             Mapping[str, Union[Runnable[Input, Any], Callable[[Input], Any]]],
  1624|         ],
  1625|     ) -> None:
  1626|         merged = {**__steps} if __steps is not None else {}
  1627|         merged.update(kwargs)
  1628|         super().__init__(
  1629|             steps={key: coerce_to_runnable(r) for key, r in merged.items()}
  1630|         )
  1631|     @classmethod
  1632|     def is_lc_serializable(cls) -> bool:
  1633|         return True
  1634|     @classmethod
  1635|     def get_lc_namespace(cls) -> List[str]:
  1636|         """Get the namespace of the langchain object."""
  1637|         return ["langchain", "schema", "runnable"]
  1638|     class Config:
  1639|         arbitrary_types_allowed = True
  1640|     @property
  1641|     def InputType(self) -> Any:
  1642|         for step in self.steps.values():
  1643|             if step.InputType:
  1644|                 return step.InputType
  1645|         return Any
  1646|     def get_input_schema(
  1647|         self, config: Optional[RunnableConfig] = None
  1648|     ) -> Type[BaseModel]:
  1649|         if all(
  1650|             s.get_input_schema(config).schema().get("type", "object") == "object"
  1651|             for s in self.steps.values()
  1652|         ):
  1653|             return create_model(  # type: ignore[call-overload]
  1654|                 "RunnableParallelInput",
  1655|                 **{
  1656|                     k: (v.annotation, v.default)
  1657|                     for step in self.steps.values()
  1658|                     for k, v in step.get_input_schema(config).__fields__.items()
  1659|                     if k != "__root__"
  1660|                 },
  1661|             )
  1662|         return super().get_input_schema(config)
  1663|     def get_output_schema(
  1664|         self, config: Optional[RunnableConfig] = None
  1665|     ) -> Type[BaseModel]:
  1666|         return create_model(  # type: ignore[call-overload]
  1667|             "RunnableParallelOutput",
  1668|             **{k: (v.OutputType, None) for k, v in self.steps.items()},
  1669|         )
  1670|     @property
  1671|     def config_specs(self) -> List[ConfigurableFieldSpec]:
  1672|         return get_unique_config_specs(
  1673|             spec for step in self.steps.values() for spec in step.config_specs
  1674|         )
  1675|     def __repr__(self) -> str:
  1676|         map_for_repr = ",\n  ".join(
  1677|             f"{k}: {indent_lines_after_first(repr(v), '  ' + k + ': ')}"
  1678|             for k, v in self.steps.items()
  1679|         )
  1680|         return "{\n  " + map_for_repr + "\n}"
  1681|     def invoke(
  1682|         self, input: Input, config: Optional[RunnableConfig] = None
  1683|     ) -> Dict[str, Any]:
  1684|         from langchain_core.callbacks.manager import CallbackManager
  1685|         config = ensure_config(config)
  1686|         callback_manager = CallbackManager.configure(
  1687|             inheritable_callbacks=config.get("callbacks"),
  1688|             local_callbacks=None,
  1689|             verbose=False,
  1690|             inheritable_tags=config.get("tags"),
  1691|             local_tags=None,
  1692|             inheritable_metadata=config.get("metadata"),
  1693|             local_metadata=None,
  1694|         )
  1695|         run_manager = callback_manager.on_chain_start(
  1696|             dumpd(self), input, name=config.get("run_name")
  1697|         )
  1698|         try:
  1699|             steps = dict(self.steps)
  1700|             with get_executor_for_config(config) as executor:
  1701|                 futures = [
  1702|                     executor.submit(
  1703|                         step.invoke,
  1704|                         input,
  1705|                         patch_config(
  1706|                             config,
  1707|                             callbacks=run_manager.get_child(f"map:key:{key}"),
  1708|                         ),
  1709|                     )
  1710|                     for key, step in steps.items()
  1711|                 ]
  1712|                 output = {key: future.result() for key, future in zip(steps, futures)}
  1713|         except BaseException as e:
  1714|             run_manager.on_chain_error(e)
  1715|             raise
  1716|         else:
  1717|             run_manager.on_chain_end(output)
  1718|             return output
  1719|     async def ainvoke(
  1720|         self,
  1721|         input: Input,
  1722|         config: Optional[RunnableConfig] = None,
  1723|         **kwargs: Optional[Any],
  1724|     ) -> Dict[str, Any]:
  1725|         config = ensure_config(config)
  1726|         callback_manager = get_async_callback_manager_for_config(config)
  1727|         run_manager = await callback_manager.on_chain_start(
  1728|             dumpd(self), input, name=config.get("run_name")
  1729|         )
  1730|         try:
  1731|             steps = dict(self.steps)
  1732|             results = await asyncio.gather(
  1733|                 *(
  1734|                     step.ainvoke(
  1735|                         input,
  1736|                         patch_config(
  1737|                             config, callbacks=run_manager.get_child(f"map:key:{key}")
  1738|                         ),
  1739|                     )
  1740|                     for key, step in steps.items()
  1741|                 )
  1742|             )
  1743|             output = {key: value for key, value in zip(steps, results)}
  1744|         except BaseException as e:
  1745|             await run_manager.on_chain_error(e)
  1746|             raise
  1747|         else:
  1748|             await run_manager.on_chain_end(output)

# --- HUNK 13: Lines 1904-1944 ---
  1904|         try:
  1905|             sig = inspect.signature(func)
  1906|             return (
  1907|                 getattr(sig.return_annotation, "__args__", (Any,))[0]
  1908|                 if sig.return_annotation != inspect.Signature.empty
  1909|                 else Any
  1910|             )
  1911|         except ValueError:
  1912|             return Any
  1913|     def __eq__(self, other: Any) -> bool:
  1914|         if isinstance(other, RunnableGenerator):
  1915|             if hasattr(self, "_transform") and hasattr(other, "_transform"):
  1916|                 return self._transform == other._transform
  1917|             elif hasattr(self, "_atransform") and hasattr(other, "_atransform"):
  1918|                 return self._atransform == other._atransform
  1919|             else:
  1920|                 return False
  1921|         else:
  1922|             return False
  1923|     def __repr__(self) -> str:
  1924|         return "RunnableGenerator(...)"
  1925|     def transform(
  1926|         self,
  1927|         input: Iterator[Input],
  1928|         config: Optional[RunnableConfig] = None,
  1929|         **kwargs: Any,
  1930|     ) -> Iterator[Output]:
  1931|         return self._transform_stream_with_config(
  1932|             input, self._transform, config, **kwargs
  1933|         )
  1934|     def stream(
  1935|         self,
  1936|         input: Input,
  1937|         config: Optional[RunnableConfig] = None,
  1938|         **kwargs: Any,
  1939|     ) -> Iterator[Output]:
  1940|         return self.transform(iter([input]), config, **kwargs)
  1941|     def invoke(
  1942|         self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
  1943|     ) -> Output:
  1944|         final = None

# --- HUNK 14: Lines 2023-2140 ---
  2023|             Union[
  2024|                 Callable[[Input], Awaitable[Output]],
  2025|                 Callable[[Input, RunnableConfig], Awaitable[Output]],
  2026|                 Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],
  2027|                 Callable[
  2028|                     [Input, AsyncCallbackManagerForChainRun, RunnableConfig],
  2029|                     Awaitable[Output],
  2030|                 ],
  2031|             ]
  2032|         ] = None,
  2033|     ) -> None:
  2034|         """Create a RunnableLambda from a callable, and async callable or both.
  2035|         Accepts both sync and async variants to allow providing efficient
  2036|         implementations for sync and async execution.
  2037|         Args:
  2038|             func: Either sync or async callable
  2039|             afunc: An async callable that takes an input and returns an output.
  2040|         """
  2041|         if afunc is not None:
  2042|             self.afunc = afunc
  2043|         if inspect.iscoroutinefunction(func):
  2044|             if afunc is not None:
  2045|                 raise TypeError(
  2046|                     "Func was provided as a coroutine function, but afunc was "
  2047|                     "also provided. If providing both, func should be a regular "
  2048|                     "function to avoid ambiguity."
  2049|                 )
  2050|             self.afunc = func
  2051|         elif callable(func):
  2052|             self.func = cast(Callable[[Input], Output], func)
  2053|         else:
  2054|             raise TypeError(
  2055|                 "Expected a callable type for `func`."
  2056|                 f"Instead got an unsupported type: {type(func)}"
  2057|             )
  2058|     @property
  2059|     def InputType(self) -> Any:
  2060|         """The type of the input to this runnable."""
  2061|         func = getattr(self, "func", None) or getattr(self, "afunc")
  2062|         try:
  2063|             params = inspect.signature(func).parameters
  2064|             first_param = next(iter(params.values()), None)
  2065|             if first_param and first_param.annotation != inspect.Parameter.empty:
  2066|                 return first_param.annotation
  2067|             else:
  2068|                 return Any
  2069|         except ValueError:
  2070|             return Any
  2071|     def get_input_schema(
  2072|         self, config: Optional[RunnableConfig] = None
  2073|     ) -> Type[BaseModel]:
  2074|         """The pydantic schema for the input to this runnable."""
  2075|         func = getattr(self, "func", None) or getattr(self, "afunc")
  2076|         if isinstance(func, itemgetter):
  2077|             items = str(func).replace("operator.itemgetter(", "")[:-1].split(", ")
  2078|             if all(
  2079|                 item[0] == "'" and item[-1] == "'" and len(item) > 2 for item in items
  2080|             ):
  2081|                 return create_model(
  2082|                     "RunnableLambdaInput",
  2083|                     **{item[1:-1]: (Any, None) for item in items},  # type: ignore
  2084|                 )
  2085|             else:
  2086|                 return create_model("RunnableLambdaInput", __root__=(List[Any], None))
  2087|         if self.InputType != Any:
  2088|             return super().get_input_schema(config)
  2089|         if dict_keys := get_function_first_arg_dict_keys(func):
  2090|             return create_model(
  2091|                 "RunnableLambdaInput",
  2092|                 **{key: (Any, None) for key in dict_keys},  # type: ignore
  2093|             )
  2094|         return super().get_input_schema(config)
  2095|     @property
  2096|     def OutputType(self) -> Any:
  2097|         """The type of the output of this runnable as a type annotation."""
  2098|         func = getattr(self, "func", None) or getattr(self, "afunc")
  2099|         try:
  2100|             sig = inspect.signature(func)
  2101|             return (
  2102|                 sig.return_annotation
  2103|                 if sig.return_annotation != inspect.Signature.empty
  2104|                 else Any
  2105|             )
  2106|         except ValueError:
  2107|             return Any
  2108|     def __eq__(self, other: Any) -> bool:
  2109|         if isinstance(other, RunnableLambda):
  2110|             if hasattr(self, "func") and hasattr(other, "func"):
  2111|                 return self.func == other.func
  2112|             elif hasattr(self, "afunc") and hasattr(other, "afunc"):
  2113|                 return self.afunc == other.afunc
  2114|             else:
  2115|                 return False
  2116|         else:
  2117|             return False
  2118|     def __repr__(self) -> str:
  2119|         """A string representation of this runnable."""
  2120|         if hasattr(self, "func"):
  2121|             return f"RunnableLambda({get_lambda_source(self.func) or '...'})"
  2122|         elif hasattr(self, "afunc"):
  2123|             return f"RunnableLambda(afunc={get_lambda_source(self.afunc) or '...'})"
  2124|         else:
  2125|             return "RunnableLambda(...)"
  2126|     def _invoke(
  2127|         self,
  2128|         input: Input,
  2129|         run_manager: CallbackManagerForChainRun,
  2130|         config: RunnableConfig,
  2131|         **kwargs: Any,
  2132|     ) -> Output:
  2133|         output = call_func_with_variable_args(
  2134|             self.func, input, config, run_manager, **kwargs
  2135|         )
  2136|         if isinstance(output, Runnable):
  2137|             recursion_limit = config["recursion_limit"]
  2138|             if recursion_limit <= 0:
  2139|                 raise RecursionError(
  2140|                     f"Recursion limit reached when invoking {self} with input {input}."

# --- HUNK 15: Lines 2143-2185 ---
  2143|                 input,
  2144|                 patch_config(
  2145|                     config,
  2146|                     callbacks=run_manager.get_child(),
  2147|                     recursion_limit=recursion_limit - 1,
  2148|                 ),
  2149|             )
  2150|         return output
  2151|     async def _ainvoke(
  2152|         self,
  2153|         input: Input,
  2154|         run_manager: AsyncCallbackManagerForChainRun,
  2155|         config: RunnableConfig,
  2156|         **kwargs: Any,
  2157|     ) -> Output:
  2158|         if hasattr(self, "afunc"):
  2159|             afunc = self.afunc
  2160|         else:
  2161|             @wraps(self.func)
  2162|             async def f(*args, **kwargs):  # type: ignore[no-untyped-def]
  2163|                 return await asyncio.get_running_loop().run_in_executor(
  2164|                     None, partial(self.func, **kwargs), *args
  2165|                 )
  2166|             afunc = f
  2167|         output = await acall_func_with_variable_args(
  2168|             afunc, input, config, run_manager, **kwargs
  2169|         )
  2170|         if isinstance(output, Runnable):
  2171|             recursion_limit = config["recursion_limit"]
  2172|             if recursion_limit <= 0:
  2173|                 raise RecursionError(
  2174|                     f"Recursion limit reached when invoking {self} with input {input}."
  2175|                 )
  2176|             output = await output.ainvoke(
  2177|                 input,
  2178|                 patch_config(
  2179|                     config,
  2180|                     callbacks=run_manager.get_child(),
  2181|                     recursion_limit=recursion_limit - 1,
  2182|                 ),
  2183|             )
  2184|         return output
  2185|     def _config(

# --- HUNK 16: Lines 2210-2288 ---
  2210|             )
  2211|         else:
  2212|             raise TypeError(
  2213|                 "Cannot invoke a coroutine function synchronously."
  2214|                 "Use `ainvoke` instead."
  2215|             )
  2216|     async def ainvoke(
  2217|         self,
  2218|         input: Input,
  2219|         config: Optional[RunnableConfig] = None,
  2220|         **kwargs: Optional[Any],
  2221|     ) -> Output:
  2222|         """Invoke this runnable asynchronously."""
  2223|         the_func = self.afunc if hasattr(self, "afunc") else self.func
  2224|         return await self._acall_with_config(
  2225|             self._ainvoke,
  2226|             input,
  2227|             self._config(config, the_func),
  2228|             **kwargs,
  2229|         )
  2230| class RunnableEachBase(RunnableSerializable[List[Input], List[Output]]):
  2231|     """
  2232|     A runnable that delegates calls to another runnable
  2233|     with each element of the input sequence.
  2234|     Use only if creating a new RunnableEach subclass with different __init__ args.
  2235|     """
  2236|     bound: Runnable[Input, Output]
  2237|     class Config:
  2238|         arbitrary_types_allowed = True
  2239|     @property
  2240|     def InputType(self) -> Any:
  2241|         return List[self.bound.InputType]  # type: ignore[name-defined]
  2242|     def get_input_schema(
  2243|         self, config: Optional[RunnableConfig] = None
  2244|     ) -> Type[BaseModel]:
  2245|         return create_model(
  2246|             "RunnableEachInput",
  2247|             __root__=(
  2248|                 List[self.bound.get_input_schema(config)],  # type: ignore
  2249|                 None,
  2250|             ),
  2251|         )
  2252|     @property
  2253|     def OutputType(self) -> Type[List[Output]]:
  2254|         return List[self.bound.OutputType]  # type: ignore[name-defined]
  2255|     def get_output_schema(
  2256|         self, config: Optional[RunnableConfig] = None
  2257|     ) -> Type[BaseModel]:
  2258|         schema = self.bound.get_output_schema(config)
  2259|         return create_model(
  2260|             "RunnableEachOutput",
  2261|             __root__=(
  2262|                 List[schema],  # type: ignore
  2263|                 None,
  2264|             ),
  2265|         )
  2266|     @property
  2267|     def config_specs(self) -> List[ConfigurableFieldSpec]:
  2268|         return self.bound.config_specs
  2269|     @classmethod
  2270|     def is_lc_serializable(cls) -> bool:
  2271|         return True
  2272|     @classmethod
  2273|     def get_lc_namespace(cls) -> List[str]:
  2274|         """Get the namespace of the langchain object."""
  2275|         return ["langchain", "schema", "runnable"]
  2276|     def _invoke(
  2277|         self,
  2278|         inputs: List[Input],
  2279|         run_manager: CallbackManagerForChainRun,
  2280|         config: RunnableConfig,
  2281|         **kwargs: Any,
  2282|     ) -> List[Output]:
  2283|         return self.bound.batch(
  2284|             inputs, patch_config(config, callbacks=run_manager.get_child()), **kwargs
  2285|         )
  2286|     def invoke(
  2287|         self, input: List[Input], config: Optional[RunnableConfig] = None, **kwargs: Any
  2288|     ) -> List[Output]:

# --- HUNK 17: Lines 2293-2332 ---
  2293|         run_manager: AsyncCallbackManagerForChainRun,
  2294|         config: RunnableConfig,
  2295|         **kwargs: Any,
  2296|     ) -> List[Output]:
  2297|         return await self.bound.abatch(
  2298|             inputs, patch_config(config, callbacks=run_manager.get_child()), **kwargs
  2299|         )
  2300|     async def ainvoke(
  2301|         self, input: List[Input], config: Optional[RunnableConfig] = None, **kwargs: Any
  2302|     ) -> List[Output]:
  2303|         return await self._acall_with_config(self._ainvoke, input, config, **kwargs)
  2304| class RunnableEach(RunnableEachBase[Input, Output]):
  2305|     """
  2306|     A runnable that delegates calls to another runnable
  2307|     with each element of the input sequence.
  2308|     """
  2309|     @classmethod
  2310|     def get_lc_namespace(cls) -> List[str]:
  2311|         """Get the namespace of the langchain object."""
  2312|         return ["langchain", "schema", "runnable"]
  2313|     def bind(self, **kwargs: Any) -> RunnableEach[Input, Output]:
  2314|         return RunnableEach(bound=self.bound.bind(**kwargs))
  2315|     def with_config(
  2316|         self, config: Optional[RunnableConfig] = None, **kwargs: Any
  2317|     ) -> RunnableEach[Input, Output]:
  2318|         return RunnableEach(bound=self.bound.with_config(config, **kwargs))
  2319|     def with_listeners(
  2320|         self,
  2321|         *,
  2322|         on_start: Optional[Listener] = None,
  2323|         on_end: Optional[Listener] = None,
  2324|         on_error: Optional[Listener] = None,
  2325|     ) -> RunnableEach[Input, Output]:
  2326|         """
  2327|         Bind lifecycle listeners to a Runnable, returning a new Runnable.
  2328|         on_start: Called before the runnable starts running, with the Run object.
  2329|         on_end: Called after the runnable finishes running, with the Run object.
  2330|         on_error: Called if the runnable throws an error, with the Run object.
  2331|         The Run object contains information about the run, including its id,
  2332|         type, input, output, error, start_time, end_time, and any tags or metadata

# --- HUNK 18: Lines 2394-2462 ---
  2394|             **other_kwargs: Unpacked into the base class.
  2395|         """
  2396|         config = config or {}
  2397|         if configurable := config.get("configurable", None):
  2398|             allowed_keys = set(s.id for s in bound.config_specs)
  2399|             for key in configurable:
  2400|                 if key not in allowed_keys:
  2401|                     raise ValueError(
  2402|                         f"Configurable key '{key}' not found in runnable with"
  2403|                         f" config keys: {allowed_keys}"
  2404|                     )
  2405|         super().__init__(
  2406|             bound=bound,
  2407|             kwargs=kwargs or {},
  2408|             config=config or {},
  2409|             config_factories=config_factories or [],
  2410|             custom_input_type=custom_input_type,
  2411|             custom_output_type=custom_output_type,
  2412|             **other_kwargs,
  2413|         )
  2414|     @property
  2415|     def InputType(self) -> Type[Input]:
  2416|         return (
  2417|             cast(Type[Input], self.custom_input_type)
  2418|             if self.custom_input_type is not None
  2419|             else self.bound.InputType
  2420|         )
  2421|     @property
  2422|     def OutputType(self) -> Type[Output]:
  2423|         return (
  2424|             cast(Type[Output], self.custom_output_type)
  2425|             if self.custom_output_type is not None
  2426|             else self.bound.OutputType
  2427|         )
  2428|     def get_input_schema(
  2429|         self, config: Optional[RunnableConfig] = None
  2430|     ) -> Type[BaseModel]:
  2431|         if self.custom_input_type is not None:
  2432|             return super().get_input_schema(config)
  2433|         return self.bound.get_input_schema(merge_configs(self.config, config))
  2434|     def get_output_schema(
  2435|         self, config: Optional[RunnableConfig] = None
  2436|     ) -> Type[BaseModel]:
  2437|         if self.custom_output_type is not None:
  2438|             return super().get_output_schema(config)
  2439|         return self.bound.get_output_schema(merge_configs(self.config, config))
  2440|     @property
  2441|     def config_specs(self) -> List[ConfigurableFieldSpec]:
  2442|         return self.bound.config_specs
  2443|     @classmethod
  2444|     def is_lc_serializable(cls) -> bool:
  2445|         return True
  2446|     @classmethod
  2447|     def get_lc_namespace(cls) -> List[str]:
  2448|         """Get the namespace of the langchain object."""
  2449|         return ["langchain", "schema", "runnable"]
  2450|     def _merge_configs(self, *configs: Optional[RunnableConfig]) -> RunnableConfig:
  2451|         config = merge_configs(self.config, *configs)
  2452|         return merge_configs(config, *(f(config) for f in self.config_factories))
  2453|     def invoke(
  2454|         self,
  2455|         input: Input,
  2456|         config: Optional[RunnableConfig] = None,
  2457|         **kwargs: Optional[Any],
  2458|     ) -> Output:
  2459|         return self.bound.invoke(
  2460|             input,
  2461|             self._merge_configs(config),
  2462|             **{**self.kwargs, **kwargs},


# ====================================================================
# FILE: libs/core/langchain_core/runnables/branch.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-44 ---
     1| from typing import (
     2|     Any,
     3|     Awaitable,
     4|     Callable,
     5|     List,
     6|     Mapping,
     7|     Optional,
     8|     Sequence,
     9|     Tuple,
    10|     Type,
    11|     Union,
    12|     cast,
    13| )
    14| from langchain_core.load.dump import dumpd
    15| from langchain_core.pydantic_v1 import BaseModel
    16| from langchain_core.runnables.base import (
    17|     Runnable,
    18|     RunnableLike,
    19|     RunnableSerializable,
    20|     coerce_to_runnable,
    21| )
    22| from langchain_core.runnables.config import (
    23|     RunnableConfig,
    24|     ensure_config,
    25|     get_callback_manager_for_config,
    26|     patch_config,
    27| )
    28| from langchain_core.runnables.utils import (
    29|     ConfigurableFieldSpec,
    30|     Input,
    31|     Output,
    32|     get_unique_config_specs,
    33| )
    34| class RunnableBranch(RunnableSerializable[Input, Output]):
    35|     """A Runnable that selects which branch to run based on a condition.
    36|     The runnable is initialized with a list of (condition, runnable) pairs and
    37|     a default branch.
    38|     When operating on an input, the first condition that evaluates to True is
    39|     selected, and the corresponding runnable is run on the input.
    40|     If no condition evaluates to True, the default branch is run on the input.
    41|     Examples:
    42|         .. code-block:: python
    43|             from langchain_core.runnables import RunnableBranch
    44|             branch = RunnableBranch(

# --- HUNK 2: Lines 163-231 ---
   163|                     ),
   164|                 )
   165|                 if expression_value:
   166|                     output = runnable.invoke(
   167|                         input,
   168|                         config=patch_config(
   169|                             config,
   170|                             callbacks=run_manager.get_child(tag=f"branch:{idx + 1}"),
   171|                         ),
   172|                         **kwargs,
   173|                     )
   174|                     break
   175|             else:
   176|                 output = self.default.invoke(
   177|                     input,
   178|                     config=patch_config(
   179|                         config, callbacks=run_manager.get_child(tag="branch:default")
   180|                     ),
   181|                     **kwargs,
   182|                 )
   183|         except Exception as e:
   184|             run_manager.on_chain_error(e)
   185|             raise
   186|         run_manager.on_chain_end(dumpd(output))
   187|         return output
   188|     async def ainvoke(
   189|         self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
   190|     ) -> Output:
   191|         """Async version of invoke."""
   192|         config = ensure_config(config)
   193|         callback_manager = get_callback_manager_for_config(config)
   194|         run_manager = callback_manager.on_chain_start(
   195|             dumpd(self),
   196|             input,
   197|             name=config.get("run_name"),
   198|         )
   199|         try:
   200|             for idx, branch in enumerate(self.branches):
   201|                 condition, runnable = branch
   202|                 expression_value = await condition.ainvoke(
   203|                     input,
   204|                     config=patch_config(
   205|                         config,
   206|                         callbacks=run_manager.get_child(tag=f"condition:{idx + 1}"),
   207|                     ),
   208|                 )
   209|                 if expression_value:
   210|                     output = await runnable.ainvoke(
   211|                         input,
   212|                         config=patch_config(
   213|                             config,
   214|                             callbacks=run_manager.get_child(tag=f"branch:{idx + 1}"),
   215|                         ),
   216|                         **kwargs,
   217|                     )
   218|                     break
   219|             else:
   220|                 output = await self.default.ainvoke(
   221|                     input,
   222|                     config=patch_config(
   223|                         config, callbacks=run_manager.get_child(tag="branch:default")
   224|                     ),
   225|                     **kwargs,
   226|                 )
   227|         except Exception as e:
   228|             run_manager.on_chain_error(e)
   229|             raise
   230|         run_manager.on_chain_end(dumpd(output))
   231|         return output


# ====================================================================
# FILE: libs/core/langchain_core/runnables/config.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-23 ---
     1| from __future__ import annotations
     2| from concurrent.futures import Executor, ThreadPoolExecutor
     3| from contextlib import contextmanager
     4| from typing import (
     5|     TYPE_CHECKING,
     6|     Any,
     7|     Awaitable,
     8|     Callable,
     9|     Dict,
    10|     Generator,
    11|     List,
    12|     Optional,
    13|     Union,
    14|     cast,
    15| )
    16| from typing_extensions import TypedDict
    17| from langchain_core.runnables.utils import (
    18|     Input,
    19|     Output,
    20|     accepts_config,
    21|     accepts_run_manager,
    22| )
    23| if TYPE_CHECKING:

# --- HUNK 2: Lines 303-332 ---
   303|     return CallbackManager.configure(
   304|         inheritable_callbacks=config.get("callbacks"),
   305|         inheritable_tags=config.get("tags"),
   306|         inheritable_metadata=config.get("metadata"),
   307|     )
   308| def get_async_callback_manager_for_config(
   309|     config: RunnableConfig,
   310| ) -> AsyncCallbackManager:
   311|     """Get an async callback manager for a config.
   312|     Args:
   313|         config (RunnableConfig): The config.
   314|     Returns:
   315|         AsyncCallbackManager: The async callback manager.
   316|     """
   317|     from langchain_core.callbacks.manager import AsyncCallbackManager
   318|     return AsyncCallbackManager.configure(
   319|         inheritable_callbacks=config.get("callbacks"),
   320|         inheritable_tags=config.get("tags"),
   321|         inheritable_metadata=config.get("metadata"),
   322|     )
   323| @contextmanager
   324| def get_executor_for_config(config: RunnableConfig) -> Generator[Executor, None, None]:
   325|     """Get an executor for a config.
   326|     Args:
   327|         config (RunnableConfig): The config.
   328|     Yields:
   329|         Generator[Executor, None, None]: The executor.
   330|     """
   331|     with ThreadPoolExecutor(max_workers=config.get("max_concurrency")) as executor:
   332|         yield executor


# ====================================================================
# FILE: libs/core/langchain_core/runnables/configurable.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 7-85 ---
     7|     AsyncIterator,
     8|     Callable,
     9|     Dict,
    10|     Iterator,
    11|     List,
    12|     Optional,
    13|     Sequence,
    14|     Tuple,
    15|     Type,
    16|     Union,
    17|     cast,
    18| )
    19| from weakref import WeakValueDictionary
    20| from langchain_core.pydantic_v1 import BaseModel
    21| from langchain_core.runnables.base import Runnable, RunnableSerializable
    22| from langchain_core.runnables.config import (
    23|     RunnableConfig,
    24|     get_config_list,
    25|     get_executor_for_config,
    26| )
    27| from langchain_core.runnables.utils import (
    28|     AnyConfigurableField,
    29|     ConfigurableField,
    30|     ConfigurableFieldMultiOption,
    31|     ConfigurableFieldSingleOption,
    32|     ConfigurableFieldSpec,
    33|     Input,
    34|     Output,
    35|     gather_with_concurrency,
    36|     get_unique_config_specs,
    37| )
    38| class DynamicRunnable(RunnableSerializable[Input, Output]):
    39|     """A Serializable Runnable that can be dynamically configured."""
    40|     default: RunnableSerializable[Input, Output]
    41|     class Config:
    42|         arbitrary_types_allowed = True
    43|     @classmethod
    44|     def is_lc_serializable(cls) -> bool:
    45|         return True
    46|     @classmethod
    47|     def get_lc_namespace(cls) -> List[str]:
    48|         """Get the namespace of the langchain object."""
    49|         return ["langchain", "schema", "runnable"]
    50|     @property
    51|     def InputType(self) -> Type[Input]:
    52|         return self.default.InputType
    53|     @property
    54|     def OutputType(self) -> Type[Output]:
    55|         return self.default.OutputType
    56|     def get_input_schema(
    57|         self, config: Optional[RunnableConfig] = None
    58|     ) -> Type[BaseModel]:
    59|         runnable, config = self._prepare(config)
    60|         return runnable.get_input_schema(config)
    61|     def get_output_schema(
    62|         self, config: Optional[RunnableConfig] = None
    63|     ) -> Type[BaseModel]:
    64|         runnable, config = self._prepare(config)
    65|         return runnable.get_output_schema(config)
    66|     @abstractmethod
    67|     def _prepare(
    68|         self, config: Optional[RunnableConfig] = None
    69|     ) -> Tuple[Runnable[Input, Output], RunnableConfig]:
    70|         ...
    71|     def invoke(
    72|         self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
    73|     ) -> Output:
    74|         runnable, config = self._prepare(config)
    75|         return runnable.invoke(input, config, **kwargs)
    76|     async def ainvoke(
    77|         self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
    78|     ) -> Output:
    79|         runnable, config = self._prepare(config)
    80|         return await runnable.ainvoke(input, config, **kwargs)
    81|     def batch(
    82|         self,
    83|         inputs: List[Input],
    84|         config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,
    85|         *,


# ====================================================================
# FILE: libs/core/langchain_core/runnables/passthrough.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 15-54 ---
    15|     Mapping,
    16|     Optional,
    17|     Type,
    18|     Union,
    19|     cast,
    20| )
    21| from langchain_core.pydantic_v1 import BaseModel, create_model
    22| from langchain_core.runnables.base import (
    23|     Other,
    24|     Runnable,
    25|     RunnableParallel,
    26|     RunnableSerializable,
    27| )
    28| from langchain_core.runnables.config import (
    29|     RunnableConfig,
    30|     acall_func_with_variable_args,
    31|     call_func_with_variable_args,
    32|     get_executor_for_config,
    33|     patch_config,
    34| )
    35| from langchain_core.runnables.utils import AddableDict, ConfigurableFieldSpec
    36| from langchain_core.utils.aiter import atee, py_anext
    37| from langchain_core.utils.iter import safetee
    38| if TYPE_CHECKING:
    39|     from langchain_core.callbacks.manager import (
    40|         AsyncCallbackManagerForChainRun,
    41|         CallbackManagerForChainRun,
    42|     )
    43| def identity(x: Other) -> Other:
    44|     """An identity function"""
    45|     return x
    46| async def aidentity(x: Other) -> Other:
    47|     """An async identity function"""
    48|     return x
    49| class RunnablePassthrough(RunnableSerializable[Other, Other]):
    50|     """A runnable to passthrough inputs unchanged or with additional keys.
    51|     This runnable behaves almost like the identity function, except that it
    52|     can be configured to add additional keys to the output, if the input is a
    53|     dict.
    54|     The examples below demonstrate this runnable works using a few simple

# --- HUNK 2: Lines 225-308 ---
   225|                     )
   226|                 elif self.func is not None:
   227|                     call_func_with_variable_args(self.func, final, config, **kwargs)
   228|     def stream(
   229|         self,
   230|         input: Other,
   231|         config: Optional[RunnableConfig] = None,
   232|         **kwargs: Any,
   233|     ) -> Iterator[Other]:
   234|         return self.transform(iter([input]), config, **kwargs)
   235|     async def astream(
   236|         self,
   237|         input: Other,
   238|         config: Optional[RunnableConfig] = None,
   239|         **kwargs: Any,
   240|     ) -> AsyncIterator[Other]:
   241|         async def input_aiter() -> AsyncIterator[Other]:
   242|             yield input
   243|         async for chunk in self.atransform(input_aiter(), config, **kwargs):
   244|             yield chunk
   245| class RunnableAssign(RunnableSerializable[Dict[str, Any], Dict[str, Any]]):
   246|     """
   247|     A runnable that assigns key-value pairs to Dict[str, Any] inputs.
   248|     """
   249|     mapper: RunnableParallel[Dict[str, Any]]
   250|     def __init__(self, mapper: RunnableParallel[Dict[str, Any]], **kwargs: Any) -> None:
   251|         super().__init__(mapper=mapper, **kwargs)
   252|     @classmethod
   253|     def is_lc_serializable(cls) -> bool:
   254|         return True
   255|     @classmethod
   256|     def get_lc_namespace(cls) -> List[str]:
   257|         """Get the namespace of the langchain object."""
   258|         return ["langchain", "schema", "runnable"]
   259|     def get_input_schema(
   260|         self, config: Optional[RunnableConfig] = None
   261|     ) -> Type[BaseModel]:
   262|         map_input_schema = self.mapper.get_input_schema(config)
   263|         if not map_input_schema.__custom_root_type__:
   264|             return map_input_schema
   265|         return super().get_input_schema(config)
   266|     def get_output_schema(
   267|         self, config: Optional[RunnableConfig] = None
   268|     ) -> Type[BaseModel]:
   269|         map_input_schema = self.mapper.get_input_schema(config)
   270|         map_output_schema = self.mapper.get_output_schema(config)
   271|         if (
   272|             not map_input_schema.__custom_root_type__
   273|             and not map_output_schema.__custom_root_type__
   274|         ):
   275|             return create_model(  # type: ignore[call-overload]
   276|                 "RunnableAssignOutput",
   277|                 **{
   278|                     k: (v.type_, v.default)
   279|                     for s in (map_input_schema, map_output_schema)
   280|                     for k, v in s.__fields__.items()
   281|                 },
   282|             )
   283|         elif not map_output_schema.__custom_root_type__:
   284|             return map_output_schema
   285|         return super().get_output_schema(config)
   286|     @property
   287|     def config_specs(self) -> List[ConfigurableFieldSpec]:
   288|         return self.mapper.config_specs
   289|     def _invoke(
   290|         self,
   291|         input: Dict[str, Any],
   292|         run_manager: CallbackManagerForChainRun,
   293|         config: RunnableConfig,
   294|         **kwargs: Any,
   295|     ) -> Dict[str, Any]:
   296|         assert isinstance(
   297|             input, dict
   298|         ), "The input to RunnablePassthrough.assign() must be a dict."
   299|         return {
   300|             **input,
   301|             **self.mapper.invoke(
   302|                 input,
   303|                 patch_config(config, callbacks=run_manager.get_child()),
   304|                 **kwargs,
   305|             ),
   306|         }
   307|     def invoke(
   308|         self,

# --- HUNK 3: Lines 422-441 ---
   422|             input, self._atransform, config, **kwargs
   423|         ):
   424|             yield chunk
   425|     def stream(
   426|         self,
   427|         input: Dict[str, Any],
   428|         config: Optional[RunnableConfig] = None,
   429|         **kwargs: Any,
   430|     ) -> Iterator[Dict[str, Any]]:
   431|         return self.transform(iter([input]), config, **kwargs)
   432|     async def astream(
   433|         self,
   434|         input: Dict[str, Any],
   435|         config: Optional[RunnableConfig] = None,
   436|         **kwargs: Any,
   437|     ) -> AsyncIterator[Dict[str, Any]]:
   438|         async def input_aiter() -> AsyncIterator[Dict[str, Any]]:
   439|             yield input
   440|         async for chunk in self.atransform(input_aiter(), config, **kwargs):
   441|             yield chunk


# ====================================================================
# FILE: libs/core/langchain_core/runnables/utils.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 65-148 ---
    65|             and isinstance(node.slice, ast.Constant)
    66|             and isinstance(node.slice.value, str)
    67|         ):
    68|             self.keys.add(node.slice.value)
    69|     def visit_Call(self, node: ast.Call) -> Any:
    70|         if (
    71|             isinstance(node.func, ast.Attribute)
    72|             and isinstance(node.func.value, ast.Name)
    73|             and node.func.value.id == self.name
    74|             and node.func.attr == "get"
    75|             and len(node.args) in (1, 2)
    76|             and isinstance(node.args[0], ast.Constant)
    77|             and isinstance(node.args[0].value, str)
    78|         ):
    79|             self.keys.add(node.args[0].value)
    80| class IsFunctionArgDict(ast.NodeVisitor):
    81|     """Check if the first argument of a function is a dict."""
    82|     def __init__(self) -> None:
    83|         self.keys: Set[str] = set()
    84|     def visit_Lambda(self, node: ast.Lambda) -> Any:
    85|         input_arg_name = node.args.args[0].arg
    86|         IsLocalDict(input_arg_name, self.keys).visit(node.body)
    87|     def visit_FunctionDef(self, node: ast.FunctionDef) -> Any:
    88|         input_arg_name = node.args.args[0].arg
    89|         IsLocalDict(input_arg_name, self.keys).visit(node)
    90|     def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> Any:
    91|         input_arg_name = node.args.args[0].arg
    92|         IsLocalDict(input_arg_name, self.keys).visit(node)
    93| class GetLambdaSource(ast.NodeVisitor):
    94|     """Get the source code of a lambda function."""
    95|     def __init__(self) -> None:
    96|         """Initialize the visitor."""
    97|         self.source: Optional[str] = None
    98|         self.count = 0
    99|     def visit_Lambda(self, node: ast.Lambda) -> Any:
   100|         """Visit a lambda function."""
   101|         self.count += 1
   102|         if hasattr(ast, "unparse"):
   103|             self.source = ast.unparse(node)
   104| def get_function_first_arg_dict_keys(func: Callable) -> Optional[List[str]]:
   105|     """Get the keys of the first argument of a function if it is a dict."""
   106|     try:
   107|         code = inspect.getsource(func)
   108|         tree = ast.parse(textwrap.dedent(code))
   109|         visitor = IsFunctionArgDict()
   110|         visitor.visit(tree)
   111|         return list(visitor.keys) if visitor.keys else None
   112|     except (SyntaxError, TypeError, OSError):
   113|         return None
   114| def get_lambda_source(func: Callable) -> Optional[str]:
   115|     """Get the source code of a lambda function.
   116|     Args:
   117|         func: a callable that can be a lambda function
   118|     Returns:
   119|         str: the source code of the lambda function
   120|     """
   121|     try:
   122|         code = inspect.getsource(func)
   123|         tree = ast.parse(textwrap.dedent(code))
   124|         visitor = GetLambdaSource()
   125|         visitor.visit(tree)
   126|         return visitor.source if visitor.count == 1 else None
   127|     except (SyntaxError, TypeError, OSError):
   128|         return None
   129| def indent_lines_after_first(text: str, prefix: str) -> str:
   130|     """Indent all lines of text after the first line.
   131|     Args:
   132|         text:  The text to indent
   133|         prefix: Used to determine the number of spaces to indent
   134|     Returns:
   135|         str: The indented text
   136|     """
   137|     n_spaces = len(prefix)
   138|     spaces = " " * n_spaces
   139|     lines = text.splitlines()
   140|     return "\n".join([lines[0]] + [spaces + line for line in lines[1:]])
   141| class AddableDict(Dict[str, Any]):
   142|     """
   143|     Dictionary that can be added to another dictionary.
   144|     """
   145|     def __add__(self, other: AddableDict) -> AddableDict:
   146|         chunk = AddableDict(self)
   147|         for key in other:
   148|             if key not in chunk or chunk[key] is None:


# ====================================================================
# FILE: libs/core/langchain_core/tracers/base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 84-124 ---
    84|                 run.child_execution_order is not None
    85|                 and parent_run.child_execution_order is not None
    86|                 and run.child_execution_order > parent_run.child_execution_order
    87|             ):
    88|                 parent_run.child_execution_order = run.child_execution_order
    89|         self.run_map.pop(str(run.id))
    90|         self._on_run_update(run)
    91|     def _get_execution_order(self, parent_run_id: Optional[str] = None) -> int:
    92|         """Get the execution order for a run."""
    93|         if parent_run_id is None:
    94|             return 1
    95|         parent_run = self.run_map.get(parent_run_id)
    96|         if parent_run is None:
    97|             logger.debug(f"Parent run with UUID {parent_run_id} not found.")
    98|             return 1
    99|         if parent_run.child_execution_order is None:
   100|             raise TracerException(
   101|                 f"Parent run with UUID {parent_run_id} has no child execution order."
   102|             )
   103|         return parent_run.child_execution_order + 1
   104|     def _get_run(self, run_id: UUID, run_type: str | None = None) -> Run:
   105|         try:
   106|             run = self.run_map[str(run_id)]
   107|         except KeyError as exc:
   108|             raise TracerException(f"No indexed run ID {run_id}.") from exc
   109|         if run_type is not None and run.run_type != run_type:
   110|             raise TracerException(
   111|                 f"Found {run.run_type} run at ID {run_id}, but expected {run_type} run."
   112|             )
   113|         return run
   114|     def on_llm_start(
   115|         self,
   116|         serialized: Dict[str, Any],
   117|         prompts: List[str],
   118|         *,
   119|         run_id: UUID,
   120|         tags: Optional[List[str]] = None,
   121|         parent_run_id: Optional[UUID] = None,
   122|         metadata: Optional[Dict[str, Any]] = None,
   123|         name: Optional[str] = None,
   124|         **kwargs: Any,


# ====================================================================
# FILE: libs/core/langchain_core/tracers/log_stream.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 19-58 ---
    19| from langchain_core.load import load
    20| from langchain_core.outputs import ChatGenerationChunk, GenerationChunk
    21| from langchain_core.tracers.base import BaseTracer
    22| from langchain_core.tracers.schemas import Run
    23| class LogEntry(TypedDict):
    24|     """A single entry in the run log."""
    25|     id: str
    26|     """ID of the sub-run."""
    27|     name: str
    28|     """Name of the object being run."""
    29|     type: str
    30|     """Type of the object being run, eg. prompt, chain, llm, etc."""
    31|     tags: List[str]
    32|     """List of tags for the run."""
    33|     metadata: Dict[str, Any]
    34|     """Key-value pairs of metadata for the run."""
    35|     start_time: str
    36|     """ISO-8601 timestamp of when the run started."""
    37|     streamed_output_str: List[str]
    38|     """List of LLM tokens streamed by this run, if applicable."""
    39|     final_output: Optional[Any]
    40|     """Final output of this run.
    41|     Only available after the run has finished successfully."""
    42|     end_time: Optional[str]
    43|     """ISO-8601 timestamp of when the run ended.
    44|     Only available after the run has finished."""
    45| class RunState(TypedDict):
    46|     """State of the run."""
    47|     id: str
    48|     """ID of the run."""
    49|     streamed_output: List[Any]
    50|     """List of output chunks streamed by Runnable.stream()"""
    51|     final_output: Optional[Any]
    52|     """Final output of the run, usually the result of aggregating (`+`) streamed_output.
    53|     Updated throughout the run when supported by the Runnable."""
    54|     logs: Dict[str, LogEntry]
    55|     """Map of run names to sub-runs. If filters were supplied, this list will
    56|     contain only the runs that matched the filters."""
    57| class RunLogPatch:
    58|     """A patch to the run log."""

# --- HUNK 2: Lines 175-214 ---
   175|         if not self.include_run(run):
   176|             return
   177|         with self.lock:
   178|             self._counter_map_by_name[run.name] += 1
   179|             count = self._counter_map_by_name[run.name]
   180|             self._key_map_by_run_id[run.id] = (
   181|                 run.name if count == 1 else f"{run.name}:{count}"
   182|             )
   183|         self.send_stream.send_nowait(
   184|             RunLogPatch(
   185|                 {
   186|                     "op": "add",
   187|                     "path": f"/logs/{self._key_map_by_run_id[run.id]}",
   188|                     "value": LogEntry(
   189|                         id=str(run.id),
   190|                         name=run.name,
   191|                         type=run.run_type,
   192|                         tags=run.tags or [],
   193|                         metadata=(run.extra or {}).get("metadata", {}),
   194|                         start_time=run.start_time.isoformat(timespec="milliseconds"),
   195|                         streamed_output_str=[],
   196|                         final_output=None,
   197|                         end_time=None,
   198|                     ),
   199|                 }
   200|             )
   201|         )
   202|     def _on_run_update(self, run: Run) -> None:
   203|         """Finish a run."""
   204|         try:
   205|             index = self._key_map_by_run_id.get(run.id)
   206|             if index is None:
   207|                 return
   208|             self.send_stream.send_nowait(
   209|                 RunLogPatch(
   210|                     {
   211|                         "op": "add",
   212|                         "path": f"/logs/{index}/final_output",
   213|                         "value": load(run.outputs),
   214|                     },

# --- HUNK 3: Lines 224-246 ---
   224|         finally:
   225|             if run.id == self.root_id:
   226|                 if self.auto_close:
   227|                     self.send_stream.close()
   228|     def _on_llm_new_token(
   229|         self,
   230|         run: Run,
   231|         token: str,
   232|         chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]],
   233|     ) -> None:
   234|         """Process new LLM token."""
   235|         index = self._key_map_by_run_id.get(run.id)
   236|         if index is None:
   237|             return
   238|         self.send_stream.send_nowait(
   239|             RunLogPatch(
   240|                 {
   241|                     "op": "add",
   242|                     "path": f"/logs/{index}/streamed_output_str/-",
   243|                     "value": token,
   244|                 }
   245|             )
   246|         )


# ====================================================================
# FILE: libs/core/langchain_core/utils/env.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 15-36 ---
    15|         "False",
    16|     )
    17| def get_from_dict_or_env(
    18|     data: Dict[str, Any], key: str, env_key: str, default: Optional[str] = None
    19| ) -> str:
    20|     """Get a value from a dictionary or an environment variable."""
    21|     if key in data and data[key]:
    22|         return data[key]
    23|     else:
    24|         return get_from_env(key, env_key, default=default)
    25| def get_from_env(key: str, env_key: str, default: Optional[str] = None) -> str:
    26|     """Get a value from a dictionary or an environment variable."""
    27|     if env_key in os.environ and os.environ[env_key]:
    28|         return os.environ[env_key]
    29|     elif default is not None:
    30|         return default
    31|     else:
    32|         raise ValueError(
    33|             f"Did not find {key}, please add an environment variable"
    34|             f" `{env_key}` which contains it, or pass"
    35|             f"  `{key}` as a named parameter."
    36|         )


# ====================================================================
# FILE: libs/core/langchain_core/utils/formatting.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| """Utilities for formatting strings."""
     2| from string import Formatter
     3| from typing import Any, List, Mapping, Sequence, Union
     4| class StrictFormatter(Formatter):
     5|     """A subclass of formatter that checks for extra keys."""
     6|     def check_unused_args(
     7|         self,
     8|         used_args: Sequence[Union[int, str]],
     9|         args: Sequence,
    10|         kwargs: Mapping[str, Any],
    11|     ) -> None:
    12|         """Check to see if extra parameters are passed."""
    13|         extra = set(kwargs).difference(used_args)
    14|         if extra:
    15|             raise KeyError(extra)
    16|     def vformat(
    17|         self, format_string: str, args: Sequence, kwargs: Mapping[str, Any]
    18|     ) -> str:
    19|         """Check that no arguments are provided."""
    20|         if len(args) > 0:
    21|             raise ValueError(
    22|                 "No arguments should be provided, "
    23|                 "everything should be passed as keyword arguments."
    24|             )
    25|         return super().vformat(format_string, args, kwargs)
    26|     def validate_input_variables(
    27|         self, format_string: str, input_variables: List[str]
    28|     ) -> None:
    29|         dummy_inputs = {input_variable: "foo" for input_variable in input_variables}
    30|         super().format(format_string, **dummy_inputs)
    31| formatter = StrictFormatter()


# ====================================================================
# FILE: libs/langchain/langchain/agents/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 25-78 ---
    25|     AgentOutputParser,
    26|     BaseMultiActionAgent,
    27|     BaseSingleActionAgent,
    28|     LLMSingleActionAgent,
    29| )
    30| from langchain.agents.agent_iterator import AgentExecutorIterator
    31| from langchain.agents.agent_toolkits import (
    32|     create_json_agent,
    33|     create_openapi_agent,
    34|     create_pbi_agent,
    35|     create_pbi_chat_agent,
    36|     create_spark_sql_agent,
    37|     create_sql_agent,
    38|     create_vectorstore_agent,
    39|     create_vectorstore_router_agent,
    40| )
    41| from langchain.agents.agent_types import AgentType
    42| from langchain.agents.conversational.base import ConversationalAgent
    43| from langchain.agents.conversational_chat.base import ConversationalChatAgent
    44| from langchain.agents.initialize import initialize_agent
    45| from langchain.agents.load_tools import (
    46|     get_all_tool_names,
    47|     load_huggingface_tool,
    48|     load_tools,
    49| )
    50| from langchain.agents.loading import load_agent
    51| from langchain.agents.mrkl.base import MRKLChain, ZeroShotAgent
    52| from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent
    53| from langchain.agents.openai_functions_multi_agent.base import OpenAIMultiFunctionsAgent
    54| from langchain.agents.react.base import ReActChain, ReActTextWorldAgent
    55| from langchain.agents.self_ask_with_search.base import SelfAskWithSearchChain
    56| from langchain.agents.structured_chat.base import StructuredChatAgent
    57| from langchain.agents.tools import Tool, tool
    58| from langchain.agents.xml.base import XMLAgent
    59| DEPRECATED_CODE = [
    60|     "create_csv_agent",
    61|     "create_pandas_dataframe_agent",
    62|     "create_spark_dataframe_agent",
    63|     "create_xorbits_agent",
    64| ]
    65| def __getattr__(name: str) -> Any:
    66|     """Get attr name."""
    67|     if name in DEPRECATED_CODE:
    68|         HERE = Path(__file__).parents[1]
    69|         relative_path = as_import_path(
    70|             Path(__file__).parent, suffix=name, relative_to=HERE
    71|         )
    72|         old_path = "langchain." + relative_path
    73|         new_path = "langchain_experimental." + relative_path
    74|         raise ImportError(
    75|             f"{name} has been moved to langchain experimental. "
    76|             "See https://github.com/langchain-ai/langchain/discussions/11680"
    77|             "for more information.\n"
    78|             f"Please update your import statement from: `{old_path}` to `{new_path}`."

# --- HUNK 2: Lines 96-116 ---
    96|     "ReActTextWorldAgent",
    97|     "SelfAskWithSearchChain",
    98|     "StructuredChatAgent",
    99|     "Tool",
   100|     "ZeroShotAgent",
   101|     "create_json_agent",
   102|     "create_openapi_agent",
   103|     "create_pbi_agent",
   104|     "create_pbi_chat_agent",
   105|     "create_spark_sql_agent",
   106|     "create_sql_agent",
   107|     "create_vectorstore_agent",
   108|     "create_vectorstore_router_agent",
   109|     "get_all_tool_names",
   110|     "initialize_agent",
   111|     "load_agent",
   112|     "load_huggingface_tool",
   113|     "load_tools",
   114|     "tool",
   115|     "XMLAgent",
   116| ]


# ====================================================================
# FILE: libs/langchain/langchain/agents/agent_iterator.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 155-195 ---
   155|                     next_step_seq.append(chunk)
   156|                     if self.yield_actions:
   157|                         if isinstance(chunk, AgentAction):
   158|                             yield AddableDict(actions=[chunk], messages=chunk.messages)
   159|                         elif isinstance(chunk, AgentStep):
   160|                             yield AddableDict(steps=[chunk], messages=chunk.messages)
   161|                 next_step = self.agent_executor._consume_next_step(next_step_seq)
   162|                 self.update_iterations()
   163|                 output = self._process_next_step_output(next_step, run_manager)
   164|                 is_final = "intermediate_step" not in output
   165|                 if not self.yield_actions or is_final:
   166|                     yield output
   167|                 if is_final:
   168|                     return
   169|         except BaseException as e:
   170|             run_manager.on_chain_error(e)
   171|             raise
   172|         yield self._stop(run_manager)
   173|     async def __aiter__(self) -> AsyncIterator[AddableDict]:
   174|         """
   175|         N.B. __aiter__ must be a normal method, so need to initialise async run manager
   176|         on first __anext__ call where we can await it
   177|         """
   178|         logger.debug("Initialising AgentExecutorIterator (async)")
   179|         self.reset()
   180|         callback_manager = AsyncCallbackManager.configure(
   181|             self.callbacks,
   182|             self.agent_executor.callbacks,
   183|             self.agent_executor.verbose,
   184|             self.tags,
   185|             self.agent_executor.tags,
   186|             self.metadata,
   187|             self.agent_executor.metadata,
   188|         )
   189|         run_manager = await callback_manager.on_chain_start(
   190|             dumpd(self.agent_executor),
   191|             self.inputs,
   192|             name=self.run_name,
   193|         )
   194|         try:
   195|             async with asyncio_timeout(self.agent_executor.max_execution_time):


# ====================================================================
# FILE: libs/langchain/langchain/agents/openai_functions_agent/base.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-36 ---
     1| """Module implements an agent that uses OpenAI's APIs function enabled API."""
     2| from typing import Any, List, Optional, Sequence, Tuple, Union
     3| from langchain_core.agents import AgentAction, AgentFinish
     4| from langchain_core.language_models import BaseLanguageModel
     5| from langchain_core.messages import (
     6|     BaseMessage,
     7|     SystemMessage,
     8| )
     9| from langchain_core.prompts import BasePromptTemplate
    10| from langchain_core.prompts.chat import (
    11|     BaseMessagePromptTemplate,
    12|     ChatPromptTemplate,
    13|     HumanMessagePromptTemplate,
    14|     MessagesPlaceholder,
    15| )
    16| from langchain_core.pydantic_v1 import root_validator
    17| from langchain_core.tools import BaseTool
    18| from langchain.agents import BaseSingleActionAgent
    19| from langchain.agents.format_scratchpad.openai_functions import (
    20|     format_to_openai_function_messages,
    21| )
    22| from langchain.agents.output_parsers.openai_functions import (
    23|     OpenAIFunctionsAgentOutputParser,
    24| )
    25| from langchain.callbacks.base import BaseCallbackManager
    26| from langchain.callbacks.manager import Callbacks
    27| from langchain.tools.render import format_tool_to_openai_function
    28| class OpenAIFunctionsAgent(BaseSingleActionAgent):
    29|     """An Agent driven by OpenAIs function powered API.
    30|     Args:
    31|         llm: This should be an instance of ChatOpenAI, specifically a model
    32|             that supports using `functions`.
    33|         tools: The tools this agent has access to.
    34|         prompt: The prompt for this agent, should support agent_scratchpad as one
    35|             of the variables. For an easy way to construct this prompt, use
    36|             `OpenAIFunctionsAgent.create_prompt(...)`

# --- HUNK 2: Lines 185-204 ---
   185|         tools: Sequence[BaseTool],
   186|         callback_manager: Optional[BaseCallbackManager] = None,
   187|         extra_prompt_messages: Optional[List[BaseMessagePromptTemplate]] = None,
   188|         system_message: Optional[SystemMessage] = SystemMessage(
   189|             content="You are a helpful AI assistant."
   190|         ),
   191|         **kwargs: Any,
   192|     ) -> BaseSingleActionAgent:
   193|         """Construct an agent from an LLM and tools."""
   194|         prompt = cls.create_prompt(
   195|             extra_prompt_messages=extra_prompt_messages,
   196|             system_message=system_message,
   197|         )
   198|         return cls(
   199|             llm=llm,
   200|             prompt=prompt,
   201|             tools=tools,
   202|             callback_manager=callback_manager,
   203|             **kwargs,
   204|         )


# ====================================================================
# FILE: libs/langchain/langchain/agents/openai_functions_multi_agent/base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 16-56 ---
    16|     ChatPromptTemplate,
    17|     HumanMessagePromptTemplate,
    18|     MessagesPlaceholder,
    19| )
    20| from langchain_core.pydantic_v1 import root_validator
    21| from langchain.agents import BaseMultiActionAgent
    22| from langchain.agents.format_scratchpad.openai_functions import (
    23|     format_to_openai_function_messages,
    24| )
    25| from langchain.callbacks.base import BaseCallbackManager
    26| from langchain.callbacks.manager import Callbacks
    27| from langchain.tools import BaseTool
    28| _FunctionsAgentAction = AgentActionMessageLog
    29| def _parse_ai_message(message: BaseMessage) -> Union[List[AgentAction], AgentFinish]:
    30|     """Parse an AI message."""
    31|     if not isinstance(message, AIMessage):
    32|         raise TypeError(f"Expected an AI message got {type(message)}")
    33|     function_call = message.additional_kwargs.get("function_call", {})
    34|     if function_call:
    35|         try:
    36|             arguments = json.loads(function_call["arguments"])
    37|         except JSONDecodeError:
    38|             raise OutputParserException(
    39|                 f"Could not parse tool input: {function_call} because "
    40|                 f"the `arguments` is not valid JSON."
    41|             )
    42|         try:
    43|             tools = arguments["actions"]
    44|         except (TypeError, KeyError):
    45|             raise OutputParserException(
    46|                 f"Could not parse tool input: {function_call} because "
    47|                 f"the `arguments` JSON does not contain `actions` key."
    48|             )
    49|         final_tools: List[AgentAction] = []
    50|         for tool_schema in tools:
    51|             _tool_input = tool_schema["action"]
    52|             function_name = tool_schema["action_name"]
    53|             if "__arg1" in _tool_input:
    54|                 tool_input = _tool_input["__arg1"]
    55|             else:
    56|                 tool_input = _tool_input


# ====================================================================
# FILE: libs/langchain/langchain/agents/output_parsers/openai_functions.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 16-56 ---
    16|     function_call parameter from OpenAI to convey what tools to use.
    17|     If a function_call parameter is passed, then that is used to get
    18|     the tool and tool input.
    19|     If one is not passed, then the AIMessage is assumed to be the final output.
    20|     """
    21|     @property
    22|     def _type(self) -> str:
    23|         return "openai-functions-agent"
    24|     @staticmethod
    25|     def _parse_ai_message(message: BaseMessage) -> Union[AgentAction, AgentFinish]:
    26|         """Parse an AI message."""
    27|         if not isinstance(message, AIMessage):
    28|             raise TypeError(f"Expected an AI message got {type(message)}")
    29|         function_call = message.additional_kwargs.get("function_call", {})
    30|         if function_call:
    31|             function_name = function_call["name"]
    32|             try:
    33|                 if len(function_call["arguments"].strip()) == 0:
    34|                     _tool_input = {}
    35|                 else:
    36|                     _tool_input = json.loads(function_call["arguments"])
    37|             except JSONDecodeError:
    38|                 raise OutputParserException(
    39|                     f"Could not parse tool input: {function_call} because "
    40|                     f"the `arguments` is not valid JSON."
    41|                 )
    42|             if "__arg1" in _tool_input:
    43|                 tool_input = _tool_input["__arg1"]
    44|             else:
    45|                 tool_input = _tool_input
    46|             content_msg = f"responded: {message.content}\n" if message.content else "\n"
    47|             log = f"\nInvoking: `{function_name}` with `{tool_input}`\n{content_msg}\n"
    48|             return AgentActionMessageLog(
    49|                 tool=function_name,
    50|                 tool_input=tool_input,
    51|                 log=log,
    52|                 message_log=[message],
    53|             )
    54|         return AgentFinish(
    55|             return_values={"output": message.content}, log=str(message.content)
    56|         )


# ====================================================================
# FILE: libs/langchain/langchain/agents/self_ask_with_search/base.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| """Chain that does self-ask with search."""
     2| from typing import Any, Sequence, Union
     3| from langchain_core.language_models import BaseLanguageModel
     4| from langchain_core.prompts import BasePromptTemplate
     5| from langchain_core.pydantic_v1 import Field
     6| from langchain_core.tools import BaseTool
     7| from langchain.agents.agent import Agent, AgentExecutor, AgentOutputParser
     8| from langchain.agents.agent_types import AgentType
     9| from langchain.agents.self_ask_with_search.output_parser import SelfAskOutputParser
    10| from langchain.agents.self_ask_with_search.prompt import PROMPT
    11| from langchain.agents.tools import Tool
    12| from langchain.agents.utils import validate_tools_single_input
    13| from langchain.utilities.google_serper import GoogleSerperAPIWrapper
    14| from langchain.utilities.searchapi import SearchApiAPIWrapper
    15| from langchain.utilities.serpapi import SerpAPIWrapper
    16| class SelfAskWithSearchAgent(Agent):
    17|     """Agent for the self-ask-with-search paper."""
    18|     output_parser: AgentOutputParser = Field(default_factory=SelfAskOutputParser)
    19|     @classmethod
    20|     def _get_default_output_parser(cls, **kwargs: Any) -> AgentOutputParser:
    21|         return SelfAskOutputParser()
    22|     @property
    23|     def _agent_type(self) -> str:
    24|         """Return Identifier of an agent type."""
    25|         return AgentType.SELF_ASK_WITH_SEARCH
    26|     @classmethod
    27|     def create_prompt(cls, tools: Sequence[BaseTool]) -> BasePromptTemplate:
    28|         """Prompt does not depend on tools."""

# --- HUNK 2: Lines 48-67 ---
    48|         return ""
    49| class SelfAskWithSearchChain(AgentExecutor):
    50|     """[Deprecated] Chain that does self-ask with search."""
    51|     def __init__(
    52|         self,
    53|         llm: BaseLanguageModel,
    54|         search_chain: Union[
    55|             GoogleSerperAPIWrapper, SearchApiAPIWrapper, SerpAPIWrapper
    56|         ],
    57|         **kwargs: Any,
    58|     ):
    59|         """Initialize only with an LLM and a search chain."""
    60|         search_tool = Tool(
    61|             name="Intermediate Answer",
    62|             func=search_chain.run,
    63|             coroutine=search_chain.arun,
    64|             description="Search",
    65|         )
    66|         agent = SelfAskWithSearchAgent.from_llm_and_tools(llm, [search_tool])
    67|         super().__init__(agent=agent, tools=[search_tool], **kwargs)


# ====================================================================
# FILE: libs/langchain/langchain/agents/structured_chat/base.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-39 ---
     1| import re
     2| from typing import Any, List, Optional, Sequence, Tuple
     3| from langchain_core.agents import AgentAction
     4| from langchain_core.language_models import BaseLanguageModel
     5| from langchain_core.prompts import BasePromptTemplate
     6| from langchain_core.prompts.chat import (
     7|     ChatPromptTemplate,
     8|     HumanMessagePromptTemplate,
     9|     SystemMessagePromptTemplate,
    10| )
    11| from langchain_core.pydantic_v1 import Field
    12| from langchain.agents.agent import Agent, AgentOutputParser
    13| from langchain.agents.structured_chat.output_parser import (
    14|     StructuredChatOutputParserWithRetries,
    15| )
    16| from langchain.agents.structured_chat.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX
    17| from langchain.callbacks.base import BaseCallbackManager
    18| from langchain.chains.llm import LLMChain
    19| from langchain.tools import BaseTool
    20| HUMAN_MESSAGE_TEMPLATE = "{input}\n\n{agent_scratchpad}"
    21| class StructuredChatAgent(Agent):
    22|     """Structured Chat Agent."""
    23|     output_parser: AgentOutputParser = Field(
    24|         default_factory=StructuredChatOutputParserWithRetries
    25|     )
    26|     """Output parser for the agent."""
    27|     @property
    28|     def observation_prefix(self) -> str:
    29|         """Prefix to append the observation with."""
    30|         return "Observation: "
    31|     @property
    32|     def llm_prefix(self) -> str:
    33|         """Prefix to append the llm call with."""
    34|         return "Thought:"
    35|     def _construct_scratchpad(
    36|         self, intermediate_steps: List[Tuple[AgentAction, str]]
    37|     ) -> str:
    38|         agent_scratchpad = super()._construct_scratchpad(intermediate_steps)
    39|         if not isinstance(agent_scratchpad, str):

# --- HUNK 2: Lines 110-129 ---
   110|             format_instructions=format_instructions,
   111|             input_variables=input_variables,
   112|             memory_prompts=memory_prompts,
   113|         )
   114|         llm_chain = LLMChain(
   115|             llm=llm,
   116|             prompt=prompt,
   117|             callback_manager=callback_manager,
   118|         )
   119|         tool_names = [tool.name for tool in tools]
   120|         _output_parser = output_parser or cls._get_default_output_parser(llm=llm)
   121|         return cls(
   122|             llm_chain=llm_chain,
   123|             allowed_tools=tool_names,
   124|             output_parser=_output_parser,
   125|             **kwargs,
   126|         )
   127|     @property
   128|     def _agent_type(self) -> str:
   129|         raise ValueError


# ====================================================================
# FILE: libs/langchain/langchain/agents/xml/base.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-53 ---
     1| from typing import Any, List, Tuple, Union
     2| from langchain_core.agents import AgentAction, AgentFinish
     3| from langchain_core.prompts.chat import AIMessagePromptTemplate, ChatPromptTemplate
     4| from langchain_core.tools import BaseTool
     5| from langchain.agents.agent import BaseSingleActionAgent
     6| from langchain.agents.output_parsers.xml import XMLAgentOutputParser
     7| from langchain.agents.xml.prompt import agent_instructions
     8| from langchain.callbacks.base import Callbacks
     9| from langchain.chains.llm import LLMChain
    10| class XMLAgent(BaseSingleActionAgent):
    11|     """Agent that uses XML tags.
    12|     Args:
    13|         tools: list of tools the agent can choose from
    14|         llm_chain: The LLMChain to call to predict the next action
    15|     Examples:
    16|         .. code-block:: python
    17|             from langchain.agents import XMLAgent
    18|             from langchain
    19|             tools = ...
    20|             model =
    21|     """
    22|     tools: List[BaseTool]
    23|     """List of tools this agent has access to."""
    24|     llm_chain: LLMChain
    25|     """Chain to use to predict action."""
    26|     @property
    27|     def input_keys(self) -> List[str]:
    28|         return ["input"]
    29|     @staticmethod
    30|     def get_default_prompt() -> ChatPromptTemplate:
    31|         return ChatPromptTemplate.from_template(
    32|             agent_instructions
    33|         ) + AIMessagePromptTemplate.from_template("{intermediate_steps}")
    34|     @staticmethod
    35|     def get_default_output_parser() -> XMLAgentOutputParser:
    36|         return XMLAgentOutputParser()
    37|     def plan(
    38|         self,
    39|         intermediate_steps: List[Tuple[AgentAction, str]],
    40|         callbacks: Callbacks = None,
    41|         **kwargs: Any,
    42|     ) -> Union[AgentAction, AgentFinish]:
    43|         log = ""
    44|         for action, observation in intermediate_steps:
    45|             log += (
    46|                 f"<tool>{action.tool}</tool><tool_input>{action.tool_input}"
    47|                 f"</tool_input><observation>{observation}</observation>"
    48|             )
    49|         tools = ""
    50|         for tool in self.tools:
    51|             tools += f"{tool.name}: {tool.description}\n"
    52|         inputs = {
    53|             "intermediate_steps": log,

# --- HUNK 2: Lines 63-82 ---
    63|         callbacks: Callbacks = None,
    64|         **kwargs: Any,
    65|     ) -> Union[AgentAction, AgentFinish]:
    66|         log = ""
    67|         for action, observation in intermediate_steps:
    68|             log += (
    69|                 f"<tool>{action.tool}</tool><tool_input>{action.tool_input}"
    70|                 f"</tool_input><observation>{observation}</observation>"
    71|             )
    72|         tools = ""
    73|         for tool in self.tools:
    74|             tools += f"{tool.name}: {tool.description}\n"
    75|         inputs = {
    76|             "intermediate_steps": log,
    77|             "tools": tools,
    78|             "question": kwargs["input"],
    79|             "stop": ["</tool_input>", "</final_answer>"],
    80|         }
    81|         response = await self.llm_chain.acall(inputs, callbacks=callbacks)
    82|         return response[self.llm_chain.output_key]


# ====================================================================
# FILE: libs/langchain/langchain/chains/__init__.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 18-81 ---
    18| from langchain.chains.combine_documents.reduce import ReduceDocumentsChain
    19| from langchain.chains.combine_documents.refine import RefineDocumentsChain
    20| from langchain.chains.combine_documents.stuff import StuffDocumentsChain
    21| from langchain.chains.constitutional_ai.base import ConstitutionalChain
    22| from langchain.chains.conversation.base import ConversationChain
    23| from langchain.chains.conversational_retrieval.base import (
    24|     ChatVectorDBChain,
    25|     ConversationalRetrievalChain,
    26| )
    27| from langchain.chains.example_generator import generate_example
    28| from langchain.chains.flare.base import FlareChain
    29| from langchain.chains.graph_qa.arangodb import ArangoGraphQAChain
    30| from langchain.chains.graph_qa.base import GraphQAChain
    31| from langchain.chains.graph_qa.cypher import GraphCypherQAChain
    32| from langchain.chains.graph_qa.falkordb import FalkorDBQAChain
    33| from langchain.chains.graph_qa.hugegraph import HugeGraphQAChain
    34| from langchain.chains.graph_qa.kuzu import KuzuQAChain
    35| from langchain.chains.graph_qa.nebulagraph import NebulaGraphQAChain
    36| from langchain.chains.graph_qa.neptune_cypher import NeptuneOpenCypherQAChain
    37| from langchain.chains.graph_qa.sparql import GraphSparqlQAChain
    38| from langchain.chains.hyde.base import HypotheticalDocumentEmbedder
    39| from langchain.chains.llm import LLMChain
    40| from langchain.chains.llm_checker.base import LLMCheckerChain
    41| from langchain.chains.llm_math.base import LLMMathChain
    42| from langchain.chains.llm_requests import LLMRequestsChain
    43| from langchain.chains.llm_summarization_checker.base import LLMSummarizationCheckerChain
    44| from langchain.chains.loading import load_chain
    45| from langchain.chains.mapreduce import MapReduceChain
    46| from langchain.chains.moderation import OpenAIModerationChain
    47| from langchain.chains.natbot.base import NatBotChain
    48| from langchain.chains.openai_functions import (
    49|     create_citation_fuzzy_match_chain,
    50|     create_extraction_chain,
    51|     create_extraction_chain_pydantic,
    52|     create_qa_with_sources_chain,
    53|     create_qa_with_structure_chain,
    54|     create_tagging_chain,
    55|     create_tagging_chain_pydantic,
    56| )
    57| from langchain.chains.qa_generation.base import QAGenerationChain
    58| from langchain.chains.qa_with_sources.base import QAWithSourcesChain
    59| from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain
    60| from langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain
    61| from langchain.chains.retrieval_qa.base import RetrievalQA, VectorDBQA
    62| from langchain.chains.router import (
    63|     LLMRouterChain,
    64|     MultiPromptChain,
    65|     MultiRetrievalQAChain,
    66|     MultiRouteChain,
    67|     RouterChain,
    68| )
    69| from langchain.chains.sequential import SequentialChain, SimpleSequentialChain
    70| from langchain.chains.sql_database.query import create_sql_query_chain
    71| from langchain.chains.transform import TransformChain
    72| __all__ = [
    73|     "APIChain",
    74|     "AnalyzeDocumentChain",
    75|     "ArangoGraphQAChain",
    76|     "ChatVectorDBChain",
    77|     "ConstitutionalChain",
    78|     "ConversationChain",
    79|     "ConversationalRetrievalChain",
    80|     "FalkorDBQAChain",
    81|     "FlareChain",

# --- HUNK 2: Lines 108-128 ---
   108|     "RefineDocumentsChain",
   109|     "RetrievalQA",
   110|     "RetrievalQAWithSourcesChain",
   111|     "RouterChain",
   112|     "SequentialChain",
   113|     "SimpleSequentialChain",
   114|     "StuffDocumentsChain",
   115|     "TransformChain",
   116|     "VectorDBQA",
   117|     "VectorDBQAWithSourcesChain",
   118|     "create_citation_fuzzy_match_chain",
   119|     "create_extraction_chain",
   120|     "create_extraction_chain_pydantic",
   121|     "create_qa_with_sources_chain",
   122|     "create_qa_with_structure_chain",
   123|     "create_tagging_chain",
   124|     "create_tagging_chain_pydantic",
   125|     "generate_example",
   126|     "load_chain",
   127|     "create_sql_query_chain",
   128| ]


# ====================================================================
# FILE: libs/langchain/langchain/chains/api/news_docs.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-26 ---
     1| NEWS_DOCS = """API documentation:
     2| Endpoint: https://newsapi.org
     3| Top headlines /v2/top-headlines
     4| This endpoint provides live top and breaking headlines for a country, specific category in a country, single source, or multiple sources. You can also search with keywords. Articles are sorted by the earliest date published first.
     5| This endpoint is great for retrieving headlines for use with news tickers or similar.
     6| Request parameters
     7|     country | The 2-letter ISO 3166-1 code of the country you want to get headlines for. Possible options: ae ar at au be bg br ca ch cn co cu cz de eg fr gb gr hk hu id ie il in it jp kr lt lv ma mx my ng nl no nz ph pl pt ro rs ru sa se sg si sk th tr tw ua us ve za. Note: you can't mix this param with the sources param.
     8|     category | The category you want to get headlines for. Possible options: business entertainment general health science sports technology. Note: you can't mix this param with the sources param.
     9|     sources | A comma-seperated string of identifiers for the news sources or blogs you want headlines from. Use the /top-headlines/sources endpoint to locate these programmatically or look at the sources index. Note: you can't mix this param with the country or category params.
    10|     q | Keywords or a phrase to search for.
    11|     pageSize | int | The number of results to return per page (request). 20 is the default, 100 is the maximum.
    12|     page | int | Use this to page through the results if the total results found is greater than the page size.
    13| Response object
    14|     status | string | If the request was successful or not. Options: ok, error. In the case of error a code and message property will be populated.
    15|     totalResults | int | The total number of results available for your request.
    16|     articles | array[article] | The results of the request.
    17|     source | object | The identifier id and a display name name for the source this article came from.
    18|     author | string | The author of the article
    19|     title | string | The headline or title of the article.
    20|     description | string | A description or snippet from the article.
    21|     url | string | The direct URL to the article.
    22|     urlToImage | string | The URL to a relevant image for the article.
    23|     publishedAt | string | The date and time that the article was published, in UTC (+000)
    24|     content | string | The unformatted content of the article, where available. This is truncated to 200 chars.
    25| Use page size: 2
    26| """


# ====================================================================
# FILE: libs/langchain/langchain/chains/combine_documents/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-7 ---
     1| """Different ways to combine documents."""
     2| from langchain.chains.combine_documents.reduce import (
     3|     acollapse_docs,
     4|     collapse_docs,
     5|     split_list_of_docs,
     6| )
     7| __all__ = ["acollapse_docs", "collapse_docs", "split_list_of_docs"]


# ====================================================================
# FILE: libs/langchain/langchain/chains/combine_documents/base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-32 ---
     1| """Base interface for chains combining documents."""
     2| from abc import ABC, abstractmethod
     3| from typing import Any, Dict, List, Optional, Tuple, Type
     4| from langchain_core.documents import Document
     5| from langchain_core.pydantic_v1 import BaseModel, Field, create_model
     6| from langchain_core.runnables.config import RunnableConfig
     7| from langchain.callbacks.manager import (
     8|     AsyncCallbackManagerForChainRun,
     9|     CallbackManagerForChainRun,
    10| )
    11| from langchain.chains.base import Chain
    12| from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter
    13| class BaseCombineDocumentsChain(Chain, ABC):
    14|     """Base interface for chains combining documents.
    15|     Subclasses of this chain deal with combining documents in a variety of
    16|     ways. This base class exists to add some uniformity in the interface these types
    17|     of chains should expose. Namely, they expect an input key related to the documents
    18|     to use (default `input_documents`), and then also expose a method to calculate
    19|     the length of a prompt from documents (useful for outside callers to use to
    20|     determine whether it's safe to pass a list of documents into this chain or whether
    21|     that will longer than the context length).
    22|     """
    23|     input_key: str = "input_documents"  #: :meta private:
    24|     output_key: str = "output_text"  #: :meta private:
    25|     def get_input_schema(
    26|         self, config: Optional[RunnableConfig] = None
    27|     ) -> Type[BaseModel]:
    28|         return create_model(
    29|             "CombineDocumentsInput",
    30|             **{self.input_key: (List[Document], None)},  # type: ignore[call-overload]
    31|         )
    32|     def get_output_schema(


# ====================================================================
# FILE: libs/langchain/langchain/chains/combine_documents/stuff.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-66 ---
     1| """Chain that combines documents by stuffing into context."""
     2| from typing import Any, Dict, List, Optional, Tuple
     3| from langchain_core.documents import Document
     4| from langchain_core.prompts import BasePromptTemplate, format_document
     5| from langchain_core.prompts.prompt import PromptTemplate
     6| from langchain_core.pydantic_v1 import Extra, Field, root_validator
     7| from langchain.callbacks.manager import Callbacks
     8| from langchain.chains.combine_documents.base import (
     9|     BaseCombineDocumentsChain,
    10| )
    11| from langchain.chains.llm import LLMChain
    12| def _get_default_document_prompt() -> PromptTemplate:
    13|     return PromptTemplate(input_variables=["page_content"], template="{page_content}")
    14| class StuffDocumentsChain(BaseCombineDocumentsChain):
    15|     """Chain that combines documents by stuffing into context.
    16|     This chain takes a list of documents and first combines them into a single string.
    17|     It does this by formatting each document into a string with the `document_prompt`
    18|     and then joining them together with `document_separator`. It then adds that new
    19|     string to the inputs with the variable name set by `document_variable_name`.
    20|     Those inputs are then passed to the `llm_chain`.
    21|     Example:
    22|         .. code-block:: python
    23|             from langchain.chains import StuffDocumentsChain, LLMChain
    24|             from langchain_core.prompts import PromptTemplate
    25|             from langchain.llms import OpenAI
    26|             document_prompt = PromptTemplate(
    27|                 input_variables=["page_content"],
    28|                 template="{page_content}"
    29|             )
    30|             document_variable_name = "context"
    31|             llm = OpenAI()
    32|             prompt = PromptTemplate.from_template(
    33|                 "Summarize this content: {context}"
    34|             )
    35|             llm_chain = LLMChain(llm=llm, prompt=prompt)
    36|             chain = StuffDocumentsChain(
    37|                 llm_chain=llm_chain,
    38|                 document_prompt=document_prompt,
    39|                 document_variable_name=document_variable_name
    40|             )
    41|     """
    42|     llm_chain: LLMChain
    43|     """LLM chain which is called with the formatted document string,
    44|     along with any other inputs."""
    45|     document_prompt: BasePromptTemplate = Field(
    46|         default_factory=_get_default_document_prompt
    47|     )
    48|     """Prompt to use to format each document, gets passed to `format_document`."""
    49|     document_variable_name: str
    50|     """The variable name in the llm_chain to put the documents in.
    51|     If only one variable in the llm_chain, this need not be provided."""
    52|     document_separator: str = "\n\n"
    53|     """The string with which to join the formatted documents"""
    54|     class Config:
    55|         """Configuration for this pydantic object."""
    56|         extra = Extra.forbid
    57|         arbitrary_types_allowed = True
    58|     @root_validator(pre=True)
    59|     def get_default_document_variable_name(cls, values: Dict) -> Dict:
    60|         """Get default document variable name, if not provided.
    61|         If only one variable is present in the llm_chain.prompt,
    62|         we can infer that the formatted documents should be passed in
    63|         with this variable name.
    64|         """
    65|         llm_chain_variables = values["llm_chain"].prompt.input_variables
    66|         if "document_variable_name" not in values:


# ====================================================================
# FILE: libs/langchain/langchain/chains/conversational_retrieval/base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-34 ---
     1| """Chain for chatting with a vector database."""
     2| from __future__ import annotations
     3| import inspect
     4| import warnings
     5| from abc import abstractmethod
     6| from pathlib import Path
     7| from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union
     8| from langchain_core.documents import Document
     9| from langchain_core.language_models import BaseLanguageModel
    10| from langchain_core.messages import BaseMessage
    11| from langchain_core.prompts import BasePromptTemplate
    12| from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
    13| from langchain_core.retrievers import BaseRetriever
    14| from langchain_core.runnables.config import RunnableConfig
    15| from langchain_core.vectorstores import VectorStore
    16| from langchain.callbacks.manager import (
    17|     AsyncCallbackManagerForChainRun,
    18|     CallbackManagerForChainRun,
    19|     Callbacks,
    20| )
    21| from langchain.chains.base import Chain
    22| from langchain.chains.combine_documents.base import BaseCombineDocumentsChain
    23| from langchain.chains.combine_documents.stuff import StuffDocumentsChain
    24| from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT
    25| from langchain.chains.llm import LLMChain
    26| from langchain.chains.question_answering import load_qa_chain
    27| CHAT_TURN_TYPE = Union[Tuple[str, str], BaseMessage]
    28| _ROLE_MAP = {"human": "Human: ", "ai": "Assistant: "}
    29| def _get_chat_history(chat_history: List[CHAT_TURN_TYPE]) -> str:
    30|     buffer = ""
    31|     for dialogue_turn in chat_history:
    32|         if isinstance(dialogue_turn, BaseMessage):
    33|             role_prefix = _ROLE_MAP.get(dialogue_turn.type, f"{dialogue_turn.type}: ")
    34|             buffer += f"\n{role_prefix}{dialogue_turn.content}"


# ====================================================================
# FILE: libs/langchain/langchain/evaluation/criteria/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| """Criteria or rubric based evaluators.
     2| These evaluators are useful for evaluating the
     3| output of a language model or chain against
     4| specified criteria or rubric.
     5| Classes
     6| -------
     7| CriteriaEvalChain : Evaluates the output of a language model or
     8| chain against specified criteria.
     9| Examples
    10| --------
    11| Using a pre-defined criterion:
    12| >>> from langchain.llms import OpenAI
    13| >>> from langchain.evaluation.criteria import CriteriaEvalChain
    14| >>> llm = OpenAI()
    15| >>> criteria = "conciseness"
    16| >>> chain = CriteriaEvalChain.from_llm(llm=llm, criteria=criteria)
    17| >>> chain.evaluate_strings(
    18|         prediction="The answer is 42.",
    19|         reference="42",
    20|         input="What is the answer to life, the universe, and everything?",
    21|     )
    22| Using a custom criterion:
    23| >>> from langchain.llms import OpenAI
    24| >>> from langchain.evaluation.criteria import LabeledCriteriaEvalChain
    25| >>> llm = OpenAI()
    26| >>> criteria = {
    27|        "hallucination": (
    28|             "Does this submission contain information"
    29|             " not present in the input or reference?"
    30|         ),
    31|     }


# ====================================================================
# FILE: libs/langchain/langchain/evaluation/parsing/base.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-21 ---
     1| """Evaluators for parsing strings."""
     2| from operator import eq
     3| from typing import Any, Callable, Optional, Union, cast
     4| from langchain.evaluation.schema import StringEvaluator
     5| from langchain.output_parsers.json import parse_json_markdown
     6| class JsonValidityEvaluator(StringEvaluator):
     7|     """Evaluates whether the prediction is valid JSON.
     8|     This evaluator checks if the prediction is a valid JSON string. It does not
     9|         require any input or reference.
    10|     Attributes:
    11|         requires_input (bool): Whether this evaluator requires an input
    12|             string. Always False.
    13|         requires_reference (bool): Whether this evaluator requires a
    14|             reference string. Always False.
    15|         evaluation_name (str): The name of the evaluation metric.
    16|             Always "json".
    17|     Examples:
    18|         >>> evaluator = JsonValidityEvaluator()
    19|         >>> prediction = '{"name": "John", "age": 30, "city": "New York"}'
    20|         >>> evaluator.evaluate(prediction)
    21|         {'score': 1}

# --- HUNK 2: Lines 36-76 ---
    36|         return "json_validity"
    37|     def _evaluate_strings(
    38|         self,
    39|         prediction: str,
    40|         input: Optional[str] = None,
    41|         reference: Optional[str] = None,
    42|         **kwargs: Any,
    43|     ) -> dict:
    44|         """Evaluate the prediction string.
    45|         Args:
    46|             prediction (str): The prediction string to evaluate.
    47|             input (str, optional): Not used in this evaluator. Defaults to None.
    48|             reference (str, optional): Not used in this evaluator. Defaults to None.
    49|         Returns:
    50|             dict: A dictionary containing the evaluation score. The score is 1 if
    51|             the prediction is valid JSON, and 0 otherwise.
    52|                 If the prediction is not valid JSON, the dictionary also contains
    53|                 a "reasoning" field with the error message.
    54|         """
    55|         try:
    56|             parse_json_markdown(prediction)
    57|             return {"score": 1}
    58|         except Exception as e:
    59|             return {"score": 0, "reasoning": str(e)}
    60| class JsonEqualityEvaluator(StringEvaluator):
    61|     """Evaluates whether the prediction is equal to the reference after
    62|         parsing both as JSON.
    63|     This evaluator checks if the prediction, after parsing as JSON, is equal
    64|         to the reference,
    65|     which is also parsed as JSON. It does not require an input string.
    66|     Attributes:
    67|         requires_input (bool): Whether this evaluator requires an
    68|             input string. Always False.
    69|         requires_reference (bool): Whether this evaluator requires
    70|             a reference string. Always True.
    71|         evaluation_name (str): The name of the evaluation metric.
    72|             Always "parsed_equality".
    73|     Examples:
    74|         >>> evaluator = JsonEqualityEvaluator()
    75|         >>> evaluator.evaluate_strings('{"a": 1}', reference='{"a": 1}')
    76|         {'score': True}


# ====================================================================
# FILE: libs/langchain/langchain/output_parsers/datetime.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 13-44 ---
    13|     """Generates n random datetime strings conforming to the
    14|     given pattern within the specified date range.
    15|     Pattern should be a string containing the desired format codes.
    16|     start_date and end_date should be datetime objects representing
    17|     the start and end of the date range.
    18|     """
    19|     examples = []
    20|     delta = end_date - start_date
    21|     for i in range(n):
    22|         random_delta = random.uniform(0, delta.total_seconds())
    23|         dt = start_date + timedelta(seconds=random_delta)
    24|         date_string = dt.strftime(pattern)
    25|         examples.append(date_string)
    26|     return examples
    27| class DatetimeOutputParser(BaseOutputParser[datetime]):
    28|     """Parse the output of an LLM call to a datetime."""
    29|     format: str = "%Y-%m-%dT%H:%M:%S.%fZ"
    30|     """The string value that used as the datetime format."""
    31|     def get_format_instructions(self) -> str:
    32|         examples = comma_list(_generate_random_datetime_strings(self.format))
    33|         return f"""Write a datetime string that matches the 
    34|             following pattern: "{self.format}". Examples: {examples}"""
    35|     def parse(self, response: str) -> datetime:
    36|         try:
    37|             return datetime.strptime(response.strip(), self.format)
    38|         except ValueError as e:
    39|             raise OutputParserException(
    40|                 f"Could not parse datetime string: {response}"
    41|             ) from e
    42|     @property
    43|     def _type(self) -> str:
    44|         return "datetime"


# ====================================================================
# FILE: libs/langchain/langchain/output_parsers/format_instructions.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 17-66 ---
    17| ```
    18| {schema}
    19| ```"""
    20| YAML_FORMAT_INSTRUCTIONS = """The output should be formatted as a YAML instance that conforms to the given JSON schema below.
    21| As an example, for the schema
    22| ```
    23| {{'title': 'Players', 'description': 'A list of players', 'type': 'array', 'items': {{'$ref': '#/definitions/Player'}}, 'definitions': {{'Player': {{'title': 'Player', 'type': 'object', 'properties': {{'name': {{'title': 'Name', 'description': 'Player name', 'type': 'string'}}, 'avg': {{'title': 'Avg', 'description': 'Batting average', 'type': 'number'}}}}, 'required': ['name', 'avg']}}}}}}
    24| ```
    25| a well formatted instance would be:
    26| ```
    27| - name: John Doe
    28|   avg: 0.3
    29| - name: Jane Maxfield
    30|   avg: 1.4
    31| ```
    32| Please follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: 
    33| ```
    34| {schema}
    35| ```
    36| Make sure to always enclose the YAML output in triple backticks (```)"""
    37| XML_FORMAT_INSTRUCTIONS = """The output should be formatted as a XML file.
    38| 1. Output should conform to the tags below. 
    39| 2. If tags are not given, make them on your own.
    40| 3. Remember to always open and close all the tags.
    41| As an example, for the tags ["foo", "bar", "baz"]:
    42| 1. String "<foo>\n   <bar>\n      <baz></baz>\n   </bar>\n</foo>" is a well-formatted instance of the schema. 
    43| 2. String "<foo>\n   <bar>\n   </foo>" is a badly-formatted instance.
    44| 3. String "<foo>\n   <tag>\n   </tag>\n</foo>" is a badly-formatted instance.
    45| Here are the output tags:
    46| ```
    47| {tags}
    48| ```"""
    49| PANDAS_DATAFRAME_FORMAT_INSTRUCTIONS = """The output should be formatted as a string as the operation, followed by a colon, followed by the column or row to be queried on, followed by optional array parameters.
    50| 1. The column names are limited to the possible columns below.
    51| 2. Arrays must either be a comma-seperated list of numbers formatted as [1,3,5], or it must be in range of numbers formatted as [0..4].
    52| 3. Remember that arrays are optional and not necessarily required.
    53| 4. If the column is not in the possible columns or the operation is not a valid Pandas DataFrame operation, return why it is invalid as a sentence starting with either "Invalid column" or "Invalid operation".
    54| As an example, for the formats:
    55| 1. String "column:num_legs" is a well-formatted instance which gets the column num_legs, where num_legs is a possible column.
    56| 2. String "row:1" is a well-formatted instance which gets row 1.
    57| 3. String "column:num_legs[1,2]" is a well-formatted instance which gets the column num_legs for rows 1 and 2, where num_legs is a possible column.
    58| 4. String "row:1[num_legs]" is a well-formatted instance which gets row 1, but for just column num_legs, where num_legs is a possible column.
    59| 5. String "mean:num_legs[1..3]" is a well-formatted instance which takes the mean of num_legs from rows 1 to 3, where num_legs is a possible column and mean is a valid Pandas DataFrame operation.
    60| 6. String "do_something:num_legs" is a badly-formatted instance, where do_something is not a valid Pandas DataFrame operation.
    61| 7. String "mean:invalid_col" is a badly-formatted instance, where invalid_col is not a possible column.
    62| Here are the possible columns:
    63| ```
    64| {columns}
    65| ```
    66| """


# ====================================================================
# FILE: libs/langchain/langchain/output_parsers/json.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-137 ---
     1| from __future__ import annotations
     2| import json
     3| import re
     4| from json import JSONDecodeError
     5| from typing import Any, Callable, List, Optional
     6| import jsonpatch
     7| from langchain_core.exceptions import OutputParserException
     8| from langchain_core.output_parsers import BaseCumulativeTransformOutputParser
     9| def _replace_new_line(match: re.Match[str]) -> str:
    10|     value = match.group(2)
    11|     value = re.sub(r"\n", r"\\n", value)
    12|     value = re.sub(r"\r", r"\\r", value)
    13|     value = re.sub(r"\t", r"\\t", value)
    14|     value = re.sub(r'(?<!\\)"', r"\"", value)
    15|     return match.group(1) + value + match.group(3)
    16| def _custom_parser(multiline_string: str) -> str:
    17|     """
    18|     The LLM response for `action_input` may be a multiline
    19|     string containing unescaped newlines, tabs or quotes. This function
    20|     replaces those characters with their escaped counterparts.
    21|     (newlines in JSON must be double-escaped: `\\n`)
    22|     """
    23|     if isinstance(multiline_string, (bytes, bytearray)):
    24|         multiline_string = multiline_string.decode()
    25|     multiline_string = re.sub(
    26|         r'("action_input"\:\s*")(.*)(")',
    27|         _replace_new_line,
    28|         multiline_string,
    29|         flags=re.DOTALL,
    30|     )
    31|     return multiline_string
    32| def parse_partial_json(s: str, *, strict: bool = False) -> Any:
    33|     """Parse a JSON string that may be missing closing braces.
    34|     Args:
    35|         s: The JSON string to parse.
    36|         strict: Whether to use strict parsing. Defaults to False.
    37|     Returns:
    38|         The parsed JSON object as a Python dictionary.
    39|     """
    40|     try:
    41|         return json.loads(s, strict=strict)
    42|     except json.JSONDecodeError:
    43|         pass
    44|     new_s = ""
    45|     stack = []
    46|     is_inside_string = False
    47|     escaped = False
    48|     for char in s:
    49|         if is_inside_string:
    50|             if char == '"' and not escaped:
    51|                 is_inside_string = False
    52|             elif char == "\n" and not escaped:
    53|                 char = "\\n"  # Replace the newline character with the escape sequence.
    54|             elif char == "\\":
    55|                 escaped = not escaped
    56|             else:
    57|                 escaped = False
    58|         else:
    59|             if char == '"':
    60|                 is_inside_string = True
    61|                 escaped = False
    62|             elif char == "{":
    63|                 stack.append("}")
    64|             elif char == "[":
    65|                 stack.append("]")
    66|             elif char == "}" or char == "]":
    67|                 if stack and stack[-1] == char:
    68|                     stack.pop()
    69|                 else:
    70|                     return None
    71|         new_s += char
    72|     if is_inside_string:
    73|         new_s += '"'
    74|     for closing_char in reversed(stack):
    75|         new_s += closing_char
    76|     try:
    77|         return json.loads(new_s, strict=strict)
    78|     except json.JSONDecodeError:
    79|         return None
    80| def parse_json_markdown(
    81|     json_string: str, *, parser: Callable[[str], Any] = json.loads
    82| ) -> dict:
    83|     """
    84|     Parse a JSON string from a Markdown string.
    85|     Args:
    86|         json_string: The Markdown string.
    87|     Returns:
    88|         The parsed JSON object as a Python dictionary.
    89|     """
    90|     match = re.search(r"```(json)?(.*)```", json_string, re.DOTALL)
    91|     if match is None:
    92|         json_str = json_string
    93|     else:
    94|         json_str = match.group(2)
    95|     json_str = json_str.strip()
    96|     json_str = _custom_parser(json_str)
    97|     parsed = parser(json_str)
    98|     return parsed
    99| def parse_and_check_json_markdown(text: str, expected_keys: List[str]) -> dict:
   100|     """
   101|     Parse a JSON string from a Markdown string and check that it
   102|     contains the expected keys.
   103|     Args:
   104|         text: The Markdown string.
   105|         expected_keys: The expected keys in the JSON string.
   106|     Returns:
   107|         The parsed JSON object as a Python dictionary.
   108|     """
   109|     try:
   110|         json_obj = parse_json_markdown(text)
   111|     except json.JSONDecodeError as e:
   112|         raise OutputParserException(f"Got invalid JSON object. Error: {e}")
   113|     for key in expected_keys:
   114|         if key not in json_obj:
   115|             raise OutputParserException(
   116|                 f"Got invalid return object. Expected key `{key}` "
   117|                 f"to be present, but got {json_obj}"
   118|             )
   119|     return json_obj
   120| class SimpleJsonOutputParser(BaseCumulativeTransformOutputParser[Any]):
   121|     """Parse the output of an LLM call to a JSON object.
   122|     When used in streaming mode, it will yield partial JSON objects containing
   123|     all the keys that have been returned so far.
   124|     In streaming, if `diff` is set to `True`, yields JSONPatch operations
   125|     describing the difference between the previous and the current object.
   126|     """
   127|     def _diff(self, prev: Optional[Any], next: Any) -> Any:
   128|         return jsonpatch.make_patch(prev, next).patch
   129|     def parse(self, text: str) -> Any:
   130|         text = text.strip()
   131|         try:
   132|             return parse_json_markdown(text.strip(), parser=parse_partial_json)
   133|         except JSONDecodeError as e:
   134|             raise OutputParserException(f"Invalid json output: {text}") from e
   135|     @property
   136|     def _type(self) -> str:
   137|         return "simple_json_output_parser"


# ====================================================================
# FILE: libs/langchain/langchain/output_parsers/retry.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 25-65 ---
    25| T = TypeVar("T")
    26| class RetryOutputParser(BaseOutputParser[T]):
    27|     """Wraps a parser and tries to fix parsing errors.
    28|     Does this by passing the original prompt and the completion to another
    29|     LLM, and telling it the completion did not satisfy criteria in the prompt.
    30|     """
    31|     parser: BaseOutputParser[T]
    32|     """The parser to use to parse the output."""
    33|     retry_chain: Any
    34|     """The LLMChain to use to retry the completion."""
    35|     max_retries: int = 1
    36|     """The maximum number of times to retry the parse."""
    37|     @classmethod
    38|     def from_llm(
    39|         cls,
    40|         llm: BaseLanguageModel,
    41|         parser: BaseOutputParser[T],
    42|         prompt: BasePromptTemplate = NAIVE_RETRY_PROMPT,
    43|         max_retries: int = 1,
    44|     ) -> RetryOutputParser[T]:
    45|         """Create an OutputFixingParser from a language model and a parser.
    46|         Args:
    47|             llm: llm to use for fixing
    48|             parser: parser to use for parsing
    49|             prompt: prompt to use for fixing
    50|             max_retries: Maximum number of retries to parse.
    51|         Returns:
    52|             RetryOutputParser
    53|         """
    54|         from langchain.chains.llm import LLMChain
    55|         chain = LLMChain(llm=llm, prompt=prompt)
    56|         return cls(parser=parser, retry_chain=chain, max_retries=max_retries)
    57|     def parse_with_prompt(self, completion: str, prompt_value: PromptValue) -> T:
    58|         """Parse the output of an LLM call using a wrapped parser.
    59|         Args:
    60|             completion: The chain completion to parse.
    61|             prompt_value: The prompt to use to parse the completion.
    62|         Returns:
    63|             The parsed completion.
    64|         """
    65|         retries = 0


# ====================================================================
# FILE: libs/langchain/langchain/output_parsers/xml.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-38 ---
     1| import re
     2| import xml.etree.ElementTree as ET
     3| from typing import Any, Dict, List, Optional
     4| from langchain_core.output_parsers import BaseOutputParser
     5| from langchain.output_parsers.format_instructions import XML_FORMAT_INSTRUCTIONS
     6| class XMLOutputParser(BaseOutputParser):
     7|     """Parse an output using xml format."""
     8|     tags: Optional[List[str]] = None
     9|     encoding_matcher: re.Pattern = re.compile(
    10|         r"<([^>]*encoding[^>]*)>\n(.*)", re.MULTILINE | re.DOTALL
    11|     )
    12|     def get_format_instructions(self) -> str:
    13|         return XML_FORMAT_INSTRUCTIONS.format(tags=self.tags)
    14|     def parse(self, text: str) -> Dict[str, List[Any]]:
    15|         text = text.strip("`").strip("xml")
    16|         encoding_match = self.encoding_matcher.search(text)
    17|         if encoding_match:
    18|             text = encoding_match.group(2)
    19|         text = text.strip()
    20|         if (text.startswith("<") or text.startswith("\n<")) and (
    21|             text.endswith(">") or text.endswith(">\n")
    22|         ):
    23|             root = ET.fromstring(text)
    24|             return self._root_to_dict(root)
    25|         else:
    26|             raise ValueError(f"Could not parse output: {text}")
    27|     def _root_to_dict(self, root: ET.Element) -> Dict[str, List[Any]]:
    28|         """Converts xml tree to python dictionary."""
    29|         result: Dict[str, List[Any]] = {root.tag: []}
    30|         for child in root:
    31|             if len(child) == 0:
    32|                 result[root.tag].append({child.tag: child.text})
    33|             else:
    34|                 result[root.tag].append(self._root_to_dict(child))
    35|         return result
    36|     @property
    37|     def _type(self) -> str:
    38|         return "xml"


# ====================================================================
# FILE: libs/langchain/langchain/storage/file_system.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-55 ---
     1| import re
     2| from pathlib import Path
     3| from typing import Iterator, List, Optional, Sequence, Tuple, Union
     4| from langchain_core.stores import ByteStore
     5| from langchain.storage.exceptions import InvalidKeyException
     6| class LocalFileStore(ByteStore):
     7|     """BaseStore interface that works on the local file system.
     8|     Examples:
     9|         Create a LocalFileStore instance and perform operations on it:
    10|         .. code-block:: python
    11|             from langchain.storage import LocalFileStore
    12|             file_store = LocalFileStore("/path/to/root")
    13|             file_store.mset([("key1", b"value1"), ("key2", b"value2")])
    14|             values = file_store.mget(["key1", "key2"])  # Returns [b"value1", b"value2"]
    15|             file_store.mdelete(["key1"])
    16|             for key in file_store.yield_keys():
    17|                 print(key)
    18|     """
    19|     def __init__(self, root_path: Union[str, Path]) -> None:
    20|         """Implement the BaseStore interface for the local file system.
    21|         Args:
    22|             root_path (Union[str, Path]): The root path of the file store. All keys are
    23|                 interpreted as paths relative to this root.
    24|         """
    25|         self.root_path = Path(root_path)
    26|     def _get_full_path(self, key: str) -> Path:
    27|         """Get the full path for a given key relative to the root path.
    28|         Args:
    29|             key (str): The key relative to the root path.
    30|         Returns:
    31|             Path: The full path for the given key.
    32|         """
    33|         if not re.match(r"^[a-zA-Z0-9_.\-/]+$", key):
    34|             raise InvalidKeyException(f"Invalid characters in key: {key}")
    35|         return self.root_path / key
    36|     def mget(self, keys: Sequence[str]) -> List[Optional[bytes]]:
    37|         """Get the values associated with the given keys.
    38|         Args:
    39|             keys: A sequence of keys.
    40|         Returns:
    41|             A sequence of optional values associated with the keys.
    42|             If a key is not found, the corresponding value will be None.
    43|         """
    44|         values: List[Optional[bytes]] = []
    45|         for key in keys:
    46|             full_path = self._get_full_path(key)
    47|             if full_path.exists():
    48|                 value = full_path.read_bytes()
    49|                 values.append(value)
    50|             else:
    51|                 values.append(None)
    52|         return values
    53|     def mset(self, key_value_pairs: Sequence[Tuple[str, bytes]]) -> None:
    54|         """Set the values for the given keys.
    55|         Args:


# ====================================================================
# FILE: templates/rag-chroma-multi-modal-multi-vector/ingest.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-33 ---
     1| import base64
     2| import io
     3| import os
     4| import uuid
     5| from io import BytesIO
     6| from pathlib import Path
     7| import pypdfium2 as pdfium
     8| from langchain.chat_models import ChatOpenAI
     9| from langchain.embeddings import OpenAIEmbeddings
    10| from langchain.retrievers.multi_vector import MultiVectorRetriever
    11| from langchain.schema.document import Document
    12| from langchain.schema.messages import HumanMessage
    13| from langchain.storage import UpstashRedisByteStore
    14| from langchain.vectorstores import Chroma
    15| from PIL import Image
    16| def image_summarize(img_base64, prompt):
    17|     """
    18|     Make image summary
    19|     :param img_base64: Base64 encoded string for image
    20|     :param prompt: Text prompt for summarizatiomn
    21|     :return: Image summarization prompt
    22|     """
    23|     chat = ChatOpenAI(model="gpt-4-vision-preview", max_tokens=1024)
    24|     msg = chat.invoke(
    25|         [
    26|             HumanMessage(
    27|                 content=[
    28|                     {"type": "text", "text": prompt},
    29|                     {
    30|                         "type": "image_url",
    31|                         "image_url": {"url": f"data:image/jpeg;base64,{img_base64}"},
    32|                     },
    33|                 ]

# --- HUNK 2: Lines 74-140 ---
    74|     :param size: Image size
    75|     :return: Re-sized Base64 string
    76|     """
    77|     img_data = base64.b64decode(base64_string)
    78|     img = Image.open(io.BytesIO(img_data))
    79|     resized_img = img.resize(size, Image.LANCZOS)
    80|     buffered = io.BytesIO()
    81|     resized_img.save(buffered, format=img.format)
    82|     return base64.b64encode(buffered.getvalue()).decode("utf-8")
    83| def convert_to_base64(pil_image):
    84|     """
    85|     Convert PIL images to Base64 encoded strings
    86|     :param pil_image: PIL image
    87|     :return: Re-sized Base64 string
    88|     """
    89|     buffered = BytesIO()
    90|     pil_image.save(buffered, format="JPEG")  # You can change the format if needed
    91|     img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
    92|     img_str = resize_base64_image(img_str, size=(960, 540))
    93|     return img_str
    94| def create_multi_vector_retriever(vectorstore, image_summaries, images):
    95|     """
    96|     Create retriever that indexes summaries, but returns raw images or texts
    97|     :param vectorstore: Vectorstore to store embedded image sumamries
    98|     :param image_summaries: Image summaries
    99|     :param images: Base64 encoded images
   100|     :return: Retriever
   101|     """
   102|     UPSTASH_URL = os.getenv("UPSTASH_URL")
   103|     UPSTASH_TOKEN = os.getenv("UPSTASH_TOKEN")
   104|     store = UpstashRedisByteStore(url=UPSTASH_URL, token=UPSTASH_TOKEN)
   105|     id_key = "doc_id"
   106|     retriever = MultiVectorRetriever(
   107|         vectorstore=vectorstore,
   108|         byte_store=store,
   109|         id_key=id_key,
   110|     )
   111|     def add_documents(retriever, doc_summaries, doc_contents):
   112|         doc_ids = [str(uuid.uuid4()) for _ in doc_contents]
   113|         summary_docs = [
   114|             Document(page_content=s, metadata={id_key: doc_ids[i]})
   115|             for i, s in enumerate(doc_summaries)
   116|         ]
   117|         retriever.vectorstore.add_documents(summary_docs)
   118|         retriever.docstore.mset(list(zip(doc_ids, doc_contents)))
   119|     add_documents(retriever, image_summaries, images)
   120|     return retriever
   121| doc_path = Path(__file__).parent / "docs/DDOG_Q3_earnings_deck.pdf"
   122| rel_doc_path = doc_path.relative_to(Path.cwd())
   123| print("Extract slides as images")
   124| pil_images = get_images_from_pdf(rel_doc_path)
   125| images_base_64 = [convert_to_base64(i) for i in pil_images]
   126| print("Generate image summaries")
   127| image_summaries, images_base_64_processed = generate_img_summaries(images_base_64)
   128| vectorstore_mvr = Chroma(
   129|     collection_name="image_summaries",
   130|     persist_directory=str(Path(__file__).parent / "chroma_db_multi_modal"),
   131|     embedding_function=OpenAIEmbeddings(),
   132| )
   133| images_base_64_processed_documents = [
   134|     Document(page_content=i) for i in images_base_64_processed
   135| ]
   136| retriever_multi_vector_img = create_multi_vector_retriever(
   137|     vectorstore_mvr,
   138|     image_summaries,
   139|     images_base_64_processed_documents,
   140| )


# ====================================================================
# FILE: templates/rag-chroma-multi-modal-multi-vector/rag_chroma_multi_modal_multi_vector/chain.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-33 ---
     1| import base64
     2| import io
     3| import os
     4| from pathlib import Path
     5| from langchain.chat_models import ChatOpenAI
     6| from langchain.embeddings import OpenAIEmbeddings
     7| from langchain.pydantic_v1 import BaseModel
     8| from langchain.retrievers.multi_vector import MultiVectorRetriever
     9| from langchain.schema.document import Document
    10| from langchain.schema.messages import HumanMessage
    11| from langchain.schema.output_parser import StrOutputParser
    12| from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
    13| from langchain.storage import UpstashRedisByteStore
    14| from langchain.vectorstores import Chroma
    15| from PIL import Image
    16| def resize_base64_image(base64_string, size=(128, 128)):
    17|     """
    18|     Resize an image encoded as a Base64 string.
    19|     :param base64_string: A Base64 encoded string of the image to be resized.
    20|     :param size: A tuple representing the new size (width, height) for the image.
    21|     :return: A Base64 encoded string of the resized image.
    22|     """
    23|     img_data = base64.b64decode(base64_string)
    24|     img = Image.open(io.BytesIO(img_data))
    25|     resized_img = img.resize(size, Image.LANCZOS)
    26|     buffered = io.BytesIO()
    27|     resized_img.save(buffered, format=img.format)
    28|     return base64.b64encode(buffered.getvalue()).decode("utf-8")
    29| def get_resized_images(docs):
    30|     """
    31|     Resize images from base64-encoded strings.
    32|     :param docs: A list of base64-encoded image to be resized.
    33|     :return: Dict containing a list of resized base64-encoded strings.

# --- HUNK 2: Lines 65-102 ---
    65|     }
    66|     messages.append(text_message)
    67|     return [HumanMessage(content=messages)]
    68| def multi_modal_rag_chain(retriever):
    69|     """
    70|     Multi-modal RAG chain,
    71|     :param retriever: A function that retrieves the necessary context for the model.
    72|     :return: A chain of functions representing the multi-modal RAG process.
    73|     """
    74|     model = ChatOpenAI(temperature=0, model="gpt-4-vision-preview", max_tokens=1024)
    75|     chain = (
    76|         {
    77|             "context": retriever | RunnableLambda(get_resized_images),
    78|             "question": RunnablePassthrough(),
    79|         }
    80|         | RunnableLambda(img_prompt_func)
    81|         | model
    82|         | StrOutputParser()
    83|     )
    84|     return chain
    85| vectorstore_mvr = Chroma(
    86|     collection_name="image_summaries",
    87|     persist_directory=str(Path(__file__).parent.parent / "chroma_db_multi_modal"),
    88|     embedding_function=OpenAIEmbeddings(),
    89| )
    90| UPSTASH_URL = os.getenv("UPSTASH_URL")
    91| UPSTASH_TOKEN = os.getenv("UPSTASH_TOKEN")
    92| store = UpstashRedisByteStore(url=UPSTASH_URL, token=UPSTASH_TOKEN)
    93| id_key = "doc_id"
    94| retriever = MultiVectorRetriever(
    95|     vectorstore=vectorstore_mvr,
    96|     byte_store=store,
    97|     id_key=id_key,
    98| )
    99| chain = multi_modal_rag_chain(retriever)
   100| class Question(BaseModel):
   101|     __root__: str
   102| chain = chain.with_types(input_type=Question)


# ====================================================================
# FILE: templates/rag-vectara-multiquery/rag_vectara_multiquery/chain.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| import os
     2| from langchain.chat_models import ChatOpenAI
     3| from langchain.prompts import ChatPromptTemplate
     4| from langchain.retrievers.multi_query import MultiQueryRetriever
     5| from langchain.vectorstores import Vectara
     6| from langchain_core.output_parsers import StrOutputParser
     7| from langchain_core.pydantic_v1 import BaseModel
     8| from langchain_core.runnables import RunnableParallel, RunnablePassthrough
     9| if os.environ.get("VECTARA_CUSTOMER_ID", None) is None:
    10|     raise Exception("Missing `VECTARA_CUSTOMER_ID` environment variable.")
    11| if os.environ.get("VECTARA_CORPUS_ID", None) is None:
    12|     raise Exception("Missing `VECTARA_CORPUS_ID` environment variable.")
    13| if os.environ.get("VECTARA_API_KEY", None) is None:
    14|     raise Exception("Missing `VECTARA_API_KEY` environment variable.")
    15| llm = ChatOpenAI(temperature=0)
    16| retriever = MultiQueryRetriever.from_llm(retriever=Vectara().as_retriever(), llm=llm)
    17| template = """Answer the question based only on the following context:
    18| {context}
    19| Question: {question}
    20| """
    21| prompt = ChatPromptTemplate.from_template(template)
    22| model = ChatOpenAI()
    23| chain = (
    24|     RunnableParallel({"context": retriever, "question": RunnablePassthrough()})
    25|     | prompt
    26|     | model
    27|     | StrOutputParser()
    28| )
    29| class Question(BaseModel):
    30|     __root__: str
    31| chain = chain.with_types(input_type=Question)


# ====================================================================
# FILE: templates/rag-vectara/rag_vectara/chain.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-29 ---
     1| import os
     2| from langchain.chat_models import ChatOpenAI
     3| from langchain.prompts import ChatPromptTemplate
     4| from langchain.vectorstores import Vectara
     5| from langchain_core.output_parsers import StrOutputParser
     6| from langchain_core.pydantic_v1 import BaseModel
     7| from langchain_core.runnables import RunnableParallel, RunnablePassthrough
     8| if os.environ.get("VECTARA_CUSTOMER_ID", None) is None:
     9|     raise Exception("Missing `VECTARA_CUSTOMER_ID` environment variable.")
    10| if os.environ.get("VECTARA_CORPUS_ID", None) is None:
    11|     raise Exception("Missing `VECTARA_CORPUS_ID` environment variable.")
    12| if os.environ.get("VECTARA_API_KEY", None) is None:
    13|     raise Exception("Missing `VECTARA_API_KEY` environment variable.")
    14| retriever = Vectara().as_retriever()
    15| template = """Answer the question based only on the following context:
    16| {context}
    17| Question: {question}
    18| """
    19| prompt = ChatPromptTemplate.from_template(template)
    20| model = ChatOpenAI()
    21| chain = (
    22|     RunnableParallel({"context": retriever, "question": RunnablePassthrough()})
    23|     | prompt
    24|     | model
    25|     | StrOutputParser()
    26| )
    27| class Question(BaseModel):
    28|     __root__: str
    29| chain = chain.with_types(input_type=Question)

