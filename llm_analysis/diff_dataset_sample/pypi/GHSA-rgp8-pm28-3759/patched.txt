# ====================================================================
# FILE: docs/sidebars.js
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 93-143 ---
    93|       label: "Components",
    94|       collapsible: false,
    95|       items: [
    96|         { type: "category", label: "LLMs", collapsed: true, items: [{type:"autogenerated", dirName: "integrations/llms" }], link: { type: 'doc', id: "integrations/llms/index"}},
    97|         { type: "category", label: "Chat models", collapsed: true, items: [{type:"autogenerated", dirName: "integrations/chat" }], link: { type: 'doc', id: "integrations/chat/index"}},
    98|         { type: "category", label: "Document loaders", collapsed: true, items: [{type:"autogenerated", dirName: "integrations/document_loaders" }], link: {type: "generated-index", slug: "integrations/document_loaders" }},
    99|         { type: "category", label: "Document transformers", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/document_transformers" }], link: {type: "generated-index", slug: "integrations/document_transformers" }},
   100|         { type: "category", label: "Text embedding models", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/text_embedding" }], link: {type: "generated-index", slug: "integrations/text_embedding" }},
   101|         { type: "category", label: "Vector stores", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/vectorstores" }], link: {type: "generated-index", slug: "integrations/vectorstores" }},
   102|         { type: "category", label: "Retrievers", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/retrievers" }], link: {type: "generated-index", slug: "integrations/retrievers" }},
   103|         { type: "category", label: "Tools", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/tools" }], link: {type: "generated-index", slug: "integrations/tools" }},
   104|         { type: "category", label: "Agents and toolkits", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/toolkits" }], link: {type: "generated-index", slug: "integrations/toolkits" }},
   105|         { type: "category", label: "Memory", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/memory" }], link: {type: "generated-index", slug: "integrations/memory" }},
   106|         { type: "category", label: "Callbacks", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/callbacks" }], link: {type: "generated-index", slug: "integrations/callbacks" }},
   107|         { type: "category", label: "Chat loaders", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/chat_loaders" }], link: {type: "generated-index", slug: "integrations/chat_loaders" }},
   108|         { type: "category", label: "Adapters", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/adapters" }], link: {type: "generated-index", slug: "integrations/adapters" }},
   109|         { type: "category", label: "Stores", collapsed: true, items: [{type: "autogenerated", dirName: "integrations/stores" }], link: {type: "doc", id: "integrations/stores/index" }},
   110|       ],
   111|       link: {
   112|         type: 'generated-index',
   113|         slug: "integrations/components",
   114|       },
   115|     },
   116|   ],
   117|   use_cases: [
   118|     {
   119|       type: "category",
   120|       label: "Use cases",
   121|       items: [
   122|         { type: "autogenerated", dirName: "use_cases" },
   123|       ],
   124|       link: { type: 'generated-index', slug: "use_cases"}
   125|     },
   126|   ],
   127|   guides: [
   128|     {type: "autogenerated", dirName: "guides" }
   129|   ],
   130|   templates: [
   131|     {
   132|       type: "category",
   133|       label: "Templates",
   134|       items: [
   135|         { type: "autogenerated", dirName: "templates" },
   136|       ],
   137|       link: { type: 'doc', id: "templates/index" }
   138|     },
   139|   ],
   140|   contributing: [
   141|     {type: "autogenerated", dirName: "contributing" }
   142|   ],
   143| };


# ====================================================================
# FILE: libs/community/langchain_community/callbacks/openai_info.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-22 ---
     1| """Callback Handler that prints to std out."""
     2| import threading
     3| from typing import Any, Dict, List
     4| from langchain_core.callbacks import BaseCallbackHandler
     5| from langchain_core.outputs import LLMResult
     6| MODEL_COST_PER_1K_TOKENS = {
     7|     "gpt-4": 0.03,
     8|     "gpt-4-0314": 0.03,
     9|     "gpt-4-0613": 0.03,
    10|     "gpt-4-32k": 0.06,
    11|     "gpt-4-32k-0314": 0.06,
    12|     "gpt-4-32k-0613": 0.06,
    13|     "gpt-4-vision-preview": 0.01,
    14|     "gpt-4-1106-preview": 0.01,
    15|     "gpt-4-completion": 0.06,
    16|     "gpt-4-0314-completion": 0.06,
    17|     "gpt-4-0613-completion": 0.06,
    18|     "gpt-4-32k-completion": 0.12,
    19|     "gpt-4-32k-0314-completion": 0.12,
    20|     "gpt-4-32k-0613-completion": 0.12,
    21|     "gpt-4-vision-preview-completion": 0.03,
    22|     "gpt-4-1106-preview-completion": 0.03,

# --- HUNK 2: Lines 111-185 ---
   111|         num_tokens: Number of tokens.
   112|         is_completion: Whether the model is used for completion or not.
   113|             Defaults to False.
   114|     Returns:
   115|         Cost in USD.
   116|     """
   117|     model_name = standardize_model_name(model_name, is_completion=is_completion)
   118|     if model_name not in MODEL_COST_PER_1K_TOKENS:
   119|         raise ValueError(
   120|             f"Unknown model: {model_name}. Please provide a valid OpenAI model name."
   121|             "Known models are: " + ", ".join(MODEL_COST_PER_1K_TOKENS.keys())
   122|         )
   123|     return MODEL_COST_PER_1K_TOKENS[model_name] * (num_tokens / 1000)
   124| class OpenAICallbackHandler(BaseCallbackHandler):
   125|     """Callback Handler that tracks OpenAI info."""
   126|     total_tokens: int = 0
   127|     prompt_tokens: int = 0
   128|     completion_tokens: int = 0
   129|     successful_requests: int = 0
   130|     total_cost: float = 0.0
   131|     def __init__(self) -> None:
   132|         super().__init__()
   133|         self._lock = threading.Lock()
   134|     def __repr__(self) -> str:
   135|         return (
   136|             f"Tokens Used: {self.total_tokens}\n"
   137|             f"\tPrompt Tokens: {self.prompt_tokens}\n"
   138|             f"\tCompletion Tokens: {self.completion_tokens}\n"
   139|             f"Successful Requests: {self.successful_requests}\n"
   140|             f"Total Cost (USD): ${self.total_cost}"
   141|         )
   142|     @property
   143|     def always_verbose(self) -> bool:
   144|         """Whether to call verbose callbacks even if verbose is False."""
   145|         return True
   146|     def on_llm_start(
   147|         self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
   148|     ) -> None:
   149|         """Print out the prompts."""
   150|         pass
   151|     def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
   152|         """Print out the token."""
   153|         pass
   154|     def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
   155|         """Collect token usage."""
   156|         if response.llm_output is None:
   157|             return None
   158|         if "token_usage" not in response.llm_output:
   159|             with self._lock:
   160|                 self.successful_requests += 1
   161|             return None
   162|         token_usage = response.llm_output["token_usage"]
   163|         completion_tokens = token_usage.get("completion_tokens", 0)
   164|         prompt_tokens = token_usage.get("prompt_tokens", 0)
   165|         model_name = standardize_model_name(response.llm_output.get("model_name", ""))
   166|         if model_name in MODEL_COST_PER_1K_TOKENS:
   167|             completion_cost = get_openai_token_cost_for_model(
   168|                 model_name, completion_tokens, is_completion=True
   169|             )
   170|             prompt_cost = get_openai_token_cost_for_model(model_name, prompt_tokens)
   171|         else:
   172|             completion_cost = 0
   173|             prompt_cost = 0
   174|         with self._lock:
   175|             self.total_cost += prompt_cost + completion_cost
   176|             self.total_tokens += token_usage.get("total_tokens", 0)
   177|             self.prompt_tokens += prompt_tokens
   178|             self.completion_tokens += completion_tokens
   179|             self.successful_requests += 1
   180|     def __copy__(self) -> "OpenAICallbackHandler":
   181|         """Return a copy of the callback handler."""
   182|         return self
   183|     def __deepcopy__(self, memo: Any) -> "OpenAICallbackHandler":
   184|         """Return a deep copy of the callback handler."""
   185|         return self


# ====================================================================
# FILE: libs/community/langchain_community/chat_message_histories/elasticsearch.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 3-63 ---
     3| from time import time
     4| from typing import TYPE_CHECKING, Any, Dict, List, Optional
     5| from langchain_core.chat_history import BaseChatMessageHistory
     6| from langchain_core.messages import (
     7|     BaseMessage,
     8|     message_to_dict,
     9|     messages_from_dict,
    10| )
    11| if TYPE_CHECKING:
    12|     from elasticsearch import Elasticsearch
    13| logger = logging.getLogger(__name__)
    14| class ElasticsearchChatMessageHistory(BaseChatMessageHistory):
    15|     """Chat message history that stores history in Elasticsearch.
    16|     Args:
    17|         es_url: URL of the Elasticsearch instance to connect to.
    18|         es_cloud_id: Cloud ID of the Elasticsearch instance to connect to.
    19|         es_user: Username to use when connecting to Elasticsearch.
    20|         es_password: Password to use when connecting to Elasticsearch.
    21|         es_api_key: API key to use when connecting to Elasticsearch.
    22|         es_connection: Optional pre-existing Elasticsearch connection.
    23|         esnsure_ascii: Used to escape ASCII symbols in json.dumps. Defaults to True.
    24|         index: Name of the index to use.
    25|         session_id: Arbitrary key that is used to store the messages
    26|             of a single chat session.
    27|     """
    28|     def __init__(
    29|         self,
    30|         index: str,
    31|         session_id: str,
    32|         *,
    33|         es_connection: Optional["Elasticsearch"] = None,
    34|         es_url: Optional[str] = None,
    35|         es_cloud_id: Optional[str] = None,
    36|         es_user: Optional[str] = None,
    37|         es_api_key: Optional[str] = None,
    38|         es_password: Optional[str] = None,
    39|         esnsure_ascii: Optional[bool] = True,
    40|     ):
    41|         self.index: str = index
    42|         self.session_id: str = session_id
    43|         self.ensure_ascii: bool = esnsure_ascii
    44|         if es_connection is not None:
    45|             self.client = es_connection.options(
    46|                 headers={"user-agent": self.get_user_agent()}
    47|             )
    48|         elif es_url is not None or es_cloud_id is not None:
    49|             self.client = ElasticsearchChatMessageHistory.connect_to_elasticsearch(
    50|                 es_url=es_url,
    51|                 username=es_user,
    52|                 password=es_password,
    53|                 cloud_id=es_cloud_id,
    54|                 api_key=es_api_key,
    55|             )
    56|         else:
    57|             raise ValueError(
    58|                 """Either provide a pre-existing Elasticsearch connection, \
    59|                 or valid credentials for creating a new connection."""
    60|             )
    61|         if self.client.indices.exists(index=index):
    62|             logger.debug(
    63|                 f"Chat history index {index} already exists, skipping creation."

# --- HUNK 2: Lines 132-173 ---
   132|         except ApiError as err:
   133|             logger.error(f"Could not retrieve messages from Elasticsearch: {err}")
   134|             raise err
   135|         if result and len(result["hits"]["hits"]) > 0:
   136|             items = [
   137|                 json.loads(document["_source"]["history"])
   138|                 for document in result["hits"]["hits"]
   139|             ]
   140|         else:
   141|             items = []
   142|         return messages_from_dict(items)
   143|     def add_message(self, message: BaseMessage) -> None:
   144|         """Add a message to the chat session in Elasticsearch"""
   145|         try:
   146|             from elasticsearch import ApiError
   147|             self.client.index(
   148|                 index=self.index,
   149|                 document={
   150|                     "session_id": self.session_id,
   151|                     "created_at": round(time() * 1000),
   152|                     "history": json.dumps(
   153|                         message_to_dict(message),
   154|                         ensure_ascii=self.ensure_ascii,
   155|                     ),
   156|                 },
   157|                 refresh=True,
   158|             )
   159|         except ApiError as err:
   160|             logger.error(f"Could not add message to Elasticsearch: {err}")
   161|             raise err
   162|     def clear(self) -> None:
   163|         """Clear session memory in Elasticsearch"""
   164|         try:
   165|             from elasticsearch import ApiError
   166|             self.client.delete_by_query(
   167|                 index=self.index,
   168|                 query={"term": {"session_id": self.session_id}},
   169|                 refresh=True,
   170|             )
   171|         except ApiError as err:
   172|             logger.error(f"Could not clear session memory in Elasticsearch: {err}")
   173|             raise err


# ====================================================================
# FILE: libs/community/langchain_community/chat_models/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 7-77 ---
     7|     BaseLanguageModel --> BaseChatModel --> <name>  # Examples: ChatOpenAI, ChatGooglePalm
     8| **Main helpers:**
     9| .. code-block::
    10|     AIMessage, BaseMessage, HumanMessage
    11| """  # noqa: E501
    12| from langchain_community.chat_models.anthropic import ChatAnthropic
    13| from langchain_community.chat_models.anyscale import ChatAnyscale
    14| from langchain_community.chat_models.azure_openai import AzureChatOpenAI
    15| from langchain_community.chat_models.baichuan import ChatBaichuan
    16| from langchain_community.chat_models.baidu_qianfan_endpoint import QianfanChatEndpoint
    17| from langchain_community.chat_models.bedrock import BedrockChat
    18| from langchain_community.chat_models.cohere import ChatCohere
    19| from langchain_community.chat_models.databricks import ChatDatabricks
    20| from langchain_community.chat_models.ernie import ErnieBotChat
    21| from langchain_community.chat_models.everlyai import ChatEverlyAI
    22| from langchain_community.chat_models.fake import FakeListChatModel
    23| from langchain_community.chat_models.fireworks import ChatFireworks
    24| from langchain_community.chat_models.gigachat import GigaChat
    25| from langchain_community.chat_models.google_palm import ChatGooglePalm
    26| from langchain_community.chat_models.gpt_router import GPTRouter
    27| from langchain_community.chat_models.huggingface import ChatHuggingFace
    28| from langchain_community.chat_models.human import HumanInputChatModel
    29| from langchain_community.chat_models.hunyuan import ChatHunyuan
    30| from langchain_community.chat_models.javelin_ai_gateway import ChatJavelinAIGateway
    31| from langchain_community.chat_models.jinachat import JinaChat
    32| from langchain_community.chat_models.konko import ChatKonko
    33| from langchain_community.chat_models.litellm import ChatLiteLLM
    34| from langchain_community.chat_models.minimax import MiniMaxChat
    35| from langchain_community.chat_models.mlflow import ChatMlflow
    36| from langchain_community.chat_models.mlflow_ai_gateway import ChatMLflowAIGateway
    37| from langchain_community.chat_models.ollama import ChatOllama
    38| from langchain_community.chat_models.openai import ChatOpenAI
    39| from langchain_community.chat_models.pai_eas_endpoint import PaiEasChatEndpoint
    40| from langchain_community.chat_models.promptlayer_openai import PromptLayerChatOpenAI
    41| from langchain_community.chat_models.vertexai import ChatVertexAI
    42| from langchain_community.chat_models.volcengine_maas import VolcEngineMaasChat
    43| from langchain_community.chat_models.yandex import ChatYandexGPT
    44| __all__ = [
    45|     "ChatOpenAI",
    46|     "BedrockChat",
    47|     "AzureChatOpenAI",
    48|     "FakeListChatModel",
    49|     "PromptLayerChatOpenAI",
    50|     "ChatDatabricks",
    51|     "ChatEverlyAI",
    52|     "ChatAnthropic",
    53|     "ChatCohere",
    54|     "ChatGooglePalm",
    55|     "ChatMlflow",
    56|     "ChatMLflowAIGateway",
    57|     "ChatOllama",
    58|     "ChatVertexAI",
    59|     "JinaChat",
    60|     "ChatHuggingFace",
    61|     "HumanInputChatModel",
    62|     "MiniMaxChat",
    63|     "ChatAnyscale",
    64|     "ChatLiteLLM",
    65|     "ErnieBotChat",
    66|     "ChatJavelinAIGateway",
    67|     "ChatKonko",
    68|     "PaiEasChatEndpoint",
    69|     "QianfanChatEndpoint",
    70|     "ChatFireworks",
    71|     "ChatYandexGPT",
    72|     "ChatBaichuan",
    73|     "ChatHunyuan",
    74|     "GigaChat",
    75|     "VolcEngineMaasChat",
    76|     "GPTRouter",
    77| ]


# ====================================================================
# FILE: libs/community/langchain_community/chat_models/azure_openai.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-41 ---
     1| """Azure OpenAI chat wrapper."""
     2| from __future__ import annotations
     3| import logging
     4| import os
     5| import warnings
     6| from typing import Any, Callable, Dict, List, Union
     7| from langchain_core.outputs import ChatResult
     8| from langchain_core.pydantic_v1 import BaseModel, Field, root_validator
     9| from langchain_core.utils import get_from_dict_or_env
    10| from langchain_community.chat_models.openai import ChatOpenAI
    11| from langchain_community.utils.openai import is_openai_v1
    12| logger = logging.getLogger(__name__)
    13| class AzureChatOpenAI(ChatOpenAI):
    14|     """`Azure OpenAI` Chat Completion API.
    15|     To use this class you
    16|     must have a deployed model on Azure OpenAI. Use `deployment_name` in the
    17|     constructor to refer to the "Model deployment name" in the Azure portal.
    18|     In addition, you should have the ``openai`` python package installed, and the
    19|     following environment variables set or passed in constructor in lower case:
    20|     - ``AZURE_OPENAI_API_KEY``
    21|     - ``AZURE_OPENAI_ENDPOINT``
    22|     - ``AZURE_OPENAI_AD_TOKEN``
    23|     - ``OPENAI_API_VERSION``
    24|     - ``OPENAI_PROXY``
    25|     For example, if you have `gpt-35-turbo` deployed, with the deployment name
    26|     `35-turbo-dev`, the constructor should look like:
    27|     .. code-block:: python
    28|         AzureChatOpenAI(
    29|             azure_deployment="35-turbo-dev",
    30|             openai_api_version="2023-05-15",
    31|         )
    32|     Be aware the API version may change.
    33|     You can also specify the version of the model using ``model_version`` constructor
    34|     parameter, as Azure OpenAI doesn't return model version with the response.
    35|     Default is empty. When you specify the version, it will be appended to the
    36|     model name in the response. Setting correct version will help you to calculate the
    37|     cost properly. Model version is not validated, so make sure you set it correctly
    38|     to get the correct cost.
    39|     Any parameters that are valid to be passed to the openai.create call can be passed
    40|     in, even if not explicitly saved on this class.
    41|     """


# ====================================================================
# FILE: libs/community/langchain_community/chat_models/gpt_router.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 173-238 ---
   173|     def _identifying_params(self) -> Dict[str, Any]:
   174|         """Get the identifying parameters."""
   175|         return {
   176|             **{"models_priority_list": self.models_priority_list},
   177|             **self._default_params,
   178|         }
   179|     @property
   180|     def _default_params(self) -> Dict[str, Any]:
   181|         """Get the default parameters for calling GPTRouter API."""
   182|         return {
   183|             "max_tokens": self.max_tokens,
   184|             "stream": self.streaming,
   185|             "n": self.n,
   186|             "temperature": self.temperature,
   187|             **self.model_kwargs,
   188|         }
   189|     def _generate(
   190|         self,
   191|         messages: List[BaseMessage],
   192|         stop: Optional[List[str]] = None,
   193|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   194|         stream: Optional[bool] = None,
   195|         **kwargs: Any,
   196|     ) -> ChatResult:
   197|         should_stream = stream if stream is not None else self.streaming
   198|         if should_stream:
   199|             stream_iter = self._stream(
   200|                 messages, stop=stop, run_manager=run_manager, **kwargs
   201|             )
   202|             return generate_from_stream(stream_iter)
   203|         message_dicts, params = self._create_message_dicts(messages, stop)
   204|         params = {**params, **kwargs, "stream": False}
   205|         response = completion_with_retry(
   206|             self,
   207|             messages=message_dicts,
   208|             models_priority_list=self.models_priority_list,
   209|             run_manager=run_manager,
   210|             **params,
   211|         )
   212|         return self._create_chat_result(response)
   213|     async def _agenerate(
   214|         self,
   215|         messages: List[BaseMessage],
   216|         stop: Optional[List[str]] = None,
   217|         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
   218|         stream: Optional[bool] = None,
   219|         **kwargs: Any,
   220|     ) -> ChatResult:
   221|         should_stream = stream if stream is not None else self.streaming
   222|         if should_stream:
   223|             stream_iter = self._astream(
   224|                 messages, stop=stop, run_manager=run_manager, **kwargs
   225|             )
   226|             return await agenerate_from_stream(stream_iter)
   227|         message_dicts, params = self._create_message_dicts(messages, stop)
   228|         params = {**params, **kwargs, "stream": False}
   229|         response = await acompletion_with_retry(
   230|             self,
   231|             messages=message_dicts,
   232|             models_priority_list=self.models_priority_list,
   233|             run_manager=run_manager,
   234|             **params,
   235|         )
   236|         return self._create_chat_result(response)
   237|     def _create_chat_generation_chunk(
   238|         self, data: Mapping[str, Any], default_chunk_class


# ====================================================================
# FILE: libs/community/langchain_community/chat_models/huggingface.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-131 ---
     1| """Hugging Face Chat Wrapper."""
     2| from typing import Any, List, Optional, Union
     3| from langchain_core.callbacks.manager import (
     4|     AsyncCallbackManagerForLLMRun,
     5|     CallbackManagerForLLMRun,
     6| )
     7| from langchain_core.language_models.chat_models import BaseChatModel
     8| from langchain_core.messages import (
     9|     AIMessage,
    10|     BaseMessage,
    11|     HumanMessage,
    12|     SystemMessage,
    13| )
    14| from langchain_core.outputs import (
    15|     ChatGeneration,
    16|     ChatResult,
    17|     LLMResult,
    18| )
    19| from langchain_community.llms.huggingface_endpoint import HuggingFaceEndpoint
    20| from langchain_community.llms.huggingface_hub import HuggingFaceHub
    21| from langchain_community.llms.huggingface_text_gen_inference import (
    22|     HuggingFaceTextGenInference,
    23| )
    24| DEFAULT_SYSTEM_PROMPT = """You are a helpful, respectful, and honest assistant."""
    25| class ChatHuggingFace(BaseChatModel):
    26|     """
    27|     Wrapper for using Hugging Face LLM's as ChatModels.
    28|     Works with `HuggingFaceTextGenInference`, `HuggingFaceEndpoint`,
    29|     and `HuggingFaceHub` LLMs.
    30|     Upon instantiating this class, the model_id is resolved from the url
    31|     provided to the LLM, and the appropriate tokenizer is loaded from
    32|     the HuggingFace Hub.
    33|     Adapted from: https://python.langchain.com/docs/integrations/chat/llama2_chat
    34|     """
    35|     llm: Union[HuggingFaceTextGenInference, HuggingFaceEndpoint, HuggingFaceHub]
    36|     system_message: SystemMessage = SystemMessage(content=DEFAULT_SYSTEM_PROMPT)
    37|     tokenizer: Any = None
    38|     model_id: str = None  # type: ignore
    39|     def __init__(self, **kwargs: Any):
    40|         super().__init__(**kwargs)
    41|         from transformers import AutoTokenizer
    42|         self._resolve_model_id()
    43|         self.tokenizer = (
    44|             AutoTokenizer.from_pretrained(self.model_id)
    45|             if self.tokenizer is None
    46|             else self.tokenizer
    47|         )
    48|     def _generate(
    49|         self,
    50|         messages: List[BaseMessage],
    51|         stop: Optional[List[str]] = None,
    52|         run_manager: Optional[CallbackManagerForLLMRun] = None,
    53|         **kwargs: Any,
    54|     ) -> ChatResult:
    55|         llm_input = self._to_chat_prompt(messages)
    56|         llm_result = self.llm._generate(
    57|             prompts=[llm_input], stop=stop, run_manager=run_manager, **kwargs
    58|         )
    59|         return self._to_chat_result(llm_result)
    60|     async def _agenerate(
    61|         self,
    62|         messages: List[BaseMessage],
    63|         stop: Optional[List[str]] = None,
    64|         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
    65|         **kwargs: Any,
    66|     ) -> ChatResult:
    67|         llm_input = self._to_chat_prompt(messages)
    68|         llm_result = await self.llm._agenerate(
    69|             prompts=[llm_input], stop=stop, run_manager=run_manager, **kwargs
    70|         )
    71|         return self._to_chat_result(llm_result)
    72|     def _to_chat_prompt(
    73|         self,
    74|         messages: List[BaseMessage],
    75|     ) -> str:
    76|         """Convert a list of messages into a prompt format expected by wrapped LLM."""
    77|         if not messages:
    78|             raise ValueError("at least one HumanMessage must be provided")
    79|         if not isinstance(messages[-1], HumanMessage):
    80|             raise ValueError("last message must be a HumanMessage")
    81|         messages_dicts = [self._to_chatml_format(m) for m in messages]
    82|         return self.tokenizer.apply_chat_template(
    83|             messages_dicts, tokenize=False, add_generation_prompt=True
    84|         )
    85|     def _to_chatml_format(self, message: BaseMessage) -> dict:
    86|         """Convert LangChain message to ChatML format."""
    87|         if isinstance(message, SystemMessage):
    88|             role = "system"
    89|         elif isinstance(message, AIMessage):
    90|             role = "assistant"
    91|         elif isinstance(message, HumanMessage):
    92|             role = "user"
    93|         else:
    94|             raise ValueError(f"Unknown message type: {type(message)}")
    95|         return {"role": role, "content": message.content}
    96|     @staticmethod
    97|     def _to_chat_result(llm_result: LLMResult) -> ChatResult:
    98|         chat_generations = []
    99|         for g in llm_result.generations[0]:
   100|             chat_generation = ChatGeneration(
   101|                 message=AIMessage(content=g.text), generation_info=g.generation_info
   102|             )
   103|             chat_generations.append(chat_generation)
   104|         return ChatResult(
   105|             generations=chat_generations, llm_output=llm_result.llm_output
   106|         )
   107|     def _resolve_model_id(self) -> None:
   108|         """Resolve the model_id from the LLM's inference_server_url"""
   109|         from huggingface_hub import list_inference_endpoints
   110|         available_endpoints = list_inference_endpoints("*")
   111|         if isinstance(self.llm, HuggingFaceTextGenInference):
   112|             endpoint_url = self.llm.inference_server_url
   113|         elif isinstance(self.llm, HuggingFaceEndpoint):
   114|             endpoint_url = self.llm.endpoint_url
   115|         elif isinstance(self.llm, HuggingFaceHub):
   116|             self.model_id = self.llm.repo_id
   117|             return
   118|         else:
   119|             raise ValueError(f"Unknown LLM type: {type(self.llm)}")
   120|         for endpoint in available_endpoints:
   121|             if endpoint.url == endpoint_url:
   122|                 self.model_id = endpoint.repository
   123|         if not self.model_id:
   124|             raise ValueError(
   125|                 "Failed to resolve model_id"
   126|                 f"Could not find model id for inference server provided: {endpoint_url}"
   127|                 "Make sure that your Hugging Face token has access to the endpoint."
   128|             )
   129|     @property
   130|     def _llm_type(self) -> str:
   131|         return "huggingface-chat-wrapper"


# ====================================================================
# FILE: libs/community/langchain_community/chat_models/ollama.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| import json
     2| from typing import Any, AsyncIterator, Dict, Iterator, List, Optional, Union
     3| from langchain_core._api import deprecated
     4| from langchain_core.callbacks import (
     5|     AsyncCallbackManagerForLLMRun,
     6|     CallbackManagerForLLMRun,
     7| )
     8| from langchain_core.language_models.chat_models import BaseChatModel
     9| from langchain_core.messages import (
    10|     AIMessage,
    11|     AIMessageChunk,
    12|     BaseMessage,
    13|     ChatMessage,
    14|     HumanMessage,
    15|     SystemMessage,
    16| )
    17| from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
    18| from langchain_community.llms.ollama import OllamaEndpointNotFoundError, _OllamaCommon
    19| @deprecated("0.0.3", alternative="_chat_stream_response_to_chat_generation_chunk")
    20| def _stream_response_to_chat_generation_chunk(
    21|     stream_response: str,
    22| ) -> ChatGenerationChunk:
    23|     """Convert a stream response to a generation chunk."""
    24|     parsed_response = json.loads(stream_response)
    25|     generation_info = parsed_response if parsed_response.get("done") is True else None

# --- HUNK 2: Lines 117-318 ---
   117|             ollama_messages.append(
   118|                 {
   119|                     "role": role,
   120|                     "content": content,
   121|                     "images": images,
   122|                 }
   123|             )
   124|         return ollama_messages
   125|     def _create_chat_stream(
   126|         self,
   127|         messages: List[BaseMessage],
   128|         stop: Optional[List[str]] = None,
   129|         **kwargs: Any,
   130|     ) -> Iterator[str]:
   131|         payload = {
   132|             "messages": self._convert_messages_to_ollama_messages(messages),
   133|         }
   134|         yield from self._create_stream(
   135|             payload=payload, stop=stop, api_url=f"{self.base_url}/api/chat/", **kwargs
   136|         )
   137|     async def _acreate_chat_stream(
   138|         self,
   139|         messages: List[BaseMessage],
   140|         stop: Optional[List[str]] = None,
   141|         **kwargs: Any,
   142|     ) -> AsyncIterator[str]:
   143|         payload = {
   144|             "messages": self._convert_messages_to_ollama_messages(messages),
   145|         }
   146|         async for stream_resp in self._acreate_stream(
   147|             payload=payload, stop=stop, api_url=f"{self.base_url}/api/chat/", **kwargs
   148|         ):
   149|             yield stream_resp
   150|     def _chat_stream_with_aggregation(
   151|         self,
   152|         messages: List[BaseMessage],
   153|         stop: Optional[List[str]] = None,
   154|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   155|         verbose: bool = False,
   156|         **kwargs: Any,
   157|     ) -> ChatGenerationChunk:
   158|         final_chunk: Optional[ChatGenerationChunk] = None
   159|         for stream_resp in self._create_chat_stream(messages, stop, **kwargs):
   160|             if stream_resp:
   161|                 chunk = _chat_stream_response_to_chat_generation_chunk(stream_resp)
   162|                 if final_chunk is None:
   163|                     final_chunk = chunk
   164|                 else:
   165|                     final_chunk += chunk
   166|                 if run_manager:
   167|                     run_manager.on_llm_new_token(
   168|                         chunk.text,
   169|                         verbose=verbose,
   170|                     )
   171|         if final_chunk is None:
   172|             raise ValueError("No data received from Ollama stream.")
   173|         return final_chunk
   174|     async def _achat_stream_with_aggregation(
   175|         self,
   176|         messages: List[BaseMessage],
   177|         stop: Optional[List[str]] = None,
   178|         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
   179|         verbose: bool = False,
   180|         **kwargs: Any,
   181|     ) -> ChatGenerationChunk:
   182|         final_chunk: Optional[ChatGenerationChunk] = None
   183|         async for stream_resp in self._acreate_chat_stream(messages, stop, **kwargs):
   184|             if stream_resp:
   185|                 chunk = _chat_stream_response_to_chat_generation_chunk(stream_resp)
   186|                 if final_chunk is None:
   187|                     final_chunk = chunk
   188|                 else:
   189|                     final_chunk += chunk
   190|                 if run_manager:
   191|                     await run_manager.on_llm_new_token(
   192|                         chunk.text,
   193|                         verbose=verbose,
   194|                     )
   195|         if final_chunk is None:
   196|             raise ValueError("No data received from Ollama stream.")
   197|         return final_chunk
   198|     def _generate(
   199|         self,
   200|         messages: List[BaseMessage],
   201|         stop: Optional[List[str]] = None,
   202|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   203|         **kwargs: Any,
   204|     ) -> ChatResult:
   205|         """Call out to Ollama's generate endpoint.
   206|         Args:
   207|             messages: The list of base messages to pass into the model.
   208|             stop: Optional list of stop words to use when generating.
   209|         Returns:
   210|             Chat generations from the model
   211|         Example:
   212|             .. code-block:: python
   213|                 response = ollama([
   214|                     HumanMessage(content="Tell me about the history of AI")
   215|                 ])
   216|         """
   217|         final_chunk = self._chat_stream_with_aggregation(
   218|             messages,
   219|             stop=stop,
   220|             run_manager=run_manager,
   221|             verbose=self.verbose,
   222|             **kwargs,
   223|         )
   224|         chat_generation = ChatGeneration(
   225|             message=AIMessage(content=final_chunk.text),
   226|             generation_info=final_chunk.generation_info,
   227|         )
   228|         return ChatResult(generations=[chat_generation])
   229|     async def _agenerate(
   230|         self,
   231|         messages: List[BaseMessage],
   232|         stop: Optional[List[str]] = None,
   233|         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
   234|         **kwargs: Any,
   235|     ) -> ChatResult:
   236|         """Call out to Ollama's generate endpoint.
   237|         Args:
   238|             messages: The list of base messages to pass into the model.
   239|             stop: Optional list of stop words to use when generating.
   240|         Returns:
   241|             Chat generations from the model
   242|         Example:
   243|             .. code-block:: python
   244|                 response = ollama([
   245|                     HumanMessage(content="Tell me about the history of AI")
   246|                 ])
   247|         """
   248|         final_chunk = await self._achat_stream_with_aggregation(
   249|             messages,
   250|             stop=stop,
   251|             run_manager=run_manager,
   252|             verbose=self.verbose,
   253|             **kwargs,
   254|         )
   255|         chat_generation = ChatGeneration(
   256|             message=AIMessage(content=final_chunk.text),
   257|             generation_info=final_chunk.generation_info,
   258|         )
   259|         return ChatResult(generations=[chat_generation])
   260|     def _stream(
   261|         self,
   262|         messages: List[BaseMessage],
   263|         stop: Optional[List[str]] = None,
   264|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   265|         **kwargs: Any,
   266|     ) -> Iterator[ChatGenerationChunk]:
   267|         try:
   268|             for stream_resp in self._create_chat_stream(messages, stop, **kwargs):
   269|                 if stream_resp:
   270|                     chunk = _chat_stream_response_to_chat_generation_chunk(stream_resp)
   271|                     yield chunk
   272|                     if run_manager:
   273|                         run_manager.on_llm_new_token(
   274|                             chunk.text,
   275|                             verbose=self.verbose,
   276|                         )
   277|         except OllamaEndpointNotFoundError:
   278|             yield from self._legacy_stream(messages, stop, **kwargs)
   279|     async def _astream(
   280|         self,
   281|         messages: List[BaseMessage],
   282|         stop: Optional[List[str]] = None,
   283|         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
   284|         **kwargs: Any,
   285|     ) -> AsyncIterator[ChatGenerationChunk]:
   286|         try:
   287|             async for stream_resp in self._acreate_chat_stream(
   288|                 messages, stop, **kwargs
   289|             ):
   290|                 if stream_resp:
   291|                     chunk = _chat_stream_response_to_chat_generation_chunk(stream_resp)
   292|                     yield chunk
   293|                     if run_manager:
   294|                         await run_manager.on_llm_new_token(
   295|                             chunk.text,
   296|                             verbose=self.verbose,
   297|                         )
   298|         except OllamaEndpointNotFoundError:
   299|             async for chunk in self._legacy_astream(messages, stop, **kwargs):
   300|                 yield chunk
   301|     @deprecated("0.0.3", alternative="_stream")
   302|     def _legacy_stream(
   303|         self,
   304|         messages: List[BaseMessage],
   305|         stop: Optional[List[str]] = None,
   306|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   307|         **kwargs: Any,
   308|     ) -> Iterator[ChatGenerationChunk]:
   309|         prompt = self._format_messages_as_text(messages)
   310|         for stream_resp in self._create_generate_stream(prompt, stop, **kwargs):
   311|             if stream_resp:
   312|                 chunk = _stream_response_to_chat_generation_chunk(stream_resp)
   313|                 yield chunk
   314|                 if run_manager:
   315|                     run_manager.on_llm_new_token(
   316|                         chunk.text,
   317|                         verbose=self.verbose,
   318|                     )


# ====================================================================
# FILE: libs/community/langchain_community/chat_models/vertexai.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-29 ---
     1| """Wrapper around Google VertexAI chat-based models."""
     2| from __future__ import annotations
     3| import base64
     4| import logging
     5| import re
     6| from dataclasses import dataclass, field
     7| from typing import TYPE_CHECKING, Any, Dict, Iterator, List, Optional, Union, cast
     8| from urllib.parse import urlparse
     9| import requests
    10| from langchain_core.callbacks import (
    11|     AsyncCallbackManagerForLLMRun,
    12|     CallbackManagerForLLMRun,
    13| )
    14| from langchain_core.language_models.chat_models import (
    15|     BaseChatModel,
    16|     generate_from_stream,
    17| )
    18| from langchain_core.messages import (
    19|     AIMessage,
    20|     AIMessageChunk,
    21|     BaseMessage,
    22|     HumanMessage,
    23|     SystemMessage,
    24| )
    25| from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
    26| from langchain_core.pydantic_v1 import root_validator
    27| from langchain_community.llms.vertexai import (
    28|     _VertexAICommon,
    29|     is_codey_model,

# --- HUNK 2: Lines 58-136 ---
    58|         first place.
    59|     """
    60|     from vertexai.language_models import ChatMessage
    61|     vertex_messages, context = [], None
    62|     for i, message in enumerate(history):
    63|         content = cast(str, message.content)
    64|         if i == 0 and isinstance(message, SystemMessage):
    65|             context = content
    66|         elif isinstance(message, AIMessage):
    67|             vertex_message = ChatMessage(content=message.content, author="bot")
    68|             vertex_messages.append(vertex_message)
    69|         elif isinstance(message, HumanMessage):
    70|             vertex_message = ChatMessage(content=message.content, author="user")
    71|             vertex_messages.append(vertex_message)
    72|         else:
    73|             raise ValueError(
    74|                 f"Unexpected message with type {type(message)} at the position {i}."
    75|             )
    76|     chat_history = _ChatHistory(context=context, history=vertex_messages)
    77|     return chat_history
    78| def _is_url(s: str) -> bool:
    79|     try:
    80|         result = urlparse(s)
    81|         return all([result.scheme, result.netloc])
    82|     except Exception as e:
    83|         logger.debug(f"Unable to parse URL: {e}")
    84|         return False
    85| def _parse_chat_history_gemini(
    86|     history: List[BaseMessage], project: Optional[str]
    87| ) -> List["Content"]:
    88|     from vertexai.preview.generative_models import Content, Image, Part
    89|     def _convert_to_prompt(part: Union[str, Dict]) -> Part:
    90|         if isinstance(part, str):
    91|             return Part.from_text(part)
    92|         if not isinstance(part, Dict):
    93|             raise ValueError(
    94|                 f"Message's content is expected to be a dict, got {type(part)}!"
    95|             )
    96|         if part["type"] == "text":
    97|             return Part.from_text(part["text"])
    98|         elif part["type"] == "image_url":
    99|             path = part["image_url"]["url"]
   100|             if path.startswith("gs://"):
   101|                 image = load_image_from_gcs(path=path, project=project)
   102|             elif path.startswith("data:image/"):
   103|                 try:
   104|                     encoded = re.search(r"data:image/\w{2,4};base64,(.*)", path).group(
   105|                         1
   106|                     )
   107|                 except AttributeError:
   108|                     raise ValueError(
   109|                         "Invalid image uri. It should be in the format "
   110|                         "data:image/<image_type>;base64,<base64_encoded_image>."
   111|                     )
   112|                 image = Image.from_bytes(base64.b64decode(encoded))
   113|             elif _is_url(path):
   114|                 response = requests.get(path)
   115|                 response.raise_for_status()
   116|                 image = Image.from_bytes(response.content)
   117|             else:
   118|                 image = Image.load_from_file(path)
   119|         else:
   120|             raise ValueError("Only text and image_url types are supported!")
   121|         return Part.from_image(image)
   122|     vertex_messages = []
   123|     for i, message in enumerate(history):
   124|         if i == 0 and isinstance(message, SystemMessage):
   125|             raise ValueError("SystemMessages are not yet supported!")
   126|         elif isinstance(message, AIMessage):
   127|             role = "model"
   128|         elif isinstance(message, HumanMessage):
   129|             role = "user"
   130|         else:
   131|             raise ValueError(
   132|                 f"Unexpected message with type {type(message)} at the position {i}."
   133|             )
   134|         raw_content = message.content
   135|         if isinstance(raw_content, str):
   136|             raw_content = [raw_content]


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 52-94 ---
    52| from langchain_community.document_loaders.browserless import BrowserlessLoader
    53| from langchain_community.document_loaders.chatgpt import ChatGPTLoader
    54| from langchain_community.document_loaders.chromium import AsyncChromiumLoader
    55| from langchain_community.document_loaders.college_confidential import (
    56|     CollegeConfidentialLoader,
    57| )
    58| from langchain_community.document_loaders.concurrent import ConcurrentLoader
    59| from langchain_community.document_loaders.confluence import ConfluenceLoader
    60| from langchain_community.document_loaders.conllu import CoNLLULoader
    61| from langchain_community.document_loaders.couchbase import CouchbaseLoader
    62| from langchain_community.document_loaders.csv_loader import (
    63|     CSVLoader,
    64|     UnstructuredCSVLoader,
    65| )
    66| from langchain_community.document_loaders.cube_semantic import CubeSemanticLoader
    67| from langchain_community.document_loaders.datadog_logs import DatadogLogsLoader
    68| from langchain_community.document_loaders.dataframe import DataFrameLoader
    69| from langchain_community.document_loaders.diffbot import DiffbotLoader
    70| from langchain_community.document_loaders.directory import DirectoryLoader
    71| from langchain_community.document_loaders.discord import DiscordChatLoader
    72| from langchain_community.document_loaders.doc_intelligence import (
    73|     AzureAIDocumentIntelligenceLoader,
    74| )
    75| from langchain_community.document_loaders.docugami import DocugamiLoader
    76| from langchain_community.document_loaders.docusaurus import DocusaurusLoader
    77| from langchain_community.document_loaders.dropbox import DropboxLoader
    78| from langchain_community.document_loaders.duckdb_loader import DuckDBLoader
    79| from langchain_community.document_loaders.email import (
    80|     OutlookMessageLoader,
    81|     UnstructuredEmailLoader,
    82| )
    83| from langchain_community.document_loaders.epub import UnstructuredEPubLoader
    84| from langchain_community.document_loaders.etherscan import EtherscanLoader
    85| from langchain_community.document_loaders.evernote import EverNoteLoader
    86| from langchain_community.document_loaders.excel import UnstructuredExcelLoader
    87| from langchain_community.document_loaders.facebook_chat import FacebookChatLoader
    88| from langchain_community.document_loaders.fauna import FaunaLoader
    89| from langchain_community.document_loaders.figma import FigmaFileLoader
    90| from langchain_community.document_loaders.gcs_directory import GCSDirectoryLoader
    91| from langchain_community.document_loaders.gcs_file import GCSFileLoader
    92| from langchain_community.document_loaders.geodataframe import GeoDataFrameLoader
    93| from langchain_community.document_loaders.git import GitLoader
    94| from langchain_community.document_loaders.gitbook import GitbookLoader

# --- HUNK 2: Lines 220-260 ---
   220|     "AsyncChromiumLoader",
   221|     "AZLyricsLoader",
   222|     "AcreomLoader",
   223|     "AirbyteCDKLoader",
   224|     "AirbyteGongLoader",
   225|     "AirbyteJSONLoader",
   226|     "AirbyteHubspotLoader",
   227|     "AirbyteSalesforceLoader",
   228|     "AirbyteShopifyLoader",
   229|     "AirbyteStripeLoader",
   230|     "AirbyteTypeformLoader",
   231|     "AirbyteZendeskSupportLoader",
   232|     "AirtableLoader",
   233|     "AmazonTextractPDFLoader",
   234|     "ApifyDatasetLoader",
   235|     "ArcGISLoader",
   236|     "ArxivLoader",
   237|     "AssemblyAIAudioTranscriptLoader",
   238|     "AsyncHtmlLoader",
   239|     "AzureAIDataLoader",
   240|     "AzureAIDocumentIntelligenceLoader",
   241|     "AzureBlobStorageContainerLoader",
   242|     "AzureBlobStorageFileLoader",
   243|     "BSHTMLLoader",
   244|     "BibtexLoader",
   245|     "BigQueryLoader",
   246|     "BiliBiliLoader",
   247|     "BlackboardLoader",
   248|     "Blob",
   249|     "BlobLoader",
   250|     "BlockchainDocumentLoader",
   251|     "BraveSearchLoader",
   252|     "BrowserlessLoader",
   253|     "CSVLoader",
   254|     "ChatGPTLoader",
   255|     "CoNLLULoader",
   256|     "CollegeConfidentialLoader",
   257|     "ConcurrentLoader",
   258|     "ConfluenceLoader",
   259|     "CouchbaseLoader",
   260|     "CubeSemanticLoader",


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/arxiv.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-21 ---
     1| from typing import Any, List, Optional
     2| from langchain_core.documents import Document
     3| from langchain_community.document_loaders.base import BaseLoader
     4| from langchain_community.utilities.arxiv import ArxivAPIWrapper
     5| class ArxivLoader(BaseLoader):
     6|     """Load a query result from `Arxiv`.
     7|     The loader converts the original PDF format into the text.
     8|     Args:
     9|         Supports all arguments of `ArxivAPIWrapper`.
    10|     """
    11|     def __init__(
    12|         self, query: str, doc_content_chars_max: Optional[int] = None, **kwargs: Any
    13|     ):
    14|         self.query = query
    15|         self.client = ArxivAPIWrapper(
    16|             doc_content_chars_max=doc_content_chars_max, **kwargs
    17|         )
    18|     def load(self) -> List[Document]:
    19|         return self.client.load(self.query)
    20|     def get_summaries_as_docs(self) -> List[Document]:
    21|         return self.client.get_summaries_as_docs(self.query)


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/directory.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 36-77 ---
    36|         sample_size: int = 0,
    37|         randomize_sample: bool = False,
    38|         sample_seed: Union[int, None] = None,
    39|     ):
    40|         """Initialize with a path to directory and how to glob over it.
    41|         Args:
    42|             path: Path to directory.
    43|             glob: Glob pattern to use to find files. Defaults to "**/[!.]*"
    44|                (all files except hidden).
    45|             silent_errors: Whether to silently ignore errors. Defaults to False.
    46|             load_hidden: Whether to load hidden files. Defaults to False.
    47|             loader_cls: Loader class to use for loading files.
    48|               Defaults to UnstructuredFileLoader.
    49|             loader_kwargs: Keyword arguments to pass to loader_cls. Defaults to None.
    50|             recursive: Whether to recursively search for files. Defaults to False.
    51|             show_progress: Whether to show a progress bar. Defaults to False.
    52|             use_multithreading: Whether to use multithreading. Defaults to False.
    53|             max_concurrency: The maximum number of threads to use. Defaults to 4.
    54|             sample_size: The maximum number of files you would like to load from the
    55|                 directory.
    56|             randomize_sample: Shuffle the files to get a random sample.
    57|             sample_seed: set the seed of the random shuffle for reproducibility.
    58|         """
    59|         if loader_kwargs is None:
    60|             loader_kwargs = {}
    61|         self.path = path
    62|         self.glob = glob
    63|         self.load_hidden = load_hidden
    64|         self.loader_cls = loader_cls
    65|         self.loader_kwargs = loader_kwargs
    66|         self.silent_errors = silent_errors
    67|         self.recursive = recursive
    68|         self.show_progress = show_progress
    69|         self.use_multithreading = use_multithreading
    70|         self.max_concurrency = max_concurrency
    71|         self.sample_size = sample_size
    72|         self.randomize_sample = randomize_sample
    73|         self.sample_seed = sample_seed
    74|     def load_file(
    75|         self, item: Path, path: Path, docs: List[Document], pbar: Optional[Any]
    76|     ) -> None:
    77|         """Load a file.


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/doc_intelligence.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-77 ---
     1| from typing import Iterator, List, Optional
     2| from langchain_core.documents import Document
     3| from langchain_community.document_loaders.base import BaseLoader
     4| from langchain_community.document_loaders.blob_loaders import Blob
     5| from langchain_community.document_loaders.parsers import (
     6|     AzureAIDocumentIntelligenceParser,
     7| )
     8| class AzureAIDocumentIntelligenceLoader(BaseLoader):
     9|     """Loads a PDF with Azure Document Intelligence"""
    10|     def __init__(
    11|         self,
    12|         api_endpoint: str,
    13|         api_key: str,
    14|         file_path: Optional[str] = None,
    15|         url_path: Optional[str] = None,
    16|         api_version: Optional[str] = None,
    17|         api_model: str = "prebuilt-layout",
    18|         mode: str = "markdown",
    19|     ) -> None:
    20|         """
    21|         Initialize the object for file processing with Azure Document Intelligence
    22|         (formerly Form Recognizer).
    23|         This constructor initializes a AzureAIDocumentIntelligenceParser object to be
    24|         used for parsing files using the Azure Document Intelligence API. The load
    25|         method generates Documents whose content representations are determined by the
    26|         mode parameter.
    27|         Parameters:
    28|         -----------
    29|         api_endpoint: str
    30|             The API endpoint to use for DocumentIntelligenceClient construction.
    31|         api_key: str
    32|             The API key to use for DocumentIntelligenceClient construction.
    33|         file_path : Optional[str]
    34|             The path to the file that needs to be loaded.
    35|             Either file_path or url_path must be specified.
    36|         url_path : Optional[str]
    37|             The URL to the file that needs to be loaded.
    38|             Either file_path or url_path must be specified.
    39|         api_version: Optional[str]
    40|             The API version for DocumentIntelligenceClient. Setting None to use
    41|             the default value from SDK.
    42|         api_model: str
    43|             The model name or ID to be used for form recognition in Azure.
    44|         Examples:
    45|         ---------
    46|         >>> obj = AzureAIDocumentIntelligenceLoader(
    47|         ...     file_path="path/to/file",
    48|         ...     api_endpoint="https://endpoint.azure.com",
    49|         ...     api_key="APIKEY",
    50|         ...     api_version="2023-10-31-preview",
    51|         ...     model="prebuilt-document"
    52|         ... )
    53|         """
    54|         assert (
    55|             file_path is not None or url_path is not None
    56|         ), "file_path or url_path must be provided"
    57|         self.file_path = file_path
    58|         self.url_path = url_path
    59|         self.parser = AzureAIDocumentIntelligenceParser(
    60|             api_endpoint=api_endpoint,
    61|             api_key=api_key,
    62|             api_version=api_version,
    63|             api_model=api_model,
    64|             mode=mode,
    65|         )
    66|     def load(self) -> List[Document]:
    67|         """Load given path as pages."""
    68|         return list(self.lazy_load())
    69|     def lazy_load(
    70|         self,
    71|     ) -> Iterator[Document]:
    72|         """Lazy load given path as pages."""
    73|         if self.file_path is not None:
    74|             blob = Blob.from_path(self.file_path)
    75|             yield from self.parser.parse(blob)
    76|         else:
    77|             yield from self.parser.parse_url(self.url_path)


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/html_bs.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-35 ---
     1| import logging
     2| from typing import Dict, List, Union
     3| from langchain_core.documents import Document
     4| from langchain_community.document_loaders.base import BaseLoader
     5| logger = logging.getLogger(__name__)
     6| class BSHTMLLoader(BaseLoader):
     7|     """Load `HTML` files and parse them with `beautiful soup`."""
     8|     def __init__(
     9|         self,
    10|         file_path: str,
    11|         open_encoding: Union[str, None] = None,
    12|         bs_kwargs: Union[dict, None] = None,
    13|         get_text_separator: str = "",
    14|     ) -> None:
    15|         """initialize with path, and optionally, file encoding to use, and any kwargs
    16|         to pass to the BeautifulSoup object.
    17|         Args:
    18|             file_path: The path to the file to load.
    19|             open_encoding: The encoding to use when opening the file.
    20|             bs_kwargs: Any kwargs to pass to the BeautifulSoup object.
    21|             get_text_separator: The separator to use when calling get_text on the soup.
    22|         """
    23|         try:
    24|             import bs4  # noqa:F401
    25|         except ImportError:
    26|             raise ImportError(
    27|                 "beautifulsoup4 package not found, please install it with "
    28|                 "`pip install beautifulsoup4`"
    29|             )
    30|         self.file_path = file_path
    31|         self.open_encoding = open_encoding
    32|         if bs_kwargs is None:
    33|             bs_kwargs = {"features": "lxml"}
    34|         self.bs_kwargs = bs_kwargs
    35|         self.get_text_separator = get_text_separator


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/markdown.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-32 ---
     1| from typing import List
     2| from langchain_community.document_loaders.unstructured import UnstructuredFileLoader
     3| class UnstructuredMarkdownLoader(UnstructuredFileLoader):
     4|     """Load `Markdown` files using `Unstructured`.
     5|     You can run the loader in one of two modes: "single" and "elements".
     6|     If you use "single" mode, the document will be returned as a single
     7|     langchain Document object. If you use "elements" mode, the unstructured
     8|     library will split the document into elements such as Title and NarrativeText.
     9|     You can pass in additional unstructured kwargs after mode to apply
    10|     different unstructured settings.
    11|     Examples
    12|     --------
    13|     from langchain_community.document_loaders import UnstructuredMarkdownLoader
    14|     loader = UnstructuredMarkdownLoader(
    15|         "example.md", mode="elements", strategy="fast",
    16|     )
    17|     docs = loader.load()
    18|     References
    19|     ----------
    20|     https://unstructured-io.github.io/unstructured/core/partition.html#partition-md
    21|     """
    22|     def _get_elements(self) -> List:
    23|         from unstructured.__version__ import __version__ as __unstructured_version__
    24|         from unstructured.partition.md import partition_md
    25|         _unstructured_version = __unstructured_version__.split("-")[0]
    26|         unstructured_version = tuple([int(x) for x in _unstructured_version.split(".")])
    27|         if unstructured_version < (0, 4, 16):
    28|             raise ValueError(
    29|                 f"You are on unstructured version {__unstructured_version__}. "
    30|                 "Partitioning markdown files is only supported in unstructured>=0.4.16."
    31|             )
    32|         return partition_md(filename=self.file_path, **self.unstructured_kwargs)


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/mhtml.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-36 ---
     1| import email
     2| import logging
     3| from typing import Dict, List, Union
     4| from langchain_core.documents import Document
     5| from langchain_community.document_loaders.base import BaseLoader
     6| logger = logging.getLogger(__name__)
     7| class MHTMLLoader(BaseLoader):
     8|     """Parse `MHTML` files with `BeautifulSoup`."""
     9|     def __init__(
    10|         self,
    11|         file_path: str,
    12|         open_encoding: Union[str, None] = None,
    13|         bs_kwargs: Union[dict, None] = None,
    14|         get_text_separator: str = "",
    15|     ) -> None:
    16|         """initialize with path, and optionally, file encoding to use, and any kwargs
    17|         to pass to the BeautifulSoup object.
    18|         Args:
    19|             file_path: Path to file to load.
    20|             open_encoding: The encoding to use when opening the file.
    21|             bs_kwargs: Any kwargs to pass to the BeautifulSoup object.
    22|             get_text_separator: The separator to use when getting the text
    23|                 from the soup.
    24|         """
    25|         try:
    26|             import bs4  # noqa:F401
    27|         except ImportError:
    28|             raise ImportError(
    29|                 "beautifulsoup4 package not found, please install it with "
    30|                 "`pip install beautifulsoup4`"
    31|             )
    32|         self.file_path = file_path
    33|         self.open_encoding = open_encoding
    34|         if bs_kwargs is None:
    35|             bs_kwargs = {"features": "lxml"}
    36|         self.bs_kwargs = bs_kwargs


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/parsers/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| from langchain_community.document_loaders.parsers.audio import OpenAIWhisperParser
     2| from langchain_community.document_loaders.parsers.doc_intelligence import (
     3|     AzureAIDocumentIntelligenceParser,
     4| )
     5| from langchain_community.document_loaders.parsers.docai import DocAIParser
     6| from langchain_community.document_loaders.parsers.grobid import GrobidParser
     7| from langchain_community.document_loaders.parsers.html import BS4HTMLParser
     8| from langchain_community.document_loaders.parsers.language import LanguageParser
     9| from langchain_community.document_loaders.parsers.pdf import (
    10|     PDFMinerParser,
    11|     PDFPlumberParser,
    12|     PyMuPDFParser,
    13|     PyPDFium2Parser,
    14|     PyPDFParser,
    15| )
    16| __all__ = [
    17|     "AzureAIDocumentIntelligenceParser",
    18|     "BS4HTMLParser",
    19|     "DocAIParser",
    20|     "GrobidParser",
    21|     "LanguageParser",
    22|     "OpenAIWhisperParser",
    23|     "PDFMinerParser",
    24|     "PDFPlumberParser",
    25|     "PyMuPDFParser",
    26|     "PyPDFium2Parser",
    27|     "PyPDFParser",
    28| ]


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/parsers/doc_intelligence.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-98 ---
     1| from typing import Any, Iterator, Optional
     2| from langchain_core.documents import Document
     3| from langchain_community.document_loaders.base import BaseBlobParser
     4| from langchain_community.document_loaders.blob_loaders import Blob
     5| class AzureAIDocumentIntelligenceParser(BaseBlobParser):
     6|     """Loads a PDF with Azure Document Intelligence
     7|     (formerly Forms Recognizer)."""
     8|     def __init__(
     9|         self,
    10|         api_endpoint: str,
    11|         api_key: str,
    12|         api_version: Optional[str] = None,
    13|         api_model: str = "prebuilt-layout",
    14|         mode: str = "markdown",
    15|     ):
    16|         from azure.ai.documentintelligence import DocumentIntelligenceClient
    17|         from azure.core.credentials import AzureKeyCredential
    18|         kwargs = {}
    19|         if api_version is not None:
    20|             kwargs["api_version"] = api_version
    21|         self.client = DocumentIntelligenceClient(
    22|             endpoint=api_endpoint,
    23|             credential=AzureKeyCredential(api_key),
    24|             headers={"x-ms-useragent": "langchain-parser/1.0.0"},
    25|             **kwargs,
    26|         )
    27|         self.api_model = api_model
    28|         self.mode = mode
    29|         assert self.mode in ["single", "page", "object", "markdown"]
    30|     def _generate_docs_page(self, result: Any) -> Iterator[Document]:
    31|         for p in result.pages:
    32|             content = " ".join([line.content for line in p.lines])
    33|             d = Document(
    34|                 page_content=content,
    35|                 metadata={
    36|                     "page": p.page_number,
    37|                 },
    38|             )
    39|             yield d
    40|     def _generate_docs_single(self, result: Any) -> Iterator[Document]:
    41|         yield Document(page_content=result.content, metadata={})
    42|     def _generate_docs_object(self, result: Any) -> Iterator[Document]:
    43|         page_offset = []
    44|         for page in result.pages:
    45|             page_offset.append(page.spans[0]["offset"])
    46|         for para in result.paragraphs:
    47|             yield Document(
    48|                 page_content=para.content,
    49|                 metadata={
    50|                     "role": para.role,
    51|                     "page": para.bounding_regions[0].page_number,
    52|                     "bounding_box": para.bounding_regions[0].polygon,
    53|                     "type": "paragraph",
    54|                 },
    55|             )
    56|         for table in result.tables:
    57|             yield Document(
    58|                 page_content=table.cells,  # json object
    59|                 metadata={
    60|                     "footnote": table.footnotes,
    61|                     "caption": table.caption,
    62|                     "page": para.bounding_regions[0].page_number,
    63|                     "bounding_box": para.bounding_regions[0].polygon,
    64|                     "row_count": table.row_count,
    65|                     "column_count": table.column_count,
    66|                     "type": "table",
    67|                 },
    68|             )
    69|     def lazy_parse(self, blob: Blob) -> Iterator[Document]:
    70|         """Lazily parse the blob."""
    71|         with blob.as_bytes_io() as file_obj:
    72|             poller = self.client.begin_analyze_document(
    73|                 self.api_model,
    74|                 file_obj,
    75|                 content_type="application/octet-stream",
    76|                 output_content_format="markdown" if self.mode == "markdown" else "text",
    77|             )
    78|             result = poller.result()
    79|             if self.mode in ["single", "markdown"]:
    80|                 yield from self._generate_docs_single(result)
    81|             elif self.mode == ["page"]:
    82|                 yield from self._generate_docs_page(result)
    83|             else:
    84|                 yield from self._generate_docs_object(result)
    85|     def parse_url(self, url: str) -> Iterator[Document]:
    86|         from azure.ai.documentintelligence.models import AnalyzeDocumentRequest
    87|         poller = self.client.begin_analyze_document(
    88|             self.api_model,
    89|             AnalyzeDocumentRequest(url_source=url),
    90|             output_content_format="markdown" if self.mode == "markdown" else "text",
    91|         )
    92|         result = poller.result()
    93|         if self.mode in ["single", "markdown"]:
    94|             yield from self._generate_docs_single(result)
    95|         elif self.mode == ["page"]:
    96|             yield from self._generate_docs_page(result)
    97|         else:
    98|             yield from self._generate_docs_object(result)


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/parsers/pdf.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 435-484 ---
   435|             textract_response_json = self.tc.call_textract(
   436|                 input_document=blob.as_bytes(),
   437|                 features=self.textract_features,
   438|                 call_mode=self.tc.Textract_Call_Mode.FORCE_SYNC,
   439|                 boto3_textract_client=self.boto3_textract_client,
   440|             )
   441|         document = self.textractor.Document.open(textract_response_json)
   442|         linearizer_config = self.textractor.TextLinearizationConfig(
   443|             hide_figure_layout=True,
   444|             title_prefix="# ",
   445|             section_header_prefix="## ",
   446|             list_element_prefix="*",
   447|         )
   448|         for idx, page in enumerate(document.pages):
   449|             yield Document(
   450|                 page_content=page.get_text(config=linearizer_config),
   451|                 metadata={"source": blob.source, "page": idx + 1},
   452|             )
   453| class DocumentIntelligenceParser(BaseBlobParser):
   454|     """Loads a PDF with Azure Document Intelligence
   455|     (formerly Form Recognizer) and chunks at character level."""
   456|     def __init__(self, client: Any, model: str):
   457|         warnings.warn(
   458|             "langchain.document_loaders.parsers.pdf.DocumentIntelligenceParser"
   459|             "and langchain.document_loaders.pdf.DocumentIntelligenceLoader"
   460|             " are deprecated. Please upgrade to "
   461|             "langchain.document_loaders.DocumentIntelligenceLoader "
   462|             "for any file parsing purpose using Azure Document Intelligence "
   463|             "service."
   464|         )
   465|         self.client = client
   466|         self.model = model
   467|     def _generate_docs(self, blob: Blob, result: Any) -> Iterator[Document]:
   468|         for p in result.pages:
   469|             content = " ".join([line.content for line in p.lines])
   470|             d = Document(
   471|                 page_content=content,
   472|                 metadata={
   473|                     "source": blob.source,
   474|                     "page": p.page_number,
   475|                 },
   476|             )
   477|             yield d
   478|     def lazy_parse(self, blob: Blob) -> Iterator[Document]:
   479|         """Lazily parse the blob."""
   480|         with blob.as_bytes_io() as file_obj:
   481|             poller = self.client.begin_analyze_document(self.model, file_obj)
   482|             result = poller.result()
   483|             docs = self._generate_docs(blob, result)
   484|             yield from docs


# ====================================================================
# FILE: libs/community/langchain_community/document_loaders/rspace.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 36-76 ---
    36|     def validate_environment(cls, values: Dict) -> Dict:
    37|         """Validate that API key and URL exist in environment."""
    38|         values["api_key"] = get_from_dict_or_env(values, "api_key", "RSPACE_API_KEY")
    39|         values["url"] = get_from_dict_or_env(values, "url", "RSPACE_URL")
    40|         if "global_id" not in values or values["global_id"] is None:
    41|             raise ValueError(
    42|                 "No value supplied for global_id. Please supply an RSpace global ID"
    43|             )
    44|         return values
    45|     def _create_rspace_client(self) -> Any:
    46|         """Create a RSpace client."""
    47|         try:
    48|             from rspace_client.eln import eln, field_content
    49|         except ImportError:
    50|             raise ImportError("You must run " "`pip install rspace_client`")
    51|         try:
    52|             eln = eln.ELNClient(self.url, self.api_key)
    53|             eln.get_status()
    54|         except Exception:
    55|             raise Exception(
    56|                 f"Unable to initialize client - is url {self.url} or "
    57|                 f"api key  correct?"
    58|             )
    59|         return eln, field_content.FieldContent
    60|     def _get_doc(self, cli: Any, field_content: Any, d_id: Union[str, int]) -> Document:
    61|         content = ""
    62|         doc = cli.get_document(d_id)
    63|         content += f"<h2>{doc['name']}<h2/>"
    64|         for f in doc["fields"]:
    65|             content += f"{f['name']}\n"
    66|             fc = field_content(f["content"])
    67|             content += fc.get_text()
    68|             content += "\n"
    69|         return Document(
    70|             metadata={"source": f"rspace: {doc['name']}-{doc['globalId']}"},
    71|             page_content=content,
    72|         )
    73|     def _load_structured_doc(self) -> Iterator[Document]:
    74|         cli, field_content = self._create_rspace_client()
    75|         yield self._get_doc(cli, field_content, self.global_id)
    76|     def _load_folder_tree(self) -> Iterator[Document]:


# ====================================================================
# FILE: libs/community/langchain_community/document_transformers/doctran_text_extract.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 35-80 ---
    35|                     "type": "string",
    36|                     "required": True,
    37|                 },
    38|             ]
    39|             property_extractor = DoctranPropertyExtractor(properties)
    40|             transformed_document = await qa_transformer.atransform_documents(documents)
    41|     """  # noqa: E501
    42|     def __init__(
    43|         self,
    44|         properties: List[dict],
    45|         openai_api_key: Optional[str] = None,
    46|         openai_api_model: Optional[str] = None,
    47|     ) -> None:
    48|         self.properties = properties
    49|         self.openai_api_key = openai_api_key or get_from_env(
    50|             "openai_api_key", "OPENAI_API_KEY"
    51|         )
    52|         self.openai_api_model = openai_api_model or get_from_env(
    53|             "openai_api_model", "OPENAI_API_MODEL"
    54|         )
    55|     async def atransform_documents(
    56|         self, documents: Sequence[Document], **kwargs: Any
    57|     ) -> Sequence[Document]:
    58|         raise NotImplementedError
    59|     def transform_documents(
    60|         self, documents: Sequence[Document], **kwargs: Any
    61|     ) -> Sequence[Document]:
    62|         """Extracts properties from text documents using doctran."""
    63|         try:
    64|             from doctran import Doctran, ExtractProperty
    65|             doctran = Doctran(
    66|                 openai_api_key=self.openai_api_key, openai_model=self.openai_api_model
    67|             )
    68|         except ImportError:
    69|             raise ImportError(
    70|                 "Install doctran to use this parser. (pip install doctran)"
    71|             )
    72|         properties = [ExtractProperty(**property) for property in self.properties]
    73|         for d in documents:
    74|             doctran_doc = (
    75|                 doctran.parse(content=d.page_content)
    76|                 .extract(properties=properties)
    77|                 .execute()
    78|             )
    79|             d.metadata["extracted_properties"] = doctran_doc.extracted_properties
    80|         return documents


# ====================================================================
# FILE: libs/community/langchain_community/document_transformers/doctran_text_qa.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 6-49 ---
     6|     Arguments:
     7|         openai_api_key: OpenAI API key. Can also be specified via environment variable
     8|             ``OPENAI_API_KEY``.
     9|     Example:
    10|         .. code-block:: python
    11|             from langchain_community.document_transformers import DoctranQATransformer
    12|             qa_transformer = DoctranQATransformer()
    13|             transformed_document = await qa_transformer.atransform_documents(documents)
    14|     """
    15|     def __init__(
    16|         self,
    17|         openai_api_key: Optional[str] = None,
    18|         openai_api_model: Optional[str] = None,
    19|     ) -> None:
    20|         self.openai_api_key = openai_api_key or get_from_env(
    21|             "openai_api_key", "OPENAI_API_KEY"
    22|         )
    23|         self.openai_api_model = openai_api_model or get_from_env(
    24|             "openai_api_model", "OPENAI_API_MODEL"
    25|         )
    26|     async def atransform_documents(
    27|         self, documents: Sequence[Document], **kwargs: Any
    28|     ) -> Sequence[Document]:
    29|         raise NotImplementedError
    30|     def transform_documents(
    31|         self, documents: Sequence[Document], **kwargs: Any
    32|     ) -> Sequence[Document]:
    33|         """Extracts QA from text documents using doctran."""
    34|         try:
    35|             from doctran import Doctran
    36|             doctran = Doctran(
    37|                 openai_api_key=self.openai_api_key, openai_model=self.openai_api_model
    38|             )
    39|         except ImportError:
    40|             raise ImportError(
    41|                 "Install doctran to use this parser. (pip install doctran)"
    42|             )
    43|         for d in documents:
    44|             doctran_doc = doctran.parse(content=d.page_content).interrogate().execute()
    45|             questions_and_answers = doctran_doc.extracted_properties.get(
    46|                 "questions_and_answers"
    47|             )
    48|             d.metadata["questions_and_answers"] = questions_and_answers
    49|         return documents


# ====================================================================
# FILE: libs/community/langchain_community/document_transformers/doctran_text_translate.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 9-55 ---
     9|         language: The language to translate *to*.
    10|     Example:
    11|         .. code-block:: python
    12|         from langchain_community.document_transformers import DoctranTextTranslator
    13|         qa_translator = DoctranTextTranslator(language="spanish")
    14|         translated_document = await qa_translator.atransform_documents(documents)
    15|     """
    16|     def __init__(
    17|         self,
    18|         openai_api_key: Optional[str] = None,
    19|         language: str = "english",
    20|         openai_api_model: Optional[str] = None,
    21|     ) -> None:
    22|         self.openai_api_key = openai_api_key or get_from_env(
    23|             "openai_api_key", "OPENAI_API_KEY"
    24|         )
    25|         self.openai_api_model = openai_api_model or get_from_env(
    26|             "openai_api_model", "OPENAI_API_MODEL"
    27|         )
    28|         self.language = language
    29|     async def atransform_documents(
    30|         self, documents: Sequence[Document], **kwargs: Any
    31|     ) -> Sequence[Document]:
    32|         raise NotImplementedError
    33|     def transform_documents(
    34|         self, documents: Sequence[Document], **kwargs: Any
    35|     ) -> Sequence[Document]:
    36|         """Translates text documents using doctran."""
    37|         try:
    38|             from doctran import Doctran
    39|             doctran = Doctran(
    40|                 openai_api_key=self.openai_api_key, openai_model=self.openai_api_model
    41|             )
    42|         except ImportError:
    43|             raise ImportError(
    44|                 "Install doctran to use this parser. (pip install doctran)"
    45|             )
    46|         doctran_docs = [
    47|             doctran.parse(content=doc.page_content, metadata=doc.metadata)
    48|             for doc in documents
    49|         ]
    50|         for i, doc in enumerate(doctran_docs):
    51|             doctran_docs[i] = doc.translate(language=self.language).execute()
    52|         return [
    53|             Document(page_content=doc.transformed_content, metadata=doc.metadata)
    54|             for doc in doctran_docs
    55|         ]


# ====================================================================
# FILE: libs/community/langchain_community/embeddings/jina.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-49 ---
     1| from typing import Any, Dict, List, Optional
     2| import requests
     3| from langchain_core.embeddings import Embeddings
     4| from langchain_core.pydantic_v1 import BaseModel, SecretStr, root_validator
     5| from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
     6| JINA_API_URL: str = "https://api.jina.ai/v1/embeddings"
     7| class JinaEmbeddings(BaseModel, Embeddings):
     8|     """Jina embedding models."""
     9|     session: Any  #: :meta private:
    10|     model_name: str = "jina-embeddings-v2-base-en"
    11|     jina_api_key: Optional[SecretStr] = None
    12|     @root_validator()
    13|     def validate_environment(cls, values: Dict) -> Dict:
    14|         """Validate that auth token exists in environment."""
    15|         try:
    16|             jina_api_key = convert_to_secret_str(
    17|                 get_from_dict_or_env(values, "jina_api_key", "JINA_API_KEY")
    18|             )
    19|         except ValueError as original_exc:
    20|             try:
    21|                 jina_api_key = convert_to_secret_str(
    22|                     get_from_dict_or_env(values, "jina_auth_token", "JINA_AUTH_TOKEN")
    23|                 )
    24|             except ValueError:
    25|                 raise original_exc
    26|         session = requests.Session()
    27|         session.headers.update(
    28|             {
    29|                 "Authorization": f"Bearer {jina_api_key.get_secret_value()}",
    30|                 "Accept-Encoding": "identity",
    31|                 "Content-type": "application/json",
    32|             }
    33|         )
    34|         values["session"] = session
    35|         return values
    36|     def _embed(self, texts: List[str]) -> List[List[float]]:
    37|         resp = self.session.post(  # type: ignore
    38|             JINA_API_URL, json={"input": texts, "model": self.model_name}
    39|         ).json()
    40|         if "data" not in resp:
    41|             raise RuntimeError(resp["detail"])
    42|         embeddings = resp["data"]
    43|         sorted_embeddings = sorted(embeddings, key=lambda e: e["index"])  # type: ignore
    44|         return [result["embedding"] for result in sorted_embeddings]
    45|     def embed_documents(self, texts: List[str]) -> List[List[float]]:
    46|         """Call out to Jina's embedding endpoint.
    47|         Args:
    48|             texts: The list of texts to embed.
    49|         Returns:


# ====================================================================
# FILE: libs/community/langchain_community/embeddings/minimax.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-27 ---
     1| from __future__ import annotations
     2| import logging
     3| from typing import Any, Callable, Dict, List, Optional
     4| import requests
     5| from langchain_core.embeddings import Embeddings
     6| from langchain_core.pydantic_v1 import BaseModel, Extra, SecretStr, root_validator
     7| from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
     8| from tenacity import (
     9|     before_sleep_log,
    10|     retry,
    11|     stop_after_attempt,
    12|     wait_exponential,
    13| )
    14| logger = logging.getLogger(__name__)
    15| def _create_retry_decorator() -> Callable[[Any], Any]:
    16|     """Returns a tenacity retry decorator."""
    17|     multiplier = 1
    18|     min_seconds = 1
    19|     max_seconds = 4
    20|     max_retries = 6
    21|     return retry(
    22|         reraise=True,
    23|         stop=stop_after_attempt(max_retries),
    24|         wait=wait_exponential(multiplier=multiplier, min=min_seconds, max=max_seconds),
    25|         before_sleep=before_sleep_log(logger, logging.WARNING),
    26|     )
    27| def embed_with_retry(embeddings: MiniMaxEmbeddings, *args: Any, **kwargs: Any) -> Any:

# --- HUNK 2: Lines 38-106 ---
    38|     the constructor.
    39|     Example:
    40|         .. code-block:: python
    41|             from langchain_community.embeddings import MiniMaxEmbeddings
    42|             embeddings = MiniMaxEmbeddings()
    43|             query_text = "This is a test query."
    44|             query_result = embeddings.embed_query(query_text)
    45|             document_text = "This is a test document."
    46|             document_result = embeddings.embed_documents([document_text])
    47|     """
    48|     endpoint_url: str = "https://api.minimax.chat/v1/embeddings"
    49|     """Endpoint URL to use."""
    50|     model: str = "embo-01"
    51|     """Embeddings model name to use."""
    52|     embed_type_db: str = "db"
    53|     """For embed_documents"""
    54|     embed_type_query: str = "query"
    55|     """For embed_query"""
    56|     minimax_group_id: Optional[str] = None
    57|     """Group ID for MiniMax API."""
    58|     minimax_api_key: Optional[SecretStr] = None
    59|     """API Key for MiniMax API."""
    60|     class Config:
    61|         """Configuration for this pydantic object."""
    62|         extra = Extra.forbid
    63|     @root_validator()
    64|     def validate_environment(cls, values: Dict) -> Dict:
    65|         """Validate that group id and api key exists in environment."""
    66|         minimax_group_id = get_from_dict_or_env(
    67|             values, "minimax_group_id", "MINIMAX_GROUP_ID"
    68|         )
    69|         minimax_api_key = convert_to_secret_str(
    70|             get_from_dict_or_env(values, "minimax_api_key", "MINIMAX_API_KEY")
    71|         )
    72|         values["minimax_group_id"] = minimax_group_id
    73|         values["minimax_api_key"] = minimax_api_key
    74|         return values
    75|     def embed(
    76|         self,
    77|         texts: List[str],
    78|         embed_type: str,
    79|     ) -> List[List[float]]:
    80|         payload = {
    81|             "model": self.model,
    82|             "type": embed_type,
    83|             "texts": texts,
    84|         }
    85|         headers = {
    86|             "Authorization": f"Bearer {self.minimax_api_key.get_secret_value()}",
    87|             "Content-Type": "application/json",
    88|         }
    89|         params = {
    90|             "GroupId": self.minimax_group_id,
    91|         }
    92|         response = requests.post(
    93|             self.endpoint_url, params=params, headers=headers, json=payload
    94|         )
    95|         parsed_response = response.json()
    96|         if parsed_response["base_resp"]["status_code"] != 0:
    97|             raise ValueError(
    98|                 f"MiniMax API returned an error: {parsed_response['base_resp']}"
    99|             )
   100|         embeddings = parsed_response["vectors"]
   101|         return embeddings
   102|     def embed_documents(self, texts: List[str]) -> List[List[float]]:
   103|         """Embed documents using a MiniMax embedding endpoint.
   104|         Args:
   105|             texts: The list of texts to embed.
   106|         Returns:


# ====================================================================
# FILE: libs/community/langchain_community/embeddings/vertexai.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 3-62 ---
     3| import string
     4| import threading
     5| from concurrent.futures import ThreadPoolExecutor, wait
     6| from typing import Any, Dict, List, Literal, Optional, Tuple
     7| from langchain_core.embeddings import Embeddings
     8| from langchain_core.language_models.llms import create_base_retry_decorator
     9| from langchain_core.pydantic_v1 import root_validator
    10| from langchain_community.llms.vertexai import _VertexAICommon
    11| from langchain_community.utilities.vertexai import raise_vertex_import_error
    12| logger = logging.getLogger(__name__)
    13| _MAX_TOKENS_PER_BATCH = 20000
    14| _MAX_BATCH_SIZE = 250
    15| _MIN_BATCH_SIZE = 5
    16| class VertexAIEmbeddings(_VertexAICommon, Embeddings):
    17|     """Google Cloud VertexAI embedding models."""
    18|     instance: Dict[str, Any] = {}  #: :meta private:
    19|     @root_validator()
    20|     def validate_environment(cls, values: Dict) -> Dict:
    21|         """Validates that the python package exists in environment."""
    22|         cls._try_init_vertexai(values)
    23|         if values["model_name"] == "textembedding-gecko-default":
    24|             logger.warning(
    25|                 "Model_name will become a required arg for VertexAIEmbeddings "
    26|                 "starting from Feb-01-2024. Currently the default is set to "
    27|                 "textembedding-gecko@001"
    28|             )
    29|             values["model_name"] = "textembedding-gecko@001"
    30|         try:
    31|             from vertexai.language_models import TextEmbeddingModel
    32|         except ImportError:
    33|             raise_vertex_import_error()
    34|         values["client"] = TextEmbeddingModel.from_pretrained(values["model_name"])
    35|         return values
    36|     def __init__(
    37|         self,
    38|         model_name: str = "textembedding-gecko-default",
    39|         project: Optional[str] = None,
    40|         location: str = "us-central1",
    41|         request_parallelism: int = 5,
    42|         max_retries: int = 6,
    43|         credentials: Optional[Any] = None,
    44|         **kwargs: Any,
    45|     ):
    46|         """Initialize the sentence_transformer."""
    47|         super().__init__(
    48|             project=project,
    49|             location=location,
    50|             credentials=credentials,
    51|             request_parallelism=request_parallelism,
    52|             max_retries=max_retries,
    53|             model_name=model_name,
    54|             **kwargs,
    55|         )
    56|         self.instance["max_batch_size"] = kwargs.get("max_batch_size", _MAX_BATCH_SIZE)
    57|         self.instance["batch_size"] = self.instance["max_batch_size"]
    58|         self.instance["min_batch_size"] = kwargs.get("min_batch_size", _MIN_BATCH_SIZE)
    59|         self.instance["min_good_batch_size"] = self.instance["min_batch_size"]
    60|         self.instance["lock"] = threading.Lock()
    61|         self.instance["batch_size_validated"] = False
    62|         self.instance["task_executor"] = ThreadPoolExecutor(


# ====================================================================
# FILE: libs/community/langchain_community/llms/__init__.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 148-197 ---
   148|     from langchain_community.llms.minimax import Minimax
   149|     return Minimax
   150| def _import_mlflow() -> Any:
   151|     from langchain_community.llms.mlflow import Mlflow
   152|     return Mlflow
   153| def _import_mlflow_chat() -> Any:
   154|     from langchain_community.chat_models.mlflow import ChatMlflow
   155|     return ChatMlflow
   156| def _import_mlflow_ai_gateway() -> Any:
   157|     from langchain_community.llms.mlflow_ai_gateway import MlflowAIGateway
   158|     return MlflowAIGateway
   159| def _import_modal() -> Any:
   160|     from langchain_community.llms.modal import Modal
   161|     return Modal
   162| def _import_mosaicml() -> Any:
   163|     from langchain_community.llms.mosaicml import MosaicML
   164|     return MosaicML
   165| def _import_nlpcloud() -> Any:
   166|     from langchain_community.llms.nlpcloud import NLPCloud
   167|     return NLPCloud
   168| def _import_oci_md_tgi() -> Any:
   169|     from langchain_community.llms.oci_data_science_model_deployment_endpoint import (
   170|         OCIModelDeploymentTGI,
   171|     )
   172|     return OCIModelDeploymentTGI
   173| def _import_oci_md_vllm() -> Any:
   174|     from langchain_community.llms.oci_data_science_model_deployment_endpoint import (
   175|         OCIModelDeploymentVLLM,
   176|     )
   177|     return OCIModelDeploymentVLLM
   178| def _import_octoai_endpoint() -> Any:
   179|     from langchain_community.llms.octoai_endpoint import OctoAIEndpoint
   180|     return OctoAIEndpoint
   181| def _import_ollama() -> Any:
   182|     from langchain_community.llms.ollama import Ollama
   183|     return Ollama
   184| def _import_opaqueprompts() -> Any:
   185|     from langchain_community.llms.opaqueprompts import OpaquePrompts
   186|     return OpaquePrompts
   187| def _import_azure_openai() -> Any:
   188|     from langchain_community.llms.openai import AzureOpenAI
   189|     return AzureOpenAI
   190| def _import_openai() -> Any:
   191|     from langchain_community.llms.openai import OpenAI
   192|     return OpenAI
   193| def _import_openai_chat() -> Any:
   194|     from langchain_community.llms.openai import OpenAIChat
   195|     return OpenAIChat
   196| def _import_openllm() -> Any:
   197|     from langchain_community.llms.openllm import OpenLLM

# --- HUNK 2: Lines 365-408 ---
   365|     elif name == "JavelinAIGateway":
   366|         return _import_javelin_ai_gateway()
   367|     elif name == "KoboldApiLLM":
   368|         return _import_koboldai()
   369|     elif name == "LlamaCpp":
   370|         return _import_llamacpp()
   371|     elif name == "ManifestWrapper":
   372|         return _import_manifest()
   373|     elif name == "Minimax":
   374|         return _import_minimax()
   375|     elif name == "Mlflow":
   376|         return _import_mlflow()
   377|     elif name == "MlflowAIGateway":
   378|         return _import_mlflow_ai_gateway()
   379|     elif name == "Modal":
   380|         return _import_modal()
   381|     elif name == "MosaicML":
   382|         return _import_mosaicml()
   383|     elif name == "NLPCloud":
   384|         return _import_nlpcloud()
   385|     elif name == "OCIModelDeploymentTGI":
   386|         return _import_oci_md_tgi()
   387|     elif name == "OCIModelDeploymentVLLM":
   388|         return _import_oci_md_vllm()
   389|     elif name == "OctoAIEndpoint":
   390|         return _import_octoai_endpoint()
   391|     elif name == "Ollama":
   392|         return _import_ollama()
   393|     elif name == "OpaquePrompts":
   394|         return _import_opaqueprompts()
   395|     elif name == "AzureOpenAI":
   396|         return _import_azure_openai()
   397|     elif name == "OpenAI":
   398|         return _import_openai()
   399|     elif name == "OpenAIChat":
   400|         return _import_openai_chat()
   401|     elif name == "OpenLLM":
   402|         return _import_openllm()
   403|     elif name == "OpenLM":
   404|         return _import_openlm()
   405|     elif name == "PaiEasEndpoint":
   406|         return _import_pai_eas_endpoint()
   407|     elif name == "Petals":
   408|         return _import_petals()

# --- HUNK 3: Lines 497-538 ---
   497|     "GPT4All",
   498|     "GooglePalm",
   499|     "GooseAI",
   500|     "GradientLLM",
   501|     "HuggingFaceEndpoint",
   502|     "HuggingFaceHub",
   503|     "HuggingFacePipeline",
   504|     "HuggingFaceTextGenInference",
   505|     "HumanInputLLM",
   506|     "KoboldApiLLM",
   507|     "LlamaCpp",
   508|     "TextGen",
   509|     "ManifestWrapper",
   510|     "Minimax",
   511|     "MlflowAIGateway",
   512|     "Modal",
   513|     "MosaicML",
   514|     "Nebula",
   515|     "NIBittensorLLM",
   516|     "NLPCloud",
   517|     "OCIModelDeploymentTGI",
   518|     "OCIModelDeploymentVLLM",
   519|     "Ollama",
   520|     "OpenAI",
   521|     "OpenAIChat",
   522|     "OpenLLM",
   523|     "OpenLM",
   524|     "PaiEasEndpoint",
   525|     "Petals",
   526|     "PipelineAI",
   527|     "Predibase",
   528|     "PredictionGuard",
   529|     "PromptLayerOpenAI",
   530|     "PromptLayerOpenAIChat",
   531|     "OpaquePrompts",
   532|     "RWKV",
   533|     "Replicate",
   534|     "SagemakerEndpoint",
   535|     "SelfHostedHuggingFaceLLM",
   536|     "SelfHostedPipeline",
   537|     "StochasticAI",
   538|     "TitanTakeoff",

# --- HUNK 4: Lines 584-625 ---
   584|         "gooseai": _import_gooseai,
   585|         "gradient": _import_gradient_ai,
   586|         "gpt4all": _import_gpt4all,
   587|         "huggingface_endpoint": _import_huggingface_endpoint,
   588|         "huggingface_hub": _import_huggingface_hub,
   589|         "huggingface_pipeline": _import_huggingface_pipeline,
   590|         "huggingface_textgen_inference": _import_huggingface_text_gen_inference,
   591|         "human-input": _import_human,
   592|         "koboldai": _import_koboldai,
   593|         "llamacpp": _import_llamacpp,
   594|         "textgen": _import_textgen,
   595|         "minimax": _import_minimax,
   596|         "mlflow": _import_mlflow,
   597|         "mlflow-chat": _import_mlflow_chat,
   598|         "mlflow-ai-gateway": _import_mlflow_ai_gateway,
   599|         "modal": _import_modal,
   600|         "mosaic": _import_mosaicml,
   601|         "nebula": _import_symblai_nebula,
   602|         "nibittensor": _import_bittensor,
   603|         "nlpcloud": _import_nlpcloud,
   604|         "oci_model_deployment_tgi_endpoint": _import_oci_md_tgi,
   605|         "oci_model_deployment_vllm_endpoint": _import_oci_md_vllm,
   606|         "ollama": _import_ollama,
   607|         "openai": _import_openai,
   608|         "openlm": _import_openlm,
   609|         "pai_eas_endpoint": _import_pai_eas_endpoint,
   610|         "petals": _import_petals,
   611|         "pipelineai": _import_pipelineai,
   612|         "predibase": _import_predibase,
   613|         "opaqueprompts": _import_opaqueprompts,
   614|         "replicate": _import_replicate,
   615|         "rwkv": _import_rwkv,
   616|         "sagemaker_endpoint": _import_sagemaker_endpoint,
   617|         "self_hosted": _import_self_hosted,
   618|         "self_hosted_hugging_face": _import_self_hosted_hugging_face,
   619|         "stochasticai": _import_stochasticai,
   620|         "together": _import_together,
   621|         "tongyi": _import_tongyi,
   622|         "titan_takeoff": _import_titan_takeoff,
   623|         "titan_takeoff_pro": _import_titan_takeoff_pro,
   624|         "vertexai": _import_vertex,
   625|         "vertexai_model_garden": _import_vertex_model_garden,


# ====================================================================
# FILE: libs/community/langchain_community/llms/baseten.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-71 ---
     1| import logging
     2| import os
     3| from typing import Any, Dict, List, Mapping, Optional
     4| import requests
     5| from langchain_core.callbacks import CallbackManagerForLLMRun
     6| from langchain_core.language_models.llms import LLM
     7| from langchain_core.pydantic_v1 import Field
     8| logger = logging.getLogger(__name__)
     9| class Baseten(LLM):
    10|     """Baseten model
    11|     This module allows using LLMs hosted on Baseten.
    12|     The LLM deployed on Baseten must have the following properties:
    13|     * Must accept input as a dictionary with the key "prompt"
    14|     * May accept other input in the dictionary passed through with kwargs
    15|     * Must return a string with the model output
    16|     To use this module, you must:
    17|     * Export your Baseten API key as the environment variable `BASETEN_API_KEY`
    18|     * Get the model ID for your model from your Baseten dashboard
    19|     * Identify the model deployment ("production" for all model library models)
    20|     These code samples use
    21|     [Mistral 7B Instruct](https://app.baseten.co/explore/mistral_7b_instruct)
    22|     from Baseten's model library.
    23|     Examples:
    24|         .. code-block:: python
    25|             from langchain_community.llms import Baseten
    26|             mistral = Baseten(model="MODEL_ID", deployment="production")
    27|             mistral("What is the Mistral wind?")
    28|         .. code-block:: python
    29|             from langchain_community.llms import Baseten
    30|             mistral = Baseten(model="MODEL_ID", deployment="development")
    31|             mistral("What is the Mistral wind?")
    32|         .. code-block:: python
    33|             from langchain_community.llms import Baseten
    34|             mistral = Baseten(model="MODEL_ID", deployment="DEPLOYMENT_ID")
    35|             mistral("What is the Mistral wind?")
    36|     """
    37|     model: str
    38|     deployment: str
    39|     input: Dict[str, Any] = Field(default_factory=dict)
    40|     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    41|     @property
    42|     def _identifying_params(self) -> Mapping[str, Any]:
    43|         """Get the identifying parameters."""
    44|         return {
    45|             **{"model_kwargs": self.model_kwargs},
    46|         }
    47|     @property
    48|     def _llm_type(self) -> str:
    49|         """Return type of model."""
    50|         return "baseten"
    51|     def _call(
    52|         self,
    53|         prompt: str,
    54|         stop: Optional[List[str]] = None,
    55|         run_manager: Optional[CallbackManagerForLLMRun] = None,
    56|         **kwargs: Any,
    57|     ) -> str:
    58|         baseten_api_key = os.environ["BASETEN_API_KEY"]
    59|         model_id = self.model
    60|         if self.deployment == "production":
    61|             model_url = f"https://model-{model_id}.api.baseten.co/production/predict"
    62|         elif self.deployment == "development":
    63|             model_url = f"https://model-{model_id}.api.baseten.co/development/predict"
    64|         else:  # try specific deployment ID
    65|             model_url = f"https://model-{model_id}.api.baseten.co/deployment/{self.deployment}/predict"
    66|         response = requests.post(
    67|             model_url,
    68|             headers={"Authorization": f"Api-Key {baseten_api_key}"},
    69|             json={"prompt": prompt, **kwargs},
    70|         )
    71|         return response.json()


# ====================================================================
# FILE: libs/community/langchain_community/llms/bedrock.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| from __future__ import annotations
     2| import json
     3| import warnings
     4| from abc import ABC
     5| from typing import TYPE_CHECKING, Any, Dict, Iterator, List, Mapping, Optional
     6| from langchain_core.callbacks import CallbackManagerForLLMRun
     7| from langchain_core.language_models.llms import LLM
     8| from langchain_core.outputs import GenerationChunk
     9| from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
    10| from langchain_core.utils import get_from_dict_or_env
    11| from langchain_community.llms.utils import enforce_stop_tokens
    12| from langchain_community.utilities.anthropic import (
    13|     get_num_tokens_anthropic,
    14|     get_token_ids_anthropic,
    15| )
    16| if TYPE_CHECKING:
    17|     from botocore.config import Config
    18| HUMAN_PROMPT = "\n\nHuman:"
    19| ASSISTANT_PROMPT = "\n\nAssistant:"
    20| ALTERNATION_ERROR = (
    21|     "Error: Prompt must alternate between '\n\nHuman:' and '\n\nAssistant:'."
    22| )
    23| def _add_newlines_before_ha(input_text: str) -> str:
    24|     new_text = input_text
    25|     for word in ["Human:", "Assistant:"]:
    26|         new_text = new_text.replace(word, "\n\n" + word)
    27|         for i in range(2):
    28|             new_text = new_text.replace("\n\n\n" + word, "\n\n" + word)
    29|     return new_text
    30| def _human_assistant_format(input_text: str) -> str:
    31|     if input_text.count("Human:") == 0 or (
    32|         input_text.find("Human:") > input_text.find("Assistant:")
    33|         and "Assistant:" in input_text
    34|     ):
    35|         input_text = HUMAN_PROMPT + " " + input_text  # SILENT CORRECTION
    36|     if input_text.count("Assistant:") == 0:
    37|         input_text = input_text + ASSISTANT_PROMPT  # SILENT CORRECTION

# --- HUNK 2: Lines 118-198 ---
   118|                     == "<EOS_TOKEN>"
   119|                 ):
   120|                     return
   121|                 yield GenerationChunk(
   122|                     text=chunk_obj[cls.provider_to_output_key_map[provider]]
   123|                 )
   124| class BedrockBase(BaseModel, ABC):
   125|     """Base class for Bedrock models."""
   126|     client: Any = Field(exclude=True)  #: :meta private:
   127|     region_name: Optional[str] = None
   128|     """The aws region e.g., `us-west-2`. Fallsback to AWS_DEFAULT_REGION env variable
   129|     or region specified in ~/.aws/config in case it is not provided here.
   130|     """
   131|     credentials_profile_name: Optional[str] = Field(default=None, exclude=True)
   132|     """The name of the profile in the ~/.aws/credentials or ~/.aws/config files, which
   133|     has either access keys or role information specified.
   134|     If not specified, the default credential profile or, if on an EC2 instance,
   135|     credentials from IMDS will be used.
   136|     See: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html
   137|     """
   138|     config: Optional[Config] = None
   139|     """An optional botocore.config.Config instance to pass to the client."""
   140|     model_id: str
   141|     """Id of the model to call, e.g., amazon.titan-text-express-v1, this is
   142|     equivalent to the modelId property in the list-foundation-models api"""
   143|     model_kwargs: Optional[Dict] = None
   144|     """Keyword arguments to pass to the model."""
   145|     endpoint_url: Optional[str] = None
   146|     """Needed if you don't want to default to us-east-1 endpoint"""
   147|     streaming: bool = False
   148|     """Whether to stream the results."""
   149|     provider_stop_sequence_key_name_map: Mapping[str, str] = {
   150|         "anthropic": "stop_sequences",
   151|         "amazon": "stopSequences",
   152|         "ai21": "stop_sequences",
   153|         "cohere": "stop_sequences",
   154|     }
   155|     @root_validator()
   156|     def validate_environment(cls, values: Dict) -> Dict:
   157|         """Validate that AWS credentials to and python package exists in environment."""
   158|         if values["client"] is not None:
   159|             return values
   160|         try:
   161|             import boto3
   162|             if values["credentials_profile_name"] is not None:
   163|                 session = boto3.Session(profile_name=values["credentials_profile_name"])
   164|             else:
   165|                 session = boto3.Session()
   166|             values["region_name"] = get_from_dict_or_env(
   167|                 values,
   168|                 "region_name",
   169|                 "AWS_DEFAULT_REGION",
   170|                 default=session.region_name,
   171|             )
   172|             client_params = {}
   173|             if values["region_name"]:
   174|                 client_params["region_name"] = values["region_name"]
   175|             if values["endpoint_url"]:
   176|                 client_params["endpoint_url"] = values["endpoint_url"]
   177|             if values["config"]:
   178|                 client_params["config"] = values["config"]
   179|             values["client"] = session.client("bedrock-runtime", **client_params)
   180|         except ImportError:
   181|             raise ModuleNotFoundError(
   182|                 "Could not import boto3 python package. "
   183|                 "Please install it with `pip install boto3`."
   184|             )
   185|         except Exception as e:
   186|             raise ValueError(
   187|                 "Could not load credentials to authenticate with AWS client. "
   188|                 "Please check that credentials in the specified "
   189|                 "profile name are valid."
   190|             ) from e
   191|         return values
   192|     @property
   193|     def _identifying_params(self) -> Mapping[str, Any]:
   194|         """Get the identifying parameters."""
   195|         _model_kwargs = self.model_kwargs or {}
   196|         return {
   197|             **{"model_kwargs": _model_kwargs},
   198|         }


# ====================================================================
# FILE: libs/community/langchain_community/llms/databricks.py
# Total hunks: 6
# ====================================================================
# --- HUNK 1: Lines 23-123 ---
    23|         response = requests.request(
    24|             method=method, url=url, headers=headers, json=request
    25|         )
    26|         if not response.ok:
    27|             raise ValueError(f"HTTP {response.status_code} error: {response.text}")
    28|         return response.json()
    29|     def _get(self, url: str) -> Any:
    30|         return self.request("GET", url, None)
    31|     def _post(self, url: str, request: Any) -> Any:
    32|         return self.request("POST", url, request)
    33|     @abstractmethod
    34|     def post(
    35|         self, request: Any, transform_output_fn: Optional[Callable[..., str]] = None
    36|     ) -> Any:
    37|         ...
    38|     @property
    39|     def llm(self) -> bool:
    40|         return False
    41| def _transform_completions(response: Dict[str, Any]) -> str:
    42|     return response["choices"][0]["text"]
    43| def _transform_llama2_chat(response: Dict[str, Any]) -> str:
    44|     return response["candidates"][0]["text"]
    45| def _transform_chat(response: Dict[str, Any]) -> str:
    46|     return response["choices"][0]["message"]["content"]
    47| class _DatabricksServingEndpointClient(_DatabricksClientBase):
    48|     """An API client that talks to a Databricks serving endpoint."""
    49|     host: str
    50|     endpoint_name: str
    51|     databricks_uri: str
    52|     client: Any = None
    53|     external_or_foundation: bool = False
    54|     task: Optional[str] = None
    55|     def __init__(self, **data: Any):
    56|         super().__init__(**data)
    57|         try:
    58|             from mlflow.deployments import get_deploy_client
    59|             self.client = get_deploy_client(self.databricks_uri)
    60|         except ImportError as e:
    61|             raise ImportError(
    62|                 "Failed to create the client. "
    63|                 "Please install mlflow with `pip install mlflow`."
    64|             ) from e
    65|         endpoint = self.client.get_endpoint(self.endpoint_name)
    66|         self.external_or_foundation = endpoint.get("endpoint_type", "").lower() in (
    67|             "external_model",
    68|             "foundation_model_api",
    69|         )
    70|         if self.task is None:
    71|             self.task = endpoint.get("task")
    72|     @property
    73|     def llm(self) -> bool:
    74|         return self.task in ("llm/v1/chat", "llm/v1/completions", "llama2/chat")
    75|     @root_validator(pre=True)
    76|     def set_api_url(cls, values: Dict[str, Any]) -> Dict[str, Any]:
    77|         if "api_url" not in values:
    78|             host = values["host"]
    79|             endpoint_name = values["endpoint_name"]
    80|             api_url = f"https://{host}/serving-endpoints/{endpoint_name}/invocations"
    81|             values["api_url"] = api_url
    82|         return values
    83|     def post(
    84|         self, request: Any, transform_output_fn: Optional[Callable[..., str]] = None
    85|     ) -> Any:
    86|         if self.external_or_foundation:
    87|             resp = self.client.predict(endpoint=self.endpoint_name, inputs=request)
    88|             if transform_output_fn:
    89|                 return transform_output_fn(resp)
    90|             if self.task == "llm/v1/chat":
    91|                 return _transform_chat(resp)
    92|             elif self.task == "llm/v1/completions":
    93|                 return _transform_completions(resp)
    94|             return resp
    95|         else:
    96|             wrapped_request = {"dataframe_records": [request]}
    97|             response = self.client.predict(
    98|                 endpoint=self.endpoint_name, inputs=wrapped_request
    99|             )
   100|             preds = response["predictions"]
   101|             pred = preds[0] if isinstance(preds, list) else preds
   102|             if self.task == "llama2/chat":
   103|                 return _transform_llama2_chat(pred)
   104|             return transform_output_fn(pred) if transform_output_fn else pred
   105| class _DatabricksClusterDriverProxyClient(_DatabricksClientBase):
   106|     """An API client that talks to a Databricks cluster driver proxy app."""
   107|     host: str
   108|     cluster_id: str
   109|     cluster_driver_port: str
   110|     @root_validator(pre=True)
   111|     def set_api_url(cls, values: Dict[str, Any]) -> Dict[str, Any]:
   112|         if "api_url" not in values:
   113|             host = values["host"]
   114|             cluster_id = values["cluster_id"]
   115|             port = values["cluster_driver_port"]
   116|             api_url = f"https://{host}/driver-proxy-api/o/0/{cluster_id}/{port}"
   117|             values["api_url"] = api_url
   118|         return values
   119|     def post(
   120|         self, request: Any, transform_output_fn: Optional[Callable[..., str]] = None
   121|     ) -> Any:
   122|         resp = self._post(self.api_url, request)
   123|         return transform_output_fn(resp) if transform_output_fn else resp

# --- HUNK 2: Lines 243-286 ---
   243|     transform_input_fn: Optional[Callable] = None
   244|     """A function that transforms ``{prompt, stop, **kwargs}`` into a JSON-compatible
   245|     request object that the endpoint accepts.
   246|     For example, you can apply a prompt template to the input prompt.
   247|     """
   248|     transform_output_fn: Optional[Callable[..., str]] = None
   249|     """A function that transforms the output from the endpoint to the generated text.
   250|     """
   251|     databricks_uri: str = "databricks"
   252|     """The databricks URI. Only used when using a serving endpoint."""
   253|     temperature: float = 0.0
   254|     """The sampling temperature."""
   255|     n: int = 1
   256|     """The number of completion choices to generate."""
   257|     stop: Optional[List[str]] = None
   258|     """The stop sequence."""
   259|     max_tokens: Optional[int] = None
   260|     """The maximum number of tokens to generate."""
   261|     extra_params: Dict[str, Any] = Field(default_factory=dict)
   262|     """Any extra parameters to pass to the endpoint."""
   263|     task: Optional[str] = None
   264|     """The task of the endpoint. Only used when using a serving endpoint.
   265|     If not provided, the task is automatically inferred from the endpoint.
   266|     """
   267|     _client: _DatabricksClientBase = PrivateAttr()
   268|     class Config:
   269|         extra = Extra.forbid
   270|         underscore_attrs_are_private = True
   271|     @property
   272|     def _llm_params(self) -> Dict[str, Any]:
   273|         params: Dict[str, Any] = {
   274|             "temperature": self.temperature,
   275|             "n": self.n,
   276|         }
   277|         if self.stop:
   278|             params["stop"] = self.stop
   279|         if self.max_tokens is not None:
   280|             params["max_tokens"] = self.max_tokens
   281|         return params
   282|     @validator("cluster_id", always=True)
   283|     def set_cluster_id(cls, v: Any, values: Dict[str, Any]) -> Optional[str]:
   284|         if v and values["endpoint_name"]:
   285|             raise ValueError("Cannot set both endpoint_name and cluster_id.")
   286|         elif values["endpoint_name"]:

# --- HUNK 3: Lines 316-384 ---
   316|     def set_model_kwargs(cls, v: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
   317|         if v:
   318|             assert "prompt" not in v, "model_kwargs must not contain key 'prompt'"
   319|             assert "stop" not in v, "model_kwargs must not contain key 'stop'"
   320|         return v
   321|     def __init__(self, **data: Any):
   322|         super().__init__(**data)
   323|         if self.model_kwargs is not None and self.extra_params is not None:
   324|             raise ValueError("Cannot set both extra_params and extra_params.")
   325|         elif self.model_kwargs is not None:
   326|             warnings.warn(
   327|                 "model_kwargs is deprecated. Please use extra_params instead.",
   328|                 DeprecationWarning,
   329|             )
   330|         if self.endpoint_name:
   331|             self._client = _DatabricksServingEndpointClient(
   332|                 host=self.host,
   333|                 api_token=self.api_token,
   334|                 endpoint_name=self.endpoint_name,
   335|                 databricks_uri=self.databricks_uri,
   336|                 task=self.task,
   337|             )
   338|         elif self.cluster_id and self.cluster_driver_port:
   339|             self._client = _DatabricksClusterDriverProxyClient(
   340|                 host=self.host,
   341|                 api_token=self.api_token,
   342|                 cluster_id=self.cluster_id,
   343|                 cluster_driver_port=self.cluster_driver_port,
   344|             )
   345|         else:
   346|             raise ValueError(
   347|                 "Must specify either endpoint_name or cluster_id/cluster_driver_port."
   348|             )
   349|     @property
   350|     def _default_params(self) -> Dict[str, Any]:
   351|         """Return default params."""
   352|         return {
   353|             "host": self.host,
   354|             "endpoint_name": self.endpoint_name,
   355|             "cluster_id": self.cluster_id,
   356|             "cluster_driver_port": self.cluster_driver_port,
   357|             "databricks_uri": self.databricks_uri,
   358|             "model_kwargs": self.model_kwargs,
   359|             "temperature": self.temperature,
   360|             "n": self.n,
   361|             "stop": self.stop,
   362|             "max_tokens": self.max_tokens,
   363|             "extra_params": self.extra_params,
   364|             "task": self.task,
   365|         }
   366|     @property
   367|     def _identifying_params(self) -> Mapping[str, Any]:
   368|         return self._default_params
   369|     @property
   370|     def _llm_type(self) -> str:
   371|         """Return type of llm."""
   372|         return "databricks"
   373|     def _call(
   374|         self,
   375|         prompt: str,
   376|         stop: Optional[List[str]] = None,
   377|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   378|         **kwargs: Any,
   379|     ) -> str:
   380|         """Queries the LLM endpoint with the given prompt and stop sequence."""
   381|         request: Dict[str, Any] = {"prompt": prompt}
   382|         if self._client.llm:
   383|             request.update(self._llm_params)
   384|         request.update(self.model_kwargs or self.extra_params)


# ====================================================================
# FILE: libs/community/langchain_community/llms/deepsparse.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 35-205 ---
    35|     def _identifying_params(self) -> Dict[str, Any]:
    36|         """Get the identifying parameters."""
    37|         return {
    38|             "model": self.model,
    39|             "model_config": self.model_config,
    40|             "generation_config": self.generation_config,
    41|             "streaming": self.streaming,
    42|         }
    43|     @property
    44|     def _llm_type(self) -> str:
    45|         """Return type of llm."""
    46|         return "deepsparse"
    47|     @root_validator()
    48|     def validate_environment(cls, values: Dict) -> Dict:
    49|         """Validate that ``deepsparse`` package is installed."""
    50|         try:
    51|             from deepsparse import Pipeline
    52|         except ImportError:
    53|             raise ImportError(
    54|                 "Could not import `deepsparse` package. "
    55|                 "Please install it with `pip install deepsparse[llm]`"
    56|             )
    57|         model_config = values["model_config"] or {}
    58|         values["pipeline"] = Pipeline.create(
    59|             task="text_generation",
    60|             model_path=values["model"],
    61|             **model_config,
    62|         )
    63|         return values
    64|     def _call(
    65|         self,
    66|         prompt: str,
    67|         stop: Optional[List[str]] = None,
    68|         run_manager: Optional[CallbackManagerForLLMRun] = None,
    69|         **kwargs: Any,
    70|     ) -> str:
    71|         """Generate text from a prompt.
    72|         Args:
    73|             prompt: The prompt to generate text from.
    74|             stop: A list of strings to stop generation when encountered.
    75|         Returns:
    76|             The generated text.
    77|         Example:
    78|             .. code-block:: python
    79|                 from langchain_community.llms import DeepSparse
    80|                 llm = DeepSparse(model="zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base_quant-none")
    81|                 llm("Tell me a joke.")
    82|         """
    83|         if self.streaming:
    84|             combined_output = ""
    85|             for chunk in self._stream(
    86|                 prompt=prompt, stop=stop, run_manager=run_manager, **kwargs
    87|             ):
    88|                 combined_output += chunk.text
    89|             text = combined_output
    90|         else:
    91|             text = (
    92|                 self.pipeline(sequences=prompt, **self.generation_config)
    93|                 .generations[0]
    94|                 .text
    95|             )
    96|         if stop is not None:
    97|             text = enforce_stop_tokens(text, stop)
    98|         return text
    99|     async def _acall(
   100|         self,
   101|         prompt: str,
   102|         stop: Optional[List[str]] = None,
   103|         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
   104|         **kwargs: Any,
   105|     ) -> str:
   106|         """Generate text from a prompt.
   107|         Args:
   108|             prompt: The prompt to generate text from.
   109|             stop: A list of strings to stop generation when encountered.
   110|         Returns:
   111|             The generated text.
   112|         Example:
   113|             .. code-block:: python
   114|                 from langchain_community.llms import DeepSparse
   115|                 llm = DeepSparse(model="zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base_quant-none")
   116|                 llm("Tell me a joke.")
   117|         """
   118|         if self.streaming:
   119|             combined_output = ""
   120|             async for chunk in self._astream(
   121|                 prompt=prompt, stop=stop, run_manager=run_manager, **kwargs
   122|             ):
   123|                 combined_output += chunk.text
   124|             text = combined_output
   125|         else:
   126|             text = (
   127|                 self.pipeline(sequences=prompt, **self.generation_config)
   128|                 .generations[0]
   129|                 .text
   130|             )
   131|         if stop is not None:
   132|             text = enforce_stop_tokens(text, stop)
   133|         return text
   134|     def _stream(
   135|         self,
   136|         prompt: str,
   137|         stop: Optional[List[str]] = None,
   138|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   139|         **kwargs: Any,
   140|     ) -> Iterator[GenerationChunk]:
   141|         """Yields results objects as they are generated in real time.
   142|         It also calls the callback manager's on_llm_new_token event with
   143|         similar parameters to the OpenAI LLM class method of the same name.
   144|         Args:
   145|             prompt: The prompt to pass into the model.
   146|             stop: Optional list of stop words to use when generating.
   147|         Returns:
   148|             A generator representing the stream of tokens being generated.
   149|         Yields:
   150|             A dictionary like object containing a string token.
   151|         Example:
   152|             .. code-block:: python
   153|                 from langchain_community.llms import DeepSparse
   154|                 llm = DeepSparse(
   155|                     model="zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base_quant-none",
   156|                     streaming=True
   157|                 )
   158|                 for chunk in llm.stream("Tell me a joke",
   159|                         stop=["'","\n"]):
   160|                     print(chunk, end='', flush=True)
   161|         """
   162|         inference = self.pipeline(
   163|             sequences=prompt, streaming=True, **self.generation_config
   164|         )
   165|         for token in inference:
   166|             chunk = GenerationChunk(text=token.generations[0].text)
   167|             yield chunk
   168|             if run_manager:
   169|                 run_manager.on_llm_new_token(token=chunk.text)
   170|     async def _astream(
   171|         self,
   172|         prompt: str,
   173|         stop: Optional[List[str]] = None,
   174|         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
   175|         **kwargs: Any,
   176|     ) -> AsyncIterator[GenerationChunk]:
   177|         """Yields results objects as they are generated in real time.
   178|         It also calls the callback manager's on_llm_new_token event with
   179|         similar parameters to the OpenAI LLM class method of the same name.
   180|         Args:
   181|             prompt: The prompt to pass into the model.
   182|             stop: Optional list of stop words to use when generating.
   183|         Returns:
   184|             A generator representing the stream of tokens being generated.
   185|         Yields:
   186|             A dictionary like object containing a string token.
   187|         Example:
   188|             .. code-block:: python
   189|                 from langchain_community.llms import DeepSparse
   190|                 llm = DeepSparse(
   191|                     model="zoo:nlg/text_generation/codegen_mono-350m/pytorch/huggingface/bigpython_bigquery_thepile/base_quant-none",
   192|                     streaming=True
   193|                 )
   194|                 for chunk in llm.stream("Tell me a joke",
   195|                         stop=["'","\n"]):
   196|                     print(chunk, end='', flush=True)
   197|         """
   198|         inference = self.pipeline(
   199|             sequences=prompt, streaming=True, **self.generation_config
   200|         )
   201|         for token in inference:
   202|             chunk = GenerationChunk(text=token.generations[0].text)
   203|             yield chunk
   204|             if run_manager:
   205|                 await run_manager.on_llm_new_token(token=chunk.text)


# ====================================================================
# FILE: libs/community/langchain_community/llms/oci_data_science_model_deployment_endpoint.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-289 ---
     1| import logging
     2| from typing import Any, Dict, List, Optional
     3| import requests
     4| from langchain_core.callbacks import CallbackManagerForLLMRun
     5| from langchain_core.language_models.llms import LLM
     6| from langchain_core.pydantic_v1 import Field, root_validator
     7| from langchain_core.utils import get_from_dict_or_env
     8| logger = logging.getLogger(__name__)
     9| DEFAULT_TIME_OUT = 300
    10| DEFAULT_CONTENT_TYPE_JSON = "application/json"
    11| class OCIModelDeploymentLLM(LLM):
    12|     """Base class for LLM deployed on OCI Data Science Model Deployment."""
    13|     auth: dict = Field(default_factory=dict, exclude=True)
    14|     """ADS auth dictionary for OCI authentication:
    15|     https://accelerated-data-science.readthedocs.io/en/latest/user_guide/cli/authentication.html.
    16|     This can be generated by calling `ads.common.auth.api_keys()`
    17|     or `ads.common.auth.resource_principal()`. If this is not
    18|     provided then the `ads.common.default_signer()` will be used."""
    19|     max_tokens: int = 256
    20|     """Denotes the number of tokens to predict per generation."""
    21|     temperature: float = 0.2
    22|     """A non-negative float that tunes the degree of randomness in generation."""
    23|     k: int = 0
    24|     """Number of most likely tokens to consider at each step."""
    25|     p: float = 0.75
    26|     """Total probability mass of tokens to consider at each step."""
    27|     endpoint: str = ""
    28|     """The uri of the endpoint from the deployed Model Deployment model."""
    29|     best_of: int = 1
    30|     """Generates best_of completions server-side and returns the "best"
    31|     (the one with the highest log probability per token).
    32|     """
    33|     stop: Optional[List[str]] = None
    34|     """Stop words to use when generating. Model output is cut off
    35|     at the first occurrence of any of these substrings."""
    36|     @root_validator()
    37|     def validate_environment(  # pylint: disable=no-self-argument
    38|         cls, values: Dict
    39|     ) -> Dict:
    40|         """Validate that python package exists in environment."""
    41|         try:
    42|             import ads
    43|         except ImportError as ex:
    44|             raise ImportError(
    45|                 "Could not import ads python package. "
    46|                 "Please install it with `pip install oracle_ads`."
    47|             ) from ex
    48|         if not values.get("auth", None):
    49|             values["auth"] = ads.common.auth.default_signer()
    50|         values["endpoint"] = get_from_dict_or_env(
    51|             values,
    52|             "endpoint",
    53|             "OCI_LLM_ENDPOINT",
    54|         )
    55|         return values
    56|     @property
    57|     def _default_params(self) -> Dict[str, Any]:
    58|         """Default parameters for the model."""
    59|         raise NotImplementedError
    60|     @property
    61|     def _identifying_params(self) -> Dict[str, Any]:
    62|         """Get the identifying parameters."""
    63|         return {
    64|             **{"endpoint": self.endpoint},
    65|             **self._default_params,
    66|         }
    67|     def _construct_json_body(self, prompt: str, params: dict) -> dict:
    68|         """Constructs the request body as a dictionary (JSON)."""
    69|         raise NotImplementedError
    70|     def _invocation_params(self, stop: Optional[List[str]], **kwargs: Any) -> dict:
    71|         """Combines the invocation parameters with default parameters."""
    72|         params = self._default_params
    73|         if self.stop is not None and stop is not None:
    74|             raise ValueError("`stop` found in both the input and default params.")
    75|         elif self.stop is not None:
    76|             params["stop"] = self.stop
    77|         elif stop is not None:
    78|             params["stop"] = stop
    79|         else:
    80|             params["stop"] = []
    81|         return {**params, **kwargs}
    82|     def _process_response(self, response_json: dict) -> str:
    83|         raise NotImplementedError
    84|     def _call(
    85|         self,
    86|         prompt: str,
    87|         stop: Optional[List[str]] = None,
    88|         run_manager: Optional[CallbackManagerForLLMRun] = None,
    89|         **kwargs: Any,
    90|     ) -> str:
    91|         """Call out to OCI Data Science Model Deployment endpoint.
    92|         Args:
    93|             prompt (str):
    94|                 The prompt to pass into the model.
    95|             stop (List[str], Optional):
    96|                 List of stop words to use when generating.
    97|             kwargs:
    98|                 requests_kwargs:
    99|                     Additional ``**kwargs`` to pass to requests.post
   100|         Returns:
   101|             The string generated by the model.
   102|         Example:
   103|             .. code-block:: python
   104|                 response = oci_md("Tell me a joke.")
   105|         """
   106|         requests_kwargs = kwargs.pop("requests_kwargs", {})
   107|         params = self._invocation_params(stop, **kwargs)
   108|         body = self._construct_json_body(prompt, params)
   109|         logger.info(f"LLM API Request:\n{prompt}")
   110|         response = self._send_request(
   111|             data=body, endpoint=self.endpoint, **requests_kwargs
   112|         )
   113|         completion = self._process_response(response)
   114|         logger.info(f"LLM API Completion:\n{completion}")
   115|         return completion
   116|     def _send_request(
   117|         self,
   118|         data: Any,
   119|         endpoint: str,
   120|         header: Optional[dict] = {},
   121|         **kwargs: Any,
   122|     ) -> Dict:
   123|         """Sends request to the oci data science model deployment endpoint.
   124|         Args:
   125|             data (Json serializable):
   126|                 data need to be sent to the endpoint.
   127|             endpoint (str):
   128|                 The model HTTP endpoint.
   129|             header (dict, optional):
   130|                 A dictionary of HTTP headers to send to the specified url.
   131|                 Defaults to {}.
   132|             kwargs:
   133|                 Additional ``**kwargs`` to pass to requests.post.
   134|         Raises:
   135|             Exception:
   136|                 Raise when invoking fails.
   137|         Returns:
   138|             A JSON representation of a requests.Response object.
   139|         """
   140|         if not header:
   141|             header = {}
   142|         header["Content-Type"] = (
   143|             header.pop("content_type", DEFAULT_CONTENT_TYPE_JSON)
   144|             or DEFAULT_CONTENT_TYPE_JSON
   145|         )
   146|         request_kwargs = {"json": data}
   147|         request_kwargs["headers"] = header
   148|         timeout = kwargs.pop("timeout", DEFAULT_TIME_OUT)
   149|         attempts = 0
   150|         while attempts < 2:
   151|             request_kwargs["auth"] = self.auth.get("signer")
   152|             response = requests.post(
   153|                 endpoint, timeout=timeout, **request_kwargs, **kwargs
   154|             )
   155|             if response.status_code == 401:
   156|                 self._refresh_signer()
   157|                 attempts += 1
   158|                 continue
   159|             break
   160|         try:
   161|             response.raise_for_status()
   162|             response_json = response.json()
   163|         except Exception:
   164|             logger.error(
   165|                 "DEBUG INFO: request_kwargs=%s, status_code=%s, content=%s",
   166|                 request_kwargs,
   167|                 response.status_code,
   168|                 response.content,
   169|             )
   170|             raise
   171|         return response_json
   172|     def _refresh_signer(self) -> None:
   173|         if self.auth.get("signer", None) and hasattr(
   174|             self.auth["signer"], "refresh_security_token"
   175|         ):
   176|             self.auth["signer"].refresh_security_token()
   177| class OCIModelDeploymentTGI(OCIModelDeploymentLLM):
   178|     """OCI Data Science Model Deployment TGI Endpoint.
   179|     To use, you must provide the model HTTP endpoint from your deployed
   180|     model, e.g. https://<MD_OCID>/predict.
   181|     To authenticate, `oracle-ads` has been used to automatically load
   182|     credentials: https://accelerated-data-science.readthedocs.io/en/latest/user_guide/cli/authentication.html
   183|     Make sure to have the required policies to access the OCI Data
   184|     Science Model Deployment endpoint. See:
   185|     https://docs.oracle.com/en-us/iaas/data-science/using/model-dep-policies-auth.htm#model_dep_policies_auth__predict-endpoint
   186|     Example:
   187|         .. code-block:: python
   188|             from langchain.llms import ModelDeploymentTGI
   189|             oci_md = ModelDeploymentTGI(endpoint="https://<MD_OCID>/predict")
   190|     """
   191|     do_sample: bool = True
   192|     """If set to True, this parameter enables decoding strategies such as
   193|     multi-nominal sampling, beam-search multi-nominal sampling, Top-K
   194|     sampling and Top-p sampling.
   195|     """
   196|     watermark = True
   197|     """Watermarking with `A Watermark for Large Language Models <https://arxiv.org/abs/2301.10226>`_.
   198|     Defaults to True."""
   199|     return_full_text = False
   200|     """Whether to prepend the prompt to the generated text. Defaults to False."""
   201|     @property
   202|     def _llm_type(self) -> str:
   203|         """Return type of llm."""
   204|         return "oci_model_deployment_tgi_endpoint"
   205|     @property
   206|     def _default_params(self) -> Dict[str, Any]:
   207|         """Get the default parameters for invoking OCI model deployment TGI endpoint."""
   208|         return {
   209|             "best_of": self.best_of,
   210|             "max_new_tokens": self.max_tokens,
   211|             "temperature": self.temperature,
   212|             "top_k": self.k
   213|             if self.k > 0
   214|             else None,  # `top_k` must be strictly positive'
   215|             "top_p": self.p,
   216|             "do_sample": self.do_sample,
   217|             "return_full_text": self.return_full_text,
   218|             "watermark": self.watermark,
   219|         }
   220|     def _construct_json_body(self, prompt: str, params: dict) -> dict:
   221|         return {
   222|             "inputs": prompt,
   223|             "parameters": params,
   224|         }
   225|     def _process_response(self, response_json: dict) -> str:
   226|         return str(response_json.get("generated_text", response_json)) + "\n"
   227| class OCIModelDeploymentVLLM(OCIModelDeploymentLLM):
   228|     """VLLM deployed on OCI Data Science Model Deployment
   229|     To use, you must provide the model HTTP endpoint from your deployed
   230|     model, e.g. https://<MD_OCID>/predict.
   231|     To authenticate, `oracle-ads` has been used to automatically load
   232|     credentials: https://accelerated-data-science.readthedocs.io/en/latest/user_guide/cli/authentication.html
   233|     Make sure to have the required policies to access the OCI Data
   234|     Science Model Deployment endpoint. See:
   235|     https://docs.oracle.com/en-us/iaas/data-science/using/model-dep-policies-auth.htm#model_dep_policies_auth__predict-endpoint
   236|     Example:
   237|         .. code-block:: python
   238|             from langchain.llms import OCIModelDeploymentVLLM
   239|             oci_md = OCIModelDeploymentVLLM(
   240|                 endpoint="https://<MD_OCID>/predict",
   241|                 model="mymodel"
   242|             )
   243|     """
   244|     model: str
   245|     """The name of the model."""
   246|     n: int = 1
   247|     """Number of output sequences to return for the given prompt."""
   248|     k: int = -1
   249|     """Number of most likely tokens to consider at each step."""
   250|     frequency_penalty: float = 0.0
   251|     """Penalizes repeated tokens according to frequency. Between 0 and 1."""
   252|     presence_penalty: float = 0.0
   253|     """Penalizes repeated tokens. Between 0 and 1."""
   254|     use_beam_search: bool = False
   255|     """Whether to use beam search instead of sampling."""
   256|     ignore_eos: bool = False
   257|     """Whether to ignore the EOS token and continue generating tokens after
   258|     the EOS token is generated."""
   259|     logprobs: Optional[int] = None
   260|     """Number of log probabilities to return per output token."""
   261|     @property
   262|     def _llm_type(self) -> str:
   263|         """Return type of llm."""
   264|         return "oci_model_deployment_vllm_endpoint"
   265|     @property
   266|     def _default_params(self) -> Dict[str, Any]:
   267|         """Get the default parameters for calling vllm."""
   268|         return {
   269|             "best_of": self.best_of,
   270|             "frequency_penalty": self.frequency_penalty,
   271|             "ignore_eos": self.ignore_eos,
   272|             "logprobs": self.logprobs,
   273|             "max_tokens": self.max_tokens,
   274|             "model": self.model,
   275|             "n": self.n,
   276|             "presence_penalty": self.presence_penalty,
   277|             "stop": self.stop,
   278|             "temperature": self.temperature,
   279|             "top_k": self.k,
   280|             "top_p": self.p,
   281|             "use_beam_search": self.use_beam_search,
   282|         }
   283|     def _construct_json_body(self, prompt: str, params: dict) -> dict:
   284|         return {
   285|             "prompt": prompt,
   286|             **params,
   287|         }
   288|     def _process_response(self, response_json: dict) -> str:
   289|         return response_json["choices"][0]["text"]


# ====================================================================
# FILE: libs/community/langchain_community/llms/ollama.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| import json
     2| from typing import Any, AsyncIterator, Dict, Iterator, List, Mapping, Optional
     3| import aiohttp
     4| import requests
     5| from langchain_core.callbacks import (
     6|     AsyncCallbackManagerForLLMRun,
     7|     CallbackManagerForLLMRun,
     8| )
     9| from langchain_core.language_models import BaseLanguageModel
    10| from langchain_core.language_models.llms import BaseLLM
    11| from langchain_core.outputs import GenerationChunk, LLMResult
    12| from langchain_core.pydantic_v1 import Extra
    13| def _stream_response_to_generation_chunk(
    14|     stream_response: str,
    15| ) -> GenerationChunk:
    16|     """Convert a stream response to a generation chunk."""
    17|     parsed_response = json.loads(stream_response)
    18|     generation_info = parsed_response if parsed_response.get("done") is True else None
    19|     return GenerationChunk(
    20|         text=parsed_response.get("response", ""), generation_info=generation_info
    21|     )
    22| class OllamaEndpointNotFoundError(Exception):
    23|     """Raised when the Ollama endpoint is not found."""
    24| class _OllamaCommon(BaseLanguageModel):
    25|     base_url: str = "http://localhost:11434"
    26|     """Base url the model is hosted under."""
    27|     model: str = "llama2"
    28|     """Model name to use."""

# --- HUNK 2: Lines 106-160 ---
   106|             "template": self.template,
   107|         }
   108|     @property
   109|     def _identifying_params(self) -> Mapping[str, Any]:
   110|         """Get the identifying parameters."""
   111|         return {**{"model": self.model, "format": self.format}, **self._default_params}
   112|     def _create_generate_stream(
   113|         self,
   114|         prompt: str,
   115|         stop: Optional[List[str]] = None,
   116|         images: Optional[List[str]] = None,
   117|         **kwargs: Any,
   118|     ) -> Iterator[str]:
   119|         payload = {"prompt": prompt, "images": images}
   120|         yield from self._create_stream(
   121|             payload=payload,
   122|             stop=stop,
   123|             api_url=f"{self.base_url}/api/generate/",
   124|             **kwargs,
   125|         )
   126|     async def _acreate_generate_stream(
   127|         self,
   128|         prompt: str,
   129|         stop: Optional[List[str]] = None,
   130|         images: Optional[List[str]] = None,
   131|         **kwargs: Any,
   132|     ) -> AsyncIterator[str]:
   133|         payload = {"prompt": prompt, "images": images}
   134|         async for item in self._acreate_stream(
   135|             payload=payload,
   136|             stop=stop,
   137|             api_url=f"{self.base_url}/api/generate/",
   138|             **kwargs,
   139|         ):
   140|             yield item
   141|     def _create_stream(
   142|         self,
   143|         api_url: str,
   144|         payload: Any,
   145|         stop: Optional[List[str]] = None,
   146|         **kwargs: Any,
   147|     ) -> Iterator[str]:
   148|         if self.stop is not None and stop is not None:
   149|             raise ValueError("`stop` found in both the input and default params.")
   150|         elif self.stop is not None:
   151|             stop = self.stop
   152|         elif stop is None:
   153|             stop = []
   154|         params = self._default_params
   155|         if "model" in kwargs:
   156|             params["model"] = kwargs["model"]
   157|         if "options" in kwargs:
   158|             params["options"] = kwargs["options"]
   159|         else:
   160|             params["options"] = {

# --- HUNK 3: Lines 164-308 ---
   164|             }
   165|         if payload.get("messages"):
   166|             request_payload = {"messages": payload.get("messages", []), **params}
   167|         else:
   168|             request_payload = {
   169|                 "prompt": payload.get("prompt"),
   170|                 "images": payload.get("images", []),
   171|                 **params,
   172|             }
   173|         response = requests.post(
   174|             url=api_url,
   175|             headers={"Content-Type": "application/json"},
   176|             json=request_payload,
   177|             stream=True,
   178|             timeout=self.timeout,
   179|         )
   180|         response.encoding = "utf-8"
   181|         if response.status_code != 200:
   182|             if response.status_code == 404:
   183|                 raise OllamaEndpointNotFoundError(
   184|                     "Ollama call failed with status code 404. "
   185|                     "Maybe your model is not found "
   186|                     f"and you should pull the model with `ollama pull {self.model}`."
   187|                 )
   188|             else:
   189|                 optional_detail = response.json().get("error")
   190|                 raise ValueError(
   191|                     f"Ollama call failed with status code {response.status_code}."
   192|                     f" Details: {optional_detail}"
   193|                 )
   194|         return response.iter_lines(decode_unicode=True)
   195|     async def _acreate_stream(
   196|         self,
   197|         api_url: str,
   198|         payload: Any,
   199|         stop: Optional[List[str]] = None,
   200|         **kwargs: Any,
   201|     ) -> AsyncIterator[str]:
   202|         if self.stop is not None and stop is not None:
   203|             raise ValueError("`stop` found in both the input and default params.")
   204|         elif self.stop is not None:
   205|             stop = self.stop
   206|         elif stop is None:
   207|             stop = []
   208|         params = self._default_params
   209|         if "model" in kwargs:
   210|             params["model"] = kwargs["model"]
   211|         if "options" in kwargs:
   212|             params["options"] = kwargs["options"]
   213|         else:
   214|             params["options"] = {
   215|                 **params["options"],
   216|                 "stop": stop,
   217|                 **kwargs,
   218|             }
   219|         if payload.get("messages"):
   220|             request_payload = {"messages": payload.get("messages", []), **params}
   221|         else:
   222|             request_payload = {
   223|                 "prompt": payload.get("prompt"),
   224|                 "images": payload.get("images", []),
   225|                 **params,
   226|             }
   227|         async with aiohttp.ClientSession() as session:
   228|             async with session.post(
   229|                 url=api_url,
   230|                 headers={"Content-Type": "application/json"},
   231|                 json=request_payload,
   232|                 timeout=self.timeout,
   233|             ) as response:
   234|                 if response.status != 200:
   235|                     if response.status == 404:
   236|                         raise OllamaEndpointNotFoundError(
   237|                             "Ollama call failed with status code 404."
   238|                         )
   239|                     else:
   240|                         optional_detail = await response.json().get("error")
   241|                         raise ValueError(
   242|                             f"Ollama call failed with status code {response.status}."
   243|                             f" Details: {optional_detail}"
   244|                         )
   245|                 async for line in response.content:
   246|                     yield line.decode("utf-8")
   247|     def _stream_with_aggregation(
   248|         self,
   249|         prompt: str,
   250|         stop: Optional[List[str]] = None,
   251|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   252|         verbose: bool = False,
   253|         **kwargs: Any,
   254|     ) -> GenerationChunk:
   255|         final_chunk: Optional[GenerationChunk] = None
   256|         for stream_resp in self._create_generate_stream(prompt, stop, **kwargs):
   257|             if stream_resp:
   258|                 chunk = _stream_response_to_generation_chunk(stream_resp)
   259|                 if final_chunk is None:
   260|                     final_chunk = chunk
   261|                 else:
   262|                     final_chunk += chunk
   263|                 if run_manager:
   264|                     run_manager.on_llm_new_token(
   265|                         chunk.text,
   266|                         verbose=verbose,
   267|                     )
   268|         if final_chunk is None:
   269|             raise ValueError("No data received from Ollama stream.")
   270|         return final_chunk
   271|     async def _astream_with_aggregation(
   272|         self,
   273|         prompt: str,
   274|         stop: Optional[List[str]] = None,
   275|         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
   276|         verbose: bool = False,
   277|         **kwargs: Any,
   278|     ) -> GenerationChunk:
   279|         final_chunk: Optional[GenerationChunk] = None
   280|         async for stream_resp in self._acreate_generate_stream(prompt, stop, **kwargs):
   281|             if stream_resp:
   282|                 chunk = _stream_response_to_generation_chunk(stream_resp)
   283|                 if final_chunk is None:
   284|                     final_chunk = chunk
   285|                 else:
   286|                     final_chunk += chunk
   287|                 if run_manager:
   288|                     await run_manager.on_llm_new_token(
   289|                         chunk.text,
   290|                         verbose=verbose,
   291|                     )
   292|         if final_chunk is None:
   293|             raise ValueError("No data received from Ollama stream.")
   294|         return final_chunk
   295| class Ollama(BaseLLM, _OllamaCommon):
   296|     """Ollama locally runs large language models.
   297|     To use, follow the instructions at https://ollama.ai/.
   298|     Example:
   299|         .. code-block:: python
   300|             from langchain_community.llms import Ollama
   301|             ollama = Ollama(model="llama2")
   302|     """
   303|     class Config:
   304|         """Configuration for this pydantic object."""
   305|         extra = Extra.forbid
   306|     @property
   307|     def _llm_type(self) -> str:
   308|         """Return type of llm."""

# --- HUNK 4: Lines 320-401 ---
   320|             prompt: The prompt to pass into the model.
   321|             stop: Optional list of stop words to use when generating.
   322|         Returns:
   323|             The string generated by the model.
   324|         Example:
   325|             .. code-block:: python
   326|                 response = ollama("Tell me a joke.")
   327|         """
   328|         generations = []
   329|         for prompt in prompts:
   330|             final_chunk = super()._stream_with_aggregation(
   331|                 prompt,
   332|                 stop=stop,
   333|                 images=images,
   334|                 run_manager=run_manager,
   335|                 verbose=self.verbose,
   336|                 **kwargs,
   337|             )
   338|             generations.append([final_chunk])
   339|         return LLMResult(generations=generations)
   340|     async def _agenerate(
   341|         self,
   342|         prompts: List[str],
   343|         stop: Optional[List[str]] = None,
   344|         images: Optional[List[str]] = None,
   345|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   346|         **kwargs: Any,
   347|     ) -> LLMResult:
   348|         """Call out to Ollama's generate endpoint.
   349|         Args:
   350|             prompt: The prompt to pass into the model.
   351|             stop: Optional list of stop words to use when generating.
   352|         Returns:
   353|             The string generated by the model.
   354|         Example:
   355|             .. code-block:: python
   356|                 response = ollama("Tell me a joke.")
   357|         """
   358|         generations = []
   359|         for prompt in prompts:
   360|             final_chunk = await super()._astream_with_aggregation(
   361|                 prompt,
   362|                 stop=stop,
   363|                 images=images,
   364|                 run_manager=run_manager,
   365|                 verbose=self.verbose,
   366|                 **kwargs,
   367|             )
   368|             generations.append([final_chunk])
   369|         return LLMResult(generations=generations)
   370|     def _stream(
   371|         self,
   372|         prompt: str,
   373|         stop: Optional[List[str]] = None,
   374|         run_manager: Optional[CallbackManagerForLLMRun] = None,
   375|         **kwargs: Any,
   376|     ) -> Iterator[GenerationChunk]:
   377|         for stream_resp in self._create_stream(prompt, stop, **kwargs):
   378|             if stream_resp:
   379|                 chunk = _stream_response_to_generation_chunk(stream_resp)
   380|                 yield chunk
   381|                 if run_manager:
   382|                     run_manager.on_llm_new_token(
   383|                         chunk.text,
   384|                         verbose=self.verbose,
   385|                     )
   386|     async def _astream(
   387|         self,
   388|         prompt: str,
   389|         stop: Optional[List[str]] = None,
   390|         run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
   391|         **kwargs: Any,
   392|     ) -> AsyncIterator[GenerationChunk]:
   393|         async for stream_resp in self._acreate_stream(prompt, stop, **kwargs):
   394|             if stream_resp:
   395|                 chunk = _stream_response_to_generation_chunk(stream_resp)
   396|                 yield chunk
   397|                 if run_manager:
   398|                     await run_manager.on_llm_new_token(
   399|                         chunk.text,
   400|                         verbose=self.verbose,
   401|                     )


# ====================================================================
# FILE: libs/community/langchain_community/llms/petals.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-97 ---
     1| import logging
     2| from typing import Any, Dict, List, Mapping, Optional
     3| from langchain_core.callbacks import CallbackManagerForLLMRun
     4| from langchain_core.language_models.llms import LLM
     5| from langchain_core.pydantic_v1 import Extra, Field, SecretStr, root_validator
     6| from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
     7| from langchain_community.llms.utils import enforce_stop_tokens
     8| logger = logging.getLogger(__name__)
     9| class Petals(LLM):
    10|     """Petals Bloom models.
    11|     To use, you should have the ``petals`` python package installed, and the
    12|     environment variable ``HUGGINGFACE_API_KEY`` set with your API key.
    13|     Any parameters that are valid to be passed to the call can be passed
    14|     in, even if not explicitly saved on this class.
    15|     Example:
    16|         .. code-block:: python
    17|             from langchain_community.llms import petals
    18|             petals = Petals()
    19|     """
    20|     client: Any
    21|     """The client to use for the API calls."""
    22|     tokenizer: Any
    23|     """The tokenizer to use for the API calls."""
    24|     model_name: str = "bigscience/bloom-petals"
    25|     """The model to use."""
    26|     temperature: float = 0.7
    27|     """What sampling temperature to use"""
    28|     max_new_tokens: int = 256
    29|     """The maximum number of new tokens to generate in the completion."""
    30|     top_p: float = 0.9
    31|     """The cumulative probability for top-p sampling."""
    32|     top_k: Optional[int] = None
    33|     """The number of highest probability vocabulary tokens
    34|     to keep for top-k-filtering."""
    35|     do_sample: bool = True
    36|     """Whether or not to use sampling; use greedy decoding otherwise."""
    37|     max_length: Optional[int] = None
    38|     """The maximum length of the sequence to be generated."""
    39|     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    40|     """Holds any model parameters valid for `create` call
    41|     not explicitly specified."""
    42|     huggingface_api_key: Optional[SecretStr] = None
    43|     class Config:
    44|         """Configuration for this pydantic config."""
    45|         extra = Extra.forbid
    46|     @root_validator(pre=True)
    47|     def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:
    48|         """Build extra kwargs from additional params that were passed in."""
    49|         all_required_field_names = {field.alias for field in cls.__fields__.values()}
    50|         extra = values.get("model_kwargs", {})
    51|         for field_name in list(values):
    52|             if field_name not in all_required_field_names:
    53|                 if field_name in extra:
    54|                     raise ValueError(f"Found {field_name} supplied twice.")
    55|                 logger.warning(
    56|                     f"""WARNING! {field_name} is not default parameter.
    57|                     {field_name} was transferred to model_kwargs.
    58|                     Please confirm that {field_name} is what you intended."""
    59|                 )
    60|                 extra[field_name] = values.pop(field_name)
    61|         values["model_kwargs"] = extra
    62|         return values
    63|     @root_validator()
    64|     def validate_environment(cls, values: Dict) -> Dict:
    65|         """Validate that api key and python package exists in environment."""
    66|         huggingface_api_key = convert_to_secret_str(
    67|             get_from_dict_or_env(values, "huggingface_api_key", "HUGGINGFACE_API_KEY")
    68|         )
    69|         try:
    70|             from petals import AutoDistributedModelForCausalLM
    71|             from transformers import AutoTokenizer
    72|             model_name = values["model_name"]
    73|             values["tokenizer"] = AutoTokenizer.from_pretrained(model_name)
    74|             values["client"] = AutoDistributedModelForCausalLM.from_pretrained(
    75|                 model_name
    76|             )
    77|             values["huggingface_api_key"] = huggingface_api_key.get_secret_value()
    78|         except ImportError:
    79|             raise ImportError(
    80|                 "Could not import transformers or petals python package."
    81|                 "Please install with `pip install -U transformers petals`."
    82|             )
    83|         return values
    84|     @property
    85|     def _default_params(self) -> Dict[str, Any]:
    86|         """Get the default parameters for calling Petals API."""
    87|         normal_params = {
    88|             "temperature": self.temperature,
    89|             "max_new_tokens": self.max_new_tokens,
    90|             "top_p": self.top_p,
    91|             "top_k": self.top_k,
    92|             "do_sample": self.do_sample,
    93|             "max_length": self.max_length,
    94|         }
    95|         return {**normal_params, **self.model_kwargs}
    96|     @property
    97|     def _identifying_params(self) -> Mapping[str, Any]:


# ====================================================================
# FILE: libs/community/langchain_community/llms/pipelineai.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-98 ---
     1| import logging
     2| from typing import Any, Dict, List, Mapping, Optional
     3| from langchain_core.callbacks import CallbackManagerForLLMRun
     4| from langchain_core.language_models.llms import LLM
     5| from langchain_core.pydantic_v1 import (
     6|     BaseModel,
     7|     Extra,
     8|     Field,
     9|     SecretStr,
    10|     root_validator,
    11| )
    12| from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
    13| from langchain_community.llms.utils import enforce_stop_tokens
    14| logger = logging.getLogger(__name__)
    15| class PipelineAI(LLM, BaseModel):
    16|     """PipelineAI large language models.
    17|     To use, you should have the ``pipeline-ai`` python package installed,
    18|     and the environment variable ``PIPELINE_API_KEY`` set with your API key.
    19|     Any parameters that are valid to be passed to the call can be passed
    20|     in, even if not explicitly saved on this class.
    21|     Example:
    22|         .. code-block:: python
    23|             from langchain_community.llms import PipelineAI
    24|             pipeline = PipelineAI(pipeline_key="")
    25|     """
    26|     pipeline_key: str = ""
    27|     """The id or tag of the target pipeline"""
    28|     pipeline_kwargs: Dict[str, Any] = Field(default_factory=dict)
    29|     """Holds any pipeline parameters valid for `create` call not
    30|     explicitly specified."""
    31|     pipeline_api_key: Optional[SecretStr] = None
    32|     class Config:
    33|         """Configuration for this pydantic config."""
    34|         extra = Extra.forbid
    35|     @root_validator(pre=True)
    36|     def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:
    37|         """Build extra kwargs from additional params that were passed in."""
    38|         all_required_field_names = {field.alias for field in cls.__fields__.values()}
    39|         extra = values.get("pipeline_kwargs", {})
    40|         for field_name in list(values):
    41|             if field_name not in all_required_field_names:
    42|                 if field_name in extra:
    43|                     raise ValueError(f"Found {field_name} supplied twice.")
    44|                 logger.warning(
    45|                     f"""{field_name} was transferred to pipeline_kwargs.
    46|                     Please confirm that {field_name} is what you intended."""
    47|                 )
    48|                 extra[field_name] = values.pop(field_name)
    49|         values["pipeline_kwargs"] = extra
    50|         return values
    51|     @root_validator()
    52|     def validate_environment(cls, values: Dict) -> Dict:
    53|         """Validate that api key and python package exists in environment."""
    54|         pipeline_api_key = convert_to_secret_str(
    55|             get_from_dict_or_env(values, "pipeline_api_key", "PIPELINE_API_KEY")
    56|         )
    57|         values["pipeline_api_key"] = pipeline_api_key
    58|         return values
    59|     @property
    60|     def _identifying_params(self) -> Mapping[str, Any]:
    61|         """Get the identifying parameters."""
    62|         return {
    63|             **{"pipeline_key": self.pipeline_key},
    64|             **{"pipeline_kwargs": self.pipeline_kwargs},
    65|         }
    66|     @property
    67|     def _llm_type(self) -> str:
    68|         """Return type of llm."""
    69|         return "pipeline_ai"
    70|     def _call(
    71|         self,
    72|         prompt: str,
    73|         stop: Optional[List[str]] = None,
    74|         run_manager: Optional[CallbackManagerForLLMRun] = None,
    75|         **kwargs: Any,
    76|     ) -> str:
    77|         """Call to Pipeline Cloud endpoint."""
    78|         try:
    79|             from pipeline import PipelineCloud
    80|         except ImportError:
    81|             raise ImportError(
    82|                 "Could not import pipeline-ai python package. "
    83|                 "Please install it with `pip install pipeline-ai`."
    84|             )
    85|         client = PipelineCloud(token=self.pipeline_api_key.get_secret_value())
    86|         params = self.pipeline_kwargs or {}
    87|         params = {**params, **kwargs}
    88|         run = client.run_pipeline(self.pipeline_key, [prompt, params])
    89|         try:
    90|             text = run.result_preview[0][0]
    91|         except AttributeError:
    92|             raise AttributeError(
    93|                 f"A pipeline run should have a `result_preview` attribute."
    94|                 f"Run was: {run}"
    95|             )
    96|         if stop is not None:
    97|             text = enforce_stop_tokens(text, stop)
    98|         return text


# ====================================================================
# FILE: libs/community/langchain_community/llms/predibase.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-40 ---
     1| from typing import Any, Dict, List, Mapping, Optional
     2| from langchain_core.callbacks import CallbackManagerForLLMRun
     3| from langchain_core.language_models.llms import LLM
     4| from langchain_core.pydantic_v1 import Field, SecretStr
     5| class Predibase(LLM):
     6|     """Use your Predibase models with Langchain.
     7|     To use, you should have the ``predibase`` python package installed,
     8|     and have your Predibase API key.
     9|     """
    10|     model: str
    11|     predibase_api_key: SecretStr
    12|     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    13|     @property
    14|     def _llm_type(self) -> str:
    15|         return "predibase"
    16|     def _call(
    17|         self,
    18|         prompt: str,
    19|         stop: Optional[List[str]] = None,
    20|         run_manager: Optional[CallbackManagerForLLMRun] = None,
    21|         **kwargs: Any,
    22|     ) -> str:
    23|         try:
    24|             from predibase import PredibaseClient
    25|             pc = PredibaseClient(token=self.predibase_api_key.get_secret_value())
    26|         except ImportError as e:
    27|             raise ImportError(
    28|                 "Could not import Predibase Python package. "
    29|                 "Please install it with `pip install predibase`."
    30|             ) from e
    31|         except ValueError as e:
    32|             raise ValueError("Your API key is not correct. Please try again") from e
    33|         results = pc.prompt(prompt, model_name=self.model)
    34|         return results[0].response
    35|     @property
    36|     def _identifying_params(self) -> Mapping[str, Any]:
    37|         """Get the identifying parameters."""
    38|         return {
    39|             **{"model_kwargs": self.model_kwargs},
    40|         }


# ====================================================================
# FILE: libs/community/langchain_community/llms/stochasticai.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-112 ---
     1| import logging
     2| import time
     3| from typing import Any, Dict, List, Mapping, Optional
     4| import requests
     5| from langchain_core.callbacks import CallbackManagerForLLMRun
     6| from langchain_core.language_models.llms import LLM
     7| from langchain_core.pydantic_v1 import Extra, Field, SecretStr, root_validator
     8| from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
     9| from langchain_community.llms.utils import enforce_stop_tokens
    10| logger = logging.getLogger(__name__)
    11| class StochasticAI(LLM):
    12|     """StochasticAI large language models.
    13|     To use, you should have the environment variable ``STOCHASTICAI_API_KEY``
    14|     set with your API key.
    15|     Example:
    16|         .. code-block:: python
    17|             from langchain_community.llms import StochasticAI
    18|             stochasticai = StochasticAI(api_url="")
    19|     """
    20|     api_url: str = ""
    21|     """Model name to use."""
    22|     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    23|     """Holds any model parameters valid for `create` call not
    24|     explicitly specified."""
    25|     stochasticai_api_key: Optional[SecretStr] = None
    26|     class Config:
    27|         """Configuration for this pydantic object."""
    28|         extra = Extra.forbid
    29|     @root_validator(pre=True)
    30|     def build_extra(cls, values: Dict[str, Any]) -> Dict[str, Any]:
    31|         """Build extra kwargs from additional params that were passed in."""
    32|         all_required_field_names = {field.alias for field in cls.__fields__.values()}
    33|         extra = values.get("model_kwargs", {})
    34|         for field_name in list(values):
    35|             if field_name not in all_required_field_names:
    36|                 if field_name in extra:
    37|                     raise ValueError(f"Found {field_name} supplied twice.")
    38|                 logger.warning(
    39|                     f"""{field_name} was transferred to model_kwargs.
    40|                     Please confirm that {field_name} is what you intended."""
    41|                 )
    42|                 extra[field_name] = values.pop(field_name)
    43|         values["model_kwargs"] = extra
    44|         return values
    45|     @root_validator()
    46|     def validate_environment(cls, values: Dict) -> Dict:
    47|         """Validate that api key exists in environment."""
    48|         stochasticai_api_key = convert_to_secret_str(
    49|             get_from_dict_or_env(values, "stochasticai_api_key", "STOCHASTICAI_API_KEY")
    50|         )
    51|         values["stochasticai_api_key"] = stochasticai_api_key
    52|         return values
    53|     @property
    54|     def _identifying_params(self) -> Mapping[str, Any]:
    55|         """Get the identifying parameters."""
    56|         return {
    57|             **{"endpoint_url": self.api_url},
    58|             **{"model_kwargs": self.model_kwargs},
    59|         }
    60|     @property
    61|     def _llm_type(self) -> str:
    62|         """Return type of llm."""
    63|         return "stochasticai"
    64|     def _call(
    65|         self,
    66|         prompt: str,
    67|         stop: Optional[List[str]] = None,
    68|         run_manager: Optional[CallbackManagerForLLMRun] = None,
    69|         **kwargs: Any,
    70|     ) -> str:
    71|         """Call out to StochasticAI's complete endpoint.
    72|         Args:
    73|             prompt: The prompt to pass into the model.
    74|             stop: Optional list of stop words to use when generating.
    75|         Returns:
    76|             The string generated by the model.
    77|         Example:
    78|             .. code-block:: python
    79|                 response = StochasticAI("Tell me a joke.")
    80|         """
    81|         params = self.model_kwargs or {}
    82|         params = {**params, **kwargs}
    83|         response_post = requests.post(
    84|             url=self.api_url,
    85|             json={"prompt": prompt, "params": params},
    86|             headers={
    87|                 "apiKey": f"{self.stochasticai_api_key.get_secret_value()}",
    88|                 "Accept": "application/json",
    89|                 "Content-Type": "application/json",
    90|             },
    91|         )
    92|         response_post.raise_for_status()
    93|         response_post_json = response_post.json()
    94|         completed = False
    95|         while not completed:
    96|             response_get = requests.get(
    97|                 url=response_post_json["data"]["responseUrl"],
    98|                 headers={
    99|                     "apiKey": f"{self.stochasticai_api_key.get_secret_value()}",
   100|                     "Accept": "application/json",
   101|                     "Content-Type": "application/json",
   102|                 },
   103|             )
   104|             response_get.raise_for_status()
   105|             response_get_json = response_get.json()["data"]
   106|             text = response_get_json.get("completion")
   107|             completed = text is not None
   108|             time.sleep(0.5)
   109|         text = text[0]
   110|         if stop is not None:
   111|             text = enforce_stop_tokens(text, stop)
   112|         return text


# ====================================================================
# FILE: libs/community/langchain_community/llms/volcengine_maas.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-81 ---
     1| from __future__ import annotations
     2| from typing import Any, Dict, Iterator, List, Optional
     3| from langchain_core.callbacks import CallbackManagerForLLMRun
     4| from langchain_core.language_models.llms import LLM
     5| from langchain_core.outputs import GenerationChunk
     6| from langchain_core.pydantic_v1 import BaseModel, Field, SecretStr, root_validator
     7| from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env
     8| class VolcEngineMaasBase(BaseModel):
     9|     """Base class for VolcEngineMaas models."""
    10|     client: Any
    11|     volc_engine_maas_ak: Optional[SecretStr] = None
    12|     """access key for volc engine"""
    13|     volc_engine_maas_sk: Optional[SecretStr] = None
    14|     """secret key for volc engine"""
    15|     endpoint: Optional[str] = "maas-api.ml-platform-cn-beijing.volces.com"
    16|     """Endpoint of the VolcEngineMaas LLM."""
    17|     region: Optional[str] = "Region"
    18|     """Region of the VolcEngineMaas LLM."""
    19|     model: str = "skylark-lite-public"
    20|     """Model name. you could check this model details here 
    21|     https://www.volcengine.com/docs/82379/1133187
    22|     and you could choose other models by change this field"""
    23|     model_version: Optional[str] = None
    24|     """Model version. Only used in moonshot large language model. 
    25|     you could check details here https://www.volcengine.com/docs/82379/1158281"""
    26|     top_p: Optional[float] = 0.8
    27|     """Total probability mass of tokens to consider at each step."""
    28|     temperature: Optional[float] = 0.95
    29|     """A non-negative float that tunes the degree of randomness in generation."""
    30|     model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    31|     """model special arguments, you could check detail on model page"""
    32|     streaming: bool = False
    33|     """Whether to stream the results."""
    34|     connect_timeout: Optional[int] = 60
    35|     """Timeout for connect to volc engine maas endpoint. Default is 60 seconds."""
    36|     read_timeout: Optional[int] = 60
    37|     """Timeout for read response from volc engine maas endpoint. 
    38|     Default is 60 seconds."""
    39|     @root_validator()
    40|     def validate_environment(cls, values: Dict) -> Dict:
    41|         volc_engine_maas_ak = convert_to_secret_str(
    42|             get_from_dict_or_env(values, "volc_engine_maas_ak", "VOLC_ACCESSKEY")
    43|         )
    44|         volc_engine_maas_sk = convert_to_secret_str(
    45|             get_from_dict_or_env(values, "volc_engine_maas_sk", "VOLC_SECRETKEY")
    46|         )
    47|         endpoint = values["endpoint"]
    48|         if values["endpoint"] is not None and values["endpoint"] != "":
    49|             endpoint = values["endpoint"]
    50|         try:
    51|             from volcengine.maas import MaasService
    52|             maas = MaasService(
    53|                 endpoint,
    54|                 values["region"],
    55|                 connection_timeout=values["connect_timeout"],
    56|                 socket_timeout=values["read_timeout"],
    57|             )
    58|             maas.set_ak(volc_engine_maas_ak.get_secret_value())
    59|             maas.set_sk(volc_engine_maas_sk.get_secret_value())
    60|             values["volc_engine_maas_ak"] = volc_engine_maas_ak
    61|             values["volc_engine_maas_sk"] = volc_engine_maas_sk
    62|             values["client"] = maas
    63|         except ImportError:
    64|             raise ImportError(
    65|                 "volcengine package not found, please install it with "
    66|                 "`pip install volcengine`"
    67|             )
    68|         return values
    69|     @property
    70|     def _default_params(self) -> Dict[str, Any]:
    71|         """Get the default parameters for calling VolcEngineMaas API."""
    72|         normal_params = {
    73|             "top_p": self.top_p,
    74|             "temperature": self.temperature,
    75|         }
    76|         return {**normal_params, **self.model_kwargs}
    77| class VolcEngineMaasLLM(LLM, VolcEngineMaasBase):
    78|     """volc engine maas hosts a plethora of models.
    79|     You can utilize these models through this class.
    80|     To use, you should have the ``volcengine`` python package installed.
    81|     and set access key and secret key by environment variable or direct pass those to


# ====================================================================
# FILE: libs/community/langchain_community/retrievers/google_vertex_ai_search.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 112-151 ---
   112|             doc_metadata = document_dict.get("struct_data", {})
   113|             doc_metadata["id"] = document_dict["id"]
   114|             if chunk_type not in derived_struct_data:
   115|                 continue
   116|             for chunk in derived_struct_data[chunk_type]:
   117|                 doc_metadata["source"] = derived_struct_data.get("link", "")
   118|                 if chunk_type == "extractive_answers":
   119|                     doc_metadata["source"] += f":{chunk.get('pageNumber', '')}"
   120|                 documents.append(
   121|                     Document(
   122|                         page_content=chunk.get("content", ""), metadata=doc_metadata
   123|                     )
   124|                 )
   125|         return documents
   126|     def _convert_website_search_response(
   127|         self, results: Sequence[SearchResult], chunk_type: str
   128|     ) -> List[Document]:
   129|         """Converts a sequence of search results to a list of LangChain documents."""
   130|         from google.protobuf.json_format import MessageToDict
   131|         documents: List[Document] = []
   132|         for result in results:
   133|             document_dict = MessageToDict(
   134|                 result.document._pb, preserving_proto_field_name=True
   135|             )
   136|             derived_struct_data = document_dict.get("derived_struct_data")
   137|             if not derived_struct_data:
   138|                 continue
   139|             doc_metadata = document_dict.get("struct_data", {})
   140|             doc_metadata["id"] = document_dict["id"]
   141|             doc_metadata["source"] = derived_struct_data.get("link", "")
   142|             if chunk_type not in derived_struct_data:
   143|                 continue
   144|             text_field = "snippet" if chunk_type == "snippets" else "content"
   145|             for chunk in derived_struct_data[chunk_type]:
   146|                 documents.append(
   147|                     Document(
   148|                         page_content=chunk.get(text_field, ""), metadata=doc_metadata
   149|                     )
   150|                 )
   151|         if not documents:


# ====================================================================
# FILE: libs/community/langchain_community/tools/e2b_data_analysis/tool.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-27 ---
     1| from __future__ import annotations
     2| import ast
     3| import json
     4| import os
     5| from io import StringIO
     6| from sys import version_info
     7| from typing import IO, TYPE_CHECKING, Any, Callable, List, Optional, Type, Union
     8| from langchain_core.callbacks import (
     9|     AsyncCallbackManagerForToolRun,
    10|     CallbackManager,
    11|     CallbackManagerForToolRun,
    12| )
    13| from langchain_core.pydantic_v1 import BaseModel, Field, PrivateAttr
    14| from langchain_community.tools import BaseTool, Tool
    15| from langchain_community.tools.e2b_data_analysis.unparse import Unparser
    16| if TYPE_CHECKING:
    17|     from e2b import EnvVars
    18|     from e2b.templates.data_analysis import Artifact
    19| base_description = """Evaluates python code in a sandbox environment. \
    20| The environment is long running and exists across multiple executions. \
    21| You must send the whole script every time and print your outputs. \
    22| Script should be pure python code that can be evaluated. \
    23| It should be in python format NOT markdown. \
    24| The code should NOT be wrapped in backticks. \
    25| All python packages including requests, matplotlib, scipy, numpy, pandas, \
    26| etc are available. Create and display chart using `plt.show()`."""
    27| def _unparse(tree: ast.AST) -> str:

# --- HUNK 2: Lines 144-187 ---
   144|         }
   145|         return json.dumps(out)
   146|     async def _arun(
   147|         self,
   148|         python_code: str,
   149|         run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
   150|     ) -> str:
   151|         raise NotImplementedError("e2b_data_analysis does not support async")
   152|     def run_command(
   153|         self,
   154|         cmd: str,
   155|     ) -> dict:
   156|         """Run shell command in the sandbox."""
   157|         proc = self.session.process.start(cmd)
   158|         output = proc.wait()
   159|         return {
   160|             "stdout": output.stdout,
   161|             "stderr": output.stderr,
   162|             "exit_code": output.exit_code,
   163|         }
   164|     def install_python_packages(self, package_names: Union[str, List[str]]) -> None:
   165|         """Install python packages in the sandbox."""
   166|         self.session.install_python_packages(package_names)
   167|     def install_system_packages(self, package_names: Union[str, List[str]]) -> None:
   168|         """Install system packages (via apt) in the sandbox."""
   169|         self.session.install_system_packages(package_names)
   170|     def download_file(self, remote_path: str) -> bytes:
   171|         """Download file from the sandbox."""
   172|         return self.session.download_file(remote_path)
   173|     def upload_file(self, file: IO, description: str) -> UploadedFile:
   174|         """Upload file to the sandbox.
   175|         The file is uploaded to the '/home/user/<filename>' path."""
   176|         remote_path = self.session.upload_file(file)
   177|         f = UploadedFile(
   178|             name=os.path.basename(file.name),
   179|             remote_path=remote_path,
   180|             description=description,
   181|         )
   182|         self._uploaded_files.append(f)
   183|         self.description = self.description + "\n" + self.uploaded_files_description
   184|         return f
   185|     def remove_uploaded_file(self, uploaded_file: UploadedFile) -> None:
   186|         """Remove uploaded file from the sandbox."""
   187|         self.session.filesystem.remove(uploaded_file.remote_path)


# ====================================================================
# FILE: libs/community/langchain_community/tools/gmail/send_message.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-57 ---
     1| """Send Gmail messages."""
     2| import base64
     3| from email.mime.multipart import MIMEMultipart
     4| from email.mime.text import MIMEText
     5| from typing import Any, Dict, List, Optional, Type, Union
     6| from langchain_core.callbacks import CallbackManagerForToolRun
     7| from langchain_core.pydantic_v1 import BaseModel, Field
     8| from langchain_community.tools.gmail.base import GmailBaseTool
     9| class SendMessageSchema(BaseModel):
    10|     """Input for SendMessageTool."""
    11|     message: str = Field(
    12|         ...,
    13|         description="The message to send.",
    14|     )
    15|     to: Union[str, List[str]] = Field(
    16|         ...,
    17|         description="The list of recipients.",
    18|     )
    19|     subject: str = Field(
    20|         ...,
    21|         description="The subject of the message.",
    22|     )
    23|     cc: Optional[Union[str, List[str]]] = Field(
    24|         None,
    25|         description="The list of CC recipients.",
    26|     )
    27|     bcc: Optional[Union[str, List[str]]] = Field(
    28|         None,
    29|         description="The list of BCC recipients.",
    30|     )
    31| class GmailSendMessage(GmailBaseTool):
    32|     """Tool that sends a message to Gmail."""
    33|     name: str = "send_gmail_message"
    34|     description: str = (
    35|         "Use this tool to send email messages." " The input is the message, recipients"
    36|     )
    37|     args_schema: Type[SendMessageSchema] = SendMessageSchema
    38|     def _prepare_message(
    39|         self,
    40|         message: str,
    41|         to: Union[str, List[str]],
    42|         subject: str,
    43|         cc: Optional[Union[str, List[str]]] = None,
    44|         bcc: Optional[Union[str, List[str]]] = None,
    45|     ) -> Dict[str, Any]:
    46|         """Create a message for an email."""
    47|         mime_message = MIMEMultipart()
    48|         mime_message.attach(MIMEText(message, "html"))
    49|         mime_message["To"] = ", ".join(to if isinstance(to, list) else [to])
    50|         mime_message["Subject"] = subject
    51|         if cc is not None:
    52|             mime_message["Cc"] = ", ".join(cc if isinstance(cc, list) else [cc])
    53|         if bcc is not None:
    54|             mime_message["Bcc"] = ", ".join(bcc if isinstance(bcc, list) else [bcc])
    55|         encoded_message = base64.urlsafe_b64encode(mime_message.as_bytes()).decode()
    56|         return {"raw": encoded_message}
    57|     def _run(


# ====================================================================
# FILE: libs/community/langchain_community/tools/tavily_search/tool.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-79 ---
     1| """Tool for the Tavily search API."""
     2| from typing import Dict, List, Optional, Type, Union
     3| from langchain_core.callbacks import (
     4|     AsyncCallbackManagerForToolRun,
     5|     CallbackManagerForToolRun,
     6| )
     7| from langchain_core.pydantic_v1 import BaseModel, Field
     8| from langchain_core.tools import BaseTool
     9| from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper
    10| class TavilyInput(BaseModel):
    11|     """Input for the Tavily tool."""
    12|     query: str = Field(description="search query to look up")
    13| class TavilySearchResults(BaseTool):
    14|     """Tool that queries the Tavily Search API and gets back json."""
    15|     name: str = "tavily_search_results_json"
    16|     description: str = (
    17|         "A search engine optimized for comprehensive, accurate, and trusted results. "
    18|         "Useful for when you need to answer questions about current events. "
    19|         "Input should be a search query."
    20|     )
    21|     api_wrapper: TavilySearchAPIWrapper = Field(default_factory=TavilySearchAPIWrapper)
    22|     max_results: int = 5
    23|     args_schema: Type[BaseModel] = TavilyInput
    24|     def _run(
    25|         self,
    26|         query: str,
    27|         run_manager: Optional[CallbackManagerForToolRun] = None,
    28|     ) -> Union[List[Dict], str]:
    29|         """Use the tool."""
    30|         try:
    31|             return self.api_wrapper.results(
    32|                 query,
    33|                 self.max_results,
    34|             )
    35|         except Exception as e:
    36|             return repr(e)
    37|     async def _arun(
    38|         self,
    39|         query: str,
    40|         run_manager: Optional[AsyncCallbackManagerForToolRun] = None,
    41|     ) -> Union[List[Dict], str]:
    42|         """Use the tool asynchronously."""
    43|         try:
    44|             return await self.api_wrapper.results_async(
    45|                 query,
    46|                 self.max_results,
    47|             )
    48|         except Exception as e:
    49|             return repr(e)
    50| class TavilyAnswer(BaseTool):
    51|     """Tool that queries the Tavily Search API and gets back an answer."""
    52|     name: str = "tavily_answer"
    53|     description: str = (
    54|         "A search engine optimized for comprehensive, accurate, and trusted results. "
    55|         "Useful for when you need to answer questions about current events. "
    56|         "Input should be a search query. "
    57|         "This returns only the answer - not the original source data."
    58|     )
    59|     api_wrapper: TavilySearchAPIWrapper = Field(default_factory=TavilySearchAPIWrapper)
    60|     args_schema: Type[BaseModel] = TavilyInput
    61|     def _run(
    62|         self,
    63|         query: str,
    64|         run_manager: Optional[CallbackManagerForToolRun] = None,
    65|     ) -> Union[List[Dict], str]:
    66|         """Use the tool."""
    67|         try:
    68|             return self.api_wrapper.raw_results(
    69|                 query,
    70|                 max_results=5,
    71|                 include_answer=True,
    72|                 search_depth="basic",
    73|             )["answer"]
    74|         except Exception as e:
    75|             return repr(e)
    76|     async def _arun(
    77|         self,
    78|         query: str,
    79|         run_manager: Optional[AsyncCallbackManagerForToolRun] = None,


# ====================================================================
# FILE: libs/community/langchain_community/utilities/github.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 49-95 ---
    49|             )
    50|         try:
    51|             with open(github_app_private_key, "r") as f:
    52|                 private_key = f.read()
    53|         except Exception:
    54|             private_key = github_app_private_key
    55|         auth = Auth.AppAuth(
    56|             github_app_id,
    57|             private_key,
    58|         )
    59|         gi = GithubIntegration(auth=auth)
    60|         installation = gi.get_installations()
    61|         if not installation:
    62|             raise ValueError(
    63|                 f"Please make sure to install the created github app with id "
    64|                 f"{github_app_id} on the repo: {github_repository}"
    65|                 "More instructions can be found at "
    66|                 "https://docs.github.com/en/apps/using-"
    67|                 "github-apps/installing-your-own-github-app"
    68|             )
    69|         try:
    70|             installation = installation[0]
    71|         except ValueError as e:
    72|             raise ValueError(
    73|                 "Please make sure to give correct github parameters "
    74|                 f"Error message: {e}"
    75|             )
    76|         g = installation.get_github_for_installation()
    77|         repo = g.get_repo(github_repository)
    78|         github_base_branch = get_from_dict_or_env(
    79|             values,
    80|             "github_base_branch",
    81|             "GITHUB_BASE_BRANCH",
    82|             default=repo.default_branch,
    83|         )
    84|         active_branch = get_from_dict_or_env(
    85|             values,
    86|             "active_branch",
    87|             "ACTIVE_BRANCH",
    88|             default=repo.default_branch,
    89|         )
    90|         values["github"] = g
    91|         values["github_repo_instance"] = repo
    92|         values["github_repository"] = github_repository
    93|         values["github_app_id"] = github_app_id
    94|         values["github_app_private_key"] = github_app_private_key
    95|         values["active_branch"] = active_branch


# ====================================================================
# FILE: libs/community/langchain_community/vectorstores/jaguar.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-68 ---
     1| from __future__ import annotations
     2| import json
     3| import logging
     4| from typing import Any, List, Optional, Tuple
     5| from langchain_core.documents import Document
     6| from langchain_core.embeddings import Embeddings
     7| from langchain_core.vectorstores import VectorStore
     8| logger = logging.getLogger(__name__)
     9| class Jaguar(VectorStore):
    10|     """`Jaguar API` vector store.
    11|     See http://www.jaguardb.com
    12|     See http://github.com/fserv/jaguar-sdk
    13|     Example:
    14|        .. code-block:: python
    15|            from langchain_community.vectorstores.jaguar import Jaguar
    16|            vectorstore = Jaguar(
    17|                pod = 'vdb',
    18|                store = 'mystore',
    19|                vector_index = 'v',
    20|                vector_type = 'cosine_fraction_float',
    21|                vector_dimension = 1536,
    22|                url='http://192.168.8.88:8080/fwww/',
    23|                embedding=openai_model
    24|            )
    25|     """
    26|     def __init__(
    27|         self,
    28|         pod: str,
    29|         store: str,
    30|         vector_index: str,
    31|         vector_type: str,
    32|         vector_dimension: int,
    33|         url: str,
    34|         embedding: Embeddings,
    35|     ):
    36|         self._pod = pod
    37|         self._store = store
    38|         self._vector_index = vector_index
    39|         self._vector_type = vector_type
    40|         self._vector_dimension = vector_dimension
    41|         self._embedding = embedding
    42|         try:
    43|             from jaguardb_http_client.JaguarHttpClient import JaguarHttpClient
    44|         except ImportError:
    45|             raise ValueError(
    46|                 "Could not import jaguardb-http-client python package. "
    47|                 "Please install it with `pip install -U jaguardb-http-client`"
    48|             )
    49|         self._jag = JaguarHttpClient(url)
    50|         self._token = ""
    51|     def login(
    52|         self,
    53|         jaguar_api_key: Optional[str] = "",
    54|     ) -> bool:
    55|         """
    56|         login to jaguardb server with a jaguar_api_key or let self._jag find a key
    57|         Args:
    58|             pod (str):  name of a Pod
    59|             store (str):  name of a vector store
    60|             optional jaguar_api_key (str): API key of user to jaguardb server
    61|         Returns:
    62|             True if successful; False if not successful
    63|         """
    64|         if jaguar_api_key == "":
    65|             jaguar_api_key = self._jag.getApiKey()
    66|         self._jaguar_api_key = jaguar_api_key
    67|         self._token = self._jag.login(jaguar_api_key)
    68|         if self._token == "":


# ====================================================================
# FILE: libs/community/langchain_community/vectorstores/momento_vector_index.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 240-286 ---
   240|         self,
   241|         embedding: List[float],
   242|         k: int = 4,
   243|         **kwargs: Any,
   244|     ) -> List[Tuple[Document, float]]:
   245|         """Search for similar documents to the query vector.
   246|         Args:
   247|             embedding (List[float]): The query vector to search for.
   248|             k (int, optional): The number of results to return. Defaults to 4.
   249|             kwargs (Any): Vector Store specific search parameters. The following are
   250|                 forwarded to the Momento Vector Index:
   251|             - top_k (int, optional): The number of results to return.
   252|         Returns:
   253|             List[Tuple[Document, float]]: A list of tuples of the form
   254|                 (Document, score).
   255|         """
   256|         from momento.requests.vector_index import ALL_METADATA
   257|         from momento.responses.vector_index import Search
   258|         if "top_k" in kwargs:
   259|             k = kwargs["k"]
   260|         filter_expression = kwargs.get("filter_expression", None)
   261|         response = self._client.search(
   262|             self.index_name,
   263|             embedding,
   264|             top_k=k,
   265|             metadata_fields=ALL_METADATA,
   266|             filter_expression=filter_expression,
   267|         )
   268|         if not isinstance(response, Search.Success):
   269|             return []
   270|         results = []
   271|         for hit in response.hits:
   272|             text = cast(str, hit.metadata.pop(self.text_field))
   273|             doc = Document(page_content=text, metadata=hit.metadata)
   274|             pair = (doc, hit.score)
   275|             results.append(pair)
   276|         return results
   277|     def similarity_search_by_vector(
   278|         self, embedding: List[float], k: int = 4, **kwargs: Any
   279|     ) -> List[Document]:
   280|         """Search for similar documents to the query vector.
   281|         Args:
   282|             embedding (List[float]): The query vector to search for.
   283|             k (int, optional): The number of results to return. Defaults to 4.
   284|         Returns:
   285|             List[Document]: A list of documents that are similar to the query.
   286|         """

# --- HUNK 2: Lines 295-341 ---
   295|         fetch_k: int = 20,
   296|         lambda_mult: float = 0.5,
   297|         **kwargs: Any,
   298|     ) -> List[Document]:
   299|         """Return docs selected using the maximal marginal relevance.
   300|         Maximal marginal relevance optimizes for similarity to query AND diversity
   301|         among selected documents.
   302|         Args:
   303|             embedding: Embedding to look up documents similar to.
   304|             k: Number of Documents to return. Defaults to 4.
   305|             fetch_k: Number of Documents to fetch to pass to MMR algorithm.
   306|             lambda_mult: Number between 0 and 1 that determines the degree
   307|                         of diversity among the results with 0 corresponding
   308|                         to maximum diversity and 1 to minimum diversity.
   309|                         Defaults to 0.5.
   310|         Returns:
   311|             List of Documents selected by maximal marginal relevance.
   312|         """
   313|         from momento.requests.vector_index import ALL_METADATA
   314|         from momento.responses.vector_index import SearchAndFetchVectors
   315|         filter_expression = kwargs.get("filter_expression", None)
   316|         response = self._client.search_and_fetch_vectors(
   317|             self.index_name,
   318|             embedding,
   319|             top_k=fetch_k,
   320|             metadata_fields=ALL_METADATA,
   321|             filter_expression=filter_expression,
   322|         )
   323|         if isinstance(response, SearchAndFetchVectors.Success):
   324|             pass
   325|         elif isinstance(response, SearchAndFetchVectors.Error):
   326|             logger.error(f"Error searching and fetching vectors: {response}")
   327|             return []
   328|         else:
   329|             logger.error(f"Unexpected response: {response}")
   330|             raise Exception(f"Unexpected response: {response}")
   331|         mmr_selected = maximal_marginal_relevance(
   332|             query_embedding=np.array([embedding], dtype=np.float32),
   333|             embedding_list=[hit.vector for hit in response.hits],
   334|             lambda_mult=lambda_mult,
   335|             k=k,
   336|         )
   337|         selected = [response.hits[i].metadata for i in mmr_selected]
   338|         return [
   339|             Document(page_content=metadata.pop(self.text_field, ""), metadata=metadata)  # type: ignore  # noqa: E501
   340|             for metadata in selected
   341|         ]


# ====================================================================
# FILE: libs/community/langchain_community/vectorstores/pgvector.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 138-246 ---
   138|         embedding_function: Embeddings,
   139|         collection_name: str = _LANGCHAIN_DEFAULT_COLLECTION_NAME,
   140|         collection_metadata: Optional[dict] = None,
   141|         distance_strategy: DistanceStrategy = DEFAULT_DISTANCE_STRATEGY,
   142|         pre_delete_collection: bool = False,
   143|         logger: Optional[logging.Logger] = None,
   144|         relevance_score_fn: Optional[Callable[[float], float]] = None,
   145|         *,
   146|         connection: Optional[sqlalchemy.engine.Connection] = None,
   147|         engine_args: Optional[dict[str, Any]] = None,
   148|     ) -> None:
   149|         self.connection_string = connection_string
   150|         self.embedding_function = embedding_function
   151|         self.collection_name = collection_name
   152|         self.collection_metadata = collection_metadata
   153|         self._distance_strategy = distance_strategy
   154|         self.pre_delete_collection = pre_delete_collection
   155|         self.logger = logger or logging.getLogger(__name__)
   156|         self.override_relevance_score_fn = relevance_score_fn
   157|         self.engine_args = engine_args or {}
   158|         self._bind = connection if connection else self._create_engine()
   159|         self.__post_init__()
   160|     def __post_init__(
   161|         self,
   162|     ) -> None:
   163|         """Initialize the store."""
   164|         self.create_vector_extension()
   165|         EmbeddingStore, CollectionStore = _get_embedding_collection_store()
   166|         self.CollectionStore = CollectionStore
   167|         self.EmbeddingStore = EmbeddingStore
   168|         self.create_tables_if_not_exists()
   169|         self.create_collection()
   170|     def __del__(self) -> None:
   171|         if isinstance(self._bind, sqlalchemy.engine.Connection):
   172|             self._bind.close()
   173|     @property
   174|     def embeddings(self) -> Embeddings:
   175|         return self.embedding_function
   176|     def _create_engine(self) -> sqlalchemy.engine.Engine:
   177|         return sqlalchemy.create_engine(url=self.connection_string, **self.engine_args)
   178|     def create_vector_extension(self) -> None:
   179|         try:
   180|             with Session(self._bind) as session:
   181|                 statement = sqlalchemy.text(
   182|                     "BEGIN;"
   183|                     "SELECT pg_advisory_xact_lock(1573678846307946496);"
   184|                     "CREATE EXTENSION IF NOT EXISTS vector;"
   185|                     "COMMIT;"
   186|                 )
   187|                 session.execute(statement)
   188|                 session.commit()
   189|         except Exception as e:
   190|             raise Exception(f"Failed to create vector extension: {e}") from e
   191|     def create_tables_if_not_exists(self) -> None:
   192|         with Session(self._bind) as session, session.begin():
   193|             Base.metadata.create_all(session.get_bind())
   194|     def drop_tables(self) -> None:
   195|         with Session(self._bind) as session, session.begin():
   196|             Base.metadata.drop_all(session.get_bind())
   197|     def create_collection(self) -> None:
   198|         if self.pre_delete_collection:
   199|             self.delete_collection()
   200|         with Session(self._bind) as session:
   201|             self.CollectionStore.get_or_create(
   202|                 session, self.collection_name, cmetadata=self.collection_metadata
   203|             )
   204|     def delete_collection(self) -> None:
   205|         self.logger.debug("Trying to delete collection")
   206|         with Session(self._bind) as session:
   207|             collection = self.get_collection(session)
   208|             if not collection:
   209|                 self.logger.warning("Collection not found")
   210|                 return
   211|             session.delete(collection)
   212|             session.commit()
   213|     @contextlib.contextmanager
   214|     def _make_session(self) -> Generator[Session, None, None]:
   215|         """Create a context manager for the session, bind to _conn string."""
   216|         yield Session(self._bind)
   217|     def delete(
   218|         self,
   219|         ids: Optional[List[str]] = None,
   220|         **kwargs: Any,
   221|     ) -> None:
   222|         """Delete vectors by ids or uuids.
   223|         Args:
   224|             ids: List of ids to delete.
   225|         """
   226|         with Session(self._bind) as session:
   227|             if ids is not None:
   228|                 self.logger.debug(
   229|                     "Trying to delete vectors by ids (represented by the model "
   230|                     "using the custom ids field)"
   231|                 )
   232|                 stmt = delete(self.EmbeddingStore).where(
   233|                     self.EmbeddingStore.custom_id.in_(ids)
   234|                 )
   235|                 session.execute(stmt)
   236|             session.commit()
   237|     def get_collection(self, session: Session) -> Any:
   238|         return self.CollectionStore.get_by_name(session, self.collection_name)
   239|     @classmethod
   240|     def __from(
   241|         cls,
   242|         texts: List[str],
   243|         embeddings: List[List[float]],
   244|         embedding: Embeddings,
   245|         metadatas: Optional[List[dict]] = None,
   246|         ids: Optional[List[str]] = None,

# --- HUNK 2: Lines 270-310 ---
   270|         return store
   271|     def add_embeddings(
   272|         self,
   273|         texts: Iterable[str],
   274|         embeddings: List[List[float]],
   275|         metadatas: Optional[List[dict]] = None,
   276|         ids: Optional[List[str]] = None,
   277|         **kwargs: Any,
   278|     ) -> List[str]:
   279|         """Add embeddings to the vectorstore.
   280|         Args:
   281|             texts: Iterable of strings to add to the vectorstore.
   282|             embeddings: List of list of embedding vectors.
   283|             metadatas: List of metadatas associated with the texts.
   284|             kwargs: vectorstore specific parameters
   285|         """
   286|         if ids is None:
   287|             ids = [str(uuid.uuid1()) for _ in texts]
   288|         if not metadatas:
   289|             metadatas = [{} for _ in texts]
   290|         with Session(self._bind) as session:
   291|             collection = self.get_collection(session)
   292|             if not collection:
   293|                 raise ValueError("Collection not found")
   294|             for text, metadata, embedding, id in zip(texts, metadatas, embeddings, ids):
   295|                 embedding_store = self.EmbeddingStore(
   296|                     embedding=embedding,
   297|                     document=text,
   298|                     cmetadata=metadata,
   299|                     custom_id=id,
   300|                     collection_id=collection.uuid,
   301|                 )
   302|                 session.add(embedding_store)
   303|             session.commit()
   304|         return ids
   305|     def add_texts(
   306|         self,
   307|         texts: Iterable[str],
   308|         metadatas: Optional[List[dict]] = None,
   309|         ids: Optional[List[str]] = None,
   310|         **kwargs: Any,

# --- HUNK 3: Lines 385-425 ---
   385|     def _results_to_docs_and_scores(self, results: Any) -> List[Tuple[Document, float]]:
   386|         """Return docs and scores from results."""
   387|         docs = [
   388|             (
   389|                 Document(
   390|                     page_content=result.EmbeddingStore.document,
   391|                     metadata=result.EmbeddingStore.cmetadata,
   392|                 ),
   393|                 result.distance if self.embedding_function is not None else None,
   394|             )
   395|             for result in results
   396|         ]
   397|         return docs
   398|     def __query_collection(
   399|         self,
   400|         embedding: List[float],
   401|         k: int = 4,
   402|         filter: Optional[Dict[str, str]] = None,
   403|     ) -> List[Any]:
   404|         """Query the collection."""
   405|         with Session(self._bind) as session:
   406|             collection = self.get_collection(session)
   407|             if not collection:
   408|                 raise ValueError("Collection not found")
   409|             filter_by = self.EmbeddingStore.collection_id == collection.uuid
   410|             if filter is not None:
   411|                 filter_clauses = []
   412|                 IN, NIN = "in", "nin"
   413|                 for key, value in filter.items():
   414|                     if isinstance(value, dict):
   415|                         value_case_insensitive = {
   416|                             k.lower(): v for k, v in value.items()
   417|                         }
   418|                         if IN in map(str.lower, value):
   419|                             filter_by_metadata = self.EmbeddingStore.cmetadata[
   420|                                 key
   421|                             ].astext.in_(value_case_insensitive[IN])
   422|                         elif NIN in map(str.lower, value):
   423|                             filter_by_metadata = self.EmbeddingStore.cmetadata[
   424|                                 key
   425|                             ].astext.not_in(value_case_insensitive[NIN])


# ====================================================================
# FILE: libs/community/langchain_community/vectorstores/semadb.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 8-48 ---
     8| from langchain_core.vectorstores import VectorStore
     9| from langchain_community.vectorstores.utils import DistanceStrategy
    10| class SemaDB(VectorStore):
    11|     """`SemaDB` vector store.
    12|     This vector store is a wrapper around the SemaDB database.
    13|     Example:
    14|         .. code-block:: python
    15|             from langchain_community.vectorstores import SemaDB
    16|             db = SemaDB('mycollection', 768, embeddings, DistanceStrategy.COSINE)
    17|     """
    18|     HOST = "semadb.p.rapidapi.com"
    19|     BASE_URL = "https://" + HOST
    20|     def __init__(
    21|         self,
    22|         collection_name: str,
    23|         vector_size: int,
    24|         embedding: Embeddings,
    25|         distance_strategy: DistanceStrategy = DistanceStrategy.EUCLIDEAN_DISTANCE,
    26|         api_key: str = "",
    27|     ):
    28|         """initialize the SemaDB vector store."""
    29|         self.collection_name = collection_name
    30|         self.vector_size = vector_size
    31|         self.api_key = api_key or get_from_env("api_key", "SEMADB_API_KEY")
    32|         self._embedding = embedding
    33|         self.distance_strategy = distance_strategy
    34|     @property
    35|     def headers(self) -> dict:
    36|         """Return the common headers."""
    37|         return {
    38|             "content-type": "application/json",
    39|             "X-RapidAPI-Key": self.api_key,
    40|             "X-RapidAPI-Host": SemaDB.HOST,
    41|         }
    42|     def _get_internal_distance_strategy(self) -> str:
    43|         """Return the internal distance strategy."""
    44|         if self.distance_strategy == DistanceStrategy.EUCLIDEAN_DISTANCE:
    45|             return "euclidean"
    46|         elif self.distance_strategy == DistanceStrategy.MAX_INNER_PRODUCT:
    47|             raise ValueError("Max inner product is not supported by SemaDB")
    48|         elif self.distance_strategy == DistanceStrategy.DOT_PRODUCT:


# ====================================================================
# FILE: libs/community/langchain_community/vectorstores/surrealdb.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 33-136 ---
    33|             db_user = "root"
    34|             db_pass = "root"
    35|             sdb = SurrealDBStore.from_texts(
    36|                     texts=texts,
    37|                     embedding=embedding_function,
    38|                     dburl,
    39|                     ns, db, collection,
    40|                     db_user=db_user, db_pass=db_pass)
    41|     """
    42|     def __init__(
    43|         self,
    44|         embedding_function: Embeddings,
    45|         **kwargs: Any,
    46|     ) -> None:
    47|         from surrealdb import Surreal
    48|         self.collection = kwargs.pop("collection", "documents")
    49|         self.ns = kwargs.pop("ns", "langchain")
    50|         self.db = kwargs.pop("db", "database")
    51|         self.dburl = kwargs.pop("dburl", "ws://localhost:8000/rpc")
    52|         self.embedding_function = embedding_function
    53|         self.sdb = Surreal(self.dburl)
    54|         self.kwargs = kwargs
    55|     async def initialize(self) -> None:
    56|         """
    57|         Initialize connection to surrealdb database
    58|         and authenticate if credentials are provided
    59|         """
    60|         await self.sdb.connect(self.dburl)
    61|         if "db_user" in self.kwargs and "db_pass" in self.kwargs:
    62|             user = self.kwargs.get("db_user")
    63|             password = self.kwargs.get("db_pass")
    64|             await self.sdb.signin({"user": user, "pass": password})
    65|         await self.sdb.use(self.ns, self.db)
    66|     @property
    67|     def embeddings(self) -> Optional[Embeddings]:
    68|         return (
    69|             self.embedding_function
    70|             if isinstance(self.embedding_function, Embeddings)
    71|             else None
    72|         )
    73|     async def aadd_texts(
    74|         self,
    75|         texts: Iterable[str],
    76|         metadatas: Optional[List[dict]] = None,
    77|         **kwargs: Any,
    78|     ) -> List[str]:
    79|         """Add list of text along with embeddings to the vector store asynchronously
    80|         Args:
    81|             texts (Iterable[str]): collection of text to add to the database
    82|         Returns:
    83|             List of ids for the newly inserted documents
    84|         """
    85|         embeddings = self.embedding_function.embed_documents(list(texts))
    86|         ids = []
    87|         for idx, text in enumerate(texts):
    88|             data = {"text": text, "embedding": embeddings[idx]}
    89|             if metadatas is not None and idx < len(metadatas):
    90|                 data["metadata"] = metadatas[idx]
    91|             record = await self.sdb.create(
    92|                 self.collection,
    93|                 data,
    94|             )
    95|             ids.append(record[0]["id"])
    96|         return ids
    97|     def add_texts(
    98|         self,
    99|         texts: Iterable[str],
   100|         metadatas: Optional[List[dict]] = None,
   101|         **kwargs: Any,
   102|     ) -> List[str]:
   103|         """Add list of text along with embeddings to the vector store
   104|         Args:
   105|             texts (Iterable[str]): collection of text to add to the database
   106|         Returns:
   107|             List of ids for the newly inserted documents
   108|         """
   109|         async def _add_texts(
   110|             texts: Iterable[str],
   111|             metadatas: Optional[List[dict]] = None,
   112|             **kwargs: Any,
   113|         ) -> List[str]:
   114|             await self.initialize()
   115|             return await self.aadd_texts(texts, metadatas, **kwargs)
   116|         return asyncio.run(_add_texts(texts, metadatas, **kwargs))
   117|     async def adelete(
   118|         self,
   119|         ids: Optional[List[str]] = None,
   120|         **kwargs: Any,
   121|     ) -> Optional[bool]:
   122|         """Delete by document ID asynchronously.
   123|         Args:
   124|             ids: List of ids to delete.
   125|             **kwargs: Other keyword arguments that subclasses might use.
   126|         Returns:
   127|             Optional[bool]: True if deletion is successful,
   128|             False otherwise.
   129|         """
   130|         if ids is None:
   131|             await self.sdb.delete(self.collection)
   132|             return True
   133|         else:
   134|             if isinstance(ids, str):
   135|                 await self.sdb.delete(ids)
   136|                 return True

# --- HUNK 2: Lines 156-210 ---
   156|             await self.initialize()
   157|             return await self.adelete(ids=ids, **kwargs)
   158|         return asyncio.run(_delete(ids, **kwargs))
   159|     async def _asimilarity_search_by_vector_with_score(
   160|         self, embedding: List[float], k: int = 4, **kwargs: Any
   161|     ) -> List[Tuple[Document, float]]:
   162|         """Run similarity search for query embedding asynchronously
   163|         and return documents and scores
   164|         Args:
   165|             embedding (List[float]): Query embedding.
   166|             k (int): Number of results to return. Defaults to 4.
   167|         Returns:
   168|             List of Documents most similar along with scores
   169|         """
   170|         args = {
   171|             "collection": self.collection,
   172|             "embedding": embedding,
   173|             "k": k,
   174|             "score_threshold": kwargs.get("score_threshold", 0),
   175|         }
   176|         query = """select id, text, metadata,
   177|         vector::similarity::cosine(embedding,{embedding}) as similarity
   178|         from {collection}
   179|         where vector::similarity::cosine(embedding,{embedding}) >= {score_threshold}
   180|         order by similarity desc LIMIT {k}
   181|         """.format(**args)
   182|         results = await self.sdb.query(query)
   183|         if len(results) == 0:
   184|             return []
   185|         return [
   186|             (
   187|                 Document(
   188|                     page_content=result["text"],
   189|                     metadata={"id": result["id"], **result["metadata"]},
   190|                 ),
   191|                 result["similarity"],
   192|             )
   193|             for result in results[0]["result"]
   194|         ]
   195|     async def asimilarity_search_with_relevance_scores(
   196|         self, query: str, k: int = 4, **kwargs: Any
   197|     ) -> List[Tuple[Document, float]]:
   198|         """Run similarity search asynchronously and return relevance scores
   199|         Args:
   200|             query (str): Query
   201|             k (int): Number of results to return. Defaults to 4.
   202|         Returns:
   203|             List of Documents most similar along with relevance scores
   204|         """
   205|         query_embedding = self.embedding_function.embed_query(query)
   206|         return [
   207|             (document, similarity)
   208|             for document, similarity in (
   209|                 await self._asimilarity_search_by_vector_with_score(
   210|                     query_embedding, k, **kwargs

# --- HUNK 3: Lines 326-366 ---
   326|         metadatas: Optional[List[dict]] = None,
   327|         **kwargs: Any,
   328|     ) -> "SurrealDBStore":
   329|         """Create SurrealDBStore from list of text asynchronously
   330|         Args:
   331|             texts (List[str]): list of text to vectorize and store
   332|             embedding (Optional[Embeddings]): Embedding function.
   333|             dburl (str): SurrealDB connection url
   334|                 (default: "ws://localhost:8000/rpc")
   335|             ns (str): surrealdb namespace for the vector store.
   336|                 (default: "langchain")
   337|             db (str): surrealdb database for the vector store.
   338|                 (default: "database")
   339|             collection (str): surrealdb collection for the vector store.
   340|                 (default: "documents")
   341|             (optional) db_user and db_pass: surrealdb credentials
   342|         Returns:
   343|             SurrealDBStore object initialized and ready for use."""
   344|         sdb = cls(embedding, **kwargs)
   345|         await sdb.initialize()
   346|         await sdb.aadd_texts(texts, metadatas, **kwargs)
   347|         return sdb
   348|     @classmethod
   349|     def from_texts(
   350|         cls,
   351|         texts: List[str],
   352|         embedding: Embeddings,
   353|         metadatas: Optional[List[dict]] = None,
   354|         **kwargs: Any,
   355|     ) -> "SurrealDBStore":
   356|         """Create SurrealDBStore from list of text
   357|         Args:
   358|             texts (List[str]): list of text to vectorize and store
   359|             embedding (Optional[Embeddings]): Embedding function.
   360|             dburl (str): SurrealDB connection url
   361|             ns (str): surrealdb namespace for the vector store.
   362|                 (default: "langchain")
   363|             db (str): surrealdb database for the vector store.
   364|                 (default: "database")
   365|             collection (str): surrealdb collection for the vector store.
   366|                 (default: "documents")


# ====================================================================
# FILE: libs/community/langchain_community/vectorstores/vectara.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 1-89 ---
     1| from __future__ import annotations
     2| import json
     3| import logging
     4| import os
     5| from dataclasses import dataclass, field
     6| from hashlib import md5
     7| from typing import Any, Iterable, List, Optional, Tuple, Type
     8| import requests
     9| from langchain_core.documents import Document
    10| from langchain_core.embeddings import Embeddings
    11| from langchain_core.pydantic_v1 import Field
    12| from langchain_core.vectorstores import VectorStore, VectorStoreRetriever
    13| logger = logging.getLogger(__name__)
    14| @dataclass
    15| class SummaryConfig:
    16|     """
    17|     is_enabled: True if summary is enabled, False otherwise
    18|     max_results: maximum number of results to summarize
    19|     response_lang: requested language for the summary
    20|     """
    21|     is_enabled: bool = False
    22|     max_results: int = 7
    23|     response_lang: str = "eng"
    24| @dataclass
    25| class MMRConfig:
    26|     """
    27|     is_enabled: True if MMR is enabled, False otherwise
    28|     mmr_k: number of results to fetch for MMR, defaults to 50
    29|     diversity_bias: number between 0 and 1 that determines the degree
    30|         of diversity among the results with 0 corresponding
    31|         to minimum diversity and 1 to maximum diversity.
    32|         Defaults to 0.3.
    33|         Note: diversity_bias is equivalent 1-lambda_mult
    34|         where lambda_mult is the value often used in max_marginal_relevance_search()
    35|         We chose to use that since we believe it's more intuitive to the user.
    36|     """
    37|     is_enabled: bool = False
    38|     mmr_k: int = 50
    39|     diversity_bias: float = 0.3
    40| @dataclass
    41| class VectaraQueryConfig:
    42|     """
    43|     k: Number of Documents to return. Defaults to 10.
    44|     lambda_val: lexical match parameter for hybrid search.
    45|     filter Dictionary of argument(s) to filter on metadata. For example a
    46|         filter can be "doc.rating > 3.0 and part.lang = 'deu'"} see
    47|         https://docs.vectara.com/docs/search-apis/sql/filter-overview
    48|         for more details.
    49|     score_threshold: minimal score threshold for the result.
    50|         If defined, results with score less than this value will be
    51|         filtered out.
    52|     n_sentence_context: number of sentences before/after the matching segment
    53|         to add, defaults to 2
    54|     mmr_config: MMRConfig configuration dataclass
    55|     summary_config: SummaryConfig configuration dataclass
    56|     """
    57|     k: int = 10
    58|     lambda_val: float = 0.0
    59|     filter: str = ""
    60|     score_threshold: Optional[float] = None
    61|     n_sentence_context: int = 2
    62|     mmr_config: MMRConfig = field(default_factory=MMRConfig)
    63|     summary_config: SummaryConfig = field(default_factory=SummaryConfig)
    64| class Vectara(VectorStore):
    65|     """`Vectara API` vector store.
    66|      See (https://vectara.com).
    67|     Example:
    68|         .. code-block:: python
    69|             from langchain.vectorstores import Vectara
    70|             vectorstore = Vectara(
    71|                 vectara_customer_id=vectara_customer_id,
    72|                 vectara_corpus_id=vectara_corpus_id,
    73|                 vectara_api_key=vectara_api_key
    74|             )
    75|     """
    76|     def __init__(
    77|         self,
    78|         vectara_customer_id: Optional[str] = None,
    79|         vectara_corpus_id: Optional[str] = None,
    80|         vectara_api_key: Optional[str] = None,
    81|         vectara_api_timeout: int = 120,
    82|         source: str = "langchain",
    83|     ):
    84|         """Initialize with Vectara API."""
    85|         self._vectara_customer_id = vectara_customer_id or os.environ.get(
    86|             "VECTARA_CUSTOMER_ID"
    87|         )
    88|         self._vectara_corpus_id = vectara_corpus_id or os.environ.get(
    89|             "VECTARA_CORPUS_ID"

# --- HUNK 2: Lines 128-180 ---
   128|         body = {
   129|             "customer_id": self._vectara_customer_id,
   130|             "corpus_id": self._vectara_corpus_id,
   131|             "document_id": doc_id,
   132|         }
   133|         response = self._session.post(
   134|             "https://api.vectara.io/v1/delete-doc",
   135|             data=json.dumps(body),
   136|             verify=True,
   137|             headers=self._get_post_headers(),
   138|             timeout=self.vectara_api_timeout,
   139|         )
   140|         if response.status_code != 200:
   141|             logger.error(
   142|                 f"Delete request failed for doc_id = {doc_id} with status code "
   143|                 f"{response.status_code}, reason {response.reason}, text "
   144|                 f"{response.text}"
   145|             )
   146|             return False
   147|         return True
   148|     def _index_doc(self, doc: dict, use_core_api: bool = False) -> str:
   149|         request: dict[str, Any] = {}
   150|         request["customer_id"] = self._vectara_customer_id
   151|         request["corpus_id"] = self._vectara_corpus_id
   152|         request["document"] = doc
   153|         api_endpoint = (
   154|             "https://api.vectara.io/v1/core/index"
   155|             if use_core_api
   156|             else "https://api.vectara.io/v1/index"
   157|         )
   158|         response = self._session.post(
   159|             headers=self._get_post_headers(),
   160|             url=api_endpoint,
   161|             data=json.dumps(request),
   162|             timeout=self.vectara_api_timeout,
   163|             verify=True,
   164|         )
   165|         status_code = response.status_code
   166|         result = response.json()
   167|         status_str = result["status"]["code"] if "status" in result else None
   168|         if status_code == 409 or status_str and (status_str == "ALREADY_EXISTS"):
   169|             return "E_ALREADY_EXISTS"
   170|         elif status_str and (status_str == "FORBIDDEN"):
   171|             return "E_NO_PERMISSIONS"
   172|         else:
   173|             return "E_SUCCEEDED"
   174|     def add_files(
   175|         self,
   176|         files_list: Iterable[str],
   177|         metadatas: Optional[List[dict]] = None,
   178|         **kwargs: Any,
   179|     ) -> List[str]:
   180|         """

# --- HUNK 3: Lines 231-507 ---
   231|             texts: Iterable of strings to add to the vectorstore.
   232|             metadatas: Optional list of metadatas associated with the texts.
   233|             doc_metadata: optional metadata for the document
   234|         This function indexes all the input text strings in the Vectara corpus as a
   235|         single Vectara document, where each input text is considered a "section" and the
   236|         metadata are associated with each section.
   237|         if 'doc_metadata' is provided, it is associated with the Vectara document.
   238|         Returns:
   239|             document ID of the document added
   240|         """
   241|         doc_hash = md5()
   242|         for t in texts:
   243|             doc_hash.update(t.encode())
   244|         doc_id = doc_hash.hexdigest()
   245|         if metadatas is None:
   246|             metadatas = [{} for _ in texts]
   247|         if doc_metadata:
   248|             doc_metadata["source"] = "langchain"
   249|         else:
   250|             doc_metadata = {"source": "langchain"}
   251|         use_core_api = kwargs.get("use_core_api", False)
   252|         section_key = "parts" if use_core_api else "section"
   253|         doc = {
   254|             "document_id": doc_id,
   255|             "metadataJson": json.dumps(doc_metadata),
   256|             section_key: [
   257|                 {"text": text, "metadataJson": json.dumps(md)}
   258|                 for text, md in zip(texts, metadatas)
   259|             ],
   260|         }
   261|         success_str = self._index_doc(doc, use_core_api=use_core_api)
   262|         if success_str == "E_ALREADY_EXISTS":
   263|             self._delete_doc(doc_id)
   264|             self._index_doc(doc)
   265|         elif success_str == "E_NO_PERMISSIONS":
   266|             print(
   267|                 """No permissions to add document to Vectara. 
   268|                 Check your corpus ID, customer ID and API key"""
   269|             )
   270|         return [doc_id]
   271|     def vectara_query(
   272|         self,
   273|         query: str,
   274|         config: VectaraQueryConfig,
   275|         **kwargs: Any,
   276|     ) -> List[Tuple[Document, float]]:
   277|         """Run a Vectara query
   278|         Args:
   279|             query: Text to look up documents similar to.
   280|             config: VectaraQueryConfig object
   281|         Returns:
   282|             A list of k Documents matching the given query
   283|             If summary is enabled, last document is the summary text with 'summary'=True
   284|         """
   285|         if isinstance(config.mmr_config, dict):
   286|             config.mmr_config = MMRConfig(**config.mmr_config)
   287|         if isinstance(config.summary_config, dict):
   288|             config.summary_config = SummaryConfig(**config.summary_config)
   289|         data = {
   290|             "query": [
   291|                 {
   292|                     "query": query,
   293|                     "start": 0,
   294|                     "numResults": config.mmr_config.mmr_k
   295|                     if config.mmr_config.is_enabled
   296|                     else config.k,
   297|                     "contextConfig": {
   298|                         "sentencesBefore": config.n_sentence_context,
   299|                         "sentencesAfter": config.n_sentence_context,
   300|                     },
   301|                     "corpusKey": [
   302|                         {
   303|                             "customerId": self._vectara_customer_id,
   304|                             "corpusId": self._vectara_corpus_id,
   305|                             "metadataFilter": config.filter,
   306|                             "lexicalInterpolationConfig": {"lambda": config.lambda_val},
   307|                         }
   308|                     ],
   309|                 }
   310|             ]
   311|         }
   312|         if config.mmr_config.is_enabled:
   313|             data["query"][0]["rerankingConfig"] = {
   314|                 "rerankerId": 272725718,
   315|                 "mmrConfig": {"diversityBias": config.mmr_config.diversity_bias},
   316|             }
   317|         if config.summary_config.is_enabled:
   318|             data["query"][0]["summary"] = [
   319|                 {
   320|                     "maxSummarizedResults": config.summary_config.max_results,
   321|                     "responseLang": config.summary_config.response_lang,
   322|                 }
   323|             ]
   324|         response = self._session.post(
   325|             headers=self._get_post_headers(),
   326|             url="https://api.vectara.io/v1/query",
   327|             data=json.dumps(data),
   328|             timeout=self.vectara_api_timeout,
   329|         )
   330|         if response.status_code != 200:
   331|             logger.error(
   332|                 "Query failed %s",
   333|                 f"(code {response.status_code}, reason {response.reason}, details "
   334|                 f"{response.text})",
   335|             )
   336|             return [], ""
   337|         result = response.json()
   338|         if config.score_threshold:
   339|             responses = [
   340|                 r
   341|                 for r in result["responseSet"][0]["response"]
   342|                 if r["score"] > config.score_threshold
   343|             ]
   344|         else:
   345|             responses = result["responseSet"][0]["response"]
   346|         documents = result["responseSet"][0]["document"]
   347|         metadatas = []
   348|         for x in responses:
   349|             md = {m["name"]: m["value"] for m in x["metadata"]}
   350|             doc_num = x["documentIndex"]
   351|             doc_md = {m["name"]: m["value"] for m in documents[doc_num]["metadata"]}
   352|             if "source" not in doc_md:
   353|                 doc_md["source"] = "vectara"
   354|             md.update(doc_md)
   355|             metadatas.append(md)
   356|         res = [
   357|             (
   358|                 Document(
   359|                     page_content=x["text"],
   360|                     metadata=md,
   361|                 ),
   362|                 x["score"],
   363|             )
   364|             for x, md in zip(responses, metadatas)
   365|         ]
   366|         if config.mmr_config.is_enabled:
   367|             res = res[: config.k]
   368|         if config.summary_config.is_enabled:
   369|             summary = result["responseSet"][0]["summary"][0]["text"]
   370|             res.append(
   371|                 (Document(page_content=summary, metadata={"summary": True}), 0.0)
   372|             )
   373|         return res
   374|     def similarity_search_with_score(
   375|         self,
   376|         query: str,
   377|         **kwargs: Any,
   378|     ) -> List[Tuple[Document, float]]:
   379|         """Return Vectara documents most similar to query, along with scores.
   380|         Args:
   381|             query: Text to look up documents similar to.
   382|             k: Number of Documents to return. Defaults to 10.
   383|             any other querying variable in VectaraQueryConfig like:
   384|             - lambda_val: lexical match parameter for hybrid search.
   385|             - filter: filter string
   386|             - score_threshold: minimal score threshold for the result.
   387|             - n_sentence_context: number of sentences before/after the matching segment
   388|             - mmr_config: optional configuration for MMR (see MMRConfig dataclass)
   389|             - summary_config: optional configuration for summary
   390|               (see SummaryConfig dataclass)
   391|         Returns:
   392|             List of Documents most similar to the query and score for each.
   393|         """
   394|         config = VectaraQueryConfig(**kwargs)
   395|         docs = self.vectara_query(query, config)
   396|         return docs
   397|     def similarity_search(
   398|         self,
   399|         query: str,
   400|         **kwargs: Any,
   401|     ) -> List[Document]:
   402|         """Return Vectara documents most similar to query, along with scores.
   403|         Args:
   404|             query: Text to look up documents similar to.
   405|             any other querying variable in VectaraQueryConfig
   406|         Returns:
   407|             List of Documents most similar to the query
   408|         """
   409|         docs_and_scores = self.similarity_search_with_score(
   410|             query,
   411|             **kwargs,
   412|         )
   413|         return [doc for doc, _ in docs_and_scores]
   414|     def max_marginal_relevance_search(
   415|         self,
   416|         query: str,
   417|         fetch_k: int = 50,
   418|         lambda_mult: float = 0.5,
   419|         **kwargs: Any,
   420|     ) -> List[Document]:
   421|         """Return docs selected using the maximal marginal relevance.
   422|         Maximal marginal relevance optimizes for similarity to query AND diversity
   423|         among selected documents.
   424|         Args:
   425|             query: Text to look up documents similar to.
   426|             k: Number of Documents to return. Defaults to 5.
   427|             fetch_k: Number of Documents to fetch to pass to MMR algorithm.
   428|                      Defaults to 50
   429|             lambda_mult: Number between 0 and 1 that determines the degree
   430|                         of diversity among the results with 0 corresponding
   431|                         to maximum diversity and 1 to minimum diversity.
   432|                         Defaults to 0.5.
   433|             kwargs: any other querying variable in VectaraQueryConfig
   434|         Returns:
   435|             List of Documents selected by maximal marginal relevance.
   436|         """
   437|         kwargs["mmr_config"] = MMRConfig(
   438|             is_enabled=True, mmr_k=fetch_k, diversity_bias=1 - lambda_mult
   439|         )
   440|         return self.similarity_search(query, **kwargs)
   441|     @classmethod
   442|     def from_texts(
   443|         cls: Type[Vectara],
   444|         texts: List[str],
   445|         embedding: Optional[Embeddings] = None,
   446|         metadatas: Optional[List[dict]] = None,
   447|         **kwargs: Any,
   448|     ) -> Vectara:
   449|         """Construct Vectara wrapper from raw documents.
   450|         This is intended to be a quick way to get started.
   451|         Example:
   452|             .. code-block:: python
   453|                 from langchain.vectorstores import Vectara
   454|                 vectara = Vectara.from_texts(
   455|                     texts,
   456|                     vectara_customer_id=customer_id,
   457|                     vectara_corpus_id=corpus_id,
   458|                     vectara_api_key=api_key,
   459|                 )
   460|         """
   461|         doc_metadata = kwargs.pop("doc_metadata", {})
   462|         vectara = cls(**kwargs)
   463|         vectara.add_texts(texts, metadatas, doc_metadata=doc_metadata, **kwargs)
   464|         return vectara
   465|     @classmethod
   466|     def from_files(
   467|         cls: Type[Vectara],
   468|         files: List[str],
   469|         embedding: Optional[Embeddings] = None,
   470|         metadatas: Optional[List[dict]] = None,
   471|         **kwargs: Any,
   472|     ) -> Vectara:
   473|         """Construct Vectara wrapper from raw documents.
   474|         This is intended to be a quick way to get started.
   475|         Example:
   476|             .. code-block:: python
   477|                 from langchain.vectorstores import Vectara
   478|                 vectara = Vectara.from_files(
   479|                     files_list,
   480|                     vectara_customer_id=customer_id,
   481|                     vectara_corpus_id=corpus_id,
   482|                     vectara_api_key=api_key,
   483|                 )
   484|         """
   485|         vectara = cls(**kwargs)
   486|         vectara.add_files(files, metadatas)
   487|         return vectara
   488| class VectaraRetriever(VectorStoreRetriever):
   489|     """Retriever class for `Vectara`."""
   490|     vectorstore: Vectara
   491|     """Vectara vectorstore."""
   492|     search_kwargs: dict = Field(
   493|         default_factory=lambda: {
   494|             "lambda_val": 0.0,
   495|             "k": 5,
   496|             "filter": "",
   497|             "n_sentence_context": "2",
   498|         }
   499|     )
   500|     """Search params.
   501|         k: Number of Documents to return. Defaults to 5.
   502|         lambda_val: lexical match parameter for hybrid search.
   503|         filter: Dictionary of argument(s) to filter on metadata. For example a
   504|             filter can be "doc.rating > 3.0 and part.lang = 'deu'"} see
   505|             https://docs.vectara.com/docs/search-apis/sql/filter-overview
   506|             for more details.
   507|         n_sentence_context: number of sentences before/after the matching segment to add


# ====================================================================
# FILE: libs/core/langchain_core/beta/runnables/context.py
# Total hunks: 5
# ====================================================================
# --- HUNK 1: Lines 1-35 ---
     1| import asyncio
     2| import threading
     3| from collections import defaultdict
     4| from functools import partial
     5| from itertools import groupby
     6| from typing import (
     7|     Any,
     8|     Awaitable,
     9|     Callable,
    10|     DefaultDict,
    11|     Dict,
    12|     List,
    13|     Mapping,
    14|     Optional,
    15|     Sequence,
    16|     Type,
    17|     TypeVar,
    18|     Union,
    19| )
    20| from langchain_core.runnables.base import (
    21|     Runnable,
    22|     RunnableSerializable,
    23|     coerce_to_runnable,
    24| )
    25| from langchain_core.runnables.config import RunnableConfig, patch_config
    26| from langchain_core.runnables.utils import ConfigurableFieldSpec, Input, Output
    27| T = TypeVar("T")
    28| Values = Dict[Union[asyncio.Event, threading.Event], Any]
    29| CONTEXT_CONFIG_PREFIX = "__context__/"
    30| CONTEXT_CONFIG_SUFFIX_GET = "/get"
    31| CONTEXT_CONFIG_SUFFIX_SET = "/set"
    32| async def _asetter(done: asyncio.Event, values: Values, value: T) -> T:
    33|     values[done] = value
    34|     done.set()
    35|     return value

# --- HUNK 2: Lines 75-155 ---
    75|     }
    76|     deps_by_key = {
    77|         key: set(
    78|             _key_from_id(dep) for spec in group for dep in (spec[0].dependencies or [])
    79|         )
    80|         for key, group in grouped_by_key.items()
    81|     }
    82|     values: Values = {}
    83|     events: DefaultDict[str, Union[asyncio.Event, threading.Event]] = defaultdict(
    84|         event_cls
    85|     )
    86|     context_funcs: Dict[str, Callable[[], Any]] = {}
    87|     for key, group in grouped_by_key.items():
    88|         getters = [s for s in group if s[0].id.endswith(CONTEXT_CONFIG_SUFFIX_GET)]
    89|         setters = [s for s in group if s[0].id.endswith(CONTEXT_CONFIG_SUFFIX_SET)]
    90|         for dep in deps_by_key[key]:
    91|             if key in deps_by_key[dep]:
    92|                 raise ValueError(
    93|                     f"Deadlock detected between context keys {key} and {dep}"
    94|                 )
    95|         if len(setters) != 1:
    96|             raise ValueError(f"Expected exactly one setter for context key {key}")
    97|         setter_idx = setters[0][1]
    98|         if any(getter_idx < setter_idx for _, getter_idx in getters):
    99|             raise ValueError(
   100|                 f"Context setter for key {key} must be defined after all getters."
   101|             )
   102|         if getters:
   103|             context_funcs[getters[0][0].id] = partial(getter, events[key], values)
   104|         context_funcs[setters[0][0].id] = partial(setter, events[key], values)
   105|     return patch_config(config, configurable=context_funcs)
   106| def aconfig_with_context(
   107|     config: RunnableConfig,
   108|     steps: List[Runnable],
   109| ) -> RunnableConfig:
   110|     """Asynchronously patch a runnable config with context getters and setters.
   111|     Args:
   112|         config: The runnable config.
   113|         steps: The runnable steps.
   114|     Returns:
   115|         The patched runnable config.
   116|     """
   117|     return _config_with_context(config, steps, _asetter, _agetter, asyncio.Event)
   118| def config_with_context(
   119|     config: RunnableConfig,
   120|     steps: List[Runnable],
   121| ) -> RunnableConfig:
   122|     """Patch a runnable config with context getters and setters.
   123|     Args:
   124|         config: The runnable config.
   125|         steps: The runnable steps.
   126|     Returns:
   127|         The patched runnable config.
   128|     """
   129|     return _config_with_context(config, steps, _setter, _getter, threading.Event)
   130| class ContextGet(RunnableSerializable):
   131|     """Get a context value."""
   132|     prefix: str = ""
   133|     key: Union[str, List[str]]
   134|     def __str__(self) -> str:
   135|         return f"ContextGet({_print_keys(self.key)})"
   136|     @property
   137|     def ids(self) -> List[str]:
   138|         prefix = self.prefix + "/" if self.prefix else ""
   139|         keys = self.key if isinstance(self.key, list) else [self.key]
   140|         return [
   141|             f"{CONTEXT_CONFIG_PREFIX}{prefix}{k}{CONTEXT_CONFIG_SUFFIX_GET}"
   142|             for k in keys
   143|         ]
   144|     @property
   145|     def config_specs(self) -> List[ConfigurableFieldSpec]:
   146|         return super().config_specs + [
   147|             ConfigurableFieldSpec(
   148|                 id=id_,
   149|                 annotation=Callable[[], Any],
   150|             )
   151|             for id_ in self.ids
   152|         ]
   153|     def invoke(self, input: Any, config: Optional[RunnableConfig] = None) -> Any:
   154|         config = config or {}
   155|         configurable = config.get("configurable", {})

# --- HUNK 3: Lines 182-223 ---
   182|     prefix: str = ""
   183|     keys: Mapping[str, Optional[Runnable]]
   184|     class Config:
   185|         arbitrary_types_allowed = True
   186|     def __init__(
   187|         self,
   188|         key: Optional[str] = None,
   189|         value: Optional[SetValue] = None,
   190|         prefix: str = "",
   191|         **kwargs: SetValue,
   192|     ):
   193|         if key is not None:
   194|             kwargs[key] = value
   195|         super().__init__(
   196|             keys={
   197|                 k: _coerce_set_value(v) if v is not None else None
   198|                 for k, v in kwargs.items()
   199|             },
   200|             prefix=prefix,
   201|         )
   202|     def __str__(self) -> str:
   203|         return f"ContextSet({_print_keys(list(self.keys.keys()))})"
   204|     @property
   205|     def ids(self) -> List[str]:
   206|         prefix = self.prefix + "/" if self.prefix else ""
   207|         return [
   208|             f"{CONTEXT_CONFIG_PREFIX}{prefix}{key}{CONTEXT_CONFIG_SUFFIX_SET}"
   209|             for key in self.keys
   210|         ]
   211|     @property
   212|     def config_specs(self) -> List[ConfigurableFieldSpec]:
   213|         mapper_config_specs = [
   214|             s
   215|             for mapper in self.keys.values()
   216|             if mapper is not None
   217|             for s in mapper.config_specs
   218|         ]
   219|         for spec in mapper_config_specs:
   220|             if spec.id.endswith(CONTEXT_CONFIG_SUFFIX_GET):
   221|                 getter_key = spec.id.split("/")[1]
   222|                 if getter_key in self.keys:
   223|                     raise ValueError(

# --- HUNK 4: Lines 270-294 ---
   270|         _value: Optional[SetValue] = None,
   271|         /,
   272|         **kwargs: SetValue,
   273|     ) -> ContextSet:
   274|         return ContextSet(_key, _value, prefix="", **kwargs)
   275| class PrefixContext:
   276|     """Context for a runnable with a prefix."""
   277|     prefix: str = ""
   278|     def __init__(self, prefix: str = ""):
   279|         self.prefix = prefix
   280|     def getter(self, key: Union[str, List[str]], /) -> ContextGet:
   281|         return ContextGet(key=key, prefix=self.prefix)
   282|     def setter(
   283|         self,
   284|         _key: Optional[str] = None,
   285|         _value: Optional[SetValue] = None,
   286|         /,
   287|         **kwargs: SetValue,
   288|     ) -> ContextSet:
   289|         return ContextSet(_key, _value, prefix=self.prefix, **kwargs)
   290| def _print_keys(keys: Union[str, Sequence[str]]) -> str:
   291|     if isinstance(keys, str):
   292|         return f"'{keys}'"
   293|     else:
   294|         return ", ".join(f"'{k}'" for k in keys)


# ====================================================================
# FILE: libs/core/langchain_core/callbacks/manager.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-28 ---
     1| from __future__ import annotations
     2| import asyncio
     3| import functools
     4| import logging
     5| import uuid
     6| from concurrent.futures import ThreadPoolExecutor
     7| from contextlib import asynccontextmanager, contextmanager
     8| from contextvars import Context, copy_context
     9| from typing import (
    10|     TYPE_CHECKING,
    11|     Any,
    12|     AsyncGenerator,
    13|     Coroutine,
    14|     Dict,
    15|     Generator,
    16|     List,
    17|     Optional,
    18|     Sequence,
    19|     Type,
    20|     TypeVar,
    21|     Union,
    22|     cast,
    23| )
    24| from uuid import UUID
    25| from langsmith.run_helpers import get_run_tree_context
    26| from tenacity import RetryCallState
    27| from langchain_core.callbacks.base import (
    28|     BaseCallbackHandler,

# --- HUNK 2: Lines 217-360 ---
   217|                     handler_name = handler.__class__.__name__
   218|                     logger.warning(
   219|                         f"NotImplementedError in {handler_name}.{event_name}"
   220|                         f" callback: {repr(e)}"
   221|                     )
   222|             except Exception as e:
   223|                 logger.warning(
   224|                     f"Error in {handler.__class__.__name__}.{event_name} callback:"
   225|                     f" {repr(e)}"
   226|                 )
   227|                 if handler.raise_error:
   228|                     raise e
   229|     finally:
   230|         if coros:
   231|             try:
   232|                 asyncio.get_running_loop()
   233|                 loop_running = True
   234|             except RuntimeError:
   235|                 loop_running = False
   236|             if loop_running:
   237|                 with _executor_w_context(1) as executor:
   238|                     executor.submit(_run_coros, coros).result()
   239|             else:
   240|                 _run_coros(coros)
   241| def _set_context(context: Context) -> None:
   242|     for var, value in context.items():
   243|         var.set(value)
   244| def _executor_w_context(max_workers: Optional[int] = None) -> ThreadPoolExecutor:
   245|     return ThreadPoolExecutor(
   246|         max_workers=max_workers,
   247|         initializer=_set_context,
   248|         initargs=(copy_context(),),
   249|     )
   250| def _run_coros(coros: List[Coroutine[Any, Any, Any]]) -> None:
   251|     if hasattr(asyncio, "Runner"):
   252|         with asyncio.Runner() as runner:
   253|             for coro in coros:
   254|                 runner.run(coro)
   255|             while pending := asyncio.all_tasks(runner.get_loop()):
   256|                 runner.run(asyncio.wait(pending))
   257|     else:
   258|         for coro in coros:
   259|             asyncio.run(coro)
   260| async def _ahandle_event_for_handler(
   261|     executor: ThreadPoolExecutor,
   262|     handler: BaseCallbackHandler,
   263|     event_name: str,
   264|     ignore_condition_name: Optional[str],
   265|     *args: Any,
   266|     **kwargs: Any,
   267| ) -> None:
   268|     try:
   269|         if ignore_condition_name is None or not getattr(handler, ignore_condition_name):
   270|             event = getattr(handler, event_name)
   271|             if asyncio.iscoroutinefunction(event):
   272|                 await event(*args, **kwargs)
   273|             else:
   274|                 if handler.run_inline:
   275|                     event(*args, **kwargs)
   276|                 else:
   277|                     await asyncio.get_event_loop().run_in_executor(
   278|                         executor, functools.partial(event, *args, **kwargs)
   279|                     )
   280|     except NotImplementedError as e:
   281|         if event_name == "on_chat_model_start":
   282|             message_strings = [get_buffer_string(m) for m in args[1]]
   283|             await _ahandle_event_for_handler(
   284|                 executor,
   285|                 handler,
   286|                 "on_llm_start",
   287|                 "ignore_llm",
   288|                 args[0],
   289|                 message_strings,
   290|                 *args[2:],
   291|                 **kwargs,
   292|             )
   293|         else:
   294|             logger.warning(
   295|                 f"NotImplementedError in {handler.__class__.__name__}.{event_name}"
   296|                 f" callback: {repr(e)}"
   297|             )
   298|     except Exception as e:
   299|         logger.warning(
   300|             f"Error in {handler.__class__.__name__}.{event_name} callback:"
   301|             f" {repr(e)}"
   302|         )
   303|         if handler.raise_error:
   304|             raise e
   305| async def ahandle_event(
   306|     handlers: List[BaseCallbackHandler],
   307|     event_name: str,
   308|     ignore_condition_name: Optional[str],
   309|     *args: Any,
   310|     **kwargs: Any,
   311| ) -> None:
   312|     """Generic event handler for AsyncCallbackManager.
   313|     Note: This function is used by langserve to handle events.
   314|     Args:
   315|         handlers: The list of handlers that will handle the event
   316|         event_name: The name of the event (e.g., "on_llm_start")
   317|         ignore_condition_name: Name of the attribute defined on handler
   318|             that if True will cause the handler to be skipped for the given event
   319|         *args: The arguments to pass to the event handler
   320|         **kwargs: The keyword arguments to pass to the event handler
   321|     """
   322|     with _executor_w_context() as executor:
   323|         for handler in [h for h in handlers if h.run_inline]:
   324|             await _ahandle_event_for_handler(
   325|                 executor, handler, event_name, ignore_condition_name, *args, **kwargs
   326|             )
   327|         await asyncio.gather(
   328|             *(
   329|                 _ahandle_event_for_handler(
   330|                     executor,
   331|                     handler,
   332|                     event_name,
   333|                     ignore_condition_name,
   334|                     *args,
   335|                     **kwargs,
   336|                 )
   337|                 for handler in handlers
   338|                 if not handler.run_inline
   339|             )
   340|         )
   341| BRM = TypeVar("BRM", bound="BaseRunManager")
   342| class BaseRunManager(RunManagerMixin):
   343|     """Base class for run manager (a bound callback manager)."""
   344|     def __init__(
   345|         self,
   346|         *,
   347|         run_id: UUID,
   348|         handlers: List[BaseCallbackHandler],
   349|         inheritable_handlers: List[BaseCallbackHandler],
   350|         parent_run_id: Optional[UUID] = None,
   351|         tags: Optional[List[str]] = None,
   352|         inheritable_tags: Optional[List[str]] = None,
   353|         metadata: Optional[Dict[str, Any]] = None,
   354|         inheritable_metadata: Optional[Dict[str, Any]] = None,
   355|     ) -> None:
   356|         """Initialize the run manager.
   357|         Args:
   358|             run_id (UUID): The ID of the run.
   359|             handlers (List[BaseCallbackHandler]): The list of handlers.
   360|             inheritable_handlers (List[BaseCallbackHandler]):


# ====================================================================
# FILE: libs/core/langchain_core/chat_history.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-62 ---
     1| from __future__ import annotations
     2| from abc import ABC, abstractmethod
     3| from typing import List, Union
     4| from langchain_core.messages import (
     5|     AIMessage,
     6|     BaseMessage,
     7|     HumanMessage,
     8|     get_buffer_string,
     9| )
    10| class BaseChatMessageHistory(ABC):
    11|     """Abstract base class for storing chat message history.
    12|     See `ChatMessageHistory` for default implementation.
    13|     Example:
    14|         .. code-block:: python
    15|             class FileChatMessageHistory(BaseChatMessageHistory):
    16|                 storage_path:  str
    17|                 session_id: str
    18|                @property
    19|                def messages(self):
    20|                    with open(os.path.join(storage_path, session_id), 'r:utf-8') as f:
    21|                        messages = json.loads(f.read())
    22|                     return messages_from_dict(messages)
    23|                def add_message(self, message: BaseMessage) -> None:
    24|                    messages = self.messages.append(_message_to_dict(message))
    25|                    with open(os.path.join(storage_path, session_id), 'w') as f:
    26|                        json.dump(f, messages)
    27|                def clear(self):
    28|                    with open(os.path.join(storage_path, session_id), 'w') as f:
    29|                        f.write("[]")
    30|     """
    31|     messages: List[BaseMessage]
    32|     """A list of Messages stored in-memory."""
    33|     def add_user_message(self, message: Union[HumanMessage, str]) -> None:
    34|         """Convenience method for adding a human message string to the store.
    35|         Args:
    36|             message: The human message to add
    37|         """
    38|         if isinstance(message, HumanMessage):
    39|             self.add_message(message)
    40|         else:
    41|             self.add_message(HumanMessage(content=message))
    42|     def add_ai_message(self, message: Union[AIMessage, str]) -> None:
    43|         """Convenience method for adding an AI message string to the store.
    44|         Args:
    45|             message: The AI message to add.
    46|         """
    47|         if isinstance(message, AIMessage):
    48|             self.add_message(message)
    49|         else:
    50|             self.add_message(AIMessage(content=message))
    51|     @abstractmethod
    52|     def add_message(self, message: BaseMessage) -> None:
    53|         """Add a Message object to the store.
    54|         Args:
    55|             message: A BaseMessage object to store.
    56|         """
    57|         raise NotImplementedError()
    58|     @abstractmethod
    59|     def clear(self) -> None:
    60|         """Remove all messages from the store"""
    61|     def __str__(self) -> str:
    62|         return get_buffer_string(self.messages)


# ====================================================================
# FILE: libs/core/langchain_core/env.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-13 ---
     1| import platform
     2| from functools import lru_cache
     3| @lru_cache(maxsize=1)
     4| def get_runtime_environment() -> dict:
     5|     """Get information about the LangChain runtime environment."""
     6|     from langchain_core import __version__
     7|     return {
     8|         "library_version": __version__,
     9|         "library": "langchain-core",
    10|         "platform": platform.platform(),
    11|         "runtime": "python",
    12|         "runtime_version": platform.python_version(),
    13|     }


# ====================================================================
# FILE: libs/core/langchain_core/example_selectors/base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-11 ---
     1| """Interface for selecting examples to include in prompts."""
     2| from abc import ABC, abstractmethod
     3| from typing import Any, Dict, List
     4| class BaseExampleSelector(ABC):
     5|     """Interface for selecting examples to include in prompts."""
     6|     @abstractmethod
     7|     def add_example(self, example: Dict[str, str]) -> Any:
     8|         """Add new example to store."""
     9|     @abstractmethod
    10|     def select_examples(self, input_variables: Dict[str, str]) -> List[dict]:
    11|         """Select which examples to use based on the inputs."""


# ====================================================================
# FILE: libs/core/langchain_core/language_models/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-20 ---
     1| from langchain_core.language_models.base import (
     2|     BaseLanguageModel,
     3|     LanguageModelInput,
     4|     LanguageModelLike,
     5|     LanguageModelOutput,
     6|     get_tokenizer,
     7| )
     8| from langchain_core.language_models.chat_models import BaseChatModel, SimpleChatModel
     9| from langchain_core.language_models.llms import LLM, BaseLLM
    10| __all__ = [
    11|     "BaseLanguageModel",
    12|     "BaseChatModel",
    13|     "SimpleChatModel",
    14|     "BaseLLM",
    15|     "LLM",
    16|     "LanguageModelInput",
    17|     "get_tokenizer",
    18|     "LanguageModelOutput",
    19|     "LanguageModelLike",
    20| ]


# ====================================================================
# FILE: libs/core/langchain_core/language_models/base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-62 ---
     1| from __future__ import annotations
     2| from abc import ABC, abstractmethod
     3| from functools import lru_cache
     4| from typing import (
     5|     TYPE_CHECKING,
     6|     Any,
     7|     List,
     8|     Optional,
     9|     Sequence,
    10|     Set,
    11|     TypeVar,
    12|     Union,
    13| )
    14| from typing_extensions import TypeAlias
    15| from langchain_core.messages import AnyMessage, BaseMessage, get_buffer_string
    16| from langchain_core.prompt_values import PromptValue
    17| from langchain_core.runnables import Runnable, RunnableSerializable
    18| from langchain_core.utils import get_pydantic_field_names
    19| if TYPE_CHECKING:
    20|     from langchain_core.callbacks import Callbacks
    21|     from langchain_core.outputs import LLMResult
    22| @lru_cache(maxsize=None)  # Cache the tokenizer
    23| def get_tokenizer() -> Any:
    24|     try:
    25|         from transformers import GPT2TokenizerFast  # type: ignore[import]
    26|     except ImportError:
    27|         raise ImportError(
    28|             "Could not import transformers python package. "
    29|             "This is needed in order to calculate get_token_ids. "
    30|             "Please install it with `pip install transformers`."
    31|         )
    32|     return GPT2TokenizerFast.from_pretrained("gpt2")
    33| def _get_token_ids_default_method(text: str) -> List[int]:
    34|     """Encode the text into token IDs."""
    35|     tokenizer = get_tokenizer()
    36|     return tokenizer.encode(text)
    37| LanguageModelInput = Union[PromptValue, str, List[BaseMessage]]
    38| LanguageModelOutput = Union[BaseMessage, str]
    39| LanguageModelLike = Runnable[LanguageModelInput, LanguageModelOutput]
    40| LanguageModelOutputVar = TypeVar("LanguageModelOutputVar", BaseMessage, str)
    41| class BaseLanguageModel(
    42|     RunnableSerializable[LanguageModelInput, LanguageModelOutputVar], ABC
    43| ):
    44|     """Abstract base class for interfacing with language models.
    45|     All language model wrappers inherit from BaseLanguageModel.
    46|     Exposes three main methods:
    47|     - generate_prompt: generate language model outputs for a sequence of prompt
    48|         values. A prompt value is a model input that can be converted to any language
    49|         model input format (string or messages).
    50|     - predict: pass in a single string to a language model and return a string
    51|         prediction.
    52|     - predict_messages: pass in a sequence of BaseMessages (corresponding to a single
    53|         model call) to a language model and return a BaseMessage prediction.
    54|     Each of these has an equivalent asynchronous method.
    55|     """
    56|     @property
    57|     def InputType(self) -> TypeAlias:
    58|         """Get the input type for this runnable."""
    59|         from langchain_core.prompt_values import (
    60|             ChatPromptValueConcrete,
    61|             StringPromptValue,
    62|         )


# ====================================================================
# FILE: libs/core/langchain_core/language_models/chat_models.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 215-256 ---
   215|             except BaseException as e:
   216|                 run_manager.on_llm_error(
   217|                     e,
   218|                     response=LLMResult(
   219|                         generations=[[generation]] if generation else []
   220|                     ),
   221|                 )
   222|                 raise e
   223|             else:
   224|                 run_manager.on_llm_end(LLMResult(generations=[[generation]]))
   225|     async def astream(
   226|         self,
   227|         input: LanguageModelInput,
   228|         config: Optional[RunnableConfig] = None,
   229|         *,
   230|         stop: Optional[List[str]] = None,
   231|         **kwargs: Any,
   232|     ) -> AsyncIterator[BaseMessageChunk]:
   233|         if type(self)._astream == BaseChatModel._astream:
   234|             yield cast(
   235|                 BaseMessageChunk,
   236|                 await self.ainvoke(input, config=config, stop=stop, **kwargs),
   237|             )
   238|         else:
   239|             config = config or {}
   240|             messages = self._convert_input(input).to_messages()
   241|             params = self._get_invocation_params(stop=stop, **kwargs)
   242|             options = {"stop": stop, **kwargs}
   243|             callback_manager = AsyncCallbackManager.configure(
   244|                 config.get("callbacks"),
   245|                 self.callbacks,
   246|                 self.verbose,
   247|                 config.get("tags"),
   248|                 self.tags,
   249|                 config.get("metadata"),
   250|                 self.metadata,
   251|             )
   252|             (run_manager,) = await callback_manager.on_chat_model_start(
   253|                 dumpd(self),
   254|                 [messages],
   255|                 invocation_params=params,
   256|                 options=options,


# ====================================================================
# FILE: libs/core/langchain_core/output_parsers/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-33 ---
     1| from langchain_core.output_parsers.base import (
     2|     BaseGenerationOutputParser,
     3|     BaseLLMOutputParser,
     4|     BaseOutputParser,
     5| )
     6| from langchain_core.output_parsers.json import JsonOutputParser, SimpleJsonOutputParser
     7| from langchain_core.output_parsers.list import (
     8|     CommaSeparatedListOutputParser,
     9|     ListOutputParser,
    10|     MarkdownListOutputParser,
    11|     NumberedListOutputParser,
    12| )
    13| from langchain_core.output_parsers.string import StrOutputParser
    14| from langchain_core.output_parsers.transform import (
    15|     BaseCumulativeTransformOutputParser,
    16|     BaseTransformOutputParser,
    17| )
    18| from langchain_core.output_parsers.xml import XMLOutputParser
    19| __all__ = [
    20|     "BaseLLMOutputParser",
    21|     "BaseGenerationOutputParser",
    22|     "BaseOutputParser",
    23|     "ListOutputParser",
    24|     "CommaSeparatedListOutputParser",
    25|     "NumberedListOutputParser",
    26|     "MarkdownListOutputParser",
    27|     "StrOutputParser",
    28|     "BaseTransformOutputParser",
    29|     "BaseCumulativeTransformOutputParser",
    30|     "SimpleJsonOutputParser",
    31|     "XMLOutputParser",
    32|     "JsonOutputParser",
    33| ]


# ====================================================================
# FILE: libs/core/langchain_core/output_parsers/base.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 58-98 ---
    58|         self, input: Union[str, BaseMessage], config: Optional[RunnableConfig] = None
    59|     ) -> T:
    60|         if isinstance(input, BaseMessage):
    61|             return self._call_with_config(
    62|                 lambda inner_input: self.parse_result(
    63|                     [ChatGeneration(message=inner_input)]
    64|                 ),
    65|                 input,
    66|                 config,
    67|                 run_type="parser",
    68|             )
    69|         else:
    70|             return self._call_with_config(
    71|                 lambda inner_input: self.parse_result([Generation(text=inner_input)]),
    72|                 input,
    73|                 config,
    74|                 run_type="parser",
    75|             )
    76|     async def ainvoke(
    77|         self,
    78|         input: Union[str, BaseMessage],
    79|         config: Optional[RunnableConfig] = None,
    80|         **kwargs: Optional[Any],
    81|     ) -> T:
    82|         if isinstance(input, BaseMessage):
    83|             return await self._acall_with_config(
    84|                 lambda inner_input: self.aparse_result(
    85|                     [ChatGeneration(message=inner_input)]
    86|                 ),
    87|                 input,
    88|                 config,
    89|                 run_type="parser",
    90|             )
    91|         else:
    92|             return await self._acall_with_config(
    93|                 lambda inner_input: self.aparse_result([Generation(text=inner_input)]),
    94|                 input,
    95|                 config,
    96|                 run_type="parser",
    97|             )
    98| class BaseOutputParser(

# --- HUNK 2: Lines 135-175 ---
   135|         self, input: Union[str, BaseMessage], config: Optional[RunnableConfig] = None
   136|     ) -> T:
   137|         if isinstance(input, BaseMessage):
   138|             return self._call_with_config(
   139|                 lambda inner_input: self.parse_result(
   140|                     [ChatGeneration(message=inner_input)]
   141|                 ),
   142|                 input,
   143|                 config,
   144|                 run_type="parser",
   145|             )
   146|         else:
   147|             return self._call_with_config(
   148|                 lambda inner_input: self.parse_result([Generation(text=inner_input)]),
   149|                 input,
   150|                 config,
   151|                 run_type="parser",
   152|             )
   153|     async def ainvoke(
   154|         self,
   155|         input: Union[str, BaseMessage],
   156|         config: Optional[RunnableConfig] = None,
   157|         **kwargs: Optional[Any],
   158|     ) -> T:
   159|         if isinstance(input, BaseMessage):
   160|             return await self._acall_with_config(
   161|                 lambda inner_input: self.aparse_result(
   162|                     [ChatGeneration(message=inner_input)]
   163|                 ),
   164|                 input,
   165|                 config,
   166|                 run_type="parser",
   167|             )
   168|         else:
   169|             return await self._acall_with_config(
   170|                 lambda inner_input: self.aparse_result([Generation(text=inner_input)]),
   171|                 input,
   172|                 config,
   173|                 run_type="parser",
   174|             )
   175|     def parse_result(self, result: List[Generation], *, partial: bool = False) -> T:


# ====================================================================
# FILE: libs/core/langchain_core/output_parsers/format_instructions.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-7 ---
     1| JSON_FORMAT_INSTRUCTIONS = """The output should be formatted as a JSON instance that conforms to the JSON schema below.
     2| As an example, for the schema {{"properties": {{"foo": {{"title": "Foo", "description": "a list of strings", "type": "array", "items": {{"type": "string"}}}}}}, "required": ["foo"]}}
     3| the object {{"foo": ["bar", "baz"]}} is a well-formatted instance of the schema. The object {{"properties": {{"foo": ["bar", "baz"]}}}} is not well-formatted.
     4| Here is the output schema:
     5| ```
     6| {schema}
     7| ```"""


# ====================================================================
# FILE: libs/core/langchain_core/output_parsers/json.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-153 ---
     1| from __future__ import annotations
     2| import json
     3| import re
     4| from json import JSONDecodeError
     5| from typing import Any, Callable, List, Optional, Type
     6| import jsonpatch  # type: ignore[import]
     7| from langchain_core.exceptions import OutputParserException
     8| from langchain_core.output_parsers.format_instructions import JSON_FORMAT_INSTRUCTIONS
     9| from langchain_core.output_parsers.transform import BaseCumulativeTransformOutputParser
    10| from langchain_core.pydantic_v1 import BaseModel
    11| def _replace_new_line(match: re.Match[str]) -> str:
    12|     value = match.group(2)
    13|     value = re.sub(r"\n", r"\\n", value)
    14|     value = re.sub(r"\r", r"\\r", value)
    15|     value = re.sub(r"\t", r"\\t", value)
    16|     value = re.sub(r'(?<!\\)"', r"\"", value)
    17|     return match.group(1) + value + match.group(3)
    18| def _custom_parser(multiline_string: str) -> str:
    19|     """
    20|     The LLM response for `action_input` may be a multiline
    21|     string containing unescaped newlines, tabs or quotes. This function
    22|     replaces those characters with their escaped counterparts.
    23|     (newlines in JSON must be double-escaped: `\\n`)
    24|     """
    25|     if isinstance(multiline_string, (bytes, bytearray)):
    26|         multiline_string = multiline_string.decode()
    27|     multiline_string = re.sub(
    28|         r'("action_input"\:\s*")(.*)(")',
    29|         _replace_new_line,
    30|         multiline_string,
    31|         flags=re.DOTALL,
    32|     )
    33|     return multiline_string
    34| def parse_partial_json(s: str, *, strict: bool = False) -> Any:
    35|     """Parse a JSON string that may be missing closing braces.
    36|     Args:
    37|         s: The JSON string to parse.
    38|         strict: Whether to use strict parsing. Defaults to False.
    39|     Returns:
    40|         The parsed JSON object as a Python dictionary.
    41|     """
    42|     try:
    43|         return json.loads(s, strict=strict)
    44|     except json.JSONDecodeError:
    45|         pass
    46|     new_s = ""
    47|     stack = []
    48|     is_inside_string = False
    49|     escaped = False
    50|     for char in s:
    51|         if is_inside_string:
    52|             if char == '"' and not escaped:
    53|                 is_inside_string = False
    54|             elif char == "\n" and not escaped:
    55|                 char = "\\n"  # Replace the newline character with the escape sequence.
    56|             elif char == "\\":
    57|                 escaped = not escaped
    58|             else:
    59|                 escaped = False
    60|         else:
    61|             if char == '"':
    62|                 is_inside_string = True
    63|                 escaped = False
    64|             elif char == "{":
    65|                 stack.append("}")
    66|             elif char == "[":
    67|                 stack.append("]")
    68|             elif char == "}" or char == "]":
    69|                 if stack and stack[-1] == char:
    70|                     stack.pop()
    71|                 else:
    72|                     return None
    73|         new_s += char
    74|     if is_inside_string:
    75|         new_s += '"'
    76|     for closing_char in reversed(stack):
    77|         new_s += closing_char
    78|     try:
    79|         return json.loads(new_s, strict=strict)
    80|     except json.JSONDecodeError:
    81|         return None
    82| def parse_json_markdown(
    83|     json_string: str, *, parser: Callable[[str], Any] = parse_partial_json
    84| ) -> dict:
    85|     """
    86|     Parse a JSON string from a Markdown string.
    87|     Args:
    88|         json_string: The Markdown string.
    89|     Returns:
    90|         The parsed JSON object as a Python dictionary.
    91|     """
    92|     match = re.search(r"```(json)?(.*)```", json_string, re.DOTALL)
    93|     if match is None:
    94|         json_str = json_string
    95|     else:
    96|         json_str = match.group(2)
    97|     json_str = json_str.strip()
    98|     json_str = _custom_parser(json_str)
    99|     parsed = parser(json_str)
   100|     return parsed
   101| def parse_and_check_json_markdown(text: str, expected_keys: List[str]) -> dict:
   102|     """
   103|     Parse a JSON string from a Markdown string and check that it
   104|     contains the expected keys.
   105|     Args:
   106|         text: The Markdown string.
   107|         expected_keys: The expected keys in the JSON string.
   108|     Returns:
   109|         The parsed JSON object as a Python dictionary.
   110|     """
   111|     try:
   112|         json_obj = parse_json_markdown(text)
   113|     except json.JSONDecodeError as e:
   114|         raise OutputParserException(f"Got invalid JSON object. Error: {e}")
   115|     for key in expected_keys:
   116|         if key not in json_obj:
   117|             raise OutputParserException(
   118|                 f"Got invalid return object. Expected key `{key}` "
   119|                 f"to be present, but got {json_obj}"
   120|             )
   121|     return json_obj
   122| class JsonOutputParser(BaseCumulativeTransformOutputParser[Any]):
   123|     """Parse the output of an LLM call to a JSON object.
   124|     When used in streaming mode, it will yield partial JSON objects containing
   125|     all the keys that have been returned so far.
   126|     In streaming, if `diff` is set to `True`, yields JSONPatch operations
   127|     describing the difference between the previous and the current object.
   128|     """
   129|     pydantic_object: Optional[Type[BaseModel]] = None
   130|     def _diff(self, prev: Optional[Any], next: Any) -> Any:
   131|         return jsonpatch.make_patch(prev, next).patch
   132|     def parse(self, text: str) -> Any:
   133|         text = text.strip()
   134|         try:
   135|             return parse_json_markdown(text.strip())
   136|         except JSONDecodeError as e:
   137|             raise OutputParserException(f"Invalid json output: {text}") from e
   138|     def get_format_instructions(self) -> str:
   139|         if self.pydantic_object is None:
   140|             return "Return a JSON object."
   141|         else:
   142|             schema = self.pydantic_object.schema()
   143|             reduced_schema = schema
   144|             if "title" in reduced_schema:
   145|                 del reduced_schema["title"]
   146|             if "type" in reduced_schema:
   147|                 del reduced_schema["type"]
   148|             schema_str = json.dumps(reduced_schema)
   149|             return JSON_FORMAT_INSTRUCTIONS.format(schema=schema_str)
   150|     @property
   151|     def _type(self) -> str:
   152|         return "simple_json_output_parser"
   153| SimpleJsonOutputParser = JsonOutputParser


# ====================================================================
# FILE: libs/core/langchain_core/output_parsers/list.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-127 ---
     1| from __future__ import annotations
     2| import re
     3| from abc import abstractmethod
     4| from collections import deque
     5| from typing import AsyncIterator, Deque, Iterator, List, TypeVar, Union
     6| from langchain_core.messages import BaseMessage
     7| from langchain_core.output_parsers.transform import BaseTransformOutputParser
     8| T = TypeVar("T")
     9| def droplastn(iter: Iterator[T], n: int) -> Iterator[T]:
    10|     """Drop the last n elements of an iterator."""
    11|     buffer: Deque[T] = deque()
    12|     for item in iter:
    13|         buffer.append(item)
    14|         if len(buffer) > n:
    15|             yield buffer.popleft()
    16| class ListOutputParser(BaseTransformOutputParser[List[str]]):
    17|     """Parse the output of an LLM call to a list."""
    18|     @property
    19|     def _type(self) -> str:
    20|         return "list"
    21|     @abstractmethod
    22|     def parse(self, text: str) -> List[str]:
    23|         """Parse the output of an LLM call."""
    24|     def parse_iter(self, text: str) -> Iterator[re.Match]:
    25|         """Parse the output of an LLM call."""
    26|         raise NotImplementedError
    27|     def _transform(
    28|         self, input: Iterator[Union[str, BaseMessage]]
    29|     ) -> Iterator[List[str]]:
    30|         buffer = ""
    31|         for chunk in input:
    32|             if isinstance(chunk, BaseMessage):
    33|                 chunk_content = chunk.content
    34|                 if not isinstance(chunk_content, str):
    35|                     continue
    36|                 chunk = chunk_content
    37|             buffer += chunk
    38|             try:
    39|                 done_idx = 0
    40|                 for m in droplastn(self.parse_iter(buffer), 1):
    41|                     done_idx = m.end()
    42|                     yield [m.group(1)]
    43|                 buffer = buffer[done_idx:]
    44|             except NotImplementedError:
    45|                 parts = self.parse(buffer)
    46|                 if len(parts) > 1:
    47|                     for part in parts[:-1]:
    48|                         yield [part]
    49|                     buffer = parts[-1]
    50|         for part in self.parse(buffer):
    51|             yield [part]
    52|     async def _atransform(
    53|         self, input: AsyncIterator[Union[str, BaseMessage]]
    54|     ) -> AsyncIterator[List[str]]:
    55|         buffer = ""
    56|         async for chunk in input:
    57|             if isinstance(chunk, BaseMessage):
    58|                 chunk_content = chunk.content
    59|                 if not isinstance(chunk_content, str):
    60|                     continue
    61|                 chunk = chunk_content
    62|             buffer += chunk
    63|             try:
    64|                 done_idx = 0
    65|                 for m in droplastn(self.parse_iter(buffer), 1):
    66|                     done_idx = m.end()
    67|                     yield [m.group(1)]
    68|                 buffer = buffer[done_idx:]
    69|             except NotImplementedError:
    70|                 parts = self.parse(buffer)
    71|                 if len(parts) > 1:
    72|                     for part in parts[:-1]:
    73|                         yield [part]
    74|                     buffer = parts[-1]
    75|         for part in self.parse(buffer):
    76|             yield [part]
    77| class CommaSeparatedListOutputParser(ListOutputParser):
    78|     """Parse the output of an LLM call to a comma-separated list."""
    79|     @classmethod
    80|     def is_lc_serializable(cls) -> bool:
    81|         return True
    82|     @classmethod
    83|     def get_lc_namespace(cls) -> List[str]:
    84|         """Get the namespace of the langchain object."""
    85|         return ["langchain", "output_parsers", "list"]
    86|     def get_format_instructions(self) -> str:
    87|         return (
    88|             "Your response should be a list of comma separated values, "
    89|             "eg: `foo, bar, baz`"
    90|         )
    91|     def parse(self, text: str) -> List[str]:
    92|         """Parse the output of an LLM call."""
    93|         return text.strip().split(", ")
    94|     @property
    95|     def _type(self) -> str:
    96|         return "comma-separated-list"
    97| class NumberedListOutputParser(ListOutputParser):
    98|     """Parse a numbered list."""
    99|     pattern = r"\d+\.\s([^\n]+)"
   100|     def get_format_instructions(self) -> str:
   101|         return (
   102|             "Your response should be a numbered list with each item on a new line. "
   103|             "For example: \n\n1. foo\n\n2. bar\n\n3. baz"
   104|         )
   105|     def parse(self, text: str) -> List[str]:
   106|         """Parse the output of an LLM call."""
   107|         return re.findall(self.pattern, text)
   108|     def parse_iter(self, text: str) -> Iterator[re.Match]:
   109|         """Parse the output of an LLM call."""
   110|         return re.finditer(self.pattern, text)
   111|     @property
   112|     def _type(self) -> str:
   113|         return "numbered-list"
   114| class MarkdownListOutputParser(ListOutputParser):
   115|     """Parse a markdown list."""
   116|     pattern = r"-\s([^\n]+)"
   117|     def get_format_instructions(self) -> str:
   118|         return "Your response should be a markdown list, " "eg: `- foo\n- bar\n- baz`"
   119|     def parse(self, text: str) -> List[str]:
   120|         """Parse the output of an LLM call."""
   121|         return re.findall(self.pattern, text)
   122|     def parse_iter(self, text: str) -> Iterator[re.Match]:
   123|         """Parse the output of an LLM call."""
   124|         return re.finditer(self.pattern, text)
   125|     @property
   126|     def _type(self) -> str:
   127|         return "markdown-list"


# ====================================================================
# FILE: libs/core/langchain_core/output_parsers/xml.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-119 ---
     1| import re
     2| import xml.etree.ElementTree as ET
     3| from typing import Any, AsyncIterator, Dict, Iterator, List, Optional, Union
     4| from langchain_core.messages import BaseMessage
     5| from langchain_core.output_parsers.transform import BaseTransformOutputParser
     6| from langchain_core.runnables.utils import AddableDict
     7| XML_FORMAT_INSTRUCTIONS = """The output should be formatted as a XML file.
     8| 1. Output should conform to the tags below. 
     9| 2. If tags are not given, make them on your own.
    10| 3. Remember to always open and close all the tags.
    11| As an example, for the tags ["foo", "bar", "baz"]:
    12| 1. String "<foo>\n   <bar>\n      <baz></baz>\n   </bar>\n</foo>" is a well-formatted instance of the schema. 
    13| 2. String "<foo>\n   <bar>\n   </foo>" is a badly-formatted instance.
    14| 3. String "<foo>\n   <tag>\n   </tag>\n</foo>" is a badly-formatted instance.
    15| Here are the output tags:
    16| ```
    17| {tags}
    18| ```"""  # noqa: E501
    19| class XMLOutputParser(BaseTransformOutputParser):
    20|     """Parse an output using xml format."""
    21|     tags: Optional[List[str]] = None
    22|     encoding_matcher: re.Pattern = re.compile(
    23|         r"<([^>]*encoding[^>]*)>\n(.*)", re.MULTILINE | re.DOTALL
    24|     )
    25|     def get_format_instructions(self) -> str:
    26|         return XML_FORMAT_INSTRUCTIONS.format(tags=self.tags)
    27|     def parse(self, text: str) -> Dict[str, List[Any]]:
    28|         match = re.search(r"```(xml)?(.*)```", text, re.DOTALL)
    29|         if match is not None:
    30|             text = match.group(2)
    31|         encoding_match = self.encoding_matcher.search(text)
    32|         if encoding_match:
    33|             text = encoding_match.group(2)
    34|         text = text.strip()
    35|         if (text.startswith("<") or text.startswith("\n<")) and (
    36|             text.endswith(">") or text.endswith(">\n")
    37|         ):
    38|             root = ET.fromstring(text)
    39|             return self._root_to_dict(root)
    40|         else:
    41|             raise ValueError(f"Could not parse output: {text}")
    42|     def _transform(
    43|         self, input: Iterator[Union[str, BaseMessage]]
    44|     ) -> Iterator[AddableDict]:
    45|         xml_start_re = re.compile(r"<[a-zA-Z:_]")
    46|         parser = ET.XMLPullParser(["start", "end"])
    47|         xml_started = False
    48|         current_path: List[str] = []
    49|         current_path_has_children = False
    50|         buffer = ""
    51|         for chunk in input:
    52|             if isinstance(chunk, BaseMessage):
    53|                 chunk_content = chunk.content
    54|                 if not isinstance(chunk_content, str):
    55|                     continue
    56|                 chunk = chunk_content
    57|             buffer += chunk
    58|             if not xml_started:
    59|                 if match := xml_start_re.search(buffer):
    60|                     buffer = buffer[match.start() :]
    61|                     xml_started = True
    62|                 else:
    63|                     continue
    64|             parser.feed(buffer)
    65|             buffer = ""
    66|             for event, elem in parser.read_events():
    67|                 if event == "start":
    68|                     current_path.append(elem.tag)
    69|                     current_path_has_children = False
    70|                 elif event == "end":
    71|                     current_path.pop()
    72|                     if not current_path_has_children:
    73|                         yield nested_element(current_path, elem)
    74|                     if current_path:
    75|                         current_path_has_children = True
    76|                     else:
    77|                         xml_started = False
    78|         parser.close()
    79|     async def _atransform(
    80|         self, input: AsyncIterator[Union[str, BaseMessage]]
    81|     ) -> AsyncIterator[AddableDict]:
    82|         parser = ET.XMLPullParser(["start", "end"])
    83|         current_path: List[str] = []
    84|         current_path_has_children = False
    85|         async for chunk in input:
    86|             if isinstance(chunk, BaseMessage):
    87|                 chunk_content = chunk.content
    88|                 if not isinstance(chunk_content, str):
    89|                     continue
    90|                 chunk = chunk_content
    91|             parser.feed(chunk)
    92|             for event, elem in parser.read_events():
    93|                 if event == "start":
    94|                     current_path.append(elem.tag)
    95|                     current_path_has_children = False
    96|                 elif event == "end":
    97|                     current_path.pop()
    98|                     if not current_path_has_children:
    99|                         yield nested_element(current_path, elem)
   100|                     current_path_has_children = True
   101|         parser.close()
   102|     def _root_to_dict(self, root: ET.Element) -> Dict[str, List[Any]]:
   103|         """Converts xml tree to python dictionary."""
   104|         result: Dict[str, List[Any]] = {root.tag: []}
   105|         for child in root:
   106|             if len(child) == 0:
   107|                 result[root.tag].append({child.tag: child.text})
   108|             else:
   109|                 result[root.tag].append(self._root_to_dict(child))
   110|         return result
   111|     @property
   112|     def _type(self) -> str:
   113|         return "xml"
   114| def nested_element(path: List[str], elem: ET.Element) -> Any:
   115|     """Get nested element from path."""
   116|     if len(path) == 0:
   117|         return AddableDict({elem.tag: elem.text})
   118|     else:
   119|         return AddableDict({path[0]: [nested_element(path[1:], elem)]})


# ====================================================================
# FILE: libs/core/langchain_core/prompts/base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 41-105 ---
    41|         """Get the namespace of the langchain object."""
    42|         return ["langchain", "schema", "prompt_template"]
    43|     @classmethod
    44|     def is_lc_serializable(cls) -> bool:
    45|         """Return whether this class is serializable."""
    46|         return True
    47|     class Config:
    48|         """Configuration for this pydantic object."""
    49|         arbitrary_types_allowed = True
    50|     @property
    51|     def OutputType(self) -> Any:
    52|         return Union[StringPromptValue, ChatPromptValueConcrete]
    53|     def get_input_schema(
    54|         self, config: Optional[RunnableConfig] = None
    55|     ) -> Type[BaseModel]:
    56|         return create_model(  # type: ignore[call-overload]
    57|             "PromptInput",
    58|             **{k: (self.input_types.get(k, str), None) for k in self.input_variables},
    59|         )
    60|     def _format_prompt_with_error_handling(self, inner_input: Dict) -> PromptValue:
    61|         if not isinstance(inner_input, dict):
    62|             raise TypeError(
    63|                 f"Expected mapping type as input to {self.__class__.__name__}. "
    64|                 f"Received {type(inner_input)}."
    65|             )
    66|         missing = set(self.input_variables).difference(inner_input)
    67|         if missing:
    68|             raise KeyError(
    69|                 f"Input to {self.__class__.__name__} is missing variables {missing}. "
    70|                 f" Expected: {self.input_variables}"
    71|                 f" Received: {list(inner_input.keys())}"
    72|             )
    73|         return self.format_prompt(**inner_input)
    74|     def invoke(
    75|         self, input: Dict, config: Optional[RunnableConfig] = None
    76|     ) -> PromptValue:
    77|         return self._call_with_config(
    78|             self._format_prompt_with_error_handling,
    79|             input,
    80|             config,
    81|             run_type="prompt",
    82|         )
    83|     @abstractmethod
    84|     def format_prompt(self, **kwargs: Any) -> PromptValue:
    85|         """Create Prompt Value."""
    86|     @root_validator()
    87|     def validate_variable_names(cls, values: Dict) -> Dict:
    88|         """Validate variable names do not include restricted names."""
    89|         if "stop" in values["input_variables"]:
    90|             raise ValueError(
    91|                 "Cannot have an input variable named 'stop', as it is used internally,"
    92|                 " please rename."
    93|             )
    94|         if "stop" in values["partial_variables"]:
    95|             raise ValueError(
    96|                 "Cannot have an partial variable named 'stop', as it is used "
    97|                 "internally, please rename."
    98|             )
    99|         overall = set(values["input_variables"]).intersection(
   100|             values["partial_variables"]
   101|         )
   102|         if overall:
   103|             raise ValueError(
   104|                 f"Found overlapping input and partial variables: {overall}"
   105|             )


# ====================================================================
# FILE: libs/core/langchain_core/prompts/chat.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 52-131 ---
    52|     @property
    53|     @abstractmethod
    54|     def input_variables(self) -> List[str]:
    55|         """Input variables for this prompt template.
    56|         Returns:
    57|             List of input variables.
    58|         """
    59|     def __add__(self, other: Any) -> ChatPromptTemplate:
    60|         """Combine two prompt templates.
    61|         Args:
    62|             other: Another prompt template.
    63|         Returns:
    64|             Combined prompt template.
    65|         """
    66|         prompt = ChatPromptTemplate(messages=[self])
    67|         return prompt + other
    68| class MessagesPlaceholder(BaseMessagePromptTemplate):
    69|     """Prompt template that assumes variable is already list of messages."""
    70|     variable_name: str
    71|     """Name of variable to use as messages."""
    72|     optional: bool = False
    73|     @classmethod
    74|     def get_lc_namespace(cls) -> List[str]:
    75|         """Get the namespace of the langchain object."""
    76|         return ["langchain", "prompts", "chat"]
    77|     def __init__(self, variable_name: str, *, optional: bool = False, **kwargs: Any):
    78|         return super().__init__(
    79|             variable_name=variable_name, optional=optional, **kwargs
    80|         )
    81|     def format_messages(self, **kwargs: Any) -> List[BaseMessage]:
    82|         """Format messages from kwargs.
    83|         Args:
    84|             **kwargs: Keyword arguments to use for formatting.
    85|         Returns:
    86|             List of BaseMessage.
    87|         """
    88|         value = (
    89|             kwargs.get(self.variable_name, [])
    90|             if self.optional
    91|             else kwargs[self.variable_name]
    92|         )
    93|         if not isinstance(value, list):
    94|             raise ValueError(
    95|                 f"variable {self.variable_name} should be a list of base messages, "
    96|                 f"got {value}"
    97|             )
    98|         for v in value:
    99|             if not isinstance(v, BaseMessage):
   100|                 raise ValueError(
   101|                     f"variable {self.variable_name} should be a list of base messages,"
   102|                     f" got {value}"
   103|                 )
   104|         return value
   105|     @property
   106|     def input_variables(self) -> List[str]:
   107|         """Input variables for this prompt template.
   108|         Returns:
   109|             List of input variable names.
   110|         """
   111|         return [self.variable_name] if not self.optional else []
   112| MessagePromptTemplateT = TypeVar(
   113|     "MessagePromptTemplateT", bound="BaseStringMessagePromptTemplate"
   114| )
   115| """Type variable for message prompt templates."""
   116| class BaseStringMessagePromptTemplate(BaseMessagePromptTemplate, ABC):
   117|     """Base class for message prompt templates that use a string prompt template."""
   118|     prompt: StringPromptTemplate
   119|     """String prompt template."""
   120|     additional_kwargs: dict = Field(default_factory=dict)
   121|     """Additional keyword arguments to pass to the prompt template."""
   122|     @classmethod
   123|     def get_lc_namespace(cls) -> List[str]:
   124|         """Get the namespace of the langchain object."""
   125|         return ["langchain", "prompts", "chat"]
   126|     @classmethod
   127|     def from_template(
   128|         cls: Type[MessagePromptTemplateT],
   129|         template: str,
   130|         template_format: str = "f-string",
   131|         partial_variables: Optional[Dict[str, Any]] = None,

# --- HUNK 2: Lines 465-505 ---
   465|         Returns:
   466|             formatted string
   467|         """
   468|         return self.format_prompt(**kwargs).to_string()
   469|     def format_messages(self, **kwargs: Any) -> List[BaseMessage]:
   470|         """Format the chat template into a list of finalized messages.
   471|         Args:
   472|             **kwargs: keyword arguments to use for filling in template variables
   473|                       in all the template messages in this chat template.
   474|         Returns:
   475|             list of formatted messages
   476|         """
   477|         kwargs = self._merge_partial_and_user_variables(**kwargs)
   478|         result = []
   479|         for message_template in self.messages:
   480|             if isinstance(message_template, BaseMessage):
   481|                 result.extend([message_template])
   482|             elif isinstance(
   483|                 message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)
   484|             ):
   485|                 message = message_template.format_messages(**kwargs)
   486|                 result.extend(message)
   487|             else:
   488|                 raise ValueError(f"Unexpected input: {message_template}")
   489|         return result
   490|     def partial(self, **kwargs: Union[str, Callable[[], str]]) -> ChatPromptTemplate:
   491|         """Get a new ChatPromptTemplate with some input variables already filled in.
   492|         Args:
   493|             **kwargs: keyword arguments to use for filling in template variables. Ought
   494|                         to be a subset of the input variables.
   495|         Returns:
   496|             A new ChatPromptTemplate.
   497|         Example:
   498|             .. code-block:: python
   499|                 from langchain_core.prompts import ChatPromptTemplate
   500|                 template = ChatPromptTemplate.from_messages(
   501|                     [
   502|                         ("system", "You are an AI assistant named {name}."),
   503|                         ("human", "Hi I'm {user}"),
   504|                         ("ai", "Hi there, {user}, I'm {name}."),
   505|                         ("human", "{input}"),


# ====================================================================
# FILE: libs/core/langchain_core/retrievers.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-41 ---
     1| from __future__ import annotations
     2| import asyncio
     3| import warnings
     4| from abc import ABC, abstractmethod
     5| from functools import partial
     6| from inspect import signature
     7| from typing import TYPE_CHECKING, Any, Dict, List, Optional
     8| from langchain_core.documents import Document
     9| from langchain_core.load.dump import dumpd
    10| from langchain_core.runnables import Runnable, RunnableConfig, RunnableSerializable
    11| if TYPE_CHECKING:
    12|     from langchain_core.callbacks.manager import (
    13|         AsyncCallbackManagerForRetrieverRun,
    14|         CallbackManagerForRetrieverRun,
    15|         Callbacks,
    16|     )
    17| RetrieverInput = str
    18| RetrieverOutput = List[Document]
    19| RetrieverLike = Runnable[RetrieverInput, RetrieverOutput]
    20| RetrieverOutputLike = Runnable[Any, RetrieverOutput]
    21| class BaseRetriever(RunnableSerializable[RetrieverInput, RetrieverOutput], ABC):
    22|     """Abstract base class for a Document retrieval system.
    23|     A retrieval system is defined as something that can take string queries and return
    24|         the most 'relevant' Documents from some source.
    25|     Example:
    26|         .. code-block:: python
    27|             class TFIDFRetriever(BaseRetriever, BaseModel):
    28|                 vectorizer: Any
    29|                 docs: List[Document]
    30|                 tfidf_array: Any
    31|                 k: int = 4
    32|                 class Config:
    33|                     arbitrary_types_allowed = True
    34|                 def get_relevant_documents(self, query: str) -> List[Document]:
    35|                     from sklearn.metrics.pairwise import cosine_similarity
    36|                     query_vec = self.vectorizer.transform([query])
    37|                     results = cosine_similarity(self.tfidf_array, query_vec).reshape((-1,))
    38|                     return [self.docs[i] for i in results.argsort()[-self.k :][::-1]]
    39|     """  # noqa: E501
    40|     class Config:
    41|         """Configuration for this pydantic object."""


# ====================================================================
# FILE: libs/core/langchain_core/runnables/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 9-68 ---
     9| creating more responsive UX.
    10| This module contains schema and implementation of LangChain Runnables primitives.
    11| """
    12| from langchain_core.runnables.base import (
    13|     Runnable,
    14|     RunnableBinding,
    15|     RunnableGenerator,
    16|     RunnableLambda,
    17|     RunnableMap,
    18|     RunnableParallel,
    19|     RunnableSequence,
    20|     RunnableSerializable,
    21| )
    22| from langchain_core.runnables.branch import RunnableBranch
    23| from langchain_core.runnables.config import (
    24|     RunnableConfig,
    25|     get_config_list,
    26|     patch_config,
    27| )
    28| from langchain_core.runnables.fallbacks import RunnableWithFallbacks
    29| from langchain_core.runnables.passthrough import (
    30|     RunnableAssign,
    31|     RunnablePassthrough,
    32|     RunnablePick,
    33| )
    34| from langchain_core.runnables.router import RouterInput, RouterRunnable
    35| from langchain_core.runnables.utils import (
    36|     AddableDict,
    37|     ConfigurableField,
    38|     ConfigurableFieldMultiOption,
    39|     ConfigurableFieldSingleOption,
    40|     aadd,
    41|     add,
    42| )
    43| __all__ = [
    44|     "AddableDict",
    45|     "ConfigurableField",
    46|     "ConfigurableFieldSingleOption",
    47|     "ConfigurableFieldMultiOption",
    48|     "patch_config",
    49|     "RouterInput",
    50|     "RouterRunnable",
    51|     "Runnable",
    52|     "RunnableSerializable",
    53|     "RunnableBinding",
    54|     "RunnableBranch",
    55|     "RunnableConfig",
    56|     "RunnableGenerator",
    57|     "RunnableLambda",
    58|     "RunnableMap",
    59|     "RunnableParallel",
    60|     "RunnablePassthrough",
    61|     "RunnableAssign",
    62|     "RunnablePick",
    63|     "RunnableSequence",
    64|     "RunnableWithFallbacks",
    65|     "get_config_list",
    66|     "aadd",
    67|     "add",
    68| ]


# ====================================================================
# FILE: libs/core/langchain_core/runnables/base.py
# Total hunks: 31
# ====================================================================
# --- HUNK 1: Lines 15-99 ---
    15|     Awaitable,
    16|     Callable,
    17|     Dict,
    18|     Generic,
    19|     Iterator,
    20|     List,
    21|     Mapping,
    22|     Optional,
    23|     Sequence,
    24|     Set,
    25|     Tuple,
    26|     Type,
    27|     TypeVar,
    28|     Union,
    29|     cast,
    30|     overload,
    31| )
    32| from typing_extensions import Literal, get_args
    33| from langchain_core.load.dump import dumpd, dumps
    34| from langchain_core.load.serializable import Serializable
    35| from langchain_core.pydantic_v1 import BaseConfig, BaseModel, Field, create_model
    36| from langchain_core.runnables.config import (
    37|     RunnableConfig,
    38|     acall_func_with_variable_args,
    39|     call_func_with_variable_args,
    40|     ensure_config,
    41|     get_async_callback_manager_for_config,
    42|     get_callback_manager_for_config,
    43|     get_config_list,
    44|     get_executor_for_config,
    45|     merge_configs,
    46|     patch_config,
    47| )
    48| from langchain_core.runnables.graph import Graph
    49| from langchain_core.runnables.utils import (
    50|     AddableDict,
    51|     AnyConfigurableField,
    52|     ConfigurableField,
    53|     ConfigurableFieldSpec,
    54|     Input,
    55|     Output,
    56|     accepts_config,
    57|     accepts_run_manager,
    58|     gather_with_concurrency,
    59|     get_function_first_arg_dict_keys,
    60|     get_function_nonlocals,
    61|     get_lambda_source,
    62|     get_unique_config_specs,
    63|     indent_lines_after_first,
    64| )
    65| from langchain_core.utils.aiter import atee, py_anext
    66| from langchain_core.utils.iter import safetee
    67| if TYPE_CHECKING:
    68|     from langchain_core.callbacks.manager import (
    69|         AsyncCallbackManagerForChainRun,
    70|         CallbackManagerForChainRun,
    71|     )
    72|     from langchain_core.runnables.fallbacks import (
    73|         RunnableWithFallbacks as RunnableWithFallbacksT,
    74|     )
    75|     from langchain_core.tracers.log_stream import RunLog, RunLogPatch
    76|     from langchain_core.tracers.root_listeners import Listener
    77| Other = TypeVar("Other")
    78| class _SchemaConfig(BaseConfig):
    79|     arbitrary_types_allowed = True
    80| class Runnable(Generic[Input, Output], ABC):
    81|     """A unit of work that can be invoked, batched, streamed, transformed and composed.
    82|      Key Methods
    83|      ===========
    84|     * invoke/ainvoke: Transforms a single input into an output.
    85|     * batch/abatch: Efficiently transforms multiple inputs into outputs.
    86|     * stream/astream: Streams output from a single input as it's produced.
    87|     * astream_log: Streams output and selected intermediate results from an input.
    88|     Built-in optimizations:
    89|     * Batch: By default, batch runs invoke() in parallel using a thread pool executor.
    90|              Override to optimize batching.
    91|     * Async: Methods with "a" suffix are asynchronous. By default, they execute
    92|              the sync counterpart using asyncio's thread pool.
    93|              Override for native async.
    94|     All methods accept an optional config argument, which can be used to configure
    95|     execution, add tags and metadata for tracing and debugging etc.
    96|     Runnables expose schematic information about their input, output and config via
    97|     the input_schema property, the output_schema property and config_schema method.
    98|     LCEL and Composition
    99|     ====================

# --- HUNK 2: Lines 146-400 ---
   146|         print(sequence.input_schema.schema()) # Show inferred input schema
   147|         print(sequence.output_schema.schema()) # Show inferred output schema
   148|         print(sequence.invoke(2)) # invoke the sequence (note the retry above!!)
   149|     Debugging and tracing
   150|     =====================
   151|     As the chains get longer, it can be useful to be able to see intermediate results
   152|     to debug and trace the chain.
   153|     You can set the global debug flag to True to enable debug output for all chains:
   154|         .. code-block:: python
   155|             from langchain_core.globals import set_debug
   156|             set_debug(True)
   157|     Alternatively, you can pass existing or custom callbacks to any given chain:
   158|         .. code-block:: python
   159|             from langchain_core.tracers import ConsoleCallbackHandler
   160|             chain.invoke(
   161|                 ...,
   162|                 config={'callbacks': [ConsoleCallbackHandler()]}
   163|             )
   164|     For a UI (and much more) checkout LangSmith: https://docs.smith.langchain.com/
   165|     """
   166|     name: Optional[str] = None
   167|     """The name of the runnable. Used for debugging and tracing."""
   168|     def get_name(
   169|         self, suffix: Optional[str] = None, *, name: Optional[str] = None
   170|     ) -> str:
   171|         """Get the name of the runnable."""
   172|         name = name or self.name or self.__class__.__name__
   173|         if suffix:
   174|             if name[0].isupper():
   175|                 return name + suffix.title()
   176|             else:
   177|                 return name + "_" + suffix.lower()
   178|         else:
   179|             return name
   180|     @property
   181|     def InputType(self) -> Type[Input]:
   182|         """The type of input this runnable accepts specified as a type annotation."""
   183|         for cls in self.__class__.__orig_bases__:  # type: ignore[attr-defined]
   184|             type_args = get_args(cls)
   185|             if type_args and len(type_args) == 2:
   186|                 return type_args[0]
   187|         raise TypeError(
   188|             f"Runnable {self.get_name()} doesn't have an inferable InputType. "
   189|             "Override the InputType property to specify the input type."
   190|         )
   191|     @property
   192|     def OutputType(self) -> Type[Output]:
   193|         """The type of output this runnable produces specified as a type annotation."""
   194|         for cls in self.__class__.__orig_bases__:  # type: ignore[attr-defined]
   195|             type_args = get_args(cls)
   196|             if type_args and len(type_args) == 2:
   197|                 return type_args[1]
   198|         raise TypeError(
   199|             f"Runnable {self.get_name()} doesn't have an inferable OutputType. "
   200|             "Override the OutputType property to specify the output type."
   201|         )
   202|     @property
   203|     def input_schema(self) -> Type[BaseModel]:
   204|         """The type of input this runnable accepts specified as a pydantic model."""
   205|         return self.get_input_schema()
   206|     def get_input_schema(
   207|         self, config: Optional[RunnableConfig] = None
   208|     ) -> Type[BaseModel]:
   209|         """Get a pydantic model that can be used to validate input to the runnable.
   210|         Runnables that leverage the configurable_fields and configurable_alternatives
   211|         methods will have a dynamic input schema that depends on which
   212|         configuration the runnable is invoked with.
   213|         This method allows to get an input schema for a specific configuration.
   214|         Args:
   215|             config: A config to use when generating the schema.
   216|         Returns:
   217|             A pydantic model that can be used to validate input.
   218|         """
   219|         root_type = self.InputType
   220|         if inspect.isclass(root_type) and issubclass(root_type, BaseModel):
   221|             return root_type
   222|         return create_model(
   223|             self.get_name("Input"),
   224|             __root__=(root_type, None),
   225|             __config__=_SchemaConfig,
   226|         )
   227|     @property
   228|     def output_schema(self) -> Type[BaseModel]:
   229|         """The type of output this runnable produces specified as a pydantic model."""
   230|         return self.get_output_schema()
   231|     def get_output_schema(
   232|         self, config: Optional[RunnableConfig] = None
   233|     ) -> Type[BaseModel]:
   234|         """Get a pydantic model that can be used to validate output to the runnable.
   235|         Runnables that leverage the configurable_fields and configurable_alternatives
   236|         methods will have a dynamic output schema that depends on which
   237|         configuration the runnable is invoked with.
   238|         This method allows to get an output schema for a specific configuration.
   239|         Args:
   240|             config: A config to use when generating the schema.
   241|         Returns:
   242|             A pydantic model that can be used to validate output.
   243|         """
   244|         root_type = self.OutputType
   245|         if inspect.isclass(root_type) and issubclass(root_type, BaseModel):
   246|             return root_type
   247|         return create_model(
   248|             self.get_name("Output"),
   249|             __root__=(root_type, None),
   250|             __config__=_SchemaConfig,
   251|         )
   252|     @property
   253|     def config_specs(self) -> List[ConfigurableFieldSpec]:
   254|         """List configurable fields for this runnable."""
   255|         return []
   256|     def config_schema(
   257|         self, *, include: Optional[Sequence[str]] = None
   258|     ) -> Type[BaseModel]:
   259|         """The type of config this runnable accepts specified as a pydantic model.
   260|         To mark a field as configurable, see the `configurable_fields`
   261|         and `configurable_alternatives` methods.
   262|         Args:
   263|             include: A list of fields to include in the config schema.
   264|         Returns:
   265|             A pydantic model that can be used to validate config.
   266|         """
   267|         include = include or []
   268|         config_specs = self.config_specs
   269|         configurable = (
   270|             create_model(  # type: ignore[call-overload]
   271|                 "Configurable",
   272|                 **{
   273|                     spec.id: (
   274|                         spec.annotation,
   275|                         Field(
   276|                             spec.default, title=spec.name, description=spec.description
   277|                         ),
   278|                     )
   279|                     for spec in config_specs
   280|                 },
   281|                 __config__=_SchemaConfig,
   282|             )
   283|             if config_specs
   284|             else None
   285|         )
   286|         return create_model(  # type: ignore[call-overload]
   287|             self.get_name("Config"),
   288|             __config__=_SchemaConfig,
   289|             **({"configurable": (configurable, None)} if configurable else {}),
   290|             **{
   291|                 field_name: (field_type, None)
   292|                 for field_name, field_type in RunnableConfig.__annotations__.items()
   293|                 if field_name in [i for i in include if i != "configurable"]
   294|             },
   295|         )
   296|     def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
   297|         """Return a graph representation of this runnable."""
   298|         from langchain_core.runnables.graph import Graph
   299|         graph = Graph()
   300|         input_node = graph.add_node(self.get_input_schema(config))
   301|         runnable_node = graph.add_node(self)
   302|         output_node = graph.add_node(self.get_output_schema(config))
   303|         graph.add_edge(input_node, runnable_node)
   304|         graph.add_edge(runnable_node, output_node)
   305|         return graph
   306|     def __or__(
   307|         self,
   308|         other: Union[
   309|             Runnable[Any, Other],
   310|             Callable[[Any], Other],
   311|             Callable[[Iterator[Any]], Iterator[Other]],
   312|             Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],
   313|         ],
   314|     ) -> RunnableSerializable[Input, Other]:
   315|         """Compose this runnable with another object to create a RunnableSequence."""
   316|         return RunnableSequence(self, coerce_to_runnable(other))
   317|     def __ror__(
   318|         self,
   319|         other: Union[
   320|             Runnable[Other, Any],
   321|             Callable[[Other], Any],
   322|             Callable[[Iterator[Other]], Iterator[Any]],
   323|             Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]],
   324|         ],
   325|     ) -> RunnableSerializable[Other, Output]:
   326|         """Compose this runnable with another object to create a RunnableSequence."""
   327|         return RunnableSequence(coerce_to_runnable(other), self)
   328|     def pipe(
   329|         self,
   330|         *others: Union[Runnable[Any, Other], Callable[[Any], Other]],
   331|         name: Optional[str] = None,
   332|     ) -> RunnableSerializable[Input, Other]:
   333|         """Compose this runnable with another object to create a RunnableSequence."""
   334|         return RunnableSequence(self, *others, name=name)
   335|     def pick(self, keys: Union[str, List[str]]) -> RunnableSerializable[Any, Any]:
   336|         """Pick keys from the dict output of this runnable.
   337|         Returns a new runnable."""
   338|         from langchain_core.runnables.passthrough import RunnablePick
   339|         return self | RunnablePick(keys)
   340|     def assign(
   341|         self,
   342|         **kwargs: Union[
   343|             Runnable[Dict[str, Any], Any],
   344|             Callable[[Dict[str, Any]], Any],
   345|             Mapping[
   346|                 str,
   347|                 Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]],
   348|             ],
   349|         ],
   350|     ) -> RunnableSerializable[Any, Any]:
   351|         """Assigns new fields to the dict output of this runnable.
   352|         Returns a new runnable."""
   353|         from langchain_core.runnables.passthrough import RunnableAssign
   354|         return self | RunnableAssign(RunnableParallel(kwargs))
   355|     """ --- Public API --- """
   356|     @abstractmethod
   357|     def invoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output:
   358|         """Transform a single input into an output. Override to implement.
   359|         Args:
   360|             input: The input to the runnable.
   361|             config: A config to use when invoking the runnable.
   362|                The config supports standard keys like 'tags', 'metadata' for tracing
   363|                purposes, 'max_concurrency' for controlling how much work to do
   364|                in parallel, and other keys. Please refer to the RunnableConfig
   365|                for more details.
   366|         Returns:
   367|             The output of the runnable.
   368|         """
   369|     async def ainvoke(
   370|         self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
   371|     ) -> Output:
   372|         """Default implementation of ainvoke, calls invoke from a thread.
   373|         The default implementation allows usage of async code even if
   374|         the runnable did not implement a native async version of invoke.
   375|         Subclasses should override this method if they can run asynchronously.
   376|         """
   377|         with get_executor_for_config(config) as executor:
   378|             return await asyncio.get_running_loop().run_in_executor(
   379|                 executor, partial(self.invoke, **kwargs), input, config
   380|             )
   381|     def batch(
   382|         self,
   383|         inputs: List[Input],
   384|         config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,
   385|         *,
   386|         return_exceptions: bool = False,
   387|         **kwargs: Optional[Any],
   388|     ) -> List[Output]:
   389|         """Default implementation runs invoke in parallel using a thread pool executor.
   390|         The default implementation of batch works well for IO bound runnables.
   391|         Subclasses should override this method if they can batch more efficiently;
   392|         e.g., if the underlying runnable uses an API which supports a batch mode.
   393|         """
   394|         if not inputs:
   395|             return []
   396|         configs = get_config_list(config, len(inputs))
   397|         def invoke(input: Input, config: RunnableConfig) -> Union[Output, Exception]:
   398|             if return_exceptions:
   399|                 try:
   400|                     return self.invoke(input, config, **kwargs)

# --- HUNK 3: Lines 765-840 ---
   765|     def _call_with_config(
   766|         self,
   767|         func: Union[
   768|             Callable[[Input], Output],
   769|             Callable[[Input, CallbackManagerForChainRun], Output],
   770|             Callable[[Input, CallbackManagerForChainRun, RunnableConfig], Output],
   771|         ],
   772|         input: Input,
   773|         config: Optional[RunnableConfig],
   774|         run_type: Optional[str] = None,
   775|         **kwargs: Optional[Any],
   776|     ) -> Output:
   777|         """Helper method to transform an Input value to an Output value,
   778|         with callbacks. Use this method to implement invoke() in subclasses."""
   779|         config = ensure_config(config)
   780|         callback_manager = get_callback_manager_for_config(config)
   781|         run_manager = callback_manager.on_chain_start(
   782|             dumpd(self),
   783|             input,
   784|             run_type=run_type,
   785|             name=config.get("run_name") or self.get_name(),
   786|         )
   787|         try:
   788|             output = call_func_with_variable_args(
   789|                 func, input, config, run_manager, **kwargs
   790|             )
   791|         except BaseException as e:
   792|             run_manager.on_chain_error(e)
   793|             raise
   794|         else:
   795|             run_manager.on_chain_end(dumpd(output))
   796|             return output
   797|     async def _acall_with_config(
   798|         self,
   799|         func: Union[
   800|             Callable[[Input], Awaitable[Output]],
   801|             Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],
   802|             Callable[
   803|                 [Input, AsyncCallbackManagerForChainRun, RunnableConfig],
   804|                 Awaitable[Output],
   805|             ],
   806|         ],
   807|         input: Input,
   808|         config: Optional[RunnableConfig],
   809|         run_type: Optional[str] = None,
   810|         **kwargs: Optional[Any],
   811|     ) -> Output:
   812|         """Helper method to transform an Input value to an Output value,
   813|         with callbacks. Use this method to implement ainvoke() in subclasses."""
   814|         config = ensure_config(config)
   815|         callback_manager = get_async_callback_manager_for_config(config)
   816|         run_manager = await callback_manager.on_chain_start(
   817|             dumpd(self),
   818|             input,
   819|             run_type=run_type,
   820|             name=config.get("run_name") or self.get_name(),
   821|         )
   822|         try:
   823|             output = await acall_func_with_variable_args(
   824|                 func, input, config, run_manager, **kwargs
   825|             )
   826|         except BaseException as e:
   827|             await run_manager.on_chain_error(e)
   828|             raise
   829|         else:
   830|             await run_manager.on_chain_end(dumpd(output))
   831|             return output
   832|     def _batch_with_config(
   833|         self,
   834|         func: Union[
   835|             Callable[[List[Input]], List[Union[Exception, Output]]],
   836|             Callable[
   837|                 [List[Input], List[CallbackManagerForChainRun]],
   838|                 List[Union[Exception, Output]],
   839|             ],
   840|             Callable[

# --- HUNK 4: Lines 843-883 ---
   843|             ],
   844|         ],
   845|         input: List[Input],
   846|         config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,
   847|         *,
   848|         return_exceptions: bool = False,
   849|         run_type: Optional[str] = None,
   850|         **kwargs: Optional[Any],
   851|     ) -> List[Output]:
   852|         """Helper method to transform an Input value to an Output value,
   853|         with callbacks. Use this method to implement invoke() in subclasses."""
   854|         if not input:
   855|             return []
   856|         configs = get_config_list(config, len(input))
   857|         callback_managers = [get_callback_manager_for_config(c) for c in configs]
   858|         run_managers = [
   859|             callback_manager.on_chain_start(
   860|                 dumpd(self),
   861|                 input,
   862|                 run_type=run_type,
   863|                 name=config.get("run_name") or self.get_name(),
   864|             )
   865|             for callback_manager, input, config in zip(
   866|                 callback_managers, input, configs
   867|             )
   868|         ]
   869|         try:
   870|             if accepts_config(func):
   871|                 kwargs["config"] = [
   872|                     patch_config(c, callbacks=rm.get_child())
   873|                     for c, rm in zip(configs, run_managers)
   874|                 ]
   875|             if accepts_run_manager(func):
   876|                 kwargs["run_manager"] = run_managers
   877|             output = func(input, **kwargs)  # type: ignore[call-arg]
   878|         except BaseException as e:
   879|             for run_manager in run_managers:
   880|                 run_manager.on_chain_error(e)
   881|             if return_exceptions:
   882|                 return cast(List[Output], [e for _ in input])
   883|             else:

# --- HUNK 5: Lines 913-953 ---
   913|         ],
   914|         input: List[Input],
   915|         config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,
   916|         *,
   917|         return_exceptions: bool = False,
   918|         run_type: Optional[str] = None,
   919|         **kwargs: Optional[Any],
   920|     ) -> List[Output]:
   921|         """Helper method to transform an Input value to an Output value,
   922|         with callbacks. Use this method to implement invoke() in subclasses."""
   923|         if not input:
   924|             return []
   925|         configs = get_config_list(config, len(input))
   926|         callback_managers = [get_async_callback_manager_for_config(c) for c in configs]
   927|         run_managers: List[AsyncCallbackManagerForChainRun] = await asyncio.gather(
   928|             *(
   929|                 callback_manager.on_chain_start(
   930|                     dumpd(self),
   931|                     input,
   932|                     run_type=run_type,
   933|                     name=config.get("run_name") or self.get_name(),
   934|                 )
   935|                 for callback_manager, input, config in zip(
   936|                     callback_managers, input, configs
   937|                 )
   938|             )
   939|         )
   940|         try:
   941|             if accepts_config(func):
   942|                 kwargs["config"] = [
   943|                     patch_config(c, callbacks=rm.get_child())
   944|                     for c, rm in zip(configs, run_managers)
   945|                 ]
   946|             if accepts_run_manager(func):
   947|                 kwargs["run_manager"] = run_managers
   948|             output = await func(input, **kwargs)  # type: ignore[call-arg]
   949|         except BaseException as e:
   950|             await asyncio.gather(
   951|                 *(run_manager.on_chain_error(e) for run_manager in run_managers)
   952|             )
   953|             if return_exceptions:

# --- HUNK 6: Lines 984-1024 ---
   984|             ],
   985|         ],
   986|         config: Optional[RunnableConfig],
   987|         run_type: Optional[str] = None,
   988|         **kwargs: Optional[Any],
   989|     ) -> Iterator[Output]:
   990|         """Helper method to transform an Iterator of Input values into an Iterator of
   991|         Output values, with callbacks.
   992|         Use this to implement `stream()` or `transform()` in Runnable subclasses."""
   993|         input_for_tracing, input_for_transform = tee(input, 2)
   994|         final_input: Optional[Input] = next(input_for_tracing, None)
   995|         final_input_supported = True
   996|         final_output: Optional[Output] = None
   997|         final_output_supported = True
   998|         config = ensure_config(config)
   999|         callback_manager = get_callback_manager_for_config(config)
  1000|         run_manager = callback_manager.on_chain_start(
  1001|             dumpd(self),
  1002|             {"input": ""},
  1003|             run_type=run_type,
  1004|             name=config.get("run_name") or self.get_name(),
  1005|         )
  1006|         try:
  1007|             if accepts_config(transformer):
  1008|                 kwargs["config"] = patch_config(
  1009|                     config, callbacks=run_manager.get_child()
  1010|                 )
  1011|             if accepts_run_manager(transformer):
  1012|                 kwargs["run_manager"] = run_manager
  1013|             iterator = transformer(input_for_transform, **kwargs)  # type: ignore[call-arg]
  1014|             for chunk in iterator:
  1015|                 yield chunk
  1016|                 if final_output_supported:
  1017|                     if final_output is None:
  1018|                         final_output = chunk
  1019|                     else:
  1020|                         try:
  1021|                             final_output = final_output + chunk  # type: ignore
  1022|                         except TypeError:
  1023|                             final_output = None
  1024|                             final_output_supported = False

# --- HUNK 7: Lines 1056-1232 ---
  1056|             ],
  1057|         ],
  1058|         config: Optional[RunnableConfig],
  1059|         run_type: Optional[str] = None,
  1060|         **kwargs: Optional[Any],
  1061|     ) -> AsyncIterator[Output]:
  1062|         """Helper method to transform an Async Iterator of Input values into an Async
  1063|         Iterator of Output values, with callbacks.
  1064|         Use this to implement `astream()` or `atransform()` in Runnable subclasses."""
  1065|         input_for_tracing, input_for_transform = atee(input, 2)
  1066|         final_input: Optional[Input] = await py_anext(input_for_tracing, None)
  1067|         final_input_supported = True
  1068|         final_output: Optional[Output] = None
  1069|         final_output_supported = True
  1070|         config = ensure_config(config)
  1071|         callback_manager = get_async_callback_manager_for_config(config)
  1072|         run_manager = await callback_manager.on_chain_start(
  1073|             dumpd(self),
  1074|             {"input": ""},
  1075|             run_type=run_type,
  1076|             name=config.get("run_name") or self.get_name(),
  1077|         )
  1078|         try:
  1079|             if accepts_config(transformer):
  1080|                 kwargs["config"] = patch_config(
  1081|                     config, callbacks=run_manager.get_child()
  1082|                 )
  1083|             if accepts_run_manager(transformer):
  1084|                 kwargs["run_manager"] = run_manager
  1085|             iterator = transformer(input_for_transform, **kwargs)  # type: ignore[call-arg]
  1086|             async for chunk in iterator:
  1087|                 yield chunk
  1088|                 if final_output_supported:
  1089|                     if final_output is None:
  1090|                         final_output = chunk
  1091|                     else:
  1092|                         try:
  1093|                             final_output = final_output + chunk  # type: ignore
  1094|                         except TypeError:
  1095|                             final_output = None
  1096|                             final_output_supported = False
  1097|             async for ichunk in input_for_tracing:
  1098|                 if final_input_supported:
  1099|                     if final_input is None:
  1100|                         final_input = ichunk
  1101|                     else:
  1102|                         try:
  1103|                             final_input = final_input + ichunk  # type: ignore[operator]
  1104|                         except TypeError:
  1105|                             final_input = None
  1106|                             final_input_supported = False
  1107|         except BaseException as e:
  1108|             await run_manager.on_chain_error(e, inputs=final_input)
  1109|             raise
  1110|         else:
  1111|             await run_manager.on_chain_end(final_output, inputs=final_input)
  1112| class RunnableSerializable(Serializable, Runnable[Input, Output]):
  1113|     """A Runnable that can be serialized to JSON."""
  1114|     name: Optional[str] = None
  1115|     """The name of the runnable. Used for debugging and tracing."""
  1116|     def configurable_fields(
  1117|         self, **kwargs: AnyConfigurableField
  1118|     ) -> RunnableSerializable[Input, Output]:
  1119|         from langchain_core.runnables.configurable import RunnableConfigurableFields
  1120|         for key in kwargs:
  1121|             if key not in self.__fields__:
  1122|                 raise ValueError(
  1123|                     f"Configuration key {key} not found in {self}: "
  1124|                     "available keys are {self.__fields__.keys()}"
  1125|                 )
  1126|         return RunnableConfigurableFields(default=self, fields=kwargs)
  1127|     def configurable_alternatives(
  1128|         self,
  1129|         which: ConfigurableField,
  1130|         *,
  1131|         default_key: str = "default",
  1132|         prefix_keys: bool = False,
  1133|         **kwargs: Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]],
  1134|     ) -> RunnableSerializable[Input, Output]:
  1135|         from langchain_core.runnables.configurable import (
  1136|             RunnableConfigurableAlternatives,
  1137|         )
  1138|         return RunnableConfigurableAlternatives(
  1139|             which=which,
  1140|             default=self,
  1141|             alternatives=kwargs,
  1142|             default_key=default_key,
  1143|             prefix_keys=prefix_keys,
  1144|         )
  1145| def _seq_input_schema(
  1146|     steps: List[Runnable[Any, Any]], config: Optional[RunnableConfig]
  1147| ) -> Type[BaseModel]:
  1148|     from langchain_core.runnables.passthrough import RunnableAssign, RunnablePick
  1149|     first = steps[0]
  1150|     if len(steps) == 1:
  1151|         return first.get_input_schema(config)
  1152|     elif isinstance(first, RunnableAssign):
  1153|         next_input_schema = _seq_input_schema(steps[1:], config)
  1154|         if not next_input_schema.__custom_root_type__:
  1155|             return create_model(  # type: ignore[call-overload]
  1156|                 "RunnableSequenceInput",
  1157|                 **{
  1158|                     k: (v.annotation, v.default)
  1159|                     for k, v in next_input_schema.__fields__.items()
  1160|                     if k not in first.mapper.steps
  1161|                 },
  1162|                 __config__=_SchemaConfig,
  1163|             )
  1164|     elif isinstance(first, RunnablePick):
  1165|         return _seq_input_schema(steps[1:], config)
  1166|     return first.get_input_schema(config)
  1167| def _seq_output_schema(
  1168|     steps: List[Runnable[Any, Any]], config: Optional[RunnableConfig]
  1169| ) -> Type[BaseModel]:
  1170|     from langchain_core.runnables.passthrough import RunnableAssign, RunnablePick
  1171|     last = steps[-1]
  1172|     if len(steps) == 1:
  1173|         return last.get_input_schema(config)
  1174|     elif isinstance(last, RunnableAssign):
  1175|         mapper_output_schema = last.mapper.get_output_schema(config)
  1176|         prev_output_schema = _seq_output_schema(steps[:-1], config)
  1177|         if not prev_output_schema.__custom_root_type__:
  1178|             return create_model(  # type: ignore[call-overload]
  1179|                 "RunnableSequenceOutput",
  1180|                 **{
  1181|                     **{
  1182|                         k: (v.annotation, v.default)
  1183|                         for k, v in prev_output_schema.__fields__.items()
  1184|                     },
  1185|                     **{
  1186|                         k: (v.annotation, v.default)
  1187|                         for k, v in mapper_output_schema.__fields__.items()
  1188|                     },
  1189|                 },
  1190|                 __config__=_SchemaConfig,
  1191|             )
  1192|     elif isinstance(last, RunnablePick):
  1193|         prev_output_schema = _seq_output_schema(steps[:-1], config)
  1194|         if not prev_output_schema.__custom_root_type__:
  1195|             if isinstance(last.keys, list):
  1196|                 return create_model(  # type: ignore[call-overload]
  1197|                     "RunnableSequenceOutput",
  1198|                     **{
  1199|                         k: (v.annotation, v.default)
  1200|                         for k, v in prev_output_schema.__fields__.items()
  1201|                         if k in last.keys
  1202|                     },
  1203|                     __config__=_SchemaConfig,
  1204|                 )
  1205|             else:
  1206|                 field = prev_output_schema.__fields__[last.keys]
  1207|                 return create_model(  # type: ignore[call-overload]
  1208|                     "RunnableSequenceOutput",
  1209|                     __root__=(field.annotation, field.default),
  1210|                     __config__=_SchemaConfig,
  1211|                 )
  1212|     return last.get_output_schema(config)
  1213| class RunnableSequence(RunnableSerializable[Input, Output]):
  1214|     """A sequence of runnables, where the output of each is the input of the next.
  1215|     RunnableSequence is the most important composition operator in LangChain as it is
  1216|     used in virtually every chain.
  1217|     A RunnableSequence can be instantiated directly or more commonly by using the `|`
  1218|     operator where either the left or right operands (or both) must be a Runnable.
  1219|     Any RunnableSequence automatically supports sync, async, batch.
  1220|     The default implementations of `batch` and `abatch` utilize threadpools and
  1221|     asyncio gather and will be faster than naive invocation of invoke or ainvoke
  1222|     for IO bound runnables.
  1223|     Batching is implemented by invoking the batch method on each component of the
  1224|     RunnableSequence in order.
  1225|     A RunnableSequence preserves the streaming properties of its components, so if all
  1226|     components of the sequence implement a `transform` method -- which
  1227|     is the method that implements the logic to map a streaming input to a streaming
  1228|     output -- then the sequence will be able to stream input to output!
  1229|     If any component of the sequence does not implement transform then the
  1230|     streaming will only begin after this component is run. If there are
  1231|     multiple blocking components, streaming begins after the last one.
  1232|     Please note: RunnableLambdas do not support `transform` by default! So if

# --- HUNK 8: Lines 1253-1498 ---
  1253|         .. code-block:: python
  1254|             from langchain_core.output_parsers.json import SimpleJsonOutputParser
  1255|             from langchain_core.chat_models.openai import ChatOpenAI
  1256|             prompt = PromptTemplate.from_template(
  1257|                 'In JSON format, give me a list of {topic} and their '
  1258|                 'corresponding names in French, Spanish and in a '
  1259|                 'Cat Language.'
  1260|             )
  1261|             model = ChatOpenAI()
  1262|             chain = prompt | model | SimpleJsonOutputParser()
  1263|             async for chunk in chain.astream({'topic': 'colors'}):
  1264|                 print('-')
  1265|                 print(chunk, sep='', flush=True)
  1266|     """
  1267|     first: Runnable[Input, Any]
  1268|     """The first runnable in the sequence."""
  1269|     middle: List[Runnable[Any, Any]] = Field(default_factory=list)
  1270|     """The middle runnables in the sequence."""
  1271|     last: Runnable[Any, Output]
  1272|     """The last runnable in the sequence."""
  1273|     def __init__(
  1274|         self,
  1275|         *steps: RunnableLike,
  1276|         name: Optional[str] = None,
  1277|         first: Optional[Runnable[Any, Any]] = None,
  1278|         middle: Optional[List[Runnable[Any, Any]]] = None,
  1279|         last: Optional[Runnable[Any, Any]] = None,
  1280|     ) -> None:
  1281|         """Create a new RunnableSequence.
  1282|         Args:
  1283|             steps: The steps to include in the sequence.
  1284|         """
  1285|         steps_flat: List[Runnable] = []
  1286|         if not steps:
  1287|             if first is not None and last is not None:
  1288|                 steps_flat = [first] + (middle or []) + [last]
  1289|         for step in steps:
  1290|             if isinstance(step, RunnableSequence):
  1291|                 steps_flat.extend(step.steps)
  1292|             else:
  1293|                 steps_flat.append(coerce_to_runnable(step))
  1294|         if len(steps_flat) < 2:
  1295|             raise ValueError(
  1296|                 f"RunnableSequence must have at least 2 steps, got {len(steps_flat)}"
  1297|             )
  1298|         super().__init__(
  1299|             first=steps_flat[0],
  1300|             middle=list(steps_flat[1:-1]),
  1301|             last=steps_flat[-1],
  1302|             name=name,
  1303|         )
  1304|     @classmethod
  1305|     def get_lc_namespace(cls) -> List[str]:
  1306|         """Get the namespace of the langchain object."""
  1307|         return ["langchain", "schema", "runnable"]
  1308|     @property
  1309|     def steps(self) -> List[Runnable[Any, Any]]:
  1310|         """All the runnables that make up the sequence in order."""
  1311|         return [self.first] + self.middle + [self.last]
  1312|     @classmethod
  1313|     def is_lc_serializable(cls) -> bool:
  1314|         return True
  1315|     class Config:
  1316|         arbitrary_types_allowed = True
  1317|     @property
  1318|     def InputType(self) -> Type[Input]:
  1319|         return self.first.InputType
  1320|     @property
  1321|     def OutputType(self) -> Type[Output]:
  1322|         return self.last.OutputType
  1323|     def get_input_schema(
  1324|         self, config: Optional[RunnableConfig] = None
  1325|     ) -> Type[BaseModel]:
  1326|         return _seq_input_schema(self.steps, config)
  1327|     def get_output_schema(
  1328|         self, config: Optional[RunnableConfig] = None
  1329|     ) -> Type[BaseModel]:
  1330|         return _seq_output_schema(self.steps, config)
  1331|     @property
  1332|     def config_specs(self) -> List[ConfigurableFieldSpec]:
  1333|         from langchain_core.beta.runnables.context import (
  1334|             CONTEXT_CONFIG_PREFIX,
  1335|             _key_from_id,
  1336|         )
  1337|         all_specs = [
  1338|             (spec, idx)
  1339|             for idx, step in enumerate(self.steps)
  1340|             for spec in step.config_specs
  1341|         ]
  1342|         specs_by_pos = groupby(
  1343|             [tup for tup in all_specs if tup[0].id.startswith(CONTEXT_CONFIG_PREFIX)],
  1344|             lambda x: x[1],
  1345|         )
  1346|         next_deps: Set[str] = set()
  1347|         deps_by_pos: Dict[int, Set[str]] = {}
  1348|         for pos, specs in specs_by_pos:
  1349|             deps_by_pos[pos] = next_deps
  1350|             next_deps = next_deps | {spec[0].id for spec in specs}
  1351|         for pos, (spec, idx) in enumerate(all_specs):
  1352|             if spec.id.startswith(CONTEXT_CONFIG_PREFIX):
  1353|                 all_specs[pos] = (
  1354|                     ConfigurableFieldSpec(
  1355|                         id=spec.id,
  1356|                         annotation=spec.annotation,
  1357|                         name=spec.name,
  1358|                         default=spec.default,
  1359|                         description=spec.description,
  1360|                         is_shared=spec.is_shared,
  1361|                         dependencies=[
  1362|                             d
  1363|                             for d in deps_by_pos[idx]
  1364|                             if _key_from_id(d) != _key_from_id(spec.id)
  1365|                         ]
  1366|                         + (spec.dependencies or []),
  1367|                     ),
  1368|                     idx,
  1369|                 )
  1370|         return get_unique_config_specs(spec for spec, _ in all_specs)
  1371|     def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
  1372|         from langchain_core.runnables.graph import Graph
  1373|         graph = Graph()
  1374|         for step in self.steps:
  1375|             current_last_node = graph.last_node()
  1376|             step_graph = step.get_graph(config)
  1377|             if step is not self.first:
  1378|                 step_graph.trim_first_node()
  1379|             if step is not self.last:
  1380|                 step_graph.trim_last_node()
  1381|             graph.extend(step_graph)
  1382|             step_first_node = step_graph.first_node()
  1383|             if not step_first_node:
  1384|                 raise ValueError(f"Runnable {step} has no first node")
  1385|             if current_last_node:
  1386|                 graph.add_edge(current_last_node, step_first_node)
  1387|         return graph
  1388|     def __repr__(self) -> str:
  1389|         return "\n| ".join(
  1390|             repr(s) if i == 0 else indent_lines_after_first(repr(s), "| ")
  1391|             for i, s in enumerate(self.steps)
  1392|         )
  1393|     def __or__(
  1394|         self,
  1395|         other: Union[
  1396|             Runnable[Any, Other],
  1397|             Callable[[Any], Other],
  1398|             Callable[[Iterator[Any]], Iterator[Other]],
  1399|             Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]],
  1400|         ],
  1401|     ) -> RunnableSerializable[Input, Other]:
  1402|         if isinstance(other, RunnableSequence):
  1403|             return RunnableSequence(
  1404|                 self.first,
  1405|                 *self.middle,
  1406|                 self.last,
  1407|                 other.first,
  1408|                 *other.middle,
  1409|                 other.last,
  1410|                 name=self.name or other.name,
  1411|             )
  1412|         else:
  1413|             return RunnableSequence(
  1414|                 self.first,
  1415|                 *self.middle,
  1416|                 self.last,
  1417|                 coerce_to_runnable(other),
  1418|                 name=self.name,
  1419|             )
  1420|     def __ror__(
  1421|         self,
  1422|         other: Union[
  1423|             Runnable[Other, Any],
  1424|             Callable[[Other], Any],
  1425|             Callable[[Iterator[Other]], Iterator[Any]],
  1426|             Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]],
  1427|         ],
  1428|     ) -> RunnableSerializable[Other, Output]:
  1429|         if isinstance(other, RunnableSequence):
  1430|             return RunnableSequence(
  1431|                 other.first,
  1432|                 *other.middle,
  1433|                 other.last,
  1434|                 self.first,
  1435|                 *self.middle,
  1436|                 self.last,
  1437|                 name=other.name or self.name,
  1438|             )
  1439|         else:
  1440|             return RunnableSequence(
  1441|                 coerce_to_runnable(other),
  1442|                 self.first,
  1443|                 *self.middle,
  1444|                 self.last,
  1445|                 name=self.name,
  1446|             )
  1447|     def invoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output:
  1448|         from langchain_core.beta.runnables.context import config_with_context
  1449|         config = config_with_context(ensure_config(config), self.steps)
  1450|         callback_manager = get_callback_manager_for_config(config)
  1451|         run_manager = callback_manager.on_chain_start(
  1452|             dumpd(self), input, name=config.get("run_name") or self.get_name()
  1453|         )
  1454|         try:
  1455|             for i, step in enumerate(self.steps):
  1456|                 input = step.invoke(
  1457|                     input,
  1458|                     patch_config(
  1459|                         config, callbacks=run_manager.get_child(f"seq:step:{i+1}")
  1460|                     ),
  1461|                 )
  1462|         except BaseException as e:
  1463|             run_manager.on_chain_error(e)
  1464|             raise
  1465|         else:
  1466|             run_manager.on_chain_end(input)
  1467|             return cast(Output, input)
  1468|     async def ainvoke(
  1469|         self,
  1470|         input: Input,
  1471|         config: Optional[RunnableConfig] = None,
  1472|         **kwargs: Optional[Any],
  1473|     ) -> Output:
  1474|         from langchain_core.beta.runnables.context import aconfig_with_context
  1475|         config = aconfig_with_context(ensure_config(config), self.steps)
  1476|         callback_manager = get_async_callback_manager_for_config(config)
  1477|         run_manager = await callback_manager.on_chain_start(
  1478|             dumpd(self), input, name=config.get("run_name") or self.get_name()
  1479|         )
  1480|         try:
  1481|             for i, step in enumerate(self.steps):
  1482|                 input = await step.ainvoke(
  1483|                     input,
  1484|                     patch_config(
  1485|                         config, callbacks=run_manager.get_child(f"seq:step:{i+1}")
  1486|                     ),
  1487|                 )
  1488|         except BaseException as e:
  1489|             await run_manager.on_chain_error(e)
  1490|             raise
  1491|         else:
  1492|             await run_manager.on_chain_end(input)
  1493|             return cast(Output, input)
  1494|     def batch(
  1495|         self,
  1496|         inputs: List[Input],
  1497|         config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,
  1498|         *,

# --- HUNK 9: Lines 1506-1546 ---
  1506|         configs = [
  1507|             config_with_context(c, self.steps)
  1508|             for c in get_config_list(config, len(inputs))
  1509|         ]
  1510|         callback_managers = [
  1511|             CallbackManager.configure(
  1512|                 inheritable_callbacks=config.get("callbacks"),
  1513|                 local_callbacks=None,
  1514|                 verbose=False,
  1515|                 inheritable_tags=config.get("tags"),
  1516|                 local_tags=None,
  1517|                 inheritable_metadata=config.get("metadata"),
  1518|                 local_metadata=None,
  1519|             )
  1520|             for config in configs
  1521|         ]
  1522|         run_managers = [
  1523|             cm.on_chain_start(
  1524|                 dumpd(self),
  1525|                 input,
  1526|                 name=config.get("run_name") or self.get_name(),
  1527|             )
  1528|             for cm, input, config in zip(callback_managers, inputs, configs)
  1529|         ]
  1530|         try:
  1531|             if return_exceptions:
  1532|                 failed_inputs_map: Dict[int, Exception] = {}
  1533|                 for stepidx, step in enumerate(self.steps):
  1534|                     remaining_idxs = [
  1535|                         i for i in range(len(configs)) if i not in failed_inputs_map
  1536|                     ]
  1537|                     inputs = step.batch(
  1538|                         [
  1539|                             inp
  1540|                             for i, inp in zip(remaining_idxs, inputs)
  1541|                             if i not in failed_inputs_map
  1542|                         ],
  1543|                         [
  1544|                             patch_config(
  1545|                                 config, callbacks=rm.get_child(f"seq:step:{stepidx+1}")
  1546|                             )

# --- HUNK 10: Lines 1609-1649 ---
  1609|             aconfig_with_context(c, self.steps)
  1610|             for c in get_config_list(config, len(inputs))
  1611|         ]
  1612|         callback_managers = [
  1613|             AsyncCallbackManager.configure(
  1614|                 inheritable_callbacks=config.get("callbacks"),
  1615|                 local_callbacks=None,
  1616|                 verbose=False,
  1617|                 inheritable_tags=config.get("tags"),
  1618|                 local_tags=None,
  1619|                 inheritable_metadata=config.get("metadata"),
  1620|                 local_metadata=None,
  1621|             )
  1622|             for config in configs
  1623|         ]
  1624|         run_managers: List[AsyncCallbackManagerForChainRun] = await asyncio.gather(
  1625|             *(
  1626|                 cm.on_chain_start(
  1627|                     dumpd(self),
  1628|                     input,
  1629|                     name=config.get("run_name") or self.get_name(),
  1630|                 )
  1631|                 for cm, input, config in zip(callback_managers, inputs, configs)
  1632|             )
  1633|         )
  1634|         try:
  1635|             if return_exceptions:
  1636|                 failed_inputs_map: Dict[int, Exception] = {}
  1637|                 for stepidx, step in enumerate(self.steps):
  1638|                     remaining_idxs = [
  1639|                         i for i in range(len(configs)) if i not in failed_inputs_map
  1640|                     ]
  1641|                     inputs = await step.abatch(
  1642|                         [
  1643|                             inp
  1644|                             for i, inp in zip(remaining_idxs, inputs)
  1645|                             if i not in failed_inputs_map
  1646|                         ],
  1647|                         [
  1648|                             patch_config(
  1649|                                 config, callbacks=rm.get_child(f"seq:step:{stepidx+1}")

# --- HUNK 11: Lines 1728-1790 ---
  1728|         steps = [self.first] + self.middle + [self.last]
  1729|         config = aconfig_with_context(config, self.steps)
  1730|         final_pipeline = cast(AsyncIterator[Output], input)
  1731|         for step in steps:
  1732|             final_pipeline = step.atransform(
  1733|                 final_pipeline,
  1734|                 patch_config(
  1735|                     config,
  1736|                     callbacks=run_manager.get_child(f"seq:step:{steps.index(step)+1}"),
  1737|                 ),
  1738|             )
  1739|         async for output in final_pipeline:
  1740|             yield output
  1741|     def transform(
  1742|         self,
  1743|         input: Iterator[Input],
  1744|         config: Optional[RunnableConfig] = None,
  1745|         **kwargs: Optional[Any],
  1746|     ) -> Iterator[Output]:
  1747|         yield from self._transform_stream_with_config(
  1748|             input,
  1749|             self._transform,
  1750|             patch_config(config, run_name=(config or {}).get("run_name") or self.name),
  1751|             **kwargs,
  1752|         )
  1753|     def stream(
  1754|         self,
  1755|         input: Input,
  1756|         config: Optional[RunnableConfig] = None,
  1757|         **kwargs: Optional[Any],
  1758|     ) -> Iterator[Output]:
  1759|         yield from self.transform(iter([input]), config, **kwargs)
  1760|     async def atransform(
  1761|         self,
  1762|         input: AsyncIterator[Input],
  1763|         config: Optional[RunnableConfig] = None,
  1764|         **kwargs: Optional[Any],
  1765|     ) -> AsyncIterator[Output]:
  1766|         async for chunk in self._atransform_stream_with_config(
  1767|             input,
  1768|             self._atransform,
  1769|             patch_config(config, run_name=(config or {}).get("run_name") or self.name),
  1770|             **kwargs,
  1771|         ):
  1772|             yield chunk
  1773|     async def astream(
  1774|         self,
  1775|         input: Input,
  1776|         config: Optional[RunnableConfig] = None,
  1777|         **kwargs: Optional[Any],
  1778|     ) -> AsyncIterator[Output]:
  1779|         async def input_aiter() -> AsyncIterator[Input]:
  1780|             yield input
  1781|         async for chunk in self.atransform(input_aiter(), config, **kwargs):
  1782|             yield chunk
  1783| class RunnableParallel(RunnableSerializable[Input, Dict[str, Any]]):
  1784|     """
  1785|     A runnable that runs a mapping of runnables in parallel,
  1786|     and returns a mapping of their outputs.
  1787|     """
  1788|     steps: Mapping[str, Runnable[Input, Any]]
  1789|     def __init__(
  1790|         self,

# --- HUNK 12: Lines 1801-1958 ---
  1801|         **kwargs: Union[
  1802|             Runnable[Input, Any],
  1803|             Callable[[Input], Any],
  1804|             Mapping[str, Union[Runnable[Input, Any], Callable[[Input], Any]]],
  1805|         ],
  1806|     ) -> None:
  1807|         merged = {**__steps} if __steps is not None else {}
  1808|         merged.update(kwargs)
  1809|         super().__init__(
  1810|             steps={key: coerce_to_runnable(r) for key, r in merged.items()}
  1811|         )
  1812|     @classmethod
  1813|     def is_lc_serializable(cls) -> bool:
  1814|         return True
  1815|     @classmethod
  1816|     def get_lc_namespace(cls) -> List[str]:
  1817|         """Get the namespace of the langchain object."""
  1818|         return ["langchain", "schema", "runnable"]
  1819|     class Config:
  1820|         arbitrary_types_allowed = True
  1821|     def get_name(
  1822|         self, suffix: Optional[str] = None, *, name: Optional[str] = None
  1823|     ) -> str:
  1824|         name = name or self.name or f"RunnableParallel<{','.join(self.steps.keys())}>"
  1825|         return super().get_name(suffix, name=name)
  1826|     @property
  1827|     def InputType(self) -> Any:
  1828|         for step in self.steps.values():
  1829|             if step.InputType:
  1830|                 return step.InputType
  1831|         return Any
  1832|     def get_input_schema(
  1833|         self, config: Optional[RunnableConfig] = None
  1834|     ) -> Type[BaseModel]:
  1835|         if all(
  1836|             s.get_input_schema(config).schema().get("type", "object") == "object"
  1837|             for s in self.steps.values()
  1838|         ):
  1839|             return create_model(  # type: ignore[call-overload]
  1840|                 self.get_name("Input"),
  1841|                 **{
  1842|                     k: (v.annotation, v.default)
  1843|                     for step in self.steps.values()
  1844|                     for k, v in step.get_input_schema(config).__fields__.items()
  1845|                     if k != "__root__"
  1846|                 },
  1847|                 __config__=_SchemaConfig,
  1848|             )
  1849|         return super().get_input_schema(config)
  1850|     def get_output_schema(
  1851|         self, config: Optional[RunnableConfig] = None
  1852|     ) -> Type[BaseModel]:
  1853|         return create_model(  # type: ignore[call-overload]
  1854|             self.get_name("Output"),
  1855|             **{k: (v.OutputType, None) for k, v in self.steps.items()},
  1856|             __config__=_SchemaConfig,
  1857|         )
  1858|     @property
  1859|     def config_specs(self) -> List[ConfigurableFieldSpec]:
  1860|         return get_unique_config_specs(
  1861|             spec for step in self.steps.values() for spec in step.config_specs
  1862|         )
  1863|     def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
  1864|         from langchain_core.runnables.graph import Graph
  1865|         graph = Graph()
  1866|         input_node = graph.add_node(self.get_input_schema(config))
  1867|         output_node = graph.add_node(self.get_output_schema(config))
  1868|         for step in self.steps.values():
  1869|             step_graph = step.get_graph()
  1870|             step_graph.trim_first_node()
  1871|             step_graph.trim_last_node()
  1872|             if not step_graph:
  1873|                 graph.add_edge(input_node, output_node)
  1874|             else:
  1875|                 graph.extend(step_graph)
  1876|                 step_first_node = step_graph.first_node()
  1877|                 if not step_first_node:
  1878|                     raise ValueError(f"Runnable {step} has no first node")
  1879|                 step_last_node = step_graph.last_node()
  1880|                 if not step_last_node:
  1881|                     raise ValueError(f"Runnable {step} has no last node")
  1882|                 graph.add_edge(input_node, step_first_node)
  1883|                 graph.add_edge(step_last_node, output_node)
  1884|         return graph
  1885|     def __repr__(self) -> str:
  1886|         map_for_repr = ",\n  ".join(
  1887|             f"{k}: {indent_lines_after_first(repr(v), '  ' + k + ': ')}"
  1888|             for k, v in self.steps.items()
  1889|         )
  1890|         return "{\n  " + map_for_repr + "\n}"
  1891|     def invoke(
  1892|         self, input: Input, config: Optional[RunnableConfig] = None
  1893|     ) -> Dict[str, Any]:
  1894|         from langchain_core.callbacks.manager import CallbackManager
  1895|         config = ensure_config(config)
  1896|         callback_manager = CallbackManager.configure(
  1897|             inheritable_callbacks=config.get("callbacks"),
  1898|             local_callbacks=None,
  1899|             verbose=False,
  1900|             inheritable_tags=config.get("tags"),
  1901|             local_tags=None,
  1902|             inheritable_metadata=config.get("metadata"),
  1903|             local_metadata=None,
  1904|         )
  1905|         run_manager = callback_manager.on_chain_start(
  1906|             dumpd(self), input, name=config.get("run_name") or self.get_name()
  1907|         )
  1908|         try:
  1909|             steps = dict(self.steps)
  1910|             with get_executor_for_config(config) as executor:
  1911|                 futures = [
  1912|                     executor.submit(
  1913|                         step.invoke,
  1914|                         input,
  1915|                         patch_config(
  1916|                             config,
  1917|                             callbacks=run_manager.get_child(f"map:key:{key}"),
  1918|                         ),
  1919|                     )
  1920|                     for key, step in steps.items()
  1921|                 ]
  1922|                 output = {key: future.result() for key, future in zip(steps, futures)}
  1923|         except BaseException as e:
  1924|             run_manager.on_chain_error(e)
  1925|             raise
  1926|         else:
  1927|             run_manager.on_chain_end(output)
  1928|             return output
  1929|     async def ainvoke(
  1930|         self,
  1931|         input: Input,
  1932|         config: Optional[RunnableConfig] = None,
  1933|         **kwargs: Optional[Any],
  1934|     ) -> Dict[str, Any]:
  1935|         config = ensure_config(config)
  1936|         callback_manager = get_async_callback_manager_for_config(config)
  1937|         run_manager = await callback_manager.on_chain_start(
  1938|             dumpd(self), input, name=config.get("run_name") or self.get_name()
  1939|         )
  1940|         try:
  1941|             steps = dict(self.steps)
  1942|             results = await asyncio.gather(
  1943|                 *(
  1944|                     step.ainvoke(
  1945|                         input,
  1946|                         patch_config(
  1947|                             config, callbacks=run_manager.get_child(f"map:key:{key}")
  1948|                         ),
  1949|                     )
  1950|                     for key, step in steps.items()
  1951|                 )
  1952|             )
  1953|             output = {key: value for key, value in zip(steps, results)}
  1954|         except BaseException as e:
  1955|             await run_manager.on_chain_error(e)
  1956|             raise
  1957|         else:
  1958|             await run_manager.on_chain_end(output)

# --- HUNK 13: Lines 2114-2159 ---
  2114|         try:
  2115|             sig = inspect.signature(func)
  2116|             return (
  2117|                 getattr(sig.return_annotation, "__args__", (Any,))[0]
  2118|                 if sig.return_annotation != inspect.Signature.empty
  2119|                 else Any
  2120|             )
  2121|         except ValueError:
  2122|             return Any
  2123|     def __eq__(self, other: Any) -> bool:
  2124|         if isinstance(other, RunnableGenerator):
  2125|             if hasattr(self, "_transform") and hasattr(other, "_transform"):
  2126|                 return self._transform == other._transform
  2127|             elif hasattr(self, "_atransform") and hasattr(other, "_atransform"):
  2128|                 return self._atransform == other._atransform
  2129|             else:
  2130|                 return False
  2131|         else:
  2132|             return False
  2133|     def __repr__(self) -> str:
  2134|         if hasattr(self, "_transform"):
  2135|             return f"RunnableGenerator({self._transform.__name__})"
  2136|         elif hasattr(self, "_atransform"):
  2137|             return f"RunnableGenerator({self._atransform.__name__})"
  2138|         else:
  2139|             return "RunnableGenerator(...)"
  2140|     def transform(
  2141|         self,
  2142|         input: Iterator[Input],
  2143|         config: Optional[RunnableConfig] = None,
  2144|         **kwargs: Any,
  2145|     ) -> Iterator[Output]:
  2146|         return self._transform_stream_with_config(
  2147|             input, self._transform, config, **kwargs
  2148|         )
  2149|     def stream(
  2150|         self,
  2151|         input: Input,
  2152|         config: Optional[RunnableConfig] = None,
  2153|         **kwargs: Any,
  2154|     ) -> Iterator[Output]:
  2155|         return self.transform(iter([input]), config, **kwargs)
  2156|     def invoke(
  2157|         self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
  2158|     ) -> Output:
  2159|         final = None

# --- HUNK 14: Lines 2238-2410 ---
  2238|             Union[
  2239|                 Callable[[Input], Awaitable[Output]],
  2240|                 Callable[[Input, RunnableConfig], Awaitable[Output]],
  2241|                 Callable[[Input, AsyncCallbackManagerForChainRun], Awaitable[Output]],
  2242|                 Callable[
  2243|                     [Input, AsyncCallbackManagerForChainRun, RunnableConfig],
  2244|                     Awaitable[Output],
  2245|                 ],
  2246|             ]
  2247|         ] = None,
  2248|     ) -> None:
  2249|         """Create a RunnableLambda from a callable, and async callable or both.
  2250|         Accepts both sync and async variants to allow providing efficient
  2251|         implementations for sync and async execution.
  2252|         Args:
  2253|             func: Either sync or async callable
  2254|             afunc: An async callable that takes an input and returns an output.
  2255|         """
  2256|         if afunc is not None:
  2257|             self.afunc = afunc
  2258|             func_for_name: Callable = afunc
  2259|         if inspect.iscoroutinefunction(func):
  2260|             if afunc is not None:
  2261|                 raise TypeError(
  2262|                     "Func was provided as a coroutine function, but afunc was "
  2263|                     "also provided. If providing both, func should be a regular "
  2264|                     "function to avoid ambiguity."
  2265|                 )
  2266|             self.afunc = func
  2267|             func_for_name = func
  2268|         elif callable(func):
  2269|             self.func = cast(Callable[[Input], Output], func)
  2270|             func_for_name = func
  2271|         else:
  2272|             raise TypeError(
  2273|                 "Expected a callable type for `func`."
  2274|                 f"Instead got an unsupported type: {type(func)}"
  2275|             )
  2276|         try:
  2277|             if func_for_name.__name__ != "<lambda>":
  2278|                 self.name = func_for_name.__name__
  2279|         except AttributeError:
  2280|             pass
  2281|     @property
  2282|     def InputType(self) -> Any:
  2283|         """The type of the input to this runnable."""
  2284|         func = getattr(self, "func", None) or getattr(self, "afunc")
  2285|         try:
  2286|             params = inspect.signature(func).parameters
  2287|             first_param = next(iter(params.values()), None)
  2288|             if first_param and first_param.annotation != inspect.Parameter.empty:
  2289|                 return first_param.annotation
  2290|             else:
  2291|                 return Any
  2292|         except ValueError:
  2293|             return Any
  2294|     def get_input_schema(
  2295|         self, config: Optional[RunnableConfig] = None
  2296|     ) -> Type[BaseModel]:
  2297|         """The pydantic schema for the input to this runnable."""
  2298|         func = getattr(self, "func", None) or getattr(self, "afunc")
  2299|         if isinstance(func, itemgetter):
  2300|             items = str(func).replace("operator.itemgetter(", "")[:-1].split(", ")
  2301|             if all(
  2302|                 item[0] == "'" and item[-1] == "'" and len(item) > 2 for item in items
  2303|             ):
  2304|                 return create_model(
  2305|                     self.get_name("Input"),
  2306|                     **{item[1:-1]: (Any, None) for item in items},  # type: ignore
  2307|                     __config__=_SchemaConfig,
  2308|                 )
  2309|             else:
  2310|                 return create_model(
  2311|                     self.get_name("Input"),
  2312|                     __root__=(List[Any], None),
  2313|                     __config__=_SchemaConfig,
  2314|                 )
  2315|         if self.InputType != Any:
  2316|             return super().get_input_schema(config)
  2317|         if dict_keys := get_function_first_arg_dict_keys(func):
  2318|             return create_model(
  2319|                 self.get_name("Input"),
  2320|                 **{key: (Any, None) for key in dict_keys},  # type: ignore
  2321|                 __config__=_SchemaConfig,
  2322|             )
  2323|         return super().get_input_schema(config)
  2324|     @property
  2325|     def OutputType(self) -> Any:
  2326|         """The type of the output of this runnable as a type annotation."""
  2327|         func = getattr(self, "func", None) or getattr(self, "afunc")
  2328|         try:
  2329|             sig = inspect.signature(func)
  2330|             return (
  2331|                 sig.return_annotation
  2332|                 if sig.return_annotation != inspect.Signature.empty
  2333|                 else Any
  2334|             )
  2335|         except ValueError:
  2336|             return Any
  2337|     @property
  2338|     def deps(self) -> List[Runnable]:
  2339|         """The dependencies of this runnable."""
  2340|         if hasattr(self, "func"):
  2341|             objects = get_function_nonlocals(self.func)
  2342|         elif hasattr(self, "afunc"):
  2343|             objects = get_function_nonlocals(self.afunc)
  2344|         else:
  2345|             objects = []
  2346|         return [obj for obj in objects if isinstance(obj, Runnable)]
  2347|     @property
  2348|     def config_specs(self) -> List[ConfigurableFieldSpec]:
  2349|         return get_unique_config_specs(
  2350|             spec for dep in self.deps for spec in dep.config_specs
  2351|         )
  2352|     def get_graph(self, config: RunnableConfig | None = None) -> Graph:
  2353|         if deps := self.deps:
  2354|             graph = Graph()
  2355|             input_node = graph.add_node(self.get_input_schema(config))
  2356|             output_node = graph.add_node(self.get_output_schema(config))
  2357|             for dep in deps:
  2358|                 dep_graph = dep.get_graph()
  2359|                 dep_graph.trim_first_node()
  2360|                 dep_graph.trim_last_node()
  2361|                 if not dep_graph:
  2362|                     graph.add_edge(input_node, output_node)
  2363|                 else:
  2364|                     graph.extend(dep_graph)
  2365|                     dep_first_node = dep_graph.first_node()
  2366|                     if not dep_first_node:
  2367|                         raise ValueError(f"Runnable {dep} has no first node")
  2368|                     dep_last_node = dep_graph.last_node()
  2369|                     if not dep_last_node:
  2370|                         raise ValueError(f"Runnable {dep} has no last node")
  2371|                     graph.add_edge(input_node, dep_first_node)
  2372|                     graph.add_edge(dep_last_node, output_node)
  2373|         else:
  2374|             graph = super().get_graph(config)
  2375|         return graph
  2376|     def __eq__(self, other: Any) -> bool:
  2377|         if isinstance(other, RunnableLambda):
  2378|             if hasattr(self, "func") and hasattr(other, "func"):
  2379|                 return self.func == other.func
  2380|             elif hasattr(self, "afunc") and hasattr(other, "afunc"):
  2381|                 return self.afunc == other.afunc
  2382|             else:
  2383|                 return False
  2384|         else:
  2385|             return False
  2386|     def __repr__(self) -> str:
  2387|         """A string representation of this runnable."""
  2388|         if hasattr(self, "func") and isinstance(self.func, itemgetter):
  2389|             return f"RunnableLambda({str(self.func)[len('operator.'):]})"
  2390|         elif hasattr(self, "func"):
  2391|             return f"RunnableLambda({get_lambda_source(self.func) or '...'})"
  2392|         elif hasattr(self, "afunc"):
  2393|             return f"RunnableLambda(afunc={get_lambda_source(self.afunc) or '...'})"
  2394|         else:
  2395|             return "RunnableLambda(...)"
  2396|     def _invoke(
  2397|         self,
  2398|         input: Input,
  2399|         run_manager: CallbackManagerForChainRun,
  2400|         config: RunnableConfig,
  2401|         **kwargs: Any,
  2402|     ) -> Output:
  2403|         output = call_func_with_variable_args(
  2404|             self.func, input, config, run_manager, **kwargs
  2405|         )
  2406|         if isinstance(output, Runnable):
  2407|             recursion_limit = config["recursion_limit"]
  2408|             if recursion_limit <= 0:
  2409|                 raise RecursionError(
  2410|                     f"Recursion limit reached when invoking {self} with input {input}."

# --- HUNK 15: Lines 2413-2456 ---
  2413|                 input,
  2414|                 patch_config(
  2415|                     config,
  2416|                     callbacks=run_manager.get_child(),
  2417|                     recursion_limit=recursion_limit - 1,
  2418|                 ),
  2419|             )
  2420|         return output
  2421|     async def _ainvoke(
  2422|         self,
  2423|         input: Input,
  2424|         run_manager: AsyncCallbackManagerForChainRun,
  2425|         config: RunnableConfig,
  2426|         **kwargs: Any,
  2427|     ) -> Output:
  2428|         if hasattr(self, "afunc"):
  2429|             afunc = self.afunc
  2430|         else:
  2431|             @wraps(self.func)
  2432|             async def f(*args, **kwargs):  # type: ignore[no-untyped-def]
  2433|                 with get_executor_for_config(config) as executor:
  2434|                     return await asyncio.get_running_loop().run_in_executor(
  2435|                         executor, partial(self.func, **kwargs), *args
  2436|                     )
  2437|             afunc = f
  2438|         output = await acall_func_with_variable_args(
  2439|             afunc, input, config, run_manager, **kwargs
  2440|         )
  2441|         if isinstance(output, Runnable):
  2442|             recursion_limit = config["recursion_limit"]
  2443|             if recursion_limit <= 0:
  2444|                 raise RecursionError(
  2445|                     f"Recursion limit reached when invoking {self} with input {input}."
  2446|                 )
  2447|             output = await output.ainvoke(
  2448|                 input,
  2449|                 patch_config(
  2450|                     config,
  2451|                     callbacks=run_manager.get_child(),
  2452|                     recursion_limit=recursion_limit - 1,
  2453|                 ),
  2454|             )
  2455|         return output
  2456|     def _config(

# --- HUNK 16: Lines 2481-2694 ---
  2481|             )
  2482|         else:
  2483|             raise TypeError(
  2484|                 "Cannot invoke a coroutine function synchronously."
  2485|                 "Use `ainvoke` instead."
  2486|             )
  2487|     async def ainvoke(
  2488|         self,
  2489|         input: Input,
  2490|         config: Optional[RunnableConfig] = None,
  2491|         **kwargs: Optional[Any],
  2492|     ) -> Output:
  2493|         """Invoke this runnable asynchronously."""
  2494|         the_func = self.afunc if hasattr(self, "afunc") else self.func
  2495|         return await self._acall_with_config(
  2496|             self._ainvoke,
  2497|             input,
  2498|             self._config(config, the_func),
  2499|             **kwargs,
  2500|         )
  2501|     def _transform(
  2502|         self,
  2503|         input: Iterator[Input],
  2504|         run_manager: CallbackManagerForChainRun,
  2505|         config: RunnableConfig,
  2506|         **kwargs: Any,
  2507|     ) -> Iterator[Output]:
  2508|         final: Optional[Input] = None
  2509|         for ichunk in input:
  2510|             if final is None:
  2511|                 final = ichunk
  2512|             else:
  2513|                 try:
  2514|                     final = final + ichunk  # type: ignore[operator]
  2515|                 except TypeError:
  2516|                     final = ichunk
  2517|         output = call_func_with_variable_args(
  2518|             self.func, cast(Input, final), config, run_manager, **kwargs
  2519|         )
  2520|         if isinstance(output, Runnable):
  2521|             recursion_limit = config["recursion_limit"]
  2522|             if recursion_limit <= 0:
  2523|                 raise RecursionError(
  2524|                     f"Recursion limit reached when invoking "
  2525|                     f"{self} with input {final}."
  2526|                 )
  2527|             for chunk in output.stream(
  2528|                 final,
  2529|                 patch_config(
  2530|                     config,
  2531|                     callbacks=run_manager.get_child(),
  2532|                     recursion_limit=recursion_limit - 1,
  2533|                 ),
  2534|             ):
  2535|                 yield chunk
  2536|         else:
  2537|             yield output
  2538|     def transform(
  2539|         self,
  2540|         input: Iterator[Input],
  2541|         config: Optional[RunnableConfig] = None,
  2542|         **kwargs: Optional[Any],
  2543|     ) -> Iterator[Output]:
  2544|         if hasattr(self, "func"):
  2545|             for output in self._transform_stream_with_config(
  2546|                 input,
  2547|                 self._transform,
  2548|                 self._config(config, self.func),
  2549|                 **kwargs,
  2550|             ):
  2551|                 yield output
  2552|         else:
  2553|             raise TypeError(
  2554|                 "Cannot stream a coroutine function synchronously."
  2555|                 "Use `astream` instead."
  2556|             )
  2557|     def stream(
  2558|         self,
  2559|         input: Input,
  2560|         config: Optional[RunnableConfig] = None,
  2561|         **kwargs: Optional[Any],
  2562|     ) -> Iterator[Output]:
  2563|         return self.transform(iter([input]), config, **kwargs)
  2564|     async def _atransform(
  2565|         self,
  2566|         input: AsyncIterator[Input],
  2567|         run_manager: AsyncCallbackManagerForChainRun,
  2568|         config: RunnableConfig,
  2569|     ) -> AsyncIterator[Output]:
  2570|         final: Optional[Input] = None
  2571|         async for ichunk in input:
  2572|             if final is None:
  2573|                 final = ichunk
  2574|             else:
  2575|                 try:
  2576|                     final = final + ichunk  # type: ignore[operator]
  2577|                 except TypeError:
  2578|                     final = ichunk
  2579|         if hasattr(self, "afunc"):
  2580|             afunc = self.afunc
  2581|         else:
  2582|             @wraps(self.func)
  2583|             async def f(*args, **kwargs):  # type: ignore[no-untyped-def]
  2584|                 return await asyncio.get_running_loop().run_in_executor(
  2585|                     None, partial(self.func, **kwargs), *args
  2586|                 )
  2587|             afunc = f
  2588|         output = await acall_func_with_variable_args(
  2589|             afunc, cast(Input, final), config, run_manager
  2590|         )
  2591|         if isinstance(output, Runnable):
  2592|             recursion_limit = config["recursion_limit"]
  2593|             if recursion_limit <= 0:
  2594|                 raise RecursionError(
  2595|                     f"Recursion limit reached when invoking "
  2596|                     f"{self} with input {final}."
  2597|                 )
  2598|             async for chunk in output.astream(
  2599|                 final,
  2600|                 patch_config(
  2601|                     config,
  2602|                     callbacks=run_manager.get_child(),
  2603|                     recursion_limit=recursion_limit - 1,
  2604|                 ),
  2605|             ):
  2606|                 yield chunk
  2607|         else:
  2608|             yield output
  2609|     async def atransform(
  2610|         self,
  2611|         input: AsyncIterator[Input],
  2612|         config: Optional[RunnableConfig] = None,
  2613|         **kwargs: Optional[Any],
  2614|     ) -> AsyncIterator[Output]:
  2615|         async for output in self._atransform_stream_with_config(
  2616|             input,
  2617|             self._atransform,
  2618|             self._config(config, self.afunc if hasattr(self, "afunc") else self.func),
  2619|             **kwargs,
  2620|         ):
  2621|             yield output
  2622|     async def astream(
  2623|         self,
  2624|         input: Input,
  2625|         config: Optional[RunnableConfig] = None,
  2626|         **kwargs: Optional[Any],
  2627|     ) -> AsyncIterator[Output]:
  2628|         async def input_aiter() -> AsyncIterator[Input]:
  2629|             yield input
  2630|         async for chunk in self.atransform(input_aiter(), config, **kwargs):
  2631|             yield chunk
  2632| class RunnableEachBase(RunnableSerializable[List[Input], List[Output]]):
  2633|     """
  2634|     A runnable that delegates calls to another runnable
  2635|     with each element of the input sequence.
  2636|     Use only if creating a new RunnableEach subclass with different __init__ args.
  2637|     """
  2638|     bound: Runnable[Input, Output]
  2639|     class Config:
  2640|         arbitrary_types_allowed = True
  2641|     @property
  2642|     def InputType(self) -> Any:
  2643|         return List[self.bound.InputType]  # type: ignore[name-defined]
  2644|     def get_input_schema(
  2645|         self, config: Optional[RunnableConfig] = None
  2646|     ) -> Type[BaseModel]:
  2647|         return create_model(
  2648|             self.get_name("Input"),
  2649|             __root__=(
  2650|                 List[self.bound.get_input_schema(config)],  # type: ignore
  2651|                 None,
  2652|             ),
  2653|             __config__=_SchemaConfig,
  2654|         )
  2655|     @property
  2656|     def OutputType(self) -> Type[List[Output]]:
  2657|         return List[self.bound.OutputType]  # type: ignore[name-defined]
  2658|     def get_output_schema(
  2659|         self, config: Optional[RunnableConfig] = None
  2660|     ) -> Type[BaseModel]:
  2661|         schema = self.bound.get_output_schema(config)
  2662|         return create_model(
  2663|             self.get_name("Output"),
  2664|             __root__=(
  2665|                 List[schema],  # type: ignore
  2666|                 None,
  2667|             ),
  2668|             __config__=_SchemaConfig,
  2669|         )
  2670|     @property
  2671|     def config_specs(self) -> List[ConfigurableFieldSpec]:
  2672|         return self.bound.config_specs
  2673|     def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
  2674|         return self.bound.get_graph(config)
  2675|     @classmethod
  2676|     def is_lc_serializable(cls) -> bool:
  2677|         return True
  2678|     @classmethod
  2679|     def get_lc_namespace(cls) -> List[str]:
  2680|         """Get the namespace of the langchain object."""
  2681|         return ["langchain", "schema", "runnable"]
  2682|     def _invoke(
  2683|         self,
  2684|         inputs: List[Input],
  2685|         run_manager: CallbackManagerForChainRun,
  2686|         config: RunnableConfig,
  2687|         **kwargs: Any,
  2688|     ) -> List[Output]:
  2689|         return self.bound.batch(
  2690|             inputs, patch_config(config, callbacks=run_manager.get_child()), **kwargs
  2691|         )
  2692|     def invoke(
  2693|         self, input: List[Input], config: Optional[RunnableConfig] = None, **kwargs: Any
  2694|     ) -> List[Output]:

# --- HUNK 17: Lines 2699-2743 ---
  2699|         run_manager: AsyncCallbackManagerForChainRun,
  2700|         config: RunnableConfig,
  2701|         **kwargs: Any,
  2702|     ) -> List[Output]:
  2703|         return await self.bound.abatch(
  2704|             inputs, patch_config(config, callbacks=run_manager.get_child()), **kwargs
  2705|         )
  2706|     async def ainvoke(
  2707|         self, input: List[Input], config: Optional[RunnableConfig] = None, **kwargs: Any
  2708|     ) -> List[Output]:
  2709|         return await self._acall_with_config(self._ainvoke, input, config, **kwargs)
  2710| class RunnableEach(RunnableEachBase[Input, Output]):
  2711|     """
  2712|     A runnable that delegates calls to another runnable
  2713|     with each element of the input sequence.
  2714|     """
  2715|     @classmethod
  2716|     def get_lc_namespace(cls) -> List[str]:
  2717|         """Get the namespace of the langchain object."""
  2718|         return ["langchain", "schema", "runnable"]
  2719|     def get_name(
  2720|         self, suffix: Optional[str] = None, *, name: Optional[str] = None
  2721|     ) -> str:
  2722|         name = name or self.name or f"RunnableEach<{self.bound.get_name()}>"
  2723|         return super().get_name(suffix, name=name)
  2724|     def bind(self, **kwargs: Any) -> RunnableEach[Input, Output]:
  2725|         return RunnableEach(bound=self.bound.bind(**kwargs))
  2726|     def with_config(
  2727|         self, config: Optional[RunnableConfig] = None, **kwargs: Any
  2728|     ) -> RunnableEach[Input, Output]:
  2729|         return RunnableEach(bound=self.bound.with_config(config, **kwargs))
  2730|     def with_listeners(
  2731|         self,
  2732|         *,
  2733|         on_start: Optional[Listener] = None,
  2734|         on_end: Optional[Listener] = None,
  2735|         on_error: Optional[Listener] = None,
  2736|     ) -> RunnableEach[Input, Output]:
  2737|         """
  2738|         Bind lifecycle listeners to a Runnable, returning a new Runnable.
  2739|         on_start: Called before the runnable starts running, with the Run object.
  2740|         on_end: Called after the runnable finishes running, with the Run object.
  2741|         on_error: Called if the runnable throws an error, with the Run object.
  2742|         The Run object contains information about the run, including its id,
  2743|         type, input, output, error, start_time, end_time, and any tags or metadata

# --- HUNK 18: Lines 2805-2879 ---
  2805|             **other_kwargs: Unpacked into the base class.
  2806|         """
  2807|         config = config or {}
  2808|         if configurable := config.get("configurable", None):
  2809|             allowed_keys = set(s.id for s in bound.config_specs)
  2810|             for key in configurable:
  2811|                 if key not in allowed_keys:
  2812|                     raise ValueError(
  2813|                         f"Configurable key '{key}' not found in runnable with"
  2814|                         f" config keys: {allowed_keys}"
  2815|                     )
  2816|         super().__init__(
  2817|             bound=bound,
  2818|             kwargs=kwargs or {},
  2819|             config=config or {},
  2820|             config_factories=config_factories or [],
  2821|             custom_input_type=custom_input_type,
  2822|             custom_output_type=custom_output_type,
  2823|             **other_kwargs,
  2824|         )
  2825|     def get_name(
  2826|         self, suffix: Optional[str] = None, *, name: Optional[str] = None
  2827|     ) -> str:
  2828|         return self.bound.get_name(suffix, name=name)
  2829|     @property
  2830|     def InputType(self) -> Type[Input]:
  2831|         return (
  2832|             cast(Type[Input], self.custom_input_type)
  2833|             if self.custom_input_type is not None
  2834|             else self.bound.InputType
  2835|         )
  2836|     @property
  2837|     def OutputType(self) -> Type[Output]:
  2838|         return (
  2839|             cast(Type[Output], self.custom_output_type)
  2840|             if self.custom_output_type is not None
  2841|             else self.bound.OutputType
  2842|         )
  2843|     def get_input_schema(
  2844|         self, config: Optional[RunnableConfig] = None
  2845|     ) -> Type[BaseModel]:
  2846|         if self.custom_input_type is not None:
  2847|             return super().get_input_schema(config)
  2848|         return self.bound.get_input_schema(merge_configs(self.config, config))
  2849|     def get_output_schema(
  2850|         self, config: Optional[RunnableConfig] = None
  2851|     ) -> Type[BaseModel]:
  2852|         if self.custom_output_type is not None:
  2853|             return super().get_output_schema(config)
  2854|         return self.bound.get_output_schema(merge_configs(self.config, config))
  2855|     @property
  2856|     def config_specs(self) -> List[ConfigurableFieldSpec]:
  2857|         return self.bound.config_specs
  2858|     def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
  2859|         return self.bound.get_graph(config)
  2860|     @classmethod
  2861|     def is_lc_serializable(cls) -> bool:
  2862|         return True
  2863|     @classmethod
  2864|     def get_lc_namespace(cls) -> List[str]:
  2865|         """Get the namespace of the langchain object."""
  2866|         return ["langchain", "schema", "runnable"]
  2867|     def _merge_configs(self, *configs: Optional[RunnableConfig]) -> RunnableConfig:
  2868|         config = merge_configs(self.config, *configs)
  2869|         return merge_configs(config, *(f(config) for f in self.config_factories))
  2870|     def invoke(
  2871|         self,
  2872|         input: Input,
  2873|         config: Optional[RunnableConfig] = None,
  2874|         **kwargs: Optional[Any],
  2875|     ) -> Output:
  2876|         return self.bound.invoke(
  2877|             input,
  2878|             self._merge_configs(config),
  2879|             **{**self.kwargs, **kwargs},


# ====================================================================
# FILE: libs/core/langchain_core/runnables/branch.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-47 ---
     1| from typing import (
     2|     Any,
     3|     AsyncIterator,
     4|     Awaitable,
     5|     Callable,
     6|     Iterator,
     7|     List,
     8|     Mapping,
     9|     Optional,
    10|     Sequence,
    11|     Tuple,
    12|     Type,
    13|     Union,
    14|     cast,
    15| )
    16| from langchain_core.load.dump import dumpd
    17| from langchain_core.pydantic_v1 import BaseModel
    18| from langchain_core.runnables.base import (
    19|     Runnable,
    20|     RunnableLike,
    21|     RunnableSerializable,
    22|     coerce_to_runnable,
    23| )
    24| from langchain_core.runnables.config import (
    25|     RunnableConfig,
    26|     ensure_config,
    27|     get_async_callback_manager_for_config,
    28|     get_callback_manager_for_config,
    29|     patch_config,
    30| )
    31| from langchain_core.runnables.utils import (
    32|     ConfigurableFieldSpec,
    33|     Input,
    34|     Output,
    35|     get_unique_config_specs,
    36| )
    37| class RunnableBranch(RunnableSerializable[Input, Output]):
    38|     """A Runnable that selects which branch to run based on a condition.
    39|     The runnable is initialized with a list of (condition, runnable) pairs and
    40|     a default branch.
    41|     When operating on an input, the first condition that evaluates to True is
    42|     selected, and the corresponding runnable is run on the input.
    43|     If no condition evaluates to True, the default branch is run on the input.
    44|     Examples:
    45|         .. code-block:: python
    46|             from langchain_core.runnables import RunnableBranch
    47|             branch = RunnableBranch(

# --- HUNK 2: Lines 166-374 ---
   166|                     ),
   167|                 )
   168|                 if expression_value:
   169|                     output = runnable.invoke(
   170|                         input,
   171|                         config=patch_config(
   172|                             config,
   173|                             callbacks=run_manager.get_child(tag=f"branch:{idx + 1}"),
   174|                         ),
   175|                         **kwargs,
   176|                     )
   177|                     break
   178|             else:
   179|                 output = self.default.invoke(
   180|                     input,
   181|                     config=patch_config(
   182|                         config, callbacks=run_manager.get_child(tag="branch:default")
   183|                     ),
   184|                     **kwargs,
   185|                 )
   186|         except BaseException as e:
   187|             run_manager.on_chain_error(e)
   188|             raise
   189|         run_manager.on_chain_end(dumpd(output))
   190|         return output
   191|     async def ainvoke(
   192|         self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
   193|     ) -> Output:
   194|         """Async version of invoke."""
   195|         config = ensure_config(config)
   196|         callback_manager = get_async_callback_manager_for_config(config)
   197|         run_manager = await callback_manager.on_chain_start(
   198|             dumpd(self),
   199|             input,
   200|             name=config.get("run_name"),
   201|         )
   202|         try:
   203|             for idx, branch in enumerate(self.branches):
   204|                 condition, runnable = branch
   205|                 expression_value = await condition.ainvoke(
   206|                     input,
   207|                     config=patch_config(
   208|                         config,
   209|                         callbacks=run_manager.get_child(tag=f"condition:{idx + 1}"),
   210|                     ),
   211|                 )
   212|                 if expression_value:
   213|                     output = await runnable.ainvoke(
   214|                         input,
   215|                         config=patch_config(
   216|                             config,
   217|                             callbacks=run_manager.get_child(tag=f"branch:{idx + 1}"),
   218|                         ),
   219|                         **kwargs,
   220|                     )
   221|                     break
   222|             else:
   223|                 output = await self.default.ainvoke(
   224|                     input,
   225|                     config=patch_config(
   226|                         config, callbacks=run_manager.get_child(tag="branch:default")
   227|                     ),
   228|                     **kwargs,
   229|                 )
   230|         except BaseException as e:
   231|             await run_manager.on_chain_error(e)
   232|             raise
   233|         await run_manager.on_chain_end(dumpd(output))
   234|         return output
   235|     def stream(
   236|         self,
   237|         input: Input,
   238|         config: Optional[RunnableConfig] = None,
   239|         **kwargs: Optional[Any],
   240|     ) -> Iterator[Output]:
   241|         """First evaluates the condition,
   242|         then delegate to true or false branch."""
   243|         config = ensure_config(config)
   244|         callback_manager = get_callback_manager_for_config(config)
   245|         run_manager = callback_manager.on_chain_start(
   246|             dumpd(self),
   247|             input,
   248|             name=config.get("run_name"),
   249|         )
   250|         final_output: Optional[Output] = None
   251|         final_output_supported = True
   252|         try:
   253|             for idx, branch in enumerate(self.branches):
   254|                 condition, runnable = branch
   255|                 expression_value = condition.invoke(
   256|                     input,
   257|                     config=patch_config(
   258|                         config,
   259|                         callbacks=run_manager.get_child(tag=f"condition:{idx + 1}"),
   260|                     ),
   261|                 )
   262|                 if expression_value:
   263|                     for chunk in runnable.stream(
   264|                         input,
   265|                         config=patch_config(
   266|                             config,
   267|                             callbacks=run_manager.get_child(tag=f"branch:{idx + 1}"),
   268|                         ),
   269|                         **kwargs,
   270|                     ):
   271|                         yield chunk
   272|                         if final_output_supported:
   273|                             if final_output is None:
   274|                                 final_output = chunk
   275|                             else:
   276|                                 try:
   277|                                     final_output = final_output + chunk  # type: ignore
   278|                                 except TypeError:
   279|                                     final_output = None
   280|                                     final_output_supported = False
   281|                     break
   282|             else:
   283|                 for chunk in self.default.stream(
   284|                     input,
   285|                     config=patch_config(
   286|                         config,
   287|                         callbacks=run_manager.get_child(tag="branch:default"),
   288|                     ),
   289|                     **kwargs,
   290|                 ):
   291|                     yield chunk
   292|                     if final_output_supported:
   293|                         if final_output is None:
   294|                             final_output = chunk
   295|                         else:
   296|                             try:
   297|                                 final_output = final_output + chunk  # type: ignore
   298|                             except TypeError:
   299|                                 final_output = None
   300|                                 final_output_supported = False
   301|         except BaseException as e:
   302|             run_manager.on_chain_error(e)
   303|             raise
   304|         run_manager.on_chain_end(final_output)
   305|     async def astream(
   306|         self,
   307|         input: Input,
   308|         config: Optional[RunnableConfig] = None,
   309|         **kwargs: Optional[Any],
   310|     ) -> AsyncIterator[Output]:
   311|         """First evaluates the condition,
   312|         then delegate to true or false branch."""
   313|         config = ensure_config(config)
   314|         callback_manager = get_async_callback_manager_for_config(config)
   315|         run_manager = await callback_manager.on_chain_start(
   316|             dumpd(self),
   317|             input,
   318|             name=config.get("run_name"),
   319|         )
   320|         final_output: Optional[Output] = None
   321|         final_output_supported = True
   322|         try:
   323|             for idx, branch in enumerate(self.branches):
   324|                 condition, runnable = branch
   325|                 expression_value = await condition.ainvoke(
   326|                     input,
   327|                     config=patch_config(
   328|                         config,
   329|                         callbacks=run_manager.get_child(tag=f"condition:{idx + 1}"),
   330|                     ),
   331|                 )
   332|                 if expression_value:
   333|                     async for chunk in runnable.astream(
   334|                         input,
   335|                         config=patch_config(
   336|                             config,
   337|                             callbacks=run_manager.get_child(tag=f"branch:{idx + 1}"),
   338|                         ),
   339|                         **kwargs,
   340|                     ):
   341|                         yield chunk
   342|                         if final_output_supported:
   343|                             if final_output is None:
   344|                                 final_output = chunk
   345|                             else:
   346|                                 try:
   347|                                     final_output = final_output + chunk  # type: ignore
   348|                                 except TypeError:
   349|                                     final_output = None
   350|                                     final_output_supported = False
   351|                     break
   352|             else:
   353|                 async for chunk in self.default.astream(
   354|                     input,
   355|                     config=patch_config(
   356|                         config,
   357|                         callbacks=run_manager.get_child(tag="branch:default"),
   358|                     ),
   359|                     **kwargs,
   360|                 ):
   361|                     yield chunk
   362|                     if final_output_supported:
   363|                         if final_output is None:
   364|                             final_output = chunk
   365|                         else:
   366|                             try:
   367|                                 final_output = final_output + chunk  # type: ignore
   368|                             except TypeError:
   369|                                 final_output = None
   370|                                 final_output_supported = False
   371|         except BaseException as e:
   372|             await run_manager.on_chain_error(e)
   373|             raise
   374|         await run_manager.on_chain_end(final_output)


# ====================================================================
# FILE: libs/core/langchain_core/runnables/config.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-24 ---
     1| from __future__ import annotations
     2| from concurrent.futures import Executor, ThreadPoolExecutor
     3| from contextlib import contextmanager
     4| from contextvars import Context, copy_context
     5| from typing import (
     6|     TYPE_CHECKING,
     7|     Any,
     8|     Awaitable,
     9|     Callable,
    10|     Dict,
    11|     Generator,
    12|     List,
    13|     Optional,
    14|     Union,
    15|     cast,
    16| )
    17| from typing_extensions import TypedDict
    18| from langchain_core.runnables.utils import (
    19|     Input,
    20|     Output,
    21|     accepts_config,
    22|     accepts_run_manager,
    23| )
    24| if TYPE_CHECKING:

# --- HUNK 2: Lines 304-343 ---
   304|     return CallbackManager.configure(
   305|         inheritable_callbacks=config.get("callbacks"),
   306|         inheritable_tags=config.get("tags"),
   307|         inheritable_metadata=config.get("metadata"),
   308|     )
   309| def get_async_callback_manager_for_config(
   310|     config: RunnableConfig,
   311| ) -> AsyncCallbackManager:
   312|     """Get an async callback manager for a config.
   313|     Args:
   314|         config (RunnableConfig): The config.
   315|     Returns:
   316|         AsyncCallbackManager: The async callback manager.
   317|     """
   318|     from langchain_core.callbacks.manager import AsyncCallbackManager
   319|     return AsyncCallbackManager.configure(
   320|         inheritable_callbacks=config.get("callbacks"),
   321|         inheritable_tags=config.get("tags"),
   322|         inheritable_metadata=config.get("metadata"),
   323|     )
   324| def _set_context(context: Context) -> None:
   325|     for var, value in context.items():
   326|         var.set(value)
   327| @contextmanager
   328| def get_executor_for_config(
   329|     config: Optional[RunnableConfig]
   330| ) -> Generator[Executor, None, None]:
   331|     """Get an executor for a config.
   332|     Args:
   333|         config (RunnableConfig): The config.
   334|     Yields:
   335|         Generator[Executor, None, None]: The executor.
   336|     """
   337|     config = config or {}
   338|     with ThreadPoolExecutor(
   339|         max_workers=config.get("max_concurrency"),
   340|         initializer=_set_context,
   341|         initargs=(copy_context(),),
   342|     ) as executor:
   343|         yield executor


# ====================================================================
# FILE: libs/core/langchain_core/runnables/configurable.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 7-89 ---
     7|     AsyncIterator,
     8|     Callable,
     9|     Dict,
    10|     Iterator,
    11|     List,
    12|     Optional,
    13|     Sequence,
    14|     Tuple,
    15|     Type,
    16|     Union,
    17|     cast,
    18| )
    19| from weakref import WeakValueDictionary
    20| from langchain_core.pydantic_v1 import BaseModel
    21| from langchain_core.runnables.base import Runnable, RunnableSerializable
    22| from langchain_core.runnables.config import (
    23|     RunnableConfig,
    24|     get_config_list,
    25|     get_executor_for_config,
    26| )
    27| from langchain_core.runnables.graph import Graph
    28| from langchain_core.runnables.utils import (
    29|     AnyConfigurableField,
    30|     ConfigurableField,
    31|     ConfigurableFieldMultiOption,
    32|     ConfigurableFieldSingleOption,
    33|     ConfigurableFieldSpec,
    34|     Input,
    35|     Output,
    36|     gather_with_concurrency,
    37|     get_unique_config_specs,
    38| )
    39| class DynamicRunnable(RunnableSerializable[Input, Output]):
    40|     """A Serializable Runnable that can be dynamically configured."""
    41|     default: RunnableSerializable[Input, Output]
    42|     class Config:
    43|         arbitrary_types_allowed = True
    44|     @classmethod
    45|     def is_lc_serializable(cls) -> bool:
    46|         return True
    47|     @classmethod
    48|     def get_lc_namespace(cls) -> List[str]:
    49|         """Get the namespace of the langchain object."""
    50|         return ["langchain", "schema", "runnable"]
    51|     @property
    52|     def InputType(self) -> Type[Input]:
    53|         return self.default.InputType
    54|     @property
    55|     def OutputType(self) -> Type[Output]:
    56|         return self.default.OutputType
    57|     def get_input_schema(
    58|         self, config: Optional[RunnableConfig] = None
    59|     ) -> Type[BaseModel]:
    60|         runnable, config = self._prepare(config)
    61|         return runnable.get_input_schema(config)
    62|     def get_output_schema(
    63|         self, config: Optional[RunnableConfig] = None
    64|     ) -> Type[BaseModel]:
    65|         runnable, config = self._prepare(config)
    66|         return runnable.get_output_schema(config)
    67|     def get_graph(self, config: Optional[RunnableConfig] = None) -> Graph:
    68|         runnable, config = self._prepare(config)
    69|         return runnable.get_graph(config)
    70|     @abstractmethod
    71|     def _prepare(
    72|         self, config: Optional[RunnableConfig] = None
    73|     ) -> Tuple[Runnable[Input, Output], RunnableConfig]:
    74|         ...
    75|     def invoke(
    76|         self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
    77|     ) -> Output:
    78|         runnable, config = self._prepare(config)
    79|         return runnable.invoke(input, config, **kwargs)
    80|     async def ainvoke(
    81|         self, input: Input, config: Optional[RunnableConfig] = None, **kwargs: Any
    82|     ) -> Output:
    83|         runnable, config = self._prepare(config)
    84|         return await runnable.ainvoke(input, config, **kwargs)
    85|     def batch(
    86|         self,
    87|         inputs: List[Input],
    88|         config: Optional[Union[RunnableConfig, List[RunnableConfig]]] = None,
    89|         *,


# ====================================================================
# FILE: libs/core/langchain_core/runnables/graph.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-117 ---
     1| from __future__ import annotations
     2| from dataclasses import dataclass, field
     3| from typing import TYPE_CHECKING, Dict, List, NamedTuple, Optional, Type, Union
     4| from uuid import uuid4
     5| from langchain_core.pydantic_v1 import BaseModel
     6| from langchain_core.runnables.graph_draw import draw
     7| if TYPE_CHECKING:
     8|     from langchain_core.runnables.base import Runnable as RunnableType
     9| class Edge(NamedTuple):
    10|     source: str
    11|     target: str
    12| class Node(NamedTuple):
    13|     id: str
    14|     data: Union[Type[BaseModel], RunnableType]
    15| @dataclass
    16| class Graph:
    17|     nodes: Dict[str, Node] = field(default_factory=dict)
    18|     edges: List[Edge] = field(default_factory=list)
    19|     def __bool__(self) -> bool:
    20|         return bool(self.nodes)
    21|     def next_id(self) -> str:
    22|         return uuid4().hex
    23|     def add_node(self, data: Union[Type[BaseModel], RunnableType]) -> Node:
    24|         """Add a node to the graph and return it."""
    25|         node = Node(id=self.next_id(), data=data)
    26|         self.nodes[node.id] = node
    27|         return node
    28|     def remove_node(self, node: Node) -> None:
    29|         """Remove a node from the graphm and all edges connected to it."""
    30|         self.nodes.pop(node.id)
    31|         self.edges = [
    32|             edge
    33|             for edge in self.edges
    34|             if edge.source != node.id and edge.target != node.id
    35|         ]
    36|     def add_edge(self, source: Node, target: Node) -> Edge:
    37|         """Add an edge to the graph and return it."""
    38|         if source.id not in self.nodes:
    39|             raise ValueError(f"Source node {source.id} not in graph")
    40|         if target.id not in self.nodes:
    41|             raise ValueError(f"Target node {target.id} not in graph")
    42|         edge = Edge(source=source.id, target=target.id)
    43|         self.edges.append(edge)
    44|         return edge
    45|     def extend(self, graph: Graph) -> None:
    46|         """Add all nodes and edges from another graph.
    47|         Note this doesn't check for duplicates, nor does it connect the graphs."""
    48|         self.nodes.update(graph.nodes)
    49|         self.edges.extend(graph.edges)
    50|     def first_node(self) -> Optional[Node]:
    51|         """Find the single node that is not a target of any edge.
    52|         If there is no such node, or there are multiple, return None.
    53|         When drawing the graph this node would be the origin."""
    54|         targets = {edge.target for edge in self.edges}
    55|         found: List[Node] = []
    56|         for node in self.nodes.values():
    57|             if node.id not in targets:
    58|                 found.append(node)
    59|         return found[0] if len(found) == 1 else None
    60|     def last_node(self) -> Optional[Node]:
    61|         """Find the single node that is not a source of any edge.
    62|         If there is no such node, or there are multiple, return None.
    63|         When drawing the graph this node would be the destination.
    64|         """
    65|         sources = {edge.source for edge in self.edges}
    66|         found: List[Node] = []
    67|         for node in self.nodes.values():
    68|             if node.id not in sources:
    69|                 found.append(node)
    70|         return found[0] if len(found) == 1 else None
    71|     def trim_first_node(self) -> None:
    72|         """Remove the first node if it exists and has a single outgoing edge,
    73|         ie. if removing it would not leave the graph without a "first" node."""
    74|         first_node = self.first_node()
    75|         if first_node:
    76|             if (
    77|                 len(self.nodes) == 1
    78|                 or len([edge for edge in self.edges if edge.source == first_node.id])
    79|                 == 1
    80|             ):
    81|                 self.remove_node(first_node)
    82|     def trim_last_node(self) -> None:
    83|         """Remove the last node if it exists and has a single incoming edge,
    84|         ie. if removing it would not leave the graph without a "last" node."""
    85|         last_node = self.last_node()
    86|         if last_node:
    87|             if (
    88|                 len(self.nodes) == 1
    89|                 or len([edge for edge in self.edges if edge.target == last_node.id])
    90|                 == 1
    91|             ):
    92|                 self.remove_node(last_node)
    93|     def draw_ascii(self) -> str:
    94|         from langchain_core.runnables.base import Runnable
    95|         def node_data(node: Node) -> str:
    96|             if isinstance(node.data, Runnable):
    97|                 try:
    98|                     data = str(node.data)
    99|                     if (
   100|                         data.startswith("<")
   101|                         or data[0] != data[0].upper()
   102|                         or len(data.splitlines()) > 1
   103|                     ):
   104|                         data = node.data.__class__.__name__
   105|                     elif len(data) > 42:
   106|                         data = data[:42] + "..."
   107|                 except Exception:
   108|                     data = node.data.__class__.__name__
   109|             else:
   110|                 data = node.data.__name__
   111|             return data if not data.startswith("Runnable") else data[8:]
   112|         return draw(
   113|             {node.id: node_data(node) for node in self.nodes.values()},
   114|             [(edge.source, edge.target) for edge in self.edges],
   115|         )
   116|     def print_ascii(self) -> None:
   117|         print(self.draw_ascii())


# ====================================================================
# FILE: libs/core/langchain_core/runnables/graph_draw.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-227 ---
     1| """Draws DAG in ASCII.
     2| Adapted from https://github.com/iterative/dvc/blob/main/dvc/dagascii.py"""
     3| import math
     4| import os
     5| from typing import Any, Mapping, Sequence, Tuple
     6| class VertexViewer:
     7|     """Class to define vertex box boundaries that will be accounted for during
     8|     graph building by grandalf.
     9|     Args:
    10|         name (str): name of the vertex.
    11|     """
    12|     HEIGHT = 3  # top and bottom box edges + text
    13|     def __init__(self, name: str) -> None:
    14|         self._h = self.HEIGHT  # top and bottom box edges + text
    15|         self._w = len(name) + 2  # right and left bottom edges + text
    16|     @property
    17|     def h(self) -> int:
    18|         """Height of the box."""
    19|         return self._h
    20|     @property
    21|     def w(self) -> int:
    22|         """Width of the box."""
    23|         return self._w
    24| class AsciiCanvas:
    25|     """Class for drawing in ASCII.
    26|     Args:
    27|         cols (int): number of columns in the canvas. Should be > 1.
    28|         lines (int): number of lines in the canvas. Should be > 1.
    29|     """
    30|     TIMEOUT = 10
    31|     def __init__(self, cols: int, lines: int) -> None:
    32|         assert cols > 1
    33|         assert lines > 1
    34|         self.cols = cols
    35|         self.lines = lines
    36|         self.canvas = [[" "] * cols for line in range(lines)]
    37|     def draw(self) -> str:
    38|         """Draws ASCII canvas on the screen."""
    39|         lines = map("".join, self.canvas)
    40|         return os.linesep.join(lines)
    41|     def point(self, x: int, y: int, char: str) -> None:
    42|         """Create a point on ASCII canvas.
    43|         Args:
    44|             x (int): x coordinate. Should be >= 0 and < number of columns in
    45|                 the canvas.
    46|             y (int): y coordinate. Should be >= 0 an < number of lines in the
    47|                 canvas.
    48|             char (str): character to place in the specified point on the
    49|                 canvas.
    50|         """
    51|         assert len(char) == 1
    52|         assert x >= 0
    53|         assert x < self.cols
    54|         assert y >= 0
    55|         assert y < self.lines
    56|         self.canvas[y][x] = char
    57|     def line(self, x0: int, y0: int, x1: int, y1: int, char: str) -> None:
    58|         """Create a line on ASCII canvas.
    59|         Args:
    60|             x0 (int): x coordinate where the line should start.
    61|             y0 (int): y coordinate where the line should start.
    62|             x1 (int): x coordinate where the line should end.
    63|             y1 (int): y coordinate where the line should end.
    64|             char (str): character to draw the line with.
    65|         """
    66|         if x0 > x1:
    67|             x1, x0 = x0, x1
    68|             y1, y0 = y0, y1
    69|         dx = x1 - x0
    70|         dy = y1 - y0
    71|         if dx == 0 and dy == 0:
    72|             self.point(x0, y0, char)
    73|         elif abs(dx) >= abs(dy):
    74|             for x in range(x0, x1 + 1):
    75|                 if dx == 0:
    76|                     y = y0
    77|                 else:
    78|                     y = y0 + int(round((x - x0) * dy / float(dx)))
    79|                 self.point(x, y, char)
    80|         elif y0 < y1:
    81|             for y in range(y0, y1 + 1):
    82|                 if dy == 0:
    83|                     x = x0
    84|                 else:
    85|                     x = x0 + int(round((y - y0) * dx / float(dy)))
    86|                 self.point(x, y, char)
    87|         else:
    88|             for y in range(y1, y0 + 1):
    89|                 if dy == 0:
    90|                     x = x0
    91|                 else:
    92|                     x = x1 + int(round((y - y1) * dx / float(dy)))
    93|                 self.point(x, y, char)
    94|     def text(self, x: int, y: int, text: str) -> None:
    95|         """Print a text on ASCII canvas.
    96|         Args:
    97|             x (int): x coordinate where the text should start.
    98|             y (int): y coordinate where the text should start.
    99|             text (str): string that should be printed.
   100|         """
   101|         for i, char in enumerate(text):
   102|             self.point(x + i, y, char)
   103|     def box(self, x0: int, y0: int, width: int, height: int) -> None:
   104|         """Create a box on ASCII canvas.
   105|         Args:
   106|             x0 (int): x coordinate of the box corner.
   107|             y0 (int): y coordinate of the box corner.
   108|             width (int): box width.
   109|             height (int): box height.
   110|         """
   111|         assert width > 1
   112|         assert height > 1
   113|         width -= 1
   114|         height -= 1
   115|         for x in range(x0, x0 + width):
   116|             self.point(x, y0, "-")
   117|             self.point(x, y0 + height, "-")
   118|         for y in range(y0, y0 + height):
   119|             self.point(x0, y, "|")
   120|             self.point(x0 + width, y, "|")
   121|         self.point(x0, y0, "+")
   122|         self.point(x0 + width, y0, "+")
   123|         self.point(x0, y0 + height, "+")
   124|         self.point(x0 + width, y0 + height, "+")
   125| def _build_sugiyama_layout(
   126|     vertices: Mapping[str, str], edges: Sequence[Tuple[str, str]]
   127| ) -> Any:
   128|     try:
   129|         from grandalf.graphs import Edge, Graph, Vertex  # type: ignore[import]
   130|         from grandalf.layouts import SugiyamaLayout  # type: ignore[import]
   131|         from grandalf.routing import (  # type: ignore[import]
   132|             EdgeViewer,
   133|             route_with_lines,
   134|         )
   135|     except ImportError:
   136|         print("Install grandalf to draw graphs. `pip install grandalf`")
   137|         raise
   138|     vertices_ = {id: Vertex(f" {data} ") for id, data in vertices.items()}
   139|     edges_ = [Edge(vertices_[s], vertices_[e]) for s, e in edges]
   140|     vertices_list = vertices_.values()
   141|     graph = Graph(vertices_list, edges_)
   142|     for vertex in vertices_list:
   143|         vertex.view = VertexViewer(vertex.data)
   144|     minw = min(v.view.w for v in vertices_list)
   145|     for edge in edges_:
   146|         edge.view = EdgeViewer()
   147|     sug = SugiyamaLayout(graph.C[0])
   148|     graph = graph.C[0]
   149|     roots = list(filter(lambda x: len(x.e_in()) == 0, graph.sV))
   150|     sug.init_all(roots=roots, optimize=True)
   151|     sug.yspace = VertexViewer.HEIGHT
   152|     sug.xspace = minw
   153|     sug.route_edge = route_with_lines
   154|     sug.draw()
   155|     return sug
   156| def draw(vertices: Mapping[str, str], edges: Sequence[Tuple[str, str]]) -> str:
   157|     """Build a DAG and draw it in ASCII.
   158|     Args:
   159|         vertices (list): list of graph vertices.
   160|         edges (list): list of graph edges.
   161|     Returns:
   162|         str: ASCII representation
   163|     Example:
   164|         >>> from dvc.dagascii import draw
   165|         >>> vertices = [1, 2, 3, 4]
   166|         >>> edges = [(1, 2), (2, 3), (2, 4), (1, 4)]
   167|         >>> print(draw(vertices, edges))
   168|         +---+     +---+
   169|         | 3 |     | 4 |
   170|         +---+    *+---+
   171|           *    **   *
   172|           *  **     *
   173|           * *       *
   174|         +---+       *
   175|         | 2 |      *
   176|         +---+     *
   177|              *    *
   178|               *  *
   179|                **
   180|              +---+
   181|              | 1 |
   182|              +---+
   183|     """
   184|     Xs = []  # noqa: N806
   185|     Ys = []  # noqa: N806
   186|     sug = _build_sugiyama_layout(vertices, edges)
   187|     for vertex in sug.g.sV:
   188|         Xs.append(vertex.view.xy[0] - vertex.view.w / 2.0)
   189|         Xs.append(vertex.view.xy[0] + vertex.view.w / 2.0)
   190|         Ys.append(vertex.view.xy[1])
   191|         Ys.append(vertex.view.xy[1] + vertex.view.h)
   192|     for edge in sug.g.sE:
   193|         for x, y in edge.view._pts:
   194|             Xs.append(x)
   195|             Ys.append(y)
   196|     minx = min(Xs)
   197|     miny = min(Ys)
   198|     maxx = max(Xs)
   199|     maxy = max(Ys)
   200|     canvas_cols = int(math.ceil(math.ceil(maxx) - math.floor(minx))) + 1
   201|     canvas_lines = int(round(maxy - miny))
   202|     canvas = AsciiCanvas(canvas_cols, canvas_lines)
   203|     for edge in sug.g.sE:
   204|         assert len(edge.view._pts) > 1
   205|         for index in range(1, len(edge.view._pts)):
   206|             start = edge.view._pts[index - 1]
   207|             end = edge.view._pts[index]
   208|             start_x = int(round(start[0] - minx))
   209|             start_y = int(round(start[1] - miny))
   210|             end_x = int(round(end[0] - minx))
   211|             end_y = int(round(end[1] - miny))
   212|             assert start_x >= 0
   213|             assert start_y >= 0
   214|             assert end_x >= 0
   215|             assert end_y >= 0
   216|             canvas.line(start_x, start_y, end_x, end_y, "*")
   217|     for vertex in sug.g.sV:
   218|         x = vertex.view.xy[0] - vertex.view.w / 2.0
   219|         y = vertex.view.xy[1]
   220|         canvas.box(
   221|             int(round(x - minx)),
   222|             int(round(y - miny)),
   223|             vertex.view.w,
   224|             vertex.view.h,
   225|         )
   226|         canvas.text(int(round(x - minx)) + 1, int(round(y - miny)) + 1, vertex.data)
   227|     return canvas.draw()


# ====================================================================
# FILE: libs/core/langchain_core/runnables/passthrough.py
# Total hunks: 4
# ====================================================================
# --- HUNK 1: Lines 15-55 ---
    15|     Mapping,
    16|     Optional,
    17|     Type,
    18|     Union,
    19|     cast,
    20| )
    21| from langchain_core.pydantic_v1 import BaseModel, create_model
    22| from langchain_core.runnables.base import (
    23|     Other,
    24|     Runnable,
    25|     RunnableParallel,
    26|     RunnableSerializable,
    27| )
    28| from langchain_core.runnables.config import (
    29|     RunnableConfig,
    30|     acall_func_with_variable_args,
    31|     call_func_with_variable_args,
    32|     get_executor_for_config,
    33|     patch_config,
    34| )
    35| from langchain_core.runnables.graph import Graph
    36| from langchain_core.runnables.utils import AddableDict, ConfigurableFieldSpec
    37| from langchain_core.utils.aiter import atee, py_anext
    38| from langchain_core.utils.iter import safetee
    39| if TYPE_CHECKING:
    40|     from langchain_core.callbacks.manager import (
    41|         AsyncCallbackManagerForChainRun,
    42|         CallbackManagerForChainRun,
    43|     )
    44| def identity(x: Other) -> Other:
    45|     """An identity function"""
    46|     return x
    47| async def aidentity(x: Other) -> Other:
    48|     """An async identity function"""
    49|     return x
    50| class RunnablePassthrough(RunnableSerializable[Other, Other]):
    51|     """A runnable to passthrough inputs unchanged or with additional keys.
    52|     This runnable behaves almost like the identity function, except that it
    53|     can be configured to add additional keys to the output, if the input is a
    54|     dict.
    55|     The examples below demonstrate this runnable works using a few simple

# --- HUNK 2: Lines 226-326 ---
   226|                     )
   227|                 elif self.func is not None:
   228|                     call_func_with_variable_args(self.func, final, config, **kwargs)
   229|     def stream(
   230|         self,
   231|         input: Other,
   232|         config: Optional[RunnableConfig] = None,
   233|         **kwargs: Any,
   234|     ) -> Iterator[Other]:
   235|         return self.transform(iter([input]), config, **kwargs)
   236|     async def astream(
   237|         self,
   238|         input: Other,
   239|         config: Optional[RunnableConfig] = None,
   240|         **kwargs: Any,
   241|     ) -> AsyncIterator[Other]:
   242|         async def input_aiter() -> AsyncIterator[Other]:
   243|             yield input
   244|         async for chunk in self.atransform(input_aiter(), config, **kwargs):
   245|             yield chunk
   246| _graph_passthrough: RunnablePassthrough = RunnablePassthrough()
   247| class RunnableAssign(RunnableSerializable[Dict[str, Any], Dict[str, Any]]):
   248|     """
   249|     A runnable that assigns key-value pairs to Dict[str, Any] inputs.
   250|     """
   251|     mapper: RunnableParallel[Dict[str, Any]]
   252|     def __init__(self, mapper: RunnableParallel[Dict[str, Any]], **kwargs: Any) -> None:
   253|         super().__init__(mapper=mapper, **kwargs)
   254|     @classmethod
   255|     def is_lc_serializable(cls) -> bool:
   256|         return True
   257|     @classmethod
   258|     def get_lc_namespace(cls) -> List[str]:
   259|         """Get the namespace of the langchain object."""
   260|         return ["langchain", "schema", "runnable"]
   261|     def get_name(
   262|         self, suffix: Optional[str] = None, *, name: Optional[str] = None
   263|     ) -> str:
   264|         name = (
   265|             name or self.name or f"RunnableAssign<{','.join(self.mapper.steps.keys())}>"
   266|         )
   267|         return super().get_name(suffix, name=name)
   268|     def get_input_schema(
   269|         self, config: Optional[RunnableConfig] = None
   270|     ) -> Type[BaseModel]:
   271|         map_input_schema = self.mapper.get_input_schema(config)
   272|         if not map_input_schema.__custom_root_type__:
   273|             return map_input_schema
   274|         return super().get_input_schema(config)
   275|     def get_output_schema(
   276|         self, config: Optional[RunnableConfig] = None
   277|     ) -> Type[BaseModel]:
   278|         map_input_schema = self.mapper.get_input_schema(config)
   279|         map_output_schema = self.mapper.get_output_schema(config)
   280|         if (
   281|             not map_input_schema.__custom_root_type__
   282|             and not map_output_schema.__custom_root_type__
   283|         ):
   284|             return create_model(  # type: ignore[call-overload]
   285|                 "RunnableAssignOutput",
   286|                 **{
   287|                     k: (v.type_, v.default)
   288|                     for s in (map_input_schema, map_output_schema)
   289|                     for k, v in s.__fields__.items()
   290|                 },
   291|             )
   292|         elif not map_output_schema.__custom_root_type__:
   293|             return map_output_schema
   294|         return super().get_output_schema(config)
   295|     @property
   296|     def config_specs(self) -> List[ConfigurableFieldSpec]:
   297|         return self.mapper.config_specs
   298|     def get_graph(self, config: RunnableConfig | None = None) -> Graph:
   299|         graph = self.mapper.get_graph(config)
   300|         input_node = graph.first_node()
   301|         output_node = graph.last_node()
   302|         if input_node is not None and output_node is not None:
   303|             passthrough_node = graph.add_node(_graph_passthrough)
   304|             graph.add_edge(input_node, passthrough_node)
   305|             graph.add_edge(passthrough_node, output_node)
   306|         return graph
   307|     def _invoke(
   308|         self,
   309|         input: Dict[str, Any],
   310|         run_manager: CallbackManagerForChainRun,
   311|         config: RunnableConfig,
   312|         **kwargs: Any,
   313|     ) -> Dict[str, Any]:
   314|         assert isinstance(
   315|             input, dict
   316|         ), "The input to RunnablePassthrough.assign() must be a dict."
   317|         return {
   318|             **input,
   319|             **self.mapper.invoke(
   320|                 input,
   321|                 patch_config(config, callbacks=run_manager.get_child()),
   322|                 **kwargs,
   323|             ),
   324|         }
   325|     def invoke(
   326|         self,

# --- HUNK 3: Lines 440-570 ---
   440|             input, self._atransform, config, **kwargs
   441|         ):
   442|             yield chunk
   443|     def stream(
   444|         self,
   445|         input: Dict[str, Any],
   446|         config: Optional[RunnableConfig] = None,
   447|         **kwargs: Any,
   448|     ) -> Iterator[Dict[str, Any]]:
   449|         return self.transform(iter([input]), config, **kwargs)
   450|     async def astream(
   451|         self,
   452|         input: Dict[str, Any],
   453|         config: Optional[RunnableConfig] = None,
   454|         **kwargs: Any,
   455|     ) -> AsyncIterator[Dict[str, Any]]:
   456|         async def input_aiter() -> AsyncIterator[Dict[str, Any]]:
   457|             yield input
   458|         async for chunk in self.atransform(input_aiter(), config, **kwargs):
   459|             yield chunk
   460| class RunnablePick(RunnableSerializable[Dict[str, Any], Dict[str, Any]]):
   461|     """
   462|     A runnable that picks keys from Dict[str, Any] inputs.
   463|     """
   464|     keys: Union[str, List[str]]
   465|     def __init__(self, keys: Union[str, List[str]], **kwargs: Any) -> None:
   466|         super().__init__(keys=keys, **kwargs)
   467|     @classmethod
   468|     def is_lc_serializable(cls) -> bool:
   469|         return True
   470|     @classmethod
   471|     def get_lc_namespace(cls) -> List[str]:
   472|         """Get the namespace of the langchain object."""
   473|         return ["langchain", "schema", "runnable"]
   474|     def get_name(
   475|         self, suffix: Optional[str] = None, *, name: Optional[str] = None
   476|     ) -> str:
   477|         name = (
   478|             name
   479|             or self.name
   480|             or f"RunnablePick<{','.join([self.keys] if isinstance(self.keys, str) else self.keys)}>"  # noqa: E501
   481|         )
   482|         return super().get_name(suffix, name=name)
   483|     def _pick(self, input: Dict[str, Any]) -> Any:
   484|         assert isinstance(
   485|             input, dict
   486|         ), "The input to RunnablePassthrough.assign() must be a dict."
   487|         if isinstance(self.keys, str):
   488|             return input.get(self.keys)
   489|         else:
   490|             picked = {k: input.get(k) for k in self.keys if k in input}
   491|             if picked:
   492|                 return AddableDict(picked)
   493|             else:
   494|                 return None
   495|     def _invoke(
   496|         self,
   497|         input: Dict[str, Any],
   498|     ) -> Dict[str, Any]:
   499|         return self._pick(input)
   500|     def invoke(
   501|         self,
   502|         input: Dict[str, Any],
   503|         config: Optional[RunnableConfig] = None,
   504|         **kwargs: Any,
   505|     ) -> Dict[str, Any]:
   506|         return self._call_with_config(self._invoke, input, config, **kwargs)
   507|     async def _ainvoke(
   508|         self,
   509|         input: Dict[str, Any],
   510|     ) -> Dict[str, Any]:
   511|         return self._pick(input)
   512|     async def ainvoke(
   513|         self,
   514|         input: Dict[str, Any],
   515|         config: Optional[RunnableConfig] = None,
   516|         **kwargs: Any,
   517|     ) -> Dict[str, Any]:
   518|         return await self._acall_with_config(self._ainvoke, input, config, **kwargs)
   519|     def _transform(
   520|         self,
   521|         input: Iterator[Dict[str, Any]],
   522|     ) -> Iterator[Dict[str, Any]]:
   523|         for chunk in input:
   524|             picked = self._pick(chunk)
   525|             if picked is not None:
   526|                 yield picked
   527|     def transform(
   528|         self,
   529|         input: Iterator[Dict[str, Any]],
   530|         config: Optional[RunnableConfig] = None,
   531|         **kwargs: Any,
   532|     ) -> Iterator[Dict[str, Any]]:
   533|         yield from self._transform_stream_with_config(
   534|             input, self._transform, config, **kwargs
   535|         )
   536|     async def _atransform(
   537|         self,
   538|         input: AsyncIterator[Dict[str, Any]],
   539|     ) -> AsyncIterator[Dict[str, Any]]:
   540|         async for chunk in input:
   541|             picked = self._pick(chunk)
   542|             if picked is not None:
   543|                 yield picked
   544|     async def atransform(
   545|         self,
   546|         input: AsyncIterator[Dict[str, Any]],
   547|         config: Optional[RunnableConfig] = None,
   548|         **kwargs: Any,
   549|     ) -> AsyncIterator[Dict[str, Any]]:
   550|         async for chunk in self._atransform_stream_with_config(
   551|             input, self._atransform, config, **kwargs
   552|         ):
   553|             yield chunk
   554|     def stream(
   555|         self,
   556|         input: Dict[str, Any],
   557|         config: Optional[RunnableConfig] = None,
   558|         **kwargs: Any,
   559|     ) -> Iterator[Dict[str, Any]]:
   560|         return self.transform(iter([input]), config, **kwargs)
   561|     async def astream(
   562|         self,
   563|         input: Dict[str, Any],
   564|         config: Optional[RunnableConfig] = None,
   565|         **kwargs: Any,
   566|     ) -> AsyncIterator[Dict[str, Any]]:
   567|         async def input_aiter() -> AsyncIterator[Dict[str, Any]]:
   568|             yield input
   569|         async for chunk in self.atransform(input_aiter(), config, **kwargs):
   570|             yield chunk


# ====================================================================
# FILE: libs/core/langchain_core/runnables/utils.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 65-214 ---
    65|             and isinstance(node.slice, ast.Constant)
    66|             and isinstance(node.slice.value, str)
    67|         ):
    68|             self.keys.add(node.slice.value)
    69|     def visit_Call(self, node: ast.Call) -> Any:
    70|         if (
    71|             isinstance(node.func, ast.Attribute)
    72|             and isinstance(node.func.value, ast.Name)
    73|             and node.func.value.id == self.name
    74|             and node.func.attr == "get"
    75|             and len(node.args) in (1, 2)
    76|             and isinstance(node.args[0], ast.Constant)
    77|             and isinstance(node.args[0].value, str)
    78|         ):
    79|             self.keys.add(node.args[0].value)
    80| class IsFunctionArgDict(ast.NodeVisitor):
    81|     """Check if the first argument of a function is a dict."""
    82|     def __init__(self) -> None:
    83|         self.keys: Set[str] = set()
    84|     def visit_Lambda(self, node: ast.Lambda) -> Any:
    85|         if not node.args.args:
    86|             return
    87|         input_arg_name = node.args.args[0].arg
    88|         IsLocalDict(input_arg_name, self.keys).visit(node.body)
    89|     def visit_FunctionDef(self, node: ast.FunctionDef) -> Any:
    90|         if not node.args.args:
    91|             return
    92|         input_arg_name = node.args.args[0].arg
    93|         IsLocalDict(input_arg_name, self.keys).visit(node)
    94|     def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> Any:
    95|         if not node.args.args:
    96|             return
    97|         input_arg_name = node.args.args[0].arg
    98|         IsLocalDict(input_arg_name, self.keys).visit(node)
    99| class NonLocals(ast.NodeVisitor):
   100|     """Get nonlocal variables accessed."""
   101|     def __init__(self) -> None:
   102|         self.loads: Set[str] = set()
   103|         self.stores: Set[str] = set()
   104|     def visit_Name(self, node: ast.Name) -> Any:
   105|         if isinstance(node.ctx, ast.Load):
   106|             self.loads.add(node.id)
   107|         elif isinstance(node.ctx, ast.Store):
   108|             self.stores.add(node.id)
   109|     def visit_Attribute(self, node: ast.Attribute) -> Any:
   110|         if isinstance(node.ctx, ast.Load):
   111|             parent = node.value
   112|             attr_expr = node.attr
   113|             while isinstance(parent, ast.Attribute):
   114|                 attr_expr = parent.attr + "." + attr_expr
   115|                 parent = parent.value
   116|             if isinstance(parent, ast.Name):
   117|                 self.loads.add(parent.id + "." + attr_expr)
   118|                 self.loads.discard(parent.id)
   119| class FunctionNonLocals(ast.NodeVisitor):
   120|     """Get the nonlocal variables accessed of a function."""
   121|     def __init__(self) -> None:
   122|         self.nonlocals: Set[str] = set()
   123|     def visit_FunctionDef(self, node: ast.FunctionDef) -> Any:
   124|         visitor = NonLocals()
   125|         visitor.visit(node)
   126|         self.nonlocals.update(visitor.loads - visitor.stores)
   127|     def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> Any:
   128|         visitor = NonLocals()
   129|         visitor.visit(node)
   130|         self.nonlocals.update(visitor.loads - visitor.stores)
   131|     def visit_Lambda(self, node: ast.Lambda) -> Any:
   132|         visitor = NonLocals()
   133|         visitor.visit(node)
   134|         self.nonlocals.update(visitor.loads - visitor.stores)
   135| class GetLambdaSource(ast.NodeVisitor):
   136|     """Get the source code of a lambda function."""
   137|     def __init__(self) -> None:
   138|         """Initialize the visitor."""
   139|         self.source: Optional[str] = None
   140|         self.count = 0
   141|     def visit_Lambda(self, node: ast.Lambda) -> Any:
   142|         """Visit a lambda function."""
   143|         self.count += 1
   144|         if hasattr(ast, "unparse"):
   145|             self.source = ast.unparse(node)
   146| def get_function_first_arg_dict_keys(func: Callable) -> Optional[List[str]]:
   147|     """Get the keys of the first argument of a function if it is a dict."""
   148|     try:
   149|         code = inspect.getsource(func)
   150|         tree = ast.parse(textwrap.dedent(code))
   151|         visitor = IsFunctionArgDict()
   152|         visitor.visit(tree)
   153|         return list(visitor.keys) if visitor.keys else None
   154|     except (SyntaxError, TypeError, OSError):
   155|         return None
   156| def get_lambda_source(func: Callable) -> Optional[str]:
   157|     """Get the source code of a lambda function.
   158|     Args:
   159|         func: a callable that can be a lambda function
   160|     Returns:
   161|         str: the source code of the lambda function
   162|     """
   163|     try:
   164|         name = func.__name__ if func.__name__ != "<lambda>" else None
   165|     except AttributeError:
   166|         name = None
   167|     try:
   168|         code = inspect.getsource(func)
   169|         tree = ast.parse(textwrap.dedent(code))
   170|         visitor = GetLambdaSource()
   171|         visitor.visit(tree)
   172|         return visitor.source if visitor.count == 1 else name
   173|     except (SyntaxError, TypeError, OSError):
   174|         return name
   175| def get_function_nonlocals(func: Callable) -> List[Any]:
   176|     """Get the nonlocal variables accessed by a function."""
   177|     try:
   178|         code = inspect.getsource(func)
   179|         tree = ast.parse(textwrap.dedent(code))
   180|         visitor = FunctionNonLocals()
   181|         visitor.visit(tree)
   182|         values: List[Any] = []
   183|         for k, v in inspect.getclosurevars(func).nonlocals.items():
   184|             if k in visitor.nonlocals:
   185|                 values.append(v)
   186|             for kk in visitor.nonlocals:
   187|                 if "." in kk and kk.startswith(k):
   188|                     vv = v
   189|                     for part in kk.split(".")[1:]:
   190|                         vv = getattr(vv, part)
   191|                     values.append(vv)
   192|         return values
   193|     except (SyntaxError, TypeError, OSError):
   194|         return []
   195| def indent_lines_after_first(text: str, prefix: str) -> str:
   196|     """Indent all lines of text after the first line.
   197|     Args:
   198|         text:  The text to indent
   199|         prefix: Used to determine the number of spaces to indent
   200|     Returns:
   201|         str: The indented text
   202|     """
   203|     n_spaces = len(prefix)
   204|     spaces = " " * n_spaces
   205|     lines = text.splitlines()
   206|     return "\n".join([lines[0]] + [spaces + line for line in lines[1:]])
   207| class AddableDict(Dict[str, Any]):
   208|     """
   209|     Dictionary that can be added to another dictionary.
   210|     """
   211|     def __add__(self, other: AddableDict) -> AddableDict:
   212|         chunk = AddableDict(self)
   213|         for key in other:
   214|             if key not in chunk or chunk[key] is None:


# ====================================================================
# FILE: libs/core/langchain_core/tracers/base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 84-124 ---
    84|                 run.child_execution_order is not None
    85|                 and parent_run.child_execution_order is not None
    86|                 and run.child_execution_order > parent_run.child_execution_order
    87|             ):
    88|                 parent_run.child_execution_order = run.child_execution_order
    89|         self.run_map.pop(str(run.id))
    90|         self._on_run_update(run)
    91|     def _get_execution_order(self, parent_run_id: Optional[str] = None) -> int:
    92|         """Get the execution order for a run."""
    93|         if parent_run_id is None:
    94|             return 1
    95|         parent_run = self.run_map.get(parent_run_id)
    96|         if parent_run is None:
    97|             logger.debug(f"Parent run with UUID {parent_run_id} not found.")
    98|             return 1
    99|         if parent_run.child_execution_order is None:
   100|             raise TracerException(
   101|                 f"Parent run with UUID {parent_run_id} has no child execution order."
   102|             )
   103|         return parent_run.child_execution_order + 1
   104|     def _get_run(self, run_id: UUID, run_type: Optional[str] = None) -> Run:
   105|         try:
   106|             run = self.run_map[str(run_id)]
   107|         except KeyError as exc:
   108|             raise TracerException(f"No indexed run ID {run_id}.") from exc
   109|         if run_type is not None and run.run_type != run_type:
   110|             raise TracerException(
   111|                 f"Found {run.run_type} run at ID {run_id}, but expected {run_type} run."
   112|             )
   113|         return run
   114|     def on_llm_start(
   115|         self,
   116|         serialized: Dict[str, Any],
   117|         prompts: List[str],
   118|         *,
   119|         run_id: UUID,
   120|         tags: Optional[List[str]] = None,
   121|         parent_run_id: Optional[UUID] = None,
   122|         metadata: Optional[Dict[str, Any]] = None,
   123|         name: Optional[str] = None,
   124|         **kwargs: Any,


# ====================================================================
# FILE: libs/core/langchain_core/tracers/log_stream.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 19-60 ---
    19| from langchain_core.load import load
    20| from langchain_core.outputs import ChatGenerationChunk, GenerationChunk
    21| from langchain_core.tracers.base import BaseTracer
    22| from langchain_core.tracers.schemas import Run
    23| class LogEntry(TypedDict):
    24|     """A single entry in the run log."""
    25|     id: str
    26|     """ID of the sub-run."""
    27|     name: str
    28|     """Name of the object being run."""
    29|     type: str
    30|     """Type of the object being run, eg. prompt, chain, llm, etc."""
    31|     tags: List[str]
    32|     """List of tags for the run."""
    33|     metadata: Dict[str, Any]
    34|     """Key-value pairs of metadata for the run."""
    35|     start_time: str
    36|     """ISO-8601 timestamp of when the run started."""
    37|     streamed_output_str: List[str]
    38|     """List of LLM tokens streamed by this run, if applicable."""
    39|     streamed_output: List[Any]
    40|     """List of output chunks streamed by this run, if available."""
    41|     final_output: Optional[Any]
    42|     """Final output of this run.
    43|     Only available after the run has finished successfully."""
    44|     end_time: Optional[str]
    45|     """ISO-8601 timestamp of when the run ended.
    46|     Only available after the run has finished."""
    47| class RunState(TypedDict):
    48|     """State of the run."""
    49|     id: str
    50|     """ID of the run."""
    51|     streamed_output: List[Any]
    52|     """List of output chunks streamed by Runnable.stream()"""
    53|     final_output: Optional[Any]
    54|     """Final output of the run, usually the result of aggregating (`+`) streamed_output.
    55|     Updated throughout the run when supported by the Runnable."""
    56|     logs: Dict[str, LogEntry]
    57|     """Map of run names to sub-runs. If filters were supplied, this list will
    58|     contain only the runs that matched the filters."""
    59| class RunLogPatch:
    60|     """A patch to the run log."""

# --- HUNK 2: Lines 177-217 ---
   177|         if not self.include_run(run):
   178|             return
   179|         with self.lock:
   180|             self._counter_map_by_name[run.name] += 1
   181|             count = self._counter_map_by_name[run.name]
   182|             self._key_map_by_run_id[run.id] = (
   183|                 run.name if count == 1 else f"{run.name}:{count}"
   184|             )
   185|         self.send_stream.send_nowait(
   186|             RunLogPatch(
   187|                 {
   188|                     "op": "add",
   189|                     "path": f"/logs/{self._key_map_by_run_id[run.id]}",
   190|                     "value": LogEntry(
   191|                         id=str(run.id),
   192|                         name=run.name,
   193|                         type=run.run_type,
   194|                         tags=run.tags or [],
   195|                         metadata=(run.extra or {}).get("metadata", {}),
   196|                         start_time=run.start_time.isoformat(timespec="milliseconds"),
   197|                         streamed_output=[],
   198|                         streamed_output_str=[],
   199|                         final_output=None,
   200|                         end_time=None,
   201|                     ),
   202|                 }
   203|             )
   204|         )
   205|     def _on_run_update(self, run: Run) -> None:
   206|         """Finish a run."""
   207|         try:
   208|             index = self._key_map_by_run_id.get(run.id)
   209|             if index is None:
   210|                 return
   211|             self.send_stream.send_nowait(
   212|                 RunLogPatch(
   213|                     {
   214|                         "op": "add",
   215|                         "path": f"/logs/{index}/final_output",
   216|                         "value": load(run.outputs),
   217|                     },

# --- HUNK 3: Lines 227-256 ---
   227|         finally:
   228|             if run.id == self.root_id:
   229|                 if self.auto_close:
   230|                     self.send_stream.close()
   231|     def _on_llm_new_token(
   232|         self,
   233|         run: Run,
   234|         token: str,
   235|         chunk: Optional[Union[GenerationChunk, ChatGenerationChunk]],
   236|     ) -> None:
   237|         """Process new LLM token."""
   238|         index = self._key_map_by_run_id.get(run.id)
   239|         if index is None:
   240|             return
   241|         self.send_stream.send_nowait(
   242|             RunLogPatch(
   243|                 {
   244|                     "op": "add",
   245|                     "path": f"/logs/{index}/streamed_output_str/-",
   246|                     "value": token,
   247|                 },
   248|                 {
   249|                     "op": "add",
   250|                     "path": f"/logs/{index}/streamed_output/-",
   251|                     "value": chunk.message
   252|                     if isinstance(chunk, ChatGenerationChunk)
   253|                     else token,
   254|                 },
   255|             )
   256|         )


# ====================================================================
# FILE: libs/core/langchain_core/utils/env.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 15-36 ---
    15|         "False",
    16|     )
    17| def get_from_dict_or_env(
    18|     data: Dict[str, Any], key: str, env_key: str, default: Optional[str] = None
    19| ) -> str:
    20|     """Get a value from a dictionary or an environment variable."""
    21|     if key in data and data[key]:
    22|         return data[key]
    23|     else:
    24|         return get_from_env(key, env_key, default=default)
    25| def get_from_env(key: str, env_key: str, default: Optional[str] = None) -> str:
    26|     """Get a value from a dictionary or an environment variable."""
    27|     if env_key in os.environ and os.environ[env_key]:
    28|         return os.environ[env_key]
    29|     elif default is not None:
    30|         return default
    31|     else:
    32|         raise ValueError(
    33|             f"Did not find {key}, please add an environment variable"
    34|             f" `{env_key}` which contains it, or pass"
    35|             f" `{key}` as a named parameter."
    36|         )


# ====================================================================
# FILE: libs/core/langchain_core/utils/formatting.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-21 ---
     1| """Utilities for formatting strings."""
     2| from string import Formatter
     3| from typing import Any, List, Mapping, Sequence
     4| class StrictFormatter(Formatter):
     5|     """A subclass of formatter that checks for extra keys."""
     6|     def vformat(
     7|         self, format_string: str, args: Sequence, kwargs: Mapping[str, Any]
     8|     ) -> str:
     9|         """Check that no arguments are provided."""
    10|         if len(args) > 0:
    11|             raise ValueError(
    12|                 "No arguments should be provided, "
    13|                 "everything should be passed as keyword arguments."
    14|             )
    15|         return super().vformat(format_string, args, kwargs)
    16|     def validate_input_variables(
    17|         self, format_string: str, input_variables: List[str]
    18|     ) -> None:
    19|         dummy_inputs = {input_variable: "foo" for input_variable in input_variables}
    20|         super().format(format_string, **dummy_inputs)
    21| formatter = StrictFormatter()


# ====================================================================
# FILE: libs/langchain/langchain/agents/__init__.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 25-90 ---
    25|     AgentOutputParser,
    26|     BaseMultiActionAgent,
    27|     BaseSingleActionAgent,
    28|     LLMSingleActionAgent,
    29| )
    30| from langchain.agents.agent_iterator import AgentExecutorIterator
    31| from langchain.agents.agent_toolkits import (
    32|     create_json_agent,
    33|     create_openapi_agent,
    34|     create_pbi_agent,
    35|     create_pbi_chat_agent,
    36|     create_spark_sql_agent,
    37|     create_sql_agent,
    38|     create_vectorstore_agent,
    39|     create_vectorstore_router_agent,
    40| )
    41| from langchain.agents.agent_types import AgentType
    42| from langchain.agents.conversational.base import ConversationalAgent
    43| from langchain.agents.conversational_chat.base import ConversationalChatAgent
    44| from langchain.agents.initialize import initialize_agent
    45| from langchain.agents.json_chat.base import create_json_chat_agent
    46| from langchain.agents.load_tools import (
    47|     get_all_tool_names,
    48|     load_huggingface_tool,
    49|     load_tools,
    50| )
    51| from langchain.agents.loading import load_agent
    52| from langchain.agents.mrkl.base import MRKLChain, ZeroShotAgent
    53| from langchain.agents.openai_functions_agent.base import (
    54|     OpenAIFunctionsAgent,
    55|     create_openai_functions_agent,
    56| )
    57| from langchain.agents.openai_functions_multi_agent.base import OpenAIMultiFunctionsAgent
    58| from langchain.agents.openai_tools.base import create_openai_tools_agent
    59| from langchain.agents.react.agent import create_react_agent
    60| from langchain.agents.react.base import ReActChain, ReActTextWorldAgent
    61| from langchain.agents.self_ask_with_search.base import (
    62|     SelfAskWithSearchChain,
    63|     create_self_ask_with_search_agent,
    64| )
    65| from langchain.agents.structured_chat.base import (
    66|     StructuredChatAgent,
    67|     create_structured_chat_agent,
    68| )
    69| from langchain.agents.tools import Tool, tool
    70| from langchain.agents.xml.base import XMLAgent, create_xml_agent
    71| DEPRECATED_CODE = [
    72|     "create_csv_agent",
    73|     "create_pandas_dataframe_agent",
    74|     "create_spark_dataframe_agent",
    75|     "create_xorbits_agent",
    76| ]
    77| def __getattr__(name: str) -> Any:
    78|     """Get attr name."""
    79|     if name in DEPRECATED_CODE:
    80|         HERE = Path(__file__).parents[1]
    81|         relative_path = as_import_path(
    82|             Path(__file__).parent, suffix=name, relative_to=HERE
    83|         )
    84|         old_path = "langchain." + relative_path
    85|         new_path = "langchain_experimental." + relative_path
    86|         raise ImportError(
    87|             f"{name} has been moved to langchain experimental. "
    88|             "See https://github.com/langchain-ai/langchain/discussions/11680"
    89|             "for more information.\n"
    90|             f"Please update your import statement from: `{old_path}` to `{new_path}`."

# --- HUNK 2: Lines 108-135 ---
   108|     "ReActTextWorldAgent",
   109|     "SelfAskWithSearchChain",
   110|     "StructuredChatAgent",
   111|     "Tool",
   112|     "ZeroShotAgent",
   113|     "create_json_agent",
   114|     "create_openapi_agent",
   115|     "create_pbi_agent",
   116|     "create_pbi_chat_agent",
   117|     "create_spark_sql_agent",
   118|     "create_sql_agent",
   119|     "create_vectorstore_agent",
   120|     "create_vectorstore_router_agent",
   121|     "get_all_tool_names",
   122|     "initialize_agent",
   123|     "load_agent",
   124|     "load_huggingface_tool",
   125|     "load_tools",
   126|     "tool",
   127|     "XMLAgent",
   128|     "create_openai_functions_agent",
   129|     "create_xml_agent",
   130|     "create_react_agent",
   131|     "create_openai_tools_agent",
   132|     "create_self_ask_with_search_agent",
   133|     "create_json_chat_agent",
   134|     "create_structured_chat_agent",
   135| ]


# ====================================================================
# FILE: libs/langchain/langchain/agents/agent_iterator.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 155-195 ---
   155|                     next_step_seq.append(chunk)
   156|                     if self.yield_actions:
   157|                         if isinstance(chunk, AgentAction):
   158|                             yield AddableDict(actions=[chunk], messages=chunk.messages)
   159|                         elif isinstance(chunk, AgentStep):
   160|                             yield AddableDict(steps=[chunk], messages=chunk.messages)
   161|                 next_step = self.agent_executor._consume_next_step(next_step_seq)
   162|                 self.update_iterations()
   163|                 output = self._process_next_step_output(next_step, run_manager)
   164|                 is_final = "intermediate_step" not in output
   165|                 if not self.yield_actions or is_final:
   166|                     yield output
   167|                 if is_final:
   168|                     return
   169|         except BaseException as e:
   170|             run_manager.on_chain_error(e)
   171|             raise
   172|         yield self._stop(run_manager)
   173|     async def __aiter__(self) -> AsyncIterator[AddableDict]:
   174|         """
   175|         N.B. __aiter__ must be a normal method, so need to initialize async run manager
   176|         on first __anext__ call where we can await it
   177|         """
   178|         logger.debug("Initialising AgentExecutorIterator (async)")
   179|         self.reset()
   180|         callback_manager = AsyncCallbackManager.configure(
   181|             self.callbacks,
   182|             self.agent_executor.callbacks,
   183|             self.agent_executor.verbose,
   184|             self.tags,
   185|             self.agent_executor.tags,
   186|             self.metadata,
   187|             self.agent_executor.metadata,
   188|         )
   189|         run_manager = await callback_manager.on_chain_start(
   190|             dumpd(self.agent_executor),
   191|             self.inputs,
   192|             name=self.run_name,
   193|         )
   194|         try:
   195|             async with asyncio_timeout(self.agent_executor.max_execution_time):


# ====================================================================
# FILE: libs/langchain/langchain/agents/json_chat/base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-65 ---
     1| from typing import Sequence
     2| from langchain_core.language_models import BaseLanguageModel
     3| from langchain_core.prompts.chat import ChatPromptTemplate
     4| from langchain_core.runnables import Runnable, RunnablePassthrough
     5| from langchain_core.tools import BaseTool
     6| from langchain.agents.format_scratchpad import format_log_to_messages
     7| from langchain.agents.json_chat.prompt import TEMPLATE_TOOL_RESPONSE
     8| from langchain.agents.output_parsers import JSONAgentOutputParser
     9| from langchain.tools.render import render_text_description
    10| def create_json_chat_agent(
    11|     llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: ChatPromptTemplate
    12| ) -> Runnable:
    13|     """Create an agent that uses JSON to format its logic, build for Chat Models.
    14|     Examples:
    15|         .. code-block:: python
    16|             from langchain import hub
    17|             from langchain.chat_models import ChatOpenAI
    18|             from langchain.agents import AgentExecutor, create_json_chat_agent
    19|             prompt = hub.pull("hwchase17/react-chat-json")
    20|             model = ChatOpenAI()
    21|             tools = ...
    22|             agent = create_json_chat_agent(model, tools, prompt)
    23|             agent_executor = AgentExecutor(agent=agent, tools=tools)
    24|             agent_executor.invoke({"input": "hi"})
    25|             from langchain_core.messages import AIMessage, HumanMessage
    26|             agent_executor.invoke(
    27|                 {
    28|                     "input": "what's my name?",
    29|                     "chat_history": [
    30|                         HumanMessage(content="hi! my name is bob"),
    31|                         AIMessage(content="Hello Bob! How can I assist you today?"),
    32|                     ],
    33|                 }
    34|             )
    35|     Args:
    36|         llm: LLM to use as the agent.
    37|         tools: Tools this agent has access to.
    38|         prompt: The prompt to use, must have input keys of
    39|             `tools`, `tool_names`, and `agent_scratchpad`.
    40|     Returns:
    41|         A runnable sequence representing an agent. It takes as input all the same input
    42|         variables as the prompt passed in does. It returns as output either an
    43|         AgentAction or AgentFinish.
    44|     """
    45|     missing_vars = {"tools", "tool_names", "agent_scratchpad"}.difference(
    46|         prompt.input_variables
    47|     )
    48|     if missing_vars:
    49|         raise ValueError(f"Prompt missing required variables: {missing_vars}")
    50|     prompt = prompt.partial(
    51|         tools=render_text_description(list(tools)),
    52|         tool_names=", ".join([t.name for t in tools]),
    53|     )
    54|     llm_with_stop = llm.bind(stop=["\nObservation"])
    55|     agent = (
    56|         RunnablePassthrough.assign(
    57|             agent_scratchpad=lambda x: format_log_to_messages(
    58|                 x["intermediate_steps"], template_tool_response=TEMPLATE_TOOL_RESPONSE
    59|             )
    60|         )
    61|         | prompt
    62|         | llm_with_stop
    63|         | JSONAgentOutputParser()
    64|     )
    65|     return agent


# ====================================================================
# FILE: libs/langchain/langchain/agents/json_chat/prompt.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-6 ---
     1| TEMPLATE_TOOL_RESPONSE = """TOOL RESPONSE: 
     2| ---------------------
     3| {observation}
     4| USER'S INPUT
     5| --------------------
     6| Okay, so what is the response to my last comment? If using information obtained from the tools you must mention it explicitly without mentioning the tool names - I have forgotten all TOOL RESPONSES! Remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else - even if you just want to respond to the user. Do NOT respond with anything except a JSON snippet no matter what!"""


# ====================================================================
# FILE: libs/langchain/langchain/agents/openai_functions_agent/base.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-37 ---
     1| """Module implements an agent that uses OpenAI's APIs function enabled API."""
     2| from typing import Any, List, Optional, Sequence, Tuple, Union
     3| from langchain_core.agents import AgentAction, AgentFinish
     4| from langchain_core.language_models import BaseLanguageModel
     5| from langchain_core.messages import (
     6|     BaseMessage,
     7|     SystemMessage,
     8| )
     9| from langchain_core.prompts import BasePromptTemplate
    10| from langchain_core.prompts.chat import (
    11|     BaseMessagePromptTemplate,
    12|     ChatPromptTemplate,
    13|     HumanMessagePromptTemplate,
    14|     MessagesPlaceholder,
    15| )
    16| from langchain_core.pydantic_v1 import root_validator
    17| from langchain_core.runnables import Runnable, RunnablePassthrough
    18| from langchain_core.tools import BaseTool
    19| from langchain.agents import BaseSingleActionAgent
    20| from langchain.agents.format_scratchpad.openai_functions import (
    21|     format_to_openai_function_messages,
    22| )
    23| from langchain.agents.output_parsers.openai_functions import (
    24|     OpenAIFunctionsAgentOutputParser,
    25| )
    26| from langchain.callbacks.base import BaseCallbackManager
    27| from langchain.callbacks.manager import Callbacks
    28| from langchain.tools.render import format_tool_to_openai_function
    29| class OpenAIFunctionsAgent(BaseSingleActionAgent):
    30|     """An Agent driven by OpenAIs function powered API.
    31|     Args:
    32|         llm: This should be an instance of ChatOpenAI, specifically a model
    33|             that supports using `functions`.
    34|         tools: The tools this agent has access to.
    35|         prompt: The prompt for this agent, should support agent_scratchpad as one
    36|             of the variables. For an easy way to construct this prompt, use
    37|             `OpenAIFunctionsAgent.create_prompt(...)`

# --- HUNK 2: Lines 186-261 ---
   186|         tools: Sequence[BaseTool],
   187|         callback_manager: Optional[BaseCallbackManager] = None,
   188|         extra_prompt_messages: Optional[List[BaseMessagePromptTemplate]] = None,
   189|         system_message: Optional[SystemMessage] = SystemMessage(
   190|             content="You are a helpful AI assistant."
   191|         ),
   192|         **kwargs: Any,
   193|     ) -> BaseSingleActionAgent:
   194|         """Construct an agent from an LLM and tools."""
   195|         prompt = cls.create_prompt(
   196|             extra_prompt_messages=extra_prompt_messages,
   197|             system_message=system_message,
   198|         )
   199|         return cls(
   200|             llm=llm,
   201|             prompt=prompt,
   202|             tools=tools,
   203|             callback_manager=callback_manager,
   204|             **kwargs,
   205|         )
   206| def create_openai_functions_agent(
   207|     llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: ChatPromptTemplate
   208| ) -> Runnable:
   209|     """Create an agent that uses OpenAI function calling.
   210|     Examples:
   211|         Creating an agent with no memory
   212|         .. code-block:: python
   213|             from langchain.chat_models import ChatOpenAI
   214|             from langchain.agents import AgentExecutor, create_openai_functions_agent
   215|             from langchain import hub
   216|             prompt = hub.pull("hwchase17/openai-functions-agent")
   217|             model = ChatOpenAI()
   218|             tools = ...
   219|             agent = create_openai_functions_agent(model, tools, prompt)
   220|             agent_executor = AgentExecutor(agent=agent, tools=tools)
   221|             agent_executor.invoke({"input": "hi"})
   222|             from langchain_core.messages import AIMessage, HumanMessage
   223|             agent_executor.invoke(
   224|                 {
   225|                     "input": "what's my name?",
   226|                     "chat_history": [
   227|                         HumanMessage(content="hi! my name is bob"),
   228|                         AIMessage(content="Hello Bob! How can I assist you today?"),
   229|                     ],
   230|                 }
   231|             )
   232|     Args:
   233|         llm: LLM to use as the agent. Should work with OpenAI function calling,
   234|             so either be an OpenAI model that supports that or a wrapper of
   235|             a different model that adds in equivalent support.
   236|         tools: Tools this agent has access to.
   237|         prompt: The prompt to use, must have an input key of `agent_scratchpad`.
   238|     Returns:
   239|         A runnable sequence representing an agent. It takes as input all the same input
   240|         variables as the prompt passed in does. It returns as output either an
   241|         AgentAction or AgentFinish.
   242|     """
   243|     if "agent_scratchpad" not in prompt.input_variables:
   244|         raise ValueError(
   245|             "Prompt must have input variable `agent_scratchpad`, but wasn't found. "
   246|             f"Found {prompt.input_variables} instead."
   247|         )
   248|     llm_with_tools = llm.bind(
   249|         functions=[format_tool_to_openai_function(t) for t in tools]
   250|     )
   251|     agent = (
   252|         RunnablePassthrough.assign(
   253|             agent_scratchpad=lambda x: format_to_openai_function_messages(
   254|                 x["intermediate_steps"]
   255|             )
   256|         )
   257|         | prompt
   258|         | llm_with_tools
   259|         | OpenAIFunctionsAgentOutputParser()
   260|     )
   261|     return agent


# ====================================================================
# FILE: libs/langchain/langchain/agents/openai_functions_multi_agent/base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 16-56 ---
    16|     ChatPromptTemplate,
    17|     HumanMessagePromptTemplate,
    18|     MessagesPlaceholder,
    19| )
    20| from langchain_core.pydantic_v1 import root_validator
    21| from langchain.agents import BaseMultiActionAgent
    22| from langchain.agents.format_scratchpad.openai_functions import (
    23|     format_to_openai_function_messages,
    24| )
    25| from langchain.callbacks.base import BaseCallbackManager
    26| from langchain.callbacks.manager import Callbacks
    27| from langchain.tools import BaseTool
    28| _FunctionsAgentAction = AgentActionMessageLog
    29| def _parse_ai_message(message: BaseMessage) -> Union[List[AgentAction], AgentFinish]:
    30|     """Parse an AI message."""
    31|     if not isinstance(message, AIMessage):
    32|         raise TypeError(f"Expected an AI message got {type(message)}")
    33|     function_call = message.additional_kwargs.get("function_call", {})
    34|     if function_call:
    35|         try:
    36|             arguments = json.loads(function_call["arguments"], strict=False)
    37|         except JSONDecodeError:
    38|             raise OutputParserException(
    39|                 f"Could not parse tool input: {function_call} because "
    40|                 f"the `arguments` is not valid JSON."
    41|             )
    42|         try:
    43|             tools = arguments["actions"]
    44|         except (TypeError, KeyError):
    45|             raise OutputParserException(
    46|                 f"Could not parse tool input: {function_call} because "
    47|                 f"the `arguments` JSON does not contain `actions` key."
    48|             )
    49|         final_tools: List[AgentAction] = []
    50|         for tool_schema in tools:
    51|             _tool_input = tool_schema["action"]
    52|             function_name = tool_schema["action_name"]
    53|             if "__arg1" in _tool_input:
    54|                 tool_input = _tool_input["__arg1"]
    55|             else:
    56|                 tool_input = _tool_input


# ====================================================================
# FILE: libs/langchain/langchain/agents/openai_tools/base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-61 ---
     1| from typing import Sequence
     2| from langchain_core.language_models import BaseLanguageModel
     3| from langchain_core.prompts.chat import ChatPromptTemplate
     4| from langchain_core.runnables import Runnable, RunnablePassthrough
     5| from langchain_core.tools import BaseTool
     6| from langchain.agents.format_scratchpad.openai_tools import (
     7|     format_to_openai_tool_messages,
     8| )
     9| from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser
    10| from langchain.tools.render import format_tool_to_openai_tool
    11| def create_openai_tools_agent(
    12|     llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: ChatPromptTemplate
    13| ) -> Runnable:
    14|     """Create an agent that uses OpenAI tools.
    15|     Examples:
    16|         .. code-block:: python
    17|             from langchain import hub
    18|             from langchain.chat_models import ChatOpenAI
    19|             from langchain.agents import AgentExecutor, create_openai_tools_agent
    20|             prompt = hub.pull("hwchase17/openai-tools-agent")
    21|             model = ChatOpenAI()
    22|             tools = ...
    23|             agent = create_openai_tools_agent(model, tools, prompt)
    24|             agent_executor = AgentExecutor(agent=agent, tools=tools)
    25|             agent_executor.invoke({"input": "hi"})
    26|             from langchain_core.messages import AIMessage, HumanMessage
    27|             agent_executor.invoke(
    28|                 {
    29|                     "input": "what's my name?",
    30|                     "chat_history": [
    31|                         HumanMessage(content="hi! my name is bob"),
    32|                         AIMessage(content="Hello Bob! How can I assist you today?"),
    33|                     ],
    34|                 }
    35|             )
    36|     Args:
    37|         llm: LLM to use as the agent.
    38|         tools: Tools this agent has access to.
    39|         prompt: The prompt to use, must have input keys of `agent_scratchpad`.
    40|     Returns:
    41|         A runnable sequence representing an agent. It takes as input all the same input
    42|         variables as the prompt passed in does. It returns as output either an
    43|         AgentAction or AgentFinish.
    44|     """
    45|     missing_vars = {"agent_scratchpad"}.difference(prompt.input_variables)
    46|     if missing_vars:
    47|         raise ValueError(f"Prompt missing required variables: {missing_vars}")
    48|     llm_with_tools = llm.bind(
    49|         tools=[format_tool_to_openai_tool(tool) for tool in tools]
    50|     )
    51|     agent = (
    52|         RunnablePassthrough.assign(
    53|             agent_scratchpad=lambda x: format_to_openai_tool_messages(
    54|                 x["intermediate_steps"]
    55|             )
    56|         )
    57|         | prompt
    58|         | llm_with_tools
    59|         | OpenAIToolsAgentOutputParser()
    60|     )
    61|     return agent


# ====================================================================
# FILE: libs/langchain/langchain/agents/output_parsers/openai_functions.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 16-56 ---
    16|     function_call parameter from OpenAI to convey what tools to use.
    17|     If a function_call parameter is passed, then that is used to get
    18|     the tool and tool input.
    19|     If one is not passed, then the AIMessage is assumed to be the final output.
    20|     """
    21|     @property
    22|     def _type(self) -> str:
    23|         return "openai-functions-agent"
    24|     @staticmethod
    25|     def _parse_ai_message(message: BaseMessage) -> Union[AgentAction, AgentFinish]:
    26|         """Parse an AI message."""
    27|         if not isinstance(message, AIMessage):
    28|             raise TypeError(f"Expected an AI message got {type(message)}")
    29|         function_call = message.additional_kwargs.get("function_call", {})
    30|         if function_call:
    31|             function_name = function_call["name"]
    32|             try:
    33|                 if len(function_call["arguments"].strip()) == 0:
    34|                     _tool_input = {}
    35|                 else:
    36|                     _tool_input = json.loads(function_call["arguments"], strict=False)
    37|             except JSONDecodeError:
    38|                 raise OutputParserException(
    39|                     f"Could not parse tool input: {function_call} because "
    40|                     f"the `arguments` is not valid JSON."
    41|                 )
    42|             if "__arg1" in _tool_input:
    43|                 tool_input = _tool_input["__arg1"]
    44|             else:
    45|                 tool_input = _tool_input
    46|             content_msg = f"responded: {message.content}\n" if message.content else "\n"
    47|             log = f"\nInvoking: `{function_name}` with `{tool_input}`\n{content_msg}\n"
    48|             return AgentActionMessageLog(
    49|                 tool=function_name,
    50|                 tool_input=tool_input,
    51|                 log=log,
    52|                 message_log=[message],
    53|             )
    54|         return AgentFinish(
    55|             return_values={"output": message.content}, log=str(message.content)
    56|         )


# ====================================================================
# FILE: libs/langchain/langchain/agents/react/agent.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-60 ---
     1| from __future__ import annotations
     2| from typing import Sequence
     3| from langchain_core.language_models import BaseLanguageModel
     4| from langchain_core.prompts import BasePromptTemplate
     5| from langchain_core.runnables import Runnable, RunnablePassthrough
     6| from langchain_core.tools import BaseTool
     7| from langchain.agents.format_scratchpad import format_log_to_str
     8| from langchain.agents.output_parsers import ReActSingleInputOutputParser
     9| from langchain.tools.render import render_text_description
    10| def create_react_agent(
    11|     llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: BasePromptTemplate
    12| ) -> Runnable:
    13|     """Create an agent that uses ReAct prompting.
    14|     Examples:
    15|         .. code-block:: python
    16|             from langchain import hub
    17|             from langchain.llms import OpenAI
    18|             from langchain.agents import AgentExecutor, create_react_agent
    19|             prompt = hub.pull("hwchase17/react")
    20|             model = OpenAI()
    21|             tools = ...
    22|             agent = create_react_agent(model, tools, prompt)
    23|             agent_executor = AgentExecutor(agent=agent, tools=tools)
    24|             agent_executor.invoke({"input": "hi"})
    25|             from langchain_core.messages import AIMessage, HumanMessage
    26|             agent_executor.invoke(
    27|                 {
    28|                     "input": "what's my name?",
    29|                     "chat_history": "Human: My name is Bob\nAI: Hello Bob!",
    30|                 }
    31|             )
    32|     Args:
    33|         llm: LLM to use as the agent.
    34|         tools: Tools this agent has access to.
    35|         prompt: The prompt to use, must have input keys of
    36|             `tools`, `tool_names`, and `agent_scratchpad`.
    37|     Returns:
    38|         A runnable sequence representing an agent. It takes as input all the same input
    39|         variables as the prompt passed in does. It returns as output either an
    40|         AgentAction or AgentFinish.
    41|     """
    42|     missing_vars = {"tools", "tool_names", "agent_scratchpad"}.difference(
    43|         prompt.input_variables
    44|     )
    45|     if missing_vars:
    46|         raise ValueError(f"Prompt missing required variables: {missing_vars}")
    47|     prompt = prompt.partial(
    48|         tools=render_text_description(list(tools)),
    49|         tool_names=", ".join([t.name for t in tools]),
    50|     )
    51|     llm_with_stop = llm.bind(stop=["\nObservation"])
    52|     agent = (
    53|         RunnablePassthrough.assign(
    54|             agent_scratchpad=lambda x: format_log_to_str(x["intermediate_steps"]),
    55|         )
    56|         | prompt
    57|         | llm_with_stop
    58|         | ReActSingleInputOutputParser()
    59|     )
    60|     return agent


# ====================================================================
# FILE: libs/langchain/langchain/agents/self_ask_with_search/base.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-30 ---
     1| """Chain that does self-ask with search."""
     2| from typing import Any, Sequence, Union
     3| from langchain_core.language_models import BaseLanguageModel
     4| from langchain_core.prompts import BasePromptTemplate
     5| from langchain_core.pydantic_v1 import Field
     6| from langchain_core.runnables import Runnable, RunnablePassthrough
     7| from langchain_core.tools import BaseTool
     8| from langchain.agents.agent import Agent, AgentExecutor, AgentOutputParser
     9| from langchain.agents.agent_types import AgentType
    10| from langchain.agents.format_scratchpad import format_log_to_str
    11| from langchain.agents.self_ask_with_search.output_parser import SelfAskOutputParser
    12| from langchain.agents.self_ask_with_search.prompt import PROMPT
    13| from langchain.agents.tools import Tool
    14| from langchain.agents.utils import validate_tools_single_input
    15| from langchain.utilities.google_serper import GoogleSerperAPIWrapper
    16| from langchain.utilities.searchapi import SearchApiAPIWrapper
    17| from langchain.utilities.serpapi import SerpAPIWrapper
    18| class SelfAskWithSearchAgent(Agent):
    19|     """Agent for the self-ask-with-search paper."""
    20|     output_parser: AgentOutputParser = Field(default_factory=SelfAskOutputParser)
    21|     @classmethod
    22|     def _get_default_output_parser(cls, **kwargs: Any) -> AgentOutputParser:
    23|         return SelfAskOutputParser()
    24|     @property
    25|     def _agent_type(self) -> str:
    26|         """Return Identifier of an agent type."""
    27|         return AgentType.SELF_ASK_WITH_SEARCH
    28|     @classmethod
    29|     def create_prompt(cls, tools: Sequence[BaseTool]) -> BasePromptTemplate:
    30|         """Prompt does not depend on tools."""

# --- HUNK 2: Lines 50-121 ---
    50|         return ""
    51| class SelfAskWithSearchChain(AgentExecutor):
    52|     """[Deprecated] Chain that does self-ask with search."""
    53|     def __init__(
    54|         self,
    55|         llm: BaseLanguageModel,
    56|         search_chain: Union[
    57|             GoogleSerperAPIWrapper, SearchApiAPIWrapper, SerpAPIWrapper
    58|         ],
    59|         **kwargs: Any,
    60|     ):
    61|         """Initialize only with an LLM and a search chain."""
    62|         search_tool = Tool(
    63|             name="Intermediate Answer",
    64|             func=search_chain.run,
    65|             coroutine=search_chain.arun,
    66|             description="Search",
    67|         )
    68|         agent = SelfAskWithSearchAgent.from_llm_and_tools(llm, [search_tool])
    69|         super().__init__(agent=agent, tools=[search_tool], **kwargs)
    70| def create_self_ask_with_search_agent(
    71|     llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: BasePromptTemplate
    72| ) -> Runnable:
    73|     """Create an agent that uses self-ask with search prompting.
    74|     Examples:
    75|         .. code-block:: python
    76|             from langchain import hub
    77|             from langchain.chat_models import ChatAnthropic
    78|             from langchain.agents import (
    79|                 AgentExecutor, create_self_ask_with_search_agent
    80|             )
    81|             prompt = hub.pull("hwchase17/self-ask-with-search")
    82|             model = ChatAnthropic()
    83|             tools = [...]  # Should just be one tool with name `Intermediate Answer`
    84|             agent = create_self_ask_with_search_agent(model, tools, prompt)
    85|             agent_executor = AgentExecutor(agent=agent, tools=tools)
    86|             agent_executor.invoke({"input": "hi"})
    87|     Args:
    88|         llm: LLM to use as the agent.
    89|         tools: List of tools. Should just be of length 1, with that tool having
    90|             name `Intermediate Answer`
    91|         prompt: The prompt to use, must have input keys of `agent_scratchpad`.
    92|     Returns:
    93|         A runnable sequence representing an agent. It takes as input all the same input
    94|         variables as the prompt passed in does. It returns as output either an
    95|         AgentAction or AgentFinish.
    96|     """
    97|     missing_vars = {"agent_scratchpad"}.difference(prompt.input_variables)
    98|     if missing_vars:
    99|         raise ValueError(f"Prompt missing required variables: {missing_vars}")
   100|     if len(tools) != 1:
   101|         raise ValueError("This agent expects exactly one tool")
   102|     tool = list(tools)[0]
   103|     if tool.name != "Intermediate Answer":
   104|         raise ValueError(
   105|             "This agent expects the tool to be named `Intermediate Answer`"
   106|         )
   107|     llm_with_stop = llm.bind(stop=["\nIntermediate answer:"])
   108|     agent = (
   109|         RunnablePassthrough.assign(
   110|             agent_scratchpad=lambda x: format_log_to_str(
   111|                 x["intermediate_steps"],
   112|                 observation_prefix="\nIntermediate answer: ",
   113|                 llm_prefix="",
   114|             ),
   115|             chat_history=lambda x: x.get("chat_history", ""),
   116|         )
   117|         | prompt
   118|         | llm_with_stop
   119|         | SelfAskOutputParser()
   120|     )
   121|     return agent


# ====================================================================
# FILE: libs/langchain/langchain/agents/structured_chat/base.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-43 ---
     1| import re
     2| from typing import Any, List, Optional, Sequence, Tuple
     3| from langchain_core.agents import AgentAction
     4| from langchain_core.language_models import BaseLanguageModel
     5| from langchain_core.prompts import BasePromptTemplate
     6| from langchain_core.prompts.chat import (
     7|     ChatPromptTemplate,
     8|     HumanMessagePromptTemplate,
     9|     SystemMessagePromptTemplate,
    10| )
    11| from langchain_core.pydantic_v1 import Field
    12| from langchain_core.runnables import Runnable, RunnablePassthrough
    13| from langchain.agents.agent import Agent, AgentOutputParser
    14| from langchain.agents.format_scratchpad import format_log_to_str
    15| from langchain.agents.output_parsers import JSONAgentOutputParser
    16| from langchain.agents.structured_chat.output_parser import (
    17|     StructuredChatOutputParserWithRetries,
    18| )
    19| from langchain.agents.structured_chat.prompt import FORMAT_INSTRUCTIONS, PREFIX, SUFFIX
    20| from langchain.callbacks.base import BaseCallbackManager
    21| from langchain.chains.llm import LLMChain
    22| from langchain.tools import BaseTool
    23| from langchain.tools.render import render_text_description_and_args
    24| HUMAN_MESSAGE_TEMPLATE = "{input}\n\n{agent_scratchpad}"
    25| class StructuredChatAgent(Agent):
    26|     """Structured Chat Agent."""
    27|     output_parser: AgentOutputParser = Field(
    28|         default_factory=StructuredChatOutputParserWithRetries
    29|     )
    30|     """Output parser for the agent."""
    31|     @property
    32|     def observation_prefix(self) -> str:
    33|         """Prefix to append the observation with."""
    34|         return "Observation: "
    35|     @property
    36|     def llm_prefix(self) -> str:
    37|         """Prefix to append the llm call with."""
    38|         return "Thought:"
    39|     def _construct_scratchpad(
    40|         self, intermediate_steps: List[Tuple[AgentAction, str]]
    41|     ) -> str:
    42|         agent_scratchpad = super()._construct_scratchpad(intermediate_steps)
    43|         if not isinstance(agent_scratchpad, str):

# --- HUNK 2: Lines 114-187 ---
   114|             format_instructions=format_instructions,
   115|             input_variables=input_variables,
   116|             memory_prompts=memory_prompts,
   117|         )
   118|         llm_chain = LLMChain(
   119|             llm=llm,
   120|             prompt=prompt,
   121|             callback_manager=callback_manager,
   122|         )
   123|         tool_names = [tool.name for tool in tools]
   124|         _output_parser = output_parser or cls._get_default_output_parser(llm=llm)
   125|         return cls(
   126|             llm_chain=llm_chain,
   127|             allowed_tools=tool_names,
   128|             output_parser=_output_parser,
   129|             **kwargs,
   130|         )
   131|     @property
   132|     def _agent_type(self) -> str:
   133|         raise ValueError
   134| def create_structured_chat_agent(
   135|     llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: ChatPromptTemplate
   136| ) -> Runnable:
   137|     """Create an agent aimed at supporting tools with multiple inputs.
   138|     Examples:
   139|         .. code-block:: python
   140|             from langchain import hub
   141|             from langchain.chat_models import ChatOpenAI
   142|             from langchain.agents import AgentExecutor, create_structured_chat_agent
   143|             prompt = hub.pull("hwchase17/structured-chat-agent")
   144|             model = ChatOpenAI()
   145|             tools = ...
   146|             agent = create_structured_chat_agent(model, tools, prompt)
   147|             agent_executor = AgentExecutor(agent=agent, tools=tools)
   148|             agent_executor.invoke({"input": "hi"})
   149|             from langchain_core.messages import AIMessage, HumanMessage
   150|             agent_executor.invoke(
   151|                 {
   152|                     "input": "what's my name?",
   153|                     "chat_history": [
   154|                         HumanMessage(content="hi! my name is bob"),
   155|                         AIMessage(content="Hello Bob! How can I assist you today?"),
   156|                     ],
   157|                 }
   158|             )
   159|     Args:
   160|         llm: LLM to use as the agent.
   161|         tools: Tools this agent has access to.
   162|         prompt: The prompt to use, must have input keys of
   163|             `tools`, `tool_names`, and `agent_scratchpad`.
   164|     Returns:
   165|         A runnable sequence representing an agent. It takes as input all the same input
   166|         variables as the prompt passed in does. It returns as output either an
   167|         AgentAction or AgentFinish.
   168|     """
   169|     missing_vars = {"tools", "tool_names", "agent_scratchpad"}.difference(
   170|         prompt.input_variables
   171|     )
   172|     if missing_vars:
   173|         raise ValueError(f"Prompt missing required variables: {missing_vars}")
   174|     prompt = prompt.partial(
   175|         tools=render_text_description_and_args(list(tools)),
   176|         tool_names=", ".join([t.name for t in tools]),
   177|     )
   178|     llm_with_stop = llm.bind(stop=["Observation"])
   179|     agent = (
   180|         RunnablePassthrough.assign(
   181|             agent_scratchpad=lambda x: format_log_to_str(x["intermediate_steps"]),
   182|         )
   183|         | prompt
   184|         | llm_with_stop
   185|         | JSONAgentOutputParser()
   186|     )
   187|     return agent


# ====================================================================
# FILE: libs/langchain/langchain/agents/xml/base.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-59 ---
     1| from typing import Any, List, Sequence, Tuple, Union
     2| from langchain_core.agents import AgentAction, AgentFinish
     3| from langchain_core.language_models import BaseLanguageModel
     4| from langchain_core.prompts.base import BasePromptTemplate
     5| from langchain_core.prompts.chat import AIMessagePromptTemplate, ChatPromptTemplate
     6| from langchain_core.runnables import Runnable, RunnablePassthrough
     7| from langchain_core.tools import BaseTool
     8| from langchain.agents.agent import BaseSingleActionAgent
     9| from langchain.agents.format_scratchpad import format_xml
    10| from langchain.agents.output_parsers import XMLAgentOutputParser
    11| from langchain.agents.xml.prompt import agent_instructions
    12| from langchain.callbacks.base import Callbacks
    13| from langchain.chains.llm import LLMChain
    14| from langchain.tools.render import render_text_description
    15| class XMLAgent(BaseSingleActionAgent):
    16|     """Agent that uses XML tags.
    17|     Args:
    18|         tools: list of tools the agent can choose from
    19|         llm_chain: The LLMChain to call to predict the next action
    20|     Examples:
    21|         .. code-block:: python
    22|             from langchain.agents import XMLAgent
    23|             from langchain
    24|             tools = ...
    25|             model =
    26|     """
    27|     tools: List[BaseTool]
    28|     """List of tools this agent has access to."""
    29|     llm_chain: LLMChain
    30|     """Chain to use to predict action."""
    31|     @property
    32|     def input_keys(self) -> List[str]:
    33|         return ["input"]
    34|     @staticmethod
    35|     def get_default_prompt() -> ChatPromptTemplate:
    36|         base_prompt = ChatPromptTemplate.from_template(agent_instructions)
    37|         return base_prompt + AIMessagePromptTemplate.from_template(
    38|             "{intermediate_steps}"
    39|         )
    40|     @staticmethod
    41|     def get_default_output_parser() -> XMLAgentOutputParser:
    42|         return XMLAgentOutputParser()
    43|     def plan(
    44|         self,
    45|         intermediate_steps: List[Tuple[AgentAction, str]],
    46|         callbacks: Callbacks = None,
    47|         **kwargs: Any,
    48|     ) -> Union[AgentAction, AgentFinish]:
    49|         log = ""
    50|         for action, observation in intermediate_steps:
    51|             log += (
    52|                 f"<tool>{action.tool}</tool><tool_input>{action.tool_input}"
    53|                 f"</tool_input><observation>{observation}</observation>"
    54|             )
    55|         tools = ""
    56|         for tool in self.tools:
    57|             tools += f"{tool.name}: {tool.description}\n"
    58|         inputs = {
    59|             "intermediate_steps": log,

# --- HUNK 2: Lines 69-136 ---
    69|         callbacks: Callbacks = None,
    70|         **kwargs: Any,
    71|     ) -> Union[AgentAction, AgentFinish]:
    72|         log = ""
    73|         for action, observation in intermediate_steps:
    74|             log += (
    75|                 f"<tool>{action.tool}</tool><tool_input>{action.tool_input}"
    76|                 f"</tool_input><observation>{observation}</observation>"
    77|             )
    78|         tools = ""
    79|         for tool in self.tools:
    80|             tools += f"{tool.name}: {tool.description}\n"
    81|         inputs = {
    82|             "intermediate_steps": log,
    83|             "tools": tools,
    84|             "question": kwargs["input"],
    85|             "stop": ["</tool_input>", "</final_answer>"],
    86|         }
    87|         response = await self.llm_chain.acall(inputs, callbacks=callbacks)
    88|         return response[self.llm_chain.output_key]
    89| def create_xml_agent(
    90|     llm: BaseLanguageModel, tools: Sequence[BaseTool], prompt: BasePromptTemplate
    91| ) -> Runnable:
    92|     """Create an agent that uses XML to format its logic.
    93|     Examples:
    94|         .. code-block:: python
    95|             from langchain import hub
    96|             from langchain.chat_models import ChatAnthropic
    97|             from langchain.agents import AgentExecutor, create_xml_agent
    98|             prompt = hub.pull("hwchase17/xml-agent-convo")
    99|             model = ChatAnthropic()
   100|             tools = ...
   101|             agent = create_xml_agent(model, tools, prompt)
   102|             agent_executor = AgentExecutor(agent=agent, tools=tools)
   103|             agent_executor.invoke({"input": "hi"})
   104|             from langchain_core.messages import AIMessage, HumanMessage
   105|             agent_executor.invoke(
   106|                 {
   107|                     "input": "what's my name?",
   108|                     "chat_history": "Human: My name is Bob\nAI: Hello Bob!",
   109|                 }
   110|             )
   111|     Args:
   112|         llm: LLM to use as the agent.
   113|         tools: Tools this agent has access to.
   114|         prompt: The prompt to use, must have input keys of
   115|             `tools` and `agent_scratchpad`.
   116|     Returns:
   117|         A runnable sequence representing an agent. It takes as input all the same input
   118|         variables as the prompt passed in does. It returns as output either an
   119|         AgentAction or AgentFinish.
   120|     """
   121|     missing_vars = {"tools", "agent_scratchpad"}.difference(prompt.input_variables)
   122|     if missing_vars:
   123|         raise ValueError(f"Prompt missing required variables: {missing_vars}")
   124|     prompt = prompt.partial(
   125|         tools=render_text_description(list(tools)),
   126|     )
   127|     llm_with_stop = llm.bind(stop=["</tool_input>"])
   128|     agent = (
   129|         RunnablePassthrough.assign(
   130|             agent_scratchpad=lambda x: format_xml(x["intermediate_steps"]),
   131|         )
   132|         | prompt
   133|         | llm_with_stop
   134|         | XMLAgentOutputParser()
   135|     )
   136|     return agent


# ====================================================================
# FILE: libs/langchain/langchain/chains/__init__.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 18-86 ---
    18| from langchain.chains.combine_documents.reduce import ReduceDocumentsChain
    19| from langchain.chains.combine_documents.refine import RefineDocumentsChain
    20| from langchain.chains.combine_documents.stuff import StuffDocumentsChain
    21| from langchain.chains.constitutional_ai.base import ConstitutionalChain
    22| from langchain.chains.conversation.base import ConversationChain
    23| from langchain.chains.conversational_retrieval.base import (
    24|     ChatVectorDBChain,
    25|     ConversationalRetrievalChain,
    26| )
    27| from langchain.chains.example_generator import generate_example
    28| from langchain.chains.flare.base import FlareChain
    29| from langchain.chains.graph_qa.arangodb import ArangoGraphQAChain
    30| from langchain.chains.graph_qa.base import GraphQAChain
    31| from langchain.chains.graph_qa.cypher import GraphCypherQAChain
    32| from langchain.chains.graph_qa.falkordb import FalkorDBQAChain
    33| from langchain.chains.graph_qa.hugegraph import HugeGraphQAChain
    34| from langchain.chains.graph_qa.kuzu import KuzuQAChain
    35| from langchain.chains.graph_qa.nebulagraph import NebulaGraphQAChain
    36| from langchain.chains.graph_qa.neptune_cypher import NeptuneOpenCypherQAChain
    37| from langchain.chains.graph_qa.sparql import GraphSparqlQAChain
    38| from langchain.chains.history_aware_retriever import create_history_aware_retriever
    39| from langchain.chains.hyde.base import HypotheticalDocumentEmbedder
    40| from langchain.chains.llm import LLMChain
    41| from langchain.chains.llm_checker.base import LLMCheckerChain
    42| from langchain.chains.llm_math.base import LLMMathChain
    43| from langchain.chains.llm_requests import LLMRequestsChain
    44| from langchain.chains.llm_summarization_checker.base import LLMSummarizationCheckerChain
    45| from langchain.chains.loading import load_chain
    46| from langchain.chains.mapreduce import MapReduceChain
    47| from langchain.chains.moderation import OpenAIModerationChain
    48| from langchain.chains.natbot.base import NatBotChain
    49| from langchain.chains.openai_functions import (
    50|     create_citation_fuzzy_match_chain,
    51|     create_extraction_chain,
    52|     create_extraction_chain_pydantic,
    53|     create_qa_with_sources_chain,
    54|     create_qa_with_structure_chain,
    55|     create_tagging_chain,
    56|     create_tagging_chain_pydantic,
    57| )
    58| from langchain.chains.qa_generation.base import QAGenerationChain
    59| from langchain.chains.qa_with_sources.base import QAWithSourcesChain
    60| from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain
    61| from langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain
    62| from langchain.chains.retrieval import create_retrieval_chain
    63| from langchain.chains.retrieval_qa.base import (
    64|     RetrievalQA,
    65|     VectorDBQA,
    66| )
    67| from langchain.chains.router import (
    68|     LLMRouterChain,
    69|     MultiPromptChain,
    70|     MultiRetrievalQAChain,
    71|     MultiRouteChain,
    72|     RouterChain,
    73| )
    74| from langchain.chains.sequential import SequentialChain, SimpleSequentialChain
    75| from langchain.chains.sql_database.query import create_sql_query_chain
    76| from langchain.chains.transform import TransformChain
    77| __all__ = [
    78|     "APIChain",
    79|     "AnalyzeDocumentChain",
    80|     "ArangoGraphQAChain",
    81|     "ChatVectorDBChain",
    82|     "ConstitutionalChain",
    83|     "ConversationChain",
    84|     "ConversationalRetrievalChain",
    85|     "FalkorDBQAChain",
    86|     "FlareChain",

# --- HUNK 2: Lines 113-135 ---
   113|     "RefineDocumentsChain",
   114|     "RetrievalQA",
   115|     "RetrievalQAWithSourcesChain",
   116|     "RouterChain",
   117|     "SequentialChain",
   118|     "SimpleSequentialChain",
   119|     "StuffDocumentsChain",
   120|     "TransformChain",
   121|     "VectorDBQA",
   122|     "VectorDBQAWithSourcesChain",
   123|     "create_citation_fuzzy_match_chain",
   124|     "create_extraction_chain",
   125|     "create_extraction_chain_pydantic",
   126|     "create_qa_with_sources_chain",
   127|     "create_qa_with_structure_chain",
   128|     "create_tagging_chain",
   129|     "create_tagging_chain_pydantic",
   130|     "generate_example",
   131|     "load_chain",
   132|     "create_sql_query_chain",
   133|     "create_retrieval_chain",
   134|     "create_history_aware_retriever",
   135| ]


# ====================================================================
# FILE: libs/langchain/langchain/chains/api/news_docs.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-26 ---
     1| NEWS_DOCS = """API documentation:
     2| Endpoint: https://newsapi.org
     3| Top headlines /v2/top-headlines
     4| This endpoint provides live top and breaking headlines for a country, specific category in a country, single source, or multiple sources. You can also search with keywords. Articles are sorted by the earliest date published first.
     5| This endpoint is great for retrieving headlines for use with news tickers or similar.
     6| Request parameters
     7|     country | The 2-letter ISO 3166-1 code of the country you want to get headlines for. Possible options: ae ar at au be bg br ca ch cn co cu cz de eg fr gb gr hk hu id ie il in it jp kr lt lv ma mx my ng nl no nz ph pl pt ro rs ru sa se sg si sk th tr tw ua us ve za. Note: you can't mix this param with the sources param.
     8|     category | The category you want to get headlines for. Possible options: business entertainment general health science sports technology. Note: you can't mix this param with the sources param.
     9|     sources | A comma-separated string of identifiers for the news sources or blogs you want headlines from. Use the /top-headlines/sources endpoint to locate these programmatically or look at the sources index. Note: you can't mix this param with the country or category params.
    10|     q | Keywords or a phrase to search for.
    11|     pageSize | int | The number of results to return per page (request). 20 is the default, 100 is the maximum.
    12|     page | int | Use this to page through the results if the total results found is greater than the page size.
    13| Response object
    14|     status | string | If the request was successful or not. Options: ok, error. In the case of error a code and message property will be populated.
    15|     totalResults | int | The total number of results available for your request.
    16|     articles | array[article] | The results of the request.
    17|     source | object | The identifier id and a display name name for the source this article came from.
    18|     author | string | The author of the article
    19|     title | string | The headline or title of the article.
    20|     description | string | A description or snippet from the article.
    21|     url | string | The direct URL to the article.
    22|     urlToImage | string | The URL to a relevant image for the article.
    23|     publishedAt | string | The date and time that the article was published, in UTC (+000)
    24|     content | string | The unformatted content of the article, where available. This is truncated to 200 chars.
    25| Use page size: 2
    26| """


# ====================================================================
# FILE: libs/langchain/langchain/chains/combine_documents/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-13 ---
     1| """Different ways to combine documents."""
     2| from langchain.chains.combine_documents.reduce import (
     3|     acollapse_docs,
     4|     collapse_docs,
     5|     split_list_of_docs,
     6| )
     7| from langchain.chains.combine_documents.stuff import create_stuff_documents_chain
     8| __all__ = [
     9|     "acollapse_docs",
    10|     "collapse_docs",
    11|     "split_list_of_docs",
    12|     "create_stuff_documents_chain",
    13| ]


# ====================================================================
# FILE: libs/langchain/langchain/chains/combine_documents/base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-42 ---
     1| """Base interface for chains combining documents."""
     2| from abc import ABC, abstractmethod
     3| from typing import Any, Dict, List, Optional, Tuple, Type
     4| from langchain_core.documents import Document
     5| from langchain_core.prompts import BasePromptTemplate, PromptTemplate
     6| from langchain_core.pydantic_v1 import BaseModel, Field, create_model
     7| from langchain_core.runnables.config import RunnableConfig
     8| from langchain.callbacks.manager import (
     9|     AsyncCallbackManagerForChainRun,
    10|     CallbackManagerForChainRun,
    11| )
    12| from langchain.chains.base import Chain
    13| from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter
    14| DEFAULT_DOCUMENT_SEPARATOR = "\n\n"
    15| DOCUMENTS_KEY = "context"
    16| DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template("{page_content}")
    17| def _validate_prompt(prompt: BasePromptTemplate) -> None:
    18|     if DOCUMENTS_KEY not in prompt.input_variables:
    19|         raise ValueError(
    20|             f"Prompt must accept {DOCUMENTS_KEY} as an input variable. Received prompt "
    21|             f"with input variables: {prompt.input_variables}"
    22|         )
    23| class BaseCombineDocumentsChain(Chain, ABC):
    24|     """Base interface for chains combining documents.
    25|     Subclasses of this chain deal with combining documents in a variety of
    26|     ways. This base class exists to add some uniformity in the interface these types
    27|     of chains should expose. Namely, they expect an input key related to the documents
    28|     to use (default `input_documents`), and then also expose a method to calculate
    29|     the length of a prompt from documents (useful for outside callers to use to
    30|     determine whether it's safe to pass a list of documents into this chain or whether
    31|     that will longer than the context length).
    32|     """
    33|     input_key: str = "input_documents"  #: :meta private:
    34|     output_key: str = "output_text"  #: :meta private:
    35|     def get_input_schema(
    36|         self, config: Optional[RunnableConfig] = None
    37|     ) -> Type[BaseModel]:
    38|         return create_model(
    39|             "CombineDocumentsInput",
    40|             **{self.input_key: (List[Document], None)},  # type: ignore[call-overload]
    41|         )
    42|     def get_output_schema(


# ====================================================================
# FILE: libs/langchain/langchain/chains/combine_documents/stuff.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-127 ---
     1| """Chain that combines documents by stuffing into context."""
     2| from typing import Any, Dict, List, Optional, Tuple
     3| from langchain_core.documents import Document
     4| from langchain_core.language_models import LanguageModelLike
     5| from langchain_core.output_parsers import BaseOutputParser, StrOutputParser
     6| from langchain_core.prompts import BasePromptTemplate, format_document
     7| from langchain_core.pydantic_v1 import Extra, Field, root_validator
     8| from langchain_core.runnables import Runnable, RunnablePassthrough
     9| from langchain.callbacks.manager import Callbacks
    10| from langchain.chains.combine_documents.base import (
    11|     DEFAULT_DOCUMENT_PROMPT,
    12|     DEFAULT_DOCUMENT_SEPARATOR,
    13|     DOCUMENTS_KEY,
    14|     BaseCombineDocumentsChain,
    15|     _validate_prompt,
    16| )
    17| from langchain.chains.llm import LLMChain
    18| def create_stuff_documents_chain(
    19|     llm: LanguageModelLike,
    20|     prompt: BasePromptTemplate,
    21|     *,
    22|     output_parser: Optional[BaseOutputParser] = None,
    23|     document_prompt: Optional[BasePromptTemplate] = None,
    24|     document_separator: str = DEFAULT_DOCUMENT_SEPARATOR,
    25| ) -> Runnable[Dict[str, Any], Any]:
    26|     """Create a chain for passing a list of Documents to a model.
    27|     Args:
    28|         llm: Language model.
    29|         prompt: Prompt template. Must contain input variable "context", which will be
    30|             used for passing in the formatted documents.
    31|         output_parser: Output parser. Defaults to StrOutputParser.
    32|         document_prompt: Prompt used for formatting each document into a string. Input
    33|             variables can be "page_content" or any metadata keys that are in all
    34|             documents. "page_content" will automatically retrieve the
    35|             `Document.page_content`, and all other inputs variables will be
    36|             automatically retrieved from the `Document.metadata` dictionary. Default to
    37|             a prompt that only contains `Document.page_content`.
    38|         document_separator: String separator to use between formatted document strings.
    39|     Returns:
    40|         An LCEL Runnable. The input is a dictionary that must have a "context" key that
    41|         maps to a List[Document], and any other input variables expected in the prompt.
    42|         The Runnable return type depends on output_parser used.
    43|     Example:
    44|         .. code-block:: python
    45|             from langchain_community.chat_models import ChatOpenAI
    46|             from langchain_core.documents import Document
    47|             from langchain_core.prompts import ChatPromptTemplate
    48|             from langchain.chains.combine_documents import create_stuff_documents_chain
    49|             prompt = ChatPromptTemplate.from_messages(
    50|                 [("system", "What are everyone's favorite colors:\n\n{context}")]
    51|             )
    52|             llm = ChatOpenAI(model_name="gpt-3.5-turbo")
    53|             chain = create_stuff_documents_chain(llm, prompt)
    54|             docs = [
    55|                 Document(page_content="Jesse loves red but not yellow"),
    56|                 Document(page_content = "Jamal loves green but not as much as he loves orange")
    57|             ]
    58|             chain.invoke({"context": docs})
    59|     """  # noqa: E501
    60|     _validate_prompt(prompt)
    61|     _document_prompt = document_prompt or DEFAULT_DOCUMENT_PROMPT
    62|     _output_parser = output_parser or StrOutputParser()
    63|     def format_docs(inputs: dict) -> str:
    64|         return document_separator.join(
    65|             format_document(doc, _document_prompt) for doc in inputs[DOCUMENTS_KEY]
    66|         )
    67|     return (
    68|         RunnablePassthrough.assign(**{DOCUMENTS_KEY: format_docs}).with_config(
    69|             run_name="format_inputs"
    70|         )
    71|         | prompt
    72|         | llm
    73|         | _output_parser
    74|     ).with_config(run_name="stuff_documents_chain")
    75| class StuffDocumentsChain(BaseCombineDocumentsChain):
    76|     """Chain that combines documents by stuffing into context.
    77|     This chain takes a list of documents and first combines them into a single string.
    78|     It does this by formatting each document into a string with the `document_prompt`
    79|     and then joining them together with `document_separator`. It then adds that new
    80|     string to the inputs with the variable name set by `document_variable_name`.
    81|     Those inputs are then passed to the `llm_chain`.
    82|     Example:
    83|         .. code-block:: python
    84|             from langchain.chains import StuffDocumentsChain, LLMChain
    85|             from langchain_core.prompts import PromptTemplate
    86|             from langchain.llms import OpenAI
    87|             document_prompt = PromptTemplate(
    88|                 input_variables=["page_content"],
    89|                 template="{page_content}"
    90|             )
    91|             document_variable_name = "context"
    92|             llm = OpenAI()
    93|             prompt = PromptTemplate.from_template(
    94|                 "Summarize this content: {context}"
    95|             )
    96|             llm_chain = LLMChain(llm=llm, prompt=prompt)
    97|             chain = StuffDocumentsChain(
    98|                 llm_chain=llm_chain,
    99|                 document_prompt=document_prompt,
   100|                 document_variable_name=document_variable_name
   101|             )
   102|     """
   103|     llm_chain: LLMChain
   104|     """LLM chain which is called with the formatted document string,
   105|     along with any other inputs."""
   106|     document_prompt: BasePromptTemplate = Field(
   107|         default_factory=lambda: DEFAULT_DOCUMENT_PROMPT
   108|     )
   109|     """Prompt to use to format each document, gets passed to `format_document`."""
   110|     document_variable_name: str
   111|     """The variable name in the llm_chain to put the documents in.
   112|     If only one variable in the llm_chain, this need not be provided."""
   113|     document_separator: str = "\n\n"
   114|     """The string with which to join the formatted documents"""
   115|     class Config:
   116|         """Configuration for this pydantic object."""
   117|         extra = Extra.forbid
   118|         arbitrary_types_allowed = True
   119|     @root_validator(pre=True)
   120|     def get_default_document_variable_name(cls, values: Dict) -> Dict:
   121|         """Get default document variable name, if not provided.
   122|         If only one variable is present in the llm_chain.prompt,
   123|         we can infer that the formatted documents should be passed in
   124|         with this variable name.
   125|         """
   126|         llm_chain_variables = values["llm_chain"].prompt.input_variables
   127|         if "document_variable_name" not in values:


# ====================================================================
# FILE: libs/langchain/langchain/chains/conversational_retrieval/base.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-34 ---
     1| """Chain for chatting with a vector database."""
     2| from __future__ import annotations
     3| import inspect
     4| import warnings
     5| from abc import abstractmethod
     6| from pathlib import Path
     7| from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union
     8| from langchain_core.documents import Document
     9| from langchain_core.language_models import BaseLanguageModel
    10| from langchain_core.messages import BaseMessage
    11| from langchain_core.prompts import BasePromptTemplate
    12| from langchain_core.pydantic_v1 import BaseModel, Extra, Field, root_validator
    13| from langchain_core.retrievers import BaseRetriever
    14| from langchain_core.runnables import RunnableConfig
    15| from langchain_core.vectorstores import VectorStore
    16| from langchain.callbacks.manager import (
    17|     AsyncCallbackManagerForChainRun,
    18|     CallbackManagerForChainRun,
    19|     Callbacks,
    20| )
    21| from langchain.chains.base import Chain
    22| from langchain.chains.combine_documents.base import BaseCombineDocumentsChain
    23| from langchain.chains.combine_documents.stuff import StuffDocumentsChain
    24| from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT
    25| from langchain.chains.llm import LLMChain
    26| from langchain.chains.question_answering import load_qa_chain
    27| CHAT_TURN_TYPE = Union[Tuple[str, str], BaseMessage]
    28| _ROLE_MAP = {"human": "Human: ", "ai": "Assistant: "}
    29| def _get_chat_history(chat_history: List[CHAT_TURN_TYPE]) -> str:
    30|     buffer = ""
    31|     for dialogue_turn in chat_history:
    32|         if isinstance(dialogue_turn, BaseMessage):
    33|             role_prefix = _ROLE_MAP.get(dialogue_turn.type, f"{dialogue_turn.type}: ")
    34|             buffer += f"\n{role_prefix}{dialogue_turn.content}"


# ====================================================================
# FILE: libs/langchain/langchain/chains/history_aware_retriever.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-50 ---
     1| from __future__ import annotations
     2| from langchain_core.language_models import LanguageModelLike
     3| from langchain_core.output_parsers import StrOutputParser
     4| from langchain_core.prompts import BasePromptTemplate
     5| from langchain_core.retrievers import RetrieverLike, RetrieverOutputLike
     6| from langchain_core.runnables import RunnableBranch
     7| def create_history_aware_retriever(
     8|     llm: LanguageModelLike,
     9|     retriever: RetrieverLike,
    10|     prompt: BasePromptTemplate,
    11| ) -> RetrieverOutputLike:
    12|     """Create a chain that takes conversation history and returns documents.
    13|     If there is no `chat_history`, then the `input` is just passed directly to the
    14|     retriever. If there is `chat_history`, then the prompt and LLM will be used
    15|     to generate a search query. That search query is then passed to the retriever.
    16|     Args:
    17|         llm: Language model to use for generating a search term given chat history
    18|         retriever: RetrieverLike object that takes a string as input and outputs
    19|             a list of Documents.
    20|         prompt: The prompt used to generate the search query for the retriever.
    21|     Returns:
    22|         An LCEL Runnable. The runnable input must take in `input`, and if there
    23|         is chat history should take it in the form of `chat_history`.
    24|         The Runnable output is a list of Documents
    25|     Example:
    26|         .. code-block:: python
    27|             from langchain_community.chat_models import ChatOpenAI
    28|             from langchain.chains import create_history_aware_retriever
    29|             from langchain import hub
    30|             rephrase_prompt = hub.pull("langchain-ai/chat-langchain-rephrase")
    31|             llm = ChatOpenAI()
    32|             retriever = ...
    33|             chat_retriever_chain = create_history_aware_retriever(
    34|                 llm, retriever, rephrase_prompt
    35|             )
    36|             chain.invoke({"input": "...", "chat_history": })
    37|     """
    38|     if "input" not in prompt.input_variables:
    39|         raise ValueError(
    40|             "Expected `input` to be a prompt variable, "
    41|             f"but got {prompt.input_variables}"
    42|         )
    43|     retrieve_documents: RetrieverOutputLike = RunnableBranch(
    44|         (
    45|             lambda x: not x.get("chat_history", False),
    46|             (lambda x: x["input"]) | retriever,
    47|         ),
    48|         prompt | llm | StrOutputParser() | retriever,
    49|     ).with_config(run_name="chat_retriever_chain")
    50|     return retrieve_documents


# ====================================================================
# FILE: libs/langchain/langchain/chains/retrieval.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-54 ---
     1| from __future__ import annotations
     2| from typing import Any, Dict, Union
     3| from langchain_core.retrievers import (
     4|     BaseRetriever,
     5|     RetrieverOutput,
     6| )
     7| from langchain_core.runnables import Runnable, RunnablePassthrough
     8| def create_retrieval_chain(
     9|     retriever: Union[BaseRetriever, Runnable[dict, RetrieverOutput]],
    10|     combine_docs_chain: Runnable[Dict[str, Any], str],
    11| ) -> Runnable:
    12|     """Create retrieval chain that retrieves documents and then passes them on.
    13|     Args:
    14|         retriever: Retriever-like object that returns list of documents. Should
    15|             either be a subclass of BaseRetriever or a Runnable that returns
    16|             a list of documents. If a subclass of BaseRetriever, then it
    17|             is expected that an `input` key be passed in - this is what
    18|             is will be used to pass into the retriever. If this is NOT a
    19|             subclass of BaseRetriever, then all the inputs will be passed
    20|             into this runnable, meaning that runnable should take a dictionary
    21|             as input.
    22|         combine_docs_chain: Runnable that takes inputs and produces a string output.
    23|             The inputs to this will be any original inputs to this chain, a new
    24|             context key with the retrieved documents, and chat_history (if not present
    25|             in the inputs) with a value of `[]` (to easily enable conversational
    26|             retrieval.
    27|     Returns:
    28|         An LCEL Runnable. The Runnable return is a dictionary containing at the very
    29|         least a `context` and `answer` key.
    30|     Example:
    31|         .. code-block:: python
    32|             from langchain_community.chat_models import ChatOpenAI
    33|             from langchain.chains.combine_documents import create_stuff_documents_chain
    34|             from langchain.chains import create_retrieval_chain
    35|             from langchain import hub
    36|             retrieval_qa_chat_prompt = hub.pull("langchain-ai/retrieval-qa-chat")
    37|             llm = ChatOpenAI()
    38|             retriever = ...
    39|             combine_docs_chain = create_stuff_documents_chain(
    40|                 llm, retrieval_qa_chat_prompt
    41|             )
    42|             retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)
    43|             chain.invoke({"input": "..."})
    44|     """
    45|     if not isinstance(retriever, BaseRetriever):
    46|         retrieval_docs: Runnable[dict, RetrieverOutput] = retriever
    47|     else:
    48|         retrieval_docs = (lambda x: x["input"]) | retriever
    49|     retrieval_chain = (
    50|         RunnablePassthrough.assign(
    51|             context=retrieval_docs.with_config(run_name="retrieve_documents"),
    52|         ).assign(answer=combine_docs_chain)
    53|     ).with_config(run_name="retrieval_chain")
    54|     return retrieval_chain


# ====================================================================
# FILE: libs/langchain/langchain/evaluation/criteria/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-31 ---
     1| """Criteria or rubric based evaluators.
     2| These evaluators are useful for evaluating the
     3| output of a language model or chain against
     4| specified criteria or rubric.
     5| Classes
     6| -------
     7| CriteriaEvalChain : Evaluates the output of a language model or
     8| chain against specified criteria.
     9| Examples
    10| --------
    11| Using a predefined criterion:
    12| >>> from langchain.llms import OpenAI
    13| >>> from langchain.evaluation.criteria import CriteriaEvalChain
    14| >>> llm = OpenAI()
    15| >>> criteria = "conciseness"
    16| >>> chain = CriteriaEvalChain.from_llm(llm=llm, criteria=criteria)
    17| >>> chain.evaluate_strings(
    18|         prediction="The answer is 42.",
    19|         reference="42",
    20|         input="What is the answer to life, the universe, and everything?",
    21|     )
    22| Using a custom criterion:
    23| >>> from langchain.llms import OpenAI
    24| >>> from langchain.evaluation.criteria import LabeledCriteriaEvalChain
    25| >>> llm = OpenAI()
    26| >>> criteria = {
    27|        "hallucination": (
    28|             "Does this submission contain information"
    29|             " not present in the input or reference?"
    30|         ),
    31|     }


# ====================================================================
# FILE: libs/langchain/langchain/evaluation/parsing/base.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-22 ---
     1| """Evaluators for parsing strings."""
     2| import json
     3| from operator import eq
     4| from typing import Any, Callable, Optional, Union, cast
     5| from langchain.evaluation.schema import StringEvaluator
     6| from langchain.output_parsers.json import parse_json_markdown
     7| class JsonValidityEvaluator(StringEvaluator):
     8|     """Evaluates whether the prediction is valid JSON.
     9|     This evaluator checks if the prediction is a valid JSON string. It does not
    10|         require any input or reference.
    11|     Attributes:
    12|         requires_input (bool): Whether this evaluator requires an input
    13|             string. Always False.
    14|         requires_reference (bool): Whether this evaluator requires a
    15|             reference string. Always False.
    16|         evaluation_name (str): The name of the evaluation metric.
    17|             Always "json".
    18|     Examples:
    19|         >>> evaluator = JsonValidityEvaluator()
    20|         >>> prediction = '{"name": "John", "age": 30, "city": "New York"}'
    21|         >>> evaluator.evaluate(prediction)
    22|         {'score': 1}

# --- HUNK 2: Lines 37-77 ---
    37|         return "json_validity"
    38|     def _evaluate_strings(
    39|         self,
    40|         prediction: str,
    41|         input: Optional[str] = None,
    42|         reference: Optional[str] = None,
    43|         **kwargs: Any,
    44|     ) -> dict:
    45|         """Evaluate the prediction string.
    46|         Args:
    47|             prediction (str): The prediction string to evaluate.
    48|             input (str, optional): Not used in this evaluator. Defaults to None.
    49|             reference (str, optional): Not used in this evaluator. Defaults to None.
    50|         Returns:
    51|             dict: A dictionary containing the evaluation score. The score is 1 if
    52|             the prediction is valid JSON, and 0 otherwise.
    53|                 If the prediction is not valid JSON, the dictionary also contains
    54|                 a "reasoning" field with the error message.
    55|         """
    56|         try:
    57|             parse_json_markdown(prediction, parser=json.loads)
    58|             return {"score": 1}
    59|         except Exception as e:
    60|             return {"score": 0, "reasoning": str(e)}
    61| class JsonEqualityEvaluator(StringEvaluator):
    62|     """Evaluates whether the prediction is equal to the reference after
    63|         parsing both as JSON.
    64|     This evaluator checks if the prediction, after parsing as JSON, is equal
    65|         to the reference,
    66|     which is also parsed as JSON. It does not require an input string.
    67|     Attributes:
    68|         requires_input (bool): Whether this evaluator requires an
    69|             input string. Always False.
    70|         requires_reference (bool): Whether this evaluator requires
    71|             a reference string. Always True.
    72|         evaluation_name (str): The name of the evaluation metric.
    73|             Always "parsed_equality".
    74|     Examples:
    75|         >>> evaluator = JsonEqualityEvaluator()
    76|         >>> evaluator.evaluate_strings('{"a": 1}', reference='{"a": 1}')
    77|         {'score': True}


# ====================================================================
# FILE: libs/langchain/langchain/output_parsers/datetime.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 13-48 ---
    13|     """Generates n random datetime strings conforming to the
    14|     given pattern within the specified date range.
    15|     Pattern should be a string containing the desired format codes.
    16|     start_date and end_date should be datetime objects representing
    17|     the start and end of the date range.
    18|     """
    19|     examples = []
    20|     delta = end_date - start_date
    21|     for i in range(n):
    22|         random_delta = random.uniform(0, delta.total_seconds())
    23|         dt = start_date + timedelta(seconds=random_delta)
    24|         date_string = dt.strftime(pattern)
    25|         examples.append(date_string)
    26|     return examples
    27| class DatetimeOutputParser(BaseOutputParser[datetime]):
    28|     """Parse the output of an LLM call to a datetime."""
    29|     format: str = "%Y-%m-%dT%H:%M:%S.%fZ"
    30|     """The string value that used as the datetime format."""
    31|     def get_format_instructions(self) -> str:
    32|         examples = comma_list(_generate_random_datetime_strings(self.format))
    33|         return (
    34|             f"Write a datetime string that matches the "
    35|             f"following pattern: '{self.format}'.\n\n"
    36|             f"Examples: {examples}\n\n"
    37|             f"Return ONLY this string, no other words!"
    38|         )
    39|     def parse(self, response: str) -> datetime:
    40|         try:
    41|             return datetime.strptime(response.strip(), self.format)
    42|         except ValueError as e:
    43|             raise OutputParserException(
    44|                 f"Could not parse datetime string: {response}"
    45|             ) from e
    46|     @property
    47|     def _type(self) -> str:
    48|         return "datetime"


# ====================================================================
# FILE: libs/langchain/langchain/output_parsers/format_instructions.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 17-54 ---
    17| ```
    18| {schema}
    19| ```"""
    20| YAML_FORMAT_INSTRUCTIONS = """The output should be formatted as a YAML instance that conforms to the given JSON schema below.
    21| As an example, for the schema
    22| ```
    23| {{'title': 'Players', 'description': 'A list of players', 'type': 'array', 'items': {{'$ref': '#/definitions/Player'}}, 'definitions': {{'Player': {{'title': 'Player', 'type': 'object', 'properties': {{'name': {{'title': 'Name', 'description': 'Player name', 'type': 'string'}}, 'avg': {{'title': 'Avg', 'description': 'Batting average', 'type': 'number'}}}}, 'required': ['name', 'avg']}}}}}}
    24| ```
    25| a well formatted instance would be:
    26| ```
    27| - name: John Doe
    28|   avg: 0.3
    29| - name: Jane Maxfield
    30|   avg: 1.4
    31| ```
    32| Please follow the standard YAML formatting conventions with an indent of 2 spaces and make sure that the data types adhere strictly to the following JSON schema: 
    33| ```
    34| {schema}
    35| ```
    36| Make sure to always enclose the YAML output in triple backticks (```)"""
    37| PANDAS_DATAFRAME_FORMAT_INSTRUCTIONS = """The output should be formatted as a string as the operation, followed by a colon, followed by the column or row to be queried on, followed by optional array parameters.
    38| 1. The column names are limited to the possible columns below.
    39| 2. Arrays must either be a comma-separated list of numbers formatted as [1,3,5], or it must be in range of numbers formatted as [0..4].
    40| 3. Remember that arrays are optional and not necessarily required.
    41| 4. If the column is not in the possible columns or the operation is not a valid Pandas DataFrame operation, return why it is invalid as a sentence starting with either "Invalid column" or "Invalid operation".
    42| As an example, for the formats:
    43| 1. String "column:num_legs" is a well-formatted instance which gets the column num_legs, where num_legs is a possible column.
    44| 2. String "row:1" is a well-formatted instance which gets row 1.
    45| 3. String "column:num_legs[1,2]" is a well-formatted instance which gets the column num_legs for rows 1 and 2, where num_legs is a possible column.
    46| 4. String "row:1[num_legs]" is a well-formatted instance which gets row 1, but for just column num_legs, where num_legs is a possible column.
    47| 5. String "mean:num_legs[1..3]" is a well-formatted instance which takes the mean of num_legs from rows 1 to 3, where num_legs is a possible column and mean is a valid Pandas DataFrame operation.
    48| 6. String "do_something:num_legs" is a badly-formatted instance, where do_something is not a valid Pandas DataFrame operation.
    49| 7. String "mean:invalid_col" is a badly-formatted instance, where invalid_col is not a possible column.
    50| Here are the possible columns:
    51| ```
    52| {columns}
    53| ```
    54| """


# ====================================================================
# FILE: libs/langchain/langchain/output_parsers/json.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-12 ---
     1| from langchain_core.output_parsers.json import (
     2|     SimpleJsonOutputParser,
     3|     parse_and_check_json_markdown,
     4|     parse_json_markdown,
     5|     parse_partial_json,
     6| )
     7| __all__ = [
     8|     "SimpleJsonOutputParser",
     9|     "parse_partial_json",
    10|     "parse_json_markdown",
    11|     "parse_and_check_json_markdown",
    12| ]


# ====================================================================
# FILE: libs/langchain/langchain/output_parsers/retry.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 25-65 ---
    25| T = TypeVar("T")
    26| class RetryOutputParser(BaseOutputParser[T]):
    27|     """Wraps a parser and tries to fix parsing errors.
    28|     Does this by passing the original prompt and the completion to another
    29|     LLM, and telling it the completion did not satisfy criteria in the prompt.
    30|     """
    31|     parser: BaseOutputParser[T]
    32|     """The parser to use to parse the output."""
    33|     retry_chain: Any
    34|     """The LLMChain to use to retry the completion."""
    35|     max_retries: int = 1
    36|     """The maximum number of times to retry the parse."""
    37|     @classmethod
    38|     def from_llm(
    39|         cls,
    40|         llm: BaseLanguageModel,
    41|         parser: BaseOutputParser[T],
    42|         prompt: BasePromptTemplate = NAIVE_RETRY_PROMPT,
    43|         max_retries: int = 1,
    44|     ) -> RetryOutputParser[T]:
    45|         """Create an RetryOutputParser from a language model and a parser.
    46|         Args:
    47|             llm: llm to use for fixing
    48|             parser: parser to use for parsing
    49|             prompt: prompt to use for fixing
    50|             max_retries: Maximum number of retries to parse.
    51|         Returns:
    52|             RetryOutputParser
    53|         """
    54|         from langchain.chains.llm import LLMChain
    55|         chain = LLMChain(llm=llm, prompt=prompt)
    56|         return cls(parser=parser, retry_chain=chain, max_retries=max_retries)
    57|     def parse_with_prompt(self, completion: str, prompt_value: PromptValue) -> T:
    58|         """Parse the output of an LLM call using a wrapped parser.
    59|         Args:
    60|             completion: The chain completion to parse.
    61|             prompt_value: The prompt to use to parse the completion.
    62|         Returns:
    63|             The parsed completion.
    64|         """
    65|         retries = 0


# ====================================================================
# FILE: libs/langchain/langchain/output_parsers/xml.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-2 ---
     1| from langchain_core.output_parsers.xml import XMLOutputParser
     2| __all__ = ["XMLOutputParser"]


# ====================================================================
# FILE: libs/langchain/langchain/storage/file_system.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-63 ---
     1| import os
     2| import re
     3| from pathlib import Path
     4| from typing import Iterator, List, Optional, Sequence, Tuple, Union
     5| from langchain_core.stores import ByteStore
     6| from langchain.storage.exceptions import InvalidKeyException
     7| class LocalFileStore(ByteStore):
     8|     """BaseStore interface that works on the local file system.
     9|     Examples:
    10|         Create a LocalFileStore instance and perform operations on it:
    11|         .. code-block:: python
    12|             from langchain.storage import LocalFileStore
    13|             file_store = LocalFileStore("/path/to/root")
    14|             file_store.mset([("key1", b"value1"), ("key2", b"value2")])
    15|             values = file_store.mget(["key1", "key2"])  # Returns [b"value1", b"value2"]
    16|             file_store.mdelete(["key1"])
    17|             for key in file_store.yield_keys():
    18|                 print(key)
    19|     """
    20|     def __init__(self, root_path: Union[str, Path]) -> None:
    21|         """Implement the BaseStore interface for the local file system.
    22|         Args:
    23|             root_path (Union[str, Path]): The root path of the file store. All keys are
    24|                 interpreted as paths relative to this root.
    25|         """
    26|         self.root_path = Path(root_path).absolute()
    27|     def _get_full_path(self, key: str) -> Path:
    28|         """Get the full path for a given key relative to the root path.
    29|         Args:
    30|             key (str): The key relative to the root path.
    31|         Returns:
    32|             Path: The full path for the given key.
    33|         """
    34|         if not re.match(r"^[a-zA-Z0-9_.\-/]+$", key):
    35|             raise InvalidKeyException(f"Invalid characters in key: {key}")
    36|         full_path = os.path.abspath(self.root_path / key)
    37|         common_path = os.path.commonpath([str(self.root_path), full_path])
    38|         if common_path != str(self.root_path):
    39|             raise InvalidKeyException(
    40|                 f"Invalid key: {key}. Key should be relative to the full path."
    41|                 f"{self.root_path} vs. {common_path} and full path of {full_path}"
    42|             )
    43|         return Path(full_path)
    44|     def mget(self, keys: Sequence[str]) -> List[Optional[bytes]]:
    45|         """Get the values associated with the given keys.
    46|         Args:
    47|             keys: A sequence of keys.
    48|         Returns:
    49|             A sequence of optional values associated with the keys.
    50|             If a key is not found, the corresponding value will be None.
    51|         """
    52|         values: List[Optional[bytes]] = []
    53|         for key in keys:
    54|             full_path = self._get_full_path(key)
    55|             if full_path.exists():
    56|                 value = full_path.read_bytes()
    57|                 values.append(value)
    58|             else:
    59|                 values.append(None)
    60|         return values
    61|     def mset(self, key_value_pairs: Sequence[Tuple[str, bytes]]) -> None:
    62|         """Set the values for the given keys.
    63|         Args:


# ====================================================================
# FILE: templates/rag-chroma-multi-modal-multi-vector/ingest.py
# Total hunks: 3
# ====================================================================
# --- HUNK 1: Lines 1-33 ---
     1| import base64
     2| import io
     3| import os
     4| import uuid
     5| from io import BytesIO
     6| from pathlib import Path
     7| import pypdfium2 as pdfium
     8| from langchain.chat_models import ChatOpenAI
     9| from langchain.embeddings import OpenAIEmbeddings
    10| from langchain.retrievers.multi_vector import MultiVectorRetriever
    11| from langchain.schema.document import Document
    12| from langchain.schema.messages import HumanMessage
    13| from langchain.storage import LocalFileStore, UpstashRedisByteStore
    14| from langchain.vectorstores import Chroma
    15| from PIL import Image
    16| def image_summarize(img_base64, prompt):
    17|     """
    18|     Make image summary
    19|     :param img_base64: Base64 encoded string for image
    20|     :param prompt: Text prompt for summarizatiomn
    21|     :return: Image summarization prompt
    22|     """
    23|     chat = ChatOpenAI(model="gpt-4-vision-preview", max_tokens=1024)
    24|     msg = chat.invoke(
    25|         [
    26|             HumanMessage(
    27|                 content=[
    28|                     {"type": "text", "text": prompt},
    29|                     {
    30|                         "type": "image_url",
    31|                         "image_url": {"url": f"data:image/jpeg;base64,{img_base64}"},
    32|                     },
    33|                 ]

# --- HUNK 2: Lines 74-149 ---
    74|     :param size: Image size
    75|     :return: Re-sized Base64 string
    76|     """
    77|     img_data = base64.b64decode(base64_string)
    78|     img = Image.open(io.BytesIO(img_data))
    79|     resized_img = img.resize(size, Image.LANCZOS)
    80|     buffered = io.BytesIO()
    81|     resized_img.save(buffered, format=img.format)
    82|     return base64.b64encode(buffered.getvalue()).decode("utf-8")
    83| def convert_to_base64(pil_image):
    84|     """
    85|     Convert PIL images to Base64 encoded strings
    86|     :param pil_image: PIL image
    87|     :return: Re-sized Base64 string
    88|     """
    89|     buffered = BytesIO()
    90|     pil_image.save(buffered, format="JPEG")  # You can change the format if needed
    91|     img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
    92|     img_str = resize_base64_image(img_str, size=(960, 540))
    93|     return img_str
    94| def create_multi_vector_retriever(
    95|     vectorstore, image_summaries, images, local_file_store
    96| ):
    97|     """
    98|     Create retriever that indexes summaries, but returns raw images or texts
    99|     :param vectorstore: Vectorstore to store embedded image sumamries
   100|     :param image_summaries: Image summaries
   101|     :param images: Base64 encoded images
   102|     :param local_file_store: Use local file storage
   103|     :return: Retriever
   104|     """
   105|     if local_file_store:
   106|         store = LocalFileStore(
   107|             str(Path(__file__).parent / "multi_vector_retriever_metadata")
   108|         )
   109|     else:
   110|         UPSTASH_URL = os.getenv("UPSTASH_URL")
   111|         UPSTASH_TOKEN = os.getenv("UPSTASH_TOKEN")
   112|         store = UpstashRedisByteStore(url=UPSTASH_URL, token=UPSTASH_TOKEN)
   113|     id_key = "doc_id"
   114|     retriever = MultiVectorRetriever(
   115|         vectorstore=vectorstore,
   116|         byte_store=store,
   117|         id_key=id_key,
   118|     )
   119|     def add_documents(retriever, doc_summaries, doc_contents):
   120|         doc_ids = [str(uuid.uuid4()) for _ in doc_contents]
   121|         summary_docs = [
   122|             Document(page_content=s, metadata={id_key: doc_ids[i]})
   123|             for i, s in enumerate(doc_summaries)
   124|         ]
   125|         retriever.vectorstore.add_documents(summary_docs)
   126|         retriever.docstore.mset(list(zip(doc_ids, doc_contents)))
   127|     add_documents(retriever, image_summaries, images)
   128|     return retriever
   129| doc_path = Path(__file__).parent / "docs/DDOG_Q3_earnings_deck.pdf"
   130| rel_doc_path = doc_path.relative_to(Path.cwd())
   131| print("Extract slides as images")
   132| pil_images = get_images_from_pdf(rel_doc_path)
   133| images_base_64 = [convert_to_base64(i) for i in pil_images]
   134| print("Generate image summaries")
   135| image_summaries, images_base_64_processed = generate_img_summaries(images_base_64)
   136| vectorstore_mvr = Chroma(
   137|     collection_name="image_summaries",
   138|     persist_directory=str(Path(__file__).parent / "chroma_db_multi_modal"),
   139|     embedding_function=OpenAIEmbeddings(),
   140| )
   141| images_base_64_processed_documents = [
   142|     Document(page_content=i) for i in images_base_64_processed
   143| ]
   144| retriever_multi_vector_img = create_multi_vector_retriever(
   145|     vectorstore_mvr,
   146|     image_summaries,
   147|     images_base_64_processed_documents,
   148|     local_file_store=True,
   149| )


# ====================================================================
# FILE: templates/rag-chroma-multi-modal-multi-vector/rag_chroma_multi_modal_multi_vector/chain.py
# Total hunks: 2
# ====================================================================
# --- HUNK 1: Lines 1-33 ---
     1| import base64
     2| import io
     3| import os
     4| from pathlib import Path
     5| from langchain.chat_models import ChatOpenAI
     6| from langchain.embeddings import OpenAIEmbeddings
     7| from langchain.pydantic_v1 import BaseModel
     8| from langchain.retrievers.multi_vector import MultiVectorRetriever
     9| from langchain.schema.document import Document
    10| from langchain.schema.messages import HumanMessage
    11| from langchain.schema.output_parser import StrOutputParser
    12| from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
    13| from langchain.storage import LocalFileStore, UpstashRedisByteStore
    14| from langchain.vectorstores import Chroma
    15| from PIL import Image
    16| def resize_base64_image(base64_string, size=(128, 128)):
    17|     """
    18|     Resize an image encoded as a Base64 string.
    19|     :param base64_string: A Base64 encoded string of the image to be resized.
    20|     :param size: A tuple representing the new size (width, height) for the image.
    21|     :return: A Base64 encoded string of the resized image.
    22|     """
    23|     img_data = base64.b64decode(base64_string)
    24|     img = Image.open(io.BytesIO(img_data))
    25|     resized_img = img.resize(size, Image.LANCZOS)
    26|     buffered = io.BytesIO()
    27|     resized_img.save(buffered, format=img.format)
    28|     return base64.b64encode(buffered.getvalue()).decode("utf-8")
    29| def get_resized_images(docs):
    30|     """
    31|     Resize images from base64-encoded strings.
    32|     :param docs: A list of base64-encoded image to be resized.
    33|     :return: Dict containing a list of resized base64-encoded strings.

# --- HUNK 2: Lines 65-108 ---
    65|     }
    66|     messages.append(text_message)
    67|     return [HumanMessage(content=messages)]
    68| def multi_modal_rag_chain(retriever):
    69|     """
    70|     Multi-modal RAG chain,
    71|     :param retriever: A function that retrieves the necessary context for the model.
    72|     :return: A chain of functions representing the multi-modal RAG process.
    73|     """
    74|     model = ChatOpenAI(temperature=0, model="gpt-4-vision-preview", max_tokens=1024)
    75|     chain = (
    76|         {
    77|             "context": retriever | RunnableLambda(get_resized_images),
    78|             "question": RunnablePassthrough(),
    79|         }
    80|         | RunnableLambda(img_prompt_func)
    81|         | model
    82|         | StrOutputParser()
    83|     )
    84|     return chain
    85| local_file_store = True
    86| vectorstore_mvr = Chroma(
    87|     collection_name="image_summaries",
    88|     persist_directory=str(Path(__file__).parent.parent / "chroma_db_multi_modal"),
    89|     embedding_function=OpenAIEmbeddings(),
    90| )
    91| if local_file_store:
    92|     store = LocalFileStore(
    93|         str(Path(__file__).parent.parent / "multi_vector_retriever_metadata")
    94|     )
    95| else:
    96|     UPSTASH_URL = os.getenv("UPSTASH_URL")
    97|     UPSTASH_TOKEN = os.getenv("UPSTASH_TOKEN")
    98|     store = UpstashRedisByteStore(url=UPSTASH_URL, token=UPSTASH_TOKEN)
    99| id_key = "doc_id"
   100| retriever = MultiVectorRetriever(
   101|     vectorstore=vectorstore_mvr,
   102|     byte_store=store,
   103|     id_key=id_key,
   104| )
   105| chain = multi_modal_rag_chain(retriever)
   106| class Question(BaseModel):
   107|     __root__: str
   108| chain = chain.with_types(input_type=Question)


# ====================================================================
# FILE: templates/rag-multi-modal-local/ingest.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-24 ---
     1| import os
     2| from pathlib import Path
     3| from langchain.vectorstores import Chroma
     4| from langchain_experimental.open_clip import OpenCLIPEmbeddings
     5| img_dump_path = Path(__file__).parent / "docs/"
     6| rel_img_dump_path = img_dump_path.relative_to(Path.cwd())
     7| image_uris = sorted(
     8|     [
     9|         os.path.join(rel_img_dump_path, image_name)
    10|         for image_name in os.listdir(rel_img_dump_path)
    11|         if image_name.endswith(".jpg")
    12|     ]
    13| )
    14| vectorstore = Path(__file__).parent / "chroma_db_multi_modal"
    15| re_vectorstore_path = vectorstore.relative_to(Path.cwd())
    16| print("Loading embedding function")
    17| embedding = OpenCLIPEmbeddings(model_name="ViT-H-14", checkpoint="laion2b_s32b_b79k")
    18| vectorstore_mmembd = Chroma(
    19|     collection_name="multi-modal-rag",
    20|     persist_directory=str(Path(__file__).parent / "chroma_db_multi_modal"),
    21|     embedding_function=embedding,
    22| )
    23| print("Embedding images")
    24| vectorstore_mmembd.add_images(uris=image_uris)


# ====================================================================
# FILE: templates/rag-multi-modal-local/rag_multi_modal_local/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-2 ---
     1| from rag_multi_modal_local.chain import chain
     2| __all__ = ["chain"]


# ====================================================================
# FILE: templates/rag-multi-modal-local/rag_multi_modal_local/chain.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-91 ---
     1| import base64
     2| import io
     3| from pathlib import Path
     4| from langchain.chat_models import ChatOllama
     5| from langchain.vectorstores import Chroma
     6| from langchain_core.documents import Document
     7| from langchain_core.messages import HumanMessage
     8| from langchain_core.output_parsers import StrOutputParser
     9| from langchain_core.pydantic_v1 import BaseModel
    10| from langchain_core.runnables import RunnableLambda, RunnablePassthrough
    11| from langchain_experimental.open_clip import OpenCLIPEmbeddings
    12| from PIL import Image
    13| def resize_base64_image(base64_string, size=(128, 128)):
    14|     """
    15|     Resize an image encoded as a Base64 string.
    16|     :param base64_string: A Base64 encoded string of the image to be resized.
    17|     :param size: A tuple representing the new size (width, height) for the image.
    18|     :return: A Base64 encoded string of the resized image.
    19|     """
    20|     img_data = base64.b64decode(base64_string)
    21|     img = Image.open(io.BytesIO(img_data))
    22|     resized_img = img.resize(size, Image.LANCZOS)
    23|     buffered = io.BytesIO()
    24|     resized_img.save(buffered, format=img.format)
    25|     return base64.b64encode(buffered.getvalue()).decode("utf-8")
    26| def get_resized_images(docs):
    27|     """
    28|     Resize images from base64-encoded strings.
    29|     :param docs: A list of base64-encoded image to be resized.
    30|     :return: Dict containing a list of resized base64-encoded strings.
    31|     """
    32|     b64_images = []
    33|     for doc in docs:
    34|         if isinstance(doc, Document):
    35|             doc = doc.page_content
    36|         b64_images.append(doc)
    37|     return {"images": b64_images}
    38| def img_prompt_func(data_dict, num_images=1):
    39|     """
    40|     GPT-4V prompt for image analysis.
    41|     :param data_dict: A dict with images and a user-provided question.
    42|     :param num_images: Number of images to include in the prompt.
    43|     :return: A list containing message objects for each image and the text prompt.
    44|     """
    45|     messages = []
    46|     if data_dict["context"]["images"]:
    47|         for image in data_dict["context"]["images"][:num_images]:
    48|             image_message = {
    49|                 "type": "image_url",
    50|                 "image_url": f"data:image/jpeg;base64,{image}",
    51|             }
    52|             messages.append(image_message)
    53|     text_message = {
    54|         "type": "text",
    55|         "text": (
    56|             "You are a helpful assistant that gives a description of food pictures.\n"
    57|             "Give a detailed summary of the image.\n"
    58|             "Give reccomendations for similar foods to try.\n"
    59|         ),
    60|     }
    61|     messages.append(text_message)
    62|     return [HumanMessage(content=messages)]
    63| def multi_modal_rag_chain(retriever):
    64|     """
    65|     Multi-modal RAG chain,
    66|     :param retriever: A function that retrieves the necessary context for the model.
    67|     :return: A chain of functions representing the multi-modal RAG process.
    68|     """
    69|     model = ChatOllama(model="bakllava", temperature=0)
    70|     chain = (
    71|         {
    72|             "context": retriever | RunnableLambda(get_resized_images),
    73|             "question": RunnablePassthrough(),
    74|         }
    75|         | RunnableLambda(img_prompt_func)
    76|         | model
    77|         | StrOutputParser()
    78|     )
    79|     return chain
    80| vectorstore_mmembd = Chroma(
    81|     collection_name="multi-modal-rag",
    82|     persist_directory=str(Path(__file__).parent.parent / "chroma_db_multi_modal"),
    83|     embedding_function=OpenCLIPEmbeddings(
    84|         model_name="ViT-H-14", checkpoint="laion2b_s32b_b79k"
    85|     ),
    86| )
    87| retriever_mmembd = vectorstore_mmembd.as_retriever()
    88| chain = multi_modal_rag_chain(retriever_mmembd)
    89| class Question(BaseModel):
    90|     __root__: str
    91| chain = chain.with_types(input_type=Question)


# ====================================================================
# FILE: templates/rag-multi-modal-mv-local/ingest.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-133 ---
     1| import base64
     2| import io
     3| import os
     4| import uuid
     5| from io import BytesIO
     6| from pathlib import Path
     7| from langchain.chat_models import ChatOllama
     8| from langchain.embeddings import OllamaEmbeddings
     9| from langchain.retrievers.multi_vector import MultiVectorRetriever
    10| from langchain.schema.document import Document
    11| from langchain.schema.messages import HumanMessage
    12| from langchain.storage import LocalFileStore
    13| from langchain.vectorstores import Chroma
    14| from PIL import Image
    15| def image_summarize(img_base64, prompt):
    16|     """
    17|     Make image summary
    18|     :param img_base64: Base64 encoded string for image
    19|     :param prompt: Text prompt for summarizatiomn
    20|     :return: Image summarization prompt
    21|     """
    22|     chat = ChatOllama(model="bakllava", temperature=0)
    23|     msg = chat.invoke(
    24|         [
    25|             HumanMessage(
    26|                 content=[
    27|                     {"type": "text", "text": prompt},
    28|                     {
    29|                         "type": "image_url",
    30|                         "image_url": f"data:image/jpeg;base64,{img_base64}",
    31|                     },
    32|                 ]
    33|             )
    34|         ]
    35|     )
    36|     return msg.content
    37| def generate_img_summaries(img_base64_list):
    38|     """
    39|     Generate summaries for images
    40|     :param img_base64_list: Base64 encoded images
    41|     :return: List of image summaries and processed images
    42|     """
    43|     image_summaries = []
    44|     processed_images = []
    45|     prompt = """Give a detailed summary of the image."""
    46|     for i, base64_image in enumerate(img_base64_list):
    47|         try:
    48|             image_summaries.append(image_summarize(base64_image, prompt))
    49|             processed_images.append(base64_image)
    50|         except Exception as e:
    51|             print(f"Error with image {i+1}: {e}")
    52|     return image_summaries, processed_images
    53| def get_images(img_path):
    54|     """
    55|     Extract images.
    56|     :param img_path: A string representing the path to the images.
    57|     """
    58|     pil_images = [
    59|         Image.open(os.path.join(img_path, image_name))
    60|         for image_name in os.listdir(img_path)
    61|         if image_name.endswith(".jpg")
    62|     ]
    63|     return pil_images
    64| def resize_base64_image(base64_string, size=(128, 128)):
    65|     """
    66|     Resize an image encoded as a Base64 string
    67|     :param base64_string: Base64 string
    68|     :param size: Image size
    69|     :return: Re-sized Base64 string
    70|     """
    71|     img_data = base64.b64decode(base64_string)
    72|     img = Image.open(io.BytesIO(img_data))
    73|     resized_img = img.resize(size, Image.LANCZOS)
    74|     buffered = io.BytesIO()
    75|     resized_img.save(buffered, format=img.format)
    76|     return base64.b64encode(buffered.getvalue()).decode("utf-8")
    77| def convert_to_base64(pil_image):
    78|     """
    79|     Convert PIL images to Base64 encoded strings
    80|     :param pil_image: PIL image
    81|     :return: Re-sized Base64 string
    82|     """
    83|     buffered = BytesIO()
    84|     pil_image.save(buffered, format="JPEG")  # You can change the format if needed
    85|     img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
    86|     return img_str
    87| def create_multi_vector_retriever(vectorstore, image_summaries, images):
    88|     """
    89|     Create retriever that indexes summaries, but returns raw images or texts
    90|     :param vectorstore: Vectorstore to store embedded image sumamries
    91|     :param image_summaries: Image summaries
    92|     :param images: Base64 encoded images
    93|     :return: Retriever
    94|     """
    95|     store = LocalFileStore(
    96|         str(Path(__file__).parent / "multi_vector_retriever_metadata")
    97|     )
    98|     id_key = "doc_id"
    99|     retriever = MultiVectorRetriever(
   100|         vectorstore=vectorstore,
   101|         byte_store=store,
   102|         id_key=id_key,
   103|     )
   104|     def add_documents(retriever, doc_summaries, doc_contents):
   105|         doc_ids = [str(uuid.uuid4()) for _ in doc_contents]
   106|         summary_docs = [
   107|             Document(page_content=s, metadata={id_key: doc_ids[i]})
   108|             for i, s in enumerate(doc_summaries)
   109|         ]
   110|         retriever.vectorstore.add_documents(summary_docs)
   111|         retriever.docstore.mset(list(zip(doc_ids, doc_contents)))
   112|     add_documents(retriever, image_summaries, images)
   113|     return retriever
   114| doc_path = Path(__file__).parent / "docs/"
   115| rel_doc_path = doc_path.relative_to(Path.cwd())
   116| print("Read images")
   117| pil_images = get_images(rel_doc_path)
   118| images_base_64 = [convert_to_base64(i) for i in pil_images]
   119| print("Generate image summaries")
   120| image_summaries, images_base_64_processed = generate_img_summaries(images_base_64)
   121| vectorstore_mvr = Chroma(
   122|     collection_name="image_summaries",
   123|     persist_directory=str(Path(__file__).parent / "chroma_db_multi_modal"),
   124|     embedding_function=OllamaEmbeddings(model="llama2:7b"),
   125| )
   126| images_base_64_processed_documents = [
   127|     Document(page_content=i) for i in images_base_64_processed
   128| ]
   129| retriever_multi_vector_img = create_multi_vector_retriever(
   130|     vectorstore_mvr,
   131|     image_summaries,
   132|     images_base_64_processed_documents,
   133| )


# ====================================================================
# FILE: templates/rag-multi-modal-mv-local/rag_multi_modal_mv_local/__init__.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-2 ---
     1| from rag_multi_modal_mv_local.chain import chain
     2| __all__ = ["chain"]


# ====================================================================
# FILE: templates/rag-multi-modal-mv-local/rag_multi_modal_mv_local/chain.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-98 ---
     1| import base64
     2| import io
     3| from pathlib import Path
     4| from langchain.chat_models import ChatOllama
     5| from langchain.embeddings import OllamaEmbeddings
     6| from langchain.pydantic_v1 import BaseModel
     7| from langchain.retrievers.multi_vector import MultiVectorRetriever
     8| from langchain.schema.document import Document
     9| from langchain.schema.messages import HumanMessage
    10| from langchain.schema.output_parser import StrOutputParser
    11| from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
    12| from langchain.storage import LocalFileStore
    13| from langchain.vectorstores import Chroma
    14| from PIL import Image
    15| def resize_base64_image(base64_string, size=(128, 128)):
    16|     """
    17|     Resize an image encoded as a Base64 string.
    18|     :param base64_string: A Base64 encoded string of the image to be resized.
    19|     :param size: A tuple representing the new size (width, height) for the image.
    20|     :return: A Base64 encoded string of the resized image.
    21|     """
    22|     img_data = base64.b64decode(base64_string)
    23|     img = Image.open(io.BytesIO(img_data))
    24|     resized_img = img.resize(size, Image.LANCZOS)
    25|     buffered = io.BytesIO()
    26|     resized_img.save(buffered, format=img.format)
    27|     return base64.b64encode(buffered.getvalue()).decode("utf-8")
    28| def get_resized_images(docs):
    29|     """
    30|     Resize images from base64-encoded strings.
    31|     :param docs: A list of base64-encoded image to be resized.
    32|     :return: Dict containing a list of resized base64-encoded strings.
    33|     """
    34|     b64_images = []
    35|     for doc in docs:
    36|         if isinstance(doc, Document):
    37|             doc = doc.page_content
    38|         b64_images.append(doc)
    39|     return {"images": b64_images}
    40| def img_prompt_func(data_dict, num_images=1):
    41|     """
    42|     Ollama prompt for image analysis.
    43|     :param data_dict: A dict with images and a user-provided question.
    44|     :param num_images: Number of images to include in the prompt.
    45|     :return: A list containing message objects for each image and the text prompt.
    46|     """
    47|     messages = []
    48|     if data_dict["context"]["images"]:
    49|         for image in data_dict["context"]["images"][:num_images]:
    50|             image_message = {
    51|                 "type": "image_url",
    52|                 "image_url": f"data:image/jpeg;base64,{image}",
    53|             }
    54|             messages.append(image_message)
    55|     text_message = {
    56|         "type": "text",
    57|         "text": (
    58|             "You are a helpful assistant that gives a description of food pictures.\n"
    59|             "Give a detailed summary of the image.\n"
    60|         ),
    61|     }
    62|     messages.append(text_message)
    63|     return [HumanMessage(content=messages)]
    64| def multi_modal_rag_chain(retriever):
    65|     """
    66|     Multi-modal RAG chain,
    67|     :param retriever: A function that retrieves the necessary context for the model.
    68|     :return: A chain of functions representing the multi-modal RAG process.
    69|     """
    70|     model = ChatOllama(model="bakllava", temperature=0)
    71|     chain = (
    72|         {
    73|             "context": retriever | RunnableLambda(get_resized_images),
    74|             "question": RunnablePassthrough(),
    75|         }
    76|         | RunnableLambda(img_prompt_func)
    77|         | model
    78|         | StrOutputParser()
    79|     )
    80|     return chain
    81| vectorstore_mvr = Chroma(
    82|     collection_name="image_summaries",
    83|     persist_directory=str(Path(__file__).parent.parent / "chroma_db_multi_modal"),
    84|     embedding_function=OllamaEmbeddings(model="llama2:7b"),
    85| )
    86| store = LocalFileStore(
    87|     str(Path(__file__).parent.parent / "multi_vector_retriever_metadata")
    88| )
    89| id_key = "doc_id"
    90| retriever = MultiVectorRetriever(
    91|     vectorstore=vectorstore_mvr,
    92|     byte_store=store,
    93|     id_key=id_key,
    94| )
    95| chain = multi_modal_rag_chain(retriever)
    96| class Question(BaseModel):
    97|     __root__: str
    98| chain = chain.with_types(input_type=Question)


# ====================================================================
# FILE: templates/rag-vectara-multiquery/rag_vectara_multiquery/chain.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-25 ---
     1| import os
     2| from langchain.chat_models import ChatOpenAI
     3| from langchain.retrievers.multi_query import MultiQueryRetriever
     4| from langchain.vectorstores import Vectara
     5| from langchain_core.output_parsers import StrOutputParser
     6| from langchain_core.pydantic_v1 import BaseModel
     7| from langchain_core.runnables import RunnableParallel, RunnablePassthrough
     8| if os.environ.get("VECTARA_CUSTOMER_ID", None) is None:
     9|     raise Exception("Missing `VECTARA_CUSTOMER_ID` environment variable.")
    10| if os.environ.get("VECTARA_CORPUS_ID", None) is None:
    11|     raise Exception("Missing `VECTARA_CORPUS_ID` environment variable.")
    12| if os.environ.get("VECTARA_API_KEY", None) is None:
    13|     raise Exception("Missing `VECTARA_API_KEY` environment variable.")
    14| vectara_retriever = Vectara().as_retriever()
    15| llm = ChatOpenAI(temperature=0)
    16| retriever = MultiQueryRetriever.from_llm(retriever=vectara_retriever, llm=llm)
    17| model = ChatOpenAI()
    18| chain = (
    19|     RunnableParallel({"context": retriever, "question": RunnablePassthrough()})
    20|     | (lambda res: res[-1])
    21|     | StrOutputParser()
    22| )
    23| class Question(BaseModel):
    24|     __root__: str
    25| chain = chain.with_types(input_type=Question)


# ====================================================================
# FILE: templates/rag-vectara/rag_vectara/chain.py
# Total hunks: 1
# ====================================================================
# --- HUNK 1: Lines 1-20 ---
     1| import os
     2| from langchain.vectorstores import Vectara
     3| from langchain_core.output_parsers import StrOutputParser
     4| from langchain_core.pydantic_v1 import BaseModel
     5| from langchain_core.runnables import RunnableParallel, RunnablePassthrough
     6| if os.environ.get("VECTARA_CUSTOMER_ID", None) is None:
     7|     raise Exception("Missing `VECTARA_CUSTOMER_ID` environment variable.")
     8| if os.environ.get("VECTARA_CORPUS_ID", None) is None:
     9|     raise Exception("Missing `VECTARA_CORPUS_ID` environment variable.")
    10| if os.environ.get("VECTARA_API_KEY", None) is None:
    11|     raise Exception("Missing `VECTARA_API_KEY` environment variable.")
    12| retriever = Vectara().as_retriever()
    13| chain = (
    14|     RunnableParallel({"context": retriever, "question": RunnablePassthrough()})
    15|     | (lambda res: res[-1])
    16|     | StrOutputParser()
    17| )
    18| class Question(BaseModel):
    19|     __root__: str
    20| chain = chain.with_types(input_type=Question)

